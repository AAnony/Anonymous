[{"instruction": "Summarizing the code.", "input": "def _shuffle_tfrecord(path, random_gen):\n  \"\"\"\"\"\"\n  # Read all records\n  record_iter = tf.compat.v1.io.tf_record_iterator(path)\n  all_records = [\n      r for r in utils.tqdm(\n          record_iter, desc=\"Reading...\", unit=\" examples\", leave=False)\n  ]\n  # Shuffling in memory\n  random_gen.shuffle(all_records)\n  # Write all record back\n  with tf.io.TFRecordWriter(path) as writer:\n    for record in utils.tqdm(\n        all_records, desc=\"Writing...\", unit=\" examples\", leave=False):\n      writer.write(record)", "output": "Shuffle a single record file in memory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def append(self, key, _item):  # type: (Union[Key, str], Any) -> Table\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(_item, Item):\n            _item = item(_item)\n\n        self._value.append(key, _item)\n\n        if isinstance(key, Key):\n            key = key.key\n\n        if key is not None:\n            super(Table, self).__setitem__(key, _item)\n\n        m = re.match(\"(?s)^[^ ]*([ ]+).*$\", self._trivia.indent)\n        if not m:\n            return self\n\n        indent = m.group(1)\n\n        if not isinstance(_item, Whitespace):\n            m = re.match(\"(?s)^([^ ]*)(.*)$\", _item.trivia.indent)\n            if not m:\n                _item.trivia.indent = indent\n            else:\n                _item.trivia.indent = m.group(1) + indent + m.group(2)\n\n        return self", "output": "Appends a (key, item) to the table.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_principal(name, enctypes=None):\n    '''\n    \n    '''\n    ret = {}\n\n    krb_cmd = 'addprinc -randkey'\n\n    if enctypes:\n        krb_cmd += ' -e {0}'.format(enctypes)\n\n    krb_cmd += ' {0}'.format(name)\n\n    cmd = __execute_kadmin(krb_cmd)\n\n    if cmd['retcode'] != 0 or cmd['stderr']:\n        if not cmd['stderr'].splitlines()[-1].startswith('WARNING:'):\n            ret['comment'] = cmd['stderr'].splitlines()[-1]\n            ret['result'] = False\n\n            return ret\n\n    return True", "output": "Create Principal\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'kdc.example.com' kerberos.create_principal host/example.com", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _load_params(params, logger=logging):\n    \"\"\"\n    \"\"\"\n    if isinstance(params, str):\n        cur_path = os.path.dirname(os.path.realpath(__file__))\n        param_file_path = os.path.join(cur_path, params)\n        logger.info('Loading params from file %s' % param_file_path)\n        save_dict = nd_load(param_file_path)\n        arg_params = {}\n        aux_params = {}\n        for k, v in save_dict.items():\n            tp, name = k.split(':', 1)\n            if tp == 'arg':\n                arg_params[name] = v\n            if tp == 'aux':\n                aux_params[name] = v\n        return arg_params, aux_params\n    elif isinstance(params, (tuple, list)) and len(params) == 2:\n        return params[0], params[1]\n    else:\n        raise ValueError('Unsupported params provided. Must be either a path to the param file or'\n                         ' a pair of dictionaries representing arg_params and aux_params')", "output": "Given a str as a path to the .params file or a pair of params,\n    returns two dictionaries representing arg_params and aux_params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(self):\n        \"\"\n        if not self.removed:\n            self.hook.remove()\n            self.removed=True", "output": "Remove the hook from the model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shuffle_step(entries, step):\n    '''\n    \n    '''\n    answer = []\n    for i in range(0, len(entries), step):\n        sub = entries[i:i+step]\n        shuffle(sub)\n        answer += sub\n    return answer", "output": "Shuffle the step", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imagetransformerpp_base_8l_8h_big_cond_dr03_dan():\n  \"\"\"\"\"\"\n  hparams = imagetransformerpp_sep_channels_8l_8h()\n  hparams.hidden_size = 512\n  hparams.num_heads = 8\n  hparams.filter_size = 2048\n  hparams.batch_size = 4\n  hparams.max_length = 3075\n  hparams.layer_prepostprocess_dropout = 0.3\n  hparams.layer_preprocess_sequence = \"none\"\n  hparams.layer_postprocess_sequence = \"dan\"\n  hparams.summarize_grads = True\n  hparams.learning_rate = 0.01\n  return hparams", "output": "big 1d model for conditional image generation.2.99 on cifar10.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _enum_elements(name, server=None):\n    '''\n    \n    '''\n    elements = []\n    data = _api_get(name, server)\n\n    if any(data['extraProperties']['childResources']):\n        for element in data['extraProperties']['childResources']:\n            elements.append(element)\n        return elements\n    return None", "output": "Enum elements", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sample(path, start, length):\n    \"\"\"\n    \n    \"\"\"\n    # initialize vocabulary and sequence length\n    param.seq_len = 1\n    ds = CharRNNData(param.corpus, 100000)\n\n    pred = OfflinePredictor(PredictConfig(\n        model=Model(),\n        session_init=SaverRestore(path),\n        input_names=['input', 'c0', 'h0', 'c1', 'h1'],\n        output_names=['prob', 'last_state']))\n\n    # feed the starting sentence\n    initial = np.zeros((1, param.rnn_size))\n    for c in start[:-1]:\n        x = np.array([[ds.char2idx[c]]], dtype='int32')\n        _, state = pred(x, initial, initial, initial, initial)\n\n    def pick(prob):\n        t = np.cumsum(prob)\n        s = np.sum(prob)\n        return(int(np.searchsorted(t, np.random.rand(1) * s)))\n\n    # generate more\n    ret = start\n    c = start[-1]\n    for k in range(length):\n        x = np.array([[ds.char2idx[c]]], dtype='int32')\n        prob, state = pred(x, state[0, 0], state[0, 1], state[1, 0], state[1, 1])\n        c = ds.chars[pick(prob[0])]\n        ret += c\n    print(ret)", "output": ":param path: path to the model\n    :param start: a `str`. the starting characters\n    :param length: a `int`. the length of text to generate", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_layer_converter_fn(layer, add_custom_layers = False):\n    \"\"\"\n    \"\"\"\n    layer_type = type(layer)\n    if layer_type in _KERAS_LAYER_REGISTRY:\n        convert_func = _KERAS_LAYER_REGISTRY[layer_type]\n        if convert_func is _layers2.convert_activation:\n            act_name = _layers2._get_activation_name_from_keras_layer(layer)\n            if act_name == 'CUSTOM':\n                return None\n        return convert_func\n    elif add_custom_layers:\n        return None\n    else:\n        raise TypeError(\"Keras layer of type %s is not supported.\" % type(layer))", "output": "Get the right converter function for Keras", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_feature(value):\n  \"\"\"\"\"\"\n  if isinstance(value, FeatureConnector):\n    return value\n  elif utils.is_dtype(value):  # tf.int32, tf.string,...\n    return Tensor(shape=(), dtype=tf.as_dtype(value))\n  elif isinstance(value, dict):\n    return FeaturesDict(value)\n  else:\n    raise ValueError('Feature not supported: {}'.format(value))", "output": "Convert the given value to Feature if necessary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put_multi(self, entities):\n        \"\"\"\n        \"\"\"\n        if isinstance(entities, Entity):\n            raise ValueError(\"Pass a sequence of entities\")\n\n        if not entities:\n            return\n\n        current = self.current_batch\n        in_batch = current is not None\n\n        if not in_batch:\n            current = self.batch()\n            current.begin()\n\n        for entity in entities:\n            current.put(entity)\n\n        if not in_batch:\n            current.commit()", "output": "Save entities in the Cloud Datastore.\n\n        :type entities: list of :class:`google.cloud.datastore.entity.Entity`\n        :param entities: The entities to be saved to the datastore.\n\n        :raises: :class:`ValueError` if ``entities`` is a single entity.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_coeffs(orig_pts:Points, targ_pts:Points)->Tensor:\n    \"\"\n    matrix = []\n    #The equations we'll need to solve.\n    for p1, p2 in zip(targ_pts, orig_pts):\n        matrix.append([p1[0], p1[1], 1, 0, 0, 0, -p2[0]*p1[0], -p2[0]*p1[1]])\n        matrix.append([0, 0, 0, p1[0], p1[1], 1, -p2[1]*p1[0], -p2[1]*p1[1]])\n\n    A = FloatTensor(matrix)\n    B = FloatTensor(orig_pts).view(8, 1)\n    #The 8 scalars we seek are solution of AX = B\n    return _solve_func(B,A)[0][:,0]", "output": "Find 8 coeff mentioned [here](https://web.archive.org/web/20150222120106/xenia.media.mit.edu/~cwren/interpolator/).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sorted(self, wantdirs=False):\n        \"\"\"\n        \n        \"\"\"\n\n        def add_dir(dirs, d):\n            dirs.add(d)\n            logger.debug('add_dir added %s', d)\n            if d != self.base:\n                parent, _ = os.path.split(d)\n                assert parent not in ('', '/')\n                add_dir(dirs, parent)\n\n        result = set(self.files)    # make a copy!\n        if wantdirs:\n            dirs = set()\n            for f in result:\n                add_dir(dirs, os.path.dirname(f))\n            result |= dirs\n        return [os.path.join(*path_tuple) for path_tuple in\n                sorted(os.path.split(path) for path in result)]", "output": "Return sorted files in directory order", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_policy(policyName, policyDocument,\n            region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        if not isinstance(policyDocument, string_types):\n            policyDocument = salt.utils.json.dumps(policyDocument)\n        policy = conn.create_policy(policyName=policyName,\n                                    policyDocument=policyDocument)\n        if policy:\n            log.info('The newly created policy version is %s', policy['policyVersionId'])\n\n            return {'created': True, 'versionId': policy['policyVersionId']}\n        else:\n            log.warning('Policy was not created')\n            return {'created': False}\n    except ClientError as e:\n        return {'created': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Given a valid config, create a policy.\n\n    Returns {created: true} if the policy was created and returns\n    {created: False} if the policy was not created.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iot.create_policy my_policy \\\\\n              '{\"Version\":\"2015-12-12\",\\\\\n              \"Statement\":[{\"Effect\":\"Allow\",\\\\\n                            \"Action\":[\"iot:Publish\"],\\\\\n                            \"Resource\":[\"arn:::::topic/foo/bar\"]}]}'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _SetPath(self, path):\n    \"\"\"\n    \"\"\"\n    old_path = self._path\n    if old_path and not io_wrapper.IsCloudPath(old_path):\n      try:\n        # We're done with the path, so store its size.\n        size = tf.io.gfile.stat(old_path).length\n        logger.debug('Setting latest size of %s to %d', old_path, size)\n        self._finalized_sizes[old_path] = size\n      except tf.errors.OpError as e:\n        logger.error('Unable to get size of %s: %s', old_path, e)\n\n    self._path = path\n    self._loader = self._loader_factory(path)", "output": "Sets the current path to watch for new events.\n\n    This also records the size of the old path, if any. If the size can't be\n    found, an error is logged.\n\n    Args:\n      path: The full path of the file to watch.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def construct_error_message(driver_id, error_type, message, timestamp):\n    \"\"\"\n    \"\"\"\n    builder = flatbuffers.Builder(0)\n    driver_offset = builder.CreateString(driver_id.binary())\n    error_type_offset = builder.CreateString(error_type)\n    message_offset = builder.CreateString(message)\n\n    ray.core.generated.ErrorTableData.ErrorTableDataStart(builder)\n    ray.core.generated.ErrorTableData.ErrorTableDataAddDriverId(\n        builder, driver_offset)\n    ray.core.generated.ErrorTableData.ErrorTableDataAddType(\n        builder, error_type_offset)\n    ray.core.generated.ErrorTableData.ErrorTableDataAddErrorMessage(\n        builder, message_offset)\n    ray.core.generated.ErrorTableData.ErrorTableDataAddTimestamp(\n        builder, timestamp)\n    error_data_offset = ray.core.generated.ErrorTableData.ErrorTableDataEnd(\n        builder)\n    builder.Finish(error_data_offset)\n\n    return bytes(builder.Output())", "output": "Construct a serialized ErrorTableData object.\n\n    Args:\n        driver_id: The ID of the driver that the error should go to. If this is\n            nil, then the error will go to all drivers.\n        error_type: The type of the error.\n        message: The error message.\n        timestamp: The time of the error.\n\n    Returns:\n        The serialized object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def link(src, path):\n    '''\n    \n    '''\n    src = os.path.expanduser(src)\n\n    if not os.path.isabs(src):\n        raise SaltInvocationError('File path must be absolute.')\n\n    try:\n        os.link(src, path)\n        return True\n    except (OSError, IOError):\n        raise CommandExecutionError('Could not create \\'{0}\\''.format(path))\n    return False", "output": ".. versionadded:: 2014.1.0\n\n    Create a hard link to a file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.link /path/to/file /path/to/link", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_all(recommended=False, restart=True):\n    '''\n    \n    '''\n    to_update = _get_available(recommended, restart)\n\n    if not to_update:\n        return {}\n\n    for _update in to_update:\n        cmd = ['softwareupdate', '--install', _update]\n        salt.utils.mac_utils.execute_return_success(cmd)\n\n    ret = {}\n    updates_left = _get_available()\n\n    for _update in to_update:\n        ret[_update] = True if _update not in updates_left else False\n\n    return ret", "output": "Install all available updates. Returns a dictionary containing the name\n    of the update and the status of its installation.\n\n    :param bool recommended: If set to True, only install the recommended\n        updates. If set to False (default) all updates are installed.\n\n    :param bool restart: Set this to False if you do not want to install updates\n        that require a restart. Default is True\n\n    :return: A dictionary containing the updates that were installed and the\n        status of its installation. If no updates were installed an empty\n        dictionary is returned.\n\n    :rtype: dict\n\n    CLI Example:\n\n    .. code-block:: bash\n\n       salt '*' softwareupdate.update_all", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setIfMissing(self, key, value):\n        \"\"\"\"\"\"\n        if self.get(key) is None:\n            self.set(key, value)\n        return self", "output": "Set a configuration property, if not already set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, resource, client):\n        \"\"\"\n        \"\"\"\n        job_id, config_resource = cls._get_resource_config(resource)\n        config = CopyJobConfig.from_api_repr(config_resource)\n        # Copy required fields to the job.\n        copy_resource = config_resource[\"copy\"]\n        destination = TableReference.from_api_repr(copy_resource[\"destinationTable\"])\n        sources = []\n        source_configs = copy_resource.get(\"sourceTables\")\n        if source_configs is None:\n            single = copy_resource.get(\"sourceTable\")\n            if single is None:\n                raise KeyError(\"Resource missing 'sourceTables' / 'sourceTable'\")\n            source_configs = [single]\n        for source_config in source_configs:\n            table_ref = TableReference.from_api_repr(source_config)\n            sources.append(table_ref)\n        job = cls(job_id, sources, destination, client=client, job_config=config)\n        job._set_properties(resource)\n        return job", "output": "Factory:  construct a job given its API representation\n\n        .. note:\n\n           This method assumes that the project found in the resource matches\n           the client's project.\n\n        :type resource: dict\n        :param resource: dataset job representation returned from the API\n\n        :type client: :class:`google.cloud.bigquery.client.Client`\n        :param client: Client which holds credentials and project\n                       configuration for the dataset.\n\n        :rtype: :class:`google.cloud.bigquery.job.CopyJob`\n        :returns: Job parsed from ``resource``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_repository(self, repository, body, params=None):\n        \"\"\"\n        \n        \"\"\"\n        for param in (repository, body):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request('PUT', _make_path('_snapshot',\n            repository), params=params, body=body)", "output": "Registers a shared file system repository.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html>`_\n\n        :arg repository: A repository name\n        :arg body: The repository definition\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg timeout: Explicit operation timeout\n        :arg verify: Whether to verify the repository after creation", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def _retrieve_guilds_after_strategy(self, retrieve):\n        \"\"\"\"\"\"\n        after = self.after.id if self.after else None\n        data = await self.get_guilds(retrieve, after=after)\n        if len(data):\n            if self.limit is not None:\n                self.limit -= retrieve\n            self.after = Object(id=int(data[0]['id']))\n        return data", "output": "Retrieve guilds using after parameter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def disable(name, stop=False, **kwargs):\n    '''\n    \n    '''\n\n    # non-existent as registrered service\n    if not enabled(name):\n        return False\n\n    # down_file: file that prevent sv autostart\n    svc_realpath = _get_svc_path(name)[0]\n    down_file = os.path.join(svc_realpath, 'down')\n\n    if stop:\n        stop(name)\n\n    if not os.path.exists(down_file):\n        try:\n            salt.utils.files.fopen(down_file, \"w\").close()  # pylint: disable=resource-leakage\n        except IOError:\n            log.error('Unable to create file %s', down_file)\n            return False\n\n    return True", "output": "Don't start service ``name`` at boot\n    Returns ``True`` if operation is successful\n\n    name\n        the service's name\n\n    stop\n        if True, also stops the service\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.disable <name> [stop=True]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def datasets_list_files(self, owner_slug, dataset_slug, **kwargs):  # noqa: E501\n        \"\"\"\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.datasets_list_files_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501\n        else:\n            (data) = self.datasets_list_files_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501\n            return data", "output": "List dataset files  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.datasets_list_files(owner_slug, dataset_slug, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str owner_slug: Dataset owner (required)\n        :param str dataset_slug: Dataset name (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def path(self):\n        \"\"\"\n        \n        \"\"\"\n        location = self.client.table_location(self.table, self.database)\n        if not location:\n            raise Exception(\"Couldn't find location for table: {0}\".format(str(self)))\n        return location", "output": "Returns the path to this table in HDFS.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_valid_project(self, path):\r\n        \"\"\"\"\"\"\r\n        spy_project_dir = osp.join(path, '.spyproject')\r\n        if osp.isdir(path) and osp.isdir(spy_project_dir):\r\n            return True\r\n        else:\r\n            return False", "output": "Check if a directory is a valid Spyder project", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve_pos(token):\n    \"\"\"\n    \"\"\"\n    # TODO: This is a first take. The rules here are crude approximations.\n    # For many of these, full dependencies are needed to properly resolve\n    # PoS mappings.\n    if token.pos == \"\u9023\u4f53\u8a5e,*,*,*\":\n        if re.match(r\"[\u3053\u305d\u3042\u3069\u6b64\u5176\u5f7c]\u306e\", token.surface):\n            return token.pos + \",DET\"\n        if re.match(r\"[\u3053\u305d\u3042\u3069\u6b64\u5176\u5f7c]\", token.surface):\n            return token.pos + \",PRON\"\n        return token.pos + \",ADJ\"\n    return token.pos", "output": "If necessary, add a field to the POS tag for UD mapping.\n    Under Universal Dependencies, sometimes the same Unidic POS tag can\n    be mapped differently depending on the literal token or its context\n    in the sentence. This function adds information to the POS tag to\n    resolve ambiguous mappings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_update_available(self):\n        \"\"\"\n        \"\"\"\n        # Don't perform any check for development versions\n        if 'dev' in self.version:\n            return (False, latest_release)\n\n        # Filter releases\n        if is_stable_version(self.version):\n            releases = [r for r in self.releases if is_stable_version(r)]\n        else:\n            releases = [r for r in self.releases\n                        if not is_stable_version(r) or r in self.version]\n\n        latest_release = releases[-1]\n\n        return (check_version(self.version, latest_release, '<'),\n                latest_release)", "output": "Checks if there is an update available.\n\n        It takes as parameters the current version of Spyder and a list of\n        valid cleaned releases in chronological order.\n        Example: ['2.3.2', '2.3.3' ...] or with github ['2.3.4', '2.3.3' ...]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pairwise_euclid_distance(A, B):\n    \"\"\"\n    \"\"\"\n    batchA = tf.shape(A)[0]\n    batchB = tf.shape(B)[0]\n\n    sqr_norm_A = tf.reshape(tf.reduce_sum(tf.pow(A, 2), 1), [1, batchA])\n    sqr_norm_B = tf.reshape(tf.reduce_sum(tf.pow(B, 2), 1), [batchB, 1])\n    inner_prod = tf.matmul(B, A, transpose_b=True)\n\n    tile_1 = tf.tile(sqr_norm_A, [batchB, 1])\n    tile_2 = tf.tile(sqr_norm_B, [1, batchA])\n    return (tile_1 + tile_2 - 2 * inner_prod)", "output": "Pairwise Euclidean distance between two matrices.\n    :param A: a matrix.\n    :param B: a matrix.\n\n    :returns: A tensor for the pairwise Euclidean between A and B.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dropna(self, how='any', thresh=None, subset=None):\n        \"\"\"\n        \"\"\"\n        if how is not None and how not in ['any', 'all']:\n            raise ValueError(\"how ('\" + how + \"') should be 'any' or 'all'\")\n\n        if subset is None:\n            subset = self.columns\n        elif isinstance(subset, basestring):\n            subset = [subset]\n        elif not isinstance(subset, (list, tuple)):\n            raise ValueError(\"subset should be a list or tuple of column names\")\n\n        if thresh is None:\n            thresh = len(subset) if how == 'any' else 1\n\n        return DataFrame(self._jdf.na().drop(thresh, self._jseq(subset)), self.sql_ctx)", "output": "Returns a new :class:`DataFrame` omitting rows with null values.\n        :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n\n        :param how: 'any' or 'all'.\n            If 'any', drop a row if it contains any nulls.\n            If 'all', drop a row only if all its values are null.\n        :param thresh: int, default None\n            If specified, drop rows that have less than `thresh` non-null values.\n            This overwrites the `how` parameter.\n        :param subset: optional list of column names to consider.\n\n        >>> df4.na.drop().show()\n        +---+------+-----+\n        |age|height| name|\n        +---+------+-----+\n        | 10|    80|Alice|\n        +---+------+-----+", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_to_border(cls, border_dict):\n        \"\"\"\n        \n        \"\"\"\n\n        from openpyxl.styles import Border\n\n        _border_key_map = {\n            'diagonalup': 'diagonalUp',\n            'diagonaldown': 'diagonalDown',\n        }\n\n        border_kwargs = {}\n        for k, v in border_dict.items():\n            if k in _border_key_map:\n                k = _border_key_map[k]\n            if k == 'color':\n                v = cls._convert_to_color(v)\n            if k in ['left', 'right', 'top', 'bottom', 'diagonal']:\n                v = cls._convert_to_side(v)\n            border_kwargs[k] = v\n\n        return Border(**border_kwargs)", "output": "Convert ``border_dict`` to an openpyxl v2 Border object\n        Parameters\n        ----------\n        border_dict : dict\n            A dict with zero or more of the following keys (or their synonyms).\n                'left'\n                'right'\n                'top'\n                'bottom'\n                'diagonal'\n                'diagonal_direction'\n                'vertical'\n                'horizontal'\n                'diagonalUp' ('diagonalup')\n                'diagonalDown' ('diagonaldown')\n                'outline'\n        Returns\n        -------\n        border : openpyxl.styles.Border", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main():\n    \"\"\"\"\"\"\n    desc = 'Generate character statistics from a source tree'\n    parser = argparse.ArgumentParser(description=desc)\n    parser.add_argument(\n        '--src',\n        dest='src',\n        required=True,\n        help='The root of the source tree'\n    )\n    parser.add_argument(\n        '--out',\n        dest='out',\n        default='chars.py',\n        help='The output filename'\n    )\n\n    args = parser.parse_args()\n\n    stats = generate_statistics(args.src)\n    with open(args.out, 'wb') as out_f:\n        out_f.write('CHARS={0}\\n'.format(stats))", "output": "The main function of the script", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def annotation_path(cls, project, incident, annotation):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/incidents/{incident}/annotations/{annotation}\",\n            project=project,\n            incident=incident,\n            annotation=annotation,\n        )", "output": "Return a fully-qualified annotation string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __clean_tmp(sfn):\n    '''\n    \n    '''\n    if sfn.startswith(os.path.join(tempfile.gettempdir(),\n                                   salt.utils.files.TEMPFILE_PREFIX)):\n        # Don't remove if it exists in file_roots (any saltenv)\n        all_roots = itertools.chain.from_iterable(\n                six.itervalues(__opts__['file_roots']))\n        in_roots = any(sfn.startswith(root) for root in all_roots)\n        # Only clean up files that exist\n        if os.path.exists(sfn) and not in_roots:\n            os.remove(sfn)", "output": "Clean out a template temp file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def catalog(self):\n        \"\"\"\n        \"\"\"\n        from pyspark.sql.catalog import Catalog\n        if not hasattr(self, \"_catalog\"):\n            self._catalog = Catalog(self)\n        return self._catalog", "output": "Interface through which the user may create, drop, alter or query underlying\n        databases, tables, functions etc.\n\n        :return: :class:`Catalog`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode(self, s):\n    \"\"\"\n    \"\"\"\n    try:\n      import matplotlib.image as im  # pylint: disable=g-import-not-at-top\n    except ImportError as e:\n      tf.logging.warning(\n          \"Reading an image requires matplotlib to be installed: %s\", e)\n      raise NotImplementedError(\"Image reading not implemented.\")\n    return im.imread(s)", "output": "Transform a string with a filename into a list of RGB integers.\n\n    Args:\n      s: path to the file with an image.\n\n    Returns:\n      ids: list of integers", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download_mp4(from_idx, to_idx, _params):\n    \"\"\"\n    \n    \"\"\"\n    succ = set()\n    fail = set()\n    for idx in range(from_idx, to_idx):\n        name = 's' + str(idx)\n        save_folder = '{src_path}/{nm}'.format(src_path=_params['src_path'], nm=name)\n        if idx == 0 or os.path.isdir(save_folder):\n            continue\n        script = \"http://spandh.dcs.shef.ac.uk/gridcorpus/{nm}/video/{nm}.mpg_vcd.zip\".format( \\\n                    nm=name)\n        down_sc = 'cd {src_path} && curl {script} --output {nm}.mpg_vcd.zip && \\\n                    unzip {nm}.mpg_vcd.zip'.format(script=script,\n                                                   nm=name,\n                                                   src_path=_params['src_path'])\n        try:\n            print(down_sc)\n            os.system(down_sc)\n            succ.add(idx)\n        except OSError as error:\n            print(error)\n            fail.add(idx)\n    return (succ, fail)", "output": "download mp4s", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def MapDecoder(field_descriptor, new_default, is_message_map):\n  \"\"\"\"\"\"\n\n  key = field_descriptor\n  tag_bytes = encoder.TagBytes(field_descriptor.number,\n                               wire_format.WIRETYPE_LENGTH_DELIMITED)\n  tag_len = len(tag_bytes)\n  local_DecodeVarint = _DecodeVarint\n  # Can't read _concrete_class yet; might not be initialized.\n  message_type = field_descriptor.message_type\n\n  def DecodeMap(buffer, pos, end, message, field_dict):\n    submsg = message_type._concrete_class()\n    value = field_dict.get(key)\n    if value is None:\n      value = field_dict.setdefault(key, new_default(message))\n    while 1:\n      # Read length.\n      (size, pos) = local_DecodeVarint(buffer, pos)\n      new_pos = pos + size\n      if new_pos > end:\n        raise _DecodeError('Truncated message.')\n      # Read sub-message.\n      submsg.Clear()\n      if submsg._InternalParse(buffer, pos, new_pos) != new_pos:\n        # The only reason _InternalParse would return early is if it\n        # encountered an end-group tag.\n        raise _DecodeError('Unexpected end-group tag.')\n\n      if is_message_map:\n        value[submsg.key].MergeFrom(submsg.value)\n      else:\n        value[submsg.key] = submsg.value\n\n      # Predict that the next tag is another copy of the same repeated field.\n      pos = new_pos + tag_len\n      if buffer[new_pos:pos] != tag_bytes or new_pos == end:\n        # Prediction failed.  Return.\n        return new_pos\n\n  return DecodeMap", "output": "Returns a decoder for a map field.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_summary_metadata(display_name, description, num_thresholds):\n  \"\"\"\n  \"\"\"\n  pr_curve_plugin_data = plugin_data_pb2.PrCurvePluginData(\n      version=PROTO_VERSION, num_thresholds=num_thresholds)\n  content = pr_curve_plugin_data.SerializeToString()\n  return summary_pb2.SummaryMetadata(\n      display_name=display_name,\n      summary_description=description,\n      plugin_data=summary_pb2.SummaryMetadata.PluginData(\n          plugin_name=PLUGIN_NAME,\n          content=content))", "output": "Create a `summary_pb2.SummaryMetadata` proto for pr_curves plugin data.\n\n  Arguments:\n    display_name: The display name used in TensorBoard.\n    description: The description to show in TensorBoard.\n    num_thresholds: The number of thresholds to use for PR curves.\n\n  Returns:\n    A `summary_pb2.SummaryMetadata` protobuf object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def error_msg_from_exception(e):\n    \"\"\"\n    \"\"\"\n    msg = ''\n    if hasattr(e, 'message'):\n        if isinstance(e.message, dict):\n            msg = e.message.get('message')\n        elif e.message:\n            msg = '{}'.format(e.message)\n    return msg or '{}'.format(e)", "output": "Translate exception into error message\n\n    Database have different ways to handle exception. This function attempts\n    to make sense of the exception object and construct a human readable\n    sentence.\n\n    TODO(bkyryliuk): parse the Presto error message from the connection\n                     created via create_engine.\n    engine = create_engine('presto://localhost:3506/silver') -\n      gives an e.message as the str(dict)\n    presto.connect('localhost', port=3506, catalog='silver') - as a dict.\n    The latter version is parsed correctly by this function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _connect(**kwargs):\n    '''\n    \n    '''\n    connargs = {}\n    for name in ['uri', 'server', 'port', 'tls', 'no_verify', 'binddn',\n                 'bindpw', 'anonymous']:\n        connargs[name] = _config(name, **kwargs)\n\n    return _LDAPConnection(**connargs).ldap", "output": "Instantiate LDAP Connection class and return an LDAP connection object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pprint(self, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        pprint.pprint(self.asList(), *args, **kwargs)", "output": "Pretty-printer for parsed results as a list, using the\n        `pprint <https://docs.python.org/3/library/pprint.html>`_ module.\n        Accepts additional positional or keyword args as defined for\n        `pprint.pprint <https://docs.python.org/3/library/pprint.html#pprint.pprint>`_ .\n\n        Example::\n\n            ident = Word(alphas, alphanums)\n            num = Word(nums)\n            func = Forward()\n            term = ident | num | Group('(' + func + ')')\n            func <<= ident + Group(Optional(delimitedList(term)))\n            result = func.parseString(\"fna a,b,(fnb c,d,200),100\")\n            result.pprint(width=40)\n\n        prints::\n\n            ['fna',\n             ['a',\n              'b',\n              ['(', 'fnb', ['c', 'd', '200'], ')'],\n              '100']]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filter_by_func(self, func:Callable)->'ItemList':\n        \"\"\n        self.items = array([o for o in self.items if func(o)])\n        return self", "output": "Only keep elements for which `func` returns `True`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _path_condition_name(self, api_id, path):\n        \"\"\"\n        \n        \"\"\"\n        # only valid characters for CloudFormation logical id are [A-Za-z0-9], but swagger paths can contain\n        # slashes and curly braces for templated params, e.g., /foo/{customerId}. So we'll replace\n        # non-alphanumeric characters.\n        path_logical_id = path.replace('/', 'SLASH').replace('{', 'OB').replace('}', 'CB')\n        return '{}{}PathCondition'.format(api_id, path_logical_id)", "output": "Generate valid condition logical id from the given API logical id and swagger resource path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_request_body(fn):\n    '''\n    \n    '''\n    @functools.wraps(fn)\n    def wrapped(*args, **kwargs):  # pylint: disable=C0111\n        if cherrypy.request.process_request_body is not False:\n            fn(*args, **kwargs)\n    return wrapped", "output": "A decorator to skip a processor function if process_request_body is False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_column(self, data, column_name=\"\", inplace=False):\n        \"\"\"\n        \n        \"\"\"\n        # Check type for pandas dataframe or SArray?\n        if not isinstance(data, SArray):\n            raise TypeError(\"Must give column as SArray\")\n        if not isinstance(column_name, str):\n            raise TypeError(\"Invalid column name: must be str\")\n\n        if inplace:\n            self.__is_dirty__ = True\n            with cython_context():\n                if self._is_vertex_frame():\n                    graph_proxy = self.__graph__.__proxy__.add_vertex_field(data.__proxy__, column_name)\n                    self.__graph__.__proxy__ = graph_proxy\n                elif self._is_edge_frame():\n                    graph_proxy = self.__graph__.__proxy__.add_edge_field(data.__proxy__, column_name)\n                    self.__graph__.__proxy__ = graph_proxy\n            return self\n        else:\n            return super(GFrame, self).add_column(data, column_name, inplace=inplace)", "output": "Adds the specified column to this SFrame.  The number of elements in\n        the data given must match every other column of the SFrame.\n\n        If inplace == False (default) this operation does not modify the\n        current SFrame, returning a new SFrame.\n\n        If inplace == True, this operation modifies the current\n        SFrame, returning self.\n\n        Parameters\n        ----------\n        data : SArray\n            The 'column' of data.\n\n        column_name : string\n            The name of the column. If no name is given, a default name is chosen.\n\n        inplace : bool, optional. Defaults to False.\n            Whether the SFrame is modified in place.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reraise(additional_msg):\n  \"\"\"\"\"\"\n  exc_type, exc_value, exc_traceback = sys.exc_info()\n  msg = str(exc_value) + \"\\n\" + additional_msg\n  six.reraise(exc_type, exc_type(msg), exc_traceback)", "output": "Reraise an exception with an additional message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _memoize(f):\n  \"\"\"\"\"\"\n  nothing = object()  # Unique \"no value\" sentinel object.\n  cache = {}\n  # Use a reentrant lock so that if f references the resulting wrapper we die\n  # with recursion depth exceeded instead of deadlocking.\n  lock = threading.RLock()\n  @functools.wraps(f)\n  def wrapper(arg):\n    if cache.get(arg, nothing) is nothing:\n      with lock:\n        if cache.get(arg, nothing) is nothing:\n          cache[arg] = f(arg)\n    return cache[arg]\n  return wrapper", "output": "Memoizing decorator for f, which must have exactly 1 hashable argument.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_close(self):\n        ''' \n\n        '''\n        log.info('WebSocket connection closed: code=%s, reason=%r', self.close_code, self.close_reason)\n        if self.connection is not None:\n            self.application.client_lost(self.connection)", "output": "Clean up when the connection is closed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lossless_float_to_int(funcname, func, argname, arg):\n    \"\"\"\n    \n    \"\"\"\n    if not isinstance(arg, float):\n        return arg\n\n    arg_as_int = int(arg)\n    if arg == arg_as_int:\n        warnings.warn(\n            \"{f} expected an int for argument {name!r}, but got float {arg}.\"\n            \" Coercing to int.\".format(\n                f=funcname,\n                name=argname,\n                arg=arg,\n            ),\n        )\n        return arg_as_int\n\n    raise TypeError(arg)", "output": "A preprocessor that coerces integral floats to ints.\n\n    Receipt of non-integral floats raises a TypeError.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_indicator_MFI(DataFrame, N=14):\n    \"\"\"\n    \n    \"\"\"\n    C = DataFrame['close']\n    H = DataFrame['high']\n    L = DataFrame['low']\n    VOL = DataFrame['volume']\n    TYP = (C + H + L) / 3\n    V1 = SUM(IF(TYP > REF(TYP, 1), TYP * VOL, 0), N) / \\\n        SUM(IF(TYP < REF(TYP, 1), TYP * VOL, 0), N)\n    mfi = 100 - (100 / (1 + V1))\n    DICT = {'MFI': mfi}\n\n    return pd.DataFrame(DICT)", "output": "\u8d44\u91d1\u6307\u6807\n    TYP := (HIGH + LOW + CLOSE)/3;\n    V1:=SUM(IF(TYP>REF(TYP,1),TYP*VOL,0),N)/SUM(IF(TYP<REF(TYP,1),TYP*VOL,0),N);\n    MFI:100-(100/(1+V1));\n    \u8d4b\u503c: (\u6700\u9ad8\u4ef7 + \u6700\u4f4e\u4ef7 + \u6536\u76d8\u4ef7)/3\n    V1\u8d4b\u503c:\u5982\u679cTYP>1\u65e5\u524d\u7684TYP,\u8fd4\u56deTYP*\u6210\u4ea4\u91cf(\u624b),\u5426\u5219\u8fd4\u56de0\u7684N\u65e5\u7d2f\u548c/\u5982\u679cTYP<1\u65e5\u524d\u7684TYP,\u8fd4\u56deTYP*\u6210\u4ea4\u91cf(\u624b),\u5426\u5219\u8fd4\u56de0\u7684N\u65e5\u7d2f\u548c\n    \u8f93\u51fa\u8d44\u91d1\u6d41\u91cf\u6307\u6807:100-(100/(1+V1))", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expand(tmpl, *args, **kwargs):\n    \"\"\"\n    \"\"\"\n    replacer = functools.partial(_expand_variable_match, list(args), kwargs)\n    return _VARIABLE_RE.sub(replacer, tmpl)", "output": "Expand a path template with the given variables.\n\n    ..code-block:: python\n\n        >>> expand('users/*/messages/*', 'me', '123')\n        users/me/messages/123\n        >>> expand('/v1/{name=shelves/*/books/*}', name='shelves/1/books/3')\n        /v1/shelves/1/books/3\n\n    Args:\n        tmpl (str): The path template.\n        args: The positional variables for the path.\n        kwargs: The named variables for the path.\n\n    Returns:\n        str: The expanded path\n\n    Raises:\n        ValueError: If a positional or named variable is required by the\n            template but not specified or if an unexpected template expression\n            is encountered.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def items(self):\n        \"\"\"\"\"\"\n        result = [(key, self._mapping[key]) for key in list(self._queue)]\n        result.reverse()\n        return result", "output": "Return a list of items.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait_for_winexe(host, port, username, password, timeout=900):\n    '''\n    \n    '''\n    start = time.time()\n    log.debug(\n        'Attempting winexe connection to host %s on port %s',\n        host, port\n    )\n    try_count = 0\n    while True:\n        try_count += 1\n        try:\n            # Shell out to winexe to check %TEMP%\n            ret_code = run_winexe_command(\n                \"sc\", \"query winexesvc\", host, username, password, port\n            )\n            if ret_code == 0:\n                log.debug('winexe connected...')\n                return True\n            log.debug('Return code was %s', ret_code)\n        except socket.error as exc:\n            log.debug('Caught exception in wait_for_winexesvc: %s', exc)\n\n        if time.time() - start > timeout:\n            return False\n        time.sleep(1)", "output": "Wait until winexe connection can be established.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_author(self, *, name, url=EmptyEmbed, icon_url=EmptyEmbed):\n        \"\"\"\n        \"\"\"\n\n        self._author = {\n            'name': str(name)\n        }\n\n        if url is not EmptyEmbed:\n            self._author['url'] = str(url)\n\n        if icon_url is not EmptyEmbed:\n            self._author['icon_url'] = str(icon_url)\n\n        return self", "output": "Sets the author for the embed content.\n\n        This function returns the class instance to allow for fluent-style\n        chaining.\n\n        Parameters\n        -----------\n        name: :class:`str`\n            The name of the author.\n        url: :class:`str`\n            The URL for the author.\n        icon_url: :class:`str`\n            The URL of the author icon. Only HTTP(S) is supported.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def minimum(lhs, rhs):\n    \"\"\"\n    \"\"\"\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_minimum,\n        lambda x, y: x if x < y else y,\n        _internal._minimum_scalar,\n        None)", "output": "Returns element-wise minimum of the input arrays with broadcasting.\n\n    Equivalent to ``mx.nd.broadcast_minimum(lhs, rhs)``.\n\n    .. note::\n\n       If the corresponding dimensions of two arrays have the same size or one of them has size 1,\n       then the arrays are broadcastable to a common shape.\n\n    Parameters\n    ----------\n    lhs : scalar or mxnet.ndarray.array\n        First array to be compared.\n    rhs : scalar or mxnet.ndarray.array\n         Second array to be compared. If ``lhs.shape != rhs.shape``, they must be\n        broadcastable to a common shape.\n\n    Returns\n    -------\n    NDArray\n        The element-wise minimum of the input arrays.\n\n    Examples\n    --------\n    >>> x = mx.nd.ones((2,3))\n    >>> y = mx.nd.arange(2).reshape((2,1))\n    >>> z = mx.nd.arange(2).reshape((1,2))\n    >>> x.asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> y.asnumpy()\n    array([[ 0.],\n           [ 1.]], dtype=float32)\n    >>> z.asnumpy()\n    array([[ 0.,  1.]], dtype=float32)\n    >>> mx.nd.minimum(x, 2).asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> mx.nd.minimum(x, y).asnumpy()\n    array([[ 0.,  0.,  0.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> mx.nd.minimum(z, y).asnumpy()\n    array([[ 0.,  0.],\n           [ 0.,  1.]], dtype=float32)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def diff(self, n, axis=0):\n        \"\"\"\n        \"\"\"\n        if axis == 0:\n            # Cannot currently calculate diff across multiple blocks since this\n            # function is invoked via apply\n            raise NotImplementedError\n        new_values = (self.values - self.shift(n, axis=axis)[0].values).asi8\n\n        # Reshape the new_values like how algos.diff does for timedelta data\n        new_values = new_values.reshape(1, len(new_values))\n        new_values = new_values.astype('timedelta64[ns]')\n        return [TimeDeltaBlock(new_values, placement=self.mgr_locs.indexer)]", "output": "1st discrete difference\n\n        Parameters\n        ----------\n        n : int, number of periods to diff\n        axis : int, axis to diff upon. default 0\n\n        Return\n        ------\n        A list with a new TimeDeltaBlock.\n\n        Note\n        ----\n        The arguments here are mimicking shift so they are called correctly\n        by apply.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read(domain, key, user=None):\n    '''\n    \n\n    '''\n    cmd = 'defaults read \"{0}\" \"{1}\"'.format(domain, key)\n    return __salt__['cmd.run'](cmd, runas=user)", "output": "Write a default to the system\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' macdefaults.read com.apple.CrashReporter DialogType\n\n        salt '*' macdefaults.read NSGlobalDomain ApplePersistence\n\n    domain\n        The name of the domain to read from\n\n    key\n        The key of the given domain to read from\n\n    user\n        The user to write the defaults to", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset(self):\n        \"\"\"\"\"\"\n        self.curr_idx = 0\n        #shuffle data in each bucket\n        random.shuffle(self.idx)\n        for i, buck in enumerate(self.sentences):\n            self.indices[i], self.sentences[i], self.characters[i], self.label[i] = shuffle(self.indices[i],\n                                                                                            self.sentences[i],\n                                                                                            self.characters[i],\n                                                                                            self.label[i])\n\n        self.ndindex = []\n        self.ndsent = []\n        self.ndchar = []\n        self.ndlabel = []\n\n        #for each bucket of data\n        for i, buck in enumerate(self.sentences):\n            #append the lists with an array\n            self.ndindex.append(ndarray.array(self.indices[i], dtype=self.dtype))\n            self.ndsent.append(ndarray.array(self.sentences[i], dtype=self.dtype))\n            self.ndchar.append(ndarray.array(self.characters[i], dtype=self.dtype))\n            self.ndlabel.append(ndarray.array(self.label[i], dtype=self.dtype))", "output": "Resets the iterator to the beginning of the data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def DiagonalGate(x, params, **kwargs):\n  \"\"\"\"\"\"\n  del params\n  del kwargs\n  # x : [batch, 1, length, depth]\n  x = np.pad(\n      x, [(0, 0), (0, 0), (1, 1), (0, 0)], mode='constant', constant_values=0.0)\n  depth = x.shape[-1] // 3\n  assert 3 * depth == x.shape[-1], ('Depth must be divisible by 3', depth,\n                                    x.shape)\n  xs = [\n      x[:, :, :-2, :depth], x[:, :, 1:-1, depth:2 * depth],\n      x[:, :, 2:, 2 * depth:3 * depth]\n  ]\n  return np.concatenate(xs, axis=3)", "output": "Split channels in 3 parts. Shifts 1st and 3rd sections to left/right.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_thing_shadow(self, **kwargs):\n        \n        \"\"\"\n        thing_name = self._get_required_parameter('thingName', **kwargs)\n        payload = self._get_required_parameter('payload', **kwargs)\n\n        return self._shadow_op('update', thing_name, payload)", "output": "r\"\"\"\n        Updates the thing shadow for the specified thing.\n\n        :Keyword Arguments:\n            * *thingName* (``string``) --\n              [REQUIRED]\n              The name of the thing.\n            * *payload* (``bytes or seekable file-like object``) --\n              [REQUIRED]\n              The state information, in JSON format.\n\n        :returns: (``dict``) --\n        The output from the UpdateThingShadow operation\n            * *payload* (``bytes``) --\n              The state information, in JSON format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def refresh(self):\r\n        \"\"\"\"\"\"\r\n        self._update_id_list()\r\n        for _id in self.history[:]:\r\n            if _id not in self.id_list:\r\n                self.history.remove(_id)", "output": "Remove editors that are not longer open.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_summary_struct(self):\n        \"\"\"\n        \n        \"\"\"\n\n        model_fields = [\n            (\"Method\", 'method'),\n            (\"Number of distance components\", 'num_distance_components'),\n            (\"Number of examples\", 'num_examples'),\n            (\"Number of feature columns\", 'num_features'),\n            (\"Number of unpacked features\", 'num_unpacked_features'),\n            (\"Total training time (seconds)\", 'training_time')]\n\n        ball_tree_fields = [\n            (\"Tree depth\", 'tree_depth'),\n            (\"Leaf size\", 'leaf_size')]\n\n        lsh_fields = [\n            (\"Number of hash tables\", 'num_tables'),\n            (\"Number of projections per table\", 'num_projections_per_table')]\n\n        sections = [model_fields]\n        section_titles = ['Attributes']\n\n        if (self.method == 'ball_tree'):\n            sections.append(ball_tree_fields)\n            section_titles.append('Ball Tree Attributes')\n\n        if (self.method == 'lsh'):\n            sections.append(lsh_fields)\n            section_titles.append('LSH Attributes')\n\n        return (sections, section_titles)", "output": "Returns a structured description of the model, including (where\n        relevant) the schema of the training data, description of the training\n        data, training statistics, and model hyperparameters.\n\n        Returns\n        -------\n        sections : list (of list of tuples)\n            A list of summary sections.\n              Each section is a list.\n                Each item in a section list is a tuple of the form:\n                  ('<label>','<field>')\n        section_titles: list\n            A list of section titles.\n              The order matches that of the 'sections' object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(self, input):\n        \"\"\"\n        \n        \"\"\"\n        result = self._parseIso8601(input)\n        if not result:\n            result = self._parseSimple(input)\n        if result is not None:\n            return result\n        else:\n            raise ParameterException(\"Invalid time delta - could not parse %s\" % input)", "output": "Parses a time delta from the input.\n\n        See :py:class:`TimeDeltaParameter` for details on supported formats.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_migrations_online():\n    \"\"\"\n\n    \"\"\"\n\n    # this callback is used to prevent an auto-migration from being generated\n    # when there are no changes to the schema\n    # reference: https://alembic.sqlalchemy.org/en/latest/cookbook.html\n    def process_revision_directives(context, revision, directives):\n        if getattr(config.cmd_opts, 'autogenerate', False):\n            script = directives[0]\n            if script.upgrade_ops.is_empty():\n                directives[:] = []\n                logger.info('No changes in schema detected.')\n\n    engine = engine_from_config(config.get_section(config.config_ini_section),\n                                prefix='sqlalchemy.',\n                                poolclass=pool.NullPool)\n\n    connection = engine.connect()\n    kwargs = {}\n    if engine.name in ('sqlite', 'mysql'):\n        kwargs = {\n            'transaction_per_migration': True,\n            'transactional_ddl': True,\n        }\n    configure_args = current_app.extensions['migrate'].configure_args\n    if configure_args:\n        kwargs.update(configure_args)\n\n    context.configure(connection=connection,\n                      target_metadata=target_metadata,\n                      # compare_type=True,\n                      process_revision_directives=process_revision_directives,\n                      **kwargs)\n\n    try:\n        with context.begin_transaction():\n            context.run_migrations()\n    finally:\n        connection.close()", "output": "Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def center_crop(im, min_sz=None):\n    \"\"\"  \"\"\"\n    r,c,*_ = im.shape\n    if min_sz is None: min_sz = min(r,c)\n    start_r = math.ceil((r-min_sz)/2)\n    start_c = math.ceil((c-min_sz)/2)\n    return crop(im, start_r, start_c, min_sz)", "output": "Return a center crop of an image", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _lazily_initialize(self):\n    \"\"\"\"\"\"\n    # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n    import tensorflow.compat.v1 as tf\n    with self._initialization_lock:\n      if self._session:\n        return\n      graph = tf.Graph()\n      with graph.as_default():\n        self.initialize_graph()\n      # Don't reserve GPU because libpng can't run on GPU.\n      config = tf.ConfigProto(device_count={'GPU': 0})\n      self._session = tf.Session(graph=graph, config=config)", "output": "Initialize the graph and session, if this has not yet been done.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def global_lppooling(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    p_value = attrs.get('p', 2)\n    new_attrs = translation_utils._add_extra_attributes(attrs, {'global_pool': True,\n                                                                'kernel': (1, 1),\n                                                                'pool_type': 'lp',\n                                                                'p_value': p_value})\n    new_attrs = translation_utils._remove_attributes(new_attrs, ['p'])\n    return 'Pooling', new_attrs, inputs", "output": "Performs global lp pooling on the input.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_train_begin(self, **kwargs: Any) -> None:\n        \"\"\n        self.path.parent.mkdir(parents=True, exist_ok=True)      \n        self.file = self.path.open('a') if self.append else self.path.open('w')\n        self.file.write(','.join(self.learn.recorder.names[:(None if self.add_time else -1)]) + '\\n')", "output": "Prepare file with metric names.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply(\n        self,\n        func,\n        num_splits=None,\n        other_axis_partition=None,\n        maintain_partitioning=True,\n        **kwargs\n    ):\n        \"\"\"\n        \"\"\"\n        import dask\n\n        if num_splits is None:\n            num_splits = len(self.list_of_blocks)\n\n        if other_axis_partition is not None:\n            return [\n                DaskFramePartition(dask.delayed(obj))\n                for obj in deploy_func_between_two_axis_partitions(\n                    self.axis,\n                    func,\n                    num_splits,\n                    len(self.list_of_blocks),\n                    kwargs,\n                    *dask.compute(\n                        *tuple(\n                            self.list_of_blocks + other_axis_partition.list_of_blocks\n                        )\n                    )\n                )\n            ]\n\n        args = [self.axis, func, num_splits, kwargs, maintain_partitioning]\n\n        args.extend(dask.compute(*self.list_of_blocks))\n        return [\n            DaskFramePartition(dask.delayed(obj)) for obj in deploy_axis_func(*args)\n        ]", "output": "Applies func to the object.\n\n        See notes in Parent class about this method.\n\n        Args:\n            func: The function to apply.\n            num_splits: The number of times to split the result object.\n            other_axis_partition: Another `DaskFrameAxisPartition` object to apply to\n                func with this one.\n\n        Returns:\n            A list of `DaskFramePartition` objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _field_to_json(field, row_value):\n    \"\"\"\n    \"\"\"\n    if row_value is None:\n        return None\n\n    if field.mode == \"REPEATED\":\n        return _repeated_field_to_json(field, row_value)\n\n    if field.field_type == \"RECORD\":\n        return _record_field_to_json(field.fields, row_value)\n\n    return _scalar_field_to_json(field, row_value)", "output": "Convert a field into JSON-serializable values.\n\n    Args:\n        field ( \\\n            :class:`~google.cloud.bigquery.schema.SchemaField`, \\\n        ):\n            The SchemaField to use for type conversion and field name.\n\n        row_value (Union[ \\\n            Sequence[list], \\\n            any, \\\n        ]):\n            Row data to be inserted. If the SchemaField's mode is\n            REPEATED, assume this is a list. If not, the type\n            is inferred from the SchemaField's field_type.\n\n    Returns:\n        any:\n            A JSON-serializable object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_examples(self, filepath):\n    \"\"\"\n    \"\"\"\n    # Simultaneously iterating through the different data sets in the hdf5\n    # file is >100x slower and the data set is small (26.7MB). Hence, we first\n    # load everything into memory before yielding the samples.\n    image_array, class_array, values_array = _load_data(filepath)\n    for image, classes, values in moves.zip(image_array, class_array,\n                                            values_array):\n      yield dict(\n          image=np.expand_dims(image, -1),\n          label_shape=classes[1],\n          label_scale=classes[2],\n          label_orientation=classes[3],\n          label_x_position=classes[4],\n          label_y_position=classes[5],\n          value_shape=values[1],\n          value_scale=values[2],\n          value_orientation=values[3],\n          value_x_position=values[4],\n          value_y_position=values[5])", "output": "Generates examples for the dSprites data set.\n\n    Args:\n      filepath: path to the dSprites hdf5 file.\n\n    Yields:\n      Dictionaries with images, latent classes, and latent values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy(self, existing_inputs):\n        \"\"\"\"\"\"\n        return PPOPolicyGraph(\n            self.observation_space,\n            self.action_space,\n            self.config,\n            existing_inputs=existing_inputs)", "output": "Creates a copy of self using existing input placeholders.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sinks_api(self):\n        \"\"\"\n        \"\"\"\n        if self._sinks_api is None:\n            if self._use_grpc:\n                self._sinks_api = _gapic.make_sinks_api(self)\n            else:\n                self._sinks_api = JSONSinksAPI(self)\n        return self._sinks_api", "output": "Helper for log sink-related API calls.\n\n        See\n        https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.sinks", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def detect_django_settings():\n    \"\"\"\n    \n    \"\"\"\n\n    matches = []\n    for root, dirnames, filenames in os.walk(os.getcwd()):\n        for filename in fnmatch.filter(filenames, '*settings.py'):\n            full = os.path.join(root, filename)\n            if 'site-packages' in full:\n                continue\n            full = os.path.join(root, filename)\n            package_path = full.replace(os.getcwd(), '')\n            package_module = package_path.replace(os.sep, '.').split('.', 1)[1].replace('.py', '')\n\n            matches.append(package_module)\n    return matches", "output": "Automatically try to discover Django settings files,\n    return them as relative module paths.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(self, path):\n        \"\"\"\n        \n        \"\"\"\n        import hdfs\n        try:\n            self.client.status(path)\n            return True\n        except hdfs.util.HdfsError as e:\n            if str(e).startswith('File does not exist: '):\n                return False\n            else:\n                raise e", "output": "Returns true if the path exists and false otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dump(self, directory=''):\n        \"\"\"\n        \n        \"\"\"\n        with self.no_unpicklable_properties():\n            file_name = os.path.join(directory, 'job-instance.pickle')\n            if self.__module__ == '__main__':\n                d = pickle.dumps(self)\n                module_name = os.path.basename(sys.argv[0]).rsplit('.', 1)[0]\n                d = d.replace(b'(c__main__', \"(c\" + module_name)\n                open(file_name, \"wb\").write(d)\n\n            else:\n                pickle.dump(self, open(file_name, \"wb\"))", "output": "Dump instance to file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install_given_reqs(\n    to_install,  # type: List[InstallRequirement]\n    install_options,  # type: List[str]\n    global_options=(),  # type: Sequence[str]\n    *args, **kwargs\n):\n    # type: (...) -> List[InstallRequirement]\n    \"\"\"\n    \n    \"\"\"\n\n    if to_install:\n        logger.info(\n            'Installing collected packages: %s',\n            ', '.join([req.name for req in to_install]),\n        )\n\n    with indent_log():\n        for requirement in to_install:\n            if requirement.conflicts_with:\n                logger.info(\n                    'Found existing installation: %s',\n                    requirement.conflicts_with,\n                )\n                with indent_log():\n                    uninstalled_pathset = requirement.uninstall(\n                        auto_confirm=True\n                    )\n            try:\n                requirement.install(\n                    install_options,\n                    global_options,\n                    *args,\n                    **kwargs\n                )\n            except Exception:\n                should_rollback = (\n                    requirement.conflicts_with and\n                    not requirement.install_succeeded\n                )\n                # if install did not succeed, rollback previous uninstall\n                if should_rollback:\n                    uninstalled_pathset.rollback()\n                raise\n            else:\n                should_commit = (\n                    requirement.conflicts_with and\n                    requirement.install_succeeded\n                )\n                if should_commit:\n                    uninstalled_pathset.commit()\n            requirement.remove_temporary_source()\n\n    return to_install", "output": "Install everything in the given list.\n\n    (to be called after having downloaded and unpacked the packages)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_balancers(profile, **libcloud_kwargs):\n    '''\n    \n    '''\n    conn = _get_driver(profile=profile)\n    libcloud_kwargs = salt.utils.args.clean_kwargs(**libcloud_kwargs)\n    balancers = conn.list_balancers(**libcloud_kwargs)\n    ret = []\n    for balancer in balancers:\n        ret.append(_simple_balancer(balancer))\n    return ret", "output": "Return a list of load balancers.\n\n    :param profile: The profile key\n    :type  profile: ``str``\n\n    :param libcloud_kwargs: Extra arguments for the driver's list_balancers method\n    :type  libcloud_kwargs: ``dict``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion libcloud_storage.list_balancers profile1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self):\n        \"\"\"\n        \"\"\"\n        if len(self.call_queue):\n            return self.apply(lambda x: x).get()\n        try:\n            return ray.get(self.oid)\n        except RayTaskError as e:\n            handle_ray_task_error(e)", "output": "Gets the object out of the plasma store.\n\n        Returns:\n            The object from the plasma store.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_blank_page(self):\r\n        \"\"\"\"\"\"\r\n        loading_template = Template(BLANK)\r\n        page = loading_template.substitute(css_path=self.css_path)\r\n        return page", "output": "Create html page to show while the kernel is starting", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _record_hyper_configs(self, hyper_configs):\n        \"\"\"\n        \"\"\"\n        self.hyper_configs.append(hyper_configs)\n        self.configs_perf.append(dict())\n        self.num_finished_configs.append(0)\n        self.num_configs_to_run.append(len(hyper_configs))\n        self.increase_i()", "output": "after generating one round of hyperconfigs, this function records the generated hyperconfigs,\n        creates a dict to record the performance when those hyperconifgs are running, set the number of finished configs\n        in this round to be 0, and increase the round number.\n\n        Parameters\n        ----------\n        hyper_configs: list\n            the generated hyperconfigs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _to_java_object_rdd(self):\n        \"\"\" \n        \"\"\"\n        rdd = self._pickled()\n        return self.ctx._jvm.SerDeUtil.pythonToJava(rdd._jrdd, True)", "output": "Return a JavaRDD of Object by unpickling\n\n        It will convert each Python object into Java object by Pyrolite, whenever the\n        RDD is serialized in batch or not.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chhome(name, home, persist=False):\n    '''\n    \n    '''\n    pre_info = info(name)\n    if not pre_info:\n        raise CommandExecutionError(\n            'User \\'{0}\\' does not exist'.format(name)\n        )\n    if home == pre_info['home']:\n        return True\n    cmd = ['usermod', '-d', home]\n    if persist:\n        cmd.append('-m')\n    cmd.append(name)\n    __salt__['cmd.run'](cmd, python_shell=False)\n    return info(name).get('home') == home", "output": "Set a new home directory for an existing user\n\n    name\n        Username to modify\n\n    home\n        New home directory to set\n\n    persist : False\n        Set to ``True`` to prevent configuration files in the new home\n        directory from being overwritten by the files from the skeleton\n        directory.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.chhome foo /home/users/foo True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_breakpoints(self):\n        \"\"\"\"\"\"\n        breakpoints = []\n        block = self.editor.document().firstBlock()\n        for line_number in range(1, self.editor.document().blockCount()+1):\n            data = block.userData()\n            if data and data.breakpoint:\n                breakpoints.append((line_number, data.breakpoint_condition))\n            block = block.next()\n        return breakpoints", "output": "Get breakpoints", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imagetransformer_sep_channels_16l_16h_imgnet_lrg_loc():\n  \"\"\"\"\"\"\n  hparams = imagetransformer_sep_channels_12l_16h_imagenet_large()\n  hparams.num_hidden_layers = 16\n  hparams.local_attention = True\n  hparams.batch_size = 1\n  hparams.block_length = 256\n  return hparams", "output": "separate rgb embeddings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stats(local=False, remote=False, jail=None, chroot=None, root=None):\n    '''\n    \n    '''\n\n    opts = ''\n    if local:\n        opts += 'l'\n    if remote:\n        opts += 'r'\n\n    cmd = _pkg(jail, chroot, root)\n    cmd.append('stats')\n    if opts:\n        cmd.append('-' + opts)\n    out = __salt__['cmd.run'](cmd, output_loglevel='trace', python_shell=False)\n    return [x.strip('\\t') for x in salt.utils.itertools.split(out, '\\n')]", "output": "Return pkgng stats.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.stats\n\n    local\n        Display stats only for the local package database.\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.stats local=True\n\n    remote\n        Display stats only for the remote package database(s).\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.stats remote=True\n\n    jail\n        Retrieve stats from the specified jail.\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.stats jail=<jail name or id>\n            salt '*' pkg.stats jail=<jail name or id> local=True\n            salt '*' pkg.stats jail=<jail name or id> remote=True\n\n    chroot\n        Retrieve stats from the specified chroot (ignored if ``jail`` is\n        specified).\n\n    root\n        Retrieve stats from the specified root (ignored if ``jail`` is\n        specified).\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.stats chroot=/path/to/chroot\n            salt '*' pkg.stats chroot=/path/to/chroot local=True\n            salt '*' pkg.stats chroot=/path/to/chroot remote=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toArray(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.isTransposed:\n            return np.asfortranarray(\n                self.values.reshape((self.numRows, self.numCols)))\n        else:\n            return self.values.reshape((self.numRows, self.numCols), order='F')", "output": "Return an numpy.ndarray\n\n        >>> m = DenseMatrix(2, 2, range(4))\n        >>> m.toArray()\n        array([[ 0.,  2.],\n               [ 1.,  3.]])", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def identify_repo(repo_url):\n    \"\"\"\n    \"\"\"\n    repo_url_values = repo_url.split('+')\n    if len(repo_url_values) == 2:\n        repo_type = repo_url_values[0]\n        if repo_type in [\"git\", \"hg\"]:\n            return repo_type, repo_url_values[1]\n        else:\n            raise UnknownRepoType\n    else:\n        if 'git' in repo_url:\n            return 'git', repo_url\n        elif 'bitbucket' in repo_url:\n            return 'hg', repo_url\n        else:\n            raise UnknownRepoType", "output": "Determine if `repo_url` should be treated as a URL to a git or hg repo.\n\n    Repos can be identified by prepending \"hg+\" or \"git+\" to the repo URL.\n\n    :param repo_url: Repo URL of unknown type.\n    :returns: ('git', repo_url), ('hg', repo_url), or None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_list(user=None, password=None, host=None, port=None, database='admin', authdb=None):\n    '''\n    \n    '''\n    conn = _connect(user, password, host, port, authdb=authdb)\n    if not conn:\n        return 'Failed to connect to mongo database'\n\n    try:\n        log.info('Listing users')\n        mdb = pymongo.database.Database(conn, database)\n\n        output = []\n        mongodb_version = _version(mdb)\n\n        if _LooseVersion(mongodb_version) >= _LooseVersion('2.6'):\n            for user in mdb.command('usersInfo')['users']:\n                output.append(\n                    {'user': user['user'],\n                     'roles': user['roles']}\n                )\n        else:\n            for user in mdb.system.users.find():\n                output.append(\n                    {'user': user['user'],\n                     'readOnly': user.get('readOnly', 'None')}\n                )\n        return output\n\n    except pymongo.errors.PyMongoError as err:\n        log.error('Listing users failed with error: %s', err)\n        return six.text_type(err)", "output": "List users of a MongoDB database\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mongodb.user_list <user> <password> <host> <port> <database>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_data(self, minion):\n        '''\n        \n        '''\n        if isinstance(self.raw[minion], six.string_types):\n            return {'host': self.raw[minion]}\n        if isinstance(self.raw[minion], dict):\n            return self.raw[minion]\n        return False", "output": "Return the configured ip", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def instr(str, substr):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.instr(_to_java_column(str), substr))", "output": "Locate the position of the first occurrence of substr column in the given string.\n    Returns null if either of the arguments are null.\n\n    .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\n        could not be found in str.\n\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n    >>> df.select(instr(df.s, 'b').alias('s')).collect()\n    [Row(s=2)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def num_data(self):\n        \"\"\"\n        \"\"\"\n        if self.handle is not None:\n            ret = ctypes.c_int()\n            _safe_call(_LIB.LGBM_DatasetGetNumData(self.handle,\n                                                   ctypes.byref(ret)))\n            return ret.value\n        else:\n            raise LightGBMError(\"Cannot get num_data before construct dataset\")", "output": "Get the number of rows in the Dataset.\n\n        Returns\n        -------\n        number_of_rows : int\n            The number of rows in the Dataset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def continuous_future(self,\n                          root_symbol_str,\n                          offset=0,\n                          roll='volume',\n                          adjustment='mul'):\n        \"\"\"\n        \"\"\"\n        return self.asset_finder.create_continuous_future(\n            root_symbol_str,\n            offset,\n            roll,\n            adjustment,\n        )", "output": "Create a specifier for a continuous contract.\n\n        Parameters\n        ----------\n        root_symbol_str : str\n            The root symbol for the future chain.\n\n        offset : int, optional\n            The distance from the primary contract. Default is 0.\n\n        roll_style : str, optional\n            How rolls are determined. Default is 'volume'.\n\n        adjustment : str, optional\n            Method for adjusting lookback prices between rolls. Options are\n            'mul', 'add', and None. Default is 'mul'.\n\n        Returns\n        -------\n        continuous_future : ContinuousFuture\n            The continuous future specifier.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pow(self, other, axis=\"columns\", level=None, fill_value=None):\r\n        \"\"\"\r\n        \"\"\"\r\n        return self._binary_op(\r\n            \"pow\", other, axis=axis, level=level, fill_value=fill_value\r\n        )", "output": "Pow this DataFrame against another DataFrame/Series/scalar.\r\n\r\n        Args:\r\n            other: The object to use to apply the pow against this.\r\n            axis: The axis to pow over.\r\n            level: The Multilevel index level to apply pow over.\r\n            fill_value: The value to fill NaNs with.\r\n\r\n        Returns:\r\n            A new DataFrame with the Pow applied.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def schema(self, schema):\n        \"\"\"\n        \"\"\"\n        from pyspark.sql import SparkSession\n        spark = SparkSession.builder.getOrCreate()\n        if isinstance(schema, StructType):\n            jschema = spark._jsparkSession.parseDataType(schema.json())\n            self._jreader = self._jreader.schema(jschema)\n        elif isinstance(schema, basestring):\n            self._jreader = self._jreader.schema(schema)\n        else:\n            raise TypeError(\"schema should be StructType or string\")\n        return self", "output": "Specifies the input schema.\n\n        Some data sources (e.g. JSON) can infer the input schema automatically from data.\n        By specifying the schema here, the underlying data source can skip the schema\n        inference step, and thus speed up data loading.\n\n        :param schema: a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string\n                       (For example ``col0 INT, col1 DOUBLE``).\n\n        >>> s = spark.read.schema(\"col0 INT, col1 DOUBLE\")", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _blobs_page_start(iterator, page, response):\n    \"\"\"\n    \"\"\"\n    page.prefixes = tuple(response.get(\"prefixes\", ()))\n    iterator.prefixes.update(page.prefixes)", "output": "Grab prefixes after a :class:`~google.cloud.iterator.Page` started.\n\n    :type iterator: :class:`~google.api_core.page_iterator.Iterator`\n    :param iterator: The iterator that is currently in use.\n\n    :type page: :class:`~google.cloud.api.core.page_iterator.Page`\n    :param page: The page that was just created.\n\n    :type response: dict\n    :param response: The JSON API response for a page of blobs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def config(_config=None, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    if _config is None:\n        _config = {}\n    _config.update(kwargs)\n\n    def wrapper(func):\n        func._config = _config\n        return func\n    return wrapper", "output": "A decorator for setting the default kwargs of `BaseHandler.crawl`.\n    Any self.crawl with this callback will use this config.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ensure_timestamp_field(dataset_expr, deltas, checkpoints):\n    \"\"\"\n    \"\"\"\n    measure = dataset_expr.dshape.measure\n    if TS_FIELD_NAME not in measure.names:\n        dataset_expr = bz.transform(\n            dataset_expr,\n            **{TS_FIELD_NAME: dataset_expr[AD_FIELD_NAME]}\n        )\n        deltas = _ad_as_ts(deltas)\n        checkpoints = _ad_as_ts(checkpoints)\n    else:\n        _check_datetime_field(TS_FIELD_NAME, measure)\n\n    return dataset_expr, deltas, checkpoints", "output": "Verify that the baseline and deltas expressions have a timestamp field.\n\n    If there is not a ``TS_FIELD_NAME`` on either of the expressions, it will\n    be copied from the ``AD_FIELD_NAME``. If one is provided, then we will\n    verify that it is the correct dshape.\n\n    Parameters\n    ----------\n    dataset_expr : Expr\n        The baseline expression.\n    deltas : Expr or None\n        The deltas expression if any was provided.\n    checkpoints : Expr or None\n        The checkpoints expression if any was provided.\n\n    Returns\n    -------\n    dataset_expr, deltas : Expr\n        The new baseline and deltas expressions to use.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def evaluate(data_loader):\n    \"\"\"\n    \"\"\"\n    translation_out = []\n    all_inst_ids = []\n    avg_loss_denom = 0\n    avg_loss = 0.0\n    for _, (src_seq, tgt_seq, src_valid_length, tgt_valid_length, inst_ids) \\\n            in enumerate(data_loader):\n        src_seq = src_seq.as_in_context(ctx)\n        tgt_seq = tgt_seq.as_in_context(ctx)\n        src_valid_length = src_valid_length.as_in_context(ctx)\n        tgt_valid_length = tgt_valid_length.as_in_context(ctx)\n        # Calculating Loss\n        out, _ = model(src_seq, tgt_seq[:, :-1], src_valid_length, tgt_valid_length - 1)\n        loss = loss_function(out, tgt_seq[:, 1:], tgt_valid_length - 1).mean().asscalar()\n        all_inst_ids.extend(inst_ids.asnumpy().astype(np.int32).tolist())\n        avg_loss += loss * (tgt_seq.shape[1] - 1)\n        avg_loss_denom += (tgt_seq.shape[1] - 1)\n        # Translate\n        samples, _, sample_valid_length =\\\n            translator.translate(src_seq=src_seq, src_valid_length=src_valid_length)\n        max_score_sample = samples[:, 0, :].asnumpy()\n        sample_valid_length = sample_valid_length[:, 0].asnumpy()\n        for i in range(max_score_sample.shape[0]):\n            translation_out.append(\n                [tgt_vocab.idx_to_token[ele] for ele in\n                 max_score_sample[i][1:(sample_valid_length[i] - 1)]])\n    avg_loss = avg_loss / avg_loss_denom\n    real_translation_out = [None for _ in range(len(all_inst_ids))]\n    for ind, sentence in zip(all_inst_ids, translation_out):\n        real_translation_out[ind] = sentence\n    return avg_loss, real_translation_out", "output": "Evaluate given the data loader\n\n    Parameters\n    ----------\n    data_loader : DataLoader\n\n    Returns\n    -------\n    avg_loss : float\n        Average loss\n    real_translation_out : list of list of str\n        The translation output", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_resources_vms(call=None, resFilter=None, includeConfig=True):\n    '''\n    \n    '''\n\n    timeoutTime = time.time() + 60\n    while True:\n        log.debug('Getting resource: vms.. (filter: %s)', resFilter)\n        resources = query('get', 'cluster/resources')\n        ret = {}\n        badResource = False\n        for resource in resources:\n            if 'type' in resource and resource['type'] in ['openvz', 'qemu',\n                                                           'lxc']:\n                try:\n                    name = resource['name']\n                except KeyError:\n                    badResource = True\n                    log.debug('No name in VM resource %s', repr(resource))\n                    break\n\n                ret[name] = resource\n\n                if includeConfig:\n                    # Requested to include the detailed configuration of a VM\n                    ret[name]['config'] = get_vmconfig(\n                        ret[name]['vmid'],\n                        ret[name]['node'],\n                        ret[name]['type']\n                    )\n\n        if time.time() > timeoutTime:\n            raise SaltCloudExecutionTimeout('FAILED to get the proxmox '\n                                            'resources vms')\n\n        # Carry on if there wasn't a bad resource return from Proxmox\n        if not badResource:\n            break\n\n        time.sleep(0.5)\n\n    if resFilter is not None:\n        log.debug('Filter given: %s, returning requested '\n                  'resource: nodes', resFilter)\n        return ret[resFilter]\n\n    log.debug('Filter not given: %s, returning all resource: nodes', ret)\n    return ret", "output": "Retrieve all VMs available on this environment\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f get_resources_vms my-proxmox-config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_namespaced_replication_controller(self, namespace, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_replication_controller_with_http_info(namespace, body, **kwargs)\n        else:\n            (data) = self.create_namespaced_replication_controller_with_http_info(namespace, body, **kwargs)\n            return data", "output": "create a ReplicationController\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_replication_controller(namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1ReplicationController body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1ReplicationController\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def table_spec_path(cls, project, location, dataset, table_spec):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/locations/{location}/datasets/{dataset}/tableSpecs/{table_spec}\",\n            project=project,\n            location=location,\n            dataset=dataset,\n            table_spec=table_spec,\n        )", "output": "Return a fully-qualified table_spec string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install(app_id, enable=True):\n    '''\n    \n    '''\n    ge_el_capitan = True if _LooseVersion(__grains__['osrelease']) >= salt.utils.stringutils.to_str('10.11') else False\n    client_type = _client_type(app_id)\n    enable_str = '1' if enable else '0'\n    cmd = 'sqlite3 \"/Library/Application Support/com.apple.TCC/TCC.db\" ' \\\n          '\"INSERT or REPLACE INTO access VALUES(\\'kTCCServiceAccessibility\\',\\'{0}\\',{1},{2},1,NULL{3})\"'.\\\n        format(app_id, client_type, enable_str, ',NULL' if ge_el_capitan else '')\n\n    call = __salt__['cmd.run_all'](\n        cmd,\n        output_loglevel='debug',\n        python_shell=False\n    )\n    if call['retcode'] != 0:\n        comment = ''\n        if 'stderr' in call:\n            comment += call['stderr']\n        if 'stdout' in call:\n            comment += call['stdout']\n\n        raise CommandExecutionError('Error installing app: {0}'.format(comment))\n\n    return True", "output": "Install a bundle ID or command as being allowed to use\n    assistive access.\n\n    app_id\n        The bundle ID or command to install for assistive access.\n\n    enabled\n        Sets enabled or disabled status. Default is ``True``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' assistive.install /usr/bin/osascript\n        salt '*' assistive.install com.smileonmymac.textexpander", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def arrays_to_mgr(arrays, arr_names, index, columns, dtype=None):\n    \"\"\"\n    \n    \"\"\"\n    # figure out the index, if necessary\n    if index is None:\n        index = extract_index(arrays)\n    else:\n        index = ensure_index(index)\n\n    # don't force copy because getting jammed in an ndarray anyway\n    arrays = _homogenize(arrays, index, dtype)\n\n    # from BlockManager perspective\n    axes = [ensure_index(columns), index]\n\n    return create_block_manager_from_arrays(arrays, arr_names, axes)", "output": "Segregate Series based on type and coerce into matrices.\n\n    Needs to handle a lot of exceptional cases.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _common_prefix(names):\n    \"\"\"\"\"\"\n    if not names:\n        return ''\n    prefix = names[0]\n    for name in names:\n        i = 0\n        while i < len(prefix) and i < len(name) and prefix[i] == name[i]:\n            i += 1\n        prefix = prefix[:i]\n    return prefix", "output": "Get the common prefix for all names", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rank():\n    \"\"\" \n    \"\"\"\n    rank_data_url = 'https://raw.githubusercontent.com/Microsoft/LightGBM/master/examples/lambdarank/'\n    x_train, y_train = sklearn.datasets.load_svmlight_file(cache(rank_data_url + 'rank.train'))\n    x_test, y_test = sklearn.datasets.load_svmlight_file(cache(rank_data_url + 'rank.test'))\n    q_train = np.loadtxt(cache(rank_data_url + 'rank.train.query'))\n    q_test = np.loadtxt(cache(rank_data_url + 'rank.test.query'))\n    return x_train, y_train, x_test, y_test, q_train, q_test", "output": "Ranking datasets from lightgbm repository.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def registerJavaUDAF(self, name, javaClassName):\n        \"\"\"\n        \"\"\"\n\n        self.sparkSession._jsparkSession.udf().registerJavaUDAF(name, javaClassName)", "output": "Register a Java user-defined aggregate function as a SQL function.\n\n        :param name: name of the user-defined aggregate function\n        :param javaClassName: fully qualified name of java class\n\n        >>> spark.udf.registerJavaUDAF(\"javaUDAF\", \"test.org.apache.spark.sql.MyDoubleAvg\")\n        >>> df = spark.createDataFrame([(1, \"a\"),(2, \"b\"), (3, \"a\")],[\"id\", \"name\"])\n        >>> df.createOrReplaceTempView(\"df\")\n        >>> spark.sql(\"SELECT name, javaUDAF(id) as avg from df group by name\").collect()\n        [Row(name=u'b', avg=102.0), Row(name=u'a', avg=102.0)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_df(self, data_file):\n        \"\"\"\n        \n        \"\"\"\n\n        crawler = QAHistoryFinancialCrawler()\n\n        with open(data_file, 'rb') as df:\n            data = crawler.parse(download_file=df)\n\n        return crawler.to_df(data)", "output": "\u8bfb\u53d6\u5386\u53f2\u8d22\u52a1\u6570\u636e\u6587\u4ef6\uff0c\u5e76\u8fd4\u56depandas\u7ed3\u679c \uff0c \u7c7b\u4f3cgpcw20171231.zip\u683c\u5f0f\uff0c\u5177\u4f53\u5b57\u6bb5\u542b\u4e49\u53c2\u8003\n\n        https://github.com/rainx/pytdx/issues/133\n\n        :param data_file: \u6570\u636e\u6587\u4ef6\u5730\u5740\uff0c \u6570\u636e\u6587\u4ef6\u7c7b\u578b\u53ef\u4ee5\u4e3a .zip \u6587\u4ef6\uff0c\u4e5f\u53ef\u4ee5\u4e3a\u89e3\u538b\u540e\u7684 .dat\n        :return: pandas DataFrame\u683c\u5f0f\u7684\u5386\u53f2\u8d22\u52a1\u6570\u636e", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def seek(self, idx):\n        \"\"\"\"\"\"\n        assert not self.writable\n        self._check_pid(allow_reset=True)\n        pos = ctypes.c_size_t(self.idx[idx])\n        check_call(_LIB.MXRecordIOReaderSeek(self.handle, pos))", "output": "Sets the current read pointer position.\n\n        This function is internally called by `read_idx(idx)` to find the current\n        reader pointer position. It doesn't return anything.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_render_wrapper(func):\n    \"\"\"\n    \"\"\"\n    global RENDER_WRAPPER\n    if not hasattr(func, \"__call__\"):\n        raise ValueError(Errors.E110.format(obj=type(func)))\n    RENDER_WRAPPER = func", "output": "Set an optional wrapper function that is called around the generated\n    HTML markup on displacy.render. This can be used to allow integration into\n    other platforms, similar to Jupyter Notebooks that require functions to be\n    called around the HTML. It can also be used to implement custom callbacks\n    on render, or to embed the visualization in a custom page.\n\n    func (callable): Function to call around markup before rendering it. Needs\n        to take one argument, the HTML markup, and should return the desired\n        output of displacy.render.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup(app):\n    '''  '''\n    app.add_autodocumenter(ColorDocumenter)\n    app.add_autodocumenter(EnumDocumenter)\n    app.add_autodocumenter(PropDocumenter)\n    app.add_autodocumenter(ModelDocumenter)", "output": "Required Sphinx extension setup function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_train_knns(self, data_activations):\n    \"\"\"\n    \n    \"\"\"\n    knns_ind = {}\n    knns_labels = {}\n\n    for layer in self.layers:\n      # Pre-process representations of data to normalize and remove training data mean.\n      data_activations_layer = copy.copy(data_activations[layer])\n      nb_data = data_activations_layer.shape[0]\n      data_activations_layer /= np.linalg.norm(\n          data_activations_layer, axis=1).reshape(-1, 1)\n      data_activations_layer -= self.centers[layer]\n\n      # Use FALCONN to find indices of nearest neighbors in training data.\n      knns_ind[layer] = np.zeros(\n          (data_activations_layer.shape[0], self.neighbors), dtype=np.int32)\n      knn_errors = 0\n      for i in range(data_activations_layer.shape[0]):\n        query_res = self.query_objects[layer].find_k_nearest_neighbors(\n            data_activations_layer[i], self.neighbors)\n        try:\n          knns_ind[layer][i, :] = query_res\n        except:  # pylint: disable-msg=W0702\n          knns_ind[layer][i, :len(query_res)] = query_res\n          knn_errors += knns_ind[layer].shape[1] - len(query_res)\n\n      # Find labels of neighbors found in the training data.\n      knns_labels[layer] = np.zeros((nb_data, self.neighbors), dtype=np.int32)\n      for data_id in range(nb_data):\n        knns_labels[layer][data_id, :] = self.train_labels[knns_ind[layer][data_id]]\n\n    return knns_ind, knns_labels", "output": "Given a data_activation dictionary that contains a np array with activations for each layer,\n    find the knns in the training data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_multi_precision(self, index, weight, grad, state):\n        \"\"\"\n        \"\"\"\n        if self.multi_precision and weight.dtype == numpy.float16:\n            # Wrapper for mixed precision\n            weight_master_copy = state[0]\n            original_state = state[1]\n            grad32 = grad.astype(numpy.float32)\n            self.update(index, weight_master_copy, grad32, original_state)\n            cast(weight_master_copy, dtype=weight.dtype, out=weight)\n        else:\n            self.update(index, weight, grad, state)", "output": "Updates the given parameter using the corresponding gradient and state.\n        Mixed precision version.\n\n        Parameters\n        ----------\n        index : int\n            The unique index of the parameter into the individual learning\n            rates and weight decays. Learning rates and weight decay\n            may be set via `set_lr_mult()` and `set_wd_mult()`, respectively.\n        weight : NDArray\n            The parameter to be updated.\n        grad : NDArray\n            The gradient of the objective with respect to this parameter.\n        state : any obj\n            The state returned by `create_state()`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _service_is_chkconfig(name):\n    '''\n    \n    '''\n    cmdline = '/sbin/chkconfig --list {0}'.format(name)\n    return __salt__['cmd.retcode'](cmdline, python_shell=False, ignore_retcode=True) == 0", "output": "Return True if the service is managed by chkconfig.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_categorical(y, nb_classes, num_classes=None):\n  \"\"\"\n  \n  \"\"\"\n  if num_classes is not None:\n    if nb_classes is not None:\n      raise ValueError(\"Should not specify both nb_classes and its deprecated \"\n                       \"alias, num_classes\")\n    warnings.warn(\"`num_classes` is deprecated. Switch to `nb_classes`.\"\n                  \" `num_classes` may be removed on or after 2019-04-23.\")\n    nb_classes = num_classes\n    del num_classes\n  y = np.array(y, dtype='int').ravel()\n  n = y.shape[0]\n  categorical = np.zeros((n, nb_classes))\n  categorical[np.arange(n), y] = 1\n  return categorical", "output": "Converts a class vector (integers) to binary class matrix.\n  This is adapted from the Keras function with the same name.\n  :param y: class vector to be converted into a matrix\n            (integers from 0 to nb_classes).\n  :param nb_classes: nb_classes: total number of classes.\n  :param num_classses: depricated version of nb_classes\n  :return: A binary matrix representation of the input.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _minion_event(self, load):\n        '''\n        \n        '''\n        if 'id' not in load:\n            return False\n        if 'events' not in load and ('tag' not in load or 'data' not in load):\n            return False\n        if 'events' in load:\n            for event in load['events']:\n                if 'data' in event:\n                    event_data = event['data']\n                else:\n                    event_data = event\n                self.event.fire_event(event_data, event['tag'])  # old dup event\n                if load.get('pretag') is not None:\n                    self.event.fire_event(event_data, salt.utils.event.tagify(event['tag'], base=load['pretag']))\n        else:\n            tag = load['tag']\n            self.event.fire_event(load, tag)\n        return True", "output": "Receive an event from the minion and fire it on the master event\n        interface", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def open_fileswitcher_dlg(self):\r\n        \"\"\"\"\"\"\r\n        if not self.tabs.count():\r\n            return\r\n        if self.fileswitcher_dlg is not None and \\\r\n          self.fileswitcher_dlg.is_visible:\r\n            self.fileswitcher_dlg.hide()\r\n            self.fileswitcher_dlg.is_visible = False\r\n            return\r\n        self.fileswitcher_dlg = FileSwitcher(self, self, self.tabs, self.data,\r\n                                             ima.icon('TextFileIcon'))\r\n        self.fileswitcher_dlg.sig_goto_file.connect(self.set_stack_index)\r\n        self.fileswitcher_dlg.show()\r\n        self.fileswitcher_dlg.is_visible = True", "output": "Open file list management dialog box", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_rsa_key(path, passphrase):\n    '''\n    \n    '''\n    log.debug('salt.crypt.get_rsa_key: Loading private key')\n    return _get_key_with_evict(path, six.text_type(os.path.getmtime(path)), passphrase)", "output": "Read a private key off the disk.  Poor man's simple cache in effect here,\n    we memoize the result of calling _get_rsa_with_evict.  This means the first\n    time _get_key_with_evict is called with a path and a timestamp the result\n    is cached.  If the file (the private key) does not change then its\n    timestamp will not change and the next time the result is returned from the\n    cache.  If the key DOES change the next time _get_rsa_with_evict is called\n    it is called with different parameters and the fn is run fully to retrieve\n    the key from disk.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absent(name, **connection_args):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    #check if db exists and remove it\n    if __salt__['mysql.db_exists'](name, **connection_args):\n        if __opts__['test']:\n            ret['result'] = None\n            ret['comment'] = \\\n                'Database {0} is present and needs to be removed'.format(name)\n            return ret\n        if __salt__['mysql.db_remove'](name, **connection_args):\n            ret['comment'] = 'Database {0} has been removed'.format(name)\n            ret['changes'][name] = 'Absent'\n            return ret\n        else:\n            err = _get_mysql_error()\n            if err is not None:\n                ret['comment'] = 'Unable to remove database {0} ' \\\n                                 '({1})'.format(name, err)\n                ret['result'] = False\n                return ret\n    else:\n        err = _get_mysql_error()\n        if err is not None:\n            ret['comment'] = err\n            ret['result'] = False\n            return ret\n\n    # fallback\n    ret['comment'] = ('Database {0} is not present, so it cannot be removed'\n            ).format(name)\n    return ret", "output": "Ensure that the named database is absent\n\n    name\n        The name of the database to remove", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def resolve(self, host: str,\n                      port: int, family: int) -> List[Dict[str, Any]]:\n        \"\"\"\"\"\"", "output": "Return IP address for given hostname", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_(prefix='', ruby=None, runas=None, gem_bin=None):\n    '''\n    \n    '''\n    cmd = ['list']\n    if prefix:\n        cmd.append(prefix)\n    stdout = _gem(cmd,\n                  ruby,\n                  gem_bin=gem_bin,\n                  runas=runas)\n    ret = {}\n    for line in salt.utils.itertools.split(stdout, '\\n'):\n        match = re.match(r'^([^ ]+) \\((.+)\\)', line)\n        if match:\n            gem = match.group(1)\n            versions = match.group(2).split(', ')\n            ret[gem] = versions\n    return ret", "output": "List locally installed gems.\n\n    :param prefix: string :\n        Only list gems when the name matches this prefix.\n    :param gem_bin: string : None\n        Full path to ``gem`` binary to use.\n    :param ruby: string : None\n        If RVM or rbenv are installed, the ruby version and gemset to use.\n        Ignored if ``gem_bin`` is specified.\n    :param runas: string : None\n        The user to run gem as.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' gem.list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_punctuation(self, char):\n        \"\"\"\"\"\"\n        cp = ord(char)\n        # We treat all non-letter/number ASCII as punctuation.\n        # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n        # Punctuation class but we treat them as punctuation anyways, for\n        # consistency.\n        group0 = cp >= 33 and cp <= 47\n        group1 = cp >= 58 and cp <= 64\n        group2 = cp >= 91 and cp <= 96\n        group3 = cp >= 123 and cp <= 126\n        if (group0 or group1 or group2 or group3):\n            return True\n        cat = unicodedata.category(char)\n        if cat.startswith('P'):\n            return True\n        return False", "output": "Checks whether `chars` is a punctuation character.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_plugin_title(self):\n        \"\"\"\"\"\"\n        if self.dockwidget is not None:\n            win = self.dockwidget\n        elif self.undocked_window is not None:\n            win = self.undocked_window\n        else:\n            return\n        win.setWindowTitle(self.get_plugin_title())", "output": "Update plugin title, i.e. dockwidget or window title", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wiki_2x2_base():\n  \"\"\"\n  \"\"\"\n  hparams = mtf_transformer.mtf_transformer_base_lm()\n  hparams.shared_embedding_and_softmax_weights = False\n  # no dropout - dataset is big enough to avoid overfitting.\n  hparams.attention_dropout = 0.0\n  hparams.relu_dropout = 0.0\n  hparams.layer_prepostprocess_dropout = 0.0\n  hparams.max_length = 1024\n  # 4 sequences per core\n  hparams.batch_size = 32\n  # We don't use linear decay in these experiments, since we don't want\n  # a sharp jump in quality at the end of the training schedule.\n  # You can insert this once you find the right architecture.\n  hparams.learning_rate_schedule = \"rsqrt_decay\"\n  hparams.mesh_shape = \"all:8\"\n  hparams.layout = \"batch:all;experts:all\"\n\n  # parameters for mixture-of-experts\n  moe.set_default_moe_hparams(hparams)\n  hparams.moe_num_experts = 16\n  hparams.moe_hidden_size = 8192\n\n  hparams.decoder_layers = [\"att\", \"drd\"] * 6\n  hparams.d_model = 1024\n  hparams.d_ff = 2048\n  hparams.d_kv = 128\n  hparams.num_heads = 4\n\n  return hparams", "output": "Set of architectural experiments - language model on wikipedia on a 2x2.\n\n  1 epoch = ~180k steps at batch size 32 - we may never finish an epoch!\n\n  Returns:\n    a hparams", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def optionally(preprocessor):\n    \"\"\"\n    \"\"\"\n    @wraps(preprocessor)\n    def wrapper(func, argname, arg):\n        return arg if arg is None else preprocessor(func, argname, arg)\n\n    return wrapper", "output": "Modify a preprocessor to explicitly allow `None`.\n\n    Parameters\n    ----------\n    preprocessor : callable[callable, str, any -> any]\n        A preprocessor to delegate to when `arg is not None`.\n\n    Returns\n    -------\n    optional_preprocessor : callable[callable, str, any -> any]\n        A preprocessor that delegates to `preprocessor` when `arg is not None`.\n\n    Examples\n    --------\n    >>> def preprocessor(func, argname, arg):\n    ...     if not isinstance(arg, int):\n    ...         raise TypeError('arg must be int')\n    ...     return arg\n    ...\n    >>> @preprocess(a=optionally(preprocessor))\n    ... def f(a):\n    ...     return a\n    ...\n    >>> f(1)  # call with int\n    1\n    >>> f('a')  # call with not int\n    Traceback (most recent call last):\n       ...\n    TypeError: arg must be int\n    >>> f(None) is None  # call with explicit None\n    True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stats_timing(stats_key, stats_logger):\n    \"\"\"\"\"\"\n    start_ts = now_as_float()\n    try:\n        yield start_ts\n    except Exception as e:\n        raise e\n    finally:\n        stats_logger.timing(stats_key, now_as_float() - start_ts)", "output": "Provide a transactional scope around a series of operations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def isID(self, elem, attr):\n        \"\"\" \"\"\"\n        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        if attr is None: attr__o = None\n        else: attr__o = attr._o\n        ret = libxml2mod.xmlIsID(self._o, elem__o, attr__o)\n        return ret", "output": "Determine whether an attribute is of type ID. In case we\n          have DTD(s) then this is done if DTD loading has been\n          requested. In the case of HTML documents parsed with the\n           HTML parser, then ID detection is done systematically.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _StructMessageToJsonObject(self, message):\n    \"\"\"\"\"\"\n    fields = message.fields\n    ret = {}\n    for key in fields:\n      ret[key] = self._ValueMessageToJsonObject(fields[key])\n    return ret", "output": "Converts Struct message according to Proto3 JSON Specification.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_system_date(newdate):\n    '''\n    \n    '''\n    fmts = ['%Y-%m-%d', '%m-%d-%Y', '%m-%d-%y',\n            '%m/%d/%Y', '%m/%d/%y', '%Y/%m/%d']\n    # Get date/time object from newdate\n    dt_obj = _try_parse_datetime(newdate, fmts)\n    if dt_obj is None:\n        return False\n\n    # Set time using set_system_date_time()\n    return set_system_date_time(years=dt_obj.year,\n                                months=dt_obj.month,\n                                days=dt_obj.day)", "output": "Set the Windows system date. Use <mm-dd-yy> format for the date.\n\n    Args:\n        newdate (str):\n            The date to set. Can be any of the following formats\n\n            - YYYY-MM-DD\n            - MM-DD-YYYY\n            - MM-DD-YY\n            - MM/DD/YYYY\n            - MM/DD/YY\n            - YYYY/MM/DD\n\n    Returns:\n        bool: ``True`` if successful, otherwise ``False``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' system.set_system_date '03-28-13'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ConvertScalarFieldValue(value, field, require_str=False):\n  \"\"\"\n  \"\"\"\n  if field.cpp_type in _INT_TYPES:\n    return _ConvertInteger(value)\n  elif field.cpp_type in _FLOAT_TYPES:\n    return _ConvertFloat(value)\n  elif field.cpp_type == descriptor.FieldDescriptor.CPPTYPE_BOOL:\n    return _ConvertBool(value, require_str)\n  elif field.cpp_type == descriptor.FieldDescriptor.CPPTYPE_STRING:\n    if field.type == descriptor.FieldDescriptor.TYPE_BYTES:\n      return base64.b64decode(value)\n    else:\n      # Checking for unpaired surrogates appears to be unreliable,\n      # depending on the specific Python version, so we check manually.\n      if _UNPAIRED_SURROGATE_PATTERN.search(value):\n        raise ParseError('Unpaired surrogate')\n      return value\n  elif field.cpp_type == descriptor.FieldDescriptor.CPPTYPE_ENUM:\n    # Convert an enum value.\n    enum_value = field.enum_type.values_by_name.get(value, None)\n    if enum_value is None:\n      try:\n        number = int(value)\n        enum_value = field.enum_type.values_by_number.get(number, None)\n      except ValueError:\n        raise ParseError('Invalid enum value {0} for enum type {1}.'.format(\n            value, field.enum_type.full_name))\n      if enum_value is None:\n        raise ParseError('Invalid enum value {0} for enum type {1}.'.format(\n            value, field.enum_type.full_name))\n    return enum_value.number", "output": "Convert a single scalar field value.\n\n  Args:\n    value: A scalar value to convert the scalar field value.\n    field: The descriptor of the field to convert.\n    require_str: If True, the field value must be a str.\n\n  Returns:\n    The converted scalar field value\n\n  Raises:\n    ParseError: In case of convert problems.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _expand_sources(sources):\n    '''\n    \n    '''\n    if sources is None:\n        return []\n    if isinstance(sources, six.string_types):\n        sources = [x.strip() for x in sources.split(',')]\n    elif isinstance(sources, (float, six.integer_types)):\n        sources = [six.text_type(sources)]\n    return [path\n            for source in sources\n            for path in _glob(source)]", "output": "Expands a user-provided specification of source files into a list of paths.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset(self):\n        \"\"\"\"\"\"\n        self.alive.value = False\n        qsize = 0\n        try:\n            while True:\n                self.queue.get(timeout=0.1)\n                qsize += 1\n        except QEmptyExcept:\n            pass\n        print(\"Queue size on reset: {}\".format(qsize))\n        for i, p in enumerate(self.proc):\n            p.join()\n        self.proc.clear()", "output": "Resets the generator by stopping all processes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_images(conn=None):\n    '''\n    \n    '''\n    if not conn:\n        conn = get_conn()\n\n    all_images = []\n    # The list of public image projects can be found via:\n    #   % gcloud compute images list\n    # and looking at the \"PROJECT\" column in the output.\n    public_image_projects = (\n        'centos-cloud', 'coreos-cloud', 'debian-cloud', 'google-containers',\n        'opensuse-cloud', 'rhel-cloud', 'suse-cloud', 'ubuntu-os-cloud',\n        'windows-cloud'\n    )\n    for project in public_image_projects:\n        all_images.extend(conn.list_images(project))\n\n    # Finally, add the images in this current project last so that it overrides\n    # any image that also exists in any public project.\n    all_images.extend(conn.list_images())\n\n    ret = {}\n    for img in all_images:\n        ret[img.name] = {}\n        for attr in dir(img):\n            if attr.startswith('_'):\n                continue\n            ret[img.name][attr] = getattr(img, attr)\n    return ret", "output": "Return a dict of all available VM images on the cloud provider with\n    relevant data.\n\n    Note that for GCE, there are custom images within the project, but the\n    generic images are in other projects.  This returns a dict of images in\n    the project plus images in well-known public projects that provide supported\n    images, as listed on this page:\n    https://cloud.google.com/compute/docs/operating-systems/\n\n    If image names overlap, the image in the current project is used.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def info(vm, info_type='all', key='uuid'):\n    '''\n    \n    '''\n    ret = {}\n    if info_type not in ['all', 'block', 'blockstats', 'chardev', 'cpus', 'kvm', 'pci', 'spice', 'version', 'vnc']:\n        ret['Error'] = 'Requested info_type is not available'\n        return ret\n    if key not in ['uuid', 'alias', 'hostname']:\n        ret['Error'] = 'Key must be either uuid, alias or hostname'\n        return ret\n    vm = lookup('{0}={1}'.format(key, vm), one=True)\n    if 'Error' in vm:\n        return vm\n    # vmadm info <uuid> [type,...]\n    cmd = 'vmadm info {uuid} {type}'.format(\n        uuid=vm,\n        type=info_type\n    )\n    res = __salt__['cmd.run_all'](cmd)\n    retcode = res['retcode']\n    if retcode != 0:\n        ret['Error'] = res['stderr'] if 'stderr' in res else _exit_status(retcode)\n        return ret\n    return salt.utils.json.loads(res['stdout'])", "output": "Lookup info on running kvm\n\n    vm : string\n        vm to be targeted\n    info_type : string [all|block|blockstats|chardev|cpus|kvm|pci|spice|version|vnc]\n        info type to return\n    key : string [uuid|alias|hostname]\n        value type of 'vm' parameter\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' vmadm.info 186da9ab-7392-4f55-91a5-b8f1fe770543\n        salt '*' vmadm.info 186da9ab-7392-4f55-91a5-b8f1fe770543 vnc\n        salt '*' vmadm.info nacl key=alias\n        salt '*' vmadm.info nacl vnc key=alias", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_persistent_module(mod):\n    '''\n    \n    '''\n    conf = _get_modules_conf()\n    if not os.path.exists(conf):\n        __salt__['file.touch'](conf)\n    mod_name = _strip_module_name(mod)\n    if not mod_name or mod_name in mod_list(True) or mod_name \\\n            not in available():\n        return set()\n    escape_mod = re.escape(mod)\n    # If module is commented only uncomment it\n    if __salt__['file.search'](conf,\n                               '^#[\\t ]*{0}[\\t ]*$'.format(escape_mod),\n                               multiline=True):\n        __salt__['file.uncomment'](conf, escape_mod)\n    else:\n        __salt__['file.append'](conf, mod)\n    return set([mod_name])", "output": "Add module to configuration file to make it persistent. If module is\n    commented uncomment it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cluster_remove(version,\n                   name='main',\n                   stop=False):\n    '''\n    \n\n    '''\n    cmd = [salt.utils.path.which('pg_dropcluster')]\n    if stop:\n        cmd += ['--stop']\n    cmd += [version, name]\n    cmdstr = ' '.join([pipes.quote(c) for c in cmd])\n    ret = __salt__['cmd.run_all'](cmdstr, python_shell=False)\n    # FIXME - return Boolean ?\n    if ret.get('retcode', 0) != 0:\n        log.error('Error removing a Postgresql cluster %s/%s', version, name)\n    else:\n        ret['changes'] = ('Successfully removed'\n                          ' cluster {0}/{1}').format(version, name)\n    return ret", "output": "Remove a cluster on a Postgres server. By default it doesn't try\n    to stop the cluster.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' postgres.cluster_remove '9.3'\n\n        salt '*' postgres.cluster_remove '9.3' 'main'\n\n        salt '*' postgres.cluster_remove '9.3' 'main' stop=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _aws_encode_changebatch(o):\n    '''\n    \n    '''\n    change_idx = 0\n    while change_idx < len(o['Changes']):\n        o['Changes'][change_idx]['ResourceRecordSet']['Name'] = aws_encode(o['Changes'][change_idx]['ResourceRecordSet']['Name'])\n        if 'ResourceRecords' in o['Changes'][change_idx]['ResourceRecordSet']:\n            rr_idx = 0\n            while rr_idx < len(o['Changes'][change_idx]['ResourceRecordSet']['ResourceRecords']):\n                o['Changes'][change_idx]['ResourceRecordSet']['ResourceRecords'][rr_idx]['Value'] = aws_encode(o['Changes'][change_idx]['ResourceRecordSet']['ResourceRecords'][rr_idx]['Value'])\n                rr_idx += 1\n        if 'AliasTarget' in o['Changes'][change_idx]['ResourceRecordSet']:\n            o['Changes'][change_idx]['ResourceRecordSet']['AliasTarget']['DNSName'] = aws_encode(o['Changes'][change_idx]['ResourceRecordSet']['AliasTarget']['DNSName'])\n        change_idx += 1\n    return o", "output": "helper method to process a change batch & encode the bits which need encoding.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def acquire(\n        self, timeout: Union[float, datetime.timedelta] = None\n    ) -> Awaitable[_ReleasingContextManager]:\n        \"\"\"\n        \"\"\"\n        waiter = Future()  # type: Future[_ReleasingContextManager]\n        if self._value > 0:\n            self._value -= 1\n            waiter.set_result(_ReleasingContextManager(self))\n        else:\n            self._waiters.append(waiter)\n            if timeout:\n\n                def on_timeout() -> None:\n                    if not waiter.done():\n                        waiter.set_exception(gen.TimeoutError())\n                    self._garbage_collect()\n\n                io_loop = ioloop.IOLoop.current()\n                timeout_handle = io_loop.add_timeout(timeout, on_timeout)\n                waiter.add_done_callback(\n                    lambda _: io_loop.remove_timeout(timeout_handle)\n                )\n        return waiter", "output": "Decrement the counter. Returns an awaitable.\n\n        Block if the counter is zero and wait for a `.release`. The awaitable\n        raises `.TimeoutError` after the deadline.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _split_result_for_readers(axis, num_splits, df):  # pragma: no cover\n    \"\"\"\n    \"\"\"\n    splits = split_result_of_axis_func_pandas(axis, num_splits, df)\n    if not isinstance(splits, list):\n        splits = [splits]\n    return splits", "output": "Splits the DataFrame read into smaller DataFrames and handles all edge cases.\n\n    Args:\n        axis: Which axis to split over.\n        num_splits: The number of splits to create.\n        df: The DataFrame after it has been read.\n\n    Returns:\n        A list of pandas DataFrames.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_backup(name):\n    \n    '''\n    if name in list_backups():\n        raise CommandExecutionError('Backup already present: {0}'.format(name))\n\n    ps_cmd = ['Backup-WebConfiguration',\n              '-Name', \"'{0}'\".format(name)]\n\n    cmd_ret = _srvmgr(ps_cmd)\n\n    if cmd_ret['retcode'] != 0:\n        msg = 'Unable to backup web configuration: {0}\\nError: {1}' \\\n              ''.format(name, cmd_ret['stderr'])\n        raise CommandExecutionError(msg)\n\n    return name in list_backups()", "output": "r'''\n    Backup an IIS Configuration on the System.\n\n    .. versionadded:: 2017.7.0\n\n    .. note::\n        Backups are stored in the ``$env:Windir\\System32\\inetsrv\\backup``\n        folder.\n\n    Args:\n        name (str): The name to give the backup\n\n    Returns:\n        bool: True if successful, otherwise False\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' win_iis.create_backup good_config_20170209", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def active():\n    '''\n    \n    '''\n    ret = {}\n    # TODO: This command should be extended to collect more information, such as UUID.\n    devices = __salt__['cmd.run_stdout']('dmsetup ls --target crypt')\n    out_regex = re.compile(r'(?P<devname>\\w+)\\W+\\((?P<major>\\d+), (?P<minor>\\d+)\\)')\n\n    log.debug(devices)\n    for line in devices.split('\\n'):\n        match = out_regex.match(line)\n        if match:\n            dev_info = match.groupdict()\n            ret[dev_info['devname']] = dev_info\n        else:\n            log.warning('dmsetup output does not match expected format')\n\n    return ret", "output": "List existing device-mapper device details.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dump(self):\n        \"\"\"\"\"\"\n        assert self.database is not None\n\n        cmd = \"SELECT count from {} WHERE rowid={}\"\n        self._execute(cmd.format(self.STATE_INFO_TABLE, self.STATE_INFO_ROW))\n        ret = self._fetchall()\n        assert len(ret) == 1\n        assert len(ret[0]) == 1\n        count = self._from_sqlite(ret[0][0]) + self.inserts\n\n        if count > self.row_limit:\n            msg = \"cleaning up state, this might take a while.\"\n            logger.warning(msg)\n\n            delete = count - self.row_limit\n            delete += int(self.row_limit * (self.row_cleanup_quota / 100.0))\n            cmd = (\n                \"DELETE FROM {} WHERE timestamp IN (\"\n                \"SELECT timestamp FROM {} ORDER BY timestamp ASC LIMIT {});\"\n            )\n            self._execute(\n                cmd.format(self.STATE_TABLE, self.STATE_TABLE, delete)\n            )\n\n            self._vacuum()\n\n            cmd = \"SELECT COUNT(*) FROM {}\"\n\n            self._execute(cmd.format(self.STATE_TABLE))\n            ret = self._fetchall()\n            assert len(ret) == 1\n            assert len(ret[0]) == 1\n            count = ret[0][0]\n\n        cmd = \"UPDATE {} SET count = {} WHERE rowid = {}\"\n        self._execute(\n            cmd.format(\n                self.STATE_INFO_TABLE,\n                self._to_sqlite(count),\n                self.STATE_INFO_ROW,\n            )\n        )\n\n        self._update_cache_directory_state()\n\n        self.database.commit()\n        self.cursor.close()\n        self.database.close()\n        self.database = None\n        self.cursor = None\n        self.inserts = 0", "output": "Saves state database.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_exists(name, user=None, password=None, host=None, port=None,\n                database='admin', authdb=None):\n    '''\n    \n    '''\n    users = user_list(user, password, host, port, database, authdb)\n\n    if isinstance(users, six.string_types):\n        return 'Failed to connect to mongo database'\n\n    for user in users:\n        if name == dict(user).get('user'):\n            return True\n\n    return False", "output": "Checks if a user exists in MongoDB\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mongodb.user_exists <name> <user> <password> <host> <port> <database>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def DownloadResource(url, path):\n    ''''''\n    import requests\n    from six import BytesIO\n    import zipfile\n    print(\"Downloading... {} to {}\".format(url, path))\n    r = requests.get(url, stream=True)\n    z = zipfile.ZipFile(BytesIO(r.content))\n    z.extractall(path)\n    print(\"Completed download and extraction.\")", "output": "Downloads resources from s3 by url and unzips them to the provided path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_disk(self, path, **kwargs):\n        \"\"\"\n        \"\"\"\n        path = ensure_path(path)\n        path = path.with_suffix(\".jsonl\")\n        srsly.write_jsonl(path, self.patterns)", "output": "Save the entity ruler patterns to a directory. The patterns will be\n        saved as newline-delimited JSON (JSONL).\n\n        path (unicode / Path): The JSONL file to load.\n        **kwargs: Other config paramters, mostly for consistency.\n        RETURNS (EntityRuler): The loaded entity ruler.\n\n        DOCS: https://spacy.io/api/entityruler#to_disk", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(format, string, extra_types=None, evaluate_result=True, case_sensitive=False):\n    '''\n    '''\n    p = Parser(format, extra_types=extra_types, case_sensitive=case_sensitive)\n    return p.parse(string, evaluate_result=evaluate_result)", "output": "Using \"format\" attempt to pull values from \"string\".\n\n    The format must match the string contents exactly. If the value\n    you're looking for is instead just a part of the string use\n    search().\n\n    If ``evaluate_result`` is True the return value will be an Result instance with two attributes:\n\n     .fixed - tuple of fixed-position values from the string\n     .named - dict of named values from the string\n\n    If ``evaluate_result`` is False the return value will be a Match instance with one method:\n\n     .evaluate_result() - This will return a Result instance like you would get\n                          with ``evaluate_result`` set to True\n\n    The default behaviour is to match strings case insensitively. You may match with\n    case by specifying case_sensitive=True.\n\n    If the format is invalid a ValueError will be raised.\n\n    See the module documentation for the use of \"extra_types\".\n\n    In the case there is no match parse() will return None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def strip_extras_markers_from_requirement(req):\n    # type: (TRequirement) -> TRequirement\n    \"\"\"\n    \n    \"\"\"\n    if req is None:\n        raise TypeError(\"Must pass in a valid requirement, received {0!r}\".format(req))\n    if getattr(req, \"marker\", None) is not None:\n        marker = req.marker  # type: TMarker\n        marker._markers = _strip_extras_markers(marker._markers)\n        if not marker._markers:\n            req.marker = None\n        else:\n            req.marker = marker\n    return req", "output": "Given a :class:`~packaging.requirements.Requirement` instance with markers defining\n    *extra == 'name'*, strip out the extras from the markers and return the cleaned\n    requirement\n\n    :param PackagingRequirement req: A packaging requirement to clean\n    :return: A cleaned requirement\n    :rtype: PackagingRequirement", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def readerWalker(self):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlReaderWalker(self._o)\n        if ret is None:raise treeError('xmlReaderWalker() failed')\n        __tmp = xmlTextReader(_obj=ret)\n        return __tmp", "output": "Create an xmltextReader for a preparsed document.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exe(self):\n        \"\"\"\n        \"\"\"\n        return self._buckets[self.curr_bucket_key]['exe'][tuple(self.data_shapes.items())]", "output": "Get the current executor\n\n        Returns\n        -------\n        exe : mxnet.executor.Executor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def safe_isinstance(value, types=None, class_names=None):\n    \"\"\"\n    \"\"\"\n    # inspect is being imported here because I seriously doubt\n    # that this function will be used outside of the type\n    # checking below.\n    import inspect\n    result = False\n    if types is not None:\n        result = result or isinstance(value, types)\n    if class_names is not None and not result:\n        # this doesn't work with inheritance, but normally\n        # either the class will already be imported within the module,\n        # or the class doesn't have any subclasses. For example: PropertySet\n        if isinstance(class_names, basestring):\n            class_names = [class_names]\n        # this is the part that makes it \"safe\".\n        try:\n            base_names = [class_.__name__ for class_ in inspect.getmro(value.__class__)]\n            for name in class_names:\n                if name in base_names:\n                    return True\n        except AttributeError:\n            pass\n    return result", "output": "To prevent circular imports, this extends isinstance()\n    by checking also if `value` has a particular class name (or inherits from a\n    particular class name). This check is safe in that an AttributeError is not\n    raised in case `value` doesn't have a __class__ attribute.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def revnet_step(name, x, hparams, reverse=True):\n  \"\"\"\n  \"\"\"\n  with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n    if hparams.coupling == \"additive\":\n      coupling_layer = functools.partial(\n          additive_coupling, name=\"additive\", reverse=reverse,\n          mid_channels=hparams.coupling_width,\n          activation=hparams.activation, dropout=hparams.coupling_dropout)\n    else:\n      coupling_layer = functools.partial(\n          affine_coupling, name=\"affine\", reverse=reverse,\n          mid_channels=hparams.coupling_width,\n          activation=hparams.activation, dropout=hparams.coupling_dropout)\n    ops = [\n        functools.partial(actnorm, name=\"actnorm\", reverse=reverse),\n        functools.partial(invertible_1x1_conv, name=\"invertible\",\n                          reverse=reverse), coupling_layer]\n\n    if reverse:\n      ops = ops[::-1]\n\n    objective = 0.0\n    for op in ops:\n      x, curr_obj = op(x=x)\n      objective += curr_obj\n    return x, objective", "output": "One step of glow generative flow.\n\n  Actnorm + invertible 1X1 conv + affine_coupling.\n\n  Args:\n    name: used for variable scope.\n    x: input\n    hparams: coupling_width is the only hparam that is being used in\n             this function.\n    reverse: forward or reverse pass.\n  Returns:\n    z: Output of one step of reversible flow.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_path(python):\n    \"\"\"\n    \"\"\"\n\n    python = Path(python).as_posix()\n    out, err = run(\n        [python, \"-c\", \"import json, sys; print(json.dumps(sys.path))\"], nospin=True\n    )\n    if out:\n        return json.loads(out)\n    else:\n        return []", "output": "Load the :mod:`sys.path` from the given python executable's environment as json\n\n    :param str python: Path to a valid python executable\n    :return: A python representation of the `sys.path` value of the given python executable.\n    :rtype: list\n\n    >>> load_path(\"/home/user/.virtualenvs/requirementslib-5MhGuG3C/bin/python\")\n    ['', '/home/user/.virtualenvs/requirementslib-5MhGuG3C/lib/python37.zip', '/home/user/.virtualenvs/requirementslib-5MhGuG3C/lib/python3.7', '/home/user/.virtualenvs/requirementslib-5MhGuG3C/lib/python3.7/lib-dynload', '/home/user/.pyenv/versions/3.7.0/lib/python3.7', '/home/user/.virtualenvs/requirementslib-5MhGuG3C/lib/python3.7/site-packages', '/home/user/git/requirementslib/src']", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _transform_should_cast(self, func_nm):\n        \"\"\"\n        \n        \"\"\"\n        return (self.size().fillna(0) > 0).any() and (\n            func_nm not in base.cython_cast_blacklist)", "output": "Parameters:\n        -----------\n        func_nm: str\n            The name of the aggregation function being performed\n\n        Returns:\n        --------\n        bool\n            Whether transform should attempt to cast the result of aggregation", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait_input(self, prompt=''):\r\n        \"\"\"\"\"\"\r\n        self.new_prompt(prompt)\r\n        self.setFocus()\r\n        self.input_mode = True\r\n        self.input_loop = QEventLoop()\r\n        self.input_loop.exec_()\r\n        self.input_loop = None", "output": "Wait for input (raw_input support)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_template_data(template_file):\n    \"\"\"\n    \n    \"\"\"\n\n    if not pathlib.Path(template_file).exists():\n        raise ValueError(\"Template file not found at {}\".format(template_file))\n\n    with open(template_file, 'r') as fp:\n        try:\n            return yaml_parse(fp.read())\n        except (ValueError, yaml.YAMLError) as ex:\n            raise ValueError(\"Failed to parse template: {}\".format(str(ex)))", "output": "Read the template file, parse it as JSON/YAML and return the template as a dictionary.\n\n    Parameters\n    ----------\n    template_file : string\n        Path to the template to read\n\n    Returns\n    -------\n    Template data as a dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def row(*args, **kwargs):\n    \"\"\" \n    \"\"\"\n\n    sizing_mode = kwargs.pop('sizing_mode', None)\n    children = kwargs.pop('children', None)\n\n    children = _handle_children(*args, children=children)\n\n    row_children = []\n    for item in children:\n        if isinstance(item, LayoutDOM):\n            if sizing_mode is not None and _has_auto_sizing(item):\n                item.sizing_mode = sizing_mode\n            row_children.append(item)\n        else:\n            raise ValueError(\"\"\"Only LayoutDOM items can be inserted into a row. Tried to insert: %s of type %s\"\"\" % (item, type(item)))\n\n    return Row(children=row_children, sizing_mode=sizing_mode, **kwargs)", "output": "Create a row of Bokeh Layout objects. Forces all objects to\n    have the same sizing_mode, which is required for complex layouts to work.\n\n    Args:\n        children (list of :class:`~bokeh.models.layouts.LayoutDOM` ): A list of instances for\n            the row. Can be any of the following - :class:`~bokeh.models.plots.Plot`,\n            :class:`~bokeh.models.widgets.widget.Widget`,\n            :class:`~bokeh.models.layouts.Row`,\n            :class:`~bokeh.models.layouts.Column`,\n            :class:`~bokeh.models.tools.ToolbarBox`,\n            :class:`~bokeh.models.layouts.Spacer`.\n\n        sizing_mode (``\"fixed\"``, ``\"stretch_both\"``, ``\"scale_width\"``, ``\"scale_height\"``, ``\"scale_both\"`` ): How\n            will the items in the layout resize to fill the available space.\n            Default is ``\"fixed\"``. For more information on the different\n            modes see :attr:`~bokeh.models.layouts.LayoutDOM.sizing_mode`\n            description on :class:`~bokeh.models.layouts.LayoutDOM`.\n\n    Returns:\n        Row: A row of LayoutDOM objects all with the same sizing_mode.\n\n    Examples:\n\n        >>> row([plot_1, plot_2])\n        >>> row(children=[widget_box_1, plot_1], sizing_mode='stretch_both')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def data(self, index, role):\r\n        \"\"\"\"\"\"\r\n        if role == Qt.ToolTipRole:\r\n            root_dir = self.path_list[0].split(osp.sep)[-1]\r\n            if index.data() == root_dir:\r\n                return osp.join(self.root_path, root_dir)\r\n        return QSortFilterProxyModel.data(self, index, role)", "output": "Show tooltip with full path only for the root directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_protobuf_value(value_pb, val):\n    \"\"\"\n    \"\"\"\n    attr, val = _pb_attr_value(val)\n    if attr == \"key_value\":\n        value_pb.key_value.CopyFrom(val)\n    elif attr == \"timestamp_value\":\n        value_pb.timestamp_value.CopyFrom(val)\n    elif attr == \"entity_value\":\n        entity_pb = entity_to_protobuf(val)\n        value_pb.entity_value.CopyFrom(entity_pb)\n    elif attr == \"array_value\":\n        if len(val) == 0:\n            array_value = entity_pb2.ArrayValue(values=[])\n            value_pb.array_value.CopyFrom(array_value)\n        else:\n            l_pb = value_pb.array_value.values\n            for item in val:\n                i_pb = l_pb.add()\n                _set_protobuf_value(i_pb, item)\n    elif attr == \"geo_point_value\":\n        value_pb.geo_point_value.CopyFrom(val)\n    else:  # scalar, just assign\n        setattr(value_pb, attr, val)", "output": "Assign 'val' to the correct subfield of 'value_pb'.\n\n    The Protobuf API uses different attribute names based on value types\n    rather than inferring the type.\n\n    Some value types (entities, keys, lists) cannot be directly\n    assigned; this function handles them correctly.\n\n    :type value_pb: :class:`.entity_pb2.Value`\n    :param value_pb: The value protobuf to which the value is being assigned.\n\n    :type val: :class:`datetime.datetime`, boolean, float, integer, string,\n               :class:`google.cloud.datastore.key.Key`,\n               :class:`google.cloud.datastore.entity.Entity`\n    :param val: The value to be assigned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def contextMenuEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        if self.model.showndata:\r\n            self.refresh_menu()\r\n            self.menu.popup(event.globalPos())\r\n            event.accept()\r\n        else:\r\n            self.empty_ws_menu.popup(event.globalPos())\r\n            event.accept()", "output": "Reimplement Qt method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_img(object, handle, **kwargs):\n    \"\"\"\"\"\"\n\n    if isinstance(object, np.ndarray):\n        normalized = _normalize_array(object)\n        object = PIL.Image.fromarray(normalized)\n\n    if isinstance(object, PIL.Image.Image):\n        object.save(handle, **kwargs)  # will infer format from handle's url ext.\n    else:\n        raise ValueError(\"Can only save_img for numpy arrays or PIL.Images!\")", "output": "Save numpy array as image file on CNS.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uniform_binning_correction(x, n_bits=8):\n  \"\"\"\n  \"\"\"\n  n_bins = 2**n_bits\n  batch_size, height, width, n_channels = common_layers.shape_list(x)\n  hwc = float(height * width * n_channels)\n\n  x = x + tf.random_uniform(\n      shape=(batch_size, height, width, n_channels),\n      minval=0.0, maxval=1.0/n_bins)\n  objective = -np.log(n_bins) * hwc * tf.ones(batch_size)\n  return x, objective", "output": "Replaces x^i with q^i(x) = U(x, x + 1.0 / 256.0).\n\n  Args:\n    x: 4-D Tensor of shape (NHWC)\n    n_bits: optional.\n  Returns:\n    x: x ~ U(x, x + 1.0 / 256)\n    objective: Equivalent to -q(x)*log(q(x)).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def draw_boxes(im, boxes, labels=None, color=None):\n    \"\"\"\n    \n    \"\"\"\n    boxes = np.asarray(boxes, dtype='int32')\n    if labels is not None:\n        assert len(labels) == len(boxes), \"{} != {}\".format(len(labels), len(boxes))\n    areas = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n    sorted_inds = np.argsort(-areas)    # draw large ones first\n    assert areas.min() > 0, areas.min()\n    # allow equal, because we are not very strict about rounding error here\n    assert boxes[:, 0].min() >= 0 and boxes[:, 1].min() >= 0 \\\n        and boxes[:, 2].max() <= im.shape[1] and boxes[:, 3].max() <= im.shape[0], \\\n        \"Image shape: {}\\n Boxes:\\n{}\".format(str(im.shape), str(boxes))\n\n    im = im.copy()\n    if color is None:\n        color = (15, 128, 15)\n    if im.ndim == 2 or (im.ndim == 3 and im.shape[2] == 1):\n        im = cv2.cvtColor(im, cv2.COLOR_GRAY2BGR)\n    for i in sorted_inds:\n        box = boxes[i, :]\n        if labels is not None:\n            im = draw_text(im, (box[0], box[1]), labels[i], color=color)\n        cv2.rectangle(im, (box[0], box[1]), (box[2], box[3]),\n                      color=color, thickness=1)\n    return im", "output": "Args:\n        im (np.ndarray): a BGR image in range [0,255]. It will not be modified.\n        boxes (np.ndarray): a numpy array of shape Nx4 where each row is [x1, y1, x2, y2].\n        labels: (list[str] or None)\n        color: a 3-tuple BGR color (in range [0, 255])\n\n    Returns:\n        np.ndarray: a new image.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_blobs(self, prefix=''):\n    \"\"\"\"\"\"\n    return [b.name for b in self.bucket.list_blobs(prefix=prefix)]", "output": "Lists names of all blobs by their prefix.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_service(protocol=None, service_address=None, scheduler='wlc'):\n    '''\n    \n    '''\n\n    cmd = '{0} -A {1}'.format(__detect_os(),\n                              _build_cmd(protocol=protocol,\n                                         service_address=service_address,\n                                         scheduler=scheduler))\n    out = __salt__['cmd.run_all'](cmd, python_shell=False)\n\n    # A non-zero return code means fail\n    if out['retcode']:\n        ret = out['stderr'].strip()\n    else:\n        ret = True\n    return ret", "output": "Add a virtual service.\n\n    protocol\n        The service protocol(only support tcp, udp and fwmark service).\n\n    service_address\n        The LVS service address.\n\n    scheduler\n        Algorithm for allocating TCP connections and UDP datagrams to real servers.\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' lvs.add_service tcp 1.1.1.1:80 rr", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dataset(self, dataset_ref, retry=DEFAULT_RETRY):\n        \"\"\"\n        \"\"\"\n        if isinstance(dataset_ref, str):\n            dataset_ref = DatasetReference.from_string(\n                dataset_ref, default_project=self.project\n            )\n\n        api_response = self._call_api(retry, method=\"GET\", path=dataset_ref.path)\n        return Dataset.from_api_repr(api_response)", "output": "Fetch the dataset referenced by ``dataset_ref``\n\n        Args:\n            dataset_ref (Union[ \\\n                :class:`~google.cloud.bigquery.dataset.DatasetReference`, \\\n                str, \\\n            ]):\n                A reference to the dataset to fetch from the BigQuery API.\n                If a string is passed in, this method attempts to create a\n                dataset reference from a string using\n                :func:`~google.cloud.bigquery.dataset.DatasetReference.from_string`.\n            retry (:class:`google.api_core.retry.Retry`):\n                (Optional) How to retry the RPC.\n\n        Returns:\n            google.cloud.bigquery.dataset.Dataset:\n                A ``Dataset`` instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deploy_api(self, ret):\n        '''\n        \n        '''\n        if self.restApiId:\n            res = self._cleanup_api()\n            if not res.get('deleted'):\n                ret['comment'] = 'Failed to cleanup restAreId {0}'.format(self.restApiId)\n                ret['abort'] = True\n                ret['result'] = False\n                return ret\n            return ret\n\n        response = __salt__['boto_apigateway.create_api'](name=self.rest_api_name,\n                                                          description=_Swagger.AWS_API_DESCRIPTION,\n                                                          **self._common_aws_args)\n\n        if not response.get('created'):\n            ret['result'] = False\n            ret['abort'] = True\n            if 'error' in response:\n                ret['comment'] = 'Failed to create rest api: {0}.'.format(response['error']['message'])\n            return ret\n\n        self.restApiId = response.get('restapi', {}).get('id')\n\n        return _log_changes(ret, 'deploy_api', response.get('restapi'))", "output": "this method create the top level rest api in AWS apigateway", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_data(label, image):\n    \"\"\"\n    \n    \"\"\"\n    base_url = 'http://yann.lecun.com/exdb/mnist/'\n    with gzip.open(download_file(base_url+label, os.path.join('data',label))) as flbl:\n        magic, num = struct.unpack(\">II\", flbl.read(8))\n        label = np.fromstring(flbl.read(), dtype=np.int8)\n    with gzip.open(download_file(base_url+image, os.path.join('data',image)), 'rb') as fimg:\n        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n        image = np.fromstring(fimg.read(), dtype=np.uint8).reshape(len(label), rows, cols)\n    return (label, image)", "output": "download and read data into numpy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_plugin(self):\r\n        \"\"\"\"\"\"\r\n        self.main.add_dockwidget(self)\r\n\r\n        self.focus_changed.connect(self.main.plugin_focus_changed)\r\n        self.edit_goto.connect(self.main.editor.load)\r\n        self.edit_goto[str, int, str, bool].connect(\r\n                         lambda fname, lineno, word, processevents:\r\n                         self.main.editor.load(fname, lineno, word,\r\n                                               processevents=processevents))\r\n        self.main.editor.breakpoints_saved.connect(self.set_spyder_breakpoints)\r\n        self.main.editor.run_in_current_ipyclient.connect(self.run_script)\r\n        self.main.editor.run_cell_in_ipyclient.connect(self.run_cell)\r\n        self.main.workingdirectory.set_current_console_wd.connect(\r\n                                     self.set_current_client_working_directory)\r\n\r\n        self.tabwidget.currentChanged.connect(self.update_working_directory)\r\n\r\n        self._remove_old_stderr_files()", "output": "Register plugin in Spyder's main window", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_elements_by_class_name(self, name):\n        \"\"\"\n        \n        \"\"\"\n        return self.find_elements(by=By.CLASS_NAME, value=name)", "output": "Finds elements by class name.\n\n        :Args:\n         - name: The class name of the elements to find.\n\n        :Returns:\n         - list of WebElement - a list with elements if any was found.  An\n           empty list if not\n\n        :Usage:\n            ::\n\n                elements = driver.find_elements_by_class_name('foo')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ntile(n):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.ntile(int(n)))", "output": "Window function: returns the ntile group id (from 1 to `n` inclusive)\n    in an ordered window partition. For example, if `n` is 4, the first\n    quarter of the rows will get value 1, the second quarter will get 2,\n    the third quarter will get 3, and the last quarter will get 4.\n\n    This is equivalent to the NTILE function in SQL.\n\n    :param n: an integer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gather_file_data(name):\n    \"\"\"\n    \n    \"\"\"\n    res = {'name': name}\n    try:\n        res['mtime'] = osp.getmtime(name)\n        res['size'] = osp.getsize(name)\n    except OSError:\n        pass\n    return res", "output": "Gather data about a given file.\n\n    Returns a dict with fields name, mtime and size, containing the relevant\n    data for the fiel.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def PositionalEncoding(x, params, **unused_kwargs):\n  \"\"\"\"\"\"\n  if not isinstance(x, (list, tuple)):  # non-chunked inputs\n    symbol_size = np.shape(x)[1]\n    return x + params[:, :symbol_size, :]\n  # Chunked case: apply to all chunks selecting as much as needed.\n  offset = 0\n  results = []\n  for chunk in x:\n    symbol_size = np.shape(chunk)[1]\n    results.append(chunk + params[:, offset:offset + symbol_size, :])\n    offset += symbol_size\n  return results", "output": "Implements bare positional encoding.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_shell(pid=None, max_depth=6):\n    \"\"\"\n    \"\"\"\n    pid = str(pid or os.getpid())\n    mapping = _get_process_mapping()\n    login_shell = os.environ.get('SHELL', '')\n    for _ in range(max_depth):\n        try:\n            proc = mapping[pid]\n        except KeyError:\n            break\n        name = os.path.basename(proc.args[0]).lower()\n        if name in SHELL_NAMES:\n            return (name, proc.args[0])\n        elif proc.args[0].startswith('-'):\n            # This is the login shell. Use the SHELL environ if possible\n            # because it provides better information.\n            if login_shell:\n                name = login_shell.lower()\n            else:\n                name = proc.args[0][1:].lower()\n            return (os.path.basename(name), name)\n        pid = proc.ppid     # Go up one level.\n    return None", "output": "Get the shell that the supplied pid or os.getpid() is running in.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_project(self, path):\r\n        \"\"\"\"\"\"\r\n        self.open_project(path=path)\r\n        self.setup_menu_actions()\r\n        self.add_to_recent(path)", "output": "Create a new project.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, name_or_klass):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(name_or_klass, str):\n            name_or_klass = name_or_klass.__name__\n        return self._extensions[name_or_klass]", "output": "Get a extension by name (or class).\n\n        :param name_or_klass: The name or the class of the extension to get\n        :type name_or_klass: str or type\n        :rtype: spyder.api.mode.EditorExtension", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_rpmmacros(runas='root'):\n    '''\n    \n    '''\n    home = os.path.expanduser('~')\n    rpmbuilddir = os.path.join(home, 'rpmbuild')\n    if not os.path.isdir(rpmbuilddir):\n        __salt__['file.makedirs_perms'](name=rpmbuilddir, user=runas, group='mock')\n\n    mockdir = os.path.join(home, 'mock')\n    if not os.path.isdir(mockdir):\n        __salt__['file.makedirs_perms'](name=mockdir, user=runas, group='mock')\n\n    rpmmacros = os.path.join(home, '.rpmmacros')\n    with salt.utils.files.fopen(rpmmacros, 'w') as afile:\n        afile.write(\n            salt.utils.stringutils.to_str('%_topdir {0}\\n'.format(rpmbuilddir))\n        )\n        afile.write('%signature gpg\\n')\n        afile.write('%_source_filedigest_algorithm 8\\n')\n        afile.write('%_binary_filedigest_algorithm 8\\n')\n        afile.write('%_gpg_name packaging@saltstack.com\\n')", "output": "Create the .rpmmacros file in user's home directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rae(label, pred):\n    \"\"\"\"\"\"\n    numerator = np.mean(np.abs(label - pred), axis=None)\n    denominator = np.mean(np.abs(label - np.mean(label, axis=None)), axis=None)\n    return numerator / denominator", "output": "computes the relative absolute error (condensed using standard deviation formula)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ParseAbstractInteger(text, is_long=False):\n  \"\"\"\n  \"\"\"\n  # Do the actual parsing. Exception handling is propagated to caller.\n  try:\n    # We force 32-bit values to int and 64-bit values to long to make\n    # alternate implementations where the distinction is more significant\n    # (e.g. the C++ implementation) simpler.\n    if is_long:\n      return long(text, 0)\n    else:\n      return int(text, 0)\n  except ValueError:\n    raise ValueError('Couldn\\'t parse integer: %s' % text)", "output": "Parses an integer without checking size/signedness.\n\n  Args:\n    text: The text to parse.\n    is_long: True if the value should be returned as a long integer.\n\n  Returns:\n    The integer value.\n\n  Raises:\n    ValueError: Thrown Iff the text is not a valid integer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_selected_cb(parents, combobox):\n    \"\"\"\n    \n    \"\"\"\n    if parents is not None and len(parents) == 0:\n        combobox.setCurrentIndex(0)\n    else:\n        item = parents[-1]\n        for i in range(combobox.count()):\n            if combobox.itemData(i) == item:\n                combobox.setCurrentIndex(i)\n                break", "output": "Update the combobox with the selected item based on the parents.\n\n    Parameters\n    ----------\n    parents : list of :class:`FoldScopeHelper`\n    combobox : :class:`qtpy.QtWidets.QComboBox`\n        The combobox to populate\n\n    Returns\n    -------\n    None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bias(self, arr:Collection, is_item:bool=True):\n        \"\"\n        idx = self.get_idx(arr, is_item)\n        m = self.model\n        layer = m.i_bias if is_item else m.u_bias\n        return layer(idx).squeeze()", "output": "Bias for item or user (based on `is_item`) for all in `arr`. (Set model to `cpu` and no grad.)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def indexables(self):\n        \"\"\"  \"\"\"\n        if self._indexables is None:\n\n            d = self.description\n\n            # the index columns is just a simple index\n            self._indexables = [GenericIndexCol(name='index', axis=0)]\n\n            for i, n in enumerate(d._v_names):\n\n                dc = GenericDataIndexableCol(\n                    name=n, pos=i, values=[n], version=self.version)\n                self._indexables.append(dc)\n\n        return self._indexables", "output": "create the indexables from the table description", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install(cert,\n            password,\n            keychain=\"/Library/Keychains/System.keychain\",\n            allow_any=False,\n            keychain_password=None):\n    '''\n    \n    '''\n    if keychain_password is not None:\n        unlock_keychain(keychain, keychain_password)\n\n    cmd = 'security import {0} -P {1} -k {2}'.format(cert, password, keychain)\n    if allow_any:\n        cmd += ' -A'\n    return __salt__['cmd.run'](cmd)", "output": "Install a certificate\n\n    cert\n        The certificate to install\n\n    password\n        The password for the certificate being installed formatted in the way\n        described for openssl command in the PASS PHRASE ARGUMENTS section.\n\n        Note: The password given here will show up as plaintext in the job returned\n        info.\n\n    keychain\n        The keychain to install the certificate to, this defaults to\n        /Library/Keychains/System.keychain\n\n    allow_any\n        Allow any application to access the imported certificate without warning\n\n    keychain_password\n        If your keychain is likely to be locked pass the password and it will be unlocked\n        before running the import\n\n        Note: The password given here will show up as plaintext in the returned job\n        info.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keychain.install test.p12 test123", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_factors(n):\n    \"\"\"\n    \"\"\"\n\n    def factor(n, i, combi, res):\n        \"\"\"[summary]\n        helper function\n\n        Arguments:\n            n {[int]} -- [number]\n            i {[int]} -- [to tested divisor]\n            combi {[list]} -- [catch divisors]\n            res {[list]} -- [all factors of the number n]\n        \n        Returns:\n            [list] -- [res]\n        \"\"\"\n\n        while i * i <= n:\n            if n % i == 0:\n                res += combi + [i, int(n/i)],\n                factor(n/i, i, combi+[i], res)\n            i += 1\n        return res\n    return factor(n, 2, [], [])", "output": "[summary]\n    \n    Arguments:\n        n {[int]} -- [to analysed number]\n    \n    Returns:\n        [list of lists] -- [all factors of the number n]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __get_managed_files_rpm(self):\n        '''\n        \n        '''\n        dirs = set()\n        links = set()\n        files = set()\n\n        for line in salt.utils.stringutils.to_str(self._syscall(\"rpm\", None, None, '-qlav')[0]).split(os.linesep):\n            line = line.strip()\n            if not line:\n                continue\n            line = line.replace(\"\\t\", \" \").split(\" \")\n            if line[0][0] == \"d\":\n                dirs.add(line[-1])\n            elif line[0][0] == \"l\":\n                links.add(line[-1])\n            elif line[0][0] == \"-\":\n                files.add(line[-1])\n\n        return sorted(files), sorted(dirs), sorted(links)", "output": "Get a list of all system files, belonging to the RedHat package manager.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chown(self, path, owner, group, recursive=False):\n        \"\"\"\n        \n        \"\"\"\n        bite = self.get_bite()\n        if owner:\n            if group:\n                return all(bite.chown(self.list_path(path), \"%s:%s\" % (owner, group),\n                                      recurse=recursive))\n            return all(bite.chown(self.list_path(path), owner, recurse=recursive))\n        return list(bite.chgrp(self.list_path(path), group, recurse=recursive))", "output": "Use snakebite.chown/chgrp, if available.\n\n        One of owner or group must be set. Just setting group calls chgrp.\n\n        :param path: update-able file(s)\n        :type path: either a string or sequence of strings\n        :param owner: new owner, can be blank\n        :type owner: string\n        :param group: new group, can be blank\n        :type group: string\n        :param recursive: change just listed entry(ies) or all in directories\n        :type recursive: boolean, default is False\n        :return: list of all changed items", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_list(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.list_images(**kwargs)", "output": "List images\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' glanceng.image_list\n        salt '*' glanceng.image_list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rand_zoom(scale:uniform=1.0, p:float=1.):\n    \"\"\n    return zoom(scale=scale, **rand_pos, p=p)", "output": "Randomized version of `zoom`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_vocab_from_list(self, vocab_list):\n    \"\"\"\n    \"\"\"\n    def token_gen():\n      for token in vocab_list:\n        if token not in RESERVED_TOKENS:\n          yield token\n\n    self._init_vocab(token_gen())", "output": "Initialize tokens from a list of tokens.\n\n    It is ok if reserved tokens appear in the vocab list. They will be\n    removed. The set of tokens in vocab_list should be unique.\n\n    Args:\n      vocab_list: A list of tokens.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dzip_exact(*dicts):\n    \"\"\"\n    \n    \"\"\"\n    if not same(*map(viewkeys, dicts)):\n        raise ValueError(\n            \"dict keys not all equal:\\n\\n%s\" % _format_unequal_keys(dicts)\n        )\n    return {k: tuple(d[k] for d in dicts) for k in dicts[0]}", "output": "Parameters\n    ----------\n    *dicts : iterable[dict]\n        A sequence of dicts all sharing the same keys.\n\n    Returns\n    -------\n    zipped : dict\n        A dict whose keys are the union of all keys in *dicts, and whose values\n        are tuples of length len(dicts) containing the result of looking up\n        each key in each dict.\n\n    Raises\n    ------\n    ValueError\n        If dicts don't all have the same keys.\n\n    Examples\n    --------\n    >>> result = dzip_exact({'a': 1, 'b': 2}, {'a': 3, 'b': 4})\n    >>> result == {'a': (1, 3), 'b': (2, 4)}\n    True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forward_backward(self, x):\n        \"\"\"\"\"\"\n        (src_seq, tgt_seq, src_valid_length, tgt_valid_length), batch_size = x\n        with mx.autograd.record():\n            out, _ = self._model(src_seq, tgt_seq[:, :-1],\n                                 src_valid_length, tgt_valid_length - 1)\n            smoothed_label = self._label_smoothing(tgt_seq[:, 1:])\n            ls = self._loss(out, smoothed_label, tgt_valid_length - 1).sum()\n            ls = (ls * (tgt_seq.shape[1] - 1)) / batch_size / self._rescale_loss\n        ls.backward()\n        return ls", "output": "Perform forward and backward computation for a batch of src seq and dst seq", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_delete(self, path: str, handler: _WebHandler,\n                   **kwargs: Any) -> AbstractRoute:\n        \"\"\"\n        \n        \"\"\"\n        return self.add_route(hdrs.METH_DELETE, path, handler, **kwargs)", "output": "Shortcut for add_route with method DELETE", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dump(self):\n        \"\"\"\"\"\"\n        id = self.get(\"id\")\n        if not id:\n            id = \"(none)\"\n        else:\n            id = id[0]\n\n        parent = self.get(\"parent\")\n        if not parent:\n            parent = \"(none)\"\n        else:\n            parent = parent[0]\n\n        print \"'%s'\" % id\n        print \"Parent project:%s\", parent\n        print \"Requirements:%s\", self.get(\"requirements\")\n        print \"Default build:%s\", string.join(self.get(\"debuild-build\"))\n        print \"Source location:%s\", string.join(self.get(\"source-location\"))\n        print \"Projects to build:%s\", string.join(self.get(\"projects-to-build\").sort());", "output": "Prints the project attributes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_search_space(path):\n    ''''''\n    content = json.dumps(get_json_content(path))\n    if not content:\n        raise ValueError('searchSpace file should not be empty')\n    return content", "output": "load search space content", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_bool(val):\n    '''\n    \n    '''\n    if val is None:\n        return False\n    if isinstance(val, bool):\n        return val\n    if isinstance(val, (six.text_type, six.string_types)):\n        return val.lower() in ('yes', '1', 'true')\n    if isinstance(val, six.integer_types):\n        return val > 0\n    if not isinstance(val, collections.Hashable):\n        return bool(val)\n    return False", "output": "Returns the logical value.\n\n    .. code-block:: jinja\n\n        {{ 'yes' | to_bool }}\n\n    will be rendered as:\n\n    .. code-block:: text\n\n        True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def masked_rec_array_to_mgr(data, index, columns, dtype, copy):\n    \"\"\"\n    \n    \"\"\"\n\n    # essentially process a record array then fill it\n    fill_value = data.fill_value\n    fdata = ma.getdata(data)\n    if index is None:\n        index = get_names_from_index(fdata)\n        if index is None:\n            index = ibase.default_index(len(data))\n    index = ensure_index(index)\n\n    if columns is not None:\n        columns = ensure_index(columns)\n    arrays, arr_columns = to_arrays(fdata, columns)\n\n    # fill if needed\n    new_arrays = []\n    for fv, arr, col in zip(fill_value, arrays, arr_columns):\n        mask = ma.getmaskarray(data[col])\n        if mask.any():\n            arr, fv = maybe_upcast(arr, fill_value=fv, copy=True)\n            arr[mask] = fv\n        new_arrays.append(arr)\n\n    # create the manager\n    arrays, arr_columns = reorder_arrays(new_arrays, arr_columns, columns)\n    if columns is None:\n        columns = arr_columns\n\n    mgr = arrays_to_mgr(arrays, arr_columns, index, columns, dtype)\n\n    if copy:\n        mgr = mgr.copy()\n    return mgr", "output": "Extract from a masked rec array and create the manager.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_running():\n    '''\n    \n    '''\n    ret = []\n    for line in _machinectl('list')['stdout'].splitlines():\n        try:\n            ret.append(line.split()[0])\n        except IndexError:\n            pass\n    return sorted(ret)", "output": "Lists running nspawn containers\n\n    .. note::\n\n        ``nspawn.list`` also works to list running containers\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion nspawn.list_running\n        salt myminion nspawn.list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _nodetool(cmd):\n    '''\n    \n    '''\n    nodetool = __salt__['config.option']('cassandra.nodetool')\n    host = __salt__['config.option']('cassandra.host')\n    return __salt__['cmd.run_stdout']('{0} -h {1} {2}'.format(nodetool, host, cmd))", "output": "Internal cassandra nodetool wrapper. Some functions are not\n    available via pycassa so we must rely on nodetool.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def instance_norm(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'epsilon' : 'eps'})\n    new_attrs['eps'] = attrs.get('epsilon', 1e-5)\n    return 'InstanceNorm', new_attrs, inputs", "output": "Instance Normalization.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_hg_revision(repopath):\r\n    \"\"\"\r\n    \"\"\"\r\n    try:\r\n        assert osp.isdir(osp.join(repopath, '.hg'))\r\n        proc = programs.run_program('hg', ['id', '-nib', repopath])\r\n        output, _err = proc.communicate()\r\n        # output is now: ('eba7273c69df+ 2015+ default\\n', None)\r\n        # Split 2 times max to allow spaces in branch names.\r\n        return tuple(output.decode().strip().split(None, 2))\r\n    except (subprocess.CalledProcessError, AssertionError, AttributeError,\r\n            OSError):\r\n        return (None, None, None)", "output": "Return Mercurial revision for the repository located at repopath\r\n       Result is a tuple (global, local, branch), with None values on error\r\n       For example:\r\n           >>> get_hg_revision(\".\")\r\n           ('eba7273c69df+', '2015+', 'default')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download_and_uncompress(self, fileobj, dst_path):\n    \"\"\"\n    \"\"\"\n    try:\n      with tarfile.open(mode=\"r|*\", fileobj=fileobj) as tgz:\n        for tarinfo in tgz:\n          abs_target_path = _merge_relative_path(dst_path, tarinfo.name)\n\n          if tarinfo.isfile():\n            self._extract_file(tgz, tarinfo, abs_target_path)\n          elif tarinfo.isdir():\n            tf_v1.gfile.MakeDirs(abs_target_path)\n          else:\n            # We do not support symlinks and other uncommon objects.\n            raise ValueError(\n                \"Unexpected object type in tar archive: %s\" % tarinfo.type)\n\n        total_size_str = tf_utils.bytes_to_readable_str(\n            self._total_bytes_downloaded, True)\n        self._print_download_progress_msg(\n            \"Downloaded %s, Total size: %s\" % (self._url, total_size_str),\n            flush=True)\n    except tarfile.ReadError:\n      raise IOError(\"%s does not appear to be a valid module.\" % self._url)", "output": "Streams the content for the 'fileobj' and stores the result in dst_path.\n\n    Args:\n      fileobj: File handle pointing to .tar/.tar.gz content.\n      dst_path: Absolute path where to store uncompressed data from 'fileobj'.\n\n    Raises:\n      ValueError: Unknown object encountered inside the TAR file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gpu_memory_info(device_id=0):\n    \"\"\"\n\n    \"\"\"\n    free = ctypes.c_uint64()\n    total = ctypes.c_uint64()\n    dev_id = ctypes.c_int(device_id)\n    check_call(_LIB.MXGetGPUMemoryInformation64(dev_id, ctypes.byref(free), ctypes.byref(total)))\n    return (free.value, total.value)", "output": "Query CUDA for the free and total bytes of GPU global memory.\n\n    Parameters\n    ----------\n    device_id : int, optional\n        The device id of the GPU device.\n\n    Raises\n    ------\n    Will raise an exception on any CUDA error.\n\n    Returns\n    -------\n    (free, total) : (int, int)\n        The number of GPUs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def discount_rewards(r):\n    \"\"\"\"\"\"\n    discounted_r = np.zeros_like(r)\n    running_add = 0\n    for t in reversed(range(0, r.size)):\n        # Reset the sum, since this was a game boundary (pong specific!).\n        if r[t] != 0:\n            running_add = 0\n        running_add = running_add * gamma + r[t]\n        discounted_r[t] = running_add\n    return discounted_r", "output": "take 1D float array of rewards and compute discounted reward", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mtf_image_transformer_cifar_mp_4x():\n  \"\"\"\"\"\"\n  hparams = mtf_image_transformer_base_cifar()\n  hparams.mesh_shape = \"model:4;batch:8\"\n  hparams.layout = \"batch:batch;d_ff:model;heads:model\"\n  hparams.batch_size = 32\n  hparams.num_heads = 8\n  hparams.d_ff = 8192\n  return hparams", "output": "Data parallel CIFAR parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def status(self, targets, jobs=None, remote=None, show_checksums=False):\n        \"\"\"\n        \"\"\"\n        cloud = self._get_cloud(remote, \"status\")\n        return self.repo.cache.local.status(\n            targets, jobs=jobs, remote=cloud, show_checksums=show_checksums\n        )", "output": "Check status of data items in a cloud-agnostic way.\n\n        Args:\n            targets (list): list of targets to check status for.\n            jobs (int): number of jobs that can be running simultaneously.\n            remote (dvc.remote.base.RemoteBase): optional remote to compare\n                targets to. By default remote from core.remote config option\n                is used.\n            show_checksums (bool): show checksums instead of file names in\n                information messages.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def post_card(message,\n              hook_url=None,\n              title=None,\n              theme_color=None):\n    '''\n    \n    '''\n\n    if not hook_url:\n        hook_url = _get_hook_url()\n\n    if not message:\n        log.error('message is a required option.')\n\n    payload = {\n        \"text\": message,\n        \"title\": title,\n        \"themeColor\": theme_color\n    }\n\n    result = salt.utils.http.query(hook_url,\n                                   method='POST',\n                                   data=salt.utils.json.dumps(payload),\n                                   status=True)\n\n    if result['status'] <= 201:\n        return True\n    else:\n        return {\n            'res': False,\n            'message': result.get('body', result['status'])\n        }", "output": "Send a message to an MS Teams channel.\n    :param message:     The message to send to the MS Teams channel.\n    :param hook_url:    The Teams webhook URL, if not specified in the configuration.\n    :param title:       Optional title for the posted card\n    :param theme_color:  Optional hex color highlight for the posted card\n    :return:            Boolean if message was sent successfully.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' msteams.post_card message=\"Build is done\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_with_random_selector(x, func, num_cases):\n  \"\"\"\n  \"\"\"\n  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n  # Pass the real x only to one of the func calls.\n  return control_flow_ops.merge([\n      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n      for case in range(num_cases)\n  ])[0]", "output": "Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode(self, input_str):\n    \"\"\"\"\"\"\n    inputs = self.encoders[\"inputs\"].encode(input_str) + [EOS_ID]\n    batch_inputs = np.reshape(inputs, [1, -1, 1, 1])  # Make it 3D.\n    return batch_inputs", "output": "Input str to features dict, ready for inference.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_tags(filesystemid,\n                tags,\n                keyid=None,\n                key=None,\n                profile=None,\n                region=None,\n                **kwargs):\n    '''\n    \n    '''\n\n    client = _get_conn(key=key, keyid=keyid, profile=profile, region=region)\n\n    new_tags = []\n    for k, v in six.iteritems(tags):\n        new_tags.append({'Key': k, 'Value': v})\n\n    client.create_tags(FileSystemId=filesystemid, Tags=new_tags)", "output": "Creates or overwrites tags associated with a file system.\n    Each tag is a key-value pair. If a tag key specified in the request\n    already exists on the file system, this operation overwrites\n    its value with the value provided in the request.\n\n    filesystemid\n        (string) - ID of the file system for whose tags will be modified.\n\n    tags\n        (dict) - The tags to add to the file system\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' boto_efs.create_tags", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_table_data(self):\r\n        \"\"\"\"\"\"\r\n        data = self._simplify_shape(\r\n                self.table_widget.get_data())\r\n        if self.table_widget.array_btn.isChecked():\r\n            return array(data)\r\n        elif pd and self.table_widget.df_btn.isChecked():\r\n            info = self.table_widget.pd_info\r\n            buf = io.StringIO(self.table_widget.pd_text)\r\n            return pd.read_csv(buf, **info)\r\n        return data", "output": "Return clipboard processed as data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_delete(user_id=None, name=None, profile=None, **connection_args):\n    '''\n    \n    '''\n    kstone = auth(profile, **connection_args)\n    if name:\n        for user in kstone.users.list():\n            if user.name == name:\n                user_id = user.id\n                break\n    if not user_id:\n        return {'Error': 'Unable to resolve user id'}\n    kstone.users.delete(user_id)\n    ret = 'User ID {0} deleted'.format(user_id)\n    if name:\n\n        ret += ' ({0})'.format(name)\n    return ret", "output": "Delete a user (keystone user-delete)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.user_delete c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.user_delete user_id=c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.user_delete name=nova", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hook(self, m:nn.Module, i:Tensors, o:Tensors)->Tuple[Rank0Tensor,Rank0Tensor]:\n        \"\"\n        return o.mean().item(),o.std().item()", "output": "Take the mean and std of `o`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _interfaces_ipconfig(out):\n    '''\n    \n    '''\n    ifaces = dict()\n    iface = None\n    adapter_iface_regex = re.compile(r'adapter (\\S.+):$')\n\n    for line in out.splitlines():\n        if not line:\n            continue\n        # TODO what does Windows call Infiniband and 10/40gige adapters\n        if line.startswith('Ethernet'):\n            iface = ifaces[adapter_iface_regex.search(line).group(1)]\n            iface['up'] = True\n            addr = None\n            continue\n        if iface:\n            key, val = line.split(',', 1)\n            key = key.strip(' .')\n            val = val.strip()\n            if addr and key == 'Subnet Mask':\n                addr['netmask'] = val\n            elif key in ('IP Address', 'IPv4 Address'):\n                if 'inet' not in iface:\n                    iface['inet'] = list()\n                addr = {'address': val.rstrip('(Preferred)'),\n                        'netmask': None,\n                        'broadcast': None}  # TODO find the broadcast\n                iface['inet'].append(addr)\n            elif 'IPv6 Address' in key:\n                if 'inet6' not in iface:\n                    iface['inet'] = list()\n                # XXX What is the prefixlen!?\n                addr = {'address': val.rstrip('(Preferred)'),\n                        'prefixlen': None}\n                iface['inet6'].append(addr)\n            elif key == 'Physical Address':\n                iface['hwaddr'] = val\n            elif key == 'Media State':\n                # XXX seen used for tunnel adaptors\n                # might be useful\n                iface['up'] = (val != 'Media disconnected')", "output": "Returns a dictionary of interfaces with various information about each\n    (up/down state, ip address, netmask, and hwaddr)\n\n    NOTE: This is not used by any function and may be able to be removed in the\n    future.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_(saltenv='base', test=None):\n    '''\n    \n    '''\n    sevent = salt.utils.event.get_event(\n            'master',\n            __opts__['sock_dir'],\n            __opts__['transport'],\n            opts=__opts__,\n            listen=True)\n\n    master_key = salt.utils.master.get_master_key('root', __opts__)\n\n    __jid_event__.fire_event({'key': master_key}, 'salt/reactors/manage/list')\n\n    results = sevent.get_event(wait=30, tag='salt/reactors/manage/list-results')\n    reactors = results['reactors']\n    return reactors", "output": "List currently configured reactors\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run reactor.list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_instancenorm(net, node, model, builder):\n    \"\"\"\n    \"\"\"\n    import numpy as _np\n    input_name, output_name = _get_input_output_name(net, node)\n\n    name = node['name']\n    inputs = node['inputs']\n    outputs = node['outputs']\n\n\n    data_blob_name = _get_node_name(net, inputs[0][0])\n    gamma_blob_name = _get_node_name(net, inputs[1][0])\n    beta_blob_name = _get_node_name(net, inputs[2][0])\n    channels = _get_node_channels(net, inputs[0][0])\n\n    bn_output_name = output_name + '_bn_'\n\n    builder.add_batchnorm(\n        name = name + '_normalize',\n        channels = channels,\n        gamma = _np.ones((channels, )),\n        beta = _np.zeros((channels, )),\n        mean = None,\n        variance = None,\n        input_name = input_name,\n        output_name = bn_output_name,\n        compute_mean_var = True,\n        instance_normalization = True)\n\n    gamma_input_names = [bn_output_name, gamma_blob_name]\n    gamma_output_name = output_name + '_mult_gamma'\n    builder.add_elementwise(name=name+'_mult_gamma', input_names=gamma_input_names,\n        output_name = gamma_output_name, mode='MULTIPLY', alpha = None)\n    beta_input_names = [gamma_output_name, beta_blob_name]\n    builder.add_elementwise(name=name+'_add_beta', input_names=beta_input_names,\n        output_name = output_name, mode='ADD', alpha=None)", "output": "Convert an instance norm layer from mxnet to coreml.\n\n    Parameters\n    ----------\n    net: network\n        A mxnet network object.\n\n    node: layer\n        Node to convert.\n\n    model: model\n        An model for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def multi_split(s, split):\n    # type: (S, Iterable[S]) -> List[S]\n    \"\"\"\"\"\"\n    for r in split:\n        s = s.replace(r, \"|\")\n    return [i for i in s.split(\"|\") if len(i) > 0]", "output": "Splits on multiple given separators.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def AddRow(self, *args):\n        '''  '''\n        NumRows = len(self.Rows)  # number of existing rows is our row number\n        CurrentRowNumber = NumRows  # this row's number\n        CurrentRow = []  # start with a blank row and build up\n        # -------------------------  Add the elements to a row  ------------------------- #\n        for i, element in enumerate(args):  # Loop through list of elements and add them to the row\n            element.Position = (CurrentRowNumber, i)\n            element.ParentContainer = self\n            CurrentRow.append(element)\n        # -------------------------  Append the row to list of Rows  ------------------------- #\n        self.Rows.append(CurrentRow)", "output": "Parms are a variable number of Elements", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _copy_dist_from_dir(link_path, location):\n    \"\"\"\n\n    \"\"\"\n\n    # Note: This is currently VERY SLOW if you have a lot of data in the\n    # directory, because it copies everything with `shutil.copytree`.\n    # What it should really do is build an sdist and install that.\n    # See https://github.com/pypa/pip/issues/2195\n\n    if os.path.isdir(location):\n        rmtree(location)\n\n    # build an sdist\n    setup_py = 'setup.py'\n    sdist_args = [sys.executable]\n    sdist_args.append('-c')\n    sdist_args.append(SETUPTOOLS_SHIM % setup_py)\n    sdist_args.append('sdist')\n    sdist_args += ['--dist-dir', location]\n    logger.info('Running setup.py sdist for %s', link_path)\n\n    with indent_log():\n        call_subprocess(sdist_args, cwd=link_path, show_stdout=False)\n\n    # unpack sdist into `location`\n    sdist = os.path.join(location, os.listdir(location)[0])\n    logger.info('Unpacking sdist %s into %s', sdist, location)\n    unpack_file(sdist, location, content_type=None, link=None)", "output": "Copy distribution files in `link_path` to `location`.\n\n    Invoked when user requests to install a local directory. E.g.:\n\n        pip install .\n        pip install ~/dev/git-repos/python-prompt-toolkit", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_gl_class_type(obj_class):\n    \"\"\"\n    \n\n    \"\"\"\n\n    if obj_class == _SFrame:\n        return \"SFrame\"\n    elif obj_class == _SGraph:\n        return \"SGraph\"\n    elif obj_class == _SArray:\n        return \"SArray\"\n    elif _is_not_pickle_safe_gl_model_class(obj_class):\n        return \"Model\"\n    else:\n        return None", "output": "Internal util to get the type of the GLC class. The pickle file stores\n    this name so that it knows how to construct the object on unpickling.\n\n    Parameters\n    ----------\n    obj_class    : Class which has to be categorized.\n\n    Returns\n    ----------\n    A class type for the pickle file to save.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ensure_running(name, no_start=False, path=None):\n    '''\n    \n    '''\n    _ensure_exists(name, path=path)\n    pre = state(name, path=path)\n    if pre == 'running':\n        # This will be a no-op but running the function will give us a pretty\n        # return dict.\n        return start(name, path=path)\n    elif pre == 'stopped':\n        if no_start:\n            raise CommandExecutionError(\n                'Container \\'{0}\\' is not running'.format(name)\n            )\n        return start(name, path=path)\n    elif pre == 'frozen':\n        if no_start:\n            raise CommandExecutionError(\n                'Container \\'{0}\\' is not running'.format(name)\n            )\n        return unfreeze(name, path=path)", "output": "If the container is not currently running, start it. This function returns\n    the state that the container was in before changing\n\n    path\n        path to the container parent directory\n        default: /var/lib/lxc (system)\n\n        .. versionadded:: 2015.8.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def addCondition(self, *fns, **kwargs):\n        \"\"\"\n        \"\"\"\n        msg = kwargs.get(\"message\", \"failed user-defined condition\")\n        exc_type = ParseFatalException if kwargs.get(\"fatal\", False) else ParseException\n        for fn in fns:\n            fn = _trim_arity(fn)\n            def pa(s,l,t):\n                if not bool(fn(s,l,t)):\n                    raise exc_type(s,l,msg)\n            self.parseAction.append(pa)\n        self.callDuringTry = self.callDuringTry or kwargs.get(\"callDuringTry\", False)\n        return self", "output": "Add a boolean predicate function to expression's list of parse actions. See\n        :class:`setParseAction` for function call signatures. Unlike ``setParseAction``,\n        functions passed to ``addCondition`` need to return boolean success/fail of the condition.\n\n        Optional keyword arguments:\n        - message = define a custom message to be used in the raised exception\n        - fatal   = if True, will raise ParseFatalException to stop parsing immediately; otherwise will raise ParseException\n\n        Example::\n\n            integer = Word(nums).setParseAction(lambda toks: int(toks[0]))\n            year_int = integer.copy()\n            year_int.addCondition(lambda toks: toks[0] >= 2000, message=\"Only support years 2000 and later\")\n            date_str = year_int + '/' + integer + '/' + integer\n\n            result = date_str.parseString(\"1999/12/31\")  # -> Exception: Only support years 2000 and later (at char 0), (line:1, col:1)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_group(path, follow_symlinks=True):\n    '''\n    \n    '''\n    func_name = '{0}.get_group'.format(__virtualname__)\n    if __opts__.get('fun', '') == func_name:\n        log.info('The function %s should not be used on Windows systems; '\n                 'see function docs for details. The value returned is the '\n                 'user (owner).', func_name)\n\n    return get_user(path, follow_symlinks)", "output": "Return the group that owns a given file\n\n    Under Windows, this will return the user (owner) of the file.\n\n    While a file in Windows does have a 'primary group', this rarely used\n    attribute generally has no bearing on permissions unless intentionally\n    configured and is only used to support Unix compatibility features (e.g.\n    Services For Unix, NFS services).\n\n    Salt, therefore, remaps this function to provide functionality that\n    somewhat resembles Unix behavior for API compatibility reasons. When\n    managing Windows systems, this function is superfluous and will generate\n    an info level log entry if used directly.\n\n    If you do actually want to access the 'primary group' of a file, use\n    `file.get_pgroup`.\n\n    Args:\n        path (str): The path to the file or directory\n\n        follow_symlinks (bool):\n            If the object specified by ``path`` is a symlink, get attributes of\n            the linked file instead of the symlink itself. Default is True\n\n    Returns:\n        str: The name of the owner\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.get_group c:\\\\temp\\\\test.txt", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare_global_logging(serialization_dir: str, file_friendly_logging: bool) -> logging.FileHandler:\n    \"\"\"\n    \n    \"\"\"\n\n    # If we don't have a terminal as stdout,\n    # force tqdm to be nicer.\n    if not sys.stdout.isatty():\n        file_friendly_logging = True\n\n    Tqdm.set_slower_interval(file_friendly_logging)\n    std_out_file = os.path.join(serialization_dir, \"stdout.log\")\n    sys.stdout = TeeLogger(std_out_file, # type: ignore\n                           sys.stdout,\n                           file_friendly_logging)\n    sys.stderr = TeeLogger(os.path.join(serialization_dir, \"stderr.log\"), # type: ignore\n                           sys.stderr,\n                           file_friendly_logging)\n\n    stdout_handler = logging.FileHandler(std_out_file)\n    stdout_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s'))\n    logging.getLogger().addHandler(stdout_handler)\n\n    return stdout_handler", "output": "This function configures 3 global logging attributes - streaming stdout and stderr\n    to a file as well as the terminal, setting the formatting for the python logging\n    library and setting the interval frequency for the Tqdm progress bar.\n\n    Note that this function does not set the logging level, which is set in ``allennlp/run.py``.\n\n    Parameters\n    ----------\n    serialization_dir : ``str``, required.\n        The directory to stream logs to.\n    file_friendly_logging : ``bool``, required.\n        Whether logs should clean the output to prevent carriage returns\n        (used to update progress bars on a single terminal line). This\n        option is typically only used if you are running in an environment\n        without a terminal.\n\n    Returns\n    -------\n    ``logging.FileHandler``\n        A logging file handler that can later be closed and removed from the global logger.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def returner(ret):\n    '''\n    \n    '''\n    write_profile = __opts__.get('etcd.returner_write_profile')\n    if write_profile:\n        ttl = __opts__.get(write_profile, {}).get('etcd.ttl')\n    else:\n        ttl = __opts__.get('etcd.ttl')\n\n    client, path = _get_conn(__opts__, write_profile)\n    # Make a note of this minion for the external job cache\n    client.set(\n        '/'.join((path, 'minions', ret['id'])),\n        ret['jid'],\n        ttl=ttl,\n    )\n\n    for field in ret:\n        # Not using os.path.join because we're not dealing with file paths\n        dest = '/'.join((\n            path,\n            'jobs',\n            ret['jid'],\n            ret['id'],\n            field\n        ))\n        client.set(dest, salt.utils.json.dumps(ret[field]), ttl=ttl)", "output": "Return data to an etcd server or cluster", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dorefa(bitW, bitA, bitG):\n    \"\"\"\n    \n    \"\"\"\n    def quantize(x, k):\n        n = float(2 ** k - 1)\n\n        @tf.custom_gradient\n        def _quantize(x):\n            return tf.round(x * n) / n, lambda dy: dy\n\n        return _quantize(x)\n\n    def fw(x):\n        if bitW == 32:\n            return x\n\n        if bitW == 1:   # BWN\n            E = tf.stop_gradient(tf.reduce_mean(tf.abs(x)))\n\n            @tf.custom_gradient\n            def _sign(x):\n                return tf.where(tf.equal(x, 0), tf.ones_like(x), tf.sign(x / E)) * E, lambda dy: dy\n\n            return _sign(x)\n\n        x = tf.tanh(x)\n        x = x / tf.reduce_max(tf.abs(x)) * 0.5 + 0.5\n        return 2 * quantize(x, bitW) - 1\n\n    def fa(x):\n        if bitA == 32:\n            return x\n        return quantize(x, bitA)\n\n    def fg(x):\n        if bitG == 32:\n            return x\n\n        @tf.custom_gradient\n        def _identity(input):\n            def grad_fg(x):\n                rank = x.get_shape().ndims\n                assert rank is not None\n                maxx = tf.reduce_max(tf.abs(x), list(range(1, rank)), keep_dims=True)\n                x = x / maxx\n                n = float(2**bitG - 1)\n                x = x * 0.5 + 0.5 + tf.random_uniform(\n                    tf.shape(x), minval=-0.5 / n, maxval=0.5 / n)\n                x = tf.clip_by_value(x, 0.0, 1.0)\n                x = quantize(x, bitG) - 0.5\n                return x * maxx * 2\n\n            return input, grad_fg\n\n        return _identity(x)\n    return fw, fa, fg", "output": "Return the three quantization functions fw, fa, fg, for weights, activations and gradients respectively", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chmod(self, mode):\n        \"\"\"\n        \n        \"\"\"\n        if self._closed:\n            self._raise_closed()\n        self._accessor.chmod(self, mode)", "output": "Change the permissions of the path, like os.chmod().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mouseReleaseEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        self.QT_CLASS.mouseReleaseEvent(self, event)\r\n        text = self.get_line_at(event.pos())\r\n        if get_error_match(text) and not self.has_selected_text():\r\n            if self.go_to_error is not None:\r\n                self.go_to_error.emit(text)", "output": "Go to error", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(\n    ctx,\n    state,\n    bare=False,\n    dry_run=None,\n    outdated=False,\n    **kwargs\n):\n    \"\"\"\"\"\"\n    from ..core import (\n        ensure_project,\n        do_outdated,\n        do_lock,\n        do_sync,\n        project,\n    )\n\n    ensure_project(three=state.three, python=state.python, warn=True, pypi_mirror=state.pypi_mirror)\n    if not outdated:\n        outdated = bool(dry_run)\n    if outdated:\n        do_outdated(pypi_mirror=state.pypi_mirror)\n    packages = [p for p in state.installstate.packages if p]\n    editable = [p for p in state.installstate.editables if p]\n    if not packages:\n        echo(\n            \"{0} {1} {2} {3}{4}\".format(\n                crayons.white(\"Running\", bold=True),\n                crayons.red(\"$ pipenv lock\", bold=True),\n                crayons.white(\"then\", bold=True),\n                crayons.red(\"$ pipenv sync\", bold=True),\n                crayons.white(\".\", bold=True),\n            )\n        )\n    else:\n        for package in packages + editable:\n            if package not in project.all_packages:\n                echo(\n                    \"{0}: {1} was not found in your Pipfile! Aborting.\"\n                    \"\".format(\n                        crayons.red(\"Warning\", bold=True),\n                        crayons.green(package, bold=True),\n                    ),\n                    err=True,\n                )\n                ctx.abort()\n\n    do_lock(\n        clear=state.clear,\n        pre=state.installstate.pre,\n        keep_outdated=state.installstate.keep_outdated,\n        pypi_mirror=state.pypi_mirror,\n    )\n    do_sync(\n        ctx=ctx,\n        dev=state.installstate.dev,\n        three=state.three,\n        python=state.python,\n        bare=bare,\n        dont_upgrade=not state.installstate.keep_outdated,\n        user=False,\n        clear=state.clear,\n        unused=False,\n        sequential=state.installstate.sequential,\n        pypi_mirror=state.pypi_mirror,\n    )", "output": "Runs lock, then sync.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_lars_path(weighted_data, weighted_labels):\n        \"\"\"\n        \"\"\"\n        x_vector = weighted_data\n        alphas, _, coefs = lars_path(x_vector,\n                                     weighted_labels,\n                                     method='lasso',\n                                     verbose=False)\n        return alphas, coefs", "output": "Generates the lars path for weighted data.\n\n        Args:\n            weighted_data: data that has been weighted by kernel\n            weighted_label: labels, weighted by kernel\n\n        Returns:\n            (alphas, coefs), both are arrays corresponding to the\n            regularization parameter and coefficients, respectively", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dummy_dataloader(dataloader, target_shape):\n    \"\"\"\"\"\"\n    data_iter = enumerate(dataloader)\n    _, data_batch = next(data_iter)\n    logging.debug('Searching target batch shape: %s', target_shape)\n    while data_batch[0].shape != target_shape:\n        logging.debug('Skip batch with shape %s', data_batch[0].shape)\n        _, data_batch = next(data_iter)\n    logging.debug('Found target dummy batch.')\n\n    class DummyIter():\n        def __init__(self, batch):\n            self._batch = batch\n\n        def __iter__(self):\n            while True:\n                yield self._batch\n\n    return DummyIter(data_batch)", "output": "Return a dummy data loader which returns a fixed data batch of target shape", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_model_snapshots(self, job_id, snapshot_id=None, body=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if job_id in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n        return self.transport.perform_request(\n            \"GET\",\n            _make_path(\n                \"_ml\", \"anomaly_detectors\", job_id, \"model_snapshots\", snapshot_id\n            ),\n            params=params,\n            body=body,\n        )", "output": "`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-snapshot.html>`_\n\n        :arg job_id: The ID of the job to fetch\n        :arg snapshot_id: The ID of the snapshot to fetch\n        :arg body: Model snapshot selection criteria\n        :arg desc: True if the results should be sorted in descending order\n        :arg end: The filter 'end' query parameter\n        :arg from_: Skips a number of documents\n        :arg size: The default number of documents returned in queries as a\n            string.\n        :arg sort: Name of the field to sort on\n        :arg start: The filter 'start' query parameter", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_header_accept(self, accepts):\n        \"\"\"\n        \n        \"\"\"\n        if not accepts:\n            return\n\n        accepts = [x.lower() for x in accepts]\n\n        if 'application/json' in accepts:\n            return 'application/json'\n        else:\n            return ', '.join(accepts)", "output": "Returns `Accept` based on an array of accepts provided.\n\n        :param accepts: List of headers.\n        :return: Accept (e.g. application/json).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def train(cls, rdd, k=10, maxIterations=20, docConcentration=-1.0,\n              topicConcentration=-1.0, seed=None, checkpointInterval=10, optimizer=\"em\"):\n        \"\"\"\n        \"\"\"\n        model = callMLlibFunc(\"trainLDAModel\", rdd, k, maxIterations,\n                              docConcentration, topicConcentration, seed,\n                              checkpointInterval, optimizer)\n        return LDAModel(model)", "output": "Train a LDA model.\n\n        :param rdd:\n          RDD of documents, which are tuples of document IDs and term\n          (word) count vectors. The term count vectors are \"bags of\n          words\" with a fixed-size vocabulary (where the vocabulary size\n          is the length of the vector). Document IDs must be unique\n          and >= 0.\n        :param k:\n          Number of topics to infer, i.e., the number of soft cluster\n          centers.\n          (default: 10)\n        :param maxIterations:\n          Maximum number of iterations allowed.\n          (default: 20)\n        :param docConcentration:\n          Concentration parameter (commonly named \"alpha\") for the prior\n          placed on documents' distributions over topics (\"theta\").\n          (default: -1.0)\n        :param topicConcentration:\n          Concentration parameter (commonly named \"beta\" or \"eta\") for\n          the prior placed on topics' distributions over terms.\n          (default: -1.0)\n        :param seed:\n          Random seed for cluster initialization. Set as None to generate\n          seed based on system time.\n          (default: None)\n        :param checkpointInterval:\n          Period (in iterations) between checkpoints.\n          (default: 10)\n        :param optimizer:\n          LDAOptimizer used to perform the actual calculation. Currently\n          \"em\", \"online\" are supported.\n          (default: \"em\")", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_parent_scope(block):\n        \"\"\"\"\"\"\n        original = block\n        if not TextBlockHelper.is_fold_trigger(block):\n            # search level of next non blank line\n            while block.text().strip() == '' and block.isValid():\n                block = block.next()\n            ref_lvl = TextBlockHelper.get_fold_lvl(block) - 1\n            block = original\n            while (block.blockNumber() and\n                   (not TextBlockHelper.is_fold_trigger(block) or\n                    TextBlockHelper.get_fold_lvl(block) > ref_lvl)):\n                block = block.previous()\n        return block", "output": "Find parent scope, if the block is not a fold trigger.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_keyid(keyname):\n    '''\n    \n    '''\n    if not keyname:\n        return None\n    keypairs = list_keypairs(call='function')\n    keyid = keypairs[keyname]['id']\n    if keyid:\n        return keyid\n    raise SaltCloudNotFound('The specified ssh key could not be found.')", "output": "Return the ID of the keyname", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cached_path(url_or_filename: Union[str, Path], cache_dir: str = None) -> str:\n    \"\"\"\n    \n    \"\"\"\n    if cache_dir is None:\n        cache_dir = CACHE_DIRECTORY\n    if isinstance(url_or_filename, Path):\n        url_or_filename = str(url_or_filename)\n\n    url_or_filename = os.path.expanduser(url_or_filename)\n    parsed = urlparse(url_or_filename)\n\n    if parsed.scheme in ('http', 'https', 's3'):\n        # URL, so get it from the cache (downloading if necessary)\n        return get_from_cache(url_or_filename, cache_dir)\n    elif os.path.exists(url_or_filename):\n        # File, and it exists.\n        return url_or_filename\n    elif parsed.scheme == '':\n        # File, but it doesn't exist.\n        raise FileNotFoundError(\"file {} not found\".format(url_or_filename))\n    else:\n        # Something unknown\n        raise ValueError(\"unable to parse {} as a URL or as a local path\".format(url_or_filename))", "output": "Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_node(graph, text):\n    \"\"\"\"\"\"\n    match = _NODEPAT.match(text)\n    if match is not None:\n        node = match.group(1)\n        graph.node(node, label=match.group(2), shape='circle')\n        return node\n    match = _LEAFPAT.match(text)\n    if match is not None:\n        node = match.group(1)\n        graph.node(node, label=match.group(2), shape='box')\n        return node\n    raise ValueError('Unable to parse node: {0}'.format(text))", "output": "parse dumped node", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_pkgs(versions_as_list=False, **kwargs):\n    '''\n    \n    '''\n    versions_as_list = salt.utils.data.is_true(versions_as_list)\n    # 'removed', 'purge_desired' not yet implemented or not applicable\n    if any([salt.utils.data.is_true(kwargs.get(x))\n            for x in ('removed', 'purge_desired')]):\n        return {}\n\n    if 'pkg.list_pkgs' in __context__:\n        if versions_as_list:\n            return __context__['pkg.list_pkgs']\n        else:\n            ret = copy.deepcopy(__context__['pkg.list_pkgs'])\n            __salt__['pkg_resource.stringify'](ret)\n            return ret\n\n    ret = {}\n    cmd = ['port', 'installed']\n    out = salt.utils.mac_utils.execute_return_result(cmd)\n    for line in out.splitlines():\n        try:\n            name, version_num, active = re.split(r'\\s+', line.lstrip())[0:3]\n            version_num = version_num[1:]\n        except ValueError:\n            continue\n        if not LIST_ACTIVE_ONLY or active == '(active)':\n            __salt__['pkg_resource.add_pkg'](ret, name, version_num)\n\n    __salt__['pkg_resource.sort_pkglist'](ret)\n    __context__['pkg.list_pkgs'] = copy.deepcopy(ret)\n    if not versions_as_list:\n        __salt__['pkg_resource.stringify'](ret)\n    return ret", "output": "List the packages currently installed in a dict::\n\n        {'<package_name>': '<version>'}\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.list_pkgs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def empty(self):\n        \"\"\"\n        \n        \"\"\"\n        self._lock.acquire()\n        try:\n            out = self._buffer_tobytes()\n            del self._buffer[:]\n            if (self._event is not None) and not self._closed:\n                self._event.clear()\n            return out\n        finally:\n            self._lock.release()", "output": "Clear out the buffer and return all data that was in it.\n\n        :return:\n            any data that was in the buffer prior to clearing it out, as a\n            `str`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_save_containers(platforms, registry, load_cache) -> int:\n    \"\"\"\n    \n    \"\"\"\n    from joblib import Parallel, delayed\n    if len(platforms) == 0:\n        return 0\n\n    platform_results = Parallel(n_jobs=PARALLEL_BUILDS, backend=\"multiprocessing\")(\n        delayed(_build_save_container)(platform, registry, load_cache)\n        for platform in platforms)\n\n    is_error = False\n    for platform_result in platform_results:\n        if platform_result is not None:\n            logging.error('Failed to generate %s', platform_result)\n            is_error = True\n\n    return 1 if is_error else 0", "output": "Entry point to build and upload all built dockerimages in parallel\n    :param platforms: List of platforms\n    :param registry: Docker registry name\n    :param load_cache: Load cache before building\n    :return: 1 if error occurred, 0 otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_tf_checkpoint(path):\n    \"\"\"\"\"\"\n    from tensorflow.python import pywrap_tensorflow\n    tensors = {}\n    reader = pywrap_tensorflow.NewCheckpointReader(path)\n    var_to_shape_map = reader.get_variable_to_shape_map()\n    for key in sorted(var_to_shape_map):\n        tensor = reader.get_tensor(key)\n        tensors[key] = tensor\n    return tensors", "output": "read tensorflow checkpoint", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def utime(self, path, times):\n        \"\"\"\n        \n        \"\"\"\n        path = self._adjust_cwd(path)\n        if times is None:\n            times = (time.time(), time.time())\n        self._log(DEBUG, \"utime({!r}, {!r})\".format(path, times))\n        attr = SFTPAttributes()\n        attr.st_atime, attr.st_mtime = times\n        self._request(CMD_SETSTAT, path, attr)", "output": "Set the access and modified times of the file specified by ``path``.\n        If ``times`` is ``None``, then the file's access and modified times\n        are set to the current time.  Otherwise, ``times`` must be a 2-tuple\n        of numbers, of the form ``(atime, mtime)``, which is used to set the\n        access and modified times, respectively.  This bizarre API is mimicked\n        from Python for the sake of consistency -- I apologize.\n\n        :param str path: path of the file to modify\n        :param tuple times:\n            ``None`` or a tuple of (access time, modified time) in standard\n            internet epoch time (seconds since 01 January 1970 GMT)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_user(name, **client_args):\n    '''\n    \n    '''\n    if not user_exists(name, **client_args):\n        log.info('User \\'%s\\' does not exist', name)\n        return False\n\n    client = _client(**client_args)\n    client.drop_user(name)\n\n    return True", "output": "Remove a user.\n\n    name\n        Name of the user to remove\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb.remove_user <name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def daemonize(redirect_out=True):\n    '''\n    \n    '''\n    # Avoid circular import\n    import salt.utils.crypt\n    try:\n        pid = os.fork()\n        if pid > 0:\n            # exit first parent\n            salt.utils.crypt.reinit_crypto()\n            os._exit(salt.defaults.exitcodes.EX_OK)\n    except OSError as exc:\n        log.error('fork #1 failed: %s (%s)', exc.errno, exc)\n        sys.exit(salt.defaults.exitcodes.EX_GENERIC)\n\n    # decouple from parent environment\n    os.chdir('/')\n    # noinspection PyArgumentList\n    os.setsid()\n    os.umask(0o022)  # pylint: disable=blacklisted-function\n\n    # do second fork\n    try:\n        pid = os.fork()\n        if pid > 0:\n            salt.utils.crypt.reinit_crypto()\n            sys.exit(salt.defaults.exitcodes.EX_OK)\n    except OSError as exc:\n        log.error('fork #2 failed: %s (%s)', exc.errno, exc)\n        sys.exit(salt.defaults.exitcodes.EX_GENERIC)\n\n    salt.utils.crypt.reinit_crypto()\n\n    # A normal daemonization redirects the process output to /dev/null.\n    # Unfortunately when a python multiprocess is called the output is\n    # not cleanly redirected and the parent process dies when the\n    # multiprocessing process attempts to access stdout or err.\n    if redirect_out:\n        with salt.utils.files.fopen('/dev/null', 'r+') as dev_null:\n            # Redirect python stdin/out/err\n            # and the os stdin/out/err which can be different\n            os.dup2(dev_null.fileno(), sys.stdin.fileno())\n            os.dup2(dev_null.fileno(), sys.stdout.fileno())\n            os.dup2(dev_null.fileno(), sys.stderr.fileno())\n            os.dup2(dev_null.fileno(), 0)\n            os.dup2(dev_null.fileno(), 1)\n            os.dup2(dev_null.fileno(), 2)", "output": "Daemonize a process", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reshape(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    if len(inputs) == 1:\n        return 'reshape', attrs, inputs[0]\n    reshape_shape = list(proto_obj._params[inputs[1].name].asnumpy())\n    reshape_shape = [int(i) for i in reshape_shape]\n    new_attrs = {'shape': reshape_shape}\n    return 'reshape', new_attrs, inputs[:1]", "output": "Reshape the given array by the shape attribute.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def assign(name, value):\n    '''\n    \n    '''\n    ret = {}\n    cmd = 'sysctl {0}=\"{1}\"'.format(name, value)\n    data = __salt__['cmd.run_all'](cmd)\n\n    # Certain values cannot be set from this console, at the current\n    # securelevel or there are other restrictions that prevent us\n    # from applying the setting rightaway.\n    if re.match(r'^sysctl:.*: Operation not permitted$', data['stderr']) or \\\n      data['retcode'] != 0:\n        raise CommandExecutionError('sysctl failed: {0}'.format(\n            data['stderr']))\n    new_name, new_value = data['stdout'].split(':', 1)\n    ret[new_name] = new_value.split(' -> ')[-1]\n    return ret", "output": "Assign a single sysctl parameter for this minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sysctl.assign net.inet.ip.forwarding 1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_yielded(yielded: _Yieldable) -> Future:\n    \"\"\"\n\n    \"\"\"\n    if yielded is None or yielded is moment:\n        return moment\n    elif yielded is _null_future:\n        return _null_future\n    elif isinstance(yielded, (list, dict)):\n        return multi(yielded)  # type: ignore\n    elif is_future(yielded):\n        return typing.cast(Future, yielded)\n    elif isawaitable(yielded):\n        return _wrap_awaitable(yielded)  # type: ignore\n    else:\n        raise BadYieldError(\"yielded unknown object %r\" % (yielded,))", "output": "Convert a yielded object into a `.Future`.\n\n    The default implementation accepts lists, dictionaries, and\n    Futures. This has the side effect of starting any coroutines that\n    did not start themselves, similar to `asyncio.ensure_future`.\n\n    If the `~functools.singledispatch` library is available, this function\n    may be extended to support additional types. For example::\n\n        @convert_yielded.register(asyncio.Future)\n        def _(asyncio_future):\n            return tornado.platform.asyncio.to_tornado_future(asyncio_future)\n\n    .. versionadded:: 4.1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _canonize_content_input(self, dataset, single_style):\n        \"\"\"\n        \n        \"\"\"\n        unpack = lambda x: x\n        if isinstance(dataset, _tc.SArray):\n            dataset = _tc.SFrame({self.content_feature: dataset})\n            if single_style:\n                unpack = lambda sf: sf['stylized_' + self.content_feature]\n        elif isinstance(dataset, _tc.Image):\n            dataset = _tc.SFrame({self.content_feature: [dataset]})\n            if single_style:\n                unpack = lambda sf: sf['stylized_' + self.content_feature][0]\n        return dataset, unpack", "output": "Takes input and returns tuple of the input in canonical form (SFrame)\n        along with an unpack callback function that can be applied to\n        prediction results to \"undo\" the canonization.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def blink_figure(self):\n        \"\"\"\"\"\"\n        if self.fig:\n            self._blink_flag = not self._blink_flag\n            self.repaint()\n            if self._blink_flag:\n                timer = QTimer()\n                timer.singleShot(40, self.blink_figure)", "output": "Blink figure once.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def metric(self, name, filter_=None, description=\"\"):\n        \"\"\"\n        \"\"\"\n        return Metric(name, filter_, client=self, description=description)", "output": "Creates a metric bound to the current client.\n\n        :type name: str\n        :param name: the name of the metric to be constructed.\n\n        :type filter_: str\n        :param filter_: the advanced logs filter expression defining the\n                        entries tracked by the metric.  If not\n                        passed, the instance should already exist, to be\n                        refreshed via :meth:`Metric.reload`.\n\n        :type description: str\n        :param description: the description of the metric to be constructed.\n                            If not passed, the instance should already exist,\n                            to be refreshed via :meth:`Metric.reload`.\n\n        :rtype: :class:`google.cloud.logging.metric.Metric`\n        :returns: Metric created with the current client.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_colors(use=True, theme=None):\n    '''\n    \n\n    '''\n\n    colors = {\n        'BLACK': TextFormat('black'),\n        'DARK_GRAY': TextFormat('bold', 'black'),\n        'RED': TextFormat('red'),\n        'LIGHT_RED': TextFormat('bold', 'red'),\n        'GREEN': TextFormat('green'),\n        'LIGHT_GREEN': TextFormat('bold', 'green'),\n        'YELLOW': TextFormat('yellow'),\n        'LIGHT_YELLOW': TextFormat('bold', 'yellow'),\n        'BLUE': TextFormat('blue'),\n        'LIGHT_BLUE': TextFormat('bold', 'blue'),\n        'MAGENTA': TextFormat('magenta'),\n        'LIGHT_MAGENTA': TextFormat('bold', 'magenta'),\n        'CYAN': TextFormat('cyan'),\n        'LIGHT_CYAN': TextFormat('bold', 'cyan'),\n        'LIGHT_GRAY': TextFormat('white'),\n        'WHITE': TextFormat('bold', 'white'),\n        'DEFAULT_COLOR': TextFormat('default'),\n        'ENDC': TextFormat('reset'),\n    }\n    if theme:\n        colors.update(get_color_theme(theme))\n\n    if not use:\n        for color in colors:\n            colors[color] = ''\n    if isinstance(use, six.string_types):\n        # Try to set all of the colors to the passed color\n        if use in colors:\n            for color in colors:\n                # except for color reset\n                if color == 'ENDC':\n                    continue\n                colors[color] = colors[use]\n\n    return colors", "output": "Return the colors as an easy to use dict. Pass `False` to deactivate all\n    colors by setting them to empty strings. Pass a string containing only the\n    name of a single color to be used in place of all colors. Examples:\n\n    .. code-block:: python\n\n        colors = get_colors()  # enable all colors\n        no_colors = get_colors(False)  # disable all colors\n        red_colors = get_colors('RED')  # set all colors to red", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def profile_device_get(name, device_name, remote_addr=None,\n                       cert=None, key=None, verify_cert=True):\n    ''' \n    '''\n    profile = profile_get(\n        name,\n        remote_addr,\n        cert,\n        key,\n        verify_cert,\n        _raw=True\n    )\n\n    return _get_property_dict_item(profile, 'devices', device_name)", "output": "Get a profile device.\n\n        name :\n            The name of the profile to get the device from.\n\n        device_name :\n            The name of the device to retrieve.\n\n        remote_addr :\n            An URL to a remote Server, you also have to give cert and key if\n            you provide remote_addr and its a TCP Address!\n\n            Examples:\n                https://myserver.lan:8443\n                /var/lib/mysocket.sock\n\n        cert :\n            PEM Formatted SSL Certificate.\n\n            Examples:\n                ~/.config/lxc/client.crt\n\n        key :\n            PEM Formatted SSL Key.\n\n            Examples:\n                ~/.config/lxc/client.key\n\n        verify_cert : True\n            Wherever to verify the cert, this is by default True\n            but in the most cases you want to set it off as LXD\n            normaly uses self-signed certificates.\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            $ salt '*' lxd.profile_device_get default eth0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_numeric_feature_names(example):\n  \"\"\"\n  \"\"\"\n  numeric_features = ('float_list', 'int64_list')\n  features = get_example_features(example)\n  return sorted([\n      feature_name for feature_name in features\n      if features[feature_name].WhichOneof('kind') in numeric_features\n  ])", "output": "Returns a list of feature names for float and int64 type features.\n\n  Args:\n    example: An example.\n\n  Returns:\n    A list of strings of the names of numeric features.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(self):\n        \"\"\"\n        \"\"\"\n        client = self._instance._client\n        client.instance_admin_client.delete_cluster(self.name)", "output": "Delete this cluster.\n\n        For example:\n\n        .. literalinclude:: snippets.py\n            :start-after: [START bigtable_delete_cluster]\n            :end-before: [END bigtable_delete_cluster]\n\n        Marks a cluster and all of its tables for permanent deletion in 7 days.\n\n        Immediately upon completion of the request:\n\n        * Billing will cease for all of the cluster's reserved resources.\n        * The cluster's ``delete_time`` field will be set 7 days in the future.\n\n        Soon afterward:\n\n        * All tables within the cluster will become unavailable.\n\n        At the cluster's ``delete_time``:\n\n        * The cluster and **all of its tables** will immediately and\n          irrevocably disappear from the API, and their data will be\n          permanently deleted.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_batch_begin(self, train, **kwargs):\n        \"\"\n        if not self.learn.gan_trainer.gen_mode and train: self.learn.opt.lr *= self.mult_lr", "output": "Multiply the current lr if necessary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def undeploy(self, no_confirm=False, remove_logs=False):\n        \"\"\"\n        \n        \"\"\"\n\n        if not no_confirm: # pragma: no cover\n            confirm = input(\"Are you sure you want to undeploy? [y/n] \")\n            if confirm != 'y':\n                return\n\n        if self.use_alb:\n            self.zappa.undeploy_lambda_alb(self.lambda_name)\n\n        if self.use_apigateway:\n            if remove_logs:\n                self.zappa.remove_api_gateway_logs(self.lambda_name)\n\n            domain_name = self.stage_config.get('domain', None)\n            base_path = self.stage_config.get('base_path', None)\n\n            # Only remove the api key when not specified\n            if self.api_key_required and self.api_key is None:\n                api_id = self.zappa.get_api_id(self.lambda_name)\n                self.zappa.remove_api_key(api_id, self.api_stage)\n\n            gateway_id = self.zappa.undeploy_api_gateway(\n                self.lambda_name,\n                domain_name=domain_name,\n                base_path=base_path\n            )\n\n        self.unschedule()  # removes event triggers, including warm up event.\n\n        self.zappa.delete_lambda_function(self.lambda_name)\n        if remove_logs:\n            self.zappa.remove_lambda_function_logs(self.lambda_name)\n\n        click.echo(click.style(\"Done\", fg=\"green\", bold=True) + \"!\")", "output": "Tear down an existing deployment.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def train_eval_and_decode(self):\n    \"\"\"\"\"\"\n    eval_steps = self._hparams.eval_freq_in_steps\n    packed_dataset = \"_packed\" in self._hparams.problem.name\n    mlperf_log.transformer_print(key=mlperf_log.TRAIN_LOOP)\n    for i in range(0, self._train_spec.max_steps, eval_steps):\n      mlperf_log.transformer_print(\n          key=mlperf_log.TRAIN_EPOCH, value=i // eval_steps)\n      if packed_dataset and i > 0:\n        problem = registry.problem(self._hparams.problem.name + \"_packed\")\n        p_hparams = problem.get_hparams(self._hparams)\n        self._hparams.problem = problem\n        self._hparams.problem_hparams = p_hparams\n      self._estimator.train(\n          self._train_spec.input_fn,\n          steps=eval_steps,\n          hooks=self._train_spec.hooks)\n      self._set_eval_dir_name(\"eval\")\n      self._estimator.evaluate(\n          self._eval_spec.input_fn,\n          steps=self._eval_spec.steps,\n          hooks=self._eval_spec.hooks,\n          name=\"eval\")\n      if packed_dataset:\n        problem = registry.problem(\n            self._hparams.problem.name.replace(\"_packed\", \"\"))\n        p_hparams = problem.get_hparams(self._hparams)\n        self._hparams.problem = problem\n        self._hparams.problem_hparams = p_hparams\n      mlperf_log.transformer_print(key=mlperf_log.EVAL_START)\n      if self._hparams.mlperf_mode:\n        self._decode_hparams.mlperf_decode_step = i + eval_steps\n      self.decode(dataset_split=tf.estimator.ModeKeys.EVAL)\n      d_hparams = self._decode_hparams\n      if self._hparams.mlperf_mode and d_hparams.mlperf_success:\n        mlperf_log.transformer_print(\n            key=mlperf_log.RUN_STOP, value={\"success\": \"true\"})\n        break\n\n    d_hparams = self._decode_hparams\n    if self._hparams.mlperf_mode and not d_hparams.mlperf_success:\n      mlperf_log.transformer_print(\n          key=mlperf_log.RUN_STOP, value={\"success\": \"false\"})", "output": "Does eval and decode after training every eval_freq_in_steps.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def envs():\n    '''\n    \n    '''\n    saltenvs = []\n    for container in __opts__.get('azurefs', []):\n        saltenvs.append(container.get('saltenv', 'base'))\n    # Remove duplicates\n    return list(set(saltenvs))", "output": "Each container configuration can have an environment setting, or defaults\n    to base", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def orc(self, path):\n        \"\"\"\n        \"\"\"\n        if isinstance(path, basestring):\n            path = [path]\n        return self._df(self._jreader.orc(_to_seq(self._spark._sc, path)))", "output": "Loads ORC files, returning the result as a :class:`DataFrame`.\n\n        >>> df = spark.read.orc('python/test_support/sql/orc_partitioned')\n        >>> df.dtypes\n        [('a', 'bigint'), ('b', 'int'), ('c', 'int')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _player_step_tuple(self, envs_step_tuples):\n    \"\"\"\"\"\"\n    ob, reward, done, info = envs_step_tuples[\"env\"]\n    ob = self._augment_observation(ob, reward, self.cumulative_reward)\n    return ob, reward, done, info", "output": "Augment observation, return usual step tuple.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _escalation_rules_to_string(escalation_rules):\n    ''\n    result = ''\n    for rule in escalation_rules:\n        result += 'escalation_delay_in_minutes: {0} '.format(rule['escalation_delay_in_minutes'])\n        for target in rule['targets']:\n            result += '{0}:{1} '.format(target['type'], target['id'])\n    return result", "output": "convert escalation_rules dict to a string for comparison", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_properties(self, response):\n        \"\"\"\n        \"\"\"\n        self._properties.clear()\n        self._properties.update(response)", "output": "Helper for :meth:`reload`.\n\n        :type response: dict\n        :param response: resource mapping from server", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def winrm_cmd(session, command, flags, **kwargs):\n    '''\n    \n    '''\n    log.debug('Executing WinRM command: %s %s', command, flags)\n    # rebuild the session to ensure we haven't timed out\n    session.protocol.transport.build_session()\n    r = session.run_cmd(command, flags)\n    return r.status_code", "output": "Wrapper for commands to be run against Windows boxes using WinRM.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_locations(self, provider=None):\n        '''\n        \n        '''\n        mapper = salt.cloud.Map(self._opts_defaults())\n        return salt.utils.data.simple_types_filter(\n            mapper.location_list(provider)\n        )", "output": "List all available locations in configured cloud systems", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _to_dict(self):\n        \"\"\"\n        \"\"\"\n        if self.all_:\n            return {\"all\": True}\n\n        return {\n            \"keys\": self.keys,\n            \"ranges\": [keyrange._to_dict() for keyrange in self.ranges],\n        }", "output": "Return keyset's state as a dict.\n\n        The result can be used to serialize the instance and reconstitute\n        it later using :meth:`_from_dict`.\n\n        :rtype: dict\n        :returns: state of this instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keypair_add(name, pubfile=None, pubkey=None, profile=None, **kwargs):\n    '''\n    \n    '''\n    conn = _auth(profile, **kwargs)\n    return conn.keypair_add(\n        name,\n        pubfile,\n        pubkey\n    )", "output": "Add a keypair to nova (nova keypair-add)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' nova.keypair_add mykey pubfile='/home/myuser/.ssh/id_rsa.pub'\n        salt '*' nova.keypair_add mykey pubkey='ssh-rsa <key> myuser@mybox'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_list(self, name: str) -> List[str]:\n        \"\"\"\"\"\"\n        norm_name = _normalized_headers[name]\n        return self._as_list.get(norm_name, [])", "output": "Returns all values for the given header as a list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def host_update(hostid, **kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'host.update'\n            params = {\"hostid\": hostid}\n            params = _params_extend(params, _ignore_name=True, **kwargs)\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result']['hostids']\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": ".. versionadded:: 2016.3.0\n\n    Update existing hosts\n\n    .. note::\n        This function accepts all standard host and host.update properties:\n        keyword argument names differ depending on your zabbix version, see the\n        documentation for `host objects`_ and the documentation for `updating\n        hosts`_.\n\n        .. _`host objects`: https://www.zabbix.com/documentation/2.4/manual/api/reference/host/object#host\n        .. _`updating hosts`: https://www.zabbix.com/documentation/2.4/manual/api/reference/host/update\n\n    :param hostid: ID of the host to update\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n    :param visible_name: string with visible name of the host, use\n        'visible_name' instead of 'name' parameter to not mess with value\n        supplied from Salt sls file.\n\n    :return: ID of the updated host.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.host_update 10084 name='Zabbix server2'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_market_open(self, session_label, data_portal):\n        \"\"\"\n        \"\"\"\n        ledger = self._ledger\n        ledger.start_of_session(session_label)\n\n        adjustment_reader = data_portal.adjustment_reader\n        if adjustment_reader is not None:\n            # this is None when running with a dataframe source\n            ledger.process_dividends(\n                session_label,\n                self._asset_finder,\n                adjustment_reader,\n            )\n\n        self._current_session = session_label\n\n        cal = self._trading_calendar\n        self._market_open, self._market_close = self._execution_open_and_close(\n            cal,\n            session_label,\n        )\n\n        self.start_of_session(ledger, session_label, data_portal)", "output": "Handles the start of each session.\n\n        Parameters\n        ----------\n        session_label : Timestamp\n            The label of the session that is about to begin.\n        data_portal : DataPortal\n            The current data portal.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _reader(self, name, stream, outbuf):\n        \"\"\"\n        \n        \"\"\"\n        while True:\n            s = stream.readline()\n            if not s:\n                break\n            s = s.decode('utf-8').rstrip()\n            outbuf.append(s)\n            logger.debug('%s: %s' % (name, s))\n        stream.close()", "output": "Thread runner for reading lines of from a subprocess into a buffer.\n\n        :param name: The logical name of the stream (used for logging only).\n        :param stream: The stream to read from. This will typically a pipe\n                       connected to the output stream of a subprocess.\n        :param outbuf: The list to append the read lines to.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def aligned_umap(activations, umap_options={}, normalize=True, verbose=False):\n    \"\"\"\"\"\"\n\n    umap_defaults = dict(\n        n_components=2, n_neighbors=50, min_dist=0.05, verbose=verbose, metric=\"cosine\"\n    )\n    umap_defaults.update(umap_options)\n\n    # if passed a list of activations, we combine them and later split the layouts\n    if type(activations) is list or type(activations) is tuple:\n        num_activation_groups = len(activations)\n        combined_activations = np.concatenate(activations)\n    else:\n        num_activation_groups = 1\n        combined_activations = activations\n    try:\n        layout = UMAP(**umap_defaults).fit_transform(combined_activations)\n    except (RecursionError, SystemError) as exception:\n        log.error(\"UMAP failed to fit these activations. We're not yet sure why this sometimes occurs.\")\n        raise ValueError(\"UMAP failed to fit activations: %s\", exception)\n\n    if normalize:\n        layout = normalize_layout(layout)\n\n    if num_activation_groups > 1:\n        layouts = np.split(layout, num_activation_groups, axis=0)\n        return layouts\n    else:\n        return layout", "output": "`activations` can be a list of ndarrays. In that case a list of layouts is returned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sparse_series_to_coo(ss, row_levels=(0, ), column_levels=(1, ),\n                          sort_labels=False):\n    \"\"\"\n    \n    \"\"\"\n\n    import scipy.sparse\n\n    if ss.index.nlevels < 2:\n        raise ValueError('to_coo requires MultiIndex with nlevels > 2')\n    if not ss.index.is_unique:\n        raise ValueError('Duplicate index entries are not allowed in to_coo '\n                         'transformation.')\n\n    # to keep things simple, only rely on integer indexing (not labels)\n    row_levels = [ss.index._get_level_number(x) for x in row_levels]\n    column_levels = [ss.index._get_level_number(x) for x in column_levels]\n\n    v, i, j, rows, columns = _to_ijv(ss, row_levels=row_levels,\n                                     column_levels=column_levels,\n                                     sort_labels=sort_labels)\n    sparse_matrix = scipy.sparse.coo_matrix(\n        (v, (i, j)), shape=(len(rows), len(columns)))\n    return sparse_matrix, rows, columns", "output": "Convert a SparseSeries to a scipy.sparse.coo_matrix using index\n    levels row_levels, column_levels as the row and column\n    labels respectively. Returns the sparse_matrix, row and column labels.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resume(profile_process='worker'):\n    \"\"\"\n    \n    \"\"\"\n    profile_process2int = {'worker': 0, 'server': 1}\n    check_call(_LIB.MXProcessProfilePause(int(0),\n                                          profile_process2int[profile_process],\n                                          profiler_kvstore_handle))", "output": "Resume paused profiling.\n\n    Parameters\n    ----------\n    profile_process : string\n        whether to profile kvstore `server` or `worker`.\n        server can only be profiled when kvstore is of type dist.\n        if this is not passed, defaults to `worker`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def trading_pnl(self):\n        \"\"\"\n        \n        \"\"\"\n        last_price = self._data_proxy.get_last_price(self._order_book_id)\n        return self._contract_multiplier * (self._trade_quantity * last_price - self._trade_cost)", "output": "[float] \u4ea4\u6613\u76c8\u4e8f\uff0c\u7b56\u7565\u5728\u5f53\u524d\u4ea4\u6613\u65e5\u4ea7\u751f\u7684\u76c8\u4e8f\u4e2d\u6765\u6e90\u4e8e\u5f53\u65e5\u6210\u4ea4\u7684\u90e8\u5206", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_argument(self, dest, nargs=1, obj=None):\n        \"\"\"\n        \"\"\"\n        if obj is None:\n            obj = dest\n        self._args.append(Argument(dest=dest, nargs=nargs, obj=obj))", "output": "Adds a positional argument named `dest` to the parser.\n\n        The `obj` can be used to identify the option in the order list\n        that is returned from the parser.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def map_column(self, value):\n        \"\"\"\n        \n        \"\"\"\n        if value in self.null_values:\n            return r'\\\\N'\n        else:\n            return default_escape(six.text_type(value))", "output": "Applied to each column of every row returned by `rows`.\n\n        Default behaviour is to escape special characters and identify any self.null_values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def edit(self, *, reason=None, **options):\n        \"\"\"\n        \"\"\"\n\n        try:\n            position = options.pop('position')\n        except KeyError:\n            pass\n        else:\n            await self._move(position, reason=reason)\n            self.position = position\n\n        if options:\n            data = await self._state.http.edit_channel(self.id, reason=reason, **options)\n            self._update(self.guild, data)", "output": "|coro|\n\n        Edits the channel.\n\n        You must have the :attr:`~Permissions.manage_channels` permission to\n        use this.\n\n        Parameters\n        ----------\n        name: :class:`str`\n            The new category's name.\n        position: :class:`int`\n            The new category's position.\n        nsfw: :class:`bool`\n            To mark the category as NSFW or not.\n        reason: Optional[:class:`str`]\n            The reason for editing this category. Shows up on the audit log.\n\n        Raises\n        ------\n        InvalidArgument\n            If position is less than 0 or greater than the number of categories.\n        Forbidden\n            You do not have permissions to edit the category.\n        HTTPException\n            Editing the category failed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sink(self, name, filter_=None, destination=None):\n        \"\"\"\n        \"\"\"\n        return Sink(name, filter_, destination, client=self)", "output": "Creates a sink bound to the current client.\n\n        :type name: str\n        :param name: the name of the sink to be constructed.\n\n        :type filter_: str\n        :param filter_: (optional) the advanced logs filter expression\n                        defining the entries exported by the sink.  If not\n                        passed, the instance should already exist, to be\n                        refreshed via :meth:`Sink.reload`.\n\n        :type destination: str\n        :param destination: destination URI for the entries exported by\n                            the sink.  If not passed, the instance should\n                            already exist, to be refreshed via\n                            :meth:`Sink.reload`.\n\n        :rtype: :class:`google.cloud.logging.sink.Sink`\n        :returns: Sink created with the current client.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_positional_embedding(x, max_length, name=None, positions=None):\n  \"\"\"\n  \"\"\"\n  with tf.name_scope(\"add_positional_embedding\"):\n    _, length, depth = common_layers.shape_list(x)\n    var = tf.cast(tf.get_variable(name, [max_length, depth]), x.dtype)\n    if positions is None:\n      pad_length = tf.maximum(0, length - max_length)\n      sliced = tf.cond(\n          tf.less(length, max_length),\n          lambda: tf.slice(var, [0, 0], [length, -1]),\n          lambda: tf.pad(var, [[0, pad_length], [0, 0]]))\n      return x + tf.expand_dims(sliced, 0)\n    else:\n      return x + tf.gather(var, tf.to_int32(positions))", "output": "Adds positional embedding.\n\n  Args:\n    x: Tensor with shape [batch, length, depth].\n    max_length: int representing static maximum size of any dimension.\n    name: str representing name of the embedding tf.Variable.\n    positions: Tensor with shape [batch, length].\n\n  Returns:\n    Tensor of same shape as x.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def evaluate_regressor(model, data, target=\"target\", verbose=False):\n    \"\"\"\n    \n    \"\"\"\n    model = _get_model(model)\n\n    if verbose:\n        print(\"\")\n        print(\"Other Framework\\t\\tPredicted\\t\\tDelta\")\n\n    max_error = 0\n    error_squared = 0\n\n    for index,row in data.iterrows():\n        predicted = model.predict(dict(row))[_to_unicode(target)]\n        other_framework = row[\"prediction\"]\n        delta = predicted - other_framework\n\n        if verbose:\n            print(\"%s\\t\\t\\t\\t%s\\t\\t\\t%0.4f\" % (other_framework, predicted, delta))\n\n        max_error = max(abs(delta), max_error)\n        error_squared = error_squared + (delta * delta)\n\n    ret = {\n        \"samples\": len(data),\n        \"rmse\": _math.sqrt(error_squared / len(data)),\n        \"max_error\": max_error\n    }\n\n    if verbose:\n        print(\"results: %s\" % ret)\n    return ret", "output": "Evaluate a CoreML regression model and compare against predictions\n    from the original framework (for testing correctness of conversion)\n\n    Parameters\n    ----------\n    filename: [str | MLModel]\n        File path from which to load the MLModel from (OR) a loaded version of\n        MLModel.\n\n    data: [str | Dataframe]\n        Test data on which to evaluate the models (dataframe,\n        or path to a .csv file).\n\n    target: str\n       Name of the column in the dataframe that must be interpreted\n       as the target column.\n\n    verbose: bool\n       Set to true for a more verbose output.\n\n    See Also\n    --------\n    evaluate_classifier\n\n    Examples\n    --------\n    .. sourcecode:: python\n\n        >>> metrics = coremltools.utils.evaluate_regressor(spec, 'data_and_predictions.csv', 'target')\n        >>> print(metrics)\n        {\"samples\": 10, \"rmse\": 0.0, max_error: 0.0}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_checkout_target(self):\n        '''\n        \n        '''\n        if self.role == 'git_pillar' and self.branch == '__env__':\n            try:\n                return self.all_saltenvs\n            except AttributeError:\n                # all_saltenvs not configured for this remote\n                pass\n            target = self.opts.get('pillarenv') \\\n                or self.opts.get('saltenv') \\\n                or 'base'\n            return self.base \\\n                if target == 'base' \\\n                else six.text_type(target)\n        return self.branch", "output": "Resolve dynamically-set branch", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def group_get(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.get_group(**kwargs)", "output": "Get a single group\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystoneng.group_get name=group1\n        salt '*' keystoneng.group_get name=group2 domain_id=b62e76fbeeff4e8fb77073f591cf211e\n        salt '*' keystoneng.group_get name=0e4febc2a5ab4f2c8f374b054162506d", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, remotepath, localpath, callback=None):\n        \"\"\"\n        \n        \"\"\"\n        with open(localpath, \"wb\") as fl:\n            size = self.getfo(remotepath, fl, callback)\n        s = os.stat(localpath)\n        if s.st_size != size:\n            raise IOError(\n                \"size mismatch in get!  {} != {}\".format(s.st_size, size)\n            )", "output": "Copy a remote file (``remotepath``) from the SFTP server to the local\n        host as ``localpath``.  Any exception raised by operations will be\n        passed through.  This method is primarily provided as a convenience.\n\n        :param str remotepath: the remote file to copy\n        :param str localpath: the destination path on the local host\n        :param callable callback:\n            optional callback function (form: ``func(int, int)``) that accepts\n            the bytes transferred so far and the total bytes to be transferred\n\n        .. versionadded:: 1.4\n        .. versionchanged:: 1.7.4\n            Added the ``callback`` param", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zip_html(self):\n        \"\"\"\n        \n        \"\"\"\n        zip_fname = os.path.join(BUILD_PATH, 'html', 'pandas.zip')\n        if os.path.exists(zip_fname):\n            os.remove(zip_fname)\n        dirname = os.path.join(BUILD_PATH, 'html')\n        fnames = os.listdir(dirname)\n        os.chdir(dirname)\n        self._run_os('zip',\n                     zip_fname,\n                     '-r',\n                     '-q',\n                     *fnames)", "output": "Compress HTML documentation into a zip file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rlmb_long_stochastic_discrete_simulation_deterministic_starts():\n  \"\"\"\"\"\"\n  hparams = rlmb_base_stochastic_discrete()\n  hparams.generative_model_params = \"next_frame_basic_stochastic_discrete_long\"\n  hparams.ppo_epochs_num = 1000\n  hparams.simulation_random_starts = False\n  return hparams", "output": "Long setting with stochastic discrete model & deterministic sim starts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode_schedule(schedule):\n  \"\"\"\n  \"\"\"\n  interpolation, steps, pmfs = schedule\n  return interpolation + ' ' + ' '.join(\n      '@' + str(s) + ' ' + ' '.join(map(str, p)) for s, p in zip(steps, pmfs))", "output": "Encodes a schedule tuple into a string.\n\n  Args:\n    schedule: A tuple containing (interpolation, steps, pmfs), where\n      interpolation is a string specifying the interpolation strategy, steps\n      is an int array_like of shape [N] specifying the global steps, and pmfs is\n      an array_like of shape [N, M] where pmf[i] is the sampling distribution\n      at global step steps[i]. N is the number of schedule requirements to\n      interpolate and M is the size of the probability space.\n\n  Returns:\n    The string encoding of the schedule tuple.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def RandomNormalInitializer(stddev=1e-2):\n  \"\"\"\"\"\"\n  def init(shape, rng):\n    return (stddev * backend.random.normal(rng, shape)).astype('float32')\n  return init", "output": "An initializer function for random normal coefficients.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def has_path_sum(root, sum):\n    \"\"\"\n    \n    \"\"\"\n    if root is None:\n        return False\n    if root.left is None and root.right is None and root.val == sum:\n        return True\n    sum -= root.val\n    return has_path_sum(root.left, sum) or has_path_sum(root.right, sum)", "output": ":type root: TreeNode\n    :type sum: int\n    :rtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dataset_generator(filepath,\n                      dataset,\n                      chunk_size=1,\n                      start_idx=None,\n                      end_idx=None):\n  \"\"\"\"\"\"\n  encoder = dna_encoder.DNAEncoder(chunk_size=chunk_size)\n  with h5py.File(filepath, \"r\") as h5_file:\n    # Get input keys from h5_file\n    src_keys = [s % dataset for s in [\"%s_in\", \"%s_na\", \"%s_out\"]]\n    src_values = [h5_file[k] for k in src_keys]\n    inp_data, mask_data, out_data = src_values\n    assert len(set([v.len() for v in src_values])) == 1\n\n    if start_idx is None:\n      start_idx = 0\n    if end_idx is None:\n      end_idx = inp_data.len()\n\n    for i in range(start_idx, end_idx):\n      if i % 100 == 0:\n        print(\"Generating example %d for %s\" % (i, dataset))\n      inputs, mask, outputs = inp_data[i], mask_data[i], out_data[i]\n      ex_dict = to_example_dict(encoder, inputs, mask, outputs)\n      # Original data has one output for every 128 input bases. Ensure that the\n      # ratio has been maintained given the chunk size and removing EOS.\n      assert (len(ex_dict[\"inputs\"]) - 1) == ((\n          128 // chunk_size) * ex_dict[\"targets_shape\"][0])\n      yield ex_dict", "output": "Generate example dicts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_series_or_dataframe_operations(cls):\n        \"\"\"\n        \n        \"\"\"\n\n        from pandas.core import window as rwindow\n\n        @Appender(rwindow.rolling.__doc__)\n        def rolling(self, window, min_periods=None, center=False,\n                    win_type=None, on=None, axis=0, closed=None):\n            axis = self._get_axis_number(axis)\n            return rwindow.rolling(self, window=window,\n                                   min_periods=min_periods,\n                                   center=center, win_type=win_type,\n                                   on=on, axis=axis, closed=closed)\n\n        cls.rolling = rolling\n\n        @Appender(rwindow.expanding.__doc__)\n        def expanding(self, min_periods=1, center=False, axis=0):\n            axis = self._get_axis_number(axis)\n            return rwindow.expanding(self, min_periods=min_periods,\n                                     center=center, axis=axis)\n\n        cls.expanding = expanding\n\n        @Appender(rwindow.ewm.__doc__)\n        def ewm(self, com=None, span=None, halflife=None, alpha=None,\n                min_periods=0, adjust=True, ignore_na=False,\n                axis=0):\n            axis = self._get_axis_number(axis)\n            return rwindow.ewm(self, com=com, span=span, halflife=halflife,\n                               alpha=alpha, min_periods=min_periods,\n                               adjust=adjust, ignore_na=ignore_na, axis=axis)\n\n        cls.ewm = ewm", "output": "Add the series or dataframe only operations to the cls; evaluate\n        the doc strings again.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_simple_binding(jboss_config, binding_name, profile=None):\n    '''\n    \n       '''\n    log.debug(\"======================== MODULE FUNCTION: jboss7.read_simple_binding, %s\", binding_name)\n    return __read_simple_binding(jboss_config, binding_name, profile=profile)", "output": "Read jndi binding in the running jboss instance\n\n    jboss_config\n        Configuration dictionary with properties specified above.\n    binding_name\n        Binding name to be created\n    profile\n        The profile name (JBoss domain mode only)\n\n    CLI Example:\n\n        .. code-block:: bash\n\n        salt '*' jboss7.read_simple_binding '{\"cli_path\": \"integration.modules.sysmod.SysModuleTest.test_valid_docs\", \"controller\": \"10.11.12.13:9999\", \"cli_user\": \"jbossadm\", \"cli_password\": \"jbossadm\"}' my_binding_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_floating(self):\n        \"\"\"\"\"\"\n        return (\n            self.is_numpy_compatible and np.issubdtype(self.as_numpy_dtype, np.floating)\n        ) or self.base_dtype == bfloat16", "output": "Returns whether this is a (non-quantized, real) floating point type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lookup_default(self, name):\n        \"\"\"\n        \"\"\"\n        if self.default_map is not None:\n            rv = self.default_map.get(name)\n            if callable(rv):\n                rv = rv()\n            return rv", "output": "Looks up the default for a parameter name.  This by default\n        looks into the :attr:`default_map` if available.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_to_ascii(statement):\n    \"\"\"\n    \n    \"\"\"\n    import unicodedata\n\n    text = unicodedata.normalize('NFKD', statement.text)\n    text = text.encode('ascii', 'ignore').decode('utf-8')\n\n    statement.text = str(text)\n    return statement", "output": "Converts unicode characters to ASCII character equivalents.\n    For example: \"p\u00e5 f\u00e9d\u00e9ral\" becomes \"pa federal\".", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def super(self):\n        \"\"\"\"\"\"\n        if self._depth + 1 >= len(self._stack):\n            return self._context.environment. \\\n                undefined('there is no parent block called %r.' %\n                          self.name, name='super')\n        return BlockReference(self.name, self._context, self._stack,\n                              self._depth + 1)", "output": "Super the block.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _size_fmt(num):\n    '''\n    \n    '''\n    try:\n        num = int(num)\n        if num < 1024:\n            return '{0} bytes'.format(num)\n        num /= 1024.0\n        for unit in ('KiB', 'MiB', 'GiB', 'TiB', 'PiB'):\n            if num < 1024.0:\n                return '{0:3.1f} {1}'.format(num, unit)\n            num /= 1024.0\n    except Exception:\n        log.error('Unable to format file size for \\'%s\\'', num)\n        return 'unknown'", "output": "Format bytes as human-readable file sizes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_job_cache(hours=24):\n    '''\n    \n    '''\n    threshold = time.time() - hours * 60 * 60\n    for root, dirs, files in salt.utils.files.safe_walk(os.path.join(__opts__['cachedir'], 'minion_jobs'),\n                                                  followlinks=False):\n        for name in dirs:\n            try:\n                directory = os.path.join(root, name)\n                mtime = os.path.getmtime(directory)\n                if mtime < threshold:\n                    shutil.rmtree(directory)\n            except OSError as exc:\n                log.error('Attempt to clear cache with saltutil.clear_job_cache FAILED with: %s', exc)\n                return False\n    return True", "output": "Forcibly removes job cache folders and files on a minion.\n\n    .. versionadded:: 2018.3.0\n\n    WARNING: The safest way to clear a minion cache is by first stopping\n    the minion and then deleting the cache files before restarting it.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.clear_job_cache hours=12", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def can_user_run(self, user, command, groups):\n        '''\n        \n\n        '''\n        log.info('%s wants to run %s with groups %s', user, command, groups)\n        for key, val in groups.items():\n            if user not in val['users']:\n                if '*' not in val['users']:\n                    continue  # this doesn't grant permissions, pass\n            if (command not in val['commands']) and (command not in val.get('aliases', {}).keys()):\n                if '*' not in val['commands']:\n                    continue  # again, pass\n            log.info('Slack user %s permitted to run %s', user, command)\n            return (key, val,)  # matched this group, return the group\n        log.info('Slack user %s denied trying to run %s', user, command)\n        return ()", "output": "Break out the permissions into the following:\n\n        Check whether a user is in any group, including whether a group has the '*' membership\n\n        :type user: str\n        :param user: The username being checked against\n\n        :type command: str\n        :param command: The command that is being invoked (e.g. test.ping)\n\n        :type groups: dict\n        :param groups: the dictionary with groups permissions structure.\n\n        :rtype: tuple\n        :returns: On a successful permitting match, returns 2-element tuple that contains\n            the name of the group that successfully matched, and a dictionary containing\n            the configuration of the group so it can be referenced.\n\n            On failure it returns an empty tuple", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_range_vector(size: int, device: int) -> torch.Tensor:\n    \"\"\"\n    \n    \"\"\"\n    if device > -1:\n        return torch.cuda.LongTensor(size, device=device).fill_(1).cumsum(0) - 1\n    else:\n        return torch.arange(0, size, dtype=torch.long)", "output": "Returns a range vector with the desired size, starting at 0. The CUDA implementation\n    is meant to avoid copy data from CPU to GPU.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_mirrors(config_path=_DEFAULT_CONFIG_PATH):\n    '''\n    \n    '''\n    _validate_config(config_path)\n\n    cmd = ['mirror', 'list', '-config={}'.format(config_path), '-raw=true']\n\n    cmd_ret = _cmd_run(cmd)\n    ret = [line.strip() for line in cmd_ret.splitlines()]\n\n    log.debug('Found mirrors: %s', len(ret))\n    return ret", "output": "Get a list of all the mirrored remote repositories.\n\n    :param str config_path: The path to the configuration file for the aptly instance.\n\n    :return: A list of the mirror names.\n    :rtype: list\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' aptly.list_mirrors", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def infer(self, setup: QuaRelType, answer_0: QuaRelType, answer_1: QuaRelType) -> int:\n        \"\"\"\n        \n        \"\"\"\n        if self._check_quarels_compatible(setup, answer_0):\n            if self._check_quarels_compatible(setup, answer_1):\n                # Found two answers\n                return -2\n            else:\n                return 0\n        elif self._check_quarels_compatible(setup, answer_1):\n            return 1\n        else:\n            return -1", "output": "Take the question and check if it is compatible with either of the answer choices.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decompress_decoder_1d(x, hparams, name=None):\n  \"\"\"\n  \"\"\"\n  x = tf.expand_dims(x, axis=2)\n  output = decompress_decoder(x, hparams,\n                              strides=(2, 1),\n                              kernel=(hparams.kernel_size, 1),\n                              name=name)\n  return tf.squeeze(output, axis=2)", "output": "Decoder that decompresses 1-D inputs by 2**num_compress_steps.\n\n  Args:\n    x: Tensor of shape [batch, compress_length, channels].\n    hparams: HParams.\n    name: string, variable scope.\n\n  Returns:\n    Tensor of shape [batch, length, hparams.hidden_size].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare_request(self, request):\n        \"\"\"\n        \"\"\"\n        cookies = request.cookies or {}\n\n        # Bootstrap CookieJar.\n        if not isinstance(cookies, cookielib.CookieJar):\n            cookies = cookiejar_from_dict(cookies)\n\n        # Merge with session cookies\n        merged_cookies = merge_cookies(\n            merge_cookies(RequestsCookieJar(), self.cookies), cookies)\n\n        # Set environment's basic authentication if not explicitly set.\n        auth = request.auth\n        if self.trust_env and not auth and not self.auth:\n            auth = get_netrc_auth(request.url)\n\n        p = PreparedRequest()\n        p.prepare(\n            method=request.method.upper(),\n            url=request.url,\n            files=request.files,\n            data=request.data,\n            json=request.json,\n            headers=merge_setting(request.headers, self.headers, dict_class=CaseInsensitiveDict),\n            params=merge_setting(request.params, self.params),\n            auth=merge_setting(auth, self.auth),\n            cookies=merged_cookies,\n            hooks=merge_hooks(request.hooks, self.hooks),\n        )\n        return p", "output": "Constructs a :class:`PreparedRequest <PreparedRequest>` for\n        transmission and returns it. The :class:`PreparedRequest` has settings\n        merged from the :class:`Request <Request>` instance and those of the\n        :class:`Session`.\n\n        :param request: :class:`Request` instance to prepare with this\n            session's settings.\n        :rtype: requests.PreparedRequest", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_weight(name, backend, socket=DEFAULT_SOCKET_URL):\n    '''\n    \n    '''\n    ha_conn = _get_conn(socket)\n    ha_cmd = haproxy.cmds.getWeight(server=name, backend=backend)\n    return ha_conn.sendCmd(ha_cmd)", "output": "Get server weight\n\n    name\n        Server name\n\n    backend\n        haproxy backend\n\n    socket\n        haproxy stats socket, default ``/var/run/haproxy.sock``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' haproxy.get_weight web1.example.com www", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_network_settings():\n    '''\n    \n    '''\n    if __grains__['lsb_distrib_id'] == 'nilrt':\n        raise salt.exceptions.CommandExecutionError('Not supported in this version.')\n    settings = []\n    networking = 'no' if _get_state() == 'offline' else 'yes'\n    settings.append('networking={0}'.format(networking))\n    hostname = __salt__['network.get_hostname']\n    settings.append('hostname={0}'.format(hostname))\n    return settings", "output": "Return the contents of the global network script.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ip.get_network_settings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _passwd_opts(self):\n        '''\n        \n        '''\n        # TODO ControlMaster does not work without ControlPath\n        # user could take advantage of it if they set ControlPath in their\n        # ssh config.  Also, ControlPersist not widely available.\n        options = ['ControlMaster=auto',\n                   'StrictHostKeyChecking=no',\n                   ]\n        if self.opts['_ssh_version'] > (4, 9):\n            options.append('GSSAPIAuthentication=no')\n        options.append('ConnectTimeout={0}'.format(self.timeout))\n        if self.opts.get('ignore_host_keys'):\n            options.append('StrictHostKeyChecking=no')\n        if self.opts.get('no_host_keys'):\n            options.extend(['StrictHostKeyChecking=no',\n                            'UserKnownHostsFile=/dev/null'])\n\n        if self.passwd:\n            options.extend(['PasswordAuthentication=yes',\n                            'PubkeyAuthentication=yes'])\n        else:\n            options.extend(['PasswordAuthentication=no',\n                            'PubkeyAuthentication=yes',\n                            'KbdInteractiveAuthentication=no',\n                            'ChallengeResponseAuthentication=no',\n                            'BatchMode=yes'])\n        if self.port:\n            options.append('Port={0}'.format(self.port))\n        if self.user:\n            options.append('User={0}'.format(self.user))\n        if self.identities_only:\n            options.append('IdentitiesOnly=yes')\n\n        ret = []\n        for option in options:\n            ret.append('-o {0} '.format(option))\n        return ''.join(ret)", "output": "Return options to pass to ssh", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render_as_text(self, limit=None):\n        \"\"\"\"\"\"\n        lines = traceback.format_exception(self.exc_type, self.exc_value,\n                                           self.frames[0], limit=limit)\n        return ''.join(lines).rstrip()", "output": "Return a string with the traceback.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_backward_begin(self, smooth_loss:Tensor, **kwargs:Any)->None:\n        \"\"\n        self.losses.append(smooth_loss)\n        if self.pbar is not None and hasattr(self.pbar,'child'):\n            self.pbar.child.comment = f'{smooth_loss:.4f}'", "output": "Record the loss before any other callback has a chance to modify it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        \"\"\"\n        \"\"\"\n        _logger.info('Start dispatcher')\n        if dispatcher_env_vars.NNI_MODE == 'resume':\n            self.load_checkpoint()\n\n        while True:\n            command, data = receive()\n            if data:\n                data = json_tricks.loads(data)\n\n            if command is None or command is CommandType.Terminate:\n                break\n            if multi_thread_enabled():\n                result = self.pool.map_async(self.process_command_thread, [(command, data)])\n                self.thread_results.append(result)\n                if any([thread_result.ready() and not thread_result.successful() for thread_result in self.thread_results]):\n                    _logger.debug('Caught thread exception')\n                    break\n            else:\n                self.enqueue_command(command, data)\n                if self.worker_exceptions:\n                    break\n\n        _logger.info('Dispatcher exiting...')\n        self.stopping = True\n        if multi_thread_enabled():\n            self.pool.close()\n            self.pool.join()\n        else:\n            self.default_worker.join()\n            self.assessor_worker.join()\n\n        _logger.info('Terminated by NNI manager')", "output": "Run the tuner.\n        This function will never return unless raise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def multinomial_sample(x, vocab_size=None, sampling_method=\"random\",\n                       temperature=1.0):\n  \"\"\"\n  \"\"\"\n  vocab_size = vocab_size or common_layers.shape_list(x)[-1]\n  if sampling_method == \"random\" and temperature > 0.0:\n    samples = tf.multinomial(tf.reshape(x, [-1, vocab_size]) / temperature, 1)\n  else:\n    samples = tf.argmax(x, axis=-1)\n  reshaped_samples = tf.reshape(samples, common_layers.shape_list(x)[:-1])\n  return reshaped_samples", "output": "Multinomial sampling from a n-dimensional tensor.\n\n  Args:\n    x: Tensor of shape [..., vocab_size]. Parameterizes logits of multinomial.\n    vocab_size: Number of classes in multinomial distribution.\n    sampling_method: String, \"random\" or otherwise deterministic.\n    temperature: Positive float.\n\n  Returns:\n    Tensor of shape [...].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_cached_response(self, request, response):\n        \"\"\"\n        \"\"\"\n        cache_url = self.cache_url(request.url)\n\n        cached_response = self.serializer.loads(request, self.cache.get(cache_url))\n\n        if not cached_response:\n            # we didn't have a cached response\n            return response\n\n        # Lets update our headers with the headers from the new request:\n        # http://tools.ietf.org/html/draft-ietf-httpbis-p4-conditional-26#section-4.1\n        #\n        # The server isn't supposed to send headers that would make\n        # the cached body invalid. But... just in case, we'll be sure\n        # to strip out ones we know that might be problmatic due to\n        # typical assumptions.\n        excluded_headers = [\"content-length\"]\n\n        cached_response.headers.update(\n            dict(\n                (k, v)\n                for k, v in response.headers.items()\n                if k.lower() not in excluded_headers\n            )\n        )\n\n        # we want a 200 b/c we have content via the cache\n        cached_response.status = 200\n\n        # update our cache\n        self.cache.set(cache_url, self.serializer.dumps(request, cached_response))\n\n        return cached_response", "output": "On a 304 we will get a new set of headers that we want to\n        update our cached value with, assuming we have one.\n\n        This should only ever be called when we've sent an ETag and\n        gotten a 304 as the response.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main(argv=None):\n  \"\"\"\n  \n  \"\"\"\n  try:\n    _name_of_script, filepath = argv\n  except ValueError:\n    raise ValueError(argv)\n  print_accuracies(filepath=filepath, test_start=FLAGS.test_start,\n                   test_end=FLAGS.test_end, which_set=FLAGS.which_set,\n                   nb_iter=FLAGS.nb_iter, base_eps_iter=FLAGS.base_eps_iter,\n                   batch_size=FLAGS.batch_size)", "output": "Print accuracies", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def crossproduct(d):\n    \"\"\"\n    \n    \"\"\"\n\n    from .. import SArray\n    d = [list(zip(list(d.keys()), x)) for x in _itertools.product(*list(d.values()))]\n    sa = [{k:v for (k,v) in x} for x in d]\n    return SArray(sa).unpack(column_name_prefix='')", "output": "Create an SFrame containing the crossproduct of all provided options.\n\n    Parameters\n    ----------\n    d : dict\n        Each key is the name of an option, and each value is a list\n        of the possible values for that option.\n\n    Returns\n    -------\n    out : SFrame\n        There will be a column for each key in the provided dictionary,\n        and a row for each unique combination of all values.\n\n    Example\n    -------\n    settings = {'argument_1':[0, 1],\n                'argument_2':['a', 'b', 'c']}\n    print crossproduct(settings)\n    +------------+------------+\n    | argument_2 | argument_1 |\n    +------------+------------+\n    |     a      |     0      |\n    |     a      |     1      |\n    |     b      |     0      |\n    |     b      |     1      |\n    |     c      |     0      |\n    |     c      |     1      |\n    +------------+------------+\n    [6 rows x 2 columns]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_info_file(tensorboard_info):\n  \"\"\"\n  \"\"\"\n  payload = \"%s\\n\" % _info_to_string(tensorboard_info)\n  with open(_get_info_file_path(), \"w\") as outfile:\n    outfile.write(payload)", "output": "Write TensorBoardInfo to the current process's info file.\n\n  This should be called by `main` once the server is ready. When the\n  server shuts down, `remove_info_file` should be called.\n\n  Args:\n    tensorboard_info: A valid `TensorBoardInfo` object.\n\n  Raises:\n    ValueError: If any field on `info` is not of the correct type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read(self):\n        \"\"\"\n        \n        \"\"\"\n\n        swagger = None\n\n        # First check if there is inline swagger\n        if self.definition_body:\n            swagger = self._read_from_definition_body()\n\n        if not swagger and self.definition_uri:\n            # If not, then try to download it from the given URI\n            swagger = self._download_swagger(self.definition_uri)\n\n        return swagger", "output": "Gets the Swagger document from either of the given locations. If we fail to retrieve or parse the Swagger\n        file, this method will return None.\n\n        Returns\n        -------\n        dict:\n            Swagger document. None, if we cannot retrieve the document", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def initialize_schema(connection):\n  \"\"\"\n  \"\"\"\n  cursor = connection.cursor()\n  cursor.execute(\"PRAGMA application_id={}\".format(_TENSORBOARD_APPLICATION_ID))\n  cursor.execute(\"PRAGMA user_version={}\".format(_TENSORBOARD_USER_VERSION))\n  with connection:\n    for statement in _SCHEMA_STATEMENTS:\n      lines = statement.strip('\\n').split('\\n')\n      message = lines[0] + ('...' if len(lines) > 1 else '')\n      logger.debug('Running DB init statement: %s', message)\n      cursor.execute(statement)", "output": "Initializes the TensorBoard sqlite schema using the given connection.\n\n  Args:\n    connection: A sqlite DB connection.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _clone(self):\n        \"\"\"\n        \"\"\"\n        cloned_self = self.__class__(\n            *self.flat_path, project=self.project, namespace=self.namespace\n        )\n        # If the current parent has already been set, we re-use\n        # the same instance\n        cloned_self._parent = self._parent\n        return cloned_self", "output": "Duplicates the Key.\n\n        Most attributes are simple types, so don't require copying. Other\n        attributes like ``parent`` are long-lived and so we re-use them.\n\n        :rtype: :class:`google.cloud.datastore.key.Key`\n        :returns: A new ``Key`` instance with the same data as the current one.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __reg_query_value(handle, value_name):\n        '''\n        \n        '''\n        # item_value, item_type = win32api.RegQueryValueEx(self.__reg_uninstall_handle, value_name)\n        item_value, item_type = win32api.RegQueryValueEx(handle, value_name)  # pylint: disable=no-member\n        if six.PY2 and isinstance(item_value, six.string_types) and not isinstance(item_value, six.text_type):\n            try:\n                item_value = six.text_type(item_value, encoding='mbcs')\n            except UnicodeError:\n                pass\n        if item_type == win32con.REG_EXPAND_SZ:\n            # expects Unicode input\n            win32api.ExpandEnvironmentStrings(item_value)  # pylint: disable=no-member\n            item_type = win32con.REG_SZ\n        return item_value, item_type", "output": "Calls RegQueryValueEx\n\n        If PY2 ensure unicode string and expand REG_EXPAND_SZ before returning\n        Remember to catch not found exceptions when calling.\n\n        Args:\n            handle (object): open registry handle.\n            value_name (str): Name of the value you wished returned\n\n        Returns:\n            tuple: type, value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def connect_database(url):\n    \"\"\"\n    \n\n    \"\"\"\n    db = _connect_database(url)\n    db.copy = lambda: _connect_database(url)\n    return db", "output": "create database object by url\n\n    mysql:\n        mysql+type://user:passwd@host:port/database\n    sqlite:\n        # relative path\n        sqlite+type:///path/to/database.db\n        # absolute path\n        sqlite+type:////path/to/database.db\n        # memory database\n        sqlite+type://\n    mongodb:\n        mongodb+type://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]]\n        more: http://docs.mongodb.org/manual/reference/connection-string/\n    sqlalchemy:\n        sqlalchemy+postgresql+type://user:passwd@host:port/database\n        sqlalchemy+mysql+mysqlconnector+type://user:passwd@host:port/database\n        more: http://docs.sqlalchemy.org/en/rel_0_9/core/engines.html\n    redis:\n        redis+taskdb://host:port/db\n    elasticsearch:\n        elasticsearch+type://host:port/?index=pyspider\n    local:\n        local+projectdb://filepath,filepath\n\n    type:\n        taskdb\n        projectdb\n        resultdb", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def project_present(name, description=None, enabled=True, profile=None,\n                    **connection_args):\n    '''\n    \n\n    '''\n\n    return tenant_present(name, description=description, enabled=enabled, profile=profile,\n                          **connection_args)", "output": "Ensures that the keystone project exists\n    Alias for tenant_present from V2 API to fulfill\n    V3 API naming convention.\n\n    .. versionadded:: 2016.11.0\n\n    name\n        The name of the project to manage\n\n    description\n        The description to use for this project\n\n    enabled\n        Availability state for this project\n\n    .. code-block:: yaml\n\n        nova:\n            keystone.project_present:\n                - enabled: True\n                - description: 'Nova Compute Service'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __create_orget_address(conn, name, region):\n    '''\n    \n    '''\n    try:\n        addy = conn.ex_get_address(name, region)\n    except ResourceNotFoundError:  # pylint: disable=W0703\n        addr_kwargs = {\n            'name': name,\n            'region': region\n        }\n        new_addy = create_address(addr_kwargs, \"function\")\n        addy = conn.ex_get_address(new_addy['name'], new_addy['region'])\n\n    return addy", "output": "Reuse or create a static IP address.\n    Returns a native GCEAddress construct to use with libcloud.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def queue_put_stoppable(self, q, obj):\n        \"\"\" \"\"\"\n        while not self.stopped():\n            try:\n                q.put(obj, timeout=5)\n                break\n            except queue.Full:\n                pass", "output": "Put obj to queue, but will give up when the thread is stopped", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expires(self):\n        \"\"\"\n        \"\"\"\n        expiration_time = self._properties.get(\"expirationTime\")\n        if expiration_time is not None:\n            # expiration_time will be in milliseconds.\n            return google.cloud._helpers._datetime_from_microseconds(\n                1000.0 * float(expiration_time)\n            )", "output": "Union[datetime.datetime, None]: Datetime at which the table will be\n        deleted.\n\n        Raises:\n            ValueError: For invalid value types.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _resizeColumnsToContents(self, header, data, limit_ms):\r\n        \"\"\"\"\"\"\r\n        max_col = data.model().columnCount()\r\n        if limit_ms is None:\r\n            max_col_ms = None\r\n        else:\r\n            max_col_ms = limit_ms / max(1, max_col)\r\n        for col in range(max_col):\r\n            self._resizeColumnToContents(header, data, col, max_col_ms)", "output": "Resize all the colummns to its contents.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _query_response_to_snapshot(response_pb, collection, expected_prefix):\n    \"\"\"\n    \"\"\"\n    if not response_pb.HasField(\"document\"):\n        return None\n\n    document_id = _helpers.get_doc_id(response_pb.document, expected_prefix)\n    reference = collection.document(document_id)\n    data = _helpers.decode_dict(response_pb.document.fields, collection._client)\n    snapshot = document.DocumentSnapshot(\n        reference,\n        data,\n        exists=True,\n        read_time=response_pb.read_time,\n        create_time=response_pb.document.create_time,\n        update_time=response_pb.document.update_time,\n    )\n    return snapshot", "output": "Parse a query response protobuf to a document snapshot.\n\n    Args:\n        response_pb (google.cloud.proto.firestore.v1beta1.\\\n            firestore_pb2.RunQueryResponse): A\n        collection (~.firestore_v1beta1.collection.CollectionReference): A\n            reference to the collection that initiated the query.\n        expected_prefix (str): The expected prefix for fully-qualified\n            document names returned in the query results. This can be computed\n            directly from ``collection`` via :meth:`_parent_info`.\n\n    Returns:\n        Optional[~.firestore.document.DocumentSnapshot]: A\n        snapshot of the data returned in the query. If ``response_pb.document``\n        is not set, the snapshot will be :data:`None`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_settings(self, index=None, name=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, \"_settings\", name), params=params\n        )", "output": "Retrieve settings for one or more (or all) indices.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-settings.html>`_\n\n        :arg index: A comma-separated list of index names; use `_all` or empty\n            string to perform the operation on all indices\n        :arg name: The name of the settings that should be included\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default ['open', 'closed'],\n            valid choices are: 'open', 'closed', 'none', 'all'\n        :arg flat_settings: Return settings in flat format (default: false)\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)\n        :arg include_defaults: Whether to return all default setting for each of\n            the indices., default False\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_target_variable (self, targets, variable, value, append=0):\n        \"\"\" \n        \"\"\"\n        if isinstance (targets, str):\n            targets = [targets]\n        if isinstance(value, str):\n            value = [value]\n\n        assert is_iterable(targets)\n        assert isinstance(variable, basestring)\n        assert is_iterable(value)\n\n        if targets:\n            if append:\n                bjam_interface.call(\"set-target-variable\", targets, variable, value, \"true\")\n            else:\n                bjam_interface.call(\"set-target-variable\", targets, variable, value)", "output": "Sets a target variable.\n\n        The 'variable' will be available to bjam when it decides\n        where to generate targets, and will also be available to\n        updating rule for that 'taret'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _decode_input_tensor_to_features_dict(feature_map, hparams):\n  \"\"\"\n  \"\"\"\n  inputs = tf.convert_to_tensor(feature_map[\"inputs\"])\n  input_is_image = False\n\n  x = inputs\n  p_hparams = hparams.problem_hparams\n  # Add a third empty dimension\n  x = tf.expand_dims(x, axis=[2])\n  x = tf.to_int32(x)\n  input_space_id = tf.constant(p_hparams.input_space_id)\n  target_space_id = tf.constant(p_hparams.target_space_id)\n\n  features = {}\n  features[\"input_space_id\"] = input_space_id\n  features[\"target_space_id\"] = target_space_id\n  features[\"decode_length\"] = (\n      IMAGE_DECODE_LENGTH if input_is_image else tf.shape(x)[1] + 50)\n  features[\"inputs\"] = x\n  return features", "output": "Convert the interactive input format (see above) to a dictionary.\n\n  Args:\n    feature_map: dict with inputs.\n    hparams: model hyperparameters\n\n  Returns:\n    a features dictionary, as expected by the decoder.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def insertInto(self, tableName, overwrite=False):\n        \"\"\"\n        \"\"\"\n        self._jwrite.mode(\"overwrite\" if overwrite else \"append\").insertInto(tableName)", "output": "Inserts the content of the :class:`DataFrame` to the specified table.\n\n        It requires that the schema of the class:`DataFrame` is the same as the\n        schema of the table.\n\n        Optionally overwriting any existing data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cancel(self, client=None):\n        \"\"\"\n        \"\"\"\n        client = self._require_client(client)\n\n        extra_params = {}\n        if self.location:\n            extra_params[\"location\"] = self.location\n\n        api_response = client._connection.api_request(\n            method=\"POST\", path=\"%s/cancel\" % (self.path,), query_params=extra_params\n        )\n        self._set_properties(api_response[\"job\"])\n        # The Future interface requires that we return True if the *attempt*\n        # to cancel was successful.\n        return True", "output": "API call:  cancel job via a POST request\n\n        See\n        https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/cancel\n\n        :type client: :class:`~google.cloud.bigquery.client.Client` or\n                      ``NoneType``\n        :param client: the client to use.  If not passed, falls back to the\n                       ``client`` stored on the current dataset.\n\n        :rtype: bool\n        :returns: Boolean indicating that the cancel request was sent.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_lock(self, lock_type='update'):\n        '''\n        \n        '''\n        lock_file = self._get_lock_file(lock_type=lock_type)\n\n        def _add_error(errlist, exc):\n            msg = ('Unable to remove update lock for {0} ({1}): {2} '\n                   .format(self.url, lock_file, exc))\n            log.debug(msg)\n            errlist.append(msg)\n\n        success = []\n        failed = []\n\n        try:\n            os.remove(lock_file)\n        except OSError as exc:\n            if exc.errno == errno.ENOENT:\n                # No lock file present\n                pass\n            elif exc.errno == errno.EISDIR:\n                # Somehow this path is a directory. Should never happen\n                # unless some wiseguy manually creates a directory at this\n                # path, but just in case, handle it.\n                try:\n                    shutil.rmtree(lock_file)\n                except OSError as exc:\n                    _add_error(failed, exc)\n            else:\n                _add_error(failed, exc)\n        else:\n            msg = 'Removed {0} lock for {1} remote \\'{2}\\''.format(\n                lock_type,\n                self.role,\n                self.id\n            )\n            log.debug(msg)\n            success.append(msg)\n        return success, failed", "output": "Clear update.lk", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_args(args, root):\n    \"\"\"\n    \n    \"\"\"\n\n    extension_args = {}\n\n    for arg in args:\n        parse_extension_arg(arg, extension_args)\n\n    for name in sorted(extension_args, key=len):\n        path = name.split('.')\n        update_namespace(root, path, extension_args[name])", "output": "Encapsulates a set of custom command line arguments in key=value\n    or key.namespace=value form into a chain of Namespace objects,\n    where each next level is an attribute of the Namespace object on the\n    current level\n\n    Parameters\n    ----------\n    args : list\n        A list of strings representing arguments in key=value form\n    root : Namespace\n        The top-level element of the argument tree", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_state_changed(self, state):\n        \"\"\"\"\"\"\n        if state:\n            self.editor.sig_key_pressed.connect(self._on_key_pressed)\n        else:\n            self.editor.sig_key_pressed.disconnect(self._on_key_pressed)", "output": "Connect/disconnect sig_key_pressed signal.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.log_file != PIPE and not (self.log_file == DEVNULL and _HAS_NATIVE_DEVNULL):\n            try:\n                self.log_file.close()\n            except Exception:\n                pass\n\n        if self.process is None:\n            return\n\n        try:\n            self.send_remote_shutdown_command()\n        except TypeError:\n            pass\n\n        try:\n            if self.process:\n                for stream in [self.process.stdin,\n                               self.process.stdout,\n                               self.process.stderr]:\n                    try:\n                        stream.close()\n                    except AttributeError:\n                        pass\n                self.process.terminate()\n                self.process.wait()\n                self.process.kill()\n                self.process = None\n        except OSError:\n            pass", "output": "Stops the service.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_allowed_shape_ranges(spec):\n    \"\"\"\n    \n    \"\"\"\n\n    shaper = NeuralNetworkShaper(spec, False)\n    inputs = _get_input_names(spec)\n    output = {}\n\n    for input in inputs:\n        output[input] = shaper.shape(input)\n\n    return output", "output": "For a given model specification, returns a dictionary with a shape range object for each input feature name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_portfolio_info(self, portfolio_code):\n        \"\"\"\n        \n        \"\"\"\n        url = self.config[\"portfolio_url\"] + portfolio_code\n        html = self._get_html(url)\n        match_info = re.search(r\"(?<=SNB.cubeInfo = ).*(?=;\\n)\", html)\n        if match_info is None:\n            raise Exception(\n                \"cant get portfolio info, portfolio html : {}\".format(html)\n            )\n        try:\n            portfolio_info = json.loads(match_info.group())\n        except Exception as e:\n            raise Exception(\"get portfolio info error: {}\".format(e))\n        return portfolio_info", "output": "\u83b7\u53d6\u7ec4\u5408\u4fe1\u606f\n        :return: \u5b57\u5178", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hybrid_forward(self, F, data, gamma, beta):\n        \"\"\"\"\"\"\n        # TODO(haibin): LayerNorm does not support fp16 safe reduction. Issue is tracked at:\n        # https://github.com/apache/incubator-mxnet/issues/14073\n        if self._dtype:\n            data = data.astype('float32')\n            gamma = gamma.astype('float32')\n            beta = beta.astype('float32')\n        norm_data = F.LayerNorm(data, gamma=gamma, beta=beta, axis=self._axis, eps=self._epsilon)\n        if self._dtype:\n            norm_data = norm_data.astype(self._dtype)\n        return norm_data", "output": "forward computation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_keys_to_action(self):\n    \"\"\"\n    \"\"\"\n    # Based on gym AtariEnv.get_keys_to_action()\n    keyword_to_key = {\n        \"UP\": ord(\"w\"),\n        \"DOWN\": ord(\"s\"),\n        \"LEFT\": ord(\"a\"),\n        \"RIGHT\": ord(\"d\"),\n        \"FIRE\": ord(\" \"),\n    }\n\n    keys_to_action = {}\n\n    for action_id, action_meaning in enumerate(self.action_meanings):\n      keys_tuple = tuple(sorted([\n          key for keyword, key in keyword_to_key.items()\n          if keyword in action_meaning]))\n      assert keys_tuple not in keys_to_action\n      keys_to_action[keys_tuple] = action_id\n\n    # Special actions:\n    keys_to_action[(ord(\"r\"),)] = self.RETURN_DONE_ACTION\n    keys_to_action[(ord(\"c\"),)] = self.TOGGLE_WAIT_ACTION\n    keys_to_action[(ord(\"n\"),)] = self.WAIT_MODE_NOOP_ACTION\n\n    return keys_to_action", "output": "Get mapping from keyboard keys to actions.\n\n    Required by gym.utils.play in environment or top level wrapper.\n\n    Returns:\n      {\n        Unicode code point for keyboard key: action (formatted for step()),\n        ...\n      }", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _absent(name, dataset_type, force=False, recursive=False, recursive_all=False):\n    '''\n    \n\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    ## log configuration\n    dataset_type = dataset_type.lower()\n    log.debug('zfs.%s_absent::%s::config::force = %s',\n              dataset_type, name, force)\n    log.debug('zfs.%s_absent::%s::config::recursive = %s',\n              dataset_type, name, recursive)\n\n    ## destroy dataset if needed\n    if __salt__['zfs.exists'](name, **{'type': dataset_type}):\n        ## NOTE: dataset found with the name and dataset_type\n        if not __opts__['test']:\n            mod_res = __salt__['zfs.destroy'](name, **{'force': force, 'recursive': recursive, 'recursive_all': recursive_all})\n        else:\n            mod_res = OrderedDict([('destroyed', True)])\n\n        ret['result'] = mod_res['destroyed']\n        if ret['result']:\n            ret['changes'][name] = 'destroyed'\n            ret['comment'] = '{0} {1} was destroyed'.format(\n                dataset_type,\n                name,\n            )\n        else:\n            ret['comment'] = 'failed to destroy {0} {1}'.format(\n                dataset_type,\n                name,\n            )\n            if 'error' in mod_res:\n                ret['comment'] = mod_res['error']\n    else:\n        ## NOTE: no dataset found with name of the dataset_type\n        ret['comment'] = '{0} {1} is absent'.format(\n            dataset_type,\n            name\n        )\n\n    return ret", "output": "internal shared function for *_absent\n\n    name : string\n        name of dataset\n    dataset_type : string [filesystem, volume, snapshot, or bookmark]\n        type of dataset to remove\n    force : boolean\n        try harder to destroy the dataset\n    recursive : boolean\n        also destroy all the child datasets\n    recursive_all : boolean\n        recursively destroy all dependents, including cloned file systems\n        outside the target hierarchy. (-R)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_padding(builder, layer, input_names, output_names, keras_layer):\n    \"\"\"\n    \n    \"\"\"\n    _check_data_format(keras_layer)\n    # Get input and output names\n    input_name, output_name = (input_names[0], output_names[0])\n\n    is_1d = isinstance(keras_layer, _keras.layers.ZeroPadding1D)\n\n    padding = keras_layer.padding\n    top = left = bottom = right = 0\n    if is_1d:\n        if type(padding) is int:\n            left = right = padding\n        elif type(padding) is tuple:\n            if type(padding[0]) is int:\n                left, right = padding\n            elif type(padding[0]) is tuple and len(padding[0]) == 2:\n                left, right = padding[0]\n            else:\n                raise ValueError(\"Unrecognized padding option: %s\" % (str(padding)))\n        else:\n            raise ValueError(\"Unrecognized padding option: %s\" % (str(padding)))\n    else:\n        if type(padding) is int:\n            top = left = bottom = right = padding\n        elif type(padding) is tuple:\n            if type(padding[0]) is int:\n                top, left = padding\n                bottom, right = padding\n            elif type(padding[0]) is tuple:\n                top, bottom = padding[0]\n                left, right = padding[1]\n            else:\n                raise ValueError(\"Unrecognized padding option: %s\" % (str(padding)))\n        else:\n            raise ValueError(\"Unrecognized padding option: %s\" % (str(padding)))\n\n    # Now add the layer\n    builder.add_padding(name = layer,\n        left = left, right=right, top=top, bottom=bottom, value = 0,\n        input_name = input_name, output_name=output_name\n        )", "output": "Convert padding layer from keras to coreml.\n    Keras only supports zero padding at this time.\n    Parameters\n    ----------\n    keras_layer: layer\n        A keras layer object.\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_images(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_images function must be called with '\n            '-f or --function, or with the --list-images option.'\n        )\n\n    templates = {}\n    vm_properties = [\n        \"name\",\n        \"config.template\",\n        \"config.guestFullName\",\n        \"config.hardware.numCPU\",\n        \"config.hardware.memoryMB\"\n    ]\n\n    vm_list = salt.utils.vmware.get_mors_with_properties(_get_si(), vim.VirtualMachine, vm_properties)\n\n    for vm in vm_list:\n        if \"config.template\" in vm and vm[\"config.template\"]:\n            templates[vm[\"name\"]] = {\n                'name': vm[\"name\"],\n                'guest_fullname': vm[\"config.guestFullName\"] if \"config.guestFullName\" in vm else \"N/A\",\n                'cpus': vm[\"config.hardware.numCPU\"] if \"config.hardware.numCPU\" in vm else \"N/A\",\n                'ram': vm[\"config.hardware.memoryMB\"] if \"config.hardware.memoryMB\" in vm else \"N/A\"\n            }\n\n    return templates", "output": "Return a list of all the templates present in this VMware environment with basic\n    details\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud --list-images my-vmware-config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_account(self, account):\n        ''\n        if account.account_cookie not in self.account_list:\n            if self.cash_available > account.init_cash:\n                account.portfolio_cookie = self.portfolio_cookie\n                account.user_cookie = self.user_cookie\n                self.cash.append(self.cash_available - account.init_cash)\n                self.account_list.append(account.account_cookie)\n                account.save()\n                return account\n        else:\n            pass", "output": "portfolio add a account/stratetgy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_user_gnupghome(user):\n    '''\n    \n    '''\n    if user == 'salt':\n        gnupghome = os.path.join(__salt__['config.get']('config_dir'), 'gpgkeys')\n    else:\n        gnupghome = os.path.join(_get_user_info(user)['home'], '.gnupg')\n\n    return gnupghome", "output": "Return default GnuPG home directory path for a user", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_dups(head):\n    \"\"\"\n    \n    \"\"\"\n    hashset = set()\n    prev = Node()\n    while head:\n        if head.val in hashset:\n            prev.next = head.next\n        else:\n            hashset.add(head.val)\n            prev = head\n        head = head.next", "output": "Time Complexity: O(N)\n    Space Complexity: O(N)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unpause_trial(self, trial):\n        \"\"\"\"\"\"\n        assert trial.status == Trial.PAUSED, trial.status\n        self.set_status(trial, Trial.PENDING)", "output": "Sets PAUSED trial to pending to allow scheduler to start.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_history(self, directory):\r\n        \"\"\"\"\"\"\r\n        try:\r\n            directory = osp.abspath(to_text_string(directory))\r\n            if directory in self.history:\r\n                self.histindex = self.history.index(directory)\r\n        except Exception:\r\n            user_directory = get_home_dir()\r\n            self.chdir(directory=user_directory, browsing_history=True)", "output": "Update browse history", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(cont=None, path=None, local_file=None, return_bin=False, profile=None):\n    '''\n    \n\n    '''\n    swift_conn = _auth(profile)\n\n    if cont is None:\n        return swift_conn.get_account()\n\n    if path is None:\n        return swift_conn.get_container(cont)\n\n    if return_bin is True:\n        return swift_conn.get_object(cont, path, return_bin)\n\n    if local_file is not None:\n        return swift_conn.get_object(cont, path, local_file)\n\n    return False", "output": "List the contents of a container, or return an object from a container. Set\n    return_bin to True in order to retrieve an object wholesale. Otherwise,\n    Salt will attempt to parse an XML response.\n\n    CLI Example to list containers:\n\n    .. code-block:: bash\n\n        salt myminion swift.get\n\n    CLI Example to list the contents of a container:\n\n    .. code-block:: bash\n\n        salt myminion swift.get mycontainer\n\n    CLI Example to return the binary contents of an object:\n\n    .. code-block:: bash\n\n        salt myminion swift.get mycontainer myfile.png return_bin=True\n\n    CLI Example to save the binary contents of an object to a local file:\n\n    .. code-block:: bash\n\n        salt myminion swift.get mycontainer myfile.png local_file=/tmp/myfile.png", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def server_delete(s_name, **connection_args):\n    '''\n    \n    '''\n    ret = True\n    server = _server_get(s_name, **connection_args)\n    if server is None:\n        return False\n    nitro = _connect(**connection_args)\n    if nitro is None:\n        return False\n    try:\n        NSServer.delete(nitro, server)\n    except NSNitroError as error:\n        log.debug('netscaler module error - NSServer.delete() failed: %s', error)\n        ret = False\n    _disconnect(nitro)\n    return ret", "output": "Delete a server\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' netscaler.server_delete 'serverName'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mapPartitions(self, f, preservesPartitioning=False):\n        \"\"\"\n        \n        \"\"\"\n        def func(s, iterator):\n            return f(iterator)\n        return self.mapPartitionsWithIndex(func, preservesPartitioning)", "output": "Return a new RDD by applying a function to each partition of this RDD.\n\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n        >>> def f(iterator): yield sum(iterator)\n        >>> rdd.mapPartitions(f).collect()\n        [3, 7]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unread_data(self, data: bytes) -> None:\n        \"\"\" \n        \"\"\"\n        warnings.warn(\"unread_data() is deprecated \"\n                      \"and will be removed in future releases (#3260)\",\n                      DeprecationWarning,\n                      stacklevel=2)\n        if not data:\n            return\n\n        if self._buffer_offset:\n            self._buffer[0] = self._buffer[0][self._buffer_offset:]\n            self._buffer_offset = 0\n        self._size += len(data)\n        self._cursor -= len(data)\n        self._buffer.appendleft(data)\n        self._eof_counter = 0", "output": "rollback reading some data from stream, inserting it to buffer head.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reverse_url(self, name: str, *args: Any) -> str:\n        \"\"\"\"\"\"\n        return self.application.reverse_url(name, *args)", "output": "Alias for `Application.reverse_url`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_mixed_type_key(obj):\n    \"\"\"\n\n    \"\"\"\n    if isinstance(obj, _BaseNetwork):\n        return obj._get_networks_key()\n    elif isinstance(obj, _BaseAddress):\n        return obj._get_address_key()\n    return NotImplemented", "output": "Return a key suitable for sorting between networks and addresses.\n\n    Address and Network objects are not sortable by default; they're\n    fundamentally different so the expression\n\n        IPv4Address('192.0.2.0') <= IPv4Network('192.0.2.0/24')\n\n    doesn't make any sense.  There are some times however, where you may wish\n    to have ipaddress sort these for you anyway. If you need to do this, you\n    can use this function as the key= argument to sorted().\n\n    Args:\n      obj: either a Network or Address object.\n    Returns:\n      appropriate key.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def group(self, *args, **kwargs):\n        \"\"\"\n        \"\"\"\n        def decorator(f):\n            cmd = group(*args, **kwargs)(f)\n            self.add_command(cmd)\n            return cmd\n        return decorator", "output": "A shortcut decorator for declaring and attaching a group to\n        the group.  This takes the same arguments as :func:`group` but\n        immediately registers the created command with this instance by\n        calling into :meth:`add_command`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_nested_object(obj):\n    \"\"\"\n    \n\n    \"\"\"\n\n    if isinstance(obj, ABCSeries) and is_object_dtype(obj):\n\n        if any(isinstance(v, ABCSeries) for v in obj.values):\n            return True\n\n    return False", "output": "return a boolean if we have a nested object, e.g. a Series with 1 or\n    more Series elements\n\n    This may not be necessarily be performant.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hash_tuples(vals, encoding='utf8', hash_key=None):\n    \"\"\"\n    \n    \"\"\"\n    is_tuple = False\n    if isinstance(vals, tuple):\n        vals = [vals]\n        is_tuple = True\n    elif not is_list_like(vals):\n        raise TypeError(\"must be convertible to a list-of-tuples\")\n\n    from pandas import Categorical, MultiIndex\n\n    if not isinstance(vals, ABCMultiIndex):\n        vals = MultiIndex.from_tuples(vals)\n\n    # create a list-of-Categoricals\n    vals = [Categorical(vals.codes[level],\n                        vals.levels[level],\n                        ordered=False,\n                        fastpath=True)\n            for level in range(vals.nlevels)]\n\n    # hash the list-of-ndarrays\n    hashes = (_hash_categorical(cat,\n                                encoding=encoding,\n                                hash_key=hash_key)\n              for cat in vals)\n    h = _combine_hash_arrays(hashes, len(vals))\n    if is_tuple:\n        h = h[0]\n\n    return h", "output": "Hash an MultiIndex / list-of-tuples efficiently\n\n    .. versionadded:: 0.20.0\n\n    Parameters\n    ----------\n    vals : MultiIndex, list-of-tuples, or single tuple\n    encoding : string, default 'utf8'\n    hash_key : string key to encode, default to _default_hash_key\n\n    Returns\n    -------\n    ndarray of hashed values array", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_backward_begin(self, last_loss, last_output, **kwargs):\n        \"\"\n        last_loss = last_loss.detach().cpu()\n        if self.gen_mode:\n            self.smoothenerG.add_value(last_loss)\n            self.glosses.append(self.smoothenerG.smooth)\n            self.last_gen = last_output.detach().cpu()\n        else:\n            self.smoothenerC.add_value(last_loss)\n            self.closses.append(self.smoothenerC.smooth)", "output": "Record `last_loss` in the proper list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_logical(self, rule, field, value):\n        \"\"\"  \"\"\"\n        if not isinstance(value, Sequence):\n            self._error(field, errors.BAD_TYPE)\n            return\n\n        validator = self._get_child_validator(\n            document_crumb=rule, allow_unknown=False,\n            schema=self.target_validator.validation_rules)\n\n        for constraints in value:\n            _hash = (mapping_hash({'turing': constraints}),\n                     mapping_hash(self.target_validator.types_mapping))\n            if _hash in self.target_validator._valid_schemas:\n                continue\n\n            validator(constraints, normalize=False)\n            if validator._errors:\n                self._error(validator._errors)\n            else:\n                self.target_validator._valid_schemas.add(_hash)", "output": "{'allowed': ('allof', 'anyof', 'noneof', 'oneof')}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_from(self, parent_path):\n        \"\"\"\"\"\"\n        path_cls = type(parent_path)\n        is_dir = path_cls.is_dir\n        exists = path_cls.exists\n        scandir = parent_path._accessor.scandir\n        if not is_dir(parent_path):\n            return iter([])\n        return self._select_from(parent_path, is_dir, exists, scandir)", "output": "Iterate over all child paths of `parent_path` matched by this\n        selector.  This can contain parent_path itself.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_cond_latents_at_level(cond_latents, level, hparams):\n  \"\"\"\"\"\"\n  if cond_latents:\n    if hparams.latent_dist_encoder in [\"conv_net\", \"conv3d_net\"]:\n      return [cond_latent[level] for cond_latent in cond_latents]\n    elif hparams.latent_dist_encoder in [\"pointwise\", \"conv_lstm\"]:\n      return cond_latents[level]", "output": "Returns a single or list of conditional latents at level 'level'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def posix_rename(self, oldpath, newpath):\n        \"\"\"\n        \n        \"\"\"\n        oldpath = self._adjust_cwd(oldpath)\n        newpath = self._adjust_cwd(newpath)\n        self._log(DEBUG, \"posix_rename({!r}, {!r})\".format(oldpath, newpath))\n        self._request(\n            CMD_EXTENDED, \"posix-rename@openssh.com\", oldpath, newpath\n        )", "output": "Rename a file or folder from ``oldpath`` to ``newpath``, following\n        posix conventions.\n\n        :param str oldpath: existing name of the file or folder\n        :param str newpath: new name for the file or folder, will be\n            overwritten if it already exists\n\n        :raises:\n            ``IOError`` -- if ``newpath`` is a folder, posix-rename is not\n            supported by the server or something else goes wrong\n\n        :versionadded: 2.2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def assign_default_storage_policy_to_datastore(policy, datastore,\n                                               service_instance=None):\n    '''\n    \n    '''\n    log.trace('Assigning policy %s to datastore %s', policy, datastore)\n    profile_manager = salt.utils.pbm.get_profile_manager(service_instance)\n    # Find policy\n    policies = salt.utils.pbm.get_storage_policies(profile_manager, [policy])\n    if not policies:\n        raise VMwareObjectRetrievalError('Policy \\'{0}\\' was not found'\n                                         ''.format(policy))\n    policy_ref = policies[0]\n    # Find datastore\n    target_ref = _get_proxy_target(service_instance)\n    ds_refs = salt.utils.vmware.get_datastores(service_instance, target_ref,\n                                               datastore_names=[datastore])\n    if not ds_refs:\n        raise VMwareObjectRetrievalError('Datastore \\'{0}\\' was not '\n                                         'found'.format(datastore))\n    ds_ref = ds_refs[0]\n    salt.utils.pbm.assign_default_storage_policy_to_datastore(\n        profile_manager, policy_ref, ds_ref)\n    return True", "output": "Assigns a storage policy as the default policy to a datastore.\n\n    policy\n        Name of the policy to assign.\n\n    datastore\n        Name of the datastore to assign.\n        The datastore needs to be visible to the VMware entity the proxy\n        points to.\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.assign_storage_policy_to_datastore\n            policy='policy name' datastore=ds1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def connect_host(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The connect_host function must be called with '\n            '-f or --function.'\n        )\n\n    host_name = kwargs.get('host') if kwargs and 'host' in kwargs else None\n\n    if not host_name:\n        raise SaltCloudSystemExit(\n            'You must specify name of the host system.'\n        )\n\n    # Get the service instance\n    si = _get_si()\n\n    host_ref = salt.utils.vmware.get_mor_by_property(si, vim.HostSystem, host_name)\n    if not host_ref:\n        raise SaltCloudSystemExit(\n            'Specified host system does not exist.'\n        )\n\n    if host_ref.runtime.connectionState == 'connected':\n        return {host_name: 'host system already connected'}\n\n    try:\n        task = host_ref.ReconnectHost_Task()\n        salt.utils.vmware.wait_for_task(task, host_name, 'connect host', 5, 'info')\n    except Exception as exc:\n        log.error(\n            'Error while connecting host %s: %s',\n            host_name, exc,\n            # Show the traceback if the debug logging level is enabled\n            exc_info_on_loglevel=logging.DEBUG\n        )\n        return {host_name: 'failed to connect host'}\n\n    return {host_name: 'connected host'}", "output": "Connect the specified host system in this VMware environment\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f connect_host my-vmware-config host=\"myHostSystemName\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_environment(self, environ):\n        \"\"\"\"\"\"\n        if environ:\n            q_environ = self._process.processEnvironment()\n            for k, v in environ.items():\n                q_environ.insert(k, v)\n            self._process.setProcessEnvironment(q_environ)", "output": "Set the environment on the QProcess.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_comparison_method(cls, op):\n        \"\"\"\n        \n        \"\"\"\n        def wrapper(self, other):\n            if isinstance(other, ABCSeries):\n                # the arrays defer to Series for comparison ops but the indexes\n                #  don't, so we have to unwrap here.\n                other = other._values\n\n            result = op(self._data, maybe_unwrap_index(other))\n            return result\n\n        wrapper.__doc__ = op.__doc__\n        wrapper.__name__ = '__{}__'.format(op.__name__)\n        return wrapper", "output": "Create a comparison method that dispatches to ``cls.values``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(name, quiet=False, path=None):\n    '''\n    \n    '''\n    data = _do_names(name, 'start', path=path)\n    if data and not quiet:\n        __jid_event__.fire_event(\n            {'data': data, 'outputter': 'lxc_start'}, 'progress')\n    return data", "output": "Start the named container.\n\n    path\n        path to the container parent\n        default: /var/lib/lxc (system default)\n\n        .. versionadded:: 2015.8.0\n\n    .. code-block:: bash\n\n        salt-run lxc.start name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bleu_wrapper(ref_filename, hyp_filename, case_sensitive=False):\n  \"\"\"\"\"\"\n  ref_lines = text_encoder.native_to_unicode(\n      tf.gfile.Open(ref_filename, \"r\").read()).split(\"\\n\")\n  hyp_lines = text_encoder.native_to_unicode(\n      tf.gfile.Open(hyp_filename, \"r\").read()).split(\"\\n\")\n  assert len(ref_lines) == len(hyp_lines), (\"{} != {}\".format(\n      len(ref_lines), len(hyp_lines)))\n  if not case_sensitive:\n    ref_lines = [x.lower() for x in ref_lines]\n    hyp_lines = [x.lower() for x in hyp_lines]\n  ref_tokens = [bleu_tokenize(x) for x in ref_lines]\n  hyp_tokens = [bleu_tokenize(x) for x in hyp_lines]\n  return compute_bleu(ref_tokens, hyp_tokens)", "output": "Compute BLEU for two files (reference and hypothesis translation).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_checkpoint(model_dir, timeout_mins=240):\n  \"\"\"\n  \"\"\"\n  last_ckpt = None\n  timeout_secs = None\n  if timeout_mins != -1:\n    timeout_secs = timeout_mins * 60\n  while True:\n    last_ckpt = tf.contrib.training.wait_for_new_checkpoint(\n        model_dir, last_ckpt, seconds_to_sleep=60, timeout=timeout_secs)\n\n    if last_ckpt is None:\n      tf.logging.info(\n          \"Eval timeout: no new checkpoints within %dm\" % timeout_mins)\n      break\n\n    yield last_ckpt", "output": "Yields successive checkpoints from model_dir.\n\n  Args:\n    model_dir: The directory in which checkpoints are saved.\n    timeout_mins: The maximum amount of time in minutes to wait\n                  between checkpoints. Set this to -1 to wait indefinitely.\n  Yields:\n    last_ckpt: a new checkpoint path, or None if the timeout was reached.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_dns(ip, interface='Local Area Connection', index=1):\n    '''\n    \n    '''\n    servers = get_dns_servers(interface)\n\n    # Return False if could not find the interface\n    if servers is False:\n        return False\n\n    # Return true if configured\n    try:\n        if servers[index - 1] == ip:\n            return True\n    except IndexError:\n        pass\n\n    # If configured in the wrong order delete it\n    if ip in servers:\n        rm_dns(ip, interface)\n\n    cmd = ['netsh', 'interface', 'ip', 'add', 'dns',\n           interface, ip, 'index={0}'.format(index), 'validate=no']\n\n    return __salt__['cmd.retcode'](cmd, python_shell=False) == 0", "output": "Add the DNS server to the network interface\n    (index starts from 1)\n\n    Note: if the interface DNS is configured by DHCP, all the DNS servers will\n    be removed from the interface and the requested DNS will be the only one\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' win_dns_client.add_dns <ip> <interface> <index>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_states(self, states):\n        \"\"\"\"\"\"\n        states = pickle.loads(states)\n        if isinstance(states, tuple) and len(states) == 2:\n            self.states, self.optimizer = states\n        else:\n            self.states = states\n        self.states_synced = dict.fromkeys(self.states.keys(), False)", "output": "Sets updater states.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_unique_tags(tag_list):\n  \"\"\"\"\"\"\n  frozen_tags_seen = set()\n  for tags in tag_list:\n    frozen_tags = frozenset(tags)\n    if frozen_tags in frozen_tags_seen:\n      raise ValueError(\"Tags %r used repeatedly\" % tags)\n    frozen_tags_seen.add(frozen_tags)", "output": "Checks that tag list contains each set of tags only once.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_order_params(self,\n                              asset,\n                              amount,\n                              limit_price,\n                              stop_price,\n                              style):\n        \"\"\"\n        \n        \"\"\"\n\n        if not self.initialized:\n            raise OrderDuringInitialize(\n                msg=\"order() can only be called from within handle_data()\"\n            )\n\n        if style:\n            if limit_price:\n                raise UnsupportedOrderParameters(\n                    msg=\"Passing both limit_price and style is not supported.\"\n                )\n\n            if stop_price:\n                raise UnsupportedOrderParameters(\n                    msg=\"Passing both stop_price and style is not supported.\"\n                )\n\n        for control in self.trading_controls:\n            control.validate(asset,\n                             amount,\n                             self.portfolio,\n                             self.get_datetime(),\n                             self.trading_client.current_data)", "output": "Helper method for validating parameters to the order API function.\n\n        Raises an UnsupportedOrderParameters if invalid arguments are found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        '''\n        \n        '''\n        import salt.runner\n        self.parse_args()\n\n        # Setup file logging!\n        self.setup_logfile_logger()\n        verify_log(self.config)\n        profiling_enabled = self.options.profiling_enabled\n\n        runner = salt.runner.Runner(self.config)\n        if self.options.doc:\n            runner.print_docs()\n            self.exit(salt.defaults.exitcodes.EX_OK)\n\n        # Run this here so SystemExit isn't raised anywhere else when\n        # someone tries to use the runners via the python API\n        try:\n            if check_user(self.config['user']):\n                pr = salt.utils.profile.activate_profile(profiling_enabled)\n                try:\n                    ret = runner.run()\n                    # In older versions ret['data']['retcode'] was used\n                    # for signaling the return code. This has been\n                    # changed for the orchestrate runner, but external\n                    # runners might still use it. For this reason, we\n                    # also check ret['data']['retcode'] if\n                    # ret['retcode'] is not available.\n                    if isinstance(ret, dict) and 'retcode' in ret:\n                        self.exit(ret['retcode'])\n                    elif isinstance(ret, dict) and 'retcode' in ret.get('data', {}):\n                        self.exit(ret['data']['retcode'])\n                finally:\n                    salt.utils.profile.output_profile(\n                        pr,\n                        stats_path=self.options.profiling_path,\n                        stop=True)\n\n        except SaltClientError as exc:\n            raise SystemExit(six.text_type(exc))", "output": "Execute salt-run", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deviation(self, series, start, limit, mean):\n        '''\n        \n        '''\n        d = []\n        for x in range(start, limit):\n            d.append(float(series[x] - mean))\n        return d", "output": ":type start: int\n        :type limit: int\n        :type mean: int\n        :rtype: list()", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_bytes_list(cls, function_descriptor_list):\n        \"\"\"\n        \"\"\"\n        assert isinstance(function_descriptor_list, list)\n        if len(function_descriptor_list) == 0:\n            # This is a function descriptor of driver task.\n            return FunctionDescriptor.for_driver_task()\n        elif (len(function_descriptor_list) == 3\n              or len(function_descriptor_list) == 4):\n            module_name = ensure_str(function_descriptor_list[0])\n            class_name = ensure_str(function_descriptor_list[1])\n            function_name = ensure_str(function_descriptor_list[2])\n            if len(function_descriptor_list) == 4:\n                return cls(module_name, function_name, class_name,\n                           function_descriptor_list[3])\n            else:\n                return cls(module_name, function_name, class_name)\n        else:\n            raise Exception(\n                \"Invalid input for FunctionDescriptor.from_bytes_list\")", "output": "Create a FunctionDescriptor instance from list of bytes.\n\n        This function is used to create the function descriptor from\n        backend data.\n\n        Args:\n            cls: Current class which is required argument for classmethod.\n            function_descriptor_list: list of bytes to represent the\n                function descriptor.\n\n        Returns:\n            The FunctionDescriptor instance created from the bytes list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _lstrip_word(word, prefix):\n    '''\n    \n    '''\n\n    if six.text_type(word).startswith(prefix):\n        return six.text_type(word)[len(prefix):]\n    return word", "output": "Return a copy of the string after the specified prefix was removed\n    from the beginning of the string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_util_get_trade_gap(start, end):\n    ''\n    start, end = QA_util_get_real_datelist(start, end)\n    if start is not None:\n        return trade_date_sse.index(end) + 1 - trade_date_sse.index(start)\n    else:\n        return 0", "output": "\u8fd4\u56destart_day\u5230end_day\u4e2d\u95f4\u6709\u591a\u5c11\u4e2a\u4ea4\u6613\u5929 \u7b97\u9996\u5c3e", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toVector(value):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(value, Vector):\n            return value\n        elif TypeConverters._can_convert_to_list(value):\n            value = TypeConverters.toList(value)\n            if all(map(lambda v: TypeConverters._is_numeric(v), value)):\n                return DenseVector(value)\n        raise TypeError(\"Could not convert %s to vector\" % value)", "output": "Convert a value to a MLlib Vector, if possible.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def has_subdirectories(path, include, exclude, show_all):\r\n    \"\"\"\"\"\"\r\n    try:\r\n        # > 1 because of '..'\r\n        return len( listdir(path, include, exclude,\r\n                            show_all, folders_only=True) ) > 1\r\n    except (IOError, OSError):\r\n        return False", "output": "Return True if path has subdirectories", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def push_note(device=None, title=None, body=None):\n    '''\n    \n    '''\n    spb = _SaltPushbullet(device)\n    res = spb.push_note(title, body)\n\n    return res", "output": "Pushing a text note.\n\n    :param device:   Pushbullet target device\n    :param title:    Note title\n    :param body:     Note body\n\n    :return:            Boolean if message was sent successfully.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt \"*\" pushbullet.push_note device=\"Chrome\" title=\"Example title\" body=\"Example body.\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_secret(\n        name,\n        namespace='default',\n        data=None,\n        source=None,\n        template=None,\n        saltenv='base',\n        **kwargs):\n    '''\n    \n    '''\n    if source:\n        data = __read_and_render_yaml_file(source, template, saltenv)\n    elif data is None:\n        data = {}\n\n    data = __enforce_only_strings_dict(data)\n\n    # encode the secrets using base64 as required by kubernetes\n    for key in data:\n        data[key] = base64.b64encode(data[key])\n\n    body = kubernetes.client.V1Secret(\n        metadata=__dict_to_object_meta(name, namespace, {}),\n        data=data)\n\n    cfg = _setup_conn(**kwargs)\n\n    try:\n        api_instance = kubernetes.client.CoreV1Api()\n        api_response = api_instance.create_namespaced_secret(\n            namespace, body)\n\n        return api_response.to_dict()\n    except (ApiException, HTTPError) as exc:\n        if isinstance(exc, ApiException) and exc.status == 404:\n            return None\n        else:\n            log.exception(\n                'Exception when calling '\n                'CoreV1Api->create_namespaced_secret'\n            )\n            raise CommandExecutionError(exc)\n    finally:\n        _cleanup(**cfg)", "output": "Creates the kubernetes secret as defined by the user.\n\n    CLI Examples::\n\n        salt 'minion1' kubernetes.create_secret \\\n            passwords default '{\"db\": \"letmein\"}'\n\n        salt 'minion2' kubernetes.create_secret \\\n            name=passwords namespace=default data='{\"db\": \"letmein\"}'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _pipe(obj, func, *args, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(func, tuple):\n        func, target = func\n        if target in kwargs:\n            msg = '%s is both the pipe target and a keyword argument' % target\n            raise ValueError(msg)\n        kwargs[target] = obj\n        return func(*args, **kwargs)\n    else:\n        return func(obj, *args, **kwargs)", "output": "Apply a function ``func`` to object ``obj`` either by passing obj as the\n    first argument to the function or, in the case that the func is a tuple,\n    interpret the first element of the tuple as a function and pass the obj to\n    that function as a keyword argument whose key is the value of the second\n    element of the tuple.\n\n    Parameters\n    ----------\n    func : callable or tuple of (callable, string)\n        Function to apply to this object or, alternatively, a\n        ``(callable, data_keyword)`` tuple where ``data_keyword`` is a\n        string indicating the keyword of `callable`` that expects the\n        object.\n    args : iterable, optional\n        positional arguments passed into ``func``.\n    kwargs : dict, optional\n        a dictionary of keyword arguments passed into ``func``.\n\n    Returns\n    -------\n    object : the return type of ``func``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def PopupGetText(message, title=None, default_text='', password_char='', size=(None, None), button_color=None,\n                 background_color=None, text_color=None, icon=DEFAULT_WINDOW_ICON, font=None, no_titlebar=False,\n                 grab_anywhere=False, keep_on_top=False, location=(None, None)):\n    \"\"\"\n    \n    \"\"\"\n\n    layout = [[Text(message, auto_size_text=True, text_color=text_color, background_color=background_color, font=font)],\n              [InputText(default_text=default_text, size=size, password_char=password_char)],\n              [CloseButton('Ok', size=(60, 20), bind_return_key=True), CloseButton('Cancel', size=(60, 20))]]\n\n    _title = title if title is not None else message\n\n    window = Window(title=_title, icon=icon, auto_size_text=True, button_color=button_color, no_titlebar=no_titlebar,\n                    background_color=background_color, grab_anywhere=grab_anywhere, keep_on_top=keep_on_top,\n                    location=location)\n\n    (button, input_values) = window.Layout(layout).Read()\n\n    if button != 'Ok':\n        return None\n    else:\n        return input_values[0]", "output": "Display Popup with text entry field\n    :param message:\n    :param default_text:\n    :param password_char:\n    :param size:\n    :param button_color:\n    :param background_color:\n    :param text_color:\n    :param icon:\n    :param font:\n    :param no_titlebar:\n    :param grab_anywhere:\n    :param keep_on_top:\n    :param location:\n    :return: Text entered or None if window was closed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compress_for_rename(paths):\n    \"\"\"\n    \"\"\"\n    case_map = dict((os.path.normcase(p), p) for p in paths)\n    remaining = set(case_map)\n    unchecked = sorted(set(os.path.split(p)[0]\n                           for p in case_map.values()), key=len)\n    wildcards = set()\n\n    def norm_join(*a):\n        return os.path.normcase(os.path.join(*a))\n\n    for root in unchecked:\n        if any(os.path.normcase(root).startswith(w)\n               for w in wildcards):\n            # This directory has already been handled.\n            continue\n\n        all_files = set()\n        all_subdirs = set()\n        for dirname, subdirs, files in os.walk(root):\n            all_subdirs.update(norm_join(root, dirname, d)\n                               for d in subdirs)\n            all_files.update(norm_join(root, dirname, f)\n                             for f in files)\n        # If all the files we found are in our remaining set of files to\n        # remove, then remove them from the latter set and add a wildcard\n        # for the directory.\n        if not (all_files - remaining):\n            remaining.difference_update(all_files)\n            wildcards.add(root + os.sep)\n\n    return set(map(case_map.__getitem__, remaining)) | wildcards", "output": "Returns a set containing the paths that need to be renamed.\n\n    This set may include directories when the original sequence of paths\n    included every file on disk.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetch_defense_data(self):\n    \"\"\"\"\"\"\n    if self.defenses_data_initialized:\n      return\n    logging.info('Fetching defense data from datastore')\n    # init data from datastore\n    self.submissions.init_from_datastore()\n    self.dataset_batches.init_from_datastore()\n    self.adv_batches.init_from_datastore()\n    # read dataset metadata\n    self.read_dataset_metadata()\n    # mark as initialized\n    self.defenses_data_initialized = True", "output": "Lazy initialization of data necessary to execute defenses.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def createFileParserCtxt(filename):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlCreateFileParserCtxt(filename)\n    if ret is None:raise parserError('xmlCreateFileParserCtxt() failed')\n    return parserCtxt(_obj=ret)", "output": "Create a parser context for a file content. Automatic\n      support for ZLIB/Compress compressed document is provided\n       by default if found at compile-time.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def value_opt_step(i,\n                   opt_state,\n                   opt_update,\n                   value_net_apply,\n                   padded_observations,\n                   padded_rewards,\n                   reward_mask,\n                   gamma=0.99):\n  \"\"\"\"\"\"\n  value_params = trax_opt.get_params(opt_state)\n  # Note this partial application here and argnums above in ppo_opt_step.\n  g = grad(functools.partial(value_loss, value_net_apply))(\n      value_params,\n      padded_observations,\n      padded_rewards,\n      reward_mask,\n      gamma=gamma)\n  return opt_update(i, g, opt_state)", "output": "Value optimizer step.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def state_apply(state_name, **kwargs):\n    '''\n    \n    '''\n    # A new salt client is instantiated with the default configuration because the main module's\n    # client is hardcoded to local\n    # If the minion is running with a master, a non-local client is needed to lookup states\n    caller = salt.client.Caller()\n    if kwargs:\n        return caller.cmd('state.apply', state_name, **kwargs)\n    else:\n        return caller.cmd('state.apply', state_name)", "output": "Runs :py:func:`state.apply <salt.modules.state.apply>` with given options to set up test data.\n    Intended to be used for optional test setup or teardown\n\n    Reference the :py:func:`state.apply <salt.modules.state.apply>` module documentation for arguments and usage options\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltcheck.state_apply postfix", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_pax_header(self, info, encoding):\n        \"\"\"\n        \"\"\"\n        info[\"magic\"] = POSIX_MAGIC\n        pax_headers = self.pax_headers.copy()\n\n        # Test string fields for values that exceed the field length or cannot\n        # be represented in ASCII encoding.\n        for name, hname, length in (\n                (\"name\", \"path\", LENGTH_NAME), (\"linkname\", \"linkpath\", LENGTH_LINK),\n                (\"uname\", \"uname\", 32), (\"gname\", \"gname\", 32)):\n\n            if hname in pax_headers:\n                # The pax header has priority.\n                continue\n\n            # Try to encode the string as ASCII.\n            try:\n                info[name].encode(\"ascii\", \"strict\")\n            except UnicodeEncodeError:\n                pax_headers[hname] = info[name]\n                continue\n\n            if len(info[name]) > length:\n                pax_headers[hname] = info[name]\n\n        # Test number fields for values that exceed the field limit or values\n        # that like to be stored as float.\n        for name, digits in ((\"uid\", 8), (\"gid\", 8), (\"size\", 12), (\"mtime\", 12)):\n            if name in pax_headers:\n                # The pax header has priority. Avoid overflow.\n                info[name] = 0\n                continue\n\n            val = info[name]\n            if not 0 <= val < 8 ** (digits - 1) or isinstance(val, float):\n                pax_headers[name] = str(val)\n                info[name] = 0\n\n        # Create a pax extended header if necessary.\n        if pax_headers:\n            buf = self._create_pax_generic_header(pax_headers, XHDTYPE, encoding)\n        else:\n            buf = b\"\"\n\n        return buf + self._create_header(info, USTAR_FORMAT, \"ascii\", \"replace\")", "output": "Return the object as a ustar header block. If it cannot be\n           represented this way, prepend a pax extended header sequence\n           with supplement information.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_trial_complete(self,\n                          trial_id,\n                          result=None,\n                          error=False,\n                          early_terminated=False):\n        \"\"\"\n        \"\"\"\n        ho_trial = self._get_hyperopt_trial(trial_id)\n        if ho_trial is None:\n            return\n        ho_trial[\"refresh_time\"] = hpo.utils.coarse_utcnow()\n        if error:\n            ho_trial[\"state\"] = hpo.base.JOB_STATE_ERROR\n            ho_trial[\"misc\"][\"error\"] = (str(TuneError), \"Tune Error\")\n        elif early_terminated:\n            ho_trial[\"state\"] = hpo.base.JOB_STATE_ERROR\n            ho_trial[\"misc\"][\"error\"] = (str(TuneError), \"Tune Removed\")\n        else:\n            ho_trial[\"state\"] = hpo.base.JOB_STATE_DONE\n            hp_result = self._to_hyperopt_result(result)\n            ho_trial[\"result\"] = hp_result\n        self._hpopt_trials.refresh()\n        del self._live_trial_mapping[trial_id]", "output": "Passes the result to HyperOpt unless early terminated or errored.\n\n        The result is internally negated when interacting with HyperOpt\n        so that HyperOpt can \"maximize\" this value, as it minimizes on default.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_valid_label(self, label):\n        \"\"\"\"\"\"\n        if len(label.shape) != 2 or label.shape[1] < 5:\n            msg = \"Label with shape (1+, 5+) required, %s received.\" % str(label)\n            raise RuntimeError(msg)\n        valid_label = np.where(np.logical_and(label[:, 0] >= 0, label[:, 3] > label[:, 1],\n                                              label[:, 4] > label[:, 2]))[0]\n        if valid_label.size < 1:\n            raise RuntimeError('Invalid label occurs.')", "output": "Validate label and its shape.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def label_from_re(self, pat:str, full_path:bool=False, label_cls:Callable=None, **kwargs)->'LabelList':\n        \"\"\n        pat = re.compile(pat)\n        def _inner(o):\n            s = str((os.path.join(self.path,o) if full_path else o).as_posix())\n            res = pat.search(s)\n            assert res,f'Failed to find \"{pat}\" in \"{s}\"'\n            return res.group(1)\n        return self.label_from_func(_inner, label_cls=label_cls, **kwargs)", "output": "Apply the re in `pat` to determine the label of every filename.  If `full_path`, search in the full name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy_dash(self, dashboard_id):\n        \"\"\"\"\"\"\n        session = db.session()\n        data = json.loads(request.form.get('data'))\n        dash = models.Dashboard()\n        original_dash = (\n            session\n            .query(models.Dashboard)\n            .filter_by(id=dashboard_id).first())\n\n        dash.owners = [g.user] if g.user else []\n        dash.dashboard_title = data['dashboard_title']\n\n        if data['duplicate_slices']:\n            # Duplicating slices as well, mapping old ids to new ones\n            old_to_new_sliceids = {}\n            for slc in original_dash.slices:\n                new_slice = slc.clone()\n                new_slice.owners = [g.user] if g.user else []\n                session.add(new_slice)\n                session.flush()\n                new_slice.dashboards.append(dash)\n                old_to_new_sliceids['{}'.format(slc.id)] = \\\n                    '{}'.format(new_slice.id)\n\n            # update chartId of layout entities\n            # in v2_dash positions json data, chartId should be integer,\n            # while in older version slice_id is string type\n            for value in data['positions'].values():\n                if (\n                    isinstance(value, dict) and value.get('meta') and\n                    value.get('meta').get('chartId')\n                ):\n                    old_id = '{}'.format(value.get('meta').get('chartId'))\n                    new_id = int(old_to_new_sliceids[old_id])\n                    value['meta']['chartId'] = new_id\n        else:\n            dash.slices = original_dash.slices\n        dash.params = original_dash.params\n\n        self._set_dash_metadata(dash, data)\n        session.add(dash)\n        session.commit()\n        dash_json = json.dumps(dash.data)\n        session.close()\n        return json_success(dash_json)", "output": "Copy dashboard", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setNNIManagerIp(experiment_config, port, config_file_name):\n    ''''''\n    if experiment_config.get('nniManagerIp') is None:\n        return True, None\n    ip_config_dict = dict()\n    ip_config_dict['nni_manager_ip'] = { 'nniManagerIp' : experiment_config['nniManagerIp'] }\n    response = rest_put(cluster_metadata_url(port), json.dumps(ip_config_dict), REST_TIME_OUT)\n    err_message = None\n    if not response or not response.status_code == 200:\n        if response is not None:\n            err_message = response.text\n            _, stderr_full_path = get_log_path(config_file_name)\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n        return False, err_message\n    return True, None", "output": "set nniManagerIp", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def preserve_channel_dim(func):\n    \"\"\"\"\"\"\n    @wraps(func)\n    def wrapped_function(img, *args, **kwargs):\n        shape = img.shape\n        result = func(img, *args, **kwargs)\n        if len(shape) == 3 and shape[-1] == 1 and len(result.shape) == 2:\n            result = np.expand_dims(result, axis=-1)\n        return result\n\n    return wrapped_function", "output": "Preserve dummy channel dim.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rename_client_tab(self, client, given_name):\r\n        \"\"\"\"\"\"\r\n        index = self.get_client_index_from_id(id(client))\r\n\r\n        if given_name is not None:\r\n            client.given_name = given_name\r\n        self.tabwidget.setTabText(index, client.get_name())", "output": "Rename client's tab", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_numeric_methods_binary(cls):\n        \"\"\"\n        \n        \"\"\"\n        cls.__add__ = _make_arithmetic_op(operator.add, cls)\n        cls.__radd__ = _make_arithmetic_op(ops.radd, cls)\n        cls.__sub__ = _make_arithmetic_op(operator.sub, cls)\n        cls.__rsub__ = _make_arithmetic_op(ops.rsub, cls)\n        cls.__rpow__ = _make_arithmetic_op(ops.rpow, cls)\n        cls.__pow__ = _make_arithmetic_op(operator.pow, cls)\n\n        cls.__truediv__ = _make_arithmetic_op(operator.truediv, cls)\n        cls.__rtruediv__ = _make_arithmetic_op(ops.rtruediv, cls)\n\n        # TODO: rmod? rdivmod?\n        cls.__mod__ = _make_arithmetic_op(operator.mod, cls)\n        cls.__floordiv__ = _make_arithmetic_op(operator.floordiv, cls)\n        cls.__rfloordiv__ = _make_arithmetic_op(ops.rfloordiv, cls)\n        cls.__divmod__ = _make_arithmetic_op(divmod, cls)\n        cls.__mul__ = _make_arithmetic_op(operator.mul, cls)\n        cls.__rmul__ = _make_arithmetic_op(ops.rmul, cls)", "output": "Add in numeric methods.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def masked_max(vector: torch.Tensor,\n               mask: torch.Tensor,\n               dim: int,\n               keepdim: bool = False,\n               min_val: float = -1e7) -> torch.Tensor:\n    \"\"\"\n    \n    \"\"\"\n    one_minus_mask = (1.0 - mask).byte()\n    replaced_vector = vector.masked_fill(one_minus_mask, min_val)\n    max_value, _ = replaced_vector.max(dim=dim, keepdim=keepdim)\n    return max_value", "output": "To calculate max along certain dimensions on masked values\n\n    Parameters\n    ----------\n    vector : ``torch.Tensor``\n        The vector to calculate max, assume unmasked parts are already zeros\n    mask : ``torch.Tensor``\n        The mask of the vector. It must be broadcastable with vector.\n    dim : ``int``\n        The dimension to calculate max\n    keepdim : ``bool``\n        Whether to keep dimension\n    min_val : ``float``\n        The minimal value for paddings\n\n    Returns\n    -------\n    A ``torch.Tensor`` of including the maximum values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_parameters(self, parameter_id):\n        \"\"\"\n        \n        \"\"\"\n        if not self.history:\n            self.init_search()\n\n        new_father_id = None\n        generated_graph = None\n        if not self.training_queue:\n            new_father_id, generated_graph = self.generate()\n            new_model_id = self.model_count\n            self.model_count += 1\n            self.training_queue.append((generated_graph, new_father_id, new_model_id))\n            self.descriptors.append(generated_graph.extract_descriptor())\n\n        graph, father_id, model_id = self.training_queue.pop(0)\n\n        # from graph to json\n        json_model_path = os.path.join(self.path, str(model_id) + \".json\")\n        json_out = graph_to_json(graph, json_model_path)\n        self.total_data[parameter_id] = (json_out, father_id, model_id)\n\n        return json_out", "output": "Returns a set of trial neural architecture, as a serializable object.\n\n        Parameters\n        ----------\n        parameter_id : int", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_in_notebook(self,\n                         labels=None,\n                         predict_proba=True,\n                         show_predicted_value=True,\n                         **kwargs):\n        \"\"\"\"\"\"\n\n        from IPython.core.display import display, HTML\n        display(HTML(self.as_html(labels=labels,\n                                  predict_proba=predict_proba,\n                                  show_predicted_value=show_predicted_value,\n                                  **kwargs)))", "output": "Shows html explanation in ipython notebook.\n\n        See as_html() for parameters.\n        This will throw an error if you don't have IPython installed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_rabit_checkpoint(self):\n        \"\"\"\n        \"\"\"\n        version = ctypes.c_int()\n        _check_call(_LIB.XGBoosterLoadRabitCheckpoint(\n            self.handle, ctypes.byref(version)))\n        return version.value", "output": "Initialize the model by load from rabit checkpoint.\n\n        Returns\n        -------\n        version: integer\n            The version number of the model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_bulk_size(size):\n    \"\"\"\n    \"\"\"\n    prev = ctypes.c_int()\n    check_call(_LIB.MXEngineSetBulkSize(\n        ctypes.c_int(size), ctypes.byref(prev)))\n    return prev.value", "output": "Set size limit on bulk execution.\n\n    Bulk execution bundles many operators to run together.\n    This can improve performance when running a lot of small\n    operators sequentially.\n\n    Parameters\n    ----------\n    size : int\n        Maximum number of operators that can be bundled in a bulk.\n\n    Returns\n    -------\n    int\n        Previous bulk size.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getConf(self, key, defaultValue=_NoValue):\n        \"\"\"\n        \"\"\"\n        return self.sparkSession.conf.get(key, defaultValue)", "output": "Returns the value of Spark SQL configuration property for the given key.\n\n        If the key is not set and defaultValue is set, return\n        defaultValue. If the key is not set and defaultValue is not set, return\n        the system default value.\n\n        >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\")\n        u'200'\n        >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", u\"10\")\n        u'10'\n        >>> sqlContext.setConf(\"spark.sql.shuffle.partitions\", u\"50\")\n        >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", u\"10\")\n        u'50'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attr(self, key):\n        \"\"\"\n        \"\"\"\n        ret = ctypes.c_char_p()\n        success = ctypes.c_int()\n        _check_call(_LIB.XGBoosterGetAttr(\n            self.handle, c_str(key), ctypes.byref(ret), ctypes.byref(success)))\n        if success.value != 0:\n            return py_str(ret.value)\n        return None", "output": "Get attribute string from the Booster.\n\n        Parameters\n        ----------\n        key : str\n            The key to get attribute from.\n\n        Returns\n        -------\n        value : str\n            The attribute value of the key, returns None if attribute do not exist.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def port_policy_present(name, sel_type, protocol=None, port=None, sel_range=None):\n    '''\n    \n    '''\n    ret = {'name': name, 'result': False, 'changes': {}, 'comment': ''}\n    old_state = __salt__['selinux.port_get_policy'](\n        name=name,\n        sel_type=sel_type,\n        protocol=protocol,\n        port=port, )\n    if old_state:\n        ret.update({'result': True,\n                    'comment': 'SELinux policy for \"{0}\" already present '.format(name) +\n                               'with specified sel_type \"{0}\", protocol \"{1}\" and port \"{2}\".'.format(\n                                   sel_type, protocol, port)})\n        return ret\n    if __opts__['test']:\n        ret.update({'result': None})\n    else:\n        add_ret = __salt__['selinux.port_add_policy'](\n            name=name,\n            sel_type=sel_type,\n            protocol=protocol,\n            port=port,\n            sel_range=sel_range, )\n        if add_ret['retcode'] != 0:\n            ret.update({'comment': 'Error adding new policy: {0}'.format(add_ret)})\n        else:\n            ret.update({'result': True})\n            new_state = __salt__['selinux.port_get_policy'](\n                name=name,\n                sel_type=sel_type,\n                protocol=protocol,\n                port=port, )\n            ret['changes'].update({'old': old_state, 'new': new_state})\n    return ret", "output": ".. versionadded:: 2019.2.0\n\n    Makes sure an SELinux port policy for a given port, protocol and SELinux context type is present.\n\n    name\n        The protocol and port spec. Can be formatted as ``(tcp|udp)/(port|port-range)``.\n\n    sel_type\n        The SELinux Type.\n\n    protocol\n        The protocol for the port, ``tcp`` or ``udp``. Required if name is not formatted.\n\n    port\n        The port or port range. Required if name is not formatted.\n\n    sel_range\n        The SELinux MLS/MCS Security Range.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait(name, ignore_already_stopped=False, fail_on_exit_status=False):\n    '''\n    \n    '''\n    try:\n        pre = state(name)\n    except CommandExecutionError:\n        # Container doesn't exist anymore\n        return {'result': ignore_already_stopped,\n                'comment': 'Container \\'{0}\\' absent'.format(name)}\n    already_stopped = pre == 'stopped'\n    response = _client_wrapper('wait', name)\n    _clear_context()\n    try:\n        post = state(name)\n    except CommandExecutionError:\n        # Container doesn't exist anymore\n        post = None\n\n    if already_stopped:\n        success = ignore_already_stopped\n    elif post == 'stopped':\n        success = True\n    else:\n        success = False\n\n    result = {'result': success,\n              'state': {'old': pre, 'new': post},\n              'exit_status': response}\n    if already_stopped:\n        result['comment'] = 'Container \\'{0}\\' already stopped'.format(name)\n    if fail_on_exit_status and result['result']:\n        result['result'] = result['exit_status'] == 0\n    return result", "output": "Wait for the container to exit gracefully, and return its exit code\n\n    .. note::\n\n        This function will block until the container is stopped.\n\n    name\n        Container name or ID\n\n    ignore_already_stopped\n        Boolean flag that prevents execution to fail, if a container\n        is already stopped.\n\n    fail_on_exit_status\n        Boolean flag to report execution as failure if ``exit_status``\n        is different than 0.\n\n    **RETURN DATA**\n\n    A dictionary will be returned, containing the following keys:\n\n    - ``status`` - A dictionary showing the prior state of the container as\n      well as the new state\n    - ``result`` - A boolean noting whether or not the action was successful\n    - ``exit_status`` - Exit status for the container\n    - ``comment`` - Only present if the container is already stopped\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion docker.wait mycontainer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_(key, value, profile=None):\n    '''\n    \n    '''\n    db = _get_db(profile)\n    return db.save({'_id': uuid4().hex, key: value})", "output": "Set a key/value pair in couchdb", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_existing_weight(self, weight, trainable=None):\n    \"\"\"\"\"\"\n    if trainable is None: trainable = weight.trainable\n    self.add_weight(name=weight.name, shape=weight.shape, dtype=weight.dtype,\n                    trainable=trainable, getter=lambda *_, **__: weight)", "output": "Calls add_weight() to register but not create an existing weight.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_to_distribution(dist):\n    \"\"\"\"\"\"\n    try:\n        dist.add_qt_bindings()\n    except AttributeError:\n        raise ImportError(\"This script requires guidata 1.5+\")\n    for _modname in ('spyder', 'spyderplugins'):\n        dist.add_module_data_files(_modname, (\"\", ),\n                                   ('.png', '.svg', '.html', '.png', '.txt',\n                                    '.js', '.inv', '.ico', '.css', '.doctree',\n                                    '.qm', '.py',),\n                                   copy_to_root=False)", "output": "Add package to py2exe/cx_Freeze distribution object\n    Extension to guidata.disthelpers", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _reset_bind(self):\n        \"\"\"\"\"\"\n        self.binded = False\n        self._exec_group = None\n        self._data_shapes = None\n        self._label_shapes = None", "output": "Internal function to reset binded state.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathNextFollowing(self, cur):\n        \"\"\" \"\"\"\n        if cur is None: cur__o = None\n        else: cur__o = cur._o\n        ret = libxml2mod.xmlXPathNextFollowing(self._o, cur__o)\n        if ret is None:raise xpathError('xmlXPathNextFollowing() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Traversal function for the \"following\" direction The\n          following axis contains all nodes in the same document as\n          the context node that are after the context node in\n          document order, excluding any descendants and excluding\n          attribute nodes and namespace nodes; the nodes are ordered\n           in document order", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sample_view(min_dist, max_dist=None):\n  '''\n  '''\n  if max_dist is None:\n    max_dist = min_dist\n  dist = np.random.uniform(min_dist, max_dist)\n  eye = np.random.normal(size=3)\n  eye = normalize(eye)*dist\n  return lookat(eye)", "output": "Sample random camera position.\n  \n  Sample origin directed camera position in given distance\n  range from the origin. ModelView matrix is returned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_expression_reference(self, # pylint: disable=no-self-use\n                                     grammar: Grammar,\n                                     parent_expression_nonterminal: str,\n                                     child_expression_nonterminal: str) -> None:\n        \"\"\"\n        \n        \"\"\"\n        grammar[parent_expression_nonterminal].members = \\\n                [member if member.name != child_expression_nonterminal\n                 else grammar[child_expression_nonterminal]\n                 for member in grammar[parent_expression_nonterminal].members]", "output": "When we add a new expression, there may be other expressions that refer to\n        it, and we need to update those to point to the new expression.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def local_attention2d_spatial_decoder(x, kv_dim, heads_dim,\n                                      feedforward_dim, hparams):\n  \"\"\"\"\"\"\n  batch_dim, length_dim, model_dim = x.shape.dims\n  blocks_h_dim = mtf.Dimension(\"blocksh\", hparams.block_height)\n  blocks_w_dim = mtf.Dimension(\"blocksw\", hparams.block_width)\n  num_h_blocks_dim = mtf.Dimension(\"num_h_blocks\",\n                                   hparams.img_len // hparams.block_height)\n  num_w_blocks_dim = mtf.Dimension(\n      \"num_w_blocks\",\n      hparams.img_len * hparams.num_channels // hparams.block_width)\n  x = mtf.transpose(\n      mtf.reshape(\n          x,\n          mtf.Shape([\n              batch_dim, num_h_blocks_dim, blocks_h_dim,\n              num_w_blocks_dim, blocks_w_dim, model_dim\n          ])),\n      mtf.Shape([\n          batch_dim, num_h_blocks_dim, num_w_blocks_dim,\n          blocks_h_dim, blocks_w_dim, model_dim\n      ]))\n  # Image Transformer Decoder\n  # [ self attention - ffn - residual + dropout] x n\n  for layer in range(hparams.num_decoder_layers):\n    layer_name = \"decoder_layer_%d\" % layer\n    with tf.variable_scope(layer_name):\n      # Self attention layer\n      x += layer_prepostprocess_dropout(\n          mtf.layers.local_2d_self_attention_spatial_blocks(\n              mtf.layers.layer_norm(x, model_dim, name=\"layer_norm_att\"),\n              kv_dim,\n              heads_dim,\n              memory_h_dim=num_h_blocks_dim,\n              memory_w_dim=num_w_blocks_dim,\n              name=\"self_att\"), hparams)\n      # ffn layer\n      x += layer_prepostprocess_dropout(\n          mtf.layers.dense_relu_dense(\n              mtf.layers.layer_norm(x, model_dim, name=\"layer_norm_ffn\"),\n              feedforward_dim,\n              hparams.dropout,\n              dropout_broadcast_dims=[length_dim]), hparams)\n\n  output = mtf.layers.layer_norm(x, model_dim, name=\"final_layer_norm\")\n  return output", "output": "Image Transformer decoder with local2D spatial layers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_image(name):\n    '''\n    \n    '''\n    try:\n        images = __salt__['glance.image_list'](name=name)\n    except kstone_Unauthorized:\n        return False, 'keystoneclient: Unauthorized'\n    except glance_Unauthorized:\n        return False, 'glanceclient: Unauthorized'\n    log.debug('Got images: %s', images)\n\n    if type(images) is dict and len(images) == 1 and 'images' in images:\n        images = images['images']\n\n    images_list = images.values() if type(images) is dict else images\n\n    if not images_list:\n        return None, 'No image with name \"{0}\"'.format(name)\n    elif len(images_list) == 1:\n        return images_list[0], 'Found image {0}'.format(name)\n    elif len(images_list) > 1:\n        return False, 'Found more than one image with given name'\n    else:\n        raise NotImplementedError", "output": "Tries to find image with given name, returns\n        - image, 'Found image <name>'\n        - None, 'No such image found'\n        - False, 'Found more than one image with given name'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def close(self):\n        \"\"\"\n        \"\"\"\n        if self._closed:\n            return\n\n        await self.http.close()\n        self._closed = True\n\n        for voice in self.voice_clients:\n            try:\n                await voice.disconnect()\n            except Exception:\n                # if an error happens during disconnects, disregard it.\n                pass\n\n        if self.ws is not None and self.ws.open:\n            await self.ws.close()\n\n        self._ready.clear()", "output": "|coro|\n\n        Closes the connection to discord.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_file_ifaces(iface, data, **settings):\n    '''\n    \n    '''\n    try:\n        eth_template = JINJA.get_template('debian_eth.jinja')\n        source_template = JINJA.get_template('debian_source.jinja')\n    except jinja2.exceptions.TemplateNotFound:\n        log.error('Could not load template debian_eth.jinja')\n        return ''\n\n    # Read /etc/network/interfaces into a dict\n    adapters = _parse_interfaces()\n    # Apply supplied settings over on-disk settings\n    adapters[iface] = data\n\n    ifcfg = ''\n    for adapter in adapters:\n        if 'type' in adapters[adapter] and adapters[adapter]['type'] == 'source':\n            tmp = source_template.render({'name': adapter, 'data': adapters[adapter]})\n        else:\n            tmp = eth_template.render({'name': adapter, 'data': adapters[adapter]})\n        ifcfg = ifcfg + tmp\n        if adapter == iface:\n            saved_ifcfg = tmp\n\n    _SEPARATE_FILE = False\n    if 'filename' in settings:\n        if not settings['filename'].startswith('/'):\n            filename = '{0}/{1}'.format(_DEB_NETWORK_DIR, settings['filename'])\n        else:\n            filename = settings['filename']\n        _SEPARATE_FILE = True\n    else:\n        if 'filename' in adapters[adapter]['data']:\n            filename = adapters[adapter]['data']\n        else:\n            filename = _DEB_NETWORK_FILE\n\n    if not os.path.exists(os.path.dirname(filename)):\n        msg = '{0} cannot be written.'\n        msg = msg.format(os.path.dirname(filename))\n        log.error(msg)\n        raise AttributeError(msg)\n    with salt.utils.files.flopen(filename, 'w') as fout:\n        if _SEPARATE_FILE:\n            fout.write(salt.utils.stringutils.to_str(saved_ifcfg))\n        else:\n            fout.write(salt.utils.stringutils.to_str(ifcfg))\n\n    # Return as an array so the difflib works\n    return saved_ifcfg.split('\\n')", "output": "Writes a file to disk", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_namespaced_pod_preset(self, namespace, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_pod_preset_with_http_info(namespace, body, **kwargs)\n        else:\n            (data) = self.create_namespaced_pod_preset_with_http_info(namespace, body, **kwargs)\n            return data", "output": "create a PodPreset\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_pod_preset(namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1alpha1PodPreset body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1alpha1PodPreset\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mask_zero_div_zero(x, y, result, copy=False):\n    \"\"\"\n    \n    \"\"\"\n    if is_scalar(y):\n        y = np.array(y)\n\n    zmask = y == 0\n    if zmask.any():\n        shape = result.shape\n\n        nan_mask = (zmask & (x == 0)).ravel()\n        neginf_mask = (zmask & (x < 0)).ravel()\n        posinf_mask = (zmask & (x > 0)).ravel()\n\n        if nan_mask.any() or neginf_mask.any() or posinf_mask.any():\n            # Fill negative/0 with -inf, positive/0 with +inf, 0/0 with NaN\n            result = result.astype('float64', copy=copy).ravel()\n\n            np.putmask(result, nan_mask, np.nan)\n            np.putmask(result, posinf_mask, np.inf)\n            np.putmask(result, neginf_mask, -np.inf)\n\n            result = result.reshape(shape)\n\n    return result", "output": "Set results of 0 / 0 or 0 // 0 to np.nan, regardless of the dtypes\n    of the numerator or the denominator.\n\n    Parameters\n    ----------\n    x : ndarray\n    y : ndarray\n    result : ndarray\n    copy : bool (default False)\n        Whether to always create a new array or try to fill in the existing\n        array if possible.\n\n    Returns\n    -------\n    filled_result : ndarray\n\n    Examples\n    --------\n    >>> x = np.array([1, 0, -1], dtype=np.int64)\n    >>> y = 0       # int 0; numpy behavior is different with float\n    >>> result = x / y\n    >>> result      # raw numpy result does not fill division by zero\n    array([0, 0, 0])\n    >>> mask_zero_div_zero(x, y, result)\n    array([ inf,  nan, -inf])", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def max_date(self, rows: List[Row], column: DateColumn) -> Date:\n        \"\"\"\n        \n        \"\"\"\n        cell_values = [row.values[column.name] for row in rows]\n        if not cell_values:\n            return Date(-1, -1, -1)\n        if not all([isinstance(value, Date) for value in cell_values]):\n            raise ExecutionError(f\"Invalid values for date selection function: {cell_values}\")\n        return max(cell_values)", "output": "Takes a list of rows and a column and returns the max of the values under that column in\n        those rows.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def delete_invite(self, invite):\n        \"\"\"\n        \"\"\"\n\n        invite_id = utils.resolve_invite(invite)\n        await self.http.delete_invite(invite_id)", "output": "|coro|\n\n        Revokes an :class:`.Invite`, URL, or ID to an invite.\n\n        You must have the :attr:`~.Permissions.manage_channels` permission in\n        the associated guild to do this.\n\n        Parameters\n        ----------\n        invite: Union[:class:`.Invite`, :class:`str`]\n            The invite to revoke.\n\n        Raises\n        -------\n        Forbidden\n            You do not have permissions to revoke invites.\n        NotFound\n            The invite is invalid or expired.\n        HTTPException\n            Revoking the invite failed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _adorn_subplots(self):\n        \"\"\"\"\"\"\n        if len(self.axes) > 0:\n            all_axes = self._get_subplots()\n            nrows, ncols = self._get_axes_layout()\n            _handle_shared_axes(axarr=all_axes, nplots=len(all_axes),\n                                naxes=nrows * ncols, nrows=nrows,\n                                ncols=ncols, sharex=self.sharex,\n                                sharey=self.sharey)\n\n        for ax in self.axes:\n            if self.yticks is not None:\n                ax.set_yticks(self.yticks)\n\n            if self.xticks is not None:\n                ax.set_xticks(self.xticks)\n\n            if self.ylim is not None:\n                ax.set_ylim(self.ylim)\n\n            if self.xlim is not None:\n                ax.set_xlim(self.xlim)\n\n            ax.grid(self.grid)\n\n        if self.title:\n            if self.subplots:\n                if is_list_like(self.title):\n                    if len(self.title) != self.nseries:\n                        msg = ('The length of `title` must equal the number '\n                               'of columns if using `title` of type `list` '\n                               'and `subplots=True`.\\n'\n                               'length of title = {}\\n'\n                               'number of columns = {}').format(\n                            len(self.title), self.nseries)\n                        raise ValueError(msg)\n\n                    for (ax, title) in zip(self.axes, self.title):\n                        ax.set_title(title)\n                else:\n                    self.fig.suptitle(self.title)\n            else:\n                if is_list_like(self.title):\n                    msg = ('Using `title` of type `list` is not supported '\n                           'unless `subplots=True` is passed')\n                    raise ValueError(msg)\n                self.axes[0].set_title(self.title)", "output": "Common post process unrelated to data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list(self, parts: Any) -> str:\n        \"\"\"\n        \"\"\"\n        _ = self.translate\n        if len(parts) == 0:\n            return \"\"\n        if len(parts) == 1:\n            return parts[0]\n        comma = u\" \\u0648 \" if self.code.startswith(\"fa\") else u\", \"\n        return _(\"%(commas)s and %(last)s\") % {\n            \"commas\": comma.join(parts[:-1]),\n            \"last\": parts[len(parts) - 1],\n        }", "output": "Returns a comma-separated list for the given list of parts.\n\n        The format is, e.g., \"A, B and C\", \"A and B\" or just \"A\" for lists\n        of size 1.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def htmlNewDocNoDtD(URI, ExternalID):\n    \"\"\" \"\"\"\n    ret = libxml2mod.htmlNewDocNoDtD(URI, ExternalID)\n    if ret is None:raise treeError('htmlNewDocNoDtD() failed')\n    return xmlDoc(_obj=ret)", "output": "Creates a new HTML document without a DTD node if @URI and\n       @ExternalID are None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_resource_attribute(self, attr):\n        \"\"\"\n        \"\"\"\n        if attr not in self.resource_attributes:\n            raise KeyError(\"%s is not in resource attributes\" % attr)\n\n        return self.resource_attributes[attr]", "output": "Gets the resource attribute if available\n\n        :param attr: Name of the attribute\n        :return: Value of the attribute, if set in the resource. None otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log_proto(self, message, **kw):\n        \"\"\"\n        \"\"\"\n        self.entries.append(ProtobufEntry(payload=message, **kw))", "output": "Add a protobuf entry to be logged during :meth:`commit`.\n\n        :type message: protobuf message\n        :param message: the protobuf entry\n\n        :type kw: dict\n        :param kw: (optional) additional keyword arguments for the entry.\n                   See :class:`~google.cloud.logging.entries.LogEntry`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def proxy_type(v):\n    \"\"\"  \"\"\"\n    proxies = []\n    if re.match(r\"((http|socks5):\\/\\/.)?(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}):(\\d{1,5})\", v):\n        proxies.append({\"http\": v,\n                        \"https\": v})\n        return proxies\n    elif re.match(r\"((http|socks5):\\/\\/.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}:(\\d{1,5})\", v):\n        proxies.append({\"http\": v,\n                        \"https\": v})\n        return proxies\n    elif is_proxy_list(v, proxies):\n        return proxies\n    else:\n        raise argparse.ArgumentTypeError(\n            \"Proxy should follow IP:PORT or DOMAIN:PORT format\")", "output": "Match IP:PORT or DOMAIN:PORT in a losse manner", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(key, default=None):\n    '''\n    \n    '''\n    store = load()\n\n    if isinstance(key, six.string_types):\n        return store.get(key, default)\n    elif default is None:\n        return [store[k] for k in key if k in store]\n    else:\n        return [store.get(k, default) for k in key]", "output": "Get a (list of) value(s) from the minion datastore\n\n    .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' data.get key\n        salt '*' data.get '[\"key1\", \"key2\"]'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nearest_neighbor(self, x, means):\n    \"\"\"\n    \"\"\"\n    x_norm_sq = tf.reduce_sum(tf.square(x), axis=-1, keep_dims=True)\n    means_norm_sq = tf.reduce_sum(tf.square(means), axis=-1, keep_dims=True)\n    scalar_prod = tf.matmul(\n        tf.transpose(x, perm=[1, 0, 2]), tf.transpose(means, perm=[0, 2, 1]))\n    scalar_prod = tf.transpose(scalar_prod, perm=[1, 0, 2])\n    dist = x_norm_sq + tf.transpose(\n        means_norm_sq, perm=[2, 0, 1]) - 2 * scalar_prod\n\n    if self.hparams.soft_em:\n      nearest_idx = tf.stack(\n          [\n              tf.multinomial(\n                  -dist[:, i, :], num_samples=self.hparams.num_samples)\n              for i in range(self.hparams.num_blocks)\n          ],\n          axis=1)\n      nearest_hot = tf.one_hot(nearest_idx, depth=self.hparams.block_v_size)\n      nearest_hot = tf.reduce_mean(nearest_hot, axis=-2)\n    else:\n      if self.hparams.random_top_k > 1:\n        _, top_k_idx = tf.nn.top_k(-dist, k=self.hparams.random_top_k)\n        nearest_idx = tf.gather(\n            top_k_idx,\n            tf.random_uniform(\n                [1],\n                minval=0,\n                maxval=self.hparams.random_top_k - 1,\n                dtype=tf.int32),\n            axis=-1)\n      else:\n        if self.hparams.use_scales:\n          dist /= tf.reshape(self.hparams.scales,\n                             [1, 1, self.hparams.moe_num_experts])\n        nearest_idx = tf.argmax(-dist, axis=-1)\n      nearest_hot = tf.one_hot(nearest_idx, self.hparams.block_v_size)\n    return nearest_hot", "output": "Find the nearest element in means to elements in x.\n\n    Args:\n        x: Batch of encoder continuous latent states sliced/projected into\n           shape [-1, num_blocks, block_dim].\n        means: Embedding means of shape.\n\n    Returns:\n      Tensor with nearest element in mean encoded in one-hot notation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def as_objective(obj):\n  \"\"\"\n  \"\"\"\n  if isinstance(obj, Objective):\n    return obj\n  elif callable(obj):\n    return obj\n  elif isinstance(obj, str):\n    layer, n = obj.split(\":\")\n    layer, n = layer.strip(), int(n)\n    return channel(layer, n)", "output": "Convert obj into Objective class.\n\n  Strings of the form \"layer:n\" become the Objective channel(layer, n).\n  Objectives are returned unchanged.\n\n  Args:\n    obj: string or Objective.\n\n  Returns:\n    Objective", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def execute_only_once():\n    \"\"\"\n    \n    \"\"\"\n    f = inspect.currentframe().f_back\n    ident = (f.f_code.co_filename, f.f_lineno)\n    if ident in _EXECUTE_HISTORY:\n        return False\n    _EXECUTE_HISTORY.add(ident)\n    return True", "output": "Each called in the code to this function is guaranteed to return True the\n    first time and False afterwards.\n\n    Returns:\n        bool: whether this is the first time this function gets called from this line of code.\n\n    Example:\n        .. code-block:: python\n\n            if execute_only_once():\n                # do something only once", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def task_absent(name):\n    '''\n    \n    '''\n    ret = {'name': name, 'changes': {}, 'result': True, 'comment': ''}\n\n    task = __salt__['kapacitor.get_task'](name)\n\n    if task:\n        if __opts__['test']:\n            ret['result'] = None\n            ret['comment'] = 'Task would have been deleted'\n        else:\n            result = __salt__['kapacitor.delete_task'](name)\n            ret['result'] = result['success']\n            if not ret['result']:\n                ret['comment'] = 'Could not disable task'\n                if result.get('stderr'):\n                    ret['comment'] += '\\n' + result['stderr']\n                return ret\n            ret['comment'] = 'Task was deleted'\n        ret['changes'][name] = 'deleted'\n    else:\n        ret['comment'] = 'Task does not exist'\n\n    return ret", "output": "Ensure that a task is absent from Kapacitor.\n\n    name\n        Name of the task.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def combine_concat_plans(plans, concat_axis):\n    \"\"\"\n    \n    \"\"\"\n    if len(plans) == 1:\n        for p in plans[0]:\n            yield p[0], [p[1]]\n\n    elif concat_axis == 0:\n        offset = 0\n        for plan in plans:\n            last_plc = None\n\n            for plc, unit in plan:\n                yield plc.add(offset), [unit]\n                last_plc = plc\n\n            if last_plc is not None:\n                offset += last_plc.as_slice.stop\n\n    else:\n        num_ended = [0]\n\n        def _next_or_none(seq):\n            retval = next(seq, None)\n            if retval is None:\n                num_ended[0] += 1\n            return retval\n\n        plans = list(map(iter, plans))\n        next_items = list(map(_next_or_none, plans))\n\n        while num_ended[0] != len(next_items):\n            if num_ended[0] > 0:\n                raise ValueError(\"Plan shapes are not aligned\")\n\n            placements, units = zip(*next_items)\n\n            lengths = list(map(len, placements))\n            min_len, max_len = min(lengths), max(lengths)\n\n            if min_len == max_len:\n                yield placements[0], units\n                next_items[:] = map(_next_or_none, plans)\n            else:\n                yielded_placement = None\n                yielded_units = [None] * len(next_items)\n                for i, (plc, unit) in enumerate(next_items):\n                    yielded_units[i] = unit\n                    if len(plc) > min_len:\n                        # trim_join_unit updates unit in place, so only\n                        # placement needs to be sliced to skip min_len.\n                        next_items[i] = (plc[min_len:],\n                                         trim_join_unit(unit, min_len))\n                    else:\n                        yielded_placement = plc\n                        next_items[i] = _next_or_none(plans[i])\n\n                yield yielded_placement, yielded_units", "output": "Combine multiple concatenation plans into one.\n\n    existing_plan is updated in-place.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zfill(self, width):\n        \"\"\"\n        \n        \"\"\"\n        result = str_pad(self._parent, width, side='left', fillchar='0')\n        return self._wrap_result(result)", "output": "Pad strings in the Series/Index by prepending '0' characters.\n\n        Strings in the Series/Index are padded with '0' characters on the\n        left of the string to reach a total string length  `width`. Strings\n        in the Series/Index with length greater or equal to `width` are\n        unchanged.\n\n        Parameters\n        ----------\n        width : int\n            Minimum length of resulting string; strings with length less\n            than `width` be prepended with '0' characters.\n\n        Returns\n        -------\n        Series/Index of objects\n\n        See Also\n        --------\n        Series.str.rjust : Fills the left side of strings with an arbitrary\n            character.\n        Series.str.ljust : Fills the right side of strings with an arbitrary\n            character.\n        Series.str.pad : Fills the specified sides of strings with an arbitrary\n            character.\n        Series.str.center : Fills boths sides of strings with an arbitrary\n            character.\n\n        Notes\n        -----\n        Differs from :meth:`str.zfill` which has special handling\n        for '+'/'-' in the string.\n\n        Examples\n        --------\n        >>> s = pd.Series(['-1', '1', '1000', 10, np.nan])\n        >>> s\n        0      -1\n        1       1\n        2    1000\n        3      10\n        4     NaN\n        dtype: object\n\n        Note that ``10`` and ``NaN`` are not strings, therefore they are\n        converted to ``NaN``. The minus sign in ``'-1'`` is treated as a\n        regular character and the zero is added to the left of it\n        (:meth:`str.zfill` would have moved it to the left). ``1000``\n        remains unchanged as it is longer than `width`.\n\n        >>> s.str.zfill(3)\n        0     0-1\n        1     001\n        2    1000\n        3     NaN\n        4     NaN\n        dtype: object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _combine_args(self):\n        \"\"\"\n        \"\"\"\n        child_path = self._parse_path(self._flat_path)\n\n        if self._parent is not None:\n            if self._parent.is_partial:\n                raise ValueError(\"Parent key must be complete.\")\n\n            # We know that _parent.path() will return a copy.\n            child_path = self._parent.path + child_path\n            self._flat_path = self._parent.flat_path + self._flat_path\n            if (\n                self._namespace is not None\n                and self._namespace != self._parent.namespace\n            ):\n                raise ValueError(\"Child namespace must agree with parent's.\")\n            self._namespace = self._parent.namespace\n            if self._project is not None and self._project != self._parent.project:\n                raise ValueError(\"Child project must agree with parent's.\")\n            self._project = self._parent.project\n\n        return child_path", "output": "Sets protected data by combining raw data set from the constructor.\n\n        If a ``_parent`` is set, updates the ``_flat_path`` and sets the\n        ``_namespace`` and ``_project`` if not already set.\n\n        :rtype: :class:`list` of :class:`dict`\n        :returns: A list of key parts with kind and ID or name set.\n        :raises: :class:`ValueError` if the parent key is not complete.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __interact_writen(self, fd, data):\n        '''\n        '''\n\n        while data != b'' and self.isalive():\n            n = os.write(fd, data)\n            data = data[n:]", "output": "This is used by the interact() method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_create(alias, passwd, usrgrps, **kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'user.create'\n            params = {\"alias\": alias, \"passwd\": passwd, \"usrgrps\": []}\n            # User groups\n            if not isinstance(usrgrps, list):\n                usrgrps = [usrgrps]\n            for usrgrp in usrgrps:\n                params['usrgrps'].append({\"usrgrpid\": usrgrp})\n\n            params = _params_extend(params, _ignore_name=True, **kwargs)\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result']['userids']\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": ".. versionadded:: 2016.3.0\n\n    Create new zabbix user\n\n    .. note::\n        This function accepts all standard user properties: keyword argument\n        names differ depending on your zabbix version, see here__.\n\n        .. __: https://www.zabbix.com/documentation/2.0/manual/appendix/api/user/definitions#user\n\n    :param alias: user alias\n    :param passwd: user's password\n    :param usrgrps: user groups to add the user to\n\n    :param _connection_user: zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: url of zabbix frontend (can also be set in opts or pillar, see module's docstring)\n\n    :param firstname: string with firstname of the user, use 'firstname' instead of 'name' parameter to not mess\n                      with value supplied from Salt sls file.\n\n    :return: On success string with id of the created user.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.user_create james password007 '[7, 12]' firstname='James Bond'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def translate_bytes(val):\n    '''\n    \n    '''\n    try:\n        val = int(val)\n    except (TypeError, ValueError):\n        if not isinstance(val, six.string_types):\n            val = six.text_type(val)\n    return val", "output": "These values can be expressed as an integer number of bytes, or a string\n    expression (i.e. 100mb, 1gb, etc.).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def addlist(self, key, valuelist=[]):\n        \"\"\"\n        \n        \"\"\"\n        for value in valuelist:\n            self.add(key, value)\n        return self", "output": "Add the values in <valuelist> to the list of values for <key>. If <key>\n        is not in the dictionary, the values in <valuelist> become the values\n        for <key>.\n\n        Example:\n          omd = omdict([(1,1)])\n          omd.addlist(1, [11, 111])\n          omd.allitems() == [(1, 1), (1, 11), (1, 111)]\n          omd.addlist(2, [2])\n          omd.allitems() == [(1, 1), (1, 11), (1, 111), (2, 2)]\n\n        Returns: <self>.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_trial(self, name, specification):\n        \"\"\"\"\"\"\n        payload = {\"name\": name, \"spec\": specification}\n        response = requests.post(urljoin(self._path, \"trials\"), json=payload)\n        return self._deserialize(response)", "output": "Adds a trial by name and specification (dict).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_from_dt(self, data, nthread):\n        \"\"\"\n        \n        \"\"\"\n        ptrs = (ctypes.c_void_p * data.ncols)()\n        if hasattr(data, \"internal\") and hasattr(data.internal, \"column\"):\n            # datatable>0.8.0\n            for icol in range(data.ncols):\n                col = data.internal.column(icol)\n                ptr = col.data_pointer\n                ptrs[icol] = ctypes.c_void_p(ptr)\n        else:\n            # datatable<=0.8.0\n            from datatable.internal import frame_column_data_r  # pylint: disable=no-name-in-module,import-error\n            for icol in range(data.ncols):\n                ptrs[icol] = frame_column_data_r(data, icol)\n\n        # always return stypes for dt ingestion\n        feature_type_strings = (ctypes.c_char_p * data.ncols)()\n        for icol in range(data.ncols):\n            feature_type_strings[icol] = ctypes.c_char_p(data.stypes[icol].name.encode('utf-8'))\n\n        handle = ctypes.c_void_p()\n        _check_call(_LIB.XGDMatrixCreateFromDT(\n            ptrs, feature_type_strings,\n            c_bst_ulong(data.shape[0]),\n            c_bst_ulong(data.shape[1]),\n            ctypes.byref(handle),\n            nthread))\n        self.handle = handle", "output": "Initialize data from a datatable Frame.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pandas_dtype(dtype):\n    \"\"\"\n    \n    \"\"\"\n    # short-circuit\n    if isinstance(dtype, np.ndarray):\n        return dtype.dtype\n    elif isinstance(dtype, (np.dtype, PandasExtensionDtype, ExtensionDtype)):\n        return dtype\n\n    # registered extension types\n    result = registry.find(dtype)\n    if result is not None:\n        return result\n\n    # try a numpy dtype\n    # raise a consistent TypeError if failed\n    try:\n        npdtype = np.dtype(dtype)\n    except Exception:\n        # we don't want to force a repr of the non-string\n        if not isinstance(dtype, str):\n            raise TypeError(\"data type not understood\")\n        raise TypeError(\"data type '{}' not understood\".format(\n            dtype))\n\n    # Any invalid dtype (such as pd.Timestamp) should raise an error.\n    # np.dtype(invalid_type).kind = 0 for such objects. However, this will\n    # also catch some valid dtypes such as object, np.object_ and 'object'\n    # which we safeguard against by catching them earlier and returning\n    # np.dtype(valid_dtype) before this condition is evaluated.\n    if is_hashable(dtype) and dtype in [object, np.object_, 'object', 'O']:\n        # check hashability to avoid errors/DeprecationWarning when we get\n        # here and `dtype` is an array\n        return npdtype\n    elif npdtype.kind == 'O':\n        raise TypeError(\"dtype '{}' not understood\".format(dtype))\n\n    return npdtype", "output": "Convert input into a pandas only dtype object or a numpy dtype object.\n\n    Parameters\n    ----------\n    dtype : object to be converted\n\n    Returns\n    -------\n    np.dtype or a pandas dtype\n\n    Raises\n    ------\n    TypeError if not a dtype", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_optimizer(self, kvstore='local', optimizer='sgd',\n                       optimizer_params=(('learning_rate', 0.01),),\n                       force_init=False):\n        \"\"\"\n        \"\"\"\n        assert self.binded and self.params_initialized\n        if self.optimizer_initialized and not force_init:\n            self.logger.warning('optimizer already initialized, ignoring.')\n            return\n\n        for module in self._modules:\n            module.init_optimizer(kvstore=kvstore, optimizer=optimizer,\n                                  optimizer_params=optimizer_params, force_init=force_init)\n\n        self.optimizer_initialized = True", "output": "Installs and initializes optimizers.\n\n        Parameters\n        ----------\n        kvstore : str or KVStore\n            Default `'local'`.\n        optimizer : str or Optimizer\n            Default `'sgd'`\n        optimizer_params : dict\n            Default ``(('learning_rate', 0.01),)``. The default value is not a dictionary,\n            just to avoid pylint warning of dangerous default values.\n        force_init : bool\n            Default ``False``, indicating whether we should force re-initializing the\n            optimizer in the case an optimizer is already installed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def join_options(options):\n    \"\"\"\n    \"\"\"\n    rv = []\n    any_prefix_is_slash = False\n    for opt in options:\n        prefix = split_opt(opt)[0]\n        if prefix == '/':\n            any_prefix_is_slash = True\n        rv.append((len(prefix), opt))\n\n    rv.sort(key=lambda x: x[0])\n\n    rv = ', '.join(x[1] for x in rv)\n    return rv, any_prefix_is_slash", "output": "Given a list of option strings this joins them in the most appropriate\n    way and returns them in the form ``(formatted_string,\n    any_prefix_is_slash)`` where the second item in the tuple is a flag that\n    indicates if any of the option prefixes was a slash.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mimic_adam_with_adafactor(hparams):\n  \"\"\"\n  \"\"\"\n  assert \"adam\" in hparams.optimizer\n  hparams.optimizer = \"adafactor\"\n  hparams.optimizer_adafactor_beta1 = hparams.optimizer_adam_beta1\n  hparams.optimizer_adafactor_beta2 = hparams.optimizer_adam_beta2\n  hparams.optimizer_adafactor_multiply_by_parameter_scale = False\n  hparams.optimizer_adafactor_factored = False\n  hparams.optimizer_adafactor_clipping_threshold = None\n  hparams.optimizer_adafactor_decay_type = \"adam\"", "output": "Switch from Adam to Adafactor, approximating the behavior of Adam.\n\n  Some minor things may be different, like epsilon and beta1 correction.\n\n  Args:\n    hparams: model hyperparameters where \"adam\" in hparams.optimizer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_ipv6_filter(ip, options=None):\n    '''\n    \n    '''\n    _is_ipv6 = _is_ipv(ip, 6, options=options)\n    return isinstance(_is_ipv6, six.string_types)", "output": "Returns a bool telling if the value passed to it was a valid IPv6 address.\n\n    ip\n        The IP address.\n\n    net: False\n        Consider IP addresses followed by netmask.\n\n    options\n        CSV of options regarding the nature of the IP address. E.g.: loopback, multicast, private etc.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def train_eval():\n    \"\"\" \n    \"\"\"\n\n    global trainloader\n    global testloader\n    global net\n\n    (x_train, y_train) = trainloader\n    (x_test, y_test) = testloader\n\n    # train procedure\n    net.fit(\n        x=x_train,\n        y=y_train,\n        batch_size=args.batch_size,\n        validation_data=(x_test, y_test),\n        epochs=args.epochs,\n        shuffle=True,\n        callbacks=[\n            SendMetrics(),\n            EarlyStopping(min_delta=0.001, patience=10),\n            TensorBoard(log_dir=TENSORBOARD_DIR),\n        ],\n    )\n\n    # trial report final acc to tuner\n    _, acc = net.evaluate(x_test, y_test)\n    logger.debug(\"Final result is: %.3f\", acc)\n    nni.report_final_result(acc)", "output": "train and eval the model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_env_cache(opts, env_cache):\n    '''\n    \n    '''\n    if not os.path.isfile(env_cache):\n        return None\n    try:\n        with salt.utils.files.fopen(env_cache, 'rb') as fp_:\n            log.trace('Returning env cache data from %s', env_cache)\n            serial = salt.payload.Serial(opts)\n            return salt.utils.data.decode(serial.load(fp_))\n    except (IOError, OSError):\n        pass\n    return None", "output": "Returns cached env names, if present. Otherwise returns None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _StructPackEncoder(wire_type, format):\n  \"\"\"\n  \"\"\"\n\n  value_size = struct.calcsize(format)\n\n  def SpecificEncoder(field_number, is_repeated, is_packed):\n    local_struct_pack = struct.pack\n    if is_packed:\n      tag_bytes = TagBytes(field_number, wire_format.WIRETYPE_LENGTH_DELIMITED)\n      local_EncodeVarint = _EncodeVarint\n      def EncodePackedField(write, value):\n        write(tag_bytes)\n        local_EncodeVarint(write, len(value) * value_size)\n        for element in value:\n          write(local_struct_pack(format, element))\n      return EncodePackedField\n    elif is_repeated:\n      tag_bytes = TagBytes(field_number, wire_type)\n      def EncodeRepeatedField(write, value):\n        for element in value:\n          write(tag_bytes)\n          write(local_struct_pack(format, element))\n      return EncodeRepeatedField\n    else:\n      tag_bytes = TagBytes(field_number, wire_type)\n      def EncodeField(write, value):\n        write(tag_bytes)\n        return write(local_struct_pack(format, value))\n      return EncodeField\n\n  return SpecificEncoder", "output": "Return a constructor for an encoder for a fixed-width field.\n\n  Args:\n      wire_type:  The field's wire type, for encoding tags.\n      format:  The format string to pass to struct.pack().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def symlink_remove(link):\n    \"\"\"\n    \"\"\"\n    # https://stackoverflow.com/q/26554135/6400719\n    if os.path.isdir(path2str(link)) and is_windows:\n        # this should only be on Py2.7 and windows\n        os.rmdir(path2str(link))\n    else:\n        os.unlink(path2str(link))", "output": "Remove a symlink. Used for model shortcut links.\n\n    link (unicode / Path): The path to the symlink.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cmd(tgt,\n        fun,\n        arg=(),\n        timeout=None,\n        tgt_type='glob',\n        kwarg=None):\n    '''\n    \n    '''\n    client = salt.client.ssh.client.SSHClient(mopts=__opts__)\n    return client.cmd(\n            tgt,\n            fun,\n            arg,\n            timeout,\n            tgt_type,\n            kwarg)", "output": ".. versionadded:: 2015.5.0\n    .. versionchanged:: 2017.7.0\n        The ``expr_form`` argument has been renamed to ``tgt_type``, earlier\n        releases must use ``expr_form``.\n\n    Execute a single command via the salt-ssh subsystem and return all\n    routines at once\n\n    A wrapper around the :py:meth:`SSHClient.cmd\n    <salt.client.ssh.client.SSHClient.cmd>` method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def scheduled_sample_prob(ground_truth_x,\n                          generated_x,\n                          batch_size,\n                          scheduled_sample_var):\n  \"\"\"\n  \"\"\"\n  probability_threshold = scheduled_sample_var\n  probability_of_generated = tf.random_uniform([batch_size])\n  return tf.where(probability_of_generated > probability_threshold,\n                  generated_x, ground_truth_x)", "output": "Probability based scheduled sampling.\n\n  Args:\n    ground_truth_x: tensor of ground-truth data points.\n    generated_x: tensor of generated data points.\n    batch_size: batch size\n    scheduled_sample_var: probability of choosing from ground_truth.\n  Returns:\n    New batch with randomly selected data points.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_actions_from_items(self, items):\r\n        \"\"\"\"\"\"\r\n        fromcursor_act = create_action(self, text=_('Go to cursor position'),\r\n                                       icon=ima.icon('fromcursor'),\r\n                                       triggered=self.go_to_cursor_position)\r\n        fullpath_act = create_action(self, text=_('Show absolute path'),\r\n                                     toggled=self.toggle_fullpath_mode)\r\n        fullpath_act.setChecked(self.show_fullpath)\r\n        allfiles_act = create_action(self, text=_('Show all files'),\r\n                                     toggled=self.toggle_show_all_files)\r\n        allfiles_act.setChecked(self.show_all_files)\r\n        comment_act = create_action(self, text=_('Show special comments'),\r\n                                    toggled=self.toggle_show_comments)\r\n        comment_act.setChecked(self.show_comments)\r\n        group_cells_act = create_action(self, text=_('Group code cells'),\r\n                                        toggled=self.toggle_group_cells)\r\n        group_cells_act.setChecked(self.group_cells)\r\n        sort_files_alphabetically_act = create_action(\r\n            self, text=_('Sort files alphabetically'),\r\n            toggled=self.toggle_sort_files_alphabetically)\r\n        sort_files_alphabetically_act.setChecked(\r\n            self.sort_files_alphabetically)\r\n        actions = [fullpath_act, allfiles_act, group_cells_act, comment_act,\r\n                   sort_files_alphabetically_act, fromcursor_act]\r\n        return actions", "output": "Reimplemented OneColumnTree method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def index_fields(self, vocab: Vocabulary) -> None:\n        \"\"\"\n        \n        \"\"\"\n        if not self.indexed:\n            self.indexed = True\n            for field in self.fields.values():\n                field.index(vocab)", "output": "Indexes all fields in this ``Instance`` using the provided ``Vocabulary``.\n        This `mutates` the current object, it does not return a new ``Instance``.\n        A ``DataIterator`` will call this on each pass through a dataset; we use the ``indexed``\n        flag to make sure that indexing only happens once.\n\n        This means that if for some reason you modify your vocabulary after you've\n        indexed your instances, you might get unexpected behavior.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def volume_detach(name, profile=None, timeout=300, **kwargs):\n    '''\n    \n\n    '''\n    conn = _auth(profile, **kwargs)\n    return conn.volume_detach(\n        name,\n        timeout\n    )", "output": "Attach a block storage volume\n\n    name\n        Name of the new volume to attach\n\n    server_name\n        Name of the server to detach from\n\n    profile\n        Profile to build on\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' nova.volume_detach myblock profile=openstack", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enforce_tags(Resource, Tags, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    authargs = {'region': region, 'key': key, 'keyid': keyid, 'profile': profile}\n    current = list_tags_for_resource(Resource=Resource, **authargs)\n    if current is None:\n        log.error('Failed to list tags for CloudFront resource `%s`.', Resource)\n        return False\n    if current == Tags:  # Short-ciruits save cycles!\n        return True\n    remove = [k for k in current if k not in Tags]\n    removed = untag_resource(Resource=Resource, TagKeys=remove, **authargs)\n    if removed is False:\n        log.error('Failed to remove tags (%s) from CloudFront resource `%s`.', remove, Resource)\n        return False\n    add = {k: v for k, v in Tags.items() if current.get(k) != v}\n    added = tag_resource(Resource=Resource, Tags=add, **authargs)\n    if added is False:\n        log.error('Failed to add tags (%s) to CloudFront resource `%s`.', add, Resource)\n        return False\n    return True", "output": "Enforce a given set of tags on a CloudFront resource:  adding, removing, or changing them\n    as necessary to ensure the resource's tags are exactly and only those specified.\n\n    Resource\n        The ARN of the affected CloudFront resource.\n\n    Tags\n        Dict of {'Tag': 'Value', ...} providing the tags to be enforced.\n\n    region\n        Region to connect to.\n\n    key\n        Secret key to use.\n\n    keyid\n        Access key to use.\n\n    profile\n        Dict, or pillar key pointing to a dict, containing AWS region/key/keyid.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_cloudfront.enforce_tags Tags='{Owner: Infra, Role: salt_master}' \\\\\n                Resource='arn:aws:cloudfront::012345678012:distribution/ETLNABCDEF123'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_lib(self, version):\n        \"\"\"\n        \n        \"\"\"\n        if os.path.exists(POETRY_LIB_BACKUP):\n            shutil.rmtree(POETRY_LIB_BACKUP)\n\n        # Backup the current installation\n        if os.path.exists(POETRY_LIB):\n            shutil.copytree(POETRY_LIB, POETRY_LIB_BACKUP)\n            shutil.rmtree(POETRY_LIB)\n\n        try:\n            self._make_lib(version)\n        except Exception:\n            if not os.path.exists(POETRY_LIB_BACKUP):\n                raise\n\n            shutil.copytree(POETRY_LIB_BACKUP, POETRY_LIB)\n            shutil.rmtree(POETRY_LIB_BACKUP)\n\n            raise\n        finally:\n            if os.path.exists(POETRY_LIB_BACKUP):\n                shutil.rmtree(POETRY_LIB_BACKUP)", "output": "Packs everything into a single lib/ directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fill(self, background_shape, img):\n        \"\"\"\n        \n        \"\"\"\n        background_shape = tuple(background_shape)\n        return self._fill(background_shape, img)", "output": "Return a proper background image of background_shape, given img.\n\n        Args:\n            background_shape (tuple): a shape (h, w)\n            img: an image\n        Returns:\n            a background image", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_np(v):\n    ''''''\n    if isinstance(v, float): return np.array(v)\n    if isinstance(v, (np.ndarray, np.generic)): return v\n    if isinstance(v, (list,tuple)): return [to_np(o) for o in v]\n    if isinstance(v, Variable): v=v.data\n    if torch.cuda.is_available():\n        if is_half_tensor(v): v=v.float()\n    if isinstance(v, torch.FloatTensor): v=v.float()\n    return v.cpu().numpy()", "output": "returns an np.array object given an input of np.array, list, tuple, torch variable or tensor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_recent_repeated_responses(chatbot, conversation, sample=10, threshold=3, quantity=3):\n    \"\"\"\n    \n    \"\"\"\n    from collections import Counter\n\n    # Get the most recent statements from the conversation\n    conversation_statements = list(chatbot.storage.filter(\n        conversation=conversation,\n        order_by=['id']\n    ))[sample * -1:]\n\n    text_of_recent_responses = [\n        statement.text for statement in conversation_statements\n    ]\n\n    counter = Counter(text_of_recent_responses)\n\n    # Find the n most common responses from the conversation\n    most_common = counter.most_common(quantity)\n\n    return [\n        counted[0] for counted in most_common\n        if counted[1] >= threshold\n    ]", "output": "A filter that eliminates possibly repetitive responses to prevent\n    a chat bot from repeating statements that it has recently said.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(app_id):\n    '''\n    \n    '''\n    cmd = 'sqlite3 \"/Library/Application Support/com.apple.TCC/TCC.db\" ' \\\n          '\"DELETE from access where client=\\'{0}\\'\"'.format(app_id)\n    call = __salt__['cmd.run_all'](\n        cmd,\n        output_loglevel='debug',\n        python_shell=False\n    )\n\n    if call['retcode'] != 0:\n        comment = ''\n        if 'stderr' in call:\n            comment += call['stderr']\n        if 'stdout' in call:\n            comment += call['stdout']\n\n        raise CommandExecutionError('Error removing app: {0}'.format(comment))\n\n    return True", "output": "Remove a bundle ID or command as being allowed to use assistive access.\n\n    app_id\n        The bundle ID or command to remove from assistive access list.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' assistive.remove /usr/bin/osascript\n        salt '*' assistive.remove com.smileonmymac.textexpander", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_train_begin(self, **kwargs:Any)->None:\n        \"\"\n        self.best = float('inf') if self.operator == np.less else -float('inf')", "output": "Initializes the best value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def position_to_value(self, y):\n        \"\"\"\"\"\"\n        vsb = self.editor.verticalScrollBar()\n        return vsb.minimum()+max([0, (y-self.offset)/self.get_scale_factor()])", "output": "Convert position in pixels to value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def running(opts):\n    '''\n    \n    '''\n\n    ret = []\n    proc_dir = os.path.join(opts['cachedir'], 'proc')\n    if not os.path.isdir(proc_dir):\n        return ret\n    for fn_ in os.listdir(proc_dir):\n        path = os.path.join(proc_dir, fn_)\n        try:\n            data = _read_proc_file(path, opts)\n            if data is not None:\n                ret.append(data)\n        except (IOError, OSError):\n            # proc files may be removed at any time during this process by\n            # the minion process that is executing the JID in question, so\n            # we must ignore ENOENT during this process\n            pass\n    return ret", "output": "Return the running jobs on this minion", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge(left, right, merged):\n    \"\"\" \n    \"\"\"\n\n    left_cursor, right_cursor = 0, 0\n    while left_cursor < len(left) and right_cursor < len(right):\n        # Sort each one and place into the result\n        if left[left_cursor] <= right[right_cursor]:\n            merged[left_cursor+right_cursor]=left[left_cursor]\n            left_cursor += 1\n        else:\n            merged[left_cursor + right_cursor] = right[right_cursor]\n            right_cursor += 1\n    # Add the left overs if there's any left to the result\n    for left_cursor in range(left_cursor, len(left)):\n        merged[left_cursor + right_cursor] = left[left_cursor]\n    # Add the left overs if there's any left to the result\n    for right_cursor in range(right_cursor, len(right)):\n        merged[left_cursor + right_cursor] = right[right_cursor]\n\n    # Return result\n    return merged", "output": "Merge helper\n        Complexity: O(n)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mtf_image_transformer_base_imagenet():\n  \"\"\"\"\"\"\n  hparams = mtf_image_transformer_base_cifar()\n  hparams.mesh_shape = \"batch:32\"\n  hparams.layout = \"batch:batch\"\n  hparams.batch_size = 128\n  hparams.d_ff = 2048\n  hparams.hidden_size = 512\n  hparams.num_decoder_layers = 12\n  hparams.learning_rate = 0.5\n  hparams.learning_rate_warmup_steps = 31250\n  hparams.layer_preprocess_sequence = \"none\"\n  hparams.layer_postprocess_sequence = \"dan\"\n  hparams.layer_prepostprocess_dropout = 0.1\n  hparams.unconditional = True\n  return hparams", "output": "Data parallel CIFAR parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cp(device, from_minor, to_minor):  # pylint: disable=C0103\n    '''\n    \n    '''\n    _validate_device(device)\n\n    try:\n        int(from_minor)\n        int(to_minor)\n    except Exception:\n        raise CommandExecutionError(\n            'Invalid minor number passed to partition.cp'\n        )\n\n    cmd = 'parted -m -s {0} cp {1} {2}'.format(device, from_minor, to_minor)\n    out = __salt__['cmd.run'](cmd).splitlines()\n    return out", "output": "Copies the file system on the partition <from-minor> to partition\n    <to-minor>, deleting the original contents of the destination\n    partition.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' partition.cp /dev/sda 2 3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def foreach_model(self, fn):\n        \"\"\"\n        \"\"\"\n\n        results = ray.get([w.foreach_model.remote(fn) for w in self.workers])\n        out = []\n        for r in results:\n            out.extend(r)\n        return out", "output": "Apply the given function to each model replica in each worker.\n\n        Returns:\n            List of results from applying the function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def del_password(name, root=None):\n    '''\n    \n    '''\n    cmd = ['passwd']\n    if root is not None:\n        cmd.extend(('-R', root))\n    cmd.extend(('-d', name))\n\n    __salt__['cmd.run'](cmd, python_shell=False, output_loglevel='quiet')\n    uinfo = info(name, root=root)\n    return not uinfo['passwd'] and uinfo['name'] == name", "output": ".. versionadded:: 2014.7.0\n\n    Delete the password from name user\n\n    name\n        User to delete\n\n    root\n        Directory to chroot into\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' shadow.del_password username", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_pretty_usage(usage):\n    \"\"\"\n    \n    \"\"\"\n\n    if usage is not None and usage.strip() != \"\":\n        usage = \"\\n\".join(map(lambda u: u.strip(), usage.split(\"\\n\")))\n        return \"%s\\n\\n\" % usage", "output": "Makes the usage description pretty and returns a formatted string if `usage`\n    is not an empty string. Otherwise, returns None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def channel_interpolate(layer1, n_channel1, layer2, n_channel2):\n  \"\"\"\n  \"\"\"\n  def inner(T):\n    batch_n = T(layer1).get_shape().as_list()[0]\n    arr1 = T(layer1)[..., n_channel1]\n    arr2 = T(layer2)[..., n_channel2]\n    weights = (np.arange(batch_n)/float(batch_n-1))\n    S = 0\n    for n in range(batch_n):\n      S += (1-weights[n]) * tf.reduce_mean(arr1[n])\n      S += weights[n] * tf.reduce_mean(arr2[n])\n    return S\n  return inner", "output": "Interpolate between layer1, n_channel1 and layer2, n_channel2.\n\n  Optimize for a convex combination of layer1, n_channel1 and\n  layer2, n_channel2, transitioning across the batch.\n\n  Args:\n    layer1: layer to optimize 100% at batch=0.\n    n_channel1: neuron index to optimize 100% at batch=0.\n    layer2: layer to optimize 100% at batch=N.\n    n_channel2: neuron index to optimize 100% at batch=N.\n\n  Returns:\n    Objective", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathNextParent(self, ctxt):\n        \"\"\" \"\"\"\n        if ctxt is None: ctxt__o = None\n        else: ctxt__o = ctxt._o\n        ret = libxml2mod.xmlXPathNextParent(ctxt__o, self._o)\n        if ret is None:raise xpathError('xmlXPathNextParent() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Traversal function for the \"parent\" direction The parent\n          axis contains the parent of the context node, if there is\n           one.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_root_path(self, language):\n        \"\"\"\n        \n        \"\"\"\n        path = None\n\n        # Get path of the current project\n        if self.main and self.main.projects:\n            path = self.main.projects.get_active_project_path()\n\n        # If there's no project, use the output of getcwd_or_home.\n        if not path:\n            # We can't use getcwd_or_home for Python because if it\n            # returns home and you have a lot of Python files on it\n            # then computing Rope completions takes a long time\n            # and blocks the PyLS server.\n            # Instead we use an empty directory inside our config one,\n            # just like we did for Rope in Spyder 3.\n            if language == 'python':\n                path = get_conf_path('lsp_root_path')\n                if not osp.exists(path):\n                    os.mkdir(path)\n            else:\n                path = getcwd_or_home()\n\n        return path", "output": "Get root path to pass to the LSP servers.\n\n        This can be the current project path or the output of\n        getcwd_or_home (except for Python, see below).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def minion_config(opts, vm_):\n    '''\n    \n    '''\n\n    # Don't start with a copy of the default minion opts; they're not always\n    # what we need. Some default options are Null, let's set a reasonable default\n    minion = {\n        'master': 'salt',\n        'log_level': 'info',\n        'hash_type': 'sha256',\n    }\n\n    # Now, let's update it to our needs\n    minion['id'] = vm_['name']\n    master_finger = salt.config.get_cloud_config_value('master_finger', vm_, opts)\n    if master_finger is not None:\n        minion['master_finger'] = master_finger\n    minion.update(\n        # Get ANY defined minion settings, merging data, in the following order\n        # 1. VM config\n        # 2. Profile config\n        # 3. Global configuration\n        salt.config.get_cloud_config_value(\n            'minion', vm_, opts, default={}, search_global=True\n        )\n    )\n\n    make_master = salt.config.get_cloud_config_value('make_master', vm_, opts)\n    if 'master' not in minion and make_master is not True:\n        raise SaltCloudConfigError(\n            'A master setting was not defined in the minion\\'s configuration.'\n        )\n\n    # Get ANY defined grains settings, merging data, in the following order\n    # 1. VM config\n    # 2. Profile config\n    # 3. Global configuration\n    minion.setdefault('grains', {}).update(\n        salt.config.get_cloud_config_value(\n            'grains', vm_, opts, default={}, search_global=True\n        )\n    )\n    return minion", "output": "Return a minion's configuration for the provided options and VM", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _indent(s, indent='  '):\n    \"\"\"\"\"\"\n    if isinstance(s, six.string_types):\n        return '\\n'.join(('%s%s' % (indent, line) for line in s.splitlines()))\n    return s", "output": "Intent each line with indent", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_cpu_config(config_spec, cpu_props):\n    '''\n    \n    '''\n    log.trace('Configuring virtual machine CPU '\n              'settings cpu_props=%s', cpu_props)\n    if 'count' in cpu_props:\n        config_spec.numCPUs = int(cpu_props['count'])\n    if 'cores_per_socket' in cpu_props:\n        config_spec.numCoresPerSocket = int(cpu_props['cores_per_socket'])\n    if 'nested' in cpu_props and cpu_props['nested']:\n        config_spec.nestedHVEnabled = cpu_props['nested']  # True\n    if 'hotadd' in cpu_props and cpu_props['hotadd']:\n        config_spec.cpuHotAddEnabled = cpu_props['hotadd']  # True\n    if 'hotremove' in cpu_props and cpu_props['hotremove']:\n        config_spec.cpuHotRemoveEnabled = cpu_props['hotremove']", "output": "Sets CPU core count to the given value\n\n    config_spec\n        vm.ConfigSpec object\n\n    cpu_props\n        CPU properties dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(self, key, where=None, start=None, stop=None):\n        \"\"\"\n        \n\n        \"\"\"\n        where = _ensure_term(where, scope_level=1)\n        try:\n            s = self.get_storer(key)\n        except KeyError:\n            # the key is not a valid store, re-raising KeyError\n            raise\n        except Exception:\n\n            if where is not None:\n                raise ValueError(\n                    \"trying to remove a node with a non-None where clause!\")\n\n            # we are actually trying to remove a node (with children)\n            s = self.get_node(key)\n            if s is not None:\n                s._f_remove(recursive=True)\n                return None\n\n        # remove the node\n        if com._all_none(where, start, stop):\n            s.group._f_remove(recursive=True)\n\n        # delete from the table\n        else:\n            if not s.is_table:\n                raise ValueError(\n                    'can only remove with where on objects written as tables')\n            return s.delete(where=where, start=start, stop=stop)", "output": "Remove pandas object partially by specifying the where condition\n\n        Parameters\n        ----------\n        key : string\n            Node to remove or delete rows from\n        where : list of Term (or convertible) objects, optional\n        start : integer (defaults to None), row number to start selection\n        stop  : integer (defaults to None), row number to stop selection\n\n        Returns\n        -------\n        number of rows removed (or None if not a Table)\n\n        Exceptions\n        ----------\n        raises KeyError if key is not a valid store", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_table(self):\r\n        \"\"\"\"\"\"\r\n        self.horizontalHeader().setStretchLastSection(True)\r\n        self.adjust_columns()\r\n        self.columnAt(0)\r\n        # Sorting columns\r\n        self.setSortingEnabled(False)\r\n        self.sortByColumn(0, Qt.DescendingOrder)", "output": "Setup table", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n    \"\"\"\n    \n    \"\"\"\n    if model == \"cnn\":\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, \"cnn\")]\n    else:\n        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]", "output": "Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset_window_layout(self):\r\n        \"\"\"\"\"\"\r\n        answer = QMessageBox.warning(self, _(\"Warning\"),\r\n                     _(\"Window layout will be reset to default settings: \"\r\n                       \"this affects window position, size and dockwidgets.\\n\"\r\n                       \"Do you want to continue?\"),\r\n                     QMessageBox.Yes | QMessageBox.No)\r\n        if answer == QMessageBox.Yes:\r\n            self.setup_layout(default=True)", "output": "Reset window layout to default", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_chinese_char(self, cp):\n        \"\"\"\"\"\"\n        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n            return True\n\n        return False", "output": "Checks whether CP is the codepoint of a CJK character.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_cached_manylinux_wheel(self, package_name, package_version, disable_progress=False):\n        \"\"\"\n        \n        \"\"\"\n        cached_wheels_dir = os.path.join(tempfile.gettempdir(), 'cached_wheels')\n        if not os.path.isdir(cached_wheels_dir):\n            os.makedirs(cached_wheels_dir)\n\n        wheel_file = '{0!s}-{1!s}-{2!s}'.format(package_name, package_version, self.manylinux_wheel_file_suffix)\n        wheel_path = os.path.join(cached_wheels_dir, wheel_file)\n\n        if not os.path.exists(wheel_path) or not zipfile.is_zipfile(wheel_path):\n            # The file is not cached, download it.\n            wheel_url = self.get_manylinux_wheel_url(package_name, package_version)\n            if not wheel_url:\n                return None\n\n            print(\" - {}=={}: Downloading\".format(package_name, package_version))\n            with open(wheel_path, 'wb') as f:\n                self.download_url_with_progress(wheel_url, f, disable_progress)\n\n            if not zipfile.is_zipfile(wheel_path):\n                return None\n        else:\n            print(\" - {}=={}: Using locally cached manylinux wheel\".format(package_name, package_version))\n\n        return wheel_path", "output": "Gets the locally stored version of a manylinux wheel. If one does not exist, the function downloads it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _render_filenames(filenames, zip_file, saltenv, template):\n    '''\n    \n    '''\n    if not template:\n        return (filenames, zip_file)\n\n    # render the path as a template using path_template_engine as the engine\n    if template not in salt.utils.templates.TEMPLATE_REGISTRY:\n        raise CommandExecutionError(\n            'Attempted to render file paths with unavailable engine '\n            '{0}'.format(template)\n        )\n\n    kwargs = {}\n    kwargs['salt'] = __salt__\n    kwargs['pillar'] = __pillar__\n    kwargs['grains'] = __grains__\n    kwargs['opts'] = __opts__\n    kwargs['saltenv'] = saltenv\n\n    def _render(contents):\n        '''\n        Render :param:`contents` into a literal pathname by writing it to a\n        temp file, rendering that file, and returning the result.\n        '''\n        # write out path to temp file\n        tmp_path_fn = salt.utils.files.mkstemp()\n        with salt.utils.files.fopen(tmp_path_fn, 'w+') as fp_:\n            fp_.write(salt.utils.stringutils.to_str(contents))\n        data = salt.utils.templates.TEMPLATE_REGISTRY[template](\n            tmp_path_fn,\n            to_str=True,\n            **kwargs\n        )\n        salt.utils.files.safe_rm(tmp_path_fn)\n        if not data['result']:\n            # Failed to render the template\n            raise CommandExecutionError(\n                'Failed to render file path with error: {0}'.format(\n                    data['data']\n                )\n            )\n        else:\n            return data['data']\n\n    filenames = _render(filenames)\n    zip_file = _render(zip_file)\n    return (filenames, zip_file)", "output": "Process markup in the :param:`filenames` and :param:`zipfile` variables (NOT the\n    files under the paths they ultimately point to) according to the markup\n    format provided by :param:`template`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lock(backend=None, remote=None):\n    '''\n    \n    '''\n    fileserver = salt.fileserver.Fileserver(__opts__)\n    locked, errors = fileserver.lock(back=backend, remote=remote)\n    ret = {}\n    if locked:\n        ret['locked'] = locked\n    if errors:\n        ret['errors'] = errors\n    if not ret:\n        return 'No locks were set'\n    return ret", "output": ".. versionadded:: 2015.5.0\n\n    Set a fileserver update lock for VCS fileserver backends (:mod:`git\n    <salt.fileserver.gitfs>`, :mod:`hg <salt.fileserver.hgfs>`, :mod:`svn\n    <salt.fileserver.svnfs>`).\n\n    .. note::\n\n        This will only operate on enabled backends (those configured in\n        :conf_master:`fileserver_backend`).\n\n    backend\n        Only set the update lock for the specified backend(s).\n\n    remote\n        If not None, then any remotes which contain the passed string will have\n        their lock cleared. For example, a ``remote`` value of ``*github.com*``\n        will remove the lock from all github.com remotes.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run fileserver.lock\n        salt-run fileserver.lock backend=git,hg\n        salt-run fileserver.lock backend=git remote='*github.com*'\n        salt-run fileserver.lock remote=bitbucket", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_scanner (type, prop_set):\n    \"\"\" \n    \"\"\"\n    if __debug__:\n        from .property_set import PropertySet\n        assert isinstance(type, basestring)\n        assert isinstance(prop_set, PropertySet)\n    if registered (type):\n        scanner_type = __types [type]['scanner']\n        if scanner_type:\n            return scanner.get (scanner_type, prop_set.raw ())\n            pass\n\n    return None", "output": "Returns a scanner instance appropriate to 'type' and 'property_set'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __init_from_np2d(self, mat, params_str, ref_dataset):\n        \"\"\"\"\"\"\n        if len(mat.shape) != 2:\n            raise ValueError('Input numpy.ndarray must be 2 dimensional')\n\n        self.handle = ctypes.c_void_p()\n        if mat.dtype == np.float32 or mat.dtype == np.float64:\n            data = np.array(mat.reshape(mat.size), dtype=mat.dtype, copy=False)\n        else:\n            # change non-float data to float data, need to copy\n            data = np.array(mat.reshape(mat.size), dtype=np.float32)\n\n        ptr_data, type_ptr_data, _ = c_float_array(data)\n        _safe_call(_LIB.LGBM_DatasetCreateFromMat(\n            ptr_data,\n            ctypes.c_int(type_ptr_data),\n            ctypes.c_int(mat.shape[0]),\n            ctypes.c_int(mat.shape[1]),\n            ctypes.c_int(C_API_IS_ROW_MAJOR),\n            c_str(params_str),\n            ref_dataset,\n            ctypes.byref(self.handle)))\n        return self", "output": "Initialize data from a 2-D numpy matrix.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def public_url(self):\n        \"\"\"\n        \"\"\"\n        return \"{storage_base_url}/{bucket_name}/{quoted_name}\".format(\n            storage_base_url=_API_ACCESS_ENDPOINT,\n            bucket_name=self.bucket.name,\n            quoted_name=quote(self.name.encode(\"utf-8\")),\n        )", "output": "The public URL for this blob.\n\n        Use :meth:`make_public` to enable anonymous access via the returned\n        URL.\n\n        :rtype: `string`\n        :returns: The public URL for this blob.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _jitter(c, magnitude:uniform):\n    \"\"\n    c.flow.add_((torch.rand_like(c.flow)-0.5)*magnitude*2)\n    return c", "output": "Replace pixels by random neighbors at `magnitude`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def request_cert(name, master, ticket, port=\"5665\"):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n    cert = \"{0}ca.crt\".format(get_certs_path())\n\n    # Checking if execution is needed.\n    if os.path.isfile(cert):\n        ret['comment'] = 'No execution needed. Cert: {0} already exists.'.format(cert)\n        return ret\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Certificate request from icinga2 master would be executed'\n        return ret\n\n    # Executing the command.\n    cert_request = __salt__['icinga2.request_cert'](name, master, ticket, port)\n    if not cert_request['retcode']:\n        ret['comment'] = \"Certificate request from icinga2 master executed\"\n        ret['changes']['cert'] = \"Executed. Certificate requested: {0}\".format(cert)\n        return ret\n\n    ret['comment'] = \"FAILED. Certificate requested failed with output: {0}\".format(cert_request['stdout'])\n    ret['result'] = False\n    return ret", "output": "Request CA certificate from master icinga2 node.\n\n    name\n        The domain name for which this certificate will be saved\n\n    master\n        Icinga2 master node for which this certificate will be saved\n\n    ticket\n        Authentication ticket generated on icinga2 master\n\n    port\n        Icinga2 port, defaults to 5665", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_all(self):\n        \"\"\"\n        \"\"\"\n        if not self.vars:\n            return self.parent\n        if not self.parent:\n            return self.vars\n        return dict(self.parent, **self.vars)", "output": "Return the complete context as dict including the exported\n        variables.  For optimizations reasons this might not return an\n        actual copy so be careful with using it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _GetMessageFromFactory(factory, full_name):\n  \"\"\"\n  \"\"\"\n  proto_descriptor = factory.pool.FindMessageTypeByName(full_name)\n  proto_cls = factory.GetPrototype(proto_descriptor)\n  return proto_cls", "output": "Get a proto class from the MessageFactory by name.\n\n  Args:\n    factory: a MessageFactory instance.\n    full_name: str, the fully qualified name of the proto type.\n  Returns:\n    A class, for the type identified by full_name.\n  Raises:\n    KeyError, if the proto is not found in the factory's descriptor pool.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rename(self, new_name):\r\n        \"\"\"\"\"\"\r\n        old_name = self.name\r\n        self.name = new_name\r\n        pypath = self.relative_pythonpath  # ??\r\n        self.root_path = self.root_path[:-len(old_name)]+new_name\r\n        self.relative_pythonpath = pypath  # ??\r\n        self.save()", "output": "Rename project and rename its root path accordingly.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Expand(self, macro_ref_str):\n    \"\"\"\n    \"\"\"\n    match = _MACRO_RE.match(macro_ref_str)\n    if match is None or match.group(0) != macro_ref_str:\n      raise PDDMError('Failed to parse macro reference: \"%s\"' % macro_ref_str)\n    if match.group('name') not in self._macros:\n      raise PDDMError('No macro named \"%s\".' % match.group('name'))\n    return self._Expand(match, [], macro_ref_str)", "output": "Expands the macro reference.\n\n    Args:\n      macro_ref_str: String of a macro reference (i.e. foo(a, b)).\n\n    Returns:\n      The text from the expansion.\n\n    Raises:\n      PDDMError if there are any issues.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set(config, section, opt, value):\n        \"\"\"\n        \"\"\"\n        if section not in config.keys():\n            config[section] = {}\n\n        config[section][opt] = value", "output": "Sets specified option in the config.\n\n        Args:\n            config (configobj.ConfigObj): config to work on.\n            section (str): section name.\n            opt (str): option name.\n            value: value to set option to.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format(self, source):\n        \"\"\"\n        \"\"\"\n        self._jwrite = self._jwrite.format(source)\n        return self", "output": "Specifies the underlying output data source.\n\n        :param source: string, name of the data source, e.g. 'json', 'parquet'.\n\n        >>> df.write.format('json').save(os.path.join(tempfile.mkdtemp(), 'data'))", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_params(net):\n    \"\"\"\"\"\"\n    for module in net.modules():\n        if isinstance(module, nn.Conv2d):\n            init.kaiming_normal(module.weight, mode=\"fan_out\")\n            if module.bias:\n                init.constant(module.bias, 0)\n        elif isinstance(module, nn.BatchNorm2d):\n            init.constant(module.weight, 1)\n            init.constant(module.bias, 0)\n        elif isinstance(module, nn.Linear):\n            init.normal(module.weight, std=1e-3)\n            if module.bias:\n                init.constant(module.bias, 0)", "output": "Init layer parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_warning_menu(self):\r\n        \"\"\"\"\"\"\r\n        editor = self.get_current_editor()\r\n        check_results = editor.get_current_warnings()\r\n        self.warning_menu.clear()\r\n        filename = self.get_current_filename()\r\n        for message, line_number in check_results:\r\n            error = 'syntax' in message\r\n            text = message[:1].upper() + message[1:]\r\n            icon = ima.icon('error') if error else ima.icon('warning')\r\n            slot = lambda _checked, _l=line_number: self.load(filename, goto=_l)\r\n            action = create_action(self, text=text, icon=icon, triggered=slot)\r\n            self.warning_menu.addAction(action)", "output": "Update warning list menu", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chugid_and_umask(runas, umask, group=None):\n    '''\n    \n    '''\n    set_runas = False\n    set_grp = False\n\n    current_user = getpass.getuser()\n    if runas and runas != current_user:\n        set_runas = True\n        runas_user = runas\n    else:\n        runas_user = current_user\n\n    current_grp = grp.getgrgid(pwd.getpwnam(getpass.getuser()).pw_gid).gr_name\n    if group and group != current_grp:\n        set_grp = True\n        runas_grp = group\n    else:\n        runas_grp = current_grp\n\n    if set_runas or set_grp:\n        chugid(runas_user, runas_grp)\n    if umask is not None:\n        os.umask(umask)", "output": "Helper method for for subprocess.Popen to initialise uid/gid and umask\n    for the new process.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gae_advantages(td_deltas, mask, lambda_=0.95, gamma=0.99):\n  \n  \"\"\"\n\n  return rewards_to_go(td_deltas, mask, lambda_ * gamma)", "output": "r\"\"\"Computes the GAE advantages given the one step TD-residuals.\n\n  The formula for a GAE advantage estimator is as follows:\n\n  A_{bt} = \\sum_{l=0}^{\\infty}(\\gamma * \\lambda)^{l}(\\delta_{b,t+l}).\n\n  Internally we just call rewards_to_go, since it is the same computation.\n\n  Args:\n    td_deltas: np.ndarray of shape (B, T) of one step TD-residuals.\n    mask: np.ndarray of shape (B, T) of mask for the residuals. It maybe the\n      case that the `td_deltas` are already masked correctly since they are\n      produced by `deltas(...)`\n    lambda_: float, lambda parameter for GAE estimators.\n    gamma: float, lambda parameter for GAE estimators.\n\n  Returns:\n    GAE advantage estimates.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def highlight_max(self, subset=None, color='yellow', axis=0):\n        \"\"\"\n        \n        \"\"\"\n        return self._highlight_handler(subset=subset, color=color, axis=axis,\n                                       max_=True)", "output": "Highlight the maximum by shading the background.\n\n        Parameters\n        ----------\n        subset : IndexSlice, default None\n            a valid slice for ``data`` to limit the style application to.\n        color : str, default 'yellow'\n        axis : {0 or 'index', 1 or 'columns', None}, default 0\n            apply to each column (``axis=0`` or ``'index'``), to each row\n            (``axis=1`` or ``'columns'``), or to the entire DataFrame at once\n            with ``axis=None``.\n\n        Returns\n        -------\n        self : Styler", "category": "Python"}]