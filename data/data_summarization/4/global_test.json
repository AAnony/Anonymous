[{"instruction": "Summarizing the code.", "input": "def _get_encoding(self):\n        \"\"\"\"\"\"\n        enco = 'utf-8'\n\n        #  Currently only cp1252 is allowed?\n        if WIN:\n            import ctypes\n            codepage = to_text_string(ctypes.cdll.kernel32.GetACP())\n            # import locale\n            # locale.getpreferredencoding()  # Differences?\n            enco = 'cp' + codepage\n        return enco", "output": "Return the encoding/codepage to use.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode(self, ids, strip_extraneous=False):\n    \"\"\"\n    \"\"\"\n    del strip_extraneous\n    return \" \".join([str(i) for i in ids])", "output": "Transform sequence of float values into string (float values).\n\n    Args:\n      ids: array of floats to be converted.\n      strip_extraneous: unused\n\n    Returns:\n      String having space separated float values.\n\n    Raises:\n      ValueError: if the ids are not of the appropriate size.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enforce_tags(Resource, Tags, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    authargs = {'region': region, 'key': key, 'keyid': keyid, 'profile': profile}\n    current = list_tags_for_resource(Resource=Resource, **authargs)\n    if current is None:\n        log.error('Failed to list tags for CloudFront resource `%s`.', Resource)\n        return False\n    if current == Tags:  # Short-ciruits save cycles!\n        return True\n    remove = [k for k in current if k not in Tags]\n    removed = untag_resource(Resource=Resource, TagKeys=remove, **authargs)\n    if removed is False:\n        log.error('Failed to remove tags (%s) from CloudFront resource `%s`.', remove, Resource)\n        return False\n    add = {k: v for k, v in Tags.items() if current.get(k) != v}\n    added = tag_resource(Resource=Resource, Tags=add, **authargs)\n    if added is False:\n        log.error('Failed to add tags (%s) to CloudFront resource `%s`.', add, Resource)\n        return False\n    return True", "output": "Enforce a given set of tags on a CloudFront resource:  adding, removing, or changing them\n    as necessary to ensure the resource's tags are exactly and only those specified.\n\n    Resource\n        The ARN of the affected CloudFront resource.\n\n    Tags\n        Dict of {'Tag': 'Value', ...} providing the tags to be enforced.\n\n    region\n        Region to connect to.\n\n    key\n        Secret key to use.\n\n    keyid\n        Access key to use.\n\n    profile\n        Dict, or pillar key pointing to a dict, containing AWS region/key/keyid.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_cloudfront.enforce_tags Tags='{Owner: Infra, Role: salt_master}' \\\\\n                Resource='arn:aws:cloudfront::012345678012:distribution/ETLNABCDEF123'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_macs(vm_):\n    '''\n    \n    '''\n    macs = []\n    nics = get_nics(vm_)\n    if nics is None:\n        return None\n    for nic in nics:\n        macs.append(nic)\n\n    return macs", "output": "Return a list off MAC addresses from the named vm\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' virt.get_macs <vm name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sequence(start, stop, step=None):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    if step is None:\n        return Column(sc._jvm.functions.sequence(_to_java_column(start), _to_java_column(stop)))\n    else:\n        return Column(sc._jvm.functions.sequence(\n            _to_java_column(start), _to_java_column(stop), _to_java_column(step)))", "output": "Generate a sequence of integers from `start` to `stop`, incrementing by `step`.\n    If `step` is not set, incrementing by 1 if `start` is less than or equal to `stop`,\n    otherwise -1.\n\n    >>> df1 = spark.createDataFrame([(-2, 2)], ('C1', 'C2'))\n    >>> df1.select(sequence('C1', 'C2').alias('r')).collect()\n    [Row(r=[-2, -1, 0, 1, 2])]\n    >>> df2 = spark.createDataFrame([(4, -4, -2)], ('C1', 'C2', 'C3'))\n    >>> df2.select(sequence('C1', 'C2', 'C3').alias('r')).collect()\n    [Row(r=[4, 2, 0, -2, -4])]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_static_url(\n        cls, settings: Dict[str, Any], path: str, include_version: bool = True\n    ) -> str:\n        \"\"\"\n\n        \"\"\"\n        url = settings.get(\"static_url_prefix\", \"/static/\") + path\n        if not include_version:\n            return url\n\n        version_hash = cls.get_version(settings, path)\n        if not version_hash:\n            return url\n\n        return \"%s?v=%s\" % (url, version_hash)", "output": "Constructs a versioned url for the given path.\n\n        This method may be overridden in subclasses (but note that it\n        is a class method rather than an instance method).  Subclasses\n        are only required to implement the signature\n        ``make_static_url(cls, settings, path)``; other keyword\n        arguments may be passed through `~RequestHandler.static_url`\n        but are not standard.\n\n        ``settings`` is the `Application.settings` dictionary.  ``path``\n        is the static path being requested.  The url returned should be\n        relative to the current host.\n\n        ``include_version`` determines whether the generated URL should\n        include the query string containing the version hash of the\n        file corresponding to the given ``path``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mouseDoubleClickEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        index_clicked = self.indexAt(event.pos())\r\n        if index_clicked.isValid():\r\n            row = index_clicked.row()\r\n            # TODO: Remove hard coded \"Value\" column number (3 here)\r\n            index_clicked = index_clicked.child(row, 3)\r\n            self.edit(index_clicked)\r\n        else:\r\n            event.accept()", "output": "Reimplement Qt method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def label_for_lm(self, **kwargs):\n        \"\"\n        self.__class__ = LMTextList\n        kwargs['label_cls'] = LMLabelList\n        return self.label_const(0, **kwargs)", "output": "A special labelling method for language models.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _should_base64_decode_body(binary_types, flask_request, lamba_response_headers, is_base_64_encoded):\n        \"\"\"\n        \n\n        \"\"\"\n        best_match_mimetype = flask_request.accept_mimetypes.best_match([lamba_response_headers[\"Content-Type\"]])\n        is_best_match_in_binary_types = best_match_mimetype in binary_types or '*/*' in binary_types\n\n        return best_match_mimetype and is_best_match_in_binary_types and is_base_64_encoded", "output": "Whether or not the body should be decoded from Base64 to Binary\n\n        Parameters\n        ----------\n        binary_types list(basestring)\n            Corresponds to self.binary_types (aka. what is parsed from SAM Template\n        flask_request flask.request\n            Flask request\n        lamba_response_headers dict\n            Headers Lambda returns\n        is_base_64_encoded bool\n            True if the body is Base64 encoded\n\n        Returns\n        -------\n        True if the body from the request should be converted to binary, otherwise false", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def plot_metrics(self, skip_start:int=0, skip_end:int=0, return_fig:bool=None)->Optional[plt.Figure]:\n        \"\"\n        assert len(self.metrics) != 0, \"There are no metrics to plot.\"\n        fig, axes = plt.subplots(len(self.metrics[0]),1,figsize=(6, 4*len(self.metrics[0])))\n        val_iter = self._split_list_val(np.cumsum(self.nb_batches), skip_start, skip_end)\n        axes = axes.flatten() if len(self.metrics[0]) != 1 else [axes]\n        for i, ax in enumerate(axes):\n            values = [met[i] for met in self.metrics]\n            values = self._split_list_val(values, skip_start, skip_end)\n            ax.plot(val_iter, values)\n            ax.set_ylabel(str(self.metrics_names[i]))\n            ax.set_xlabel('Batches processed')             \n        if ifnone(return_fig, defaults.return_fig): return fig\n        if not IN_NOTEBOOK: plot_sixel(fig)", "output": "Plot metrics collected during training.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cooldown(rate, per, type=BucketType.default):\n    \"\"\"\n    \"\"\"\n\n    def decorator(func):\n        if isinstance(func, Command):\n            func._buckets = CooldownMapping(Cooldown(rate, per, type))\n        else:\n            func.__commands_cooldown__ = Cooldown(rate, per, type)\n        return func\n    return decorator", "output": "A decorator that adds a cooldown to a :class:`.Command`\n    or its subclasses.\n\n    A cooldown allows a command to only be used a specific amount\n    of times in a specific time frame. These cooldowns can be based\n    either on a per-guild, per-channel, per-user, or global basis.\n    Denoted by the third argument of ``type`` which must be of enum\n    type ``BucketType`` which could be either:\n\n    - ``BucketType.default`` for a global basis.\n    - ``BucketType.user`` for a per-user basis.\n    - ``BucketType.guild`` for a per-guild basis.\n    - ``BucketType.channel`` for a per-channel basis.\n    - ``BucketType.member`` for a per-member basis.\n    - ``BucketType.category`` for a per-category basis.\n\n    If a cooldown is triggered, then :exc:`.CommandOnCooldown` is triggered in\n    :func:`.on_command_error` and the local error handler.\n\n    A command can only have a single cooldown.\n\n    Parameters\n    ------------\n    rate: :class:`int`\n        The number of times a command can be used before triggering a cooldown.\n    per: :class:`float`\n        The amount of seconds to wait for a cooldown when it's been triggered.\n    type: ``BucketType``\n        The type of cooldown to have.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def file_list(package, **kwargs):\n    '''\n    \n    '''\n    ret = file_dict(package)\n    files = []\n    for pkg_files in six.itervalues(ret['files']):\n        files.extend(pkg_files)\n    ret['files'] = files\n    return ret", "output": "List the files that belong to a package.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' pkg.file_list nginx", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def release(self, xcoord, ycoord):\n        \"\"\"\n        \n        \"\"\"\n        self._actions.append(lambda: self._driver.execute(\n            Command.TOUCH_UP, {\n                'x': int(xcoord),\n                'y': int(ycoord)}))\n        return self", "output": "Release previously issued tap 'and hold' command at specified location.\n\n        :Args:\n         - xcoord: X Coordinate to release.\n         - ycoord: Y Coordinate to release.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bar_amplitude(self):\n        \"\"\n        res = (self.high - self.low) / self.low\n        res.name = 'bar_amplitude'\n        return res", "output": "\u8fd4\u56debar\u632f\u5e45", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_SU_save_stock_xdxr(engine, client=DATABASE):\n    \"\"\"\n    \"\"\"\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_stock_xdxr(client=client)", "output": "save stock_xdxr\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ca_exists(ca_name, cacert_path=None, ca_filename=None):\n    '''\n    \n    '''\n    set_ca_path(cacert_path)\n    if not ca_filename:\n        ca_filename = '{0}_ca_cert'.format(ca_name)\n    certp = '{0}/{1}/{2}.crt'.format(\n            cert_base_path(),\n            ca_name,\n            ca_filename)\n    if os.path.exists(certp):\n        maybe_fix_ssl_version(ca_name,\n                              cacert_path=cacert_path,\n                              ca_filename=ca_filename)\n        return True\n    return False", "output": "Verify whether a Certificate Authority (CA) already exists\n\n    ca_name\n        name of the CA\n    cacert_path\n        absolute path to ca certificates root directory\n    ca_filename\n        alternative filename for the CA\n\n        .. versionadded:: 2015.5.3\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' tls.ca_exists test_ca /etc/certs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_line_indent(src, line, indent):\n    '''\n    \n    '''\n    if not indent:\n        return line\n\n    idt = []\n    for c in src:\n        if c not in ['\\t', ' ']:\n            break\n        idt.append(c)\n\n    return ''.join(idt) + line.lstrip()", "output": "Indent the line with the source line.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compose(funcs:List[Callable])->Callable:\n    \"\"\n    def compose_(funcs, x, *args, **kwargs):\n        for f in listify(funcs): x = f(x, *args, **kwargs)\n        return x\n    return partial(compose_, funcs)", "output": "Compose `funcs`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_data(path, tokenizer, char_vcb, word_vcb, is_training=False):\n    '''\n    \n    '''\n    global root_path\n    qp_pairs = data.load_from_file(path=path, is_training=is_training)\n\n    tokenized_sent = 0\n    # qp_pairs = qp_pairs[:1000]1\n    for qp_pair in qp_pairs:\n        tokenized_sent += 1\n        data.tokenize(qp_pair, tokenizer, is_training)\n        for word in qp_pair['question_tokens']:\n            word_vcb.add(word['word'])\n            for char in word['word']:\n                char_vcb.add(char)\n        for word in qp_pair['passage_tokens']:\n            word_vcb.add(word['word'])\n            for char in word['word']:\n                char_vcb.add(char)\n\n    max_query_length = max(len(x['question_tokens']) for x in qp_pairs)\n    max_passage_length = max(len(x['passage_tokens']) for x in qp_pairs)\n    #min_passage_length = min(len(x['passage_tokens']) for x in qp_pairs)\n    cfg.max_query_length = max_query_length\n    cfg.max_passage_length = max_passage_length\n\n    return qp_pairs", "output": "Generate data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _http_date(_date_str: str) -> Optional[datetime.datetime]:\n        \"\"\"\n        \"\"\"\n        if _date_str is not None:\n            timetuple = parsedate(_date_str)\n            if timetuple is not None:\n                return datetime.datetime(*timetuple[:6],\n                                         tzinfo=datetime.timezone.utc)\n        return None", "output": "Process a date string, return a datetime object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dragMoveEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        index = self.indexAt(event.pos())\r\n        if index:\r\n            dst = self.get_filename(index)\r\n            if osp.isdir(dst):\r\n                event.acceptProposedAction()\r\n            else:\r\n                event.ignore()\r\n        else:\r\n            event.ignore()", "output": "Reimplement Qt method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_namespaced_daemon_set_status(self, name, namespace, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_daemon_set_status_with_http_info(name, namespace, **kwargs)\n        else:\n            (data) = self.read_namespaced_daemon_set_status_with_http_info(name, namespace, **kwargs)\n            return data", "output": "read status of the specified DaemonSet\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_daemon_set_status(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the DaemonSet (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1DaemonSet\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def perform(self):\n        \"\"\"\n        \n        \"\"\"\n        if self._driver.w3c:\n            self.w3c_actions.perform()\n        else:\n            for action in self._actions:\n                action()", "output": "Performs all stored actions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bought_value(self):\n        \"\"\"\n        \n        \"\"\"\n        user_system_log.warn(_(u\"[abandon] {} is no longer valid.\").format('stock_position.bought_value'))\n        return self._quantity * self._avg_price", "output": "[\u5df2\u5f03\u7528]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_sum(path, form='sha256'):\n    '''\n    \n    '''\n    path = os.path.expanduser(path)\n\n    if not os.path.isfile(path):\n        return 'File not found'\n    return salt.utils.hashutils.get_hash(path, form, 4096)", "output": "Return the checksum for the given file. The following checksum algorithms\n    are supported:\n\n    * md5\n    * sha1\n    * sha224\n    * sha256 **(default)**\n    * sha384\n    * sha512\n\n    path\n        path to the file or directory\n\n    form\n        desired sum format\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.get_sum /etc/passwd sha512", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _select_features(example, feature_list=None):\n  \"\"\"\"\"\"\n  feature_list = feature_list or [\"inputs\", \"targets\"]\n  return {f: example[f] for f in feature_list}", "output": "Select a subset of features from the example dict.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_learner(path:PathOrStr, file:PathLikeOrBinaryStream='export.pkl', test:ItemList=None, **db_kwargs):\n    \"\"\n    source = Path(path)/file if is_pathlike(file) else file\n    state = torch.load(source, map_location='cpu') if defaults.device == torch.device('cpu') else torch.load(source)\n    model = state.pop('model')\n    src = LabelLists.load_state(path, state.pop('data'))\n    if test is not None: src.add_test(test)\n    data = src.databunch(**db_kwargs)\n    cb_state = state.pop('cb_state')\n    clas_func = state.pop('cls')\n    res = clas_func(data, model, **state)\n    res.callback_fns = state['callback_fns'] #to avoid duplicates\n    res.callbacks = [load_callback(c,s, res) for c,s in cb_state.items()]\n    return res", "output": "Load a `Learner` object saved with `export_state` in `path/file` with empty data, optionally add `test` and load on `cpu`. `file` can be file-like (file or buffer)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_parameters(self, parameter_id):\n        \"\"\"\n        \"\"\"\n        if not self.population:\n            raise RuntimeError('The population is empty')\n        pos = -1\n        for i in range(len(self.population)):\n            if self.population[i].result is None:\n                pos = i\n                break\n        if pos != -1:\n            indiv = copy.deepcopy(self.population[pos])\n            self.population.pop(pos)\n            total_config = indiv.config\n        else:\n            random.shuffle(self.population)\n            if self.population[0].result < self.population[1].result:\n                self.population[0] = self.population[1]\n\n            # mutation\n            space = json2space(self.searchspace_json,\n                               self.population[0].config)\n            is_rand = dict()\n            mutation_pos = space[random.randint(0, len(space)-1)]\n            for i in range(len(self.space)):\n                is_rand[self.space[i]] = (self.space[i] == mutation_pos)\n            config = json2paramater(\n                self.searchspace_json, is_rand, self.random_state, self.population[0].config)\n            self.population.pop(1)\n            # remove \"_index\" from config and save params-id\n\n            total_config = config\n        self.total_data[parameter_id] = total_config\n        config = _split_index(total_config)\n        return config", "output": "Returns a dict of trial (hyper-)parameters, as a serializable object.\n\n        Parameters\n        ----------\n        parameter_id : int\n    \n        Returns\n        -------\n        config : dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_cond_latents_at_level(cond_latents, level, hparams):\n  \"\"\"\"\"\"\n  if cond_latents:\n    if hparams.latent_dist_encoder in [\"conv_net\", \"conv3d_net\"]:\n      return [cond_latent[level] for cond_latent in cond_latents]\n    elif hparams.latent_dist_encoder in [\"pointwise\", \"conv_lstm\"]:\n      return cond_latents[level]", "output": "Returns a single or list of conditional latents at level 'level'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def status(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            return self.__dict__[\"status\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'status' \".format(self.order_book_id)\n            )", "output": "[str] \u5408\u7ea6\u72b6\u6001\u3002\u2019Active\u2019 - \u6b63\u5e38\u4e0a\u5e02, \u2018Delisted\u2019 - \u7ec8\u6b62\u4e0a\u5e02, \u2018TemporarySuspended\u2019 - \u6682\u505c\u4e0a\u5e02,\n        \u2018PreIPO\u2019 - \u53d1\u884c\u914d\u552e\u671f\u95f4, \u2018FailIPO\u2019 - \u53d1\u884c\u5931\u8d25\uff08\u80a1\u7968\u4e13\u7528\uff09", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pattern_match(pattern, string):\n    \"\"\"\n    \n    \"\"\"\n    def backtrack(pattern, string, dic):\n\n        if len(pattern) == 0 and len(string) > 0:\n            return False\n\n        if len(pattern) == len(string) == 0:\n            return True\n\n        for end in range(1, len(string)-len(pattern)+2):\n            if pattern[0] not in dic and string[:end] not in dic.values():\n                dic[pattern[0]] = string[:end]\n                if backtrack(pattern[1:], string[end:], dic):\n                    return True\n                del dic[pattern[0]]\n            elif pattern[0] in dic and dic[pattern[0]] == string[:end]:\n                if backtrack(pattern[1:], string[end:], dic):\n                    return True\n        return False\n\n    return backtrack(pattern, string, {})", "output": ":type pattern: str\n    :type string: str\n    :rtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def syslog(server, enable=True):\n    '''\n    \n    '''\n    if enable and __execute_cmd('config -g cfgRemoteHosts -o \\\n                cfgRhostsSyslogEnable 1'):\n        return __execute_cmd('config -g cfgRemoteHosts -o \\\n                cfgRhostsSyslogServer1 {0}'.format(server))\n\n    return __execute_cmd('config -g cfgRemoteHosts -o cfgRhostsSyslogEnable 0')", "output": "Configure syslog remote logging, by default syslog will automatically be\n    enabled if a server is specified. However, if you want to disable syslog\n    you will need to specify a server followed by False\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt dell drac.syslog [SYSLOG IP] [ENABLE/DISABLE]\n        salt dell drac.syslog 0.0.0.0 False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge_subtokens(doc, label=\"subtok\"):\n    \"\"\"\n    \"\"\"\n    merger = Matcher(doc.vocab)\n    merger.add(\"SUBTOK\", None, [{\"DEP\": label, \"op\": \"+\"}])\n    matches = merger(doc)\n    spans = [doc[start : end + 1] for _, start, end in matches]\n    with doc.retokenize() as retokenizer:\n        for span in spans:\n            retokenizer.merge(span)\n    return doc", "output": "Merge subtokens into a single token.\n\n    doc (Doc): The Doc object.\n    label (unicode): The subtoken dependency label.\n    RETURNS (Doc): The Doc object with merged subtokens.\n\n    DOCS: https://spacy.io/api/pipeline-functions#merge_subtokens", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_vgg(num_layers, pretrained=False, ctx=cpu(),\n            root=os.path.join(base.data_dir(), 'models'), **kwargs):\n    \n    \"\"\"\n    layers, filters = vgg_spec[num_layers]\n    net = VGG(layers, filters, **kwargs)\n    if pretrained:\n        from ..model_store import get_model_file\n        batch_norm_suffix = '_bn' if kwargs.get('batch_norm') else ''\n        net.load_parameters(get_model_file('vgg%d%s'%(num_layers, batch_norm_suffix),\n                                           root=root), ctx=ctx)\n    return net", "output": "r\"\"\"VGG model from the `\"Very Deep Convolutional Networks for Large-Scale Image Recognition\"\n    <https://arxiv.org/abs/1409.1556>`_ paper.\n\n    Parameters\n    ----------\n    num_layers : int\n        Number of layers for the variant of densenet. Options are 11, 13, 16, 19.\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    ctx : Context, default CPU\n        The context in which to load the pretrained weights.\n    root : str, default $MXNET_HOME/models\n        Location for keeping the model parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_string(format, *cols):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.format_string(format, _to_seq(sc, cols, _to_java_column)))", "output": "Formats the arguments in printf-style and returns the result as a string column.\n\n    :param col: the column name of the numeric value to be formatted\n    :param d: the N decimal places\n\n    >>> df = spark.createDataFrame([(5, \"hello\")], ['a', 'b'])\n    >>> df.select(format_string('%d %s', df.a, df.b).alias('v')).collect()\n    [Row(v=u'5 hello')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ensure_html_response(url, session):\n    # type: (str, PipSession) -> None\n    \"\"\"\n    \"\"\"\n    scheme, netloc, path, query, fragment = urllib_parse.urlsplit(url)\n    if scheme not in {'http', 'https'}:\n        raise _NotHTTP()\n\n    resp = session.head(url, allow_redirects=True)\n    resp.raise_for_status()\n\n    _ensure_html_header(resp)", "output": "Send a HEAD request to the URL, and ensure the response contains HTML.\n\n    Raises `_NotHTTP` if the URL is not available for a HEAD request, or\n    `_NotHTML` if the content type is not text/html.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next(self):\n        '''\n        \n        '''\n        if self.__buffered is None:\n            # Use floor division to force multiplier to an integer\n            multiplier = self.__max_in_mem // self.__chunk_size\n            self.__buffered = \"\"\n        else:\n            multiplier = 1\n            self.__buffered = self.__buffered[self.__chunk_size:]\n\n        data = self.__file.read(self.__chunk_size * multiplier)\n        # Data is a byte object in Python 3\n        # Decode it in order to append to self.__buffered str later\n        # Use the salt util in case it's already a string (Windows)\n        data = salt.utils.stringutils.to_str(data)\n\n        if not data:\n            self.__file.close()\n            raise StopIteration\n\n        self.__buffered += data\n        return self.__buffered", "output": "Return the next iteration by popping `chunk_size` from the left and\n        appending `chunk_size` to the right if there's info on the file left\n        to be read.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def verify_log(opts):\n    '''\n    \n    '''\n    level = LOG_LEVELS.get(str(opts.get('log_level')).lower(), logging.NOTSET)\n\n    if level < logging.INFO:\n        log.warning('Insecure logging configuration detected! Sensitive data may be logged.')", "output": "If an insecre logging configuration is found, show a warning", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def construct (self, properties = [], targets = []):\n        \"\"\" \n        \"\"\"\n        if not targets:\n            for name, project in self.projects ().projects ():\n                targets.append (project.target ())\n\n        property_groups = build_request.expand_no_defaults (properties)\n\n        virtual_targets = []\n        build_prop_sets = []\n        for p in property_groups:\n            build_prop_sets.append (property_set.create (feature.split (p)))\n\n        if not build_prop_sets:\n            build_prop_sets = [property_set.empty ()]\n\n        for build_properties in build_prop_sets:\n            for target in targets:\n                result = target.generate (build_properties)\n                virtual_targets.extend (result.targets ())\n\n        actual_targets = []\n        for virtual_target in virtual_targets:\n            actual_targets.extend (virtual_target.actualize ())", "output": "Constructs the dependency graph.\n            properties:  the build properties.\n            targets:     the targets to consider. If none is specified, uses all.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def result(self, timeout=None):\n        \"\"\"\n        \"\"\"\n        self._blocking_poll(timeout=timeout)\n\n        if self._exception is not None:\n            # pylint: disable=raising-bad-type\n            # Pylint doesn't recognize that this is valid in this case.\n            raise self._exception\n\n        return self._result", "output": "Get the result of the operation, blocking if necessary.\n\n        Args:\n            timeout (int):\n                How long (in seconds) to wait for the operation to complete.\n                If None, wait indefinitely.\n\n        Returns:\n            google.protobuf.Message: The Operation's result.\n\n        Raises:\n            google.api_core.GoogleAPICallError: If the operation errors or if\n                the timeout is reached before the operation completes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def refresh_table(self):\r\n        \"\"\"\"\"\"\r\n        if self.is_visible and self.isVisible():\r\n            self.shellwidget.refresh_namespacebrowser()\r\n            try:\r\n                self.editor.resizeRowToContents()\r\n            except TypeError:\r\n                pass", "output": "Refresh variable table", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bfloat16_activations_var_getter(getter, *args, **kwargs):\n  \"\"\"\n  \"\"\"\n  requested_dtype = kwargs[\"dtype\"]\n  if requested_dtype == tf.bfloat16:\n    kwargs[\"dtype\"] = tf.float32\n  var = getter(*args, **kwargs)\n  # This if statement is needed to guard the cast, because batch norm\n  # assigns directly to the return value of this custom getter. The cast\n  # makes the return value not a variable so it cannot be assigned. Batch\n  # norm variables are always in fp32 so this if statement is never\n  # triggered for them.\n  if var.dtype.base_dtype != requested_dtype:\n    var = tf.cast(var, requested_dtype)\n  return var", "output": "A custom getter function for float32 parameters and bfloat16 activations.\n\n  Args:\n    getter: custom getter\n    *args: arguments\n    **kwargs: keyword arguments\n  Returns:\n    variables with the correct dtype.\n  Raises:\n    KeyError: if \"dtype\" is not provided as a kwarg.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_numpy(self, dtype=None, copy=False):\r\n        \"\"\"\r\n        \"\"\"\r\n        return self._default_to_pandas(\"to_numpy\", dtype=dtype, copy=copy)", "output": "Convert the DataFrame to a NumPy array.\r\n\r\n        Args:\r\n            dtype: The dtype to pass to numpy.asarray()\r\n            copy: Whether to ensure that the returned value is a not a view on another\r\n                array.\r\n\r\n        Returns:\r\n            A numpy array.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_request_start_line(line: str) -> RequestStartLine:\n    \"\"\"\n    \"\"\"\n    try:\n        method, path, version = line.split(\" \")\n    except ValueError:\n        # https://tools.ietf.org/html/rfc7230#section-3.1.1\n        # invalid request-line SHOULD respond with a 400 (Bad Request)\n        raise HTTPInputError(\"Malformed HTTP request line\")\n    if not re.match(r\"^HTTP/1\\.[0-9]$\", version):\n        raise HTTPInputError(\n            \"Malformed HTTP version in HTTP Request-Line: %r\" % version\n        )\n    return RequestStartLine(method, path, version)", "output": "Returns a (method, path, version) tuple for an HTTP 1.x request line.\n\n    The response is a `collections.namedtuple`.\n\n    >>> parse_request_start_line(\"GET /foo HTTP/1.1\")\n    RequestStartLine(method='GET', path='/foo', version='HTTP/1.1')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lambda_handler(event, context):\n    '''\n    '''\n    #print(\"Received event: \" + json.dumps(event, indent=2))\n\n    # Get the object from the event\n    bucket = event['Records'][0]['s3']['bucket']['name']\n    key = urllib.unquote_plus(event['Records'][0]['s3']['object']['key'].encode('utf8'))\n    try:\n        # Calls rekognition DetectFaces API to detect faces in S3 object\n        response = detect_faces(bucket, key)\n\n        # Calls rekognition DetectLabels API to detect labels in S3 object\n        #response = detect_labels(bucket, key)\n\n        # Calls rekognition IndexFaces API to detect faces in S3 object and index faces into specified collection\n        #response = index_faces(bucket, key)\n\n        # Print response to console.\n        print(response)\n\n        return response\n    except Exception as e:\n        print(e)\n        print(\"Error processing object {} from bucket {}. \".format(key, bucket) +\n              \"Make sure your object and bucket exist and your bucket is in the same region as this function.\")\n        raise e", "output": "Demonstrates S3 trigger that uses\n    Rekognition APIs to detect faces, labels and index faces in S3 Object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_mine(fun):\n    '''\n    \n    '''\n    if fun in _CACHE and _CACHE[fun]:\n        return _CACHE[fun]\n    net_runner_opts = _get_net_runner_opts()\n    _CACHE[fun] = __salt__['mine.get'](net_runner_opts.get('target'),\n                                       fun,\n                                       tgt_type=net_runner_opts.get('expr_form'))\n    return _CACHE[fun]", "output": "Return the mine function from all the targeted minions.\n    Just a small helper to avoid redundant pieces of code.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flush(self, using=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        return self._get_connection(using).indices.flush(index=self._name, **kwargs)", "output": "Preforms a flush operation on the index.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.flush`` unchanged.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def subvolume_snapshot(source, dest=None, name=None, read_only=False):\n    '''\n    \n\n    '''\n    if not dest and not name:\n        raise CommandExecutionError('Provide parameter dest, name, or both')\n\n    cmd = ['btrfs', 'subvolume', 'snapshot']\n    if read_only:\n        cmd.append('-r')\n    if dest and not name:\n        cmd.append(dest)\n    if dest and name:\n        name = os.path.join(dest, name)\n    if name:\n        cmd.append(name)\n\n    res = __salt__['cmd.run_all'](cmd)\n    salt.utils.fsutils._verify_run(res)\n    return True", "output": "Create a snapshot of a source subvolume\n\n    source\n        Source subvolume from where to create the snapshot\n\n    dest\n        If only dest is given, the subvolume will be named as the\n        basename of the source\n\n    name\n       Name of the snapshot\n\n    read_only\n        Create a read only snapshot\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' btrfs.subvolume_snapshot /var/volumes/tmp dest=/.snapshots\n        salt '*' btrfs.subvolume_snapshot /var/volumes/tmp name=backup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mtf_image_transformer_cifar_4x():\n  \"\"\"\"\"\"\n  hparams = mtf_image_transformer_base_cifar()\n  hparams.mesh_shape = \"batch:32\"\n  hparams.layout = \"batch:batch\"\n  hparams.batch_size = 128\n  return hparams", "output": "Data parallel CIFAR parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enable(name):\n    '''\n    \n    '''\n    cmd = 'systemctl enable systemd-nspawn@{0}'.format(name)\n    if __salt__['cmd.retcode'](cmd, python_shell=False) != 0:\n        __context__['retcode'] = salt.defaults.exitcodes.EX_UNAVAILABLE\n        return False\n    return True", "output": "Set the named container to be launched at boot\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion nspawn.enable <name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def signal_path(cls, project, signal):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/signals/{signal}\", project=project, signal=signal\n        )", "output": "Return a fully-qualified signal string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main():\n    \"\"\"\"\"\"\n    desc = 'Generate character statistics from a source tree'\n    parser = argparse.ArgumentParser(description=desc)\n    parser.add_argument(\n        '--src',\n        dest='src',\n        required=True,\n        help='The root of the source tree'\n    )\n    parser.add_argument(\n        '--out',\n        dest='out',\n        default='chars.py',\n        help='The output filename'\n    )\n\n    args = parser.parse_args()\n\n    stats = generate_statistics(args.src)\n    with open(args.out, 'wb') as out_f:\n        out_f.write('CHARS={0}\\n'.format(stats))", "output": "The main function of the script", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_label_shape(self, label_shape):\n        \"\"\"\"\"\"\n        if not len(label_shape) == 2:\n            raise ValueError('label_shape should have length 2')\n        if label_shape[0] < self.label_shape[0]:\n            msg = 'Attempts to reduce label count from %d to %d, not allowed.' \\\n                % (self.label_shape[0], label_shape[0])\n            raise ValueError(msg)\n        if label_shape[1] != self.provide_label[0][1][2]:\n            msg = 'label_shape object width inconsistent: %d vs %d.' \\\n                % (self.provide_label[0][1][2], label_shape[1])\n            raise ValueError(msg)", "output": "Checks if the new label shape is valid", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def l2_batch_normalize(x, epsilon=1e-12, scope=None):\n  \"\"\"\n  \n  \"\"\"\n  with tf.name_scope(scope, \"l2_batch_normalize\") as name_scope:\n    x_shape = tf.shape(x)\n    x = tf.contrib.layers.flatten(x)\n    x /= (epsilon + reduce_max(tf.abs(x), 1, keepdims=True))\n    square_sum = reduce_sum(tf.square(x), 1, keepdims=True)\n    x_inv_norm = tf.rsqrt(np.sqrt(epsilon) + square_sum)\n    x_norm = tf.multiply(x, x_inv_norm)\n    return tf.reshape(x_norm, x_shape, name_scope)", "output": "Helper function to normalize a batch of vectors.\n  :param x: the input placeholder\n  :param epsilon: stabilizes division\n  :return: the batch of l2 normalized vector", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def GlobalAvgPooling(x, data_format='channels_last'):\n    \"\"\"\n    \n    \"\"\"\n    assert x.shape.ndims == 4\n    data_format = get_data_format(data_format)\n    axis = [1, 2] if data_format == 'channels_last' else [2, 3]\n    return tf.reduce_mean(x, axis, name='output')", "output": "Global average pooling as in the paper `Network In Network\n    <http://arxiv.org/abs/1312.4400>`_.\n\n    Args:\n        x (tf.Tensor): a 4D tensor.\n\n    Returns:\n        tf.Tensor: a NC tensor named ``output``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _load_bboxes(csv_path, csv_positions, prefix):\n  \"\"\"\"\"\"\n  logging.info('Loading CSVs %s from positions %s with prefix %s',\n               csv_path, csv_positions, prefix)\n  boxes = collections.defaultdict(list)\n  with tf.io.gfile.GFile(csv_path) as csv_f:\n    if csv_positions[0] > 0:\n      csv_f.seek(csv_positions[0])\n    else:\n      csv_f.readline()  # Drop headers\n    reader = csv.reader(csv_f)\n    for (image_id, source, label, confidence, xmin, xmax, ymin, ymax,\n         is_occluded, is_truncated, is_group_of, is_depiction, is_inside,\n        ) in reader:\n      if prefix and image_id[0] != prefix:\n        break\n      csv_positions[0] = csv_f.tell()\n      image_id = int(image_id, 16)\n      del confidence  # always 1 in bounding boxes.\n      current_row = _Bbox(\n          label, source, tfds.features.BBox(\n              float(ymin), float(xmin), float(ymax), float(xmax)),\n          int(is_occluded), int(is_truncated),\n          int(is_group_of), int(is_depiction), int(is_inside))\n      boxes[image_id].append(current_row)\n  return dict(boxes)", "output": "Returns bounded boxes listed within given CSV file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_from_class_batches(self, class_batches, num_shards=None):\n    \"\"\"\n    \"\"\"\n    shards_for_submissions = {}\n    shard_idx = 0\n    for idx, (batch_id, batch_val) in enumerate(iteritems(class_batches)):\n      work_id = DEFENSE_WORK_ID_PATTERN.format(idx)\n      submission_id = batch_val['submission_id']\n      shard_id = None\n      if num_shards:\n        shard_id = shards_for_submissions.get(submission_id)\n        if shard_id is None:\n          shard_id = shard_idx % num_shards\n          shards_for_submissions[submission_id] = shard_id\n          shard_idx += 1\n      # Note: defense also might have following fields populated by worker:\n      # stat_correct, stat_error, stat_target_class, stat_num_images\n      self.work[work_id] = {\n          'claimed_worker_id': None,\n          'claimed_worker_start_time': None,\n          'is_completed': False,\n          'error': None,\n          'elapsed_time': None,\n          'submission_id': submission_id,\n          'shard_id': shard_id,\n          'output_classification_batch_id': batch_id,\n      }", "output": "Initializes work pieces from classification batches.\n\n    Args:\n      class_batches: dict with classification batches, could be obtained\n        as ClassificationBatches.data\n      num_shards: number of shards to split data into,\n        if None then no sharding is done.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def refresh_db(jail=None, chroot=None, root=None, force=False, **kwargs):\n    '''\n    \n    '''\n    # Remove rtag file to keep multiple refreshes from happening in pkg states\n    salt.utils.pkg.clear_rtag(__opts__)\n    cmd = _pkg(jail, chroot, root)\n    cmd.append('update')\n    if force:\n        cmd.append('-f')\n    return __salt__['cmd.retcode'](cmd, python_shell=False) == 0", "output": "Refresh PACKAGESITE contents\n\n    .. note::\n\n        This function can accessed using ``pkg.update`` in addition to\n        ``pkg.refresh_db``, to more closely match the CLI usage of ``pkg(8)``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.refresh_db\n\n    jail\n        Refresh the pkg database within the specified jail\n\n    chroot\n        Refresh the pkg database within the specified chroot (ignored if\n        ``jail`` is specified)\n\n    root\n        Refresh the pkg database within the specified root (ignored if\n        ``jail`` is specified)\n\n    force\n        Force a full download of the repository catalog without regard to the\n        respective ages of the local and remote copies of the catalog.\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.refresh_db force=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def asscipy(self):\n        \"\"\"\n        \"\"\"\n        data = self.data.asnumpy()\n        indices = self.indices.asnumpy()\n        indptr = self.indptr.asnumpy()\n        if not spsp:\n            raise ImportError(\"scipy is not available. \\\n                               Please check if the scipy python bindings are installed.\")\n        return spsp.csr_matrix((data, indices, indptr), shape=self.shape, dtype=self.dtype)", "output": "Returns a ``scipy.sparse.csr.csr_matrix`` object with value copied from this array\n\n        Examples\n        --------\n        >>> x = mx.nd.sparse.zeros('csr', (2,3))\n        >>> y = x.asscipy()\n        >>> type(y)\n        <type 'scipy.sparse.csr.csr_matrix'>\n        >>> y\n        <2x3 sparse matrix of type '<type 'numpy.float32'>'\n        with 0 stored elements in Compressed Sparse Row format>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_show_toolbars_action(self):\r\n        \"\"\"\"\"\"\r\n        if self.toolbars_visible:\r\n            text = _(\"Hide toolbars\")\r\n            tip = _(\"Hide toolbars\")\r\n        else:\r\n            text = _(\"Show toolbars\")\r\n            tip = _(\"Show toolbars\")\r\n        self.show_toolbars_action.setText(text)\r\n        self.show_toolbars_action.setToolTip(tip)", "output": "Update the text displayed in the menu entry.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def layer_description_extractor(layer, node_to_id):\n    '''\n    '''\n\n    layer_input = layer.input\n    layer_output = layer.output\n    if layer_input is not None:\n        if isinstance(layer_input, Iterable):\n            layer_input = list(map(lambda x: node_to_id[x], layer_input))\n        else:\n            layer_input = node_to_id[layer_input]\n\n    if layer_output is not None:\n        layer_output = node_to_id[layer_output]\n\n    if isinstance(layer, StubConv):\n        return (\n            type(layer).__name__,\n            layer_input,\n            layer_output,\n            layer.input_channel,\n            layer.filters,\n            layer.kernel_size,\n            layer.stride,\n            layer.padding,\n        )\n    elif isinstance(layer, (StubDense,)):\n        return [\n            type(layer).__name__,\n            layer_input,\n            layer_output,\n            layer.input_units,\n            layer.units,\n        ]\n    elif isinstance(layer, (StubBatchNormalization,)):\n        return (type(layer).__name__, layer_input, layer_output, layer.num_features)\n    elif isinstance(layer, (StubDropout,)):\n        return (type(layer).__name__, layer_input, layer_output, layer.rate)\n    elif isinstance(layer, StubPooling):\n        return (\n            type(layer).__name__,\n            layer_input,\n            layer_output,\n            layer.kernel_size,\n            layer.stride,\n            layer.padding,\n        )\n    else:\n        return (type(layer).__name__, layer_input, layer_output)", "output": "get layer description.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resize_imgs(fnames, targ, path, new_path, resume=True, fn=None):\n    \"\"\"\n     \n    \"\"\"\n    target_path = path_for(path, new_path, targ)\n    if resume:\n        subdirs = {os.path.dirname(p) for p in fnames}\n        subdirs = {s for s in subdirs if os.path.exists(os.path.join(target_path, s))}\n        already_resized_fnames = set()\n        for subdir in subdirs:\n            files = [os.path.join(subdir, file) for file in os.listdir(os.path.join(target_path, subdir))]\n            already_resized_fnames.update(set(files))\n        original_fnames = set(fnames)\n        fnames = list(original_fnames - already_resized_fnames)\n    \n    errors = {}\n    def safely_process(fname):\n        try:\n            resize_img(fname, targ, path, new_path, fn=fn)\n        except Exception as ex:\n            errors[fname] = str(ex)\n\n    if len(fnames) > 0:\n        with ThreadPoolExecutor(num_cpus()) as e:\n            ims = e.map(lambda fname: safely_process(fname), fnames)\n            for _ in tqdm(ims, total=len(fnames), leave=False): pass\n    if errors:\n        print('Some images failed to process:')\n        print(json.dumps(errors, indent=2))\n    return os.path.join(path,new_path,str(targ))", "output": "Enlarge or shrink a set of images in the same directory to scale, such that the smaller of the height or width dimension is equal to targ.\n    Note: \n    -- This function is multithreaded for efficiency. \n    -- When destination file or folder already exist, function exists without raising an error.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def launch_configuration_exists(name, region=None, key=None, keyid=None,\n                                profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    retries = 30\n    while True:\n        try:\n            lc = conn.get_all_launch_configurations(names=[name])\n            if lc:\n                return True\n            else:\n                msg = 'The launch configuration does not exist in region {0}'.format(region)\n                log.debug(msg)\n                return False\n        except boto.exception.BotoServerError as e:\n            if retries and e.code == 'Throttling':\n                log.debug('Throttled by AWS API, retrying in 5 seconds...')\n                time.sleep(5)\n                retries -= 1\n                continue\n            log.error(e)\n            return False", "output": "Check for a launch configuration's existence.\n\n    CLI example::\n\n        salt myminion boto_asg.launch_configuration_exists mylc", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def weight_decay_and_noise(loss, hparams, learning_rate, var_list=None):\n  \"\"\"\"\"\"\n  if var_list is None:\n    var_list = tf.trainable_variables()\n\n  decay_vars = [v for v in var_list]\n  noise_vars = [v for v in var_list if \"/body/\" in v.name]\n\n  weight_decay_loss = weight_decay(hparams.weight_decay, decay_vars)\n  if hparams.weight_decay and common_layers.should_generate_summaries():\n    tf.summary.scalar(\"losses/weight_decay\", weight_decay_loss)\n  weight_noise_ops = weight_noise(hparams.weight_noise, learning_rate,\n                                  noise_vars)\n\n  with tf.control_dependencies(weight_noise_ops):\n    loss = tf.identity(loss)\n\n  loss += weight_decay_loss\n  return loss", "output": "Apply weight decay and weight noise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render(genshi_data, saltenv='base', sls='', method='xml', **kws):\n    '''\n    \n    '''\n    if not HAS_LIBS:\n        return {}\n\n    if not isinstance(genshi_data, six.string_types):\n        genshi_data = genshi_data.read()\n\n    if genshi_data.startswith('#!'):\n        genshi_data = genshi_data[(genshi_data.find('\\n') + 1):]\n    if not genshi_data.strip():\n        return {}\n\n    if method == 'text' or method == 'newtext':\n        tmpl = NewTextTemplate(genshi_data)\n    elif method == 'oldtext':\n        tmpl = OldTextTemplate(genshi_data)\n    else:\n        tmpl = MarkupTemplate(genshi_data)\n\n    return tmpl.generate(**kws).render(method)", "output": "Render a Genshi template. A method should be passed in as part of the\n    kwargs. If no method is passed in, xml is assumed. Valid methods are:\n\n    .. code-block:\n\n        - xml\n        - xhtml\n        - html\n        - text\n        - newtext\n        - oldtext\n\n    Note that the ``text`` method will call ``NewTextTemplate``. If ``oldtext``\n    is desired, it must be called explicitly\n\n    :rtype: A Python data structure", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ndims(self):\n        \"\"\"\"\"\"\n        if self._dims is None:\n            return None\n        else:\n            if self._ndims is None:\n                self._ndims = len(self._dims)\n            return self._ndims", "output": "Returns the rank of this shape, or None if it is unspecified.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathNextSelf(self, cur):\n        \"\"\" \"\"\"\n        if cur is None: cur__o = None\n        else: cur__o = cur._o\n        ret = libxml2mod.xmlXPathNextSelf(self._o, cur__o)\n        if ret is None:raise xpathError('xmlXPathNextSelf() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Traversal function for the \"self\" direction The self axis\n           contains just the context node itself", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def query_job(request):\n    \"\"\"\n    \"\"\"\n    job_id = request.GET.get(\"job_id\")\n    jobs = JobRecord.objects.filter(job_id=job_id)\n    trials = TrialRecord.objects.filter(job_id=job_id)\n\n    total_num = len(trials)\n    running_num = sum(t.trial_status == Trial.RUNNING for t in trials)\n    success_num = sum(t.trial_status == Trial.TERMINATED for t in trials)\n    failed_num = sum(t.trial_status == Trial.ERROR for t in trials)\n    if total_num == 0:\n        progress = 0\n    else:\n        progress = int(float(success_num) / total_num * 100)\n\n    if len(jobs) == 0:\n        resp = \"Unkonwn job id %s.\\n\" % job_id\n    else:\n        job = jobs[0]\n        result = {\n            \"job_id\": job.job_id,\n            \"name\": job.name,\n            \"user\": job.user,\n            \"type\": job.type,\n            \"start_time\": job.start_time,\n            \"end_time\": job.end_time,\n            \"success_trials\": success_num,\n            \"failed_trials\": failed_num,\n            \"running_trials\": running_num,\n            \"total_trials\": total_num,\n            \"best_trial_id\": job.best_trial_id,\n            \"progress\": progress\n        }\n        resp = json.dumps(result)\n    return HttpResponse(resp, content_type=\"application/json;charset=utf-8\")", "output": "Rest API to query the job info, with the given job_id.\n\n    The url pattern should be like this:\n\n    curl http://<server>:<port>/query_job?job_id=<job_id>\n\n    The response may be:\n\n    {\n        \"running_trials\": 0,\n        \"start_time\": \"2018-07-19 20:49:40\",\n        \"current_round\": 1,\n        \"failed_trials\": 0,\n        \"best_trial_id\": \"2067R2ZD\",\n        \"name\": \"asynchyperband_test\",\n        \"job_id\": \"asynchyperband_test\",\n        \"user\": \"Grady\",\n        \"type\": \"RAY TUNE\",\n        \"total_trials\": 4,\n        \"end_time\": \"2018-07-19 20:50:10\",\n        \"progress\": 100,\n        \"success_trials\": 4\n    }", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shorten_text(self, text):\n        \"\"\"\"\"\"\n        if len(text) > self.width:\n            return text[:self.width - 3] + '...'\n        return text", "output": "Shortens text to fit into the :attr:`width`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cut(sentence, HMM=True):\n    \"\"\"\n    \n    \"\"\"\n    global dt\n    if jieba.pool is None:\n        for w in dt.cut(sentence, HMM=HMM):\n            yield w\n    else:\n        parts = strdecode(sentence).splitlines(True)\n        if HMM:\n            result = jieba.pool.map(_lcut_internal, parts)\n        else:\n            result = jieba.pool.map(_lcut_internal_no_hmm, parts)\n        for r in result:\n            for w in r:\n                yield w", "output": "Global `cut` function that supports parallel processing.\n\n    Note that this only works using dt, custom POSTokenizer\n    instances are not supported.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decoded_output_boxes(self):\n        \"\"\"  \"\"\"\n        anchors = tf.tile(tf.expand_dims(self.proposals.boxes, 1),\n                          [1, cfg.DATA.NUM_CLASS, 1])   # N x #class x 4\n        decoded_boxes = decode_bbox_target(\n            self.box_logits / self.bbox_regression_weights,\n            anchors\n        )\n        return decoded_boxes", "output": "Returns: N x #class x 4", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_value(self):\r\n        \"\"\"\"\"\"\r\n        # It is import to avoid accessing Qt C++ object as it has probably\r\n        # already been destroyed, due to the Qt.WA_DeleteOnClose attribute\r\n        df = self.dataModel.get_data()\r\n        if self.is_series:\r\n            return df.iloc[:, 0]\r\n        else:\r\n            return df", "output": "Return modified Dataframe -- this is *not* a copy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def subscribe_strategy(\n            self,\n            strategy_id: str,\n            last: int,\n            today=datetime.date.today(),\n            cost_coins=10\n    ):\n        \"\"\"\n        \"\"\"\n\n        if self.coins > cost_coins:\n            order_id = str(uuid.uuid1())\n            self._subscribed_strategy[strategy_id] = {\n                'lasttime':\n                last,\n                'start':\n                str(today),\n                'strategy_id':\n                strategy_id,\n                'end':\n                QA_util_get_next_day(\n                    QA_util_get_real_date(str(today),\n                                          towards=1),\n                    last\n                ),\n                'status':\n                'running',\n                'uuid':\n                order_id\n            }\n            self.coins -= cost_coins\n            self.coins_history.append(\n                [\n                    cost_coins,\n                    strategy_id,\n                    str(today),\n                    last,\n                    order_id,\n                    'subscribe'\n                ]\n            )\n            return True, order_id\n        else:\n            # return QAERROR.\n            return False, 'Not Enough Coins'", "output": "\u8ba2\u9605\u4e00\u4e2a\u7b56\u7565\n\n        \u4f1a\u6263\u51cf\u4f60\u7684\u79ef\u5206\n\n        Arguments:\n            strategy_id {str} -- [description]\n            last {int} -- [description]\n\n        Keyword Arguments:\n            today {[type]} -- [description] (default: {datetime.date.today()})\n            cost_coins {int} -- [description] (default: {10})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_open_spaces(board):\n  \"\"\"\"\"\"\n  open_spaces = []\n  for i in range(3):\n    for j in range(3):\n      if board[i][j] == 0:\n        open_spaces.append(encode_pos(i, j))\n  return open_spaces", "output": "Given a representation of the board, returns a list of open spaces.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def edit(self, *, reason=None, **options):\n        \"\"\"\n        \"\"\"\n\n        try:\n            position = options.pop('position')\n        except KeyError:\n            pass\n        else:\n            await self._move(position, reason=reason)\n            self.position = position\n\n        if options:\n            data = await self._state.http.edit_channel(self.id, reason=reason, **options)\n            self._update(self.guild, data)", "output": "|coro|\n\n        Edits the channel.\n\n        You must have the :attr:`~Permissions.manage_channels` permission to\n        use this.\n\n        Parameters\n        ----------\n        name: :class:`str`\n            The new category's name.\n        position: :class:`int`\n            The new category's position.\n        nsfw: :class:`bool`\n            To mark the category as NSFW or not.\n        reason: Optional[:class:`str`]\n            The reason for editing this category. Shows up on the audit log.\n\n        Raises\n        ------\n        InvalidArgument\n            If position is less than 0 or greater than the number of categories.\n        Forbidden\n            You do not have permissions to edit the category.\n        HTTPException\n            Editing the category failed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __inner_eval(self, data_name, data_idx, feval=None):\n        \"\"\"\"\"\"\n        if data_idx >= self.__num_dataset:\n            raise ValueError(\"Data_idx should be smaller than number of dataset\")\n        self.__get_eval_info()\n        ret = []\n        if self.__num_inner_eval > 0:\n            result = np.zeros(self.__num_inner_eval, dtype=np.float64)\n            tmp_out_len = ctypes.c_int(0)\n            _safe_call(_LIB.LGBM_BoosterGetEval(\n                self.handle,\n                ctypes.c_int(data_idx),\n                ctypes.byref(tmp_out_len),\n                result.ctypes.data_as(ctypes.POINTER(ctypes.c_double))))\n            if tmp_out_len.value != self.__num_inner_eval:\n                raise ValueError(\"Wrong length of eval results\")\n            for i in range_(self.__num_inner_eval):\n                ret.append((data_name, self.__name_inner_eval[i],\n                            result[i], self.__higher_better_inner_eval[i]))\n        if feval is not None:\n            if data_idx == 0:\n                cur_data = self.train_set\n            else:\n                cur_data = self.valid_sets[data_idx - 1]\n            feval_ret = feval(self.__inner_predict(data_idx), cur_data)\n            if isinstance(feval_ret, list):\n                for eval_name, val, is_higher_better in feval_ret:\n                    ret.append((data_name, eval_name, val, is_higher_better))\n            else:\n                eval_name, val, is_higher_better = feval_ret\n                ret.append((data_name, eval_name, val, is_higher_better))\n        return ret", "output": "Evaluate training or validation data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def readAudioFile(path):\n    '''\n    \n    '''\n    extension = os.path.splitext(path)[1]\n\n    try:\n        #if extension.lower() == '.wav':\n            #[Fs, x] = wavfile.read(path)\n        if extension.lower() == '.aif' or extension.lower() == '.aiff':\n            s = aifc.open(path, 'r')\n            nframes = s.getnframes()\n            strsig = s.readframes(nframes)\n            x = numpy.fromstring(strsig, numpy.short).byteswap()\n            Fs = s.getframerate()\n        elif extension.lower() == '.mp3' or extension.lower() == '.wav' or extension.lower() == '.au' or extension.lower() == '.ogg':            \n            try:\n                audiofile = AudioSegment.from_file(path)\n            #except pydub.exceptions.CouldntDecodeError:\n            except:\n                print(\"Error: file not found or other I/O error. \"\n                      \"(DECODING FAILED)\")\n                return (-1,-1)                \n\n            if audiofile.sample_width==2:                \n                data = numpy.fromstring(audiofile._data, numpy.int16)\n            elif audiofile.sample_width==4:\n                data = numpy.fromstring(audiofile._data, numpy.int32)\n            else:\n                return (-1, -1)\n            Fs = audiofile.frame_rate\n            x = []\n            for chn in list(range(audiofile.channels)):\n                x.append(data[chn::audiofile.channels])\n            x = numpy.array(x).T\n        else:\n            print(\"Error in readAudioFile(): Unknown file type!\")\n            return (-1,-1)\n    except IOError: \n        print(\"Error: file not found or other I/O error.\")\n        return (-1,-1)\n\n    if x.ndim==2:\n        if x.shape[1]==1:\n            x = x.flatten()\n\n    return (Fs, x)", "output": "This function returns a numpy array that stores the audio samples of a specified WAV of AIFF file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def valid_padding(in_size, filter_size, stride_size):\n    \"\"\"\n    \"\"\"\n    in_height, in_width = in_size\n    filter_height, filter_width = filter_size\n    stride_height, stride_width = stride_size\n\n    out_height = np.ceil(float(in_height) / float(stride_height))\n    out_width = np.ceil(float(in_width) / float(stride_width))\n\n    pad_along_height = int(\n        ((out_height - 1) * stride_height + filter_height - in_height))\n    pad_along_width = int(\n        ((out_width - 1) * stride_width + filter_width - in_width))\n    pad_top = pad_along_height // 2\n    pad_bottom = pad_along_height - pad_top\n    pad_left = pad_along_width // 2\n    pad_right = pad_along_width - pad_left\n    padding = (pad_left, pad_right, pad_top, pad_bottom)\n    output = (out_height, out_width)\n    return padding, output", "output": "Note: Padding is added to match TF conv2d `same` padding. See\n    www.tensorflow.org/versions/r0.12/api_docs/python/nn/convolution\n\n    Params:\n        in_size (tuple): Rows (Height), Column (Width) for input\n        stride_size (tuple): Rows (Height), Column (Width) for stride\n        filter_size (tuple): Rows (Height), Column (Width) for filter\n\n    Output:\n        padding (tuple): For input into torch.nn.ZeroPad2d\n        output (tuple): Output shape after padding and convolution", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_SU_save_stock_info(engine, client=DATABASE):\n    \"\"\"\n    \"\"\"\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_stock_info(client=client)", "output": "save stock info\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def interrupt_kernel(self):\r\n        \"\"\"\"\"\"\r\n        client = self.get_current_client()\r\n        if client is not None:\r\n            self.switch_to_plugin()\r\n            client.stop_button_click_handler()", "output": "Interrupt kernel of current client.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_dataframe(self, read_session, dtypes=None):\n        \"\"\"\n        \"\"\"\n        if fastavro is None:\n            raise ImportError(_FASTAVRO_REQUIRED)\n        if pandas is None:\n            raise ImportError(_PANDAS_REQUIRED)\n\n        return self.rows(read_session).to_dataframe(dtypes=dtypes)", "output": "Create a :class:`pandas.DataFrame` of all rows in the stream.\n\n        This method requires the pandas libary to create a data frame and the\n        fastavro library to parse row blocks.\n\n        .. warning::\n            DATETIME columns are not supported. They are currently parsed as\n            strings in the fastavro library.\n\n        Args:\n            read_session ( \\\n                ~google.cloud.bigquery_storage_v1beta1.types.ReadSession \\\n            ):\n                The read session associated with this read rows stream. This\n                contains the schema, which is required to parse the data\n                blocks.\n            dtypes ( \\\n                Map[str, Union[str, pandas.Series.dtype]] \\\n            ):\n                Optional. A dictionary of column names pandas ``dtype``s. The\n                provided ``dtype`` is used when constructing the series for\n                the column specified. Otherwise, the default pandas behavior\n                is used.\n\n        Returns:\n            pandas.DataFrame:\n                A data frame of all rows in the stream.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_cursor_on_first_line(self):\r\n        \"\"\"\"\"\"\r\n        cursor = self.textCursor()\r\n        cursor.movePosition(QTextCursor.StartOfBlock)\r\n        return cursor.atStart()", "output": "Return True if cursor is on the first line", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _hide_loading_page(self):\r\n        \"\"\"\"\"\"\r\n        self.infowidget.hide()\r\n        self.shellwidget.show()\r\n        self.info_page = self.blank_page\r\n        self.set_info_page()\r\n        self.shellwidget.sig_prompt_ready.disconnect(self._hide_loading_page)", "output": "Hide animation shown while the kernel is loading.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _map_promise(map_fn, all_inputs):\n  \"\"\"\"\"\"\n  all_promises = utils.map_nested(map_fn, all_inputs)  # Apply the function\n  res = utils.map_nested(_wait_on_promise, all_promises)\n  return res", "output": "Map the function into each element and resolve the promise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def freeze(obj):\n    '''\n    \n    '''\n    if isinstance(obj, dict):\n        return ImmutableDict(obj)\n    if isinstance(obj, list):\n        return ImmutableList(obj)\n    if isinstance(obj, set):\n        return ImmutableSet(obj)\n    return obj", "output": "Freeze python types by turning them into immutable structures.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_distribution_names(self):\n        \"\"\"\n        \n        \"\"\"\n        result = set()\n        page = self.get_page(self.base_url)\n        if not page:\n            raise DistlibException('Unable to get %s' % self.base_url)\n        for match in self._distname_re.finditer(page.data):\n            result.add(match.group(1))\n        return result", "output": "Return all the distribution names known to this locator.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def subvolume_show(path):\n    '''\n    \n\n    '''\n    cmd = ['btrfs', 'subvolume', 'show', path]\n\n    res = __salt__['cmd.run_all'](cmd)\n    salt.utils.fsutils._verify_run(res)\n\n    result = {}\n    table = {}\n    # The real name is the first line, later there is a table of\n    # values separated with colon.\n    stdout = res['stdout'].splitlines()\n    key = stdout.pop(0)\n    result[key.strip()] = table\n\n    for line in stdout:\n        key, value = line.split(':', 1)\n        table[key.lower().strip()] = value.strip()\n    return result", "output": "Show information of a given subvolume\n\n    path\n        Mount point for the filesystem\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' btrfs.subvolume_show /var/volumes/tmp", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def group_path(cls, project, group):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/groups/{group}\", project=project, group=group\n        )", "output": "Return a fully-qualified group string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def round(self, decimals=0, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        nv.validate_round(args, kwargs)\n        result = com.values_from_object(self).round(decimals)\n        result = self._constructor(result, index=self.index).__finalize__(self)\n\n        return result", "output": "Round each value in a Series to the given number of decimals.\n\n        Parameters\n        ----------\n        decimals : int\n            Number of decimal places to round to (default: 0).\n            If decimals is negative, it specifies the number of\n            positions to the left of the decimal point.\n\n        Returns\n        -------\n        Series\n            Rounded values of the Series.\n\n        See Also\n        --------\n        numpy.around : Round values of an np.array.\n        DataFrame.round : Round values of a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series([0.1, 1.3, 2.7])\n        >>> s.round()\n        0    0.0\n        1    1.0\n        2    3.0\n        dtype: float64", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_record(zone_id, record_id, profile):\n    '''\n    \n    '''\n    conn = _get_driver(profile=profile)\n    return _simple_record(conn.get_record(zone_id, record_id))", "output": "Get record information for the given zone_id on the given profile\n\n    :param zone_id: Zone to export.\n    :type  zone_id: ``str``\n\n    :param record_id: Record to delete.\n    :type  record_id: ``str``\n\n    :param profile: The profile key\n    :type  profile: ``str``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion libcloud_dns.get_record google.com www profile1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def local_within_block_attention(x,\n                                 self_attention_bias,\n                                 hparams,\n                                 attention_type=\"local_within_block_mask_right\",\n                                 q_padding=\"VALID\",\n                                 kv_padding=\"VALID\"):\n  \"\"\"\"\"\"\n  x_new, x_shape, is_4d = maybe_reshape_4d_to_3d(x)\n  with tf.variable_scope(\"local_within_block\"):\n    y = common_attention.multihead_attention(\n        common_layers.layer_preprocess(x_new, hparams),\n        None,\n        self_attention_bias,\n        hparams.attention_key_channels or hparams.hidden_size,\n        hparams.attention_value_channels or hparams.hidden_size,\n        hparams.hidden_size,\n        hparams.num_heads,\n        hparams.attention_dropout,\n        attention_type=attention_type,\n        block_width=hparams.block_width,\n        block_length=hparams.block_length,\n        q_padding=q_padding,\n        kv_padding=kv_padding,\n        q_filter_width=hparams.q_filter_width,\n        kv_filter_width=hparams.kv_filter_width,\n        name=\"local_within_block\")\n    if is_4d:\n      y = tf.reshape(y, x_shape)\n    return y", "output": "Local within block self attention.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cut(self, sentence, cut_all=False, HMM=True):\n        '''\n        \n        '''\n        sentence = strdecode(sentence)\n\n        if cut_all:\n            re_han = re_han_cut_all\n            re_skip = re_skip_cut_all\n        else:\n            re_han = re_han_default\n            re_skip = re_skip_default\n        if cut_all:\n            cut_block = self.__cut_all\n        elif HMM:\n            cut_block = self.__cut_DAG\n        else:\n            cut_block = self.__cut_DAG_NO_HMM\n        blocks = re_han.split(sentence)\n        for blk in blocks:\n            if not blk:\n                continue\n            if re_han.match(blk):\n                for word in cut_block(blk):\n                    yield word\n            else:\n                tmp = re_skip.split(blk)\n                for x in tmp:\n                    if re_skip.match(x):\n                        yield x\n                    elif not cut_all:\n                        for xx in x:\n                            yield xx\n                    else:\n                        yield x", "output": "The main function that segments an entire sentence that contains\n        Chinese characters into separated words.\n\n        Parameter:\n            - sentence: The str(unicode) to be segmented.\n            - cut_all: Model type. True for full pattern, False for accurate pattern.\n            - HMM: Whether to use the Hidden Markov Model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def inverse_exp_decay(max_step, min_value=0.01, step=None):\n  \"\"\"\"\"\"\n  inv_base = tf.exp(tf.log(min_value) / float(max_step))\n  if step is None:\n    step = tf.train.get_global_step()\n  if step is None:\n    return 1.0\n  step = to_float(step)\n  return inv_base**tf.maximum(float(max_step) - step, 0.0)", "output": "Inverse-decay exponentially from 0.01 to 1.0 reached at max_step.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_legacy_urlsafe(cls, urlsafe):\n        \"\"\"\n        \"\"\"\n        urlsafe = _to_bytes(urlsafe, encoding=\"ascii\")\n        padding = b\"=\" * (-len(urlsafe) % 4)\n        urlsafe += padding\n        raw_bytes = base64.urlsafe_b64decode(urlsafe)\n\n        reference = _app_engine_key_pb2.Reference()\n        reference.ParseFromString(raw_bytes)\n\n        project = _clean_app(reference.app)\n        namespace = _get_empty(reference.name_space, u\"\")\n        _check_database_id(reference.database_id)\n        flat_path = _get_flat_path(reference.path)\n        return cls(*flat_path, project=project, namespace=namespace)", "output": "Convert urlsafe string to :class:`~google.cloud.datastore.key.Key`.\n\n        This is intended to work with the \"legacy\" representation of a\n        datastore \"Key\" used within Google App Engine (a so-called\n        \"Reference\"). This assumes that ``urlsafe`` was created within an App\n        Engine app via something like ``ndb.Key(...).urlsafe()``.\n\n        :type urlsafe: bytes or unicode\n        :param urlsafe: The base64 encoded (ASCII) string corresponding to a\n                        datastore \"Key\" / \"Reference\".\n\n        :rtype: :class:`~google.cloud.datastore.key.Key`.\n        :returns: The key corresponding to ``urlsafe``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def item_length(self):\n        \"\"\"\n        \n        \"\"\"\n        if (self.dtype not in [list, dict, array.array]):\n            raise TypeError(\"item_length() is only applicable for SArray of type list, dict and array.\")\n\n\n        with cython_context():\n            return SArray(_proxy = self.__proxy__.item_length())", "output": "Length of each element in the current SArray.\n\n        Only works on SArrays of dict, array, or list type. If a given element\n        is a missing value, then the output elements is also a missing value.\n        This function is equivalent to the following but more performant:\n\n            sa_item_len =  sa.apply(lambda x: len(x) if x is not None else None)\n\n        Returns\n        -------\n        out_sf : SArray\n            A new SArray, each element in the SArray is the len of the corresponding\n            items in original SArray.\n\n        Examples\n        --------\n        >>> sa = SArray([\n        ...  {\"is_restaurant\": 1, \"is_electronics\": 0},\n        ...  {\"is_restaurant\": 1, \"is_retail\": 1, \"is_electronics\": 0},\n        ...  {\"is_restaurant\": 0, \"is_retail\": 1, \"is_electronics\": 0},\n        ...  {\"is_restaurant\": 0},\n        ...  {\"is_restaurant\": 1, \"is_electronics\": 1},\n        ...  None])\n        >>> sa.item_length()\n        dtype: int\n        Rows: 6\n        [2, 3, 3, 1, 2, None]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mutate_label(label):\n        \"\"\"\n        \n        \"\"\"\n        label_hashed = '_' + hashlib.md5(label.encode('utf-8')).hexdigest()\n\n        # if label starts with number, add underscore as first character\n        label_mutated = '_' + label if re.match(r'^\\d', label) else label\n\n        # replace non-alphanumeric characters with underscores\n        label_mutated = re.sub(r'[^\\w]+', '_', label_mutated)\n        if label_mutated != label:\n            # add md5 hash to label to avoid possible collisions\n            label_mutated += label_hashed\n\n        return label_mutated", "output": "BigQuery field_name should start with a letter or underscore and contain only\n        alphanumeric characters. Labels that start with a number are prefixed with an\n        underscore. Any unsupported characters are replaced with underscores and an\n        md5 hash is added to the end of the label to avoid possible collisions.\n        :param str label: the original label which might include unsupported characters\n        :return: String that is supported by the database", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _resolve_image(ret, image, client_timeout):\n    '''\n    \n    '''\n    image_id = __salt__['docker.resolve_image_id'](image)\n\n    if image_id is False:\n        if not __opts__['test']:\n            # Image not pulled locally, so try pulling it\n            try:\n                pull_result = __salt__['docker.pull'](\n                    image,\n                    client_timeout=client_timeout,\n                )\n            except Exception as exc:\n                raise CommandExecutionError(\n                    'Failed to pull {0}: {1}'.format(image, exc)\n                )\n            else:\n                ret['changes']['image'] = pull_result\n                # Try resolving again now that we've pulled\n                image_id = __salt__['docker.resolve_image_id'](image)\n                if image_id is False:\n                    # Shouldn't happen unless the pull failed\n                    raise CommandExecutionError(\n                        'Image \\'{0}\\' not present despite a docker pull '\n                        'raising no errors'.format(image)\n                    )\n    return image_id", "output": "Resolve the image ID and pull the image if necessary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def has_value(key):\n    '''\n    \n    '''\n    return True \\\n        if salt.utils.data.traverse_dict_and_list(__grains__, key, False) \\\n        else False", "output": "Determine whether a named value exists in the grains dictionary.\n\n    Given a grains dictionary that contains the following structure::\n\n        {'pkg': {'apache': 'httpd'}}\n\n    One would determine if the apache key in the pkg dict exists by::\n\n        pkg:apache\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' grains.has_value pkg:apache", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render(msgpack_data, saltenv='base', sls='', **kws):\n    '''\n    \n    '''\n    if not isinstance(msgpack_data, six.string_types):\n        msgpack_data = msgpack_data.read()\n\n    if msgpack_data.startswith('#!'):\n        msgpack_data = msgpack_data[(msgpack_data.find('\\n') + 1):]\n    if not msgpack_data.strip():\n        return {}\n    return salt.utils.msgpack.loads(msgpack_data)", "output": "Accepts a message pack string or a file object, renders said data back to\n    a python dict.\n\n    .. note:\n        This renderer is NOT intended for use in creating sls files by hand,\n        but exists to allow for data backends to serialize the highdata\n        structure in an easily transportable way. This is to allow for more\n        fluid fileserver backends that rely on pure data sources.\n\n    :rtype: A Python data structure", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _purge_jobs(timestamp):\n    '''\n    \n    '''\n    with _get_serv() as cur:\n        try:\n            sql = 'delete from `jids` where jid in (select distinct jid from salt_returns where alter_time < %s)'\n            cur.execute(sql, (timestamp,))\n            cur.execute('COMMIT')\n        except MySQLdb.Error as e:\n            log.error('mysql returner archiver was unable to delete contents of table \\'jids\\'')\n            log.error(six.text_type(e))\n            raise salt.exceptions.SaltRunnerError(six.text_type(e))\n\n        try:\n            sql = 'delete from `salt_returns` where alter_time < %s'\n            cur.execute(sql, (timestamp,))\n            cur.execute('COMMIT')\n        except MySQLdb.Error as e:\n            log.error('mysql returner archiver was unable to delete contents of table \\'salt_returns\\'')\n            log.error(six.text_type(e))\n            raise salt.exceptions.SaltRunnerError(six.text_type(e))\n\n        try:\n            sql = 'delete from `salt_events` where alter_time < %s'\n            cur.execute(sql, (timestamp,))\n            cur.execute('COMMIT')\n        except MySQLdb.Error as e:\n            log.error('mysql returner archiver was unable to delete contents of table \\'salt_events\\'')\n            log.error(six.text_type(e))\n            raise salt.exceptions.SaltRunnerError(six.text_type(e))\n\n    return True", "output": "Purge records from the returner tables.\n    :param job_age_in_seconds:  Purge jobs older than this\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_projects(self, max_results=None, page_token=None, retry=DEFAULT_RETRY):\n        \"\"\"\n        \"\"\"\n        return page_iterator.HTTPIterator(\n            client=self,\n            api_request=functools.partial(self._call_api, retry),\n            path=\"/projects\",\n            item_to_value=_item_to_project,\n            items_key=\"projects\",\n            page_token=page_token,\n            max_results=max_results,\n        )", "output": "List projects for the project associated with this client.\n\n        See\n        https://cloud.google.com/bigquery/docs/reference/rest/v2/projects/list\n\n        :type max_results: int\n        :param max_results: (Optional) maximum number of projects to return,\n                            If not passed, defaults to a value set by the API.\n\n        :type page_token: str\n        :param page_token:\n            (Optional) Token representing a cursor into the projects. If\n            not passed, the API will return the first page of projects.\n            The token marks the beginning of the iterator to be returned\n            and the value of the ``page_token`` can be accessed at\n            ``next_page_token`` of the\n            :class:`~google.api_core.page_iterator.HTTPIterator`.\n\n        :type retry: :class:`google.api_core.retry.Retry`\n        :param retry: (Optional) How to retry the RPC.\n\n        :rtype: :class:`~google.api_core.page_iterator.Iterator`\n        :returns: Iterator of :class:`~google.cloud.bigquery.client.Project`\n                  accessible to the current client.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_path (self, path):\n        \"\"\" \n        \"\"\"\n        assert isinstance(path, basestring)\n        self.path_ = os.path.normpath(path)", "output": "Sets the path. When generating target name, it will override any path\n            computation from properties.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_grist (features, new_grist):\n    \"\"\" \n    \"\"\"\n    assert is_iterable_typed(features, basestring) or isinstance(features, basestring)\n    assert isinstance(new_grist, basestring)\n    # this function is used a lot in the build phase and the original implementation\n    # was extremely slow; thus some of the weird-looking optimizations for this function.\n    single_item = False\n    if isinstance(features, str):\n        features = [features]\n        single_item = True\n\n    result = []\n    for feature in features:\n        # '<feature>value' -> ('<feature', '>', 'value')\n        # 'something' -> ('something', '', '')\n        # '<toolset>msvc/<feature>value' -> ('<toolset', '>', 'msvc/<feature>value')\n        grist, split, value = feature.partition('>')\n        # if a partition didn't occur, then grist is just 'something'\n        # set the value to be the grist\n        if not value and not split:\n            value = grist\n        result.append(new_grist + value)\n\n    if single_item:\n        return result[0]\n    return result", "output": "Replaces the grist of a string by a new one.\n        Returns the string with the new grist.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def app_profile_path(cls, project, instance, app_profile):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/instances/{instance}/appProfiles/{app_profile}\",\n            project=project,\n            instance=instance,\n            app_profile=app_profile,\n        )", "output": "Return a fully-qualified app_profile string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stSpectralFlux(X, X_prev):\n    \"\"\"\n    \n    \"\"\"\n    # compute the spectral flux as the sum of square distances:\n    sumX = numpy.sum(X + eps)\n    sumPrevX = numpy.sum(X_prev + eps)\n    F = numpy.sum((X / sumX - X_prev/sumPrevX) ** 2)\n\n    return F", "output": "Computes the spectral flux feature of the current frame\n    ARGUMENTS:\n        X:            the abs(fft) of the current frame\n        X_prev:        the abs(fft) of the previous frame", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_registered_stateful_ops_without_inputs():\n  \"\"\"\n  \"\"\"\n  return set([\n      name\n      for name, op in op_def_registry.get_registered_ops().items()\n      if op.is_stateful and not op.input_arg\n  ])", "output": "Returns set of registered stateful ops that do not expect inputs.\n\n  This list is used to identify the ops to be included in the state-graph and\n  that are subsequently fed into the apply-graphs.\n\n  Returns:\n    A set of strings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def authenticate_redirect(self, callback_uri: str = None) -> None:\n        \"\"\"\n        \"\"\"\n        http = self.get_auth_http_client()\n        response = await http.fetch(\n            self._oauth_request_token_url(callback_uri=callback_uri)\n        )\n        self._on_request_token(self._OAUTH_AUTHENTICATE_URL, None, response)", "output": "Just like `~OAuthMixin.authorize_redirect`, but\n        auto-redirects if authorized.\n\n        This is generally the right interface to use if you are using\n        Twitter for single-sign on.\n\n        .. versionchanged:: 3.1\n           Now returns a `.Future` and takes an optional callback, for\n           compatibility with `.gen.coroutine`.\n\n        .. versionchanged:: 6.0\n\n           The ``callback`` argument was removed. Use the returned\n           awaitable object instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_np(self, x_val, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n\n    if self.sess is None:\n      raise ValueError(\"Cannot use `generate_np` when no `sess` was\"\n                       \" provided\")\n\n    packed = self.construct_variables(kwargs)\n    fixed, feedable, _, hash_key = packed\n\n    if hash_key not in self.graphs:\n      self.construct_graph(fixed, feedable, x_val, hash_key)\n    else:\n      # remove the None arguments, they are just left blank\n      for k in list(feedable.keys()):\n        if feedable[k] is None:\n          del feedable[k]\n\n    x, new_kwargs, x_adv = self.graphs[hash_key]\n\n    feed_dict = {x: x_val}\n\n    for name in feedable:\n      feed_dict[new_kwargs[name]] = feedable[name]\n\n    return self.sess.run(x_adv, feed_dict)", "output": "Generate adversarial examples and return them as a NumPy array.\n    Sub-classes *should not* implement this method unless they must\n    perform special handling of arguments.\n\n    :param x_val: A NumPy array with the original inputs.\n    :param **kwargs: optional parameters used by child classes.\n    :return: A NumPy array holding the adversarial examples.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_distinguished_name(list_name, item_name):\n    '''\n    \n\n    '''\n    payload = {\"jsonrpc\": \"2.0\",\n               \"id\": \"ID0\",\n               \"method\": \"add_policy_distinguished_names\",\n               \"params\": [list_name, {\"item_name\": item_name}]}\n\n    response = __proxy__['bluecoat_sslv.call'](payload, True)\n\n    return _validate_change_result(response)", "output": "Adds a distinguished name to a distinguished name list.\n\n    list_name(str): The name of the specific policy distinguished name list to append to.\n\n    item_name(str): The distinguished name to append.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bluecoat_sslv.add_distinguished_name MyDistinguishedList cn=foo.bar.com", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_node(availability=str,\n                node_name=str,\n                role=str,\n                node_id=str,\n                version=int):\n    '''\n    \n    '''\n    client = docker.APIClient(base_url='unix://var/run/docker.sock')\n    try:\n        salt_return = {}\n        node_spec = {'Availability': availability,\n                     'Name': node_name,\n                     'Role': role}\n        client.update_node(node_id=node_id,\n                           version=version,\n                           node_spec=node_spec)\n        salt_return.update({'Node Information': node_spec})\n    except TypeError:\n        salt_return = {}\n        salt_return.update({'Error': 'Make sure all args are passed [availability, node_name, role, node_id, version]'})\n    return salt_return", "output": "Updates docker swarm nodes/needs to target a manager node/minion\n\n    availability\n        Drain or Active\n\n    node_name\n        minion/node\n\n    role\n        role of manager or worker\n\n    node_id\n        The Id and that can be obtained via swarm.node_ls\n\n    version\n        Is obtained by swarm.node_ls\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' swarm.update_node availability=drain node_name=minion2 \\\n            role=worker node_id=3k9x7t8m4pel9c0nqr3iajnzp version=19", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_redirects(self):\n        \"\"\"\n        \n        \"\"\"\n        html = '''\n        <html>\n            <head>\n                <meta http-equiv=\"refresh\" content=\"0;URL={url}\"/>\n            </head>\n            <body>\n                <p>\n                    The page has been moved to <a href=\"{url}\">{title}</a>\n                </p>\n            </body>\n        <html>\n        '''\n        with open(REDIRECTS_FILE) as mapping_fd:\n            reader = csv.reader(mapping_fd)\n            for row in reader:\n                if not row or row[0].strip().startswith('#'):\n                    continue\n\n                path = os.path.join(BUILD_PATH,\n                                    'html',\n                                    *row[0].split('/')) + '.html'\n\n                try:\n                    title = self._get_page_title(row[1])\n                except Exception:\n                    # the file can be an ipynb and not an rst, or docutils\n                    # may not be able to read the rst because it has some\n                    # sphinx specific stuff\n                    title = 'this page'\n\n                if os.path.exists(path):\n                    raise RuntimeError((\n                        'Redirection would overwrite an existing file: '\n                        '{}').format(path))\n\n                with open(path, 'w') as moved_page_fd:\n                    moved_page_fd.write(\n                        html.format(url='{}.html'.format(row[1]),\n                                    title=title))", "output": "Create in the build directory an html file with a redirect,\n        for every row in REDIRECTS_FILE.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode(self,\n               initial_state: State,\n               transition_function: TransitionFunction,\n               supervision: SupervisionType) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        \n        \"\"\"\n        raise NotImplementedError", "output": "Takes an initial state object, a means of transitioning from state to state, and a\n        supervision signal, and uses the supervision to train the transition function to pick\n        \"good\" states.\n\n        This function should typically return a ``loss`` key during training, which the ``Model``\n        will use as its loss.\n\n        Parameters\n        ----------\n        initial_state : ``State``\n            This is the initial state for decoding, typically initialized after running some kind\n            of encoder on some inputs.\n        transition_function : ``TransitionFunction``\n            This is the transition function that scores all possible actions that can be taken in a\n            given state, and returns a ranked list of next states at each step of decoding.\n        supervision : ``SupervisionType``\n            This is the supervision that is used to train the ``transition_function`` function to\n            pick \"good\" states.  You can use whatever kind of supervision you want (e.g., a single\n            \"gold\" action sequence, a set of possible \"gold\" action sequences, a reward function,\n            etc.).  We use ``typing.Generics`` to make sure that our static type checker is happy\n            with how you've matched the supervision that you provide in the model to the\n            ``DecoderTrainer`` that you want to use.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_member_pool(self, member, pool_name):\n        '''\n        \n        '''\n        members = self.bigIP.LocalLB.Pool.get_member(pool_names=[pool_name])[0]\n        for mem in members:\n            if member == mem.address:\n                return True\n        return False", "output": "Check a pool member exists in a specific pool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset(self):\n        \"\"\"\"\"\"\n        self._init_counter = -1\n        self._counter = -1\n        if hasattr(self, '_cells'):\n            for cell in self._cells:\n                cell.reset()", "output": "Reset before re-using the cell for another graph.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def eval_multi(self, inc_epoch=True):\n    \"\"\"\n    \n    \"\"\"\n    sess = self.sess\n    preds = self.preds\n    x = self.x_pre\n    y = self.y\n    X_train = self.X_train\n    Y_train = self.Y_train\n    X_test = self.X_test\n    Y_test = self.Y_test\n    writer = self.writer\n\n    self.summary = tf.Summary()\n    report = {}\n\n    # Evaluate on train set\n    subsample_factor = 100\n    X_train_subsampled = X_train[::subsample_factor]\n    Y_train_subsampled = Y_train[::subsample_factor]\n    acc_train = model_eval(sess, x, y, preds, X_train_subsampled,\n                           Y_train_subsampled, args=self.eval_params)\n    self.log_value('train_accuracy_subsampled', acc_train,\n                   'Clean accuracy, subsampled train')\n    report['train'] = acc_train\n\n    # Evaluate on the test set\n    acc = model_eval(sess, x, y, preds, X_test, Y_test,\n                     args=self.eval_params)\n    self.log_value('test_accuracy_natural', acc,\n                   'Clean accuracy, natural test')\n    report['test'] = acc\n\n    # Evaluate against adversarial attacks\n    if self.epoch % self.hparams.eval_iters == 0:\n      for att_type in self.attack_type_test:\n        _, preds_adv = self.attacks[att_type]\n        acc = self.eval_advs(x, y, preds_adv, X_test, Y_test, att_type)\n        report[att_type] = acc\n\n    if self.writer:\n      writer.add_summary(self.summary, self.epoch)\n\n    # Add examples of adversarial examples to the summary\n    if self.writer and self.epoch % 20 == 0 and self.sum_op is not None:\n      sm_val = self.sess.run(self.sum_op,\n                             feed_dict={x: X_test[:self.batch_size],\n                                        y: Y_test[:self.batch_size]})\n      if self.writer:\n        writer.add_summary(sm_val)\n\n    self.epoch += 1 if inc_epoch else 0\n\n    return report", "output": "Run the evaluation on multiple attacks.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_installable_file(path):\n    \"\"\"\"\"\"\n    from .vendor.pip_shims.shims import is_installable_dir, is_archive_file\n    from .patched.notpip._internal.utils.packaging import specifiers\n    from ._compat import Path\n\n    if hasattr(path, \"keys\") and any(\n        key for key in path.keys() if key in [\"file\", \"path\"]\n    ):\n        path = urlparse(path[\"file\"]).path if \"file\" in path else path[\"path\"]\n    if not isinstance(path, six.string_types) or path == \"*\":\n        return False\n\n    # If the string starts with a valid specifier operator, test if it is a valid\n    # specifier set before making a path object (to avoid breaking windows)\n    if any(path.startswith(spec) for spec in \"!=<>~\"):\n        try:\n            specifiers.SpecifierSet(path)\n        # If this is not a valid specifier, just move on and try it as a path\n        except specifiers.InvalidSpecifier:\n            pass\n        else:\n            return False\n\n    if not os.path.exists(os.path.abspath(path)):\n        return False\n\n    lookup_path = Path(path)\n    absolute_path = \"{0}\".format(lookup_path.absolute())\n    if lookup_path.is_dir() and is_installable_dir(absolute_path):\n        return True\n\n    elif lookup_path.is_file() and is_archive_file(absolute_path):\n        return True\n\n    return False", "output": "Determine if a path can potentially be installed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_files_exist(self, folder, resources):\n        \"\"\" \n        \"\"\"\n        for item in resources:\n            file_name = item.get('path')\n            full_path = os.path.join(folder, file_name)\n            if not os.path.isfile(full_path):\n                raise ValueError('%s does not exist' % full_path)", "output": "ensure that one or more resource files exist in a folder\n\n            Parameters\n            ==========\n            folder: the folder to validate\n            resources: one or more resources to validate within the folder", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def fetch_user_profile(self, user_id):\n        \"\"\"\n        \"\"\"\n\n        state = self._connection\n        data = await self.http.get_user_profile(user_id)\n\n        def transform(d):\n            return state._get_guild(int(d['id']))\n\n        since = data.get('premium_since')\n        mutual_guilds = list(filter(None, map(transform, data.get('mutual_guilds', []))))\n        user = data['user']\n        return Profile(flags=user.get('flags', 0),\n                       premium_since=utils.parse_time(since),\n                       mutual_guilds=mutual_guilds,\n                       user=User(data=user, state=state),\n                       connected_accounts=data['connected_accounts'])", "output": "|coro|\n\n        Gets an arbitrary user's profile. This can only be used by non-bot accounts.\n\n        Parameters\n        ------------\n        user_id: :class:`int`\n            The ID of the user to fetch their profile for.\n\n        Raises\n        -------\n        Forbidden\n            Not allowed to fetch profiles.\n        HTTPException\n            Fetching the profile failed.\n\n        Returns\n        --------\n        :class:`.Profile`\n            The profile of the user.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download(self):\n        \"\"\"\n        \"\"\"\n        mp3_directory = self._pre_download()\n        self.data.swifter.apply(func=lambda arg: self._download(*arg, mp3_directory), axis=1, raw=True)\n        return mp3_directory", "output": "Downloads the data associated with this instance\n        Return:\n          mp3_directory (os.path): The directory into which the associated mp3's were downloaded", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_numpy(self, dtype=None, copy=False):\n        \"\"\"\n        \n        \"\"\"\n        result = np.array(self.values, dtype=dtype, copy=copy)\n        return result", "output": "Convert the DataFrame to a NumPy array.\n\n        .. versionadded:: 0.24.0\n\n        By default, the dtype of the returned array will be the common NumPy\n        dtype of all types in the DataFrame. For example, if the dtypes are\n        ``float16`` and ``float32``, the results dtype will be ``float32``.\n        This may require copying data and coercing values, which may be\n        expensive.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype, optional\n            The dtype to pass to :meth:`numpy.asarray`\n        copy : bool, default False\n            Whether to ensure that the returned value is a not a view on\n            another array. Note that ``copy=False`` does not *ensure* that\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\n            a copy is made, even if not strictly necessary.\n\n        Returns\n        -------\n        numpy.ndarray\n\n        See Also\n        --------\n        Series.to_numpy : Similar method for Series.\n\n        Examples\n        --------\n        >>> pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]}).to_numpy()\n        array([[1, 3],\n               [2, 4]])\n\n        With heterogenous data, the lowest common type will have to\n        be used.\n\n        >>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.5]})\n        >>> df.to_numpy()\n        array([[1. , 3. ],\n               [2. , 4.5]])\n\n        For a mix of numeric and non-numeric types, the output array will\n        have object dtype.\n\n        >>> df['C'] = pd.date_range('2000', periods=2)\n        >>> df.to_numpy()\n        array([[1, 3.0, Timestamp('2000-01-01 00:00:00')],\n               [2, 4.5, Timestamp('2000-01-02 00:00:00')]], dtype=object)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_reactor(self, tag):\n        '''\n        \n        '''\n        reactors = self.list_all()\n        for reactor in reactors:\n            _tag = next(six.iterkeys(reactor))\n            if _tag == tag:\n                self.minion.opts['reactor'].remove(reactor)\n                return {'status': True, 'comment': 'Reactor deleted.'}\n\n        return {'status': False, 'comment': 'Reactor does not exists.'}", "output": "Delete a reactor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_suffix (name, new_suffix):\n    \"\"\" \n    \"\"\"\n    assert isinstance(name, basestring)\n    assert isinstance(new_suffix, basestring)\n    split = os.path.splitext (name)\n    return split [0] + new_suffix", "output": "Replaces the suffix of name by new_suffix.\n        If no suffix exists, the new one is added.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def corr(self, other=None, pairwise=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        if other is None:\n            other = self._selected_obj\n            # only default unset\n            pairwise = True if pairwise is None else pairwise\n        other = self._shallow_copy(other)\n\n        def _get_corr(X, Y):\n            X = self._shallow_copy(X)\n            Y = self._shallow_copy(Y)\n\n            def _cov(x, y):\n                return libwindow.ewmcov(x, y, self.com, int(self.adjust),\n                                        int(self.ignore_na),\n                                        int(self.min_periods),\n                                        1)\n\n            x_values = X._prep_values()\n            y_values = Y._prep_values()\n            with np.errstate(all='ignore'):\n                cov = _cov(x_values, y_values)\n                x_var = _cov(x_values, x_values)\n                y_var = _cov(y_values, y_values)\n                corr = cov / _zsqrt(x_var * y_var)\n            return X._wrap_result(corr)\n\n        return _flex_binary_moment(self._selected_obj, other._selected_obj,\n                                   _get_corr, pairwise=bool(pairwise))", "output": "Exponential weighted sample correlation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_expiration_seconds_v4(expiration):\n    \"\"\"\n    \"\"\"\n    if not isinstance(expiration, _EXPIRATION_TYPES):\n        raise TypeError(\n            \"Expected an integer timestamp, datetime, or \"\n            \"timedelta. Got %s\" % type(expiration)\n        )\n\n    now = NOW().replace(tzinfo=_helpers.UTC)\n\n    if isinstance(expiration, six.integer_types):\n        seconds = expiration\n\n    if isinstance(expiration, datetime.datetime):\n\n        if expiration.tzinfo is None:\n            expiration = expiration.replace(tzinfo=_helpers.UTC)\n\n        expiration = expiration - now\n\n    if isinstance(expiration, datetime.timedelta):\n        seconds = int(expiration.total_seconds())\n\n    if seconds > SEVEN_DAYS:\n        raise ValueError(\n            \"Max allowed expiration interval is seven days (%d seconds)\".format(\n                SEVEN_DAYS\n            )\n        )\n\n    return seconds", "output": "Convert 'expiration' to a number of seconds offset from the current time.\n\n    :type expiration: Union[Integer, datetime.datetime, datetime.timedelta]\n    :param expiration: Point in time when the signed URL should expire.\n\n    :raises: :exc:`TypeError` when expiration is not a valid type.\n    :raises: :exc:`ValueError` when expiration is too large.\n    :rtype: Integer\n    :returns: seconds in the future when the signed URL will expire", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _timeout_future(self, tag, matcher, future):\n        '''\n        \n        '''\n        if (tag, matcher) not in self.tag_map:\n            return\n        if not future.done():\n            future.set_exception(TimeoutException())\n            self.tag_map[(tag, matcher)].remove(future)\n        if not self.tag_map[(tag, matcher)]:\n            del self.tag_map[(tag, matcher)]", "output": "Timeout a specific future", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def orchestrate(mods,\n                saltenv='base',\n                test=None,\n                exclude=None,\n                pillar=None,\n                pillarenv=None):\n    '''\n    \n    '''\n    return _orchestrate(mods=mods,\n                        saltenv=saltenv,\n                        test=test,\n                        exclude=exclude,\n                        pillar=pillar,\n                        pillarenv=pillarenv)", "output": ".. versionadded:: 2016.11.0\n\n    Execute the orchestrate runner from a masterless minion.\n\n    .. seealso:: More Orchestrate documentation\n\n        * :ref:`Full Orchestrate Tutorial <orchestrate-runner>`\n        * :py:mod:`Docs for the ``salt`` state module <salt.states.saltmod>`\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-call --local state.orchestrate webserver\n        salt-call --local state.orchestrate webserver saltenv=dev test=True\n        salt-call --local state.orchestrate webserver saltenv=dev pillarenv=aws", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer(data, label):\n    \"\"\"  \"\"\"\n    data = mx.image.imresize(data, IMAGE_SIZE, IMAGE_SIZE)\n    data = mx.nd.transpose(data, (2, 0, 1))\n    data = data.astype(np.float32) / 128.0 - 1\n    return data, label", "output": "data preparation", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(name, jail=None):\n    '''\n    \n    '''\n    cmd = '{0} {1} onestart'.format(_cmd(jail), name)\n    return not __salt__['cmd.retcode'](cmd, python_shell=False)", "output": "Start the specified service\n\n    .. versionchanged:: 2016.3.4\n\n    jail: optional jid or jail name\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.start <service name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_properties(self, resource):\n        \"\"\"\n        \"\"\"\n        self._properties.clear()\n        cleaned = resource.copy()\n        if \"name\" in cleaned:\n            self.name = variable_name_from_full_name(cleaned.pop(\"name\"))\n        self._properties.update(cleaned)", "output": "Update properties from resource in body of ``api_response``\n\n        :type resource: dict\n        :param resource: variable representation returned from the API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sparse_tensor_value_to_texts(value, alphabet):\n    \n    \"\"\"\n    return sparse_tuple_to_texts((value.indices, value.values, value.dense_shape), alphabet)", "output": "r\"\"\"\n    Given a :class:`tf.SparseTensor` ``value``, return an array of Python strings\n    representing its values, converting tokens to strings using ``alphabet``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_monitor_callback(self, callback, monitor_all=False):\n        \"\"\"\n        \"\"\"\n        cb_type = ctypes.CFUNCTYPE(None, ctypes.c_char_p, NDArrayHandle, ctypes.c_void_p)\n        self._monitor_callback = cb_type(_monitor_callback_wrapper(callback))\n        check_call(_LIB.MXExecutorSetMonitorCallbackEX(\n            self.handle,\n            self._monitor_callback,\n            None,\n            ctypes.c_int(monitor_all)))", "output": "Install callback for monitor.\n\n        Parameters\n        ----------\n        callback : function\n            Takes a string and an NDArrayHandle.\n        monitor_all : bool, default False\n            If true, monitor both input and output, otherwise monitor output only.\n\n        Examples\n        --------\n        >>> def mon_callback(*args, **kwargs):\n        >>>     print(\"Do your stuff here.\")\n        >>>\n        >>> texe.set_monitor_callback(mon_callback)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _clear_scope_decos(self):\n        \"\"\"\"\"\"\n        for deco in self._scope_decos:\n            self.editor.decorations.remove(deco)\n        self._scope_decos[:] = []", "output": "Clear scope decorations (on the editor)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _prepare_method(self, pandas_func, **kwargs):\n        \"\"\"\n        \"\"\"\n        if self._is_transposed:\n\n            def helper(df, internal_indices=[]):\n                if len(internal_indices) > 0:\n                    return pandas_func(\n                        df.T, internal_indices=internal_indices, **kwargs\n                    )\n                return pandas_func(df.T, **kwargs)\n\n        else:\n\n            def helper(df, internal_indices=[]):\n                if len(internal_indices) > 0:\n                    return pandas_func(df, internal_indices=internal_indices, **kwargs)\n                return pandas_func(df, **kwargs)\n\n        return helper", "output": "Prepares methods given various metadata.\n        Args:\n            pandas_func: The function to prepare.\n\n        Returns\n            Helper function which handles potential transpose.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_hostname(hostname):\n    '''\n    \n    '''\n    with salt.utils.winapi.Com():\n        conn = wmi.WMI()\n        comp = conn.Win32_ComputerSystem()[0]\n    return comp.Rename(Name=hostname)", "output": "Set the hostname of the windows minion, requires a restart before this will\n    be updated.\n\n    .. versionadded:: 2016.3.0\n\n    Args:\n        hostname (str): The hostname to set\n\n    Returns:\n        bool: ``True`` if successful, otherwise ``False``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'minion-id' system.set_hostname newhostname", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mtf_image_transformer_cifar_mp_4x():\n  \"\"\"\"\"\"\n  hparams = mtf_image_transformer_base_cifar()\n  hparams.mesh_shape = \"model:4;batch:8\"\n  hparams.layout = \"batch:batch;d_ff:model;heads:model\"\n  hparams.batch_size = 32\n  hparams.num_heads = 8\n  hparams.d_ff = 8192\n  return hparams", "output": "Data parallel CIFAR parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_device(self, device_name):\n    \"\"\"\n    \n    \"\"\"\n    device_name = unify_device_name(device_name)\n    self.device_name = device_name\n    for layer in self.layers:\n      layer.device_name = device_name", "output": "Set the device before the next fprop to create a new graph on the\n    specified device.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def query_put_bounders(query, partition_column, start, end):\n    \"\"\" \n    \"\"\"\n    where = \" WHERE TMP_TABLE.{0} >= {1} AND TMP_TABLE.{0} <= {2}\".format(\n        partition_column, start, end\n    )\n    query_with_bounders = \"SELECT * FROM ({0}) AS TMP_TABLE {1}\".format(query, where)\n    return query_with_bounders", "output": "Put bounders in the query\n\n    Args:\n        query: SQL query string\n        partition_column: partition_column name\n        start: lower_bound\n        end: upper_bound\n\n    Returns:\n        Query with bounders", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_language_modeling_inputs(filename,\n                                  delimiter=\"\\n\",\n                                  repeat=1,\n                                  append_space_to_final_punctionation=True):\n  \"\"\"\n  \"\"\"\n  with tf.gfile.Open(filename) as f:\n    text = f.read()\n  inputs = text.split(delimiter)\n  if not inputs[-1]:\n    inputs.pop()\n  inputs *= repeat\n  if append_space_to_final_punctionation:\n    inputs = [\n        s + \" \" if s and s[-1] in string.punctuation else s for s in inputs]\n  return inputs", "output": "Read a file of partial texts to continue.\n\n  The purpose of append_space_to_final_punctionation is that SubwordTokenizer\n  groups punctuation and the ensuing space in the same token.  Adding a space\n  causes the token to be completed.\n\n  Args:\n    filename: a string\n    delimiter: a string\n    repeat: an integer - we repeat the entire file that many times.\n    append_space_to_final_punctionation: a boolean\n\n  Returns:\n    a list of strings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_envs(self):\n        '''\n        \n        '''\n        envs = set(['base'])\n        if 'pillar_roots' in self.opts:\n            envs.update(list(self.opts['pillar_roots']))\n        return envs", "output": "Pull the file server environments out of the master options", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _resizeColumnsToContents(self, header, data, limit_ms):\r\n        \"\"\"\"\"\"\r\n        max_col = data.model().columnCount()\r\n        if limit_ms is None:\r\n            max_col_ms = None\r\n        else:\r\n            max_col_ms = limit_ms / max(1, max_col)\r\n        for col in range(max_col):\r\n            self._resizeColumnToContents(header, data, col, max_col_ms)", "output": "Resize all the colummns to its contents.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_namespaced_lease(self, namespace, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_lease_with_http_info(namespace, body, **kwargs)\n        else:\n            (data) = self.create_namespaced_lease_with_http_info(namespace, body, **kwargs)\n            return data", "output": "create a Lease\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_lease(namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1Lease body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1Lease\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_model_from_external_data(model):  # type: (ModelProto) -> None\n    \"\"\"\n    \n    \"\"\"\n    for tensor in _get_all_tensors(model):\n        if uses_external_data(tensor):\n            if not tensor.HasField(\"raw_data\"):\n                raise ValueError(\"raw_data field doesn't exist.\")\n            del tensor.external_data[:]\n            tensor.data_location = TensorProto.DEFAULT", "output": "call to set all tensors data as embedded data. save_model saves all the tensors data as embedded data after calling this function.\n    @params\n    model: ModelProto to be converted.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_date(date, kwargs):\n    \"\"\"\n    \n    \"\"\"\n    if date is None:\n        if not kwargs:\n            raise ValueError('Must pass a date or kwargs')\n        else:\n            return datetime.date(**kwargs)\n\n    elif kwargs:\n        raise ValueError('Cannot pass kwargs and a date')\n    else:\n        return date", "output": "Builds the date argument for event rules.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_op_name(op, special):\n    \"\"\"\n    \n    \"\"\"\n    opname = op.__name__.strip('_')\n    if special:\n        opname = '__{opname}__'.format(opname=opname)\n    return opname", "output": "Find the name to attach to this method according to conventions\n    for special and non-special methods.\n\n    Parameters\n    ----------\n    op : binary operator\n    special : bool\n\n    Returns\n    -------\n    op_name : str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __main():\n    '''\n    '''\n    if len(sys.argv) < 3:\n        sys.stderr.write('usage: {0} <detail|list> <system|system+user>\\n'.format(sys.argv[0]))\n        sys.exit(64)\n    user_pkgs = False\n    version_only = False\n    if six.text_type(sys.argv[1]) == 'list':\n        version_only = True\n    if six.text_type(sys.argv[2]) == 'system+user':\n        user_pkgs = True\n    import salt.utils.json\n    import timeit\n\n    def run():\n        '''\n        Main run code, when this module is run directly\n        '''\n        pkg_list = WinSoftware(user_pkgs=user_pkgs, version_only=version_only)\n        print(salt.utils.json.dumps(pkg_list.data, sort_keys=True, indent=4))  # pylint: disable=superfluous-parens\n        print('Total: {}'.format(len(pkg_list)))  # pylint: disable=superfluous-parens\n\n    print('Time Taken: {}'.format(timeit.timeit(run, number=1)))", "output": "This module can also be run directly for testing\n        Args:\n            detail|list : Provide ``detail`` or version ``list``.\n            system|system+user: System installed and System and User installs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_text_stream(name, encoding=None, errors='strict'):\n    \"\"\"\n    \"\"\"\n    opener = text_streams.get(name)\n    if opener is None:\n        raise TypeError('Unknown standard stream %r' % name)\n    return opener(encoding, errors)", "output": "Returns a system stream for text processing.  This usually returns\n    a wrapped stream around a binary stream returned from\n    :func:`get_binary_stream` but it also can take shortcuts on Python 3\n    for already correctly configured streams.\n\n    :param name: the name of the stream to open.  Valid names are ``'stdin'``,\n                 ``'stdout'`` and ``'stderr'``\n    :param encoding: overrides the detected default encoding.\n    :param errors: overrides the default error mode.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_operation(database, keys, table, operation, latencies_ms):\n    \"\"\"\"\"\"\n    key = random.choice(keys)\n    start = timeit.default_timer()\n    if operation == 'read':\n        read(database, table, key)\n    elif operation == 'update':\n        update(database, table,  key)\n    else:\n        raise ValueError('Unknown operation: %s' % operation)\n    end = timeit.default_timer()\n    latencies_ms[operation].append((end - start) * 1000)", "output": "Does a single operation and records latency.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def serving_input_fn(self, hparams, decode_hparams=None, use_tpu=False):\n    \"\"\"\"\"\"\n    mode = tf.estimator.ModeKeys.PREDICT\n    serialized_example = tf.placeholder(\n        dtype=tf.string, shape=[None], name=\"serialized_example\")\n    dataset = tf.data.Dataset.from_tensor_slices(serialized_example)\n    dataset = dataset.map(self.decode_example)\n    dataset = dataset.map(lambda ex: self.preprocess_example(ex, mode, hparams))\n    dataset = dataset.map(data_reader.cast_ints_to_int32)\n\n    if use_tpu:\n      padded_shapes = data_reader.pad_for_tpu(dataset.output_shapes, hparams,\n                                              hparams.max_length)\n      batch_size = 1 if not decode_hparams else getattr(decode_hparams,\n                                                        \"batch_size\", 1)\n      dataset = dataset.padded_batch(\n          batch_size, padded_shapes, drop_remainder=False)\n      dataset = dataset.map(\n          functools.partial(data_reader.pad_batch, batch_multiple=batch_size))\n    else:\n      dataset = dataset.padded_batch(\n          tf.shape(serialized_example, out_type=tf.int64)[0],\n          dataset.output_shapes)\n\n    dataset = dataset.map(data_reader.standardize_shapes)\n    features = tf.data.experimental.get_single_element(dataset)\n\n    if self.has_inputs:\n      features.pop(\"targets\", None)\n\n    return tf.estimator.export.ServingInputReceiver(\n        features=features, receiver_tensors=serialized_example)", "output": "Input fn for serving export, starting from serialized example.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_project_filenames(self, recent_files):\r\n        \"\"\"\"\"\"\r\n        if (self.current_active_project\r\n                and self.is_valid_project(\r\n                        self.current_active_project.root_path)):\r\n            self.current_active_project.set_recent_files(recent_files)", "output": "Set the list of open file names in a project", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_valid(self, qstr=None):\r\n        \"\"\"\"\"\"\r\n        if qstr is None:\r\n            qstr = self.currentText()\r\n        return osp.isdir(to_text_string(qstr))", "output": "Return True if string is valid", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def namespaces(**kwargs):\n    '''\n    \n    '''\n    cfg = _setup_conn(**kwargs)\n    try:\n        api_instance = kubernetes.client.CoreV1Api()\n        api_response = api_instance.list_namespace()\n\n        return [nms['metadata']['name'] for nms in api_response.to_dict().get('items')]\n    except (ApiException, HTTPError) as exc:\n        if isinstance(exc, ApiException) and exc.status == 404:\n            return None\n        else:\n            log.exception('Exception when calling CoreV1Api->list_namespace')\n            raise CommandExecutionError(exc)\n    finally:\n        _cleanup(**cfg)", "output": "Return the names of the available namespaces\n\n    CLI Examples::\n\n        salt '*' kubernetes.namespaces\n        salt '*' kubernetes.namespaces kubeconfig=/etc/salt/k8s/kubeconfig context=minikube", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _authenticate(secrets_file):\n    \"\"\"\n    \"\"\"\n    flow = oauthclient.flow_from_clientsecrets(\n        secrets_file,\n        scope=OAUTH_SCOPE,\n        message=('Failed to initialized OAuth 2.0 flow with secrets '\n                 'file: %s' % secrets_file))\n    storage = oauthfile.Storage(OAUTH_CREDENTIALS_FILE)\n    credentials = storage.get()\n    if credentials is None or credentials.invalid:\n        credentials = oauthtools.run_flow(flow, storage, oauthtools.argparser.parse_args(args=[]))\n    http = httplib2.Http()\n    return credentials.authorize(http)", "output": "Runs the OAuth 2.0 installed application flow.\n\n    Returns:\n      An authorized httplib2.Http instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cumsum(self, axis=0, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        nv.validate_cumsum(args, kwargs)\n\n        if axis is None:\n            axis = self._stat_axis_number\n\n        return self.apply(lambda x: x.cumsum(), axis=axis)", "output": "Return SparseDataFrame of cumulative sums over requested axis.\n\n        Parameters\n        ----------\n        axis : {0, 1}\n            0 for row-wise, 1 for column-wise\n\n        Returns\n        -------\n        y : SparseDataFrame", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fail(self, message, param=None, ctx=None):\n        \"\"\"\"\"\"\n        raise BadParameter(message, ctx=ctx, param=param)", "output": "Helper method to fail with an invalid value message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_mount_cache(name):\n    '''\n    \n    '''\n    cache = salt.utils.mount.read_cache(__opts__)\n    if cache:\n        if 'mounts' in cache and cache['mounts']:\n            if name in cache['mounts']:\n                return cache['mounts'][name]\n    return {}", "output": ".. versionadded:: 2018.3.0\n\n    Provide information if the path is mounted\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mount.read_mount_cache /mnt/share", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def market_value(self):\n        \"\"\"\n        \n        \"\"\"\n        return sum(account.market_value for account in six.itervalues(self._accounts))", "output": "[float] \u5e02\u503c", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def removed(name):\n    '''\n    \n    '''\n    ret = {'name': name, 'result': None, 'comment': '', 'changes': {}}\n    if name not in __salt__['pecl.list']():\n        ret['result'] = True\n        ret['comment'] = 'Pecl extension {0} is not installed.'.format(name)\n        return ret\n\n    if __opts__['test']:\n        ret['comment'] = ('Pecl extension {0} would have been removed'\n                          .format(name))\n        return ret\n    if __salt__['pecl.uninstall'](name):\n        ret['result'] = True\n        ret['changes'][name] = 'Removed'\n        ret['comment'] = ('Pecl extension {0} was successfully removed.'\n                          .format(name))\n    else:\n        ret['result'] = False\n        ret['comment'] = 'Could not remove pecl extension {0}.'.format(name)\n    return ret", "output": "Make sure that a pecl extension is not installed.\n\n    name\n        The pecl extension name to uninstall", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def diff(self, a_ref, target=None, b_ref=None):\n    \"\"\"\n    \"\"\"\n    result = {}\n    diff_dct = self.scm.get_diff_trees(a_ref, b_ref=b_ref)\n    result[DIFF_A_REF] = diff_dct[DIFF_A_REF]\n    result[DIFF_B_REF] = diff_dct[DIFF_B_REF]\n    if diff_dct[DIFF_EQUAL]:\n        result[DIFF_EQUAL] = True\n        return result\n    result[DIFF_LIST] = []\n    diff_outs = _get_diff_outs(self, diff_dct)\n    if target is None:\n        result[DIFF_LIST] = [\n            _diff_royal(self, path, diff_outs[path]) for path in diff_outs\n        ]\n    elif target in diff_outs:\n        result[DIFF_LIST] = [_diff_royal(self, target, diff_outs[target])]\n    else:\n        msg = \"Have not found file/directory '{}' in the commits\"\n        raise FileNotInCommitError(msg.format(target))\n    return result", "output": "Gerenates diff message string output\n\n    Args:\n        target(str) - file/directory to check diff of\n        a_ref(str) - first tag\n        (optional) b_ref(str) - second git tag\n\n    Returns:\n        string: string of output message with diff info", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop_apppool(name):\n    '''\n    \n    '''\n    ps_cmd = ['Stop-WebAppPool', r\"'{0}'\".format(name)]\n\n    cmd_ret = _srvmgr(ps_cmd)\n\n    return cmd_ret['retcode'] == 0", "output": "Stop an IIS application pool.\n\n    .. versionadded:: 2017.7.0\n\n    Args:\n        name (str): The name of the App Pool to stop.\n\n    Returns:\n        bool: True if successful, otherwise False\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' win_iis.stop_apppool name='MyTestPool'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absent(name, keys=None, **connection_args):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': 'Key(s) specified already absent'}\n\n    if keys:\n        if not isinstance(keys, list):\n            ret['result'] = False\n            ret['comment'] = '`keys` not formed as a list type'\n            return ret\n        delete_list = [key for key in keys\n                       if __salt__['redis.exists'](key, **connection_args)]\n        if not delete_list:\n            return ret\n        __salt__['redis.delete'](*delete_list, **connection_args)\n        ret['changes']['deleted'] = delete_list\n        ret['comment'] = 'Keys deleted'\n        return ret\n\n    if __salt__['redis.exists'](name, **connection_args):\n        __salt__['redis.delete'](name, **connection_args)\n        ret['comment'] = 'Key deleted'\n        ret['changes']['deleted'] = [name]\n    return ret", "output": "Ensure key absent from redis\n\n    name\n        Key to ensure absent from redis\n\n    keys\n        list of keys to ensure absent, name will be ignored if this is used", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_namespaced_deployment_scale(self, name, namespace, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_deployment_scale_with_http_info(name, namespace, **kwargs)\n        else:\n            (data) = self.read_namespaced_deployment_scale_with_http_info(name, namespace, **kwargs)\n            return data", "output": "read scale of the specified Deployment\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_deployment_scale(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Scale (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1Scale\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_missing_flags(conf, atom, flags):\n    '''\n    \n    '''\n    new_flags = []\n    for flag in flags:\n        if not has_flag(conf, atom, flag):\n            new_flags.append(flag)\n    return new_flags", "output": "Find out which of the given flags are currently not set.\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' portage_config.get_missing_flags use salt \"['ldap', '-libvirt', 'openssl']\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _reindex_multi(self, axes, copy, fill_value):\n        \"\"\"\n        \n        \"\"\"\n\n        new_index, row_indexer = self.index.reindex(axes['index'])\n        new_columns, col_indexer = self.columns.reindex(axes['columns'])\n\n        if row_indexer is not None and col_indexer is not None:\n            indexer = row_indexer, col_indexer\n            new_values = algorithms.take_2d_multi(self.values, indexer,\n                                                  fill_value=fill_value)\n            return self._constructor(new_values, index=new_index,\n                                     columns=new_columns)\n        else:\n            return self._reindex_with_indexers({0: [new_index, row_indexer],\n                                                1: [new_columns, col_indexer]},\n                                               copy=copy,\n                                               fill_value=fill_value)", "output": "We are guaranteed non-Nones in the axes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def available():\n    '''\n    \n    '''\n    ret = []\n    for path in __salt__['file.find']('/boot/kernel', name='*.ko$'):\n        bpath = os.path.basename(path)\n        comps = bpath.split('.')\n        if 'ko' in comps:\n            # This is a kernel module, return it without the .ko extension\n            ret.append('.'.join(comps[:comps.index('ko')]))\n    return ret", "output": "Return a list of all available kernel modules\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' kmod.available", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clean_kwargs(**kwargs):\n    '''\n    \n    '''\n    ret = {}\n    for key, val in six.iteritems(kwargs):\n        if not key.startswith('__'):\n            ret[key] = val\n    return ret", "output": "Return a dict without any of the __pub* keys (or any other keys starting\n    with a dunder) from the kwargs dict passed into the execution module\n    functions. These keys are useful for tracking what was used to invoke\n    the function call, but they may not be desirable to have if passing the\n    kwargs forward wholesale.\n\n    Usage example:\n\n    .. code-block:: python\n\n        kwargs = __utils__['args.clean_kwargs'](**kwargs)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set(self, locs, values):\n        \"\"\"\n        \n        \"\"\"\n        values = conversion.ensure_datetime64ns(values, copy=False)\n\n        self.values[locs] = values", "output": "Modify Block in-place with new item value\n\n        Returns\n        -------\n        None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _display_runner(rows, labels, title, display=_DEFAULT_DISPLAY):\n    '''\n    \n    '''\n    if display:\n        net_runner_opts = _get_net_runner_opts()\n        if net_runner_opts.get('outputter') == 'table':\n            ret = salt.output.out_format({'rows': rows, 'labels': labels},\n                                         'table',\n                                         __opts__,\n                                         title=title,\n                                         rows_key='rows',\n                                         labels_key='labels')\n        else:\n            ret = salt.output.out_format(rows,\n                                         net_runner_opts.get('outputter'),\n                                         __opts__)\n        print(ret)\n    else:\n        return rows", "output": "Display or return the rows.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_remote_url(cls, location):\n        \"\"\"\n        \n        \"\"\"\n        # We need to pass 1 for extra_ok_returncodes since the command\n        # exits with return code 1 if there are no matching lines.\n        stdout = cls.run_command(\n            ['config', '--get-regexp', r'remote\\..*\\.url'],\n            extra_ok_returncodes=(1, ), show_stdout=False, cwd=location,\n        )\n        remotes = stdout.splitlines()\n        try:\n            found_remote = remotes[0]\n        except IndexError:\n            raise RemoteNotFoundError\n\n        for remote in remotes:\n            if remote.startswith('remote.origin.url '):\n                found_remote = remote\n                break\n        url = found_remote.split(' ')[1]\n        return url.strip()", "output": "Return URL of the first remote encountered.\n\n        Raises RemoteNotFoundError if the repository does not have a remote\n        url configured.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vm_snapshot_delete(vm_name, kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The vm_snapshot_delete action must be called with -a or --action.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    snapshot_id = kwargs.get('snapshot_id', None)\n    if snapshot_id is None:\n        raise SaltCloudSystemExit(\n            'The vm_snapshot_delete function requires a \\'snapshot_id\\' to be provided.'\n        )\n\n    server, user, password = _get_xml_rpc()\n    auth = ':'.join([user, password])\n    vm_id = int(get_vm_id(kwargs={'name': vm_name}))\n    response = server.one.vm.snapshotdelete(auth, vm_id, int(snapshot_id))\n\n    data = {\n        'action': 'vm.snapshotdelete',\n        'snapshot_deleted': response[0],\n        'vm_id': response[1],\n        'error_code': response[2],\n    }\n\n    return data", "output": "Deletes a virtual machine snapshot from the provided VM.\n\n    .. versionadded:: 2016.3.0\n\n    vm_name\n        The name of the VM from which to delete the snapshot.\n\n    snapshot_id\n        The ID of the snapshot to be deleted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a vm_snapshot_delete my-vm snapshot_id=8", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def persist(self, storageLevel):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(storageLevel, StorageLevel):\n            raise TypeError(\"`storageLevel` should be a StorageLevel, got %s\" % type(storageLevel))\n        javaStorageLevel = self._java_matrix_wrapper._sc._getJavaStorageLevel(storageLevel)\n        self._java_matrix_wrapper.call(\"persist\", javaStorageLevel)\n        return self", "output": "Persists the underlying RDD with the specified storage level.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def selection_range(self):\n        \"\"\"\n        \n        \"\"\"\n        editor = self._editor\n        doc = editor.document()\n        start = doc.findBlock(\n            editor.textCursor().selectionStart()).blockNumber()\n        end = doc.findBlock(\n            editor.textCursor().selectionEnd()).blockNumber()\n        text_cursor = QTextCursor(editor.textCursor())\n        text_cursor.setPosition(editor.textCursor().selectionEnd())\n        if text_cursor.columnNumber() == 0 and start != end:\n            end -= 1\n        return start, end", "output": "Returns the selected lines boundaries (start line, end line)\n\n        :return: tuple(int, int)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate(self,\n                 asset,\n                 amount,\n                 portfolio,\n                 algo_datetime,\n                 algo_current_data):\n        \"\"\"\n        \n        \"\"\"\n\n        if self.asset is not None and self.asset != asset:\n            return\n\n        current_share_count = portfolio.positions[asset].amount\n        shares_post_order = current_share_count + amount\n\n        too_many_shares = (self.max_shares is not None and\n                           abs(shares_post_order) > self.max_shares)\n        if too_many_shares:\n            self.handle_violation(asset, amount, algo_datetime)\n\n        current_price = algo_current_data.current(asset, \"price\")\n        value_post_order = shares_post_order * current_price\n\n        too_much_value = (self.max_notional is not None and\n                          abs(value_post_order) > self.max_notional)\n\n        if too_much_value:\n            self.handle_violation(asset, amount, algo_datetime)", "output": "Fail if the given order would cause the magnitude of our position to be\n        greater in shares than self.max_shares or greater in dollar value than\n        self.max_notional.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dmi_cast(key, val, clean=True):\n    '''\n    \n    '''\n    if clean and not _dmi_isclean(key, val):\n        return\n    elif not re.match(r'serial|part|asset|product', key, flags=re.IGNORECASE):\n        if ',' in val:\n            val = [el.strip() for el in val.split(',')]\n        else:\n            try:\n                val = int(val)\n            except Exception:\n                pass\n\n    return val", "output": "Simple caster thingy for trying to fish out at least ints & lists from strings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_value(self, sid, dt, field):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            country_code = self._country_code_for_assets([sid])\n        except ValueError as exc:\n            raise_from(\n                NoDataForSid(\n                    'Asset not contained in daily pricing file: {}'.format(sid)\n                ),\n                exc\n            )\n        return self._readers[country_code].get_value(sid, dt, field)", "output": "Retrieve the value at the given coordinates.\n\n        Parameters\n        ----------\n        sid : int\n            The asset identifier.\n        dt : pd.Timestamp\n            The timestamp for the desired data point.\n        field : string\n            The OHLVC name for the desired data point.\n\n        Returns\n        -------\n        value : float|int\n            The value at the given coordinates, ``float`` for OHLC, ``int``\n            for 'volume'.\n\n        Raises\n        ------\n        NoDataOnDate\n            If the given dt is not a valid market minute (in minute mode) or\n            session (in daily mode) according to this reader's tradingcalendar.\n        NoDataForSid\n            If the given sid is not valid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def has_arg(fn, arg_name):\n    \"\"\"\n    \"\"\"\n    if sys.version_info < (3,):\n        if isinstance(fn, types.FunctionType) or isinstance(fn, types.MethodType):\n            arg_spec = inspect.getargspec(fn)\n        else:\n            try:\n                arg_spec = inspect.getargspec(fn.__call__)\n            except AttributeError:\n                return False\n        return (arg_name in arg_spec.args)\n    elif sys.version_info < (3, 6):\n        arg_spec = inspect.getfullargspec(fn)\n        return (arg_name in arg_spec.args or\n                arg_name in arg_spec.kwonlyargs)\n    else:\n        try:\n            signature = inspect.signature(fn)\n        except ValueError:\n            # handling Cython\n            signature = inspect.signature(fn.__call__)\n        parameter = signature.parameters.get(arg_name)\n        if parameter is None:\n            return False\n        return (parameter.kind in (inspect.Parameter.POSITIONAL_OR_KEYWORD,\n                                   inspect.Parameter.KEYWORD_ONLY))", "output": "Checks if a callable accepts a given keyword argument.\n\n    Args:\n        fn: callable to inspect\n        arg_name: string, keyword argument name to check\n\n    Returns:\n        bool, whether `fn` accepts a `arg_name` keyword argument.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def connection_from_url(self, url, pool_kwargs=None):\n        \"\"\"\n        \n        \"\"\"\n        u = parse_url(url)\n        return self.connection_from_host(u.host, port=u.port, scheme=u.scheme,\n                                         pool_kwargs=pool_kwargs)", "output": "Similar to :func:`urllib3.connectionpool.connection_from_url`.\n\n        If ``pool_kwargs`` is not provided and a new pool needs to be\n        constructed, ``self.connection_pool_kw`` is used to initialize\n        the :class:`urllib3.connectionpool.ConnectionPool`. If ``pool_kwargs``\n        is provided, it is used instead. Note that if a new pool does not\n        need to be created for the request, the provided ``pool_kwargs`` are\n        not used.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_recursive_difference(self, type):\n        ''''''\n        if type == 'intersect':\n            return [recursive_diff(item['old'], item['new']) for item in self._intersect]\n        elif type == 'added':\n            return [recursive_diff({}, item) for item in self._added]\n        elif type == 'removed':\n            return [recursive_diff(item, {}, ignore_missing_keys=False)\n                    for item in self._removed]\n        elif type == 'all':\n            recursive_list = []\n            recursive_list.extend([recursive_diff(item['old'], item['new']) for item in self._intersect])\n            recursive_list.extend([recursive_diff({}, item) for item in self._added])\n            recursive_list.extend([recursive_diff(item, {},\n                                                  ignore_missing_keys=False)\n                                   for item in self._removed])\n            return recursive_list\n        else:\n            raise ValueError('The given type for recursive list matching '\n                             'is not supported.')", "output": "Returns the recursive diff between dict values", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def json2paramater(x, is_rand, random_state, oldy=None, Rand=False, name=NodeType.Root.value):\n    \"\"\"\n    \"\"\"\n    if isinstance(x, dict):\n        if NodeType.Type.value in x.keys():\n            _type = x[NodeType.Type.value]\n            _value = x[NodeType.Value.value]\n            name = name + '-' + _type\n            Rand |= is_rand[name]\n            if Rand is True:\n                if _type == 'choice':\n                    _index = random_state.randint(len(_value))\n                    y = {\n                        NodeType.Index.value: _index,\n                        NodeType.Value.value: json2paramater(x[NodeType.Value.value][_index],\n                                                             is_rand,\n                                                             random_state,\n                                                             None,\n                                                             Rand,\n                                                             name=name+\"[%d]\" % _index)\n                    }\n                else:\n                    y = eval('parameter_expressions.' +\n                             _type)(*(_value + [random_state]))\n            else:\n                y = copy.deepcopy(oldy)\n        else:\n            y = dict()\n            for key in x.keys():\n                y[key] = json2paramater(x[key], is_rand, random_state, oldy[key]\n                                        if oldy != None else None, Rand, name + \"[%s]\" % str(key))\n    elif isinstance(x, list):\n        y = list()\n        for i, x_i in enumerate(x):\n            y.append(json2paramater(x_i, is_rand, random_state, oldy[i]\n                                    if oldy != None else None, Rand, name + \"[%d]\" % i))\n    else:\n        y = copy.deepcopy(x)\n    return y", "output": "Json to pramaters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _replace_locals(tok):\n    \"\"\"\n    \"\"\"\n    toknum, tokval = tok\n    if toknum == tokenize.OP and tokval == '@':\n        return tokenize.OP, _LOCAL_TAG\n    return toknum, tokval", "output": "Replace local variables with a syntactically valid name.\n\n    Parameters\n    ----------\n    tok : tuple of int, str\n        ints correspond to the all caps constants in the tokenize module\n\n    Returns\n    -------\n    t : tuple of int, str\n        Either the input or token or the replacement values\n\n    Notes\n    -----\n    This is somewhat of a hack in that we rewrite a string such as ``'@a'`` as\n    ``'__pd_eval_local_a'`` by telling the tokenizer that ``__pd_eval_local_``\n    is a ``tokenize.OP`` and to replace the ``'@'`` symbol with it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_user_sign_up(name, password, client):\n    \"\"\"\n    \"\"\"\n\n    coll = client.user\n    if (coll.find({'username': name}).count() > 0):\n        print(name)\n        QA_util_log_info('user name is already exist')\n        return False\n    else:\n        return True", "output": "\u53ea\u505acheck! \u5177\u4f53\u903b\u8f91\u9700\u8981\u5728\u81ea\u5df1\u7684\u51fd\u6570\u4e2d\u5b9e\u73b0\n\n    \u53c2\u89c1:QAWEBSERVER\u4e2d\u7684\u5b9e\u73b0\n    \n    Arguments:\n        name {[type]} -- [description]\n        password {[type]} -- [description]\n        client {[type]} -- [description]\n    \n    Returns:\n        [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self, **kwargs):\n        \n        \"\"\"\n        for key, value in kwargs.items():\n            try:\n                is_property = isinstance(getattr(self.__class__, key), property)\n            except AttributeError:\n                continue\n\n            if is_property:\n                setattr(self, key, value)", "output": "r\"\"\"Bulk updates this permission object.\n\n        Allows you to set multiple attributes by using keyword\n        arguments. The names must be equivalent to the properties\n        listed. Extraneous key/value pairs will be silently ignored.\n\n        Parameters\n        ------------\n        \\*\\*kwargs\n            A list of key/value pairs to bulk update permissions with.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, id, param):\n        \"\"\"  \"\"\"\n        assert isinstance(id, basestring)\n        assert isinstance(param, basestring)\n        return self.params_.get(param, {}).get(id)", "output": "Returns the value of a configuration parameter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def collect_s3(self):\n        \"\"\"  \"\"\"\n        print('Collecting artifacts matching tag/sha %s from S3 bucket %s' % (self.gitref, s3_bucket))\n        self.s3 = boto3.resource('s3')\n        self.s3_bucket = self.s3.Bucket(s3_bucket)\n        self.s3.meta.client.head_bucket(Bucket=s3_bucket)\n        for key in self.s3_bucket.objects.all():\n            self.collect_single_s3(key.key)\n\n        for a in self.artifacts:\n            a.download(self.dlpath)", "output": "Collect and download build-artifacts from S3 based on git reference", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def common_params(task_instance, task_cls):\n    \"\"\"\n    \n    \"\"\"\n    if not isinstance(task_cls, task.Register):\n        raise TypeError(\"task_cls must be an uninstantiated Task\")\n\n    task_instance_param_names = dict(task_instance.get_params()).keys()\n    task_cls_params_dict = dict(task_cls.get_params())\n    task_cls_param_names = task_cls_params_dict.keys()\n    common_param_names = set(task_instance_param_names).intersection(set(task_cls_param_names))\n    common_param_vals = [(key, task_cls_params_dict[key]) for key in common_param_names]\n    common_kwargs = dict((key, task_instance.param_kwargs[key]) for key in common_param_names)\n    vals = dict(task_instance.get_param_values(common_param_vals, [], common_kwargs))\n    return vals", "output": "Grab all the values in task_instance that are found in task_cls.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def id2rel(self, xs):\n        \"\"\"\n        \"\"\"\n        if isinstance(xs, list):\n            return [self._id2rel[x] for x in xs]\n        return self._id2rel[xs]", "output": "Map id(s) to relation(s)\n\n        Parameters\n        ----------\n        xs : int\n            id or a list of ids\n\n        Returns\n        -------\n        str or list\n            relation or a list of relations", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def end_at(self, document_fields):\n        \"\"\"\n        \"\"\"\n        return self._cursor_helper(document_fields, before=False, start=False)", "output": "End query results at a particular document value.\n\n        The result set will **include** the document specified by\n        ``document_fields``.\n\n        If the current query already has specified an end cursor -- either\n        via this method or\n        :meth:`~.firestore_v1beta1.query.Query.end_before` -- this will\n        overwrite it.\n\n        When the query is sent to the server, the ``document_fields`` will\n        be used in the order given by fields set by\n        :meth:`~.firestore_v1beta1.query.Query.order_by`.\n\n        Args:\n            document_fields (Union[~.firestore_v1beta1.\\\n                document.DocumentSnapshot, dict, list, tuple]): a document\n                snapshot or a dictionary/list/tuple of fields representing a\n                query results cursor. A cursor is a collection of values that\n                represent a position in a query result set.\n\n        Returns:\n            ~.firestore_v1beta1.query.Query: A query with cursor. Acts as\n            a copy of the current query, modified with the newly added\n            \"end at\" cursor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_pending_file_rename():\n    '''\n    \n    '''\n    vnames = ('PendingFileRenameOperations', 'PendingFileRenameOperations2')\n    key = r'SYSTEM\\CurrentControlSet\\Control\\Session Manager'\n\n    # If any of the value names exist and have value data set,\n    # then a reboot is pending.\n\n    for vname in vnames:\n        reg_ret = __utils__['reg.read_value']('HKLM', key, vname)\n\n        if reg_ret['success']:\n            log.debug('Found key: %s', key)\n\n            if reg_ret['vdata'] and (reg_ret['vdata'] != '(value not set)'):\n                return True\n        else:\n            log.debug('Unable to access key: %s', key)\n    return False", "output": "Determine whether there are pending file rename operations that require a\n    reboot.\n\n    .. versionadded:: 2016.11.0\n\n    Returns:\n        bool: ``True`` if there are pending file rename operations, otherwise\n        ``False``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' system.get_pending_file_rename", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop(self):\n        \"\"\"\"\"\"\n        with self._operational_lock:\n            self._bidi_rpc.close()\n\n            if self._thread is not None:\n                # Resume the thread to wake it up in case it is sleeping.\n                self.resume()\n                self._thread.join()\n\n            self._thread = None", "output": "Stop consuming the stream and shutdown the background thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_host(self, url: URL) -> None:\n        \"\"\"\"\"\"\n        # get host/port\n        if not url.host:\n            raise InvalidURL(url)\n\n        # basic auth info\n        username, password = url.user, url.password\n        if username:\n            self.auth = helpers.BasicAuth(username, password or '')", "output": "Update destination host, port and connection type (ssl).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rename(self, path, raise_if_exists=False):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(path, HdfsTarget):\n            path = path.path\n        if raise_if_exists and self.fs.exists(path):\n            raise RuntimeError('Destination exists: %s' % path)\n        self.fs.rename(self.path, path)", "output": "Does not change self.path.\n\n        Unlike ``move_dir()``, ``rename()`` might cause nested directories.\n        See spotify/luigi#522", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def padded_neg_log_perplexity(predictions,\n                              labels,\n                              weights_fn=common_layers.weights_nonzero):\n  \"\"\"\"\"\"\n  num, den = common_layers.padded_cross_entropy(\n      predictions, labels, 0.0, weights_fn=weights_fn, reduce_sum=False)\n  return (-num, den)", "output": "Average log-perplexity exluding padding 0s. No smoothing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_sensor_data(**kwargs):\n    '''\n    \n    '''\n    import ast\n    with _IpmiCommand(**kwargs) as s:\n        data = {}\n        for reading in s.get_sensor_data():\n            if reading:\n                r = ast.literal_eval(repr(reading))\n                data[r.pop('name')] = r\n    return data", "output": "Get sensor readings\n\n    Iterates sensor reading objects\n\n    :param kwargs:\n        - api_host=127.0.0.1\n        - api_user=admin\n        - api_pass=example\n        - api_port=623\n        - api_kg=None\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call ipmi.get_sensor_data api_host=127.0.0.1 api_user=admin api_pass=pass", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_flags():\n  \"\"\"\"\"\"\n  assert not job_dir()\n  assert FLAGS.output_dir.startswith(\"gs://\")\n  assert FLAGS.data_dir.startswith(\"gs://\")\n  assert FLAGS.worker_replicas <= 1\n  assert FLAGS.ps_replicas <= 0\n  if FLAGS.hparams_range:\n    assert FLAGS.autotune_objective\n  if FLAGS.worker_gpu:\n    assert FLAGS.worker_gpu in [1, 4, 8]\n  if FLAGS.cloud_mlengine_master_type:\n    if FLAGS.worker_gpu:\n      if FLAGS.worker_gpu == 1:\n        assert FLAGS.cloud_mlengine_master_type in [\"standard_gpu\",\n                                                    \"standard_p100\"]\n      elif FLAGS.worker_gpu == 4:\n        assert FLAGS.cloud_mlengine_master_type in [\"complex_model_m_gpu\",\n                                                    \"complex_model_m_p100\"]\n      else:\n        assert FLAGS.cloud_mlengine_master_type == \"complex_model_l_gpu\"\n    else:\n      assert FLAGS.cloud_mlengine_master_type in [\"standard\", \"large_model\",\n                                                  \"complex_model_s\",\n                                                  \"complex_model_m\",\n                                                  \"complex_model_l\"]", "output": "Validates flags are set to acceptable values for CloudML Engine runs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read_hdf_columns(path_or_buf, columns, num_splits, kwargs):  # pragma: no cover\n    \"\"\"\n    \"\"\"\n\n    df = pandas.read_hdf(path_or_buf, columns=columns, **kwargs)\n    # Append the length of the index here to build it externally\n    return _split_result_for_readers(0, num_splits, df) + [len(df.index)]", "output": "Use a Ray task to read columns from HDF5 into a Pandas DataFrame.\n\n    Note: Ray functions are not detected by codecov (thus pragma: no cover)\n\n    Args:\n        path_or_buf: The path of the HDF5 file.\n        columns: The list of column names to read.\n        num_splits: The number of partitions to split the column into.\n\n    Returns:\n         A list containing the split Pandas DataFrames and the Index as the last\n            element. If there is not `index_col` set, then we just return the length.\n            This is used to determine the total length of the DataFrame to build a\n            default Index.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def try_one_generator_really (project, name, generator, target_type, properties, sources):\n    \"\"\" \n    \"\"\"\n    if __debug__:\n        from .targets import ProjectTarget\n        assert isinstance(project, ProjectTarget)\n        assert isinstance(name, basestring) or name is None\n        assert isinstance(generator, Generator)\n        assert isinstance(target_type, basestring)\n        assert isinstance(properties, property_set.PropertySet)\n        assert is_iterable_typed(sources, virtual_target.VirtualTarget)\n    targets = generator.run (project, name, properties, sources)\n\n    usage_requirements = []\n    success = False\n\n    dout(\"returned \" + str(targets))\n\n    if targets:\n        success = True;\n\n        if isinstance (targets[0], property_set.PropertySet):\n            usage_requirements = targets [0]\n            targets = targets [1]\n\n        else:\n            usage_requirements = property_set.empty ()\n\n    dout(  \"  generator\" + generator.id() + \" spawned \")\n    #    generators.dout [ indent ] \" \" $(targets) ;\n#    if $(usage-requirements)\n#    {\n#        generators.dout [ indent ] \"  with usage requirements:\" $(x) ;\n#    }\n\n    if success:\n        return (usage_requirements, targets)\n    else:\n        return None", "output": "Returns usage requirements + list of created targets.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy_image(source_region, image_id, name, profile, description=None, **libcloud_kwargs):\n    '''\n    \n    '''\n    conn = _get_driver(profile=profile)\n    libcloud_kwargs = salt.utils.args.clean_kwargs(**libcloud_kwargs)\n    image = conn.get_image(image_id, **libcloud_kwargs)\n    new_image = conn.copy_image(source_region, image, name,\n                                description=description, **libcloud_kwargs)\n    return _simple_image(new_image)", "output": "Copies an image from a source region to the current region.\n\n    :param source_region: Region to copy the node from.\n    :type source_region: ``str``\n\n    :param image_id: Image to copy.\n    :type image_id: ``str``\n\n    :param name: name for new image.\n    :type name: ``str``\n\n    :param profile: The profile key\n    :type  profile: ``str``\n\n    :param description: description for new image.\n    :type name: ``str``\n\n    :param libcloud_kwargs: Extra arguments for the driver's copy_image method\n    :type  libcloud_kwargs: ``dict``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion libcloud_compute.copy_image us-east1 image1 'new image' profile1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deployment_delete(name, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    result = False\n    resconn = __utils__['azurearm.get_client']('resource', **kwargs)\n    try:\n        deploy = resconn.deployments.delete(\n            deployment_name=name,\n            resource_group_name=resource_group\n        )\n        deploy.wait()\n        result = True\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('resource', str(exc), **kwargs)\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Delete a deployment.\n\n    :param name: The name of the deployment to delete.\n\n    :param resource_group: The resource group name assigned to the\n        deployment.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_resource.deployment_delete testdeploy testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _do(name, fun, path=None):\n    '''\n    \n    '''\n    host = find_guest(name, quiet=True, path=path)\n    if not host:\n        return False\n\n    client = salt.client.get_local_client(__opts__['conf_file'])\n    cmd_ret = client.cmd_iter(\n            host,\n            'lxc.{0}'.format(fun),\n            [name],\n            kwarg={'path': path},\n            timeout=60)\n    data = next(cmd_ret)\n    data = data.get(host, {}).get('ret', None)\n    if data:\n        data = {host: data}\n    return data", "output": "Invoke a function in the lxc module with no args\n\n    path\n        path to the container parent\n        default: /var/lib/lxc (system default)\n\n        .. versionadded:: 2015.8.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_relative():\n  \"\"\"\"\"\"\n  hparams = transformer_base()\n  hparams.pos = None\n  hparams.self_attention_type = \"dot_product_relative\"\n  hparams.max_relative_position = 20\n  return hparams", "output": "Use relative position embeddings instead of absolute position encodings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear():\n    \"\"\"\n    \"\"\"\n    if not isatty(sys.stdout):\n        return\n    # If we're on Windows and we don't have colorama available, then we\n    # clear the screen by shelling out.  Otherwise we can use an escape\n    # sequence.\n    if WIN:\n        os.system('cls')\n    else:\n        sys.stdout.write('\\033[2J\\033[1;1H')", "output": "Clears the terminal screen.  This will have the effect of clearing\n    the whole visible space of the terminal and moving the cursor to the\n    top left.  This does not do anything if not connected to a terminal.\n\n    .. versionadded:: 2.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_vhosts(runas=None):\n    '''\n    \n    '''\n    if runas is None and not salt.utils.platform.is_windows():\n        runas = salt.utils.user.get_user()\n    res = __salt__['cmd.run_all'](\n        [RABBITMQCTL, 'list_vhosts', '-q'],\n        reset_system_locale=False,\n        runas=runas,\n        python_shell=False)\n    _check_response(res)\n    return _output_to_list(res['stdout'])", "output": "Return a list of vhost based on rabbitmqctl list_vhosts.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rabbitmq.list_vhosts", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hook_outputs(modules:Collection[nn.Module], detach:bool=True, grad:bool=False)->Hooks:\n    \"\"\n    return Hooks(modules, _hook_inner, detach=detach, is_forward=not grad)", "output": "Return `Hooks` that store activations of all `modules` in `self.stored`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_private_key_file(self, filename, key, format, password=None):\n        \"\"\"\n        \n        \"\"\"\n        with open(filename, \"w\") as f:\n            os.chmod(filename, o600)\n            self._write_private_key(f, key, format, password=password)", "output": "Write an SSH2-format private key file in a form that can be read by\n        paramiko or openssh.  If no password is given, the key is written in\n        a trivially-encoded format (base64) which is completely insecure.  If\n        a password is given, DES-EDE3-CBC is used.\n\n        :param str tag:\n            ``\"RSA\"`` or ``\"DSA\"``, the tag used to mark the data block.\n        :param filename: name of the file to write.\n        :param str data: data blob that makes up the private key.\n        :param str password: an optional password to use to encrypt the file.\n\n        :raises: ``IOError`` -- if there was an error writing the file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def trading_pnl(self):\n        \"\"\"\n        \n        \"\"\"\n        last_price = self._data_proxy.get_last_price(self._order_book_id)\n        return self._contract_multiplier * (self._trade_quantity * last_price - self._trade_cost)", "output": "[float] \u4ea4\u6613\u76c8\u4e8f\uff0c\u7b56\u7565\u5728\u5f53\u524d\u4ea4\u6613\u65e5\u4ea7\u751f\u7684\u76c8\u4e8f\u4e2d\u6765\u6e90\u4e8e\u5f53\u65e5\u6210\u4ea4\u7684\u90e8\u5206", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _query_pageant(msg):\n    \"\"\"\n    \n    \"\"\"\n    hwnd = _get_pageant_window_object()\n    if not hwnd:\n        # Raise a failure to connect exception, pageant isn't running anymore!\n        return None\n\n    # create a name for the mmap\n    map_name = \"PageantRequest%08x\" % thread.get_ident()\n\n    pymap = _winapi.MemoryMap(\n        map_name, _AGENT_MAX_MSGLEN, _winapi.get_security_attributes_for_user()\n    )\n    with pymap:\n        pymap.write(msg)\n        # Create an array buffer containing the mapped filename\n        char_buffer = array.array(\"b\", b(map_name) + zero_byte)  # noqa\n        char_buffer_address, char_buffer_size = char_buffer.buffer_info()\n        # Create a string to use for the SendMessage function call\n        cds = COPYDATASTRUCT(\n            _AGENT_COPYDATA_ID, char_buffer_size, char_buffer_address\n        )\n\n        response = ctypes.windll.user32.SendMessageA(\n            hwnd, win32con_WM_COPYDATA, ctypes.sizeof(cds), ctypes.byref(cds)\n        )\n\n        if response > 0:\n            pymap.seek(0)\n            datalen = pymap.read(4)\n            retlen = struct.unpack(\">I\", datalen)[0]\n            return datalen + pymap.read(retlen)\n        return None", "output": "Communication with the Pageant process is done through a shared\n    memory-mapped file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wider_next_dense(layer, start_dim, total_dim, n_add, weighted=True):\n    '''\n    '''\n    if not weighted:\n        return StubDense(layer.input_units + n_add, layer.units)\n    teacher_w, teacher_b = layer.get_weights()\n    student_w = teacher_w.copy()\n    n_units_each_channel = int(teacher_w.shape[1] / total_dim)\n\n    new_weight = np.zeros((teacher_w.shape[0], n_add * n_units_each_channel))\n    student_w = np.concatenate(\n        (\n            student_w[:, : start_dim * n_units_each_channel],\n            add_noise(new_weight, student_w),\n            student_w[\n                :, start_dim * n_units_each_channel : total_dim * n_units_each_channel\n            ],\n        ),\n        axis=1,\n    )\n\n    new_layer = StubDense(layer.input_units + n_add, layer.units)\n    new_layer.set_weights((student_w, teacher_b))\n    return new_layer", "output": "wider next dense layer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_dict(self, label, pred):\n        \"\"\"\n        \"\"\"\n        if self.output_names is not None:\n            pred = [pred[name] for name in self.output_names]\n        else:\n            pred = list(pred.values())\n\n        if self.label_names is not None:\n            label = [label[name] for name in self.label_names]\n        else:\n            label = list(label.values())\n\n        self.update(label, pred)", "output": "Update the internal evaluation with named label and pred\n\n        Parameters\n        ----------\n        labels : OrderedDict of str -> NDArray\n            name to array mapping for labels.\n\n        preds : OrderedDict of str -> NDArray\n            name to array mapping of predicted outputs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_context(center_idx, sentence_boundaries, window_size,\n                 random_window_size, seed):\n    \"\"\"\n\n    \"\"\"\n    random.seed(seed + center_idx)\n\n    sentence_index = np.searchsorted(sentence_boundaries, center_idx)\n    sentence_start, sentence_end = _get_sentence_start_end(\n        sentence_boundaries, sentence_index)\n\n    if random_window_size:\n        window_size = random.randint(1, window_size)\n    start_idx = max(sentence_start, center_idx - window_size)\n    end_idx = min(sentence_end, center_idx + window_size + 1)\n\n    if start_idx != center_idx and center_idx + 1 != end_idx:\n        context = np.concatenate((np.arange(start_idx, center_idx),\n                                  np.arange(center_idx + 1, end_idx)))\n    elif start_idx != center_idx:\n        context = np.arange(start_idx, center_idx)\n    elif center_idx + 1 != end_idx:\n        context = np.arange(center_idx + 1, end_idx)\n    else:\n        context = None\n\n    return context", "output": "Compute the context with respect to a center word in a sentence.\n\n    Takes an numpy array of sentences boundaries.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _servicegroup_get_servers(sg_name, **connection_args):\n    '''\n    \n    '''\n    nitro = _connect(**connection_args)\n    if nitro is None:\n        return None\n    sg = NSServiceGroup()\n    sg.set_servicegroupname(sg_name)\n    try:\n        sg = NSServiceGroup.get_servers(nitro, sg)\n    except NSNitroError as error:\n        log.debug('netscaler module error - NSServiceGroup.get_servers failed(): %s', error)\n        sg = None\n    _disconnect(nitro)\n    return sg", "output": "Returns a list of members of a servicegroup or None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sell_close_order_quantity(self):\n        \"\"\"\n        \n        \"\"\"\n        return sum(order.unfilled_quantity for order in self.open_orders if order.side == SIDE.SELL and\n                   order.position_effect in [POSITION_EFFECT.CLOSE, POSITION_EFFECT.CLOSE_TODAY])", "output": "[int] \u5356\u65b9\u5411\u6302\u5355\u91cf", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _kcpassword(password):\n    '''\n    \n    '''\n    # The magic 11 bytes - these are just repeated\n    # 0x7D 0x89 0x52 0x23 0xD2 0xBC 0xDD 0xEA 0xA3 0xB9 0x1F\n    key = [125, 137, 82, 35, 210, 188, 221, 234, 163, 185, 31]\n    key_len = len(key)\n\n    # Convert each character to a byte\n    password = list(map(ord, password))\n\n    # pad password length out to an even multiple of key length\n    remainder = len(password) % key_len\n    if remainder > 0:\n        password = password + [0] * (key_len - remainder)\n\n    # Break the password into chunks the size of len(key) (11)\n    for chunk_index in range(0, len(password), len(key)):\n        # Reset the key_index to 0 for each iteration\n        key_index = 0\n\n        # Do an XOR on each character of that chunk of the password with the\n        # corresponding item in the key\n        # The length of the password, or the length of the key, whichever is\n        # smaller\n        for password_index in range(chunk_index,\n                                    min(chunk_index + len(key), len(password))):\n            password[password_index] = password[password_index] ^ key[key_index]\n            key_index += 1\n\n    # Convert each byte back to a character\n    password = list(map(chr, password))\n    return b''.join(salt.utils.data.encode(password))", "output": "Internal function for obfuscating the password used for AutoLogin\n    This is later written as the contents of the ``/etc/kcpassword`` file\n\n    .. versionadded:: 2017.7.3\n\n    Adapted from:\n    https://github.com/timsutton/osx-vm-templates/blob/master/scripts/support/set_kcpassword.py\n\n    Args:\n\n        password(str):\n            The password to obfuscate\n\n    Returns:\n        str: The obfuscated password", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cluster_remove(version,\n                   name='main',\n                   stop=False):\n    '''\n    \n\n    '''\n    cmd = [salt.utils.path.which('pg_dropcluster')]\n    if stop:\n        cmd += ['--stop']\n    cmd += [version, name]\n    cmdstr = ' '.join([pipes.quote(c) for c in cmd])\n    ret = __salt__['cmd.run_all'](cmdstr, python_shell=False)\n    # FIXME - return Boolean ?\n    if ret.get('retcode', 0) != 0:\n        log.error('Error removing a Postgresql cluster %s/%s', version, name)\n    else:\n        ret['changes'] = ('Successfully removed'\n                          ' cluster {0}/{1}').format(version, name)\n    return ret", "output": "Remove a cluster on a Postgres server. By default it doesn't try\n    to stop the cluster.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' postgres.cluster_remove '9.3'\n\n        salt '*' postgres.cluster_remove '9.3' 'main'\n\n        salt '*' postgres.cluster_remove '9.3' 'main' stop=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_svc_avail_path(path):\n    '''\n    \n    '''\n    if os.path.exists(path):\n        if path not in AVAIL_SVR_DIRS:\n            AVAIL_SVR_DIRS.append(path)\n        return True\n    return False", "output": "Add a path that may contain available services.\n    Return ``True`` if added (or already present), ``False`` on error.\n\n    path\n        directory to add to AVAIL_SVR_DIRS", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toggle_wrap_mode(self, checked):\r\n        \"\"\"\"\"\"\r\n        self.plain_text.editor.toggle_wrap_mode(checked)\r\n        self.set_option('wrap', checked)", "output": "Toggle wrap mode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_firefox_start_cmd(self):\n        \"\"\"\"\"\"\n        start_cmd = \"\"\n        if platform.system() == \"Darwin\":\n            start_cmd = \"/Applications/Firefox.app/Contents/MacOS/firefox-bin\"\n            # fallback to homebrew installation for mac users\n            if not os.path.exists(start_cmd):\n                start_cmd = os.path.expanduser(\"~\") + start_cmd\n        elif platform.system() == \"Windows\":\n            start_cmd = (self._find_exe_in_registry() or self._default_windows_location())\n        elif platform.system() == 'Java' and os._name == 'nt':\n            start_cmd = self._default_windows_location()\n        else:\n            for ffname in [\"firefox\", \"iceweasel\"]:\n                start_cmd = self.which(ffname)\n                if start_cmd is not None:\n                    break\n            else:\n                # couldn't find firefox on the system path\n                raise RuntimeError(\n                    \"Could not find firefox in your system PATH.\" +\n                    \" Please specify the firefox binary location or install firefox\")\n        return start_cmd", "output": "Return the command to start firefox.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render_arrow(self, label, start, end, direction, i):\n        \"\"\"\n        \"\"\"\n        level = self.levels.index(end - start) + 1\n        x_start = self.offset_x + start * self.distance + self.arrow_spacing\n        if self.direction == \"rtl\":\n            x_start = self.width - x_start\n        y = self.offset_y\n        x_end = (\n            self.offset_x\n            + (end - start) * self.distance\n            + start * self.distance\n            - self.arrow_spacing * (self.highest_level - level) / 4\n        )\n        if self.direction == \"rtl\":\n            x_end = self.width - x_end\n        y_curve = self.offset_y - level * self.distance / 2\n        if self.compact:\n            y_curve = self.offset_y - level * self.distance / 6\n        if y_curve == 0 and len(self.levels) > 5:\n            y_curve = -self.distance\n        arrowhead = self.get_arrowhead(direction, x_start, y, x_end)\n        arc = self.get_arc(x_start, y, y_curve, x_end)\n        label_side = \"right\" if self.direction == \"rtl\" else \"left\"\n        return TPL_DEP_ARCS.format(\n            id=self.id,\n            i=i,\n            stroke=self.arrow_stroke,\n            head=arrowhead,\n            label=label,\n            label_side=label_side,\n            arc=arc,\n        )", "output": "Render individual arrow.\n\n        label (unicode): Dependency label.\n        start (int): Index of start word.\n        end (int): Index of end word.\n        direction (unicode): Arrow direction, 'left' or 'right'.\n        i (int): Unique ID, typically arrow index.\n        RETURNS (unicode): Rendered SVG markup.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _collapse_addresses_internal(addresses):\n    \"\"\"\n\n    \"\"\"\n    # First merge\n    to_merge = list(addresses)\n    subnets = {}\n    while to_merge:\n        net = to_merge.pop()\n        supernet = net.supernet()\n        existing = subnets.get(supernet)\n        if existing is None:\n            subnets[supernet] = net\n        elif existing != net:\n            # Merge consecutive subnets\n            del subnets[supernet]\n            to_merge.append(supernet)\n    # Then iterate over resulting networks, skipping subsumed subnets\n    last = None\n    for net in sorted(subnets.values()):\n        if last is not None:\n            # Since they are sorted,\n            # last.network_address <= net.network_address is a given.\n            if last.broadcast_address >= net.broadcast_address:\n                continue\n        yield net\n        last = net", "output": "Loops through the addresses, collapsing concurrent netblocks.\n\n    Example:\n\n        ip1 = IPv4Network('192.0.2.0/26')\n        ip2 = IPv4Network('192.0.2.64/26')\n        ip3 = IPv4Network('192.0.2.128/26')\n        ip4 = IPv4Network('192.0.2.192/26')\n\n        _collapse_addresses_internal([ip1, ip2, ip3, ip4]) ->\n          [IPv4Network('192.0.2.0/24')]\n\n        This shouldn't be called directly; it is called via\n          collapse_addresses([]).\n\n    Args:\n        addresses: A list of IPv4Network's or IPv6Network's\n\n    Returns:\n        A list of IPv4Network's or IPv6Network's depending on what we were\n        passed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_running(fun):\n    '''\n    \n    '''\n    run = running()\n    ret = []\n    for data in run:\n        if fnmatch.fnmatch(data.get('fun', ''), fun):\n            ret.append(data)\n    return ret", "output": "If the named function is running return the data associated with it/them.\n    The argument can be a glob\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.is_running state.highstate", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keypoint_scale(keypoint, scale_x, scale_y, **params):\n    \"\"\"\"\"\"\n    x, y, a, s = keypoint\n    return [x * scale_x, y * scale_y, a, s * max(scale_x, scale_y)]", "output": "Scales a keypoint by scale_x and scale_y.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(key, default=''):\n    '''\n    \n    '''\n\n    # Determine formula namespace from query\n    if ':' in key:\n        namespace, key = key.split(':', 1)\n    else:\n        namespace, key = key, None\n\n    # Fetch and load defaults formula files from states.\n    defaults = _load(namespace)\n\n    # Fetch value\n    if key:\n        return salt.utils.data.traverse_dict_and_list(defaults, key, default)\n    else:\n        return defaults", "output": "defaults.get is used much like pillar.get except that it will read\n    a default value for a pillar from defaults.json or defaults.yaml\n    files that are stored in the root of a salt formula.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' defaults.get core:users:root\n\n    The defaults is computed from pillar key. The first entry is considered as\n    the formula namespace.\n\n    For example, querying ``core:users:root`` will try to load\n    ``salt://core/defaults.yaml`` and ``salt://core/defaults.json``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_vcenter_version(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The get_vcenter_version function must be called with '\n            '-f or --function.'\n        )\n\n    # Get the inventory\n    inv = salt.utils.vmware.get_inventory(_get_si())\n\n    return inv.about.fullName", "output": "Show the vCenter Server version with build number.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f get_vcenter_version my-vmware-config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _conf(cls, opts):\n        \"\"\"\"\"\"\n        logging_conf = cls.config.get('core', 'logging_conf_file', None)\n        if logging_conf is None:\n            return False\n\n        if not os.path.exists(logging_conf):\n            # FileNotFoundError added only in Python 3.3\n            # https://docs.python.org/3/whatsnew/3.3.html#pep-3151-reworking-the-os-and-io-exception-hierarchy\n            raise OSError(\"Error: Unable to locate specified logging configuration file!\")\n\n        logging.config.fileConfig(logging_conf)\n        return True", "output": "Setup logging via ini-file from logging_conf_file option.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_dvportgroup_dict(pg_ref):\n    '''\n    \n    '''\n    props = salt.utils.vmware.get_properties_of_managed_object(\n        pg_ref, ['name', 'config.description', 'config.numPorts',\n                 'config.type', 'config.defaultPortConfig'])\n    pg_dict = {'name': props['name'],\n               'description': props.get('config.description'),\n               'num_ports': props['config.numPorts'],\n               'type': props['config.type']}\n    if props['config.defaultPortConfig']:\n        dpg = props['config.defaultPortConfig']\n        if dpg.vlan and \\\n           isinstance(dpg.vlan,\n                      vim.VmwareDistributedVirtualSwitchVlanIdSpec):\n\n            pg_dict.update({'vlan_id': dpg.vlan.vlanId})\n        pg_dict.update({'out_shaping':\n                        _get_dvportgroup_out_shaping(\n                            props['name'],\n                            props['config.defaultPortConfig'])})\n        pg_dict.update({'security_policy':\n                        _get_dvportgroup_security_policy(\n                            props['name'],\n                            props['config.defaultPortConfig'])})\n        pg_dict.update({'teaming':\n                        _get_dvportgroup_teaming(\n                            props['name'],\n                            props['config.defaultPortConfig'])})\n    return pg_dict", "output": "Returns a dictionary with a distributed virutal portgroup data\n\n\n    pg_ref\n        Portgroup reference", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resume(vm_):\n    '''\n    \n    '''\n    with _get_xapi_session() as xapi:\n        vm_uuid = _get_label_uuid(xapi, 'VM', vm_)\n        if vm_uuid is False:\n            return False\n        try:\n            xapi.VM.unpause(vm_uuid)\n            return True\n        except Exception:\n            return False", "output": "Resume the named vm\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' virt.resume <vm name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_partition_name(cls, region=None):\n        \"\"\"\n        \n        \"\"\"\n\n        if region is None:\n            # Use Boto3 to get the region where code is running. This uses Boto's regular region resolution\n            # mechanism, starting from AWS_DEFAULT_REGION environment variable.\n            region = boto3.session.Session().region_name\n\n        region_string = region.lower()\n        if region_string.startswith(\"cn-\"):\n            return \"aws-cn\"\n        elif region_string.startswith(\"us-gov\"):\n            return \"aws-us-gov\"\n        else:\n            return \"aws\"", "output": "Gets the name of the partition given the region name. If region name is not provided, this method will\n        use Boto3 to get name of the region where this code is running.\n\n        This implementation is borrowed from AWS CLI\n        https://github.com/aws/aws-cli/blob/1.11.139/awscli/customizations/emr/createdefaultroles.py#L59\n\n        :param region: Optional name of the region\n        :return: Partition name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enable_pretty_logging(options: Any = None, logger: logging.Logger = None) -> None:\n    \"\"\"\n    \"\"\"\n    if options is None:\n        import tornado.options\n\n        options = tornado.options.options\n    if options.logging is None or options.logging.lower() == \"none\":\n        return\n    if logger is None:\n        logger = logging.getLogger()\n    logger.setLevel(getattr(logging, options.logging.upper()))\n    if options.log_file_prefix:\n        rotate_mode = options.log_rotate_mode\n        if rotate_mode == \"size\":\n            channel = logging.handlers.RotatingFileHandler(\n                filename=options.log_file_prefix,\n                maxBytes=options.log_file_max_size,\n                backupCount=options.log_file_num_backups,\n                encoding=\"utf-8\",\n            )  # type: logging.Handler\n        elif rotate_mode == \"time\":\n            channel = logging.handlers.TimedRotatingFileHandler(\n                filename=options.log_file_prefix,\n                when=options.log_rotate_when,\n                interval=options.log_rotate_interval,\n                backupCount=options.log_file_num_backups,\n                encoding=\"utf-8\",\n            )\n        else:\n            error_message = (\n                \"The value of log_rotate_mode option should be \"\n                + '\"size\" or \"time\", not \"%s\".' % rotate_mode\n            )\n            raise ValueError(error_message)\n        channel.setFormatter(LogFormatter(color=False))\n        logger.addHandler(channel)\n\n    if options.log_to_stderr or (options.log_to_stderr is None and not logger.handlers):\n        # Set up color if we are in a tty and curses is installed\n        channel = logging.StreamHandler()\n        channel.setFormatter(LogFormatter())\n        logger.addHandler(channel)", "output": "Turns on formatted logging output as configured.\n\n    This is called automatically by `tornado.options.parse_command_line`\n    and `tornado.options.parse_config_file`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _should_split_cell(cls, cell_text: str) -> bool:\n        \"\"\"\n        \n        \"\"\"\n        if ', ' in cell_text or '\\n' in cell_text or '/' in cell_text:\n            return True\n        return False", "output": "Checks whether the cell should be split.  We're just doing the same thing that SEMPRE did\n        here.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def settle(self, settle_data = None):\n        \"\"\"\n        \n\n        \"\"\"\n        #print('FROM QUANTAXIS QA_ACCOUNT: account settle')\n        if self.running_environment == RUNNING_ENVIRONMENT.TZERO and self.hold_available.sum(\n        ) != 0:\n            raise RuntimeError(\n                'QAACCOUNT: \u8be5T0\u8d26\u6237\u672a\u5f53\u65e5\u4ed3\u4f4d,\u8bf7\u5e73\u4ed3 {}'.format(\n                    self.hold_available.to_dict()\n                )\n            )\n        if self.market_type == MARKET_TYPE.FUTURE_CN:\n            # \u589e\u52a0\u9010\u65e5\u76ef\u5e02\u5236\u5ea6\n\n            self.static_balance['frozen'].append(\n                sum(\n                    [\n                        rx['money'] * rx['amount']\n                        for var in self.frozen.values()\n                        for rx in var.values()\n                    ]\n                )\n            )\n\n            self.static_balance['cash'].append(self.cash[-1])\n            self.static_balance['hold'].append(self.hold.to_dict())\n            self.static_balance['date'].append(self.date)\n            \"\"\"\u9759\u6001\u6743\u76ca\u7684\u7ed3\u7b97\n\n            \u53ea\u5173\u5fc3\u5f00\u4ed3\u4ef7/ \u4e0d\u505a\u76ef\u5e02\u5236\u5ea6\n\n            \u52a8\u6001\u6743\u76ca\u7684\u7ed3\u7b97\u9700\u8981\u5173\u5fc3\n\n            \"\"\"\n\n            self.static_balance['static_assets'].append(\n                self.static_balance['cash'][-1] +\n                self.static_balance['frozen'][-1]\n            )\n\n        self.sell_available = self.hold\n        self.buy_available = self.hold\n        self.cash_available = self.cash[-1]\n        self.datetime = '{} 09:30:00'.format(\n            QA_util_get_next_day(self.date)\n        ) if self.date is not None else None", "output": "\u80a1\u7968/\u671f\u8d27\u7684\u65e5\u7ed3\u7b97\n\n        \u80a1\u7968\u7684\u7ed3\u7b97:  \u7ed3\u8f6c\u80a1\u7968\u53ef\u5356\u989d\u5ea6\n        T0\u7684\u7ed3\u7b97: \u7ed3\u8f6cT0\u7684\u989d\u5ea6\n\n        \u671f\u8d27\u7684\u7ed3\u7b97: \u7ed3\u8f6c\u9759\u6001\u8d44\u91d1\n\n\n        @2019-02-25 yutiansut\n        hold \u5728\u4e0b\u9762\u8981\u8fdb\u884c\u5927\u53d8\u5316:\n\n        \u4ece \u53ea\u8ba1\u7b97\u6570\u91cf ==> \u6570\u91cf+\u6210\u672c+\u4e70\u5165\u4ef7 (\u643a\u5e26\u66f4\u591a\u4fe1\u606f)\n\n        \u57fa\u4e8ehistory\u53bb\u8ba1\u7b97hold ==> last_settle+ today_pos_change", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_timedeltalike_scalar(self, other):\n        \"\"\"\n        \n        \"\"\"\n        if isna(other):\n            # i.e np.timedelta64(\"NaT\"), not recognized by delta_to_nanoseconds\n            new_values = np.empty(len(self), dtype='i8')\n            new_values[:] = iNaT\n            return new_values\n\n        inc = delta_to_nanoseconds(other)\n        new_values = checked_add_with_arr(self.asi8, inc,\n                                          arr_mask=self._isnan).view('i8')\n        new_values = self._maybe_mask_results(new_values)\n        return new_values.view('i8')", "output": "Add a delta of a timedeltalike\n        return the i8 result view", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=portable_hash):\n        \"\"\"\n        \n        \"\"\"\n        def createZero():\n            return copy.deepcopy(zeroValue)\n\n        return self.combineByKey(lambda v: func(createZero(), v), func, func, numPartitions,\n                                 partitionFunc)", "output": "Merge the values for each key using an associative function \"func\"\n        and a neutral \"zeroValue\" which may be added to the result an\n        arbitrary number of times, and must not change the result\n        (e.g., 0 for addition, or 1 for multiplication.).\n\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n        >>> from operator import add\n        >>> sorted(rdd.foldByKey(0, add).collect())\n        [('a', 2), ('b', 1)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sendcontrol(self, char):\n        '''\n        '''\n        char = char.lower()\n        a = ord(char)\n        if 97 <= a <= 122:\n            a = a - ord('a') + 1\n            byte = _byte(a)\n            return self._writeb(byte), byte\n        d = {'@': 0, '`': 0,\n            '[': 27, '{': 27,\n            '\\\\': 28, '|': 28,\n            ']': 29, '}': 29,\n            '^': 30, '~': 30,\n            '_': 31,\n            '?': 127}\n        if char not in d:\n            return 0, b''\n\n        byte = _byte(d[char])\n        return self._writeb(byte), byte", "output": "Helper method that wraps send() with mnemonic access for sending control\n        character to the child (such as Ctrl-C or Ctrl-D).  For example, to send\n        Ctrl-G (ASCII 7, bell, '\\a')::\n\n            child.sendcontrol('g')\n\n        See also, sendintr() and sendeof().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_json(self, values_json):\n    \"\"\"\n    \"\"\"\n    values_map = json.loads(values_json)\n    return self.override_from_dict(values_map)", "output": "Override existing hyperparameter values, parsing new values from a json object.\n\n    Args:\n      values_json: String containing a json object of name:value pairs.\n\n    Returns:\n      The `HParams` instance.\n\n    Raises:\n      KeyError: If a hyperparameter in `values_json` doesn't exist.\n      ValueError: If `values_json` cannot be parsed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_alias(FunctionName, Name, FunctionVersion=None,\n                region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    args = {\n        'FunctionName': FunctionName\n    }\n    if FunctionVersion:\n        args['FunctionVersion'] = FunctionVersion\n\n    for aliases in __utils__['boto3.paged_call'](conn.list_aliases, **args):\n        for alias in aliases.get('Aliases'):\n            if alias['Name'] == Name:\n                return alias\n    return None", "output": "Given function name and alias name, find and return matching alias information.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_hparams_path():\n  \"\"\"\"\"\"\n  hparams_path = None\n  if FLAGS.output_dir:\n    hparams_path = os.path.join(FLAGS.output_dir, \"hparams.json\")\n  else:\n    tf.logging.warning(\n        \"--output_dir not specified. Hyper-parameters will be infered from\"\n        \"--hparams_set and --hparams only. These may not match training time\"\n        \"hyper-parameters.\")\n  return hparams_path", "output": "Get hyper-parameters file path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def execute_prebuild_script(self):\n        \"\"\"\n        \n\n        \"\"\"\n\n        (pb_mod_path, pb_func) = self.prebuild_script.rsplit('.', 1)\n\n        try:  # Prefer prebuild script in working directory\n            if pb_mod_path.count('.') >= 1:  # Prebuild script func is nested in a folder\n                (mod_folder_path, mod_name) = pb_mod_path.rsplit('.', 1)\n                mod_folder_path_fragments = mod_folder_path.split('.')\n                working_dir = os.path.join(os.getcwd(), *mod_folder_path_fragments)\n            else:\n                mod_name = pb_mod_path\n                working_dir = os.getcwd()\n\n            working_dir_importer = pkgutil.get_importer(working_dir)\n            module_ = working_dir_importer.find_module(mod_name).load_module(mod_name)\n\n        except (ImportError, AttributeError):\n\n            try:  # Prebuild func might be in virtualenv\n                module_ = importlib.import_module(pb_mod_path)\n            except ImportError:  # pragma: no cover\n                raise ClickException(click.style(\"Failed \", fg=\"red\") + 'to ' + click.style(\n                    \"import prebuild script \", bold=True) + 'module: \"{pb_mod_path}\"'.format(\n                    pb_mod_path=click.style(pb_mod_path, bold=True)))\n\n        if not hasattr(module_, pb_func):  # pragma: no cover\n            raise ClickException(click.style(\"Failed \", fg=\"red\") + 'to ' + click.style(\n                \"find prebuild script \", bold=True) + 'function: \"{pb_func}\" '.format(\n                pb_func=click.style(pb_func, bold=True)) + 'in module \"{pb_mod_path}\"'.format(\n                pb_mod_path=pb_mod_path))\n\n        prebuild_function = getattr(module_, pb_func)\n        prebuild_function()", "output": "Parse and execute the prebuild_script from the zappa_settings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def push_current(self, project):\n        \"\"\"\"\"\"\n        if __debug__:\n            from .targets import ProjectTarget\n            assert isinstance(project, ProjectTarget)\n        self.saved_current_project.append(self.current_project)\n        self.current_project = project", "output": "Temporary changes the current project to 'project'. Should\n        be followed by 'pop-current'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rewind_body(prepared_request):\n    \"\"\"\n    \"\"\"\n    body_seek = getattr(prepared_request.body, 'seek', None)\n    if body_seek is not None and isinstance(prepared_request._body_position, integer_types):\n        try:\n            body_seek(prepared_request._body_position)\n        except (IOError, OSError):\n            raise UnrewindableBodyError(\"An error occurred when rewinding request \"\n                                        \"body for redirect.\")\n    else:\n        raise UnrewindableBodyError(\"Unable to rewind request body for redirect.\")", "output": "Move file pointer back to its recorded starting position\n    so it can be read again on redirect.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sessions(self):\n        \"\"\"\n        \n        \"\"\"\n        return pd.to_datetime(\n            reduce(\n                np.union1d,\n                (reader.dates for reader in self._readers.values()),\n            ),\n            utc=True,\n        )", "output": "Returns\n        -------\n        sessions : DatetimeIndex\n           All session labels (unioning the range for all assets) which the\n           reader can provide.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_final(name, mro):\n    \"\"\"\n    \n    \"\"\"\n    return any(isinstance(getattr(c, '__dict__', {}).get(name), final)\n               for c in bases_mro(mro))", "output": "Checks if `name` is a `final` object in the given `mro`.\n    We need to check the mro because we need to directly go into the __dict__\n    of the classes. Because `final` objects are descriptor, we need to grab\n    them _BEFORE_ the `__call__` is invoked.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send(self, load, tries=None, timeout=None, raw=False):  # pylint: disable=unused-argument\n        '''\n        \n        '''\n        if 'cmd' not in load:\n            log.error('Malformed request, no cmd: %s', load)\n            return {}\n        cmd = load['cmd'].lstrip('_')\n        if cmd in self.cmd_stub:\n            return self.cmd_stub[cmd]\n        if not hasattr(self.fs, cmd):\n            log.error('Malformed request, invalid cmd: %s', load)\n            return {}\n        return getattr(self.fs, cmd)(load)", "output": "Emulate the channel send method, the tries and timeout are not used", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def densenet121(num_classes=1000, pretrained='imagenet'):\n    \n    \"\"\"\n    model = models.densenet121(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['densenet121'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_densenets(model)\n    return model", "output": "r\"\"\"Densenet-121 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_splits_if_different(self, split_dict):\n    \"\"\"\n    \"\"\"\n    assert isinstance(split_dict, splits_lib.SplitDict)\n\n    # If splits are already defined and identical, then we do not update\n    if self._splits and splits_lib.check_splits_equals(\n        self._splits, split_dict):\n      return\n\n    self._set_splits(split_dict)", "output": "Overwrite the splits if they are different from the current ones.\n\n    * If splits aren't already defined or different (ex: different number of\n      shards), then the new split dict is used. This will trigger stats\n      computation during download_and_prepare.\n    * If splits are already defined in DatasetInfo and similar (same names and\n      shards): keep the restored split which contains the statistics (restored\n      from GCS or file)\n\n    Args:\n      split_dict: `tfds.core.SplitDict`, the new split", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def example_alter_configs(a, args):\n    \"\"\" \n    \"\"\"\n\n    resources = []\n    for restype, resname, configs in zip(args[0::3], args[1::3], args[2::3]):\n        resource = ConfigResource(restype, resname)\n        resources.append(resource)\n        for k, v in [conf.split('=') for conf in configs.split(',')]:\n            resource.set_config(k, v)\n\n    fs = a.alter_configs(resources)\n\n    # Wait for operation to finish.\n    for res, f in fs.items():\n        try:\n            f.result()  # empty, but raises exception on failure\n            print(\"{} configuration successfully altered\".format(res))\n        except Exception:\n            raise", "output": "Alter configs atomically, replacing non-specified\n    configuration properties with their default values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset_window_layout(self):\r\n        \"\"\"\"\"\"\r\n        answer = QMessageBox.warning(self, _(\"Warning\"),\r\n                     _(\"Window layout will be reset to default settings: \"\r\n                       \"this affects window position, size and dockwidgets.\\n\"\r\n                       \"Do you want to continue?\"),\r\n                     QMessageBox.Yes | QMessageBox.No)\r\n        if answer == QMessageBox.Yes:\r\n            self.setup_layout(default=True)", "output": "Reset window layout to default", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rm_filesystems(name, device, config='/etc/filesystems'):\n    '''\n    \n    '''\n    modified = False\n    view_lines = []\n\n    if 'AIX' not in __grains__['kernel']:\n        return modified\n\n    criteria = _FileSystemsEntry(name=name, dev=device)\n    try:\n        fsys_filedict = _filesystems(config, False)\n        for fsys_view in six.viewitems(fsys_filedict):\n            try:\n                if criteria.match(fsys_view):\n                    modified = True\n                else:\n                    view_lines.append(fsys_view)\n\n            except _FileSystemsEntry.ParseError:\n                view_lines.append(fsys_view)\n\n    except (IOError, OSError) as exc:\n        raise CommandExecutionError(\"Couldn't read from {0}: {1}\".format(config, exc))\n\n    if modified:\n        try:\n            with salt.utils.files.fopen(config, 'wb') as ofile:\n                for fsys_view in view_lines:\n                    entry = fsys_view[1]\n                    mystrg = _FileSystemsEntry.dict_to_lines(entry)\n                    ofile.writelines(salt.utils.data.encode(mystrg))\n        except (IOError, OSError) as exc:\n            raise CommandExecutionError(\"Couldn't write to {0}: {1}\".format(config, exc))\n\n    return modified", "output": ".. versionadded:: 2018.3.3\n\n    Remove the mount point from the filesystems\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mount.rm_filesystems /mnt/foo /dev/sdg", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_indicator_shadow(DataFrame):\n    \"\"\"\n    \n    \"\"\"\n    return {\n        'LOW': lower_shadow(DataFrame), 'UP': upper_shadow(DataFrame),\n        'BODY': body(DataFrame), 'BODY_ABS': body_abs(DataFrame), 'PRICE_PCG': price_pcg(DataFrame)\n    }", "output": "\u4e0a\u4e0b\u5f71\u7ebf\u6307\u6807", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _calculate_price_by_slippage(self, action: str, price: float) -> float:\n        \"\"\"\n        \n        \"\"\"\n        if action == \"buy\":\n            return price * (1 + self.slippage)\n        if action == \"sell\":\n            return price * (1 - self.slippage)\n        return price", "output": "\u8ba1\u7b97\u8003\u8651\u6ed1\u70b9\u4e4b\u540e\u7684\u4ef7\u683c\n        :param action: \u4ea4\u6613\u52a8\u4f5c\uff0c \u652f\u6301 ['buy', 'sell']\n        :param price: \u539f\u59cb\u4ea4\u6613\u4ef7\u683c\n        :return: \u8003\u8651\u6ed1\u70b9\u540e\u7684\u4ea4\u6613\u4ef7\u683c", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_satisfy_constraints(self, label, xmin, ymin, xmax, ymax, width, height):\n        \"\"\"\"\"\"\n        if (xmax - xmin) * (ymax - ymin) < 2:\n            return False  # only 1 pixel\n        x1 = float(xmin) / width\n        y1 = float(ymin) / height\n        x2 = float(xmax) / width\n        y2 = float(ymax) / height\n        object_areas = self._calculate_areas(label[:, 1:])\n        valid_objects = np.where(object_areas * width * height > 2)[0]\n        if valid_objects.size < 1:\n            return False\n        intersects = self._intersect(label[valid_objects, 1:], x1, y1, x2, y2)\n        coverages = self._calculate_areas(intersects) / object_areas[valid_objects]\n        coverages = coverages[np.where(coverages > 0)[0]]\n        return coverages.size > 0 and np.amin(coverages) > self.min_object_covered", "output": "Check if constrains are satisfied", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def db_create(database, containment='NONE', new_database_options=None, **kwargs):\n    '''\n    \n    '''\n    if containment not in ['NONE', 'PARTIAL']:\n        return 'CONTAINMENT can be one of NONE and PARTIAL'\n    sql = \"CREATE DATABASE [{0}] CONTAINMENT = {1} \".format(database, containment)\n    if new_database_options:\n        sql += ' WITH ' + ', '.join(new_database_options)\n    conn = None\n    try:\n        conn = _get_connection(**kwargs)\n        conn.autocommit(True)\n        # cur = conn.cursor()\n        # cur.execute(sql)\n        conn.cursor().execute(sql)\n    except Exception as e:\n        return 'Could not create the login: {0}'.format(e)\n    finally:\n        if conn:\n            conn.autocommit(False)\n            conn.close()\n    return True", "output": "Creates a new database.\n    Does not update options of existing databases.\n    new_database_options can only be a list of strings\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minion mssql.db_create DB_NAME", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _smallest_size_at_least(height, width, smallest_side):\n  \"\"\"\n  \"\"\"\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  height = tf.to_float(height)\n  width = tf.to_float(width)\n  smallest_side = tf.to_float(smallest_side)\n\n  scale = tf.cond(\n      tf.greater(height, width), lambda: smallest_side / width,\n      lambda: smallest_side / height)\n  new_height = tf.to_int32(height * scale)\n  new_width = tf.to_int32(width * scale)\n  return new_height, new_width", "output": "Computes new shape with the smallest side equal to `smallest_side`.\n\n    Computes new shape with the smallest side equal to `smallest_side` while\n    preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n    the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: and int32 scalar tensor indicating the new width.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _minimize_constraints_fun_summation(x):\n    '''\n    \n    '''\n    summation = sum([x[i] for i in CONSTRAINT_PARAMS_IDX])\n    return CONSTRAINT_UPPERBOUND >= summation >= CONSTRAINT_LOWERBOUND", "output": "Minimize constraints fun summation", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def servicegroup_server_add(sg_name, s_name, s_port, **connection_args):\n    '''\n    \n    '''\n    # Nitro will throw an error if the server is already present\n    ret = True\n    server = _servicegroup_get_server(sg_name, s_name, s_port, **connection_args)\n    if server is not None:\n        return False\n    nitro = _connect(**connection_args)\n    if nitro is None:\n        return False\n    sgsb = NSServiceGroupServerBinding()\n    sgsb.set_servicegroupname(sg_name)\n    sgsb.set_servername(s_name)\n    sgsb.set_port(s_port)\n    try:\n        NSServiceGroupServerBinding.add(nitro, sgsb)\n    except NSNitroError as error:\n        log.debug('netscaler module error - NSServiceGroupServerBinding() failed: %s', error)\n        ret = False\n    _disconnect(nitro)\n    return ret", "output": "Add a server:port member to a servicegroup\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' netscaler.servicegroup_server_add 'serviceGroupName' 'serverName' 'serverPort'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deploy_api(self, ret):\n        '''\n        \n        '''\n        if self.restApiId:\n            res = self._cleanup_api()\n            if not res.get('deleted'):\n                ret['comment'] = 'Failed to cleanup restAreId {0}'.format(self.restApiId)\n                ret['abort'] = True\n                ret['result'] = False\n                return ret\n            return ret\n\n        response = __salt__['boto_apigateway.create_api'](name=self.rest_api_name,\n                                                          description=_Swagger.AWS_API_DESCRIPTION,\n                                                          **self._common_aws_args)\n\n        if not response.get('created'):\n            ret['result'] = False\n            ret['abort'] = True\n            if 'error' in response:\n                ret['comment'] = 'Failed to create rest api: {0}.'.format(response['error']['message'])\n            return ret\n\n        self.restApiId = response.get('restapi', {}).get('id')\n\n        return _log_changes(ret, 'deploy_api', response.get('restapi'))", "output": "this method create the top level rest api in AWS apigateway", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_extra_selections(self, key, extra_selections):\r\n        \"\"\"\r\n        \"\"\"\r\n        # use draw orders to highlight current_cell and current_line first\r\n        draw_order = DRAW_ORDERS.get(key)\r\n        if draw_order is None:\r\n            draw_order = DRAW_ORDERS.get('on_top')\r\n\r\n        for selection in extra_selections:\r\n            selection.draw_order = draw_order\r\n\r\n        self.clear_extra_selections(key)\r\n        self.extra_selections_dict[key] = extra_selections", "output": "Set extra selections for a key.\r\n\r\n        Also assign draw orders to leave current_cell and current_line\r\n        in the backgrund (and avoid them to cover other decorations)\r\n\r\n        NOTE: This will remove previous decorations added to  the same key.\r\n\r\n        Args:\r\n            key (str) name of the extra selections group.\r\n            extra_selections (list of sourcecode.api.TextDecoration).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _from_derivatives(xi, yi, x, order=None, der=0, extrapolate=False):\n    \"\"\"\n    \n    \"\"\"\n    from scipy import interpolate\n\n    # return the method for compat with scipy version & backwards compat\n    method = interpolate.BPoly.from_derivatives\n    m = method(xi, yi.reshape(-1, 1),\n               orders=order, extrapolate=extrapolate)\n\n    return m(x)", "output": "Convenience function for interpolate.BPoly.from_derivatives.\n\n    Construct a piecewise polynomial in the Bernstein basis, compatible\n    with the specified values and derivatives at breakpoints.\n\n    Parameters\n    ----------\n    xi : array_like\n        sorted 1D array of x-coordinates\n    yi : array_like or list of array-likes\n        yi[i][j] is the j-th derivative known at xi[i]\n    order: None or int or array_like of ints. Default: None.\n        Specifies the degree of local polynomials. If not None, some\n        derivatives are ignored.\n    der : int or list\n        How many derivatives to extract; None for all potentially nonzero\n        derivatives (that is a number equal to the number of points), or a\n        list of derivatives to extract. This numberincludes the function\n        value as 0th derivative.\n     extrapolate : bool, optional\n        Whether to extrapolate to ouf-of-bounds points based on first and last\n        intervals, or to return NaNs. Default: True.\n\n    See Also\n    --------\n    scipy.interpolate.BPoly.from_derivatives\n\n    Returns\n    -------\n    y : scalar or array_like\n        The result, of length R or length M or M by R.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_data_chunk(self, rows, indexes, mask, values):\n        \"\"\"\n        \n        \"\"\"\n\n        # 0 len\n        for v in values:\n            if not np.prod(v.shape):\n                return\n\n        try:\n            nrows = indexes[0].shape[0]\n            if nrows != len(rows):\n                rows = np.empty(nrows, dtype=self.dtype)\n            names = self.dtype.names\n            nindexes = len(indexes)\n\n            # indexes\n            for i, idx in enumerate(indexes):\n                rows[names[i]] = idx\n\n            # values\n            for i, v in enumerate(values):\n                rows[names[i + nindexes]] = v\n\n            # mask\n            if mask is not None:\n                m = ~mask.ravel().astype(bool, copy=False)\n                if not m.all():\n                    rows = rows[m]\n\n        except Exception as detail:\n            raise Exception(\n                \"cannot create row-data -> {detail}\".format(detail=detail))\n\n        try:\n            if len(rows):\n                self.table.append(rows)\n                self.table.flush()\n        except Exception as detail:\n            raise TypeError(\n                \"tables cannot write this data -> {detail}\".format(\n                    detail=detail))", "output": "Parameters\n        ----------\n        rows : an empty memory space where we are putting the chunk\n        indexes : an array of the indexes\n        mask : an array of the masks\n        values : an array of the values", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(name, profile):\n    '''\n    \n    '''\n    cmd = 'salt-cloud --out json -p {0} {1}'.format(profile, name)\n    out = __salt__['cmd.run_stdout'](cmd, python_shell=False)\n    try:\n        ret = salt.utils.json.loads(out)\n    except ValueError:\n        ret = {}\n    return ret", "output": "Create the named vm\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt <minion-id> saltcloud.create webserver rackspace_centos_512", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def van_enc_2d(x, first_depth, reuse=False):\n  \"\"\"\n  \"\"\"\n  with tf.variable_scope('van_enc', reuse=reuse):\n    a = 4  # depends on the inputs size\n    b = 4\n    # a, b = 4,4\n    enc = tf.nn.relu(x)\n    enc = tf.layers.dense(enc, first_depth * a * b, tf.nn.relu)\n    enc = tf.contrib.layers.layer_norm(enc)\n\n    enc = tf.reshape(enc, [-1, a, b, first_depth])\n\n    enc = tf.layers.conv2d_transpose(\n        enc, first_depth, 3, padding='same', activation=tf.nn.relu, strides=1)\n    enc = tf.contrib.layers.layer_norm(enc)\n    enc = tf.layers.conv2d_transpose(\n        enc,\n        first_depth * 2,\n        3,\n        padding='same',\n        activation=tf.nn.relu,\n        strides=2)\n    van_higher_level_2 = tf.reshape(enc, [-1, a * 2 * b * 2 * first_depth * 2])\n\n    enc = tf.layers.conv2d_transpose(\n        enc,\n        first_depth * 2,\n        3,\n        padding='same',\n        activation=tf.nn.relu,\n        strides=1)\n    enc = tf.contrib.layers.layer_norm(enc)\n    enc = tf.layers.conv2d_transpose(\n        enc,\n        first_depth * 4,\n        3,\n        padding='same',\n        activation=tf.nn.relu,\n        strides=1)\n    van_higher_level_4 = tf.reshape(enc, [-1, a * 2 * b * 2 * first_depth * 4])\n\n    van_higher_level = tf.concat([x, van_higher_level_2, van_higher_level_4], 1)\n\n    return enc, van_higher_level", "output": "The higher level structure encoder for the VAN.\n\n  The high level structure is a vector instead of an image.\n\n  Args:\n    x: The higher level structure to encode.\n    first_depth: The depth of the first layer. Depth is increased in subsequent\n      layers.\n    reuse: To reuse in variable scope or not.\n\n  Returns:\n    The encoded image.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lighting(im, b, c):\n    \"\"\"  \"\"\"\n    if b==0 and c==1: return im\n    mu = np.average(im)\n    return np.clip((im-mu)*c+mu+b,0.,1.).astype(np.float32)", "output": "Adjust image balance and contrast", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enum(option, *options):\n    \"\"\"\n    \n    \"\"\"\n    options = (option,) + options\n    rangeob = range(len(options))\n\n    try:\n        inttype = _inttypes[int(np.log2(len(options) - 1)) // 8]\n    except IndexError:\n        raise OverflowError(\n            'Cannot store enums with more than sys.maxsize elements, got %d' %\n            len(options),\n        )\n\n    class _enum(Structure):\n        _fields_ = [(o, inttype) for o in options]\n\n        def __iter__(self):\n            return iter(rangeob)\n\n        def __contains__(self, value):\n            return 0 <= value < len(options)\n\n        def __repr__(self):\n            return '<enum: %s>' % (\n                ('%d fields' % len(options))\n                if len(options) > 10 else\n                repr(options)\n            )\n\n    return _enum(*rangeob)", "output": "Construct a new enum object.\n\n    Parameters\n    ----------\n    *options : iterable of str\n        The names of the fields for the enum.\n\n    Returns\n    -------\n    enum\n        A new enum collection.\n\n    Examples\n    --------\n    >>> e = enum('a', 'b', 'c')\n    >>> e\n    <enum: ('a', 'b', 'c')>\n    >>> e.a\n    0\n    >>> e.b\n    1\n    >>> e.a in e\n    True\n    >>> tuple(e)\n    (0, 1, 2)\n\n    Notes\n    -----\n    Identity checking is not guaranteed to work with enum members, instead\n    equality checks should be used. From CPython's documentation:\n\n    \"The current implementation keeps an array of integer objects for all\n    integers between -5 and 256, when you create an int in that range you\n    actually just get back a reference to the existing object. So it should be\n    possible to change the value of 1. I suspect the behaviour of Python in\n    this case is undefined. :-)\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_system_encoding():\n    '''\n        \n    '''\n    # This is the most trustworthy source of the system encoding, though, if\n    # salt is being imported after being daemonized, this information is lost\n    # and reset to None\n    encoding = None\n\n    if not sys.platform.startswith('win') and sys.stdin is not None:\n        # On linux we can rely on sys.stdin for the encoding since it\n        # most commonly matches the filesystem encoding. This however\n        # does not apply to windows\n        encoding = sys.stdin.encoding\n\n    if not encoding:\n        # If the system is properly configured this should return a valid\n        # encoding. MS Windows has problems with this and reports the wrong\n        # encoding\n        import locale\n        try:\n            encoding = locale.getdefaultlocale()[-1]\n        except ValueError:\n            # A bad locale setting was most likely found:\n            #   https://github.com/saltstack/salt/issues/26063\n            pass\n\n        # This is now garbage collectable\n        del locale\n        if not encoding:\n            # This is most likely ascii which is not the best but we were\n            # unable to find a better encoding. If this fails, we fall all\n            # the way back to ascii\n            encoding = sys.getdefaultencoding()\n        if not encoding:\n            if sys.platform.startswith('darwin'):\n                # Mac OS X uses UTF-8\n                encoding = 'utf-8'\n            elif sys.platform.startswith('win'):\n                # Windows uses a configurable encoding; on Windows, Python uses the name \"mbcs\"\n                # to refer to whatever the currently configured encoding is.\n                encoding = 'mbcs'\n            else:\n                # On linux default to ascii as a last resort\n                encoding = 'ascii'\n    return encoding", "output": "Get system encoding. Most of this code is a part of salt/__init__.py", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def icon_url_as(self, *, format='webp', size=1024):\n        \"\"\"\"\"\"\n        return Asset._from_guild_image(self._state, self.id, self.icon, 'icons', format=format, size=size)", "output": ":class:`Asset`: The same operation as :meth:`Guild.icon_url_as`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vb_get_network_adapters(machine_name=None, machine=None):\n    '''\n    \n    '''\n\n    if machine_name:\n        machine = vb_get_box().findMachine(machine_name)\n    network_adapters = []\n\n    for i in range(vb_get_max_network_slots()):\n        try:\n            inetwork_adapter = machine.getNetworkAdapter(i)\n            network_adapter = vb_xpcom_to_attribute_dict(\n                inetwork_adapter, 'INetworkAdapter'\n            )\n            network_adapter['properties'] = inetwork_adapter.getProperties('')\n            network_adapters.append(network_adapter)\n        except Exception:\n            pass\n\n    return network_adapters", "output": "A valid machine_name or a machine is needed to make this work!\n\n    @param machine_name:\n    @type machine_name: str\n    @param machine:\n    @type machine: IMachine\n    @return: INetorkAdapter's converted to dicts\n    @rtype: [dict]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pkill(pattern, user=None, signal=15, full=False):\n    '''\n    \n    '''\n\n    killed = []\n    for proc in psutil.process_iter():\n        name_match = pattern in ' '.join(_get_proc_cmdline(proc)) if full \\\n            else pattern in _get_proc_name(proc)\n        user_match = True if user is None else user == _get_proc_username(proc)\n        if name_match and user_match:\n            try:\n                proc.send_signal(signal)\n                killed.append(_get_proc_pid(proc))\n            except psutil.NoSuchProcess:\n                pass\n    if not killed:\n        return None\n    else:\n        return {'killed': killed}", "output": "Kill processes matching a pattern.\n\n    .. code-block:: bash\n\n        salt '*' ps.pkill pattern [user=username] [signal=signal_number] \\\\\n                [full=(true|false)]\n\n    pattern\n        Pattern to search for in the process list.\n\n    user\n        Limit matches to the given username. Default: All users.\n\n    signal\n        Signal to send to the process(es). See manpage entry for kill\n        for possible values. Default: 15 (SIGTERM).\n\n    full\n        A boolean value indicating whether only the name of the command or\n        the full command line should be matched against the pattern.\n\n    **Examples:**\n\n    Send SIGHUP to all httpd processes on all 'www' minions:\n\n    .. code-block:: bash\n\n        salt 'www.*' ps.pkill httpd signal=1\n\n    Send SIGKILL to all bash processes owned by user 'tom':\n\n    .. code-block:: bash\n\n        salt '*' ps.pkill bash signal=9 user=tom", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset_time(path='.', amt=None):\n    '''\n    \n    '''\n    if not amt:\n        amt = int(time.time())\n    for fname in os.listdir(path):\n        fname = os.path.join(path, fname)\n        if os.path.isdir(fname):\n            reset_time(fname, amt=amt)\n        os.utime(fname, (amt, amt,))", "output": "Reset atime/mtime on all files to prevent systemd swipes only part of the files in the /tmp.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def listdir(self, path='.'):\n        \"\"\"\n        \n        \"\"\"\n        self._connect()\n\n        if self.sftp:\n            contents = self._sftp_listdir(path)\n        else:\n            contents = self._ftp_listdir(path)\n\n        self._close()\n\n        return contents", "output": "Gets an list of the contents of path in (s)FTP", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __dict_to_pod_spec(spec):\n    '''\n    \n    '''\n    spec_obj = kubernetes.client.V1PodSpec()\n    for key, value in iteritems(spec):\n        if hasattr(spec_obj, key):\n            setattr(spec_obj, key, value)\n\n    return spec_obj", "output": "Converts a dictionary into kubernetes V1PodSpec instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_published(name, config_path=_DEFAULT_CONFIG_PATH, endpoint='', prefix=None,\n                     skip_cleanup=False, force=False):\n    '''\n    \n    '''\n    _validate_config(config_path)\n    force = six.text_type(bool(force)).lower()\n    skip_cleanup = six.text_type(bool(skip_cleanup)).lower()\n\n    current_published = __salt__['aptly.get_published'](name=name, config_path=config_path,\n                                                        endpoint=endpoint, prefix=prefix)\n\n    if not current_published:\n        log.debug('Published repository already absent: %s', name)\n        return True\n\n    cmd = ['publish', 'drop', '-config={}'.format(config_path),\n           '-force-drop={}'.format(force), '-skip-cleanup={}'.format(skip_cleanup),\n           name]\n\n    if prefix:\n        cmd.append('{}:{}'.format(endpoint, prefix))\n\n    _cmd_run(cmd)\n    published = __salt__['aptly.get_published'](name=name, config_path=config_path,\n                                                endpoint=endpoint, prefix=prefix)\n\n    if published:\n        log.error('Unable to remove published snapshot: %s', name)\n        return False\n    log.debug('Removed published snapshot: %s', name)\n    return True", "output": "Remove files belonging to a published repository. Aptly tries to remove as many files\n        belonging to this repository as possible.\n\n    :param str name: The distribution name of the published repository.\n    :param str config_path: The path to the configuration file for the aptly instance.\n    :param str endpoint: The publishing endpoint.\n    :param str prefix: The prefix for publishing.\n    :param bool skip_cleanup: Whether to remove unreferenced files.\n    :param bool force: Whether to remove the published repository even if component\n        cleanup fails.\n\n    :return: A boolean representing whether all changes succeeded.\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' aptly.delete_published name=\"test-dist\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gene_expression_conv_base():\n  \"\"\"\"\"\"\n  hparams = common_hparams.basic_params1()\n\n  batch_size = 10\n  output_length = 2048\n  inputs_per_output = 128\n  chunk_size = 4\n  input_length = output_length * inputs_per_output // chunk_size\n  hparams.batch_size = input_length * batch_size\n\n  hparams.dropout = 0.1\n  hparams.add_hparam(\"num_conv_layers\", 4)\n  hparams.add_hparam(\"num_dconv_layers\", 7)\n  # The product of these pooling windows should match\n  # input_length/target_length.\n  hparams.add_hparam(\"pooling_windows\", [2, 2, 2, 4])\n\n  hparams.hidden_size = 256\n  hparams.kernel_width = 20\n  hparams.add_hparam(\"stride\", 1)\n  return hparams", "output": "Hparams for GeneExpressionConv model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _container_path(model,\n                    key=None,\n                    container=None,\n                    delim=DEFAULT_TARGET_DELIM):\n    '''\n    \n    '''\n    if not key:\n        key = ''\n    if not container:\n        container = 'config'\n    for model_key, model_value in six.iteritems(model):\n        if key:\n            key_depth = '{prev_key}{delim}{cur_key}'.format(prev_key=key,\n                                                            delim=delim,\n                                                            cur_key=model_key)\n        else:\n            key_depth = model_key\n        if model_key == container:\n            yield key_depth\n        else:\n            for value in _container_path(model_value,\n                                         key=key_depth,\n                                         container=container,\n                                         delim=delim):\n                yield value", "output": "Generate all the possible paths within an OpenConfig-like object.\n    This function returns a generator.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_data_day_resample(day_data, type_='w'):\n    \"\"\"\n    \"\"\"\n    # return day_data_p.assign(open=day_data.open.resample(type_).first(),high=day_data.high.resample(type_).max(),low=day_data.low.resample(type_).min(),\\\n    #             vol=day_data.vol.resample(type_).sum() if 'vol' in day_data.columns else day_data.volume.resample(type_).sum(),\\\n    #             amount=day_data.amount.resample(type_).sum()).dropna().set_index('date')\n    try:\n        day_data = day_data.reset_index().set_index('date', drop=False)\n    except:\n        day_data = day_data.set_index('date', drop=False)\n\n    CONVERSION = {\n        'code': 'first',\n        'open': 'first',\n        'high': 'max',\n        'low': 'min',\n        'close': 'last',\n        'vol': 'sum',\n        'amount': 'sum'\n    } if 'vol' in day_data.columns else {\n        'code': 'first',\n        'open': 'first',\n        'high': 'max',\n        'low': 'min',\n        'close': 'last',\n        'volume': 'sum',\n        'amount': 'sum'\n    }\n\n    return day_data.resample(\n        type_,\n        closed='right'\n    ).apply(CONVERSION).dropna().reset_index().set_index(['date',\n                                                          'code'])", "output": "\u65e5\u7ebf\u964d\u91c7\u6837\n\n    Arguments:\n        day_data {[type]} -- [description]\n\n    Keyword Arguments:\n        type_ {str} -- [description] (default: {'w'})\n\n    Returns:\n        [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def suspend(instance_id, profile=None, **kwargs):\n    '''\n    \n\n    '''\n    conn = _auth(profile, **kwargs)\n    return conn.suspend(instance_id)", "output": "Suspend an instance\n\n    instance_id\n        ID of the instance to be suspended\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' nova.suspend 1138", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe_event_source_mapping(UUID=None, EventSourceArn=None,\n                                  FunctionName=None,\n                                  region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    ids = _get_ids(UUID, EventSourceArn=EventSourceArn,\n                   FunctionName=FunctionName)\n    if not ids:\n        return {'event_source_mapping': None}\n\n    UUID = ids[0]\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        desc = conn.get_event_source_mapping(UUID=UUID)\n        if desc:\n            keys = ('UUID', 'BatchSize', 'EventSourceArn',\n                    'FunctionArn', 'LastModified', 'LastProcessingResult',\n                    'State', 'StateTransitionReason')\n            return {'event_source_mapping': dict([(k, desc.get(k)) for k in keys])}\n        else:\n            return {'event_source_mapping': None}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "Given an event source mapping ID or an event source ARN and FunctionName,\n    obtain the current settings of that mapping.\n\n    Returns a dictionary of interesting properties.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_lambda.describe_event_source_mapping uuid", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_topic_path(topic_path):\n    \"\"\"\n    \"\"\"\n    match = _TOPIC_REF_RE.match(topic_path)\n    if match is None:\n        raise ValueError(_BAD_TOPIC.format(topic_path))\n\n    return match.group(\"name\"), match.group(\"project\")", "output": "Verify that a topic path is in the correct format.\n\n    .. _resource manager docs: https://cloud.google.com/resource-manager/\\\n                               reference/rest/v1beta1/projects#\\\n                               Project.FIELDS.project_id\n    .. _topic spec: https://cloud.google.com/storage/docs/json_api/v1/\\\n                    notifications/insert#topic\n\n    Expected to be of the form:\n\n        //pubsub.googleapis.com/projects/{project}/topics/{topic}\n\n    where the ``project`` value must be \"6 to 30 lowercase letters, digits,\n    or hyphens. It must start with a letter. Trailing hyphens are prohibited.\"\n    (see `resource manager docs`_) and ``topic`` must have length at least two,\n    must start with a letter and may only contain alphanumeric characters or\n    ``-``, ``_``, ``.``, ``~``, ``+`` or ``%`` (i.e characters used for URL\n    encoding, see `topic spec`_).\n\n    Args:\n        topic_path (str): The topic path to be verified.\n\n    Returns:\n        Tuple[str, str]: The ``project`` and ``topic`` parsed from the\n        ``topic_path``.\n\n    Raises:\n        ValueError: If the topic path is invalid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parsed_cmd(self):\n        \"\"\"\n        \n        \"\"\"\n        if len(self.args.command) < 2:\n            return \" \".join(self.args.command)\n\n        return \" \".join(self._quote_argument(arg) for arg in self.args.command)", "output": "We need to take into account two cases:\n\n        - ['python code.py foo bar']: Used mainly with dvc as a library\n        - ['echo', 'foo bar']: List of arguments received from the CLI\n\n        The second case would need quoting, as it was passed through:\n                dvc run echo \"foo bar\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def brelu(x):\n  \"\"\"\"\"\"\n  x_shape = shape_list(x)\n  x1, x2 = tf.split(tf.reshape(x, x_shape[:-1] + [-1, 2]), 2, axis=-1)\n  y1 = tf.nn.relu(x1)\n  y2 = -tf.nn.relu(-x2)\n  return tf.reshape(tf.concat([y1, y2], axis=-1), x_shape)", "output": "Bipolar ReLU as in https://arxiv.org/abs/1709.04054.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_dvs_product_info(dvs_name, dvs_product_info):\n    '''\n    \n    '''\n    log.trace('Building the dict of the DVS \\'%s\\' product info', dvs_name)\n    return {'name': dvs_product_info.name,\n            'vendor': dvs_product_info.vendor,\n            'version': dvs_product_info.version}", "output": "Returns the dict representation of the DVS product_info\n\n    dvs_name\n        The name of the DVS\n\n    dvs_product_info\n        The DVS product info", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def secgroup_info(call=None, kwargs=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The secgroup_info function must be called with -f or --function.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    name = kwargs.get('name', None)\n    secgroup_id = kwargs.get('secgroup_id', None)\n\n    if secgroup_id:\n        if name:\n            log.warning(\n                'Both the \\'secgroup_id\\' and \\'name\\' arguments were provided. '\n                '\\'secgroup_id\\' will take precedence.'\n            )\n    elif name:\n        secgroup_id = get_secgroup_id(kwargs={'name': name})\n    else:\n        raise SaltCloudSystemExit(\n            'The secgroup_info function requires either a name or a secgroup_id '\n            'to be provided.'\n        )\n\n    server, user, password = _get_xml_rpc()\n    auth = ':'.join([user, password])\n\n    info = {}\n    response = server.one.secgroup.info(auth, int(secgroup_id))[1]\n    tree = _get_xml(response)\n    info[tree.find('NAME').text] = _xml_to_dict(tree)\n\n    return info", "output": "Retrieves information for the given security group. Either a name or a\n    secgroup_id must be supplied.\n\n    .. versionadded:: 2016.3.0\n\n    name\n        The name of the security group for which to gather information. Can be\n        used instead of ``secgroup_id``.\n\n    secgroup_id\n        The ID of the security group for which to gather information. Can be\n        used instead of ``name``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f secgroup_info opennebula name=my-secgroup\n        salt-cloud --function secgroup_info opennebula secgroup_id=5", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ptr_name(rdata):\n    '''\n    \n    '''\n    try:\n        return ipaddress.ip_address(rdata).reverse_pointer\n    except ValueError:\n        log.error(\n            'Unable to generate PTR record; %s is not a valid IP address',\n            rdata\n        )\n        return False", "output": "Return PTR name of given IP\n    :param rdata: IP address\n    :return: PTR record name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def indexables(self):\n        \"\"\"  \"\"\"\n        if self._indexables is None:\n\n            self._indexables = []\n\n            # index columns\n            self._indexables.extend([\n                IndexCol(name=name, axis=axis, pos=i)\n                for i, (axis, name) in enumerate(self.attrs.index_cols)\n            ])\n\n            # values columns\n            dc = set(self.data_columns)\n            base_pos = len(self._indexables)\n\n            def f(i, c):\n                klass = DataCol\n                if c in dc:\n                    klass = DataIndexableCol\n                return klass.create_for_block(i=i, name=c, pos=base_pos + i,\n                                              version=self.version)\n\n            self._indexables.extend(\n                [f(i, c) for i, c in enumerate(self.attrs.values_cols)])\n\n        return self._indexables", "output": "create/cache the indexables if they don't exist", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def boot(zone, single=False, altinit=None, smf_options=None):\n    '''\n    \n    '''\n    ret = {'status': True}\n\n    ## build boot_options\n    boot_options = ''\n    if single:\n        boot_options = '-s {0}'.format(boot_options)\n    if altinit:  # note: we cannot validate the path, as this is local to the zonepath.\n        boot_options = '-i {0} {1}'.format(altinit, boot_options)\n    if smf_options:\n        boot_options = '-m {0} {1}'.format(smf_options, boot_options)\n    if boot_options != '':\n        boot_options = ' -- {0}'.format(boot_options.strip())\n\n    ## execute boot\n    res = __salt__['cmd.run_all']('zoneadm {zone} boot{boot_opts}'.format(\n        zone='-u {0}'.format(zone) if _is_uuid(zone) else '-z {0}'.format(zone),\n        boot_opts=boot_options,\n    ))\n    ret['status'] = res['retcode'] == 0\n    ret['message'] = res['stdout'] if ret['status'] else res['stderr']\n    ret['message'] = ret['message'].replace('zoneadm: ', '')\n    if ret['message'] == '':\n        del ret['message']\n\n    return ret", "output": "Boot (or activate) the specified zone.\n\n    zone : string\n        name or uuid of the zone\n    single : boolean\n        boots only to milestone svc:/milestone/single-user:default.\n    altinit : string\n        valid path to an alternative executable to be the primordial process.\n    smf_options : string\n        include two categories of options to control booting behavior of\n        the service management facility: recovery options and messages options.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zoneadm.boot clementine\n        salt '*' zoneadm.boot maeve single=True\n        salt '*' zoneadm.boot teddy single=True smf_options=verbose", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_sax(walker, handler):\n    \"\"\"\n\n    \"\"\"\n    handler.startDocument()\n    for prefix, namespace in prefix_mapping.items():\n        handler.startPrefixMapping(prefix, namespace)\n\n    for token in walker:\n        type = token[\"type\"]\n        if type == \"Doctype\":\n            continue\n        elif type in (\"StartTag\", \"EmptyTag\"):\n            attrs = AttributesNSImpl(token[\"data\"],\n                                     unadjustForeignAttributes)\n            handler.startElementNS((token[\"namespace\"], token[\"name\"]),\n                                   token[\"name\"],\n                                   attrs)\n            if type == \"EmptyTag\":\n                handler.endElementNS((token[\"namespace\"], token[\"name\"]),\n                                     token[\"name\"])\n        elif type == \"EndTag\":\n            handler.endElementNS((token[\"namespace\"], token[\"name\"]),\n                                 token[\"name\"])\n        elif type in (\"Characters\", \"SpaceCharacters\"):\n            handler.characters(token[\"data\"])\n        elif type == \"Comment\":\n            pass\n        else:\n            assert False, \"Unknown token type\"\n\n    for prefix, namespace in prefix_mapping.items():\n        handler.endPrefixMapping(prefix)\n    handler.endDocument()", "output": "Call SAX-like content handler based on treewalker walker\n\n    :arg walker: the treewalker to use to walk the tree to convert it\n\n    :arg handler: SAX handler to use", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def plot_diagram(config, results, images_dir, out_filename):\n    \"\"\"\"\"\"\n    img_files = plot_temp_diagrams(config, results, images_dir)\n    join_images(img_files, out_filename)\n    for img_file in img_files:\n        os.remove(img_file)", "output": "Plot one diagram", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_dispatcher(args):\n    ''''''\n    nni_config = Config(get_config_filename(args)).get_config('experimentConfig')\n    if nni_config.get('tuner') and nni_config['tuner'].get('builtinTunerName'):\n        dispatcher_name = nni_config['tuner']['builtinTunerName']\n    elif nni_config.get('advisor') and nni_config['advisor'].get('builtinAdvisorName'):\n        dispatcher_name = nni_config['advisor']['builtinAdvisorName']\n    else: # otherwise it should be a customized one\n        return\n    if dispatcher_name not in TUNERS_SUPPORTING_IMPORT_DATA:\n        if dispatcher_name in TUNERS_NO_NEED_TO_IMPORT_DATA:\n            print_warning(\"There is no need to import data for %s\" % dispatcher_name)\n            exit(0)\n        else:\n            print_error(\"%s does not support importing addtional data\" % dispatcher_name)\n            exit(1)", "output": "validate if the dispatcher of the experiment supports importing data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_env_error_message(error, show_traceback, using_user_site):\n    \"\"\"\n    \"\"\"\n    parts = []\n\n    # Mention the error if we are not going to show a traceback\n    parts.append(\"Could not install packages due to an EnvironmentError\")\n    if not show_traceback:\n        parts.append(\": \")\n        parts.append(str(error))\n    else:\n        parts.append(\".\")\n\n    # Spilt the error indication from a helper message (if any)\n    parts[-1] += \"\\n\"\n\n    # Suggest useful actions to the user:\n    #  (1) using user site-packages or (2) verifying the permissions\n    if error.errno == errno.EACCES:\n        user_option_part = \"Consider using the `--user` option\"\n        permissions_part = \"Check the permissions\"\n\n        if not using_user_site:\n            parts.extend([\n                user_option_part, \" or \",\n                permissions_part.lower(),\n            ])\n        else:\n            parts.append(permissions_part)\n        parts.append(\".\\n\")\n\n    return \"\".join(parts).strip() + \"\\n\"", "output": "Format an error message for an EnvironmentError\n\n    It may occur anytime during the execution of the install command.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_router(self, name, ext_network=None, admin_state_up=True):\n        '''\n        \n        '''\n        body = {'name': name,\n                'admin_state_up': admin_state_up}\n        if ext_network:\n            net_id = self._find_network_id(ext_network)\n            body['external_gateway_info'] = {'network_id': net_id}\n        return self.network_conn.create_router(body={'router': body})", "output": "Creates a new router", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(name, log_file=None):\n    '''\n    \n\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': None,\n           'comment': ''}\n\n    # retrieve all log configuration\n    config = __salt__['logadm.list_conf']()\n\n    # figure out log_file and name\n    if not log_file:\n        if name.startswith('/'):\n            log_file = name\n            name = None\n        else:\n            for log in config:\n                if 'entryname' in config[log] and config[log]['entryname'] == name:\n                    log_file = config[log]['log_file']\n                    break\n    if not name:\n        for log in config:\n            if 'log_file' in config[log] and config[log]['log_file'] == log_file:\n                if 'entryname' in config[log]:\n                    name = config[log]['entryname']\n                break\n\n    # remove log if needed\n    if log_file in config:\n        res = __salt__['logadm.remove'](name if name else log_file)\n        ret['result'] = 'Error' not in res\n        if ret['result']:\n            ret['comment'] = 'Configuration for {} removed.'.format(log_file)\n            ret['changes'][log_file] = None\n        else:\n            ret['comment'] = res['Error']\n    else:\n        ret['result'] = True\n        ret['comment'] = 'No configuration for {} present.'.format(log_file)\n\n    return ret", "output": "Remove a log from the logadm configuration\n\n    name : string\n        entryname\n    log_file : string\n        (optional) log file path\n\n    .. note::\n        If log_file is specified it will be used instead of the entry name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cv_squared(x):\n  \"\"\"\n  \"\"\"\n  epsilon = 1e-10\n  float_size = tf.to_float(tf.size(x)) + epsilon\n  mean = tf.reduce_sum(x) / float_size\n  variance = tf.reduce_sum(tf.squared_difference(x, mean)) / float_size\n  return variance / (tf.square(mean) + epsilon)", "output": "The squared coefficient of variation of a sample.\n\n  Useful as a loss to encourage a positive distribution to be more uniform.\n  Epsilons added for numerical stability.\n  Returns 0 for an empty Tensor.\n\n  Args:\n    x: a `Tensor`.\n\n  Returns:\n    a `Scalar`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def action2button(action, autoraise=True, text_beside_icon=False, parent=None):\r\n    \"\"\"\"\"\"\r\n    if parent is None:\r\n        parent = action.parent()\r\n    button = QToolButton(parent)\r\n    button.setDefaultAction(action)\r\n    button.setAutoRaise(autoraise)\r\n    if text_beside_icon:\r\n        button.setToolButtonStyle(Qt.ToolButtonTextBesideIcon)\r\n    return button", "output": "Create a QToolButton directly from a QAction object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def listdir(self, dirname):\n        \"\"\"\"\"\"\n        client = boto3.client(\"s3\")\n        bucket, path = self.bucket_and_path(dirname)\n        p = client.get_paginator(\"list_objects\")\n        if not path.endswith(\"/\"):\n            path += \"/\"  # This will now only retrieve subdir content\n        keys = []\n        for r in p.paginate(Bucket=bucket, Prefix=path, Delimiter=\"/\"):\n            keys.extend(o[\"Prefix\"][len(path):-1] for o in r.get(\"CommonPrefixes\", []))\n            for o in r.get(\"Contents\", []):\n                key = o[\"Key\"][len(path):]\n                if key:  # Skip the base dir, which would add an empty string\n                    keys.append(key)\n        return keys", "output": "Returns a list of entries contained within a directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def scan(self):\n        \"\"\"\n        \n\n        \"\"\"\n        es = connections.get_connection(self._using)\n\n        for hit in scan(\n                es,\n                query=self.to_dict(),\n                index=self._index,\n                **self._params\n        ):\n            yield self._get_result(hit)", "output": "Turn the search into a scan search and return a generator that will\n        iterate over all the documents matching the query.\n\n        Use ``params`` method to specify any additional arguments you with to\n        pass to the underlying ``scan`` helper from ``elasticsearch-py`` -\n        https://elasticsearch-py.readthedocs.io/en/master/helpers.html#elasticsearch.helpers.scan", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_tree(self, tgt_env):\n        '''\n        \n        '''\n        if not self.env_is_exposed(tgt_env):\n            return None\n\n        tgt_ref = self.ref(tgt_env)\n        if tgt_ref is None:\n            return None\n\n        for ref_type in self.ref_types:\n            try:\n                func_name = 'get_tree_from_{0}'.format(ref_type)\n                func = getattr(self, func_name)\n            except AttributeError:\n                log.error(\n                    '%s class is missing function \\'%s\\'',\n                    self.__class__.__name__, func_name\n                )\n            else:\n                candidate = func(tgt_ref)\n                if candidate is not None:\n                    return candidate\n\n        # No matches found\n        return None", "output": "Return a tree object for the specified environment", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_examples(self, file_path):\n    \"\"\"\n    \"\"\"\n\n    fieldnames = [\n        'class_label', 'lepton_pT', 'lepton_eta', 'lepton_phi',\n        'missing_energy_magnitude', 'missing_energy_phi', 'jet_1_pt',\n        'jet_1_eta', 'jet_1_phi', 'jet_1_b-tag', 'jet_2_pt', 'jet_2_eta',\n        'jet_2_phi', 'jet_2_b-tag', 'jet_3_pt', 'jet_3_eta', 'jet_3_phi',\n        'jet_3_b-tag', 'jet_4_pt', 'jet_4_eta', 'jet_4_phi', 'jet_4_b-tag',\n        'm_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb'\n    ]\n\n    with tf.io.gfile.GFile(file_path) as csvfile:\n      reader = csv.DictReader(csvfile, fieldnames=fieldnames)\n      for row in reader:\n        yield row", "output": "Generate features given the directory path.\n\n    Args:\n      file_path: path where the csv file is stored\n\n    Yields:\n      The features, per row.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_esxdatacenter_proxy_details():\n    '''\n    \n    '''\n    det = __salt__['esxdatacenter.get_details']()\n    return det.get('vcenter'), det.get('username'), det.get('password'), \\\n            det.get('protocol'), det.get('port'), det.get('mechanism'), \\\n            det.get('principal'), det.get('domain'), det.get('datacenter')", "output": "Returns the running esxdatacenter's proxy details", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_req_to_reinstall(self, req):\n        # type: (InstallRequirement) -> None\n        \"\"\"\n        \n        \"\"\"\n        # Don't uninstall the conflict if doing a user install and the\n        # conflict is not a user install.\n        if not self.use_user_site or dist_in_usersite(req.satisfied_by):\n            req.conflicts_with = req.satisfied_by\n        req.satisfied_by = None", "output": "Set a requirement to be installed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def doc_type(self, *doc_type, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        # .doc_type() resets\n        s = self._clone()\n        if not doc_type and not kwargs:\n            s._doc_type = []\n            s._doc_type_map = {}\n        else:\n            s._doc_type.extend(doc_type)\n            s._doc_type.extend(kwargs.keys())\n            s._doc_type_map.update(kwargs)\n        return s", "output": "Set the type to search through. You can supply a single value or\n        multiple. Values can be strings or subclasses of ``Document``.\n\n        You can also pass in any keyword arguments, mapping a doc_type to a\n        callback that should be used instead of the Hit class.\n\n        If no doc_type is supplied any information stored on the instance will\n        be erased.\n\n        Example:\n\n            s = Search().doc_type('product', 'store', User, custom=my_callback)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_module(self, vars=None, shared=False, locals=None):\n        \"\"\"\n        \"\"\"\n        return TemplateModule(self, self.new_context(vars, shared, locals))", "output": "This method works like the :attr:`module` attribute when called\n        without arguments but it will evaluate the template on every call\n        rather than caching it.  It's also possible to provide\n        a dict which is then used as context.  The arguments are the same\n        as for the :meth:`new_context` method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_gym_env(name,\n                 rl_env_max_episode_steps=-1,\n                 maxskip_env=False,\n                 rendered_env=False,\n                 rendered_env_resize_to=None,\n                 sticky_actions=False):\n  \"\"\"\n  \"\"\"\n  env = gym.make(name)\n  return gym_env_wrapper(env, rl_env_max_episode_steps, maxskip_env,\n                         rendered_env, rendered_env_resize_to, sticky_actions)", "output": "Create a gym env optionally with a time limit and maxskip wrapper.\n\n  NOTE: The returned env may already be wrapped with TimeLimit!\n\n  Args:\n    name: `str` - base name of the gym env to make.\n    rl_env_max_episode_steps: `int` or None - Using any value < 0 returns the\n      env as-in, otherwise we impose the requested timelimit. Setting this to\n      None returns a wrapped env that doesn't have a step limit.\n    maxskip_env: whether to also use MaxAndSkip wrapper before time limit.\n    rendered_env: whether to force render for observations. Use this for\n      environments that are not natively rendering the scene for observations.\n    rendered_env_resize_to: a list of [height, width] to change the original\n      resolution of the native environment render.\n    sticky_actions: whether to use sticky_actions before MaxAndSkip wrapper.\n\n  Returns:\n    An instance of `gym.Env` or `gym.Wrapper`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Union(self, t2, off):\n        \"\"\"\"\"\"\n        assert type(t2) is Table\n        N.enforce_number(off, N.UOffsetTFlags)\n\n        off += self.Pos\n        t2.Pos = off + self.Get(N.UOffsetTFlags, off)\n        t2.Bytes = self.Bytes", "output": "Union initializes any Table-derived type to point to the union at\n           the given offset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def template_click_option(include_build=True):\n    \"\"\"\n    \n    \"\"\"\n    return click.option('--template', '-t',\n                        default=_TEMPLATE_OPTION_DEFAULT_VALUE,\n                        type=click.Path(),\n                        envvar=\"SAM_TEMPLATE_FILE\",\n                        callback=partial(get_or_default_template_file_name, include_build=include_build),\n                        show_default=True,\n                        help=\"AWS SAM template file\")", "output": "Click Option for template option", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reboot(vm_):\n    '''\n    \n    '''\n    with _get_xapi_session() as xapi:\n        vm_uuid = _get_label_uuid(xapi, 'VM', vm_)\n        if vm_uuid is False:\n            return False\n        try:\n            xapi.VM.clean_reboot(vm_uuid)\n            return True\n        except Exception:\n            return False", "output": "Reboot a domain via ACPI request\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' virt.reboot <vm name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_table(engine, sql):\n    \"\"\" \n    \"\"\"\n    if engine.dialect.has_table(engine, sql):\n        return True\n    return False", "output": "Check with the given sql arg is query or table\n\n    Args:\n        engine: SQLAlchemy connection engine\n        sql: SQL query or table name\n\n    Returns:\n        True for table or False if not", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate(node, environment, name, filename, stream=None,\n             defer_init=False, optimized=True):\n    \"\"\"\"\"\"\n    if not isinstance(node, nodes.Template):\n        raise TypeError('Can\\'t compile non template nodes')\n    generator = environment.code_generator_class(environment, name, filename,\n                                                 stream, defer_init,\n                                                 optimized)\n    generator.visit(node)\n    if stream is None:\n        return generator.stream.getvalue()", "output": "Generate the python source for a node tree.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_ufunc(self, obj):\n        \"\"\"\"\"\"\n        name = obj.__name__\n        numpy_tst_mods = ['numpy', 'scipy.special']\n        for tst_mod_name in numpy_tst_mods:\n            tst_mod = sys.modules.get(tst_mod_name, None)\n            if tst_mod and name in tst_mod.__dict__:\n                return self.save_reduce(_getobject, (tst_mod_name, name))\n        raise pickle.PicklingError('cannot save %s. Cannot resolve what module it is defined in'\n                                   % str(obj))", "output": "Hack function for saving numpy ufunc objects", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def np_to_list(elem):\n  \"\"\"\"\"\"\n  if isinstance(elem, list):\n    return elem\n  elif isinstance(elem, tuple):\n    return list(elem)\n  elif isinstance(elem, np.ndarray):\n    return list(elem)\n  else:\n    raise ValueError(\n        'Input elements of a sequence should be either a numpy array, a '\n        'python list or tuple. Got {}'.format(type(elem)))", "output": "Returns list from list, tuple or ndarray.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def terminate(name, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The terminate action must be called with '\n            '-a or --action.'\n        )\n\n    vm_properties = [\n        \"name\",\n        \"summary.runtime.powerState\"\n    ]\n\n    vm_list = salt.utils.vmware.get_mors_with_properties(_get_si(), vim.VirtualMachine, vm_properties)\n\n    for vm in vm_list:\n        if vm[\"name\"] == name:\n            if vm[\"summary.runtime.powerState\"] == \"poweredOff\":\n                ret = 'already powered off'\n                log.info('VM %s %s', name, ret)\n                return ret\n            try:\n                log.info('Terminating VM %s', name)\n                vm[\"object\"].Terminate()\n            except Exception as exc:\n                log.error(\n                    'Error while terminating VM %s: %s',\n                    name, exc,\n                    # Show the traceback if the debug logging level is enabled\n                    exc_info_on_loglevel=logging.DEBUG\n                )\n                return 'failed to terminate'\n\n    return 'terminated'", "output": "To do an immediate power off of a VM using its name. A ``SIGKILL``\n    is issued to the vmx process of the VM\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a terminate vmname", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def removeRef(self, attr):\n        \"\"\" \"\"\"\n        if attr is None: attr__o = None\n        else: attr__o = attr._o\n        ret = libxml2mod.xmlRemoveRef(self._o, attr__o)\n        return ret", "output": "Remove the given attribute from the Ref table maintained\n           internally.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_backend_for_mayavi(self, command):\n        \"\"\"\n        \n        \"\"\"\n        calling_mayavi = False\n        lines = command.splitlines()\n        for l in lines:\n            if not l.startswith('#'):\n                if 'import mayavi' in l or 'from mayavi' in l:\n                    calling_mayavi = True\n                    break\n        if calling_mayavi:\n            message = _(\"Changing backend to Qt for Mayavi\")\n            self._append_plain_text(message + '\\n')\n            self.silent_execute(\"%gui inline\\n%gui qt\")", "output": "Mayavi plots require the Qt backend, so we try to detect if one is\n        generated to change backends", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _copy_dist_from_dir(link_path, location):\n    \"\"\"\n\n    \"\"\"\n\n    # Note: This is currently VERY SLOW if you have a lot of data in the\n    # directory, because it copies everything with `shutil.copytree`.\n    # What it should really do is build an sdist and install that.\n    # See https://github.com/pypa/pip/issues/2195\n\n    if os.path.isdir(location):\n        rmtree(location)\n\n    # build an sdist\n    setup_py = 'setup.py'\n    sdist_args = [sys.executable]\n    sdist_args.append('-c')\n    sdist_args.append(SETUPTOOLS_SHIM % setup_py)\n    sdist_args.append('sdist')\n    sdist_args += ['--dist-dir', location]\n    logger.info('Running setup.py sdist for %s', link_path)\n\n    with indent_log():\n        call_subprocess(sdist_args, cwd=link_path, show_stdout=False)\n\n    # unpack sdist into `location`\n    sdist = os.path.join(location, os.listdir(location)[0])\n    logger.info('Unpacking sdist %s into %s', sdist, location)\n    unpack_file(sdist, location, content_type=None, link=None)", "output": "Copy distribution files in `link_path` to `location`.\n\n    Invoked when user requests to install a local directory. E.g.:\n\n        pip install .\n        pip install ~/dev/git-repos/python-prompt-toolkit", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_protobuf(self):\n        \"\"\"\n        \"\"\"\n        pb = _pb_from_query(self._query)\n\n        start_cursor = self.next_page_token\n        if start_cursor is not None:\n            pb.start_cursor = base64.urlsafe_b64decode(start_cursor)\n\n        end_cursor = self._end_cursor\n        if end_cursor is not None:\n            pb.end_cursor = base64.urlsafe_b64decode(end_cursor)\n\n        if self.max_results is not None:\n            pb.limit.value = self.max_results - self.num_results\n\n        if start_cursor is None and self._offset is not None:\n            # NOTE: We don't need to add an offset to the request protobuf\n            #       if we are using an existing cursor, because the offset\n            #       is only relative to the start of the result set, not\n            #       relative to each page (this method is called per-page)\n            pb.offset = self._offset\n\n        return pb", "output": "Build a query protobuf.\n\n        Relies on the current state of the iterator.\n\n        :rtype:\n            :class:`.query_pb2.Query`\n        :returns: The query protobuf object for the current\n                  state of the iterator.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _construct_lambda_function(self):\n        \"\"\"\n        \"\"\"\n        lambda_function = LambdaFunction(self.logical_id, depends_on=self.depends_on,\n                                         attributes=self.resource_attributes)\n\n        if self.FunctionName:\n            lambda_function.FunctionName = self.FunctionName\n\n        lambda_function.Handler = self.Handler\n        lambda_function.Runtime = self.Runtime\n        lambda_function.Description = self.Description\n        lambda_function.MemorySize = self.MemorySize\n        lambda_function.Timeout = self.Timeout\n        lambda_function.VpcConfig = self.VpcConfig\n        lambda_function.Role = self.Role\n        lambda_function.Environment = self.Environment\n        lambda_function.Code = self._construct_code_dict()\n        lambda_function.KmsKeyArn = self.KmsKeyArn\n        lambda_function.ReservedConcurrentExecutions = self.ReservedConcurrentExecutions\n        lambda_function.Tags = self._construct_tag_list(self.Tags)\n        lambda_function.Layers = self.Layers\n\n        if self.Tracing:\n            lambda_function.TracingConfig = {\"Mode\": self.Tracing}\n\n        if self.DeadLetterQueue:\n            lambda_function.DeadLetterConfig = {\"TargetArn\": self.DeadLetterQueue['TargetArn']}\n\n        return lambda_function", "output": "Constructs and returns the Lambda function.\n\n        :returns: a list containing the Lambda function and execution role resources\n        :rtype: list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unset_config_value(self, name, quiet=False):\n        \"\"\"\n        \"\"\"\n\n        config_data = self._read_config_file()\n\n        if name in config_data:\n\n            del config_data[name]\n\n            self._write_config_file(config_data)\n\n            if not quiet:\n                self.print_config_value(name, separator=' is now set to: ')", "output": "unset a configuration value\n            Parameters\n           ==========\n           name: the name of the value to unset (remove key in dictionary)\n           quiet: disable verbose output if True (default is False)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def query_params(*es_query_params):\n    \"\"\"\n    \n    \"\"\"\n\n    def _wrapper(func):\n        @wraps(func)\n        def _wrapped(*args, **kwargs):\n            params = {}\n            if \"params\" in kwargs:\n                params = kwargs.pop(\"params\").copy()\n            for p in es_query_params + GLOBAL_PARAMS:\n                if p in kwargs:\n                    v = kwargs.pop(p)\n                    if v is not None:\n                        params[p] = _escape(v)\n\n            # don't treat ignore and request_timeout as other params to avoid escaping\n            for p in (\"ignore\", \"request_timeout\"):\n                if p in kwargs:\n                    params[p] = kwargs.pop(p)\n            return func(*args, params=params, **kwargs)\n\n        return _wrapped\n\n    return _wrapper", "output": "Decorator that pops all accepted parameters from method's kwargs and puts\n    them in the params argument.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _visual_width(line):\n    \"\"\"\"\"\"\n\n    return len(re.sub(colorama.ansitowin32.AnsiToWin32.ANSI_CSI_RE, \"\", line))", "output": "Get the the number of columns required to display a string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def MaxPooling(\n        inputs,\n        pool_size,\n        strides=None,\n        padding='valid',\n        data_format='channels_last'):\n    \"\"\"\n    \n    \"\"\"\n    if strides is None:\n        strides = pool_size\n    layer = tf.layers.MaxPooling2D(pool_size, strides, padding=padding, data_format=data_format)\n    ret = layer.apply(inputs, scope=tf.get_variable_scope())\n    return tf.identity(ret, name='output')", "output": "Same as `tf.layers.MaxPooling2D`. Default strides is equal to pool_size.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _siftdown_max(heap, startpos, pos):\n    ''\n    newitem = heap[pos]\n    # Follow the path to the root, moving parents down until finding a place\n    # newitem fits.\n    while pos > startpos:\n        parentpos = (pos - 1) >> 1\n        parent = heap[parentpos]\n        if parent < newitem:\n            heap[pos] = parent\n            pos = parentpos\n            continue\n        break\n    heap[pos] = newitem", "output": "Maxheap variant of _siftdown", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mode_number(self, rows: List[Row], column: NumberColumn) -> Number:\n        \"\"\"\n        \n        \"\"\"\n        most_frequent_list = self._get_most_frequent_values(rows, column)\n        if not most_frequent_list:\n            return 0.0  # type: ignore\n        most_frequent_value = most_frequent_list[0]\n        if not isinstance(most_frequent_value, Number):\n            raise ExecutionError(f\"Invalid valus for mode_number: {most_frequent_value}\")\n        return most_frequent_value", "output": "Takes a list of rows and a column and returns the most frequent value under\n        that column in those rows.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge(left, right, merged):\n    \"\"\" \n    \"\"\"\n\n    left_cursor, right_cursor = 0, 0\n    while left_cursor < len(left) and right_cursor < len(right):\n        # Sort each one and place into the result\n        if left[left_cursor] <= right[right_cursor]:\n            merged[left_cursor+right_cursor]=left[left_cursor]\n            left_cursor += 1\n        else:\n            merged[left_cursor + right_cursor] = right[right_cursor]\n            right_cursor += 1\n    # Add the left overs if there's any left to the result\n    for left_cursor in range(left_cursor, len(left)):\n        merged[left_cursor + right_cursor] = left[left_cursor]\n    # Add the left overs if there's any left to the result\n    for right_cursor in range(right_cursor, len(right)):\n        merged[left_cursor + right_cursor] = right[right_cursor]\n\n    # Return result\n    return merged", "output": "Merge helper\n        Complexity: O(n)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def MessageSetItemSizer(field_number):\n  \"\"\"\n  \"\"\"\n  static_size = (_TagSize(1) * 2 + _TagSize(2) + _VarintSize(field_number) +\n                 _TagSize(3))\n  local_VarintSize = _VarintSize\n\n  def FieldSize(value):\n    l = value.ByteSize()\n    return static_size + local_VarintSize(l) + l\n\n  return FieldSize", "output": "Returns a sizer for extensions of MessageSet.\n\n  The message set message looks like this:\n    message MessageSet {\n      repeated group Item = 1 {\n        required int32 type_id = 2;\n        required string message = 3;\n      }\n    }", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def host_update(hostid, **kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'host.update'\n            params = {\"hostid\": hostid}\n            params = _params_extend(params, _ignore_name=True, **kwargs)\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result']['hostids']\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": ".. versionadded:: 2016.3.0\n\n    Update existing hosts\n\n    .. note::\n        This function accepts all standard host and host.update properties:\n        keyword argument names differ depending on your zabbix version, see the\n        documentation for `host objects`_ and the documentation for `updating\n        hosts`_.\n\n        .. _`host objects`: https://www.zabbix.com/documentation/2.4/manual/api/reference/host/object#host\n        .. _`updating hosts`: https://www.zabbix.com/documentation/2.4/manual/api/reference/host/update\n\n    :param hostid: ID of the host to update\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n    :param visible_name: string with visible name of the host, use\n        'visible_name' instead of 'name' parameter to not mess with value\n        supplied from Salt sls file.\n\n    :return: ID of the updated host.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.host_update 10084 name='Zabbix server2'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_rule(name,\n             localport=None,\n             protocol=None,\n             dir=None,\n             remoteip=None):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'result': True,\n           'changes': {},\n           'comment': ''}\n\n    # Check if rule exists\n    if __salt__['firewall.rule_exists'](name):\n        ret['changes'] = {'delete rule': name}\n    else:\n        ret['comment'] = 'A rule with that name does not exist'\n        return ret\n\n    if __opts__['test']:\n        ret['result'] = not ret['changes'] or None\n        ret['comment'] = ret['changes']\n        ret['changes'] = {}\n        return ret\n\n    # Delete rule\n    try:\n        __salt__['firewall.delete_rule'](\n            name, localport, protocol, dir, remoteip)\n    except CommandExecutionError:\n        ret['comment'] = 'Could not delete rule'\n\n    return ret", "output": "Delete an existing firewall rule identified by name and optionally by ports,\n    protocols, direction, and remote IP.\n\n    .. versionadded:: Neon\n\n    Args:\n\n        name (str): The name of the rule to delete. If the name ``all`` is used\n            you must specify additional parameters.\n\n        localport (Optional[str]): The port of the rule. If protocol is not\n            specified, protocol will be set to ``tcp``\n\n        protocol (Optional[str]): The protocol of the rule. Default is ``tcp``\n            when ``localport`` is specified\n\n        dir (Optional[str]): The direction of the rule.\n\n        remoteip (Optional[str]): The remote IP of the rule.\n\n    Example:\n\n    .. code-block:: yaml\n\n        delete_smb_port_rule:\n          win_firewall.delete_rule:\n            - name: SMB (445)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def print_class_histogram(roidbs):\n    \"\"\"\n    \n    \"\"\"\n    dataset = DetectionDataset()\n    hist_bins = np.arange(dataset.num_classes + 1)\n\n    # Histogram of ground-truth objects\n    gt_hist = np.zeros((dataset.num_classes,), dtype=np.int)\n    for entry in roidbs:\n        # filter crowd?\n        gt_inds = np.where(\n            (entry['class'] > 0) & (entry['is_crowd'] == 0))[0]\n        gt_classes = entry['class'][gt_inds]\n        gt_hist += np.histogram(gt_classes, bins=hist_bins)[0]\n    data = [[dataset.class_names[i], v] for i, v in enumerate(gt_hist)]\n    data.append(['total', sum(x[1] for x in data)])\n    # the first line is BG\n    table = tabulate(data[1:], headers=['class', '#box'], tablefmt='pipe')\n    logger.info(\"Ground-Truth Boxes:\\n\" + colored(table, 'cyan'))", "output": "Args:\n        roidbs (list[dict]): the same format as the output of `load_training_roidbs`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_fields(cls, cols):\n        \"\"\"\n        \n        \"\"\"\n        return [sqla.literal_column(c.get('name')).label(c.get('name').replace('.', '__'))\n                for c in cols]", "output": "BigQuery dialect requires us to not use backtick in the fieldname which are\n        nested.\n        Using literal_column handles that issue.\n        https://docs.sqlalchemy.org/en/latest/core/tutorial.html#using-more-specific-text-with-table-literal-column-and-column\n        Also explicility specifying column names so we don't encounter duplicate\n        column names in the result.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def callJavaFunc(sc, func, *args):\n    \"\"\"  \"\"\"\n    args = [_py2java(sc, a) for a in args]\n    return _java2py(sc, func(*args))", "output": "Call Java Function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def notebook_type(self, notebook_type):\n        ''' \n\n        '''\n        if notebook_type is None or not isinstance(notebook_type, string_types):\n            raise ValueError(\"Notebook type must be a string\")\n        self._notebook_type = notebook_type.lower()", "output": "Notebook type, acceptable values are 'jupyter' as well as any names\n        defined by external notebook hooks that have been installed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lstm_cell(inputs,\n              state,\n              num_units,\n              use_peepholes=False,\n              cell_clip=0.0,\n              initializer=None,\n              num_proj=None,\n              num_unit_shards=None,\n              num_proj_shards=None,\n              reuse=None,\n              name=None):\n  \"\"\"\"\"\"\n  input_shape = common_layers.shape_list(inputs)\n  cell = tf.nn.rnn_cell.LSTMCell(num_units,\n                                 use_peepholes=use_peepholes,\n                                 cell_clip=cell_clip,\n                                 initializer=initializer,\n                                 num_proj=num_proj,\n                                 num_unit_shards=num_unit_shards,\n                                 num_proj_shards=num_proj_shards,\n                                 reuse=reuse,\n                                 name=name,\n                                 state_is_tuple=False)\n  if state is None:\n    state = cell.zero_state(input_shape[0], tf.float32)\n  outputs, new_state = cell(inputs, state)\n  return outputs, new_state", "output": "Full LSTM cell.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transform_y(self, tfms:TfmList=None, **kwargs):\n        \"\"\n        _check_kwargs(self.y, tfms, **kwargs)\n        self.tfm_y=True\n        if tfms is None:\n            self.tfms_y = list(filter(lambda t: t.use_on_y, listify(self.tfms)))\n            self.tfmargs_y = {**self.tfmargs, **kwargs}\n        else:\n            tfms = list(filter(lambda t: t.use_on_y, tfms))\n            self.tfms_y,self.tfmargs_y = tfms,kwargs\n        return self", "output": "Set `tfms` to be applied to the targets only.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_dvportgroup_teaming(pg_name, pg_default_port_config):\n    '''\n    \n    '''\n    log.trace('Retrieving portgroup\\'s \\'%s\\' teaming config', pg_name)\n    teaming_policy = pg_default_port_config.uplinkTeamingPolicy\n    if not teaming_policy:\n        return {}\n    ret_dict = {'notify_switches': teaming_policy.notifySwitches.value,\n                'policy': teaming_policy.policy.value,\n                'reverse_policy': teaming_policy.reversePolicy.value,\n                'rolling_order': teaming_policy.rollingOrder.value}\n    if teaming_policy.failureCriteria:\n        failure_criteria = teaming_policy.failureCriteria\n        ret_dict.update({'failure_criteria': {\n            'check_beacon': failure_criteria.checkBeacon.value,\n            'check_duplex': failure_criteria.checkDuplex.value,\n            'check_error_percent': failure_criteria.checkErrorPercent.value,\n            'check_speed': failure_criteria.checkSpeed.value,\n            'full_duplex': failure_criteria.fullDuplex.value,\n            'percentage': failure_criteria.percentage.value,\n            'speed': failure_criteria.speed.value}})\n    if teaming_policy.uplinkPortOrder:\n        uplink_order = teaming_policy.uplinkPortOrder\n        ret_dict.update({'port_order': {\n            'active': uplink_order.activeUplinkPort,\n            'standby': uplink_order.standbyUplinkPort}})\n    return ret_dict", "output": "Returns the teaming of a distributed virtual portgroup\n\n    pg_name\n        The name of the portgroup\n\n    pg_default_port_config\n        The dafault port config of the portgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def activate_api_deployment(restApiId, stageName, deploymentId,\n                            region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        response = conn.update_stage(restApiId=restApiId, stageName=stageName,\n                                     patchOperations=[{'op': 'replace',\n                                                       'path': '/deploymentId',\n                                                       'value': deploymentId}])\n        return {'set': True, 'response': _convert_datetime_str(response)}\n    except ClientError as e:\n        return {'set': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Activates previously deployed deployment for a given stage\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_apigateway.activate_api_deployent restApiId stagename deploymentId", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_subset(self, other):\n        \"\"\"\"\"\"\n        if isinstance(other, Permissions):\n            return (self.value & other.value) == self.value\n        else:\n            raise TypeError(\"cannot compare {} with {}\".format(self.__class__.__name__, other.__class__.__name__))", "output": "Returns True if self has the same or fewer permissions as other.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_series_convert_timestamps_localize(s, from_timezone, to_timezone):\n    \"\"\"\n    \n    \"\"\"\n    from pyspark.sql.utils import require_minimum_pandas_version\n    require_minimum_pandas_version()\n\n    import pandas as pd\n    from pandas.api.types import is_datetime64tz_dtype, is_datetime64_dtype\n    from_tz = from_timezone or _get_local_timezone()\n    to_tz = to_timezone or _get_local_timezone()\n    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?\n    if is_datetime64tz_dtype(s.dtype):\n        return s.dt.tz_convert(to_tz).dt.tz_localize(None)\n    elif is_datetime64_dtype(s.dtype) and from_tz != to_tz:\n        # `s.dt.tz_localize('tzlocal()')` doesn't work properly when including NaT.\n        return s.apply(\n            lambda ts: ts.tz_localize(from_tz, ambiguous=False).tz_convert(to_tz).tz_localize(None)\n            if ts is not pd.NaT else pd.NaT)\n    else:\n        return s", "output": "Convert timestamp to timezone-naive in the specified timezone or local timezone\n\n    :param s: a pandas.Series\n    :param from_timezone: the timezone to convert from. if None then use local timezone\n    :param to_timezone: the timezone to convert to. if None then use local timezone\n    :return pandas.Series where if it is a timestamp, has been converted to tz-naive", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _iget_item_cache(self, item):\n        \"\"\"\"\"\"\n        ax = self._info_axis\n        if ax.is_unique:\n            lower = self._get_item_cache(ax[item])\n        else:\n            lower = self._take(item, axis=self._info_axis_number)\n        return lower", "output": "Return the cached item, item represents a positional indexer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def FilterItems(self, filterFn):\n    \"\"\"\n    \"\"\"\n    with self._mutex:\n      size_before = len(self.items)\n      self.items = list(filter(filterFn, self.items))\n      size_diff = size_before - len(self.items)\n\n      # Estimate a correction the number of items seen\n      prop_remaining = len(self.items) / float(\n          size_before) if size_before > 0 else 0\n      self._num_items_seen = int(round(self._num_items_seen * prop_remaining))\n      return size_diff", "output": "Filter items in a ReservoirBucket, using a filtering function.\n\n    Filtering items from the reservoir bucket must update the\n    internal state variable self._num_items_seen, which is used for determining\n    the rate of replacement in reservoir sampling. Ideally, self._num_items_seen\n    would contain the exact number of items that have ever seen by the\n    ReservoirBucket and satisfy filterFn. However, the ReservoirBucket does not\n    have access to all items seen -- it only has access to the subset of items\n    that have survived sampling (self.items). Therefore, we estimate\n    self._num_items_seen by scaling it by the same ratio as the ratio of items\n    not removed from self.items.\n\n    Args:\n      filterFn: A function that returns True for items to be kept.\n\n    Returns:\n      The number of items removed from the bucket.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def disable(name, stop=False, **kwargs):\n    '''\n    \n    '''\n\n    # non-existent as registrered service\n    if not enabled(name):\n        return False\n\n    # down_file: file that prevent sv autostart\n    svc_realpath = _get_svc_path(name)[0]\n    down_file = os.path.join(svc_realpath, 'down')\n\n    if stop:\n        stop(name)\n\n    if not os.path.exists(down_file):\n        try:\n            salt.utils.files.fopen(down_file, \"w\").close()  # pylint: disable=resource-leakage\n        except IOError:\n            log.error('Unable to create file %s', down_file)\n            return False\n\n    return True", "output": "Don't start service ``name`` at boot\n    Returns ``True`` if operation is successful\n\n    name\n        the service's name\n\n    stop\n        if True, also stops the service\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.disable <name> [stop=True]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_tall_pretrain_lm_tpu():\n  \"\"\"\"\"\"\n  hparams = transformer_tall_pretrain_lm_tpu_adafactor()\n  # Optimizer gets reset in update_hparams_for_tpu so we set it again here.\n  hparams.learning_rate_constant = 2e-4\n  hparams.learning_rate_schedule = (\"linear_warmup * constant * cosdecay\")\n  hparams.optimizer = \"adam_w\"\n  return hparams", "output": "Hparams for transformer on LM pretraining on TPU with AdamW.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup(self):\n        \"\"\"\n        \"\"\"\n        if os.path.lexists(self.dest_path):\n            if not self.overwrite:\n                raise OSError(errno.EEXIST,\n                              'Overwrite disabled and file already exists',\n                              self.dest_path)\n        if self.overwrite_part and os.path.lexists(self.part_path):\n            os.unlink(self.part_path)\n        self._open_part_file()\n        return", "output": "Called on context manager entry (the :keyword:`with` statement),\n        the ``setup()`` method creates the temporary file in the same\n        directory as the destination file.\n\n        ``setup()`` tests for a writable directory with rename permissions\n        early, as the part file may not be written to immediately (not\n        using :func:`os.access` because of the potential issues of\n        effective vs. real privileges).\n\n        If the caller is not using the :class:`AtomicSaver` as a\n        context manager, this method should be called explicitly\n        before writing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_balancers_list(resource_group, **kwargs):\n    '''\n    \n\n    '''\n    result = {}\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        load_balancers = __utils__['azurearm.paged_object_to_list'](\n            netconn.load_balancers.list(\n                resource_group_name=resource_group\n            )\n        )\n\n        for load_balancer in load_balancers:\n            result[load_balancer['name']] = load_balancer\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    List all load balancers within a resource group.\n\n    :param resource_group: The resource group name to list load balancers\n        within.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.load_balancers_list testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_indicator_RSI(DataFrame, N1=12, N2=26, N3=9):\n    ''\n    CLOSE = DataFrame['close']\n    LC = REF(CLOSE, 1)\n    RSI1 = SMA(MAX(CLOSE - LC, 0), N1) / SMA(ABS(CLOSE - LC), N1) * 100\n    RSI2 = SMA(MAX(CLOSE - LC, 0), N2) / SMA(ABS(CLOSE - LC), N2) * 100\n    RSI3 = SMA(MAX(CLOSE - LC, 0), N3) / SMA(ABS(CLOSE - LC), N3) * 100\n    DICT = {'RSI1': RSI1, 'RSI2': RSI2, 'RSI3': RSI3}\n\n    return pd.DataFrame(DICT)", "output": "\u76f8\u5bf9\u5f3a\u5f31\u6307\u6807RSI1:SMA(MAX(CLOSE-LC,0),N1,1)/SMA(ABS(CLOSE-LC),N1,1)*100;", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def broadcast(df):\n    \"\"\"\"\"\"\n\n    sc = SparkContext._active_spark_context\n    return DataFrame(sc._jvm.functions.broadcast(df._jdf), df.sql_ctx)", "output": "Marks a DataFrame as small enough for use in broadcast joins.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def serialize(input, tree=\"etree\", encoding=None, **serializer_opts):\n    \"\"\"\n\n    \"\"\"\n    # XXX: Should we cache this?\n    walker = treewalkers.getTreeWalker(tree)\n    s = HTMLSerializer(**serializer_opts)\n    return s.render(walker(input), encoding)", "output": "Serializes the input token stream using the specified treewalker\n\n    :arg input: the token stream to serialize\n\n    :arg tree: the treewalker to use\n\n    :arg encoding: the encoding to use\n\n    :arg serializer_opts: any options to pass to the\n        :py:class:`html5lib.serializer.HTMLSerializer` that gets created\n\n    :returns: the tree serialized as a string\n\n    Example:\n\n    >>> from html5lib.html5parser import parse\n    >>> from html5lib.serializer import serialize\n    >>> token_stream = parse('<html><body><p>Hi!</p></body></html>')\n    >>> serialize(token_stream, omit_optional_tags=False)\n    '<html><head></head><body><p>Hi!</p></body></html>'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resize(self, size:Union[int,TensorImageSize])->'Image':\n        \"\"\n        assert self._flow is None\n        if isinstance(size, int): size=(self.shape[0], size, size)\n        if tuple(size)==tuple(self.shape): return self\n        self.flow = _affine_grid(size)\n        return self", "output": "Resize the image to `size`, size can be a single int.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def auth(username, password):\n    '''\n    \n    '''\n    django_auth_path = __opts__['django_auth_path']\n    if django_auth_path not in sys.path:\n        sys.path.append(django_auth_path)\n    os.environ.setdefault('DJANGO_SETTINGS_MODULE', __opts__['django_auth_settings'])\n\n    __django_auth_setup()\n\n    if not is_connection_usable():\n        connection.close()\n\n    import django.contrib.auth  # pylint: disable=import-error,3rd-party-module-not-gated\n    user = django.contrib.auth.authenticate(username=username, password=password)\n    if user is not None:\n        if user.is_active:\n            log.debug('Django authentication successful')\n            return True\n        else:\n            log.debug('Django authentication: the password is valid but the account is disabled.')\n    else:\n        log.debug('Django authentication failed.')\n\n    return False", "output": "Simple Django auth", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(name, profile=\"splunk\", **kwargs):\n    '''\n    \n    '''\n    client = _get_splunk(profile)\n    search = client.saved_searches.create(name, **kwargs)\n\n    # use the REST API to set owner and permissions\n    # this is hard-coded for now; all managed searches are app scope and\n    # readable by all\n    config = __salt__['config.option'](profile)\n    url = \"https://{0}:{1}\".format(config.get('host'), config.get('port'))\n    auth = (config.get('username'), config.get('password'))\n    data = {\n        \"owner\": config.get(\"username\"),\n        \"sharing\": \"app\",\n        \"perms.read\": \"*\",\n    }\n    _req_url = \"{0}/servicesNS/{1}/search/saved/searches/{2}/acl\".format(\n        url, config.get(\"username\"), urllib.quote(name)\n    )\n    requests.post(_req_url, auth=auth, verify=True, data=data)\n    return _get_splunk_search_props(search)", "output": "Create a splunk search\n\n    CLI Example:\n\n        splunk_search.create 'my search name' search='error msg'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def project(self):\n        \"\"\"\n        \"\"\"\n        if self._project is None:\n            _, self._project = google.auth.default()\n        return self._project", "output": "str: Default project to use for queries performed through IPython\n        magics\n\n        Note:\n            The project does not need to be explicitly defined if you have an\n            environment default project set. If you do not have a default\n            project set in your environment, manually assign the project as\n            demonstrated in the example below.\n\n        Example:\n            Manually setting the context project:\n\n            >>> from google.cloud.bigquery import magics\n            >>> magics.context.project = 'my-project'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dict(self, time, code):\n        '''\n        \n        '''\n        try:\n            return self.dicts[(QA_util_to_datetime(time), str(code))]\n        except Exception as e:\n            raise e", "output": "'give the time,code tuple and turn the dict'\n        :param time:\n        :param code:\n        :return:  \u5b57\u5178dict \u7c7b\u578b", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_virtual_machine_scale_set_vm_network_interfaces(scale_set,\n                                                         vm_index,\n                                                         resource_group,\n                                                         **kwargs):\n    '''\n    \n\n    '''\n    result = {}\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        nics = __utils__['azurearm.paged_object_to_list'](\n            netconn.network_interfaces.list_virtual_machine_scale_set_vm_network_interfaces(\n                virtual_machine_scale_set_name=scale_set,\n                virtualmachine_index=vm_index,\n                resource_group_name=resource_group\n            )\n        )\n\n        for nic in nics:\n            result[nic['name']] = nic\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Get information about all network interfaces in a specific virtual machine within a scale set.\n\n    :param scale_set: The name of the scale set to query.\n\n    :param vm_index: The virtual machine index.\n\n    :param resource_group: The resource group name assigned to the\n        scale set.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.list_virtual_machine_scale_set_vm_network_interfaces testset testvm testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def eye(N, M=0, k=0, dtype=None, **kwargs):\n    \"\"\"\n    \"\"\"\n    if dtype is None:\n        dtype = _numpy.float32\n    return _internal._eye(N, M, k, dtype=dtype, **kwargs)", "output": "Returns a new symbol of 2-D shpae, filled with ones on the diagonal and zeros elsewhere.\n\n    Parameters\n    ----------\n    N: int\n        Number of rows in the output.\n    M: int, optional\n        Number of columns in the output. If 0, defaults to N.\n    k: int, optional\n        Index of the diagonal: 0 (the default) refers to the main diagonal,\n        a positive value refers to an upper diagonal,\n        and a negative value to a lower diagonal.\n    dtype : str or numpy.dtype, optional\n        The value type of the inner value, default to ``np.float32``.\n\n    Returns\n    -------\n    out : Symbol\n        The created Symbol.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_qstat_state(qstat_out, job_id):\n    \"\"\"\n\n    \"\"\"\n    if qstat_out.strip() == '':\n        return 'u'\n    lines = qstat_out.split('\\n')\n    # skip past header\n    while not lines.pop(0).startswith('---'):\n        pass\n    for line in lines:\n        if line:\n            job, prior, name, user, state = line.strip().split()[0:5]\n            if int(job) == int(job_id):\n                return state\n    return 'u'", "output": "Parse \"state\" column from `qstat` output for given job_id\n\n    Returns state for the *first* job matching job_id. Returns 'u' if\n    `qstat` output is empty or job_id is not found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def retention_period(self, value):\n        \"\"\"\n        \"\"\"\n        policy = self._properties.setdefault(\"retentionPolicy\", {})\n        if value is not None:\n            policy[\"retentionPeriod\"] = str(value)\n        else:\n            policy = None\n        self._patch_property(\"retentionPolicy\", policy)", "output": "Set the retention period for items in the bucket.\n\n        :type value: int\n        :param value:\n            number of seconds to retain items after upload or release from\n            event-based lock.\n\n        :raises ValueError: if the bucket's retention policy is locked.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ConvertAnyMessage(self, value, message):\n    \"\"\"\"\"\"\n    if isinstance(value, dict) and not value:\n      return\n    try:\n      type_url = value['@type']\n    except KeyError:\n      raise ParseError('@type is missing when parsing any message.')\n\n    sub_message = _CreateMessageFromTypeUrl(type_url)\n    message_descriptor = sub_message.DESCRIPTOR\n    full_name = message_descriptor.full_name\n    if _IsWrapperMessage(message_descriptor):\n      self._ConvertWrapperMessage(value['value'], sub_message)\n    elif full_name in _WKTJSONMETHODS:\n      methodcaller(\n          _WKTJSONMETHODS[full_name][1], value['value'], sub_message)(self)\n    else:\n      del value['@type']\n      self._ConvertFieldValuePair(value, sub_message)\n    # Sets Any message\n    message.value = sub_message.SerializeToString()\n    message.type_url = type_url", "output": "Convert a JSON representation into Any message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _rfc3339_to_datetime(dt_str):\n    \"\"\"\n    \"\"\"\n    return datetime.datetime.strptime(dt_str, _RFC3339_MICROS).replace(tzinfo=UTC)", "output": "Convert a microsecond-precision timestamp to a native datetime.\n\n    :type dt_str: str\n    :param dt_str: The string to convert.\n\n    :rtype: :class:`datetime.datetime`\n    :returns: The datetime object created from the string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gradient_helper(optimizer, loss, var_list=None):\n    '''\n    '''\n    if var_list is None:\n      var_list = tf.compat.v1.trainable_variables()\n\n    grads_and_vars = optimizer.compute_gradients(loss, var_list=var_list)\n    grads = [pair[0] for pair in grads_and_vars]\n\n    return grads, optimizer.apply_gradients(grads_and_vars)", "output": "A helper to get the gradients out at each step.\n\n    Args:\n      optimizer: the optimizer op.\n      loss: the op that computes your loss value.\n\n    Returns: the gradient tensors and the train_step op.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delegate_names(delegate, accessors, typ, overwrite=False):\n    \"\"\"\n    \n    \"\"\"\n    def add_delegate_accessors(cls):\n        cls._add_delegate_accessors(delegate, accessors, typ,\n                                    overwrite=overwrite)\n        return cls\n\n    return add_delegate_accessors", "output": "Add delegated names to a class using a class decorator.  This provides\n    an alternative usage to directly calling `_add_delegate_accessors`\n    below a class definition.\n\n    Parameters\n    ----------\n    delegate : object\n        the class to get methods/properties & doc-strings\n    accessors : Sequence[str]\n        List of accessor to add\n    typ : {'property', 'method'}\n    overwrite : boolean, default False\n       overwrite the method/property in the target class if it exists\n\n    Returns\n    -------\n    callable\n        A class decorator.\n\n    Examples\n    --------\n    @delegate_names(Categorical, [\"categories\", \"ordered\"], \"property\")\n    class CategoricalAccessor(PandasDelegate):\n        [...]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_tall_pretrain_lm():\n  \"\"\"\"\"\"\n  hparams = transformer_tall()\n  hparams.learning_rate_constant = 2e-4\n  hparams.learning_rate_schedule = (\"linear_warmup*constant*cosdecay\")\n  hparams.optimizer = \"adam_w\"\n  hparams.optimizer_adam_beta1 = 0.9\n  hparams.optimizer_adam_beta2 = 0.999\n  hparams.optimizer_adam_epsilon = 1e-8\n  # Set max examples to something big when pretraining only the LM, definitely\n  # something an order of magnitude bigger than number of train steps.\n  hparams.multiproblem_schedule_max_examples = 5e8\n  # Set train steps to learning_rate_decay_steps or less\n  hparams.learning_rate_decay_steps = 5000000\n  return hparams", "output": "Hparams for transformer on LM pretraining (with 64k vocab).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def open(self, path, binary=False):\n        \"\"\"\"\"\"\n        if binary:\n            return open(path, \"rb\")\n        return open(path, encoding=\"utf-8\")", "output": "Open file and return a stream.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def edit(self, *, name, roles=None, reason=None):\n        \n        \"\"\"\n\n        if roles:\n            roles = [role.id for role in roles]\n        await self._state.http.edit_custom_emoji(self.guild.id, self.id, name=name, roles=roles, reason=reason)", "output": "r\"\"\"|coro|\n\n        Edits the custom emoji.\n\n        You must have :attr:`~Permissions.manage_emojis` permission to\n        do this.\n\n        Parameters\n        -----------\n        name: :class:`str`\n            The new emoji name.\n        roles: Optional[list[:class:`Role`]]\n            A :class:`list` of :class:`Role`\\s that can use this emoji. Leave empty to make it available to everyone.\n        reason: Optional[:class:`str`]\n            The reason for editing this emoji. Shows up on the audit log.\n\n        Raises\n        -------\n        Forbidden\n            You are not allowed to edit emojis.\n        HTTPException\n            An error occurred editing the emoji.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __output_path(toolchain, rule, output_dir):\n    \"\"\"\"\"\"\n    filename = '%s_%s.json' % (toolchain, rule)\n    return os.path.join(output_dir, filename)", "output": "Gets the output path for a file given the toolchain, rule and output_dir", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode_label(label_data):\n    \"\"\"\n    \"\"\"\n    systole = label_data[:, 1]\n    diastole = label_data[:, 2]\n    systole_encode = np.array([\n            (x < np.arange(600)) for x in systole\n        ], dtype=np.uint8)\n    diastole_encode = np.array([\n            (x < np.arange(600)) for x in diastole\n        ], dtype=np.uint8)\n    return systole_encode, diastole_encode", "output": "Run encoding to encode the label into the CDF target.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_archive_formats():\n    \"\"\"\n    \"\"\"\n    formats = [(name, registry[2]) for name, registry in\n               _ARCHIVE_FORMATS.items()]\n    formats.sort()\n    return formats", "output": "Returns a list of supported formats for archiving and unarchiving.\n\n    Each element of the returned sequence is a tuple (name, description)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _format_output(content, typ):\n    \"\"\"\n    \"\"\"\n\n    if \"csv\" in str(typ):\n        return _format_csv(content, delimiter=\",\")\n\n    if \"tsv\" in str(typ):\n        return _format_csv(content, delimiter=\"\\t\")\n\n    return content", "output": "Tabularize the content according to its type.\n\n    Args:\n        content (str): The content of a metric.\n        typ (str): The type of metric -- (raw|json|tsv|htsv|csv|hcsv).\n\n    Returns:\n        str: Content in a raw or tabular format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_build_env(env):\n    '''\n    \n    '''\n    env_override = ''\n    if env is None:\n        return env_override\n    if not isinstance(env, dict):\n        raise SaltInvocationError(\n            '\\'env\\' must be a Python dictionary'\n        )\n    for key, value in env.items():\n        env_override += '{0}={1}\\n'.format(key, value)\n        env_override += 'export {0}\\n'.format(key)\n    return env_override", "output": "Get build environment overrides dictionary to use in build process", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def detach(self):\n        \"\"\"\"\"\"\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is not None and self._registry.pop(self, None):\n            return (obj, info.func, info.args, info.kwargs or {})", "output": "If alive then mark as dead and return (obj, func, args, kwargs);\n        otherwise return None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copymode(src, dst):\n    \"\"\"\"\"\"\n    if hasattr(os, 'chmod'):\n        st = os.stat(src)\n        mode = stat.S_IMODE(st.st_mode)\n        os.chmod(dst, mode)", "output": "Copy mode bits from src to dst", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_epoch_end(self, pbar, epoch, last_metrics, **kwargs):\n        \"\"\n        if not hasattr(self, 'last_gen') or not self.show_img: return\n        data = self.learn.data\n        img = self.last_gen[0]\n        norm = getattr(data,'norm',False)\n        if norm and norm.keywords.get('do_y',False): img = data.denorm(img)\n        img = data.train_ds.y.reconstruct(img)\n        self.imgs.append(img)\n        self.titles.append(f'Epoch {epoch}')\n        pbar.show_imgs(self.imgs, self.titles)\n        return add_metrics(last_metrics, [getattr(self.smoothenerG,'smooth',None),getattr(self.smoothenerC,'smooth',None)])", "output": "Put the various losses in the recorder and show a sample image.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filter_by_folder(self, include=None, exclude=None):\n        \"\"\n        include,exclude = listify(include),listify(exclude)\n        def _inner(o):\n            if isinstance(o, Path): n = o.relative_to(self.path).parts[0]\n            else: n = o.split(os.path.sep)[len(str(self.path).split(os.path.sep))]\n            if include and not n in include: return False\n            if exclude and     n in exclude: return False\n            return True\n        return self.filter_by_func(_inner)", "output": "Only keep filenames in `include` folder or reject the ones in `exclude`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rlmb_long_stochastic_discrete_simulation_deterministic_starts():\n  \"\"\"\"\"\"\n  hparams = rlmb_base_stochastic_discrete()\n  hparams.generative_model_params = \"next_frame_basic_stochastic_discrete_long\"\n  hparams.ppo_epochs_num = 1000\n  hparams.simulation_random_starts = False\n  return hparams", "output": "Long setting with stochastic discrete model & deterministic sim starts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_popularity_baseline(self):\n\n        \"\"\"\n        \n        \"\"\"\n\n        response = self.__proxy__.get_popularity_baseline()\n        from .popularity_recommender import PopularityRecommender\n        return PopularityRecommender(response)", "output": "Returns a new popularity model matching the data set this model was\n        trained with.  Can be used for comparison purposes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_all(self) -> Iterable[Tuple[str, str]]:\n        \"\"\"\n        \"\"\"\n        for name, values in self._as_list.items():\n            for value in values:\n                yield (name, value)", "output": "Returns an iterable of all (name, value) pairs.\n\n        If a header has multiple values, multiple pairs will be\n        returned with the same name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def receive_trial_result(self, parameter_id, parameters, value):\n        \"\"\"\n        \"\"\"\n        reward = extract_scalar_reward(value)\n        if self.optimize_mode is OptimizeMode.Maximize:\n            reward = -reward\n\n        if parameter_id not in self.total_data:\n            raise RuntimeError('Received parameter_id not in total_data.')\n        if self.first_one:\n            self.smbo_solver.nni_smac_receive_first_run(self.total_data[parameter_id], reward)\n            self.first_one = False\n        else:\n            self.smbo_solver.nni_smac_receive_runs(self.total_data[parameter_id], reward)", "output": "receive_trial_result\n       \n        Parameters\n        ----------\n        parameter_id: int\n            parameter id\n        parameters:\n            parameters\n        value:\n            value\n        \n        Raises\n        ------\n        RuntimeError\n            Received parameter id not in total_data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def seed(range=10, hash=None):\n    '''\n    \n    '''\n    if hash is None:\n        hash = __grains__['id']\n\n    random.seed(hash)\n    return random.randrange(range)", "output": "Returns a random number within a range. Optional hash argument can\n    be any hashable object. If hash is omitted or None, the id of the minion is used.\n\n    .. versionadded: 2015.8.0\n\n    hash: None\n        Any hashable object.\n\n    range: 10\n        Any valid integer number\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' random.seed 10 hash=None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shuffle_and_batch_data(dataset,\n                           target_names,\n                           features_info,\n                           training,\n                           num_devices,\n                           shuffle_buffer_size=1024,\n                           preprocess_fun=no_preprocess):\n  \"\"\"\"\"\"\n  def append_targets(example):\n    \"\"\"Append targets to the example dictionary. Needed for Keras.\"\"\"\n    if len(target_names) == 1:\n      return (example, example[target_names[0]])\n    targets = {}\n    for name in target_names:\n      targets[name] = example[name]\n    return (example, targets)\n  dataset = dataset.map(append_targets)\n  if training:\n    dataset = dataset.repeat()\n    # Skip a random fraction at the beginning of the stream.  The skip is\n    # essential for synchronous highly-parallel training to avoid multiple\n    # replicas reading the same data in lock-step.\n    dataset = dataset.skip(random.randint(0, _MAX_SKIP_EXAMPLES))\n  dataset = preprocess_fun(dataset, training)\n  shapes = {k: features_info[k].shape for k in features_info}\n  shapes = (shapes, shapes[target_names[0]])\n  dataset = dataset.shuffle(shuffle_buffer_size)\n  dataset = batch_fun(dataset, training, shapes, target_names, num_devices)\n  return dataset.prefetch(2)", "output": "Shuffle and batch the given dataset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def concat(lists):\n    \"\"\"\n    \"\"\"\n\n    return lists[0].new(\n        pd.concat([lists.data for lists in lists]).drop_duplicates()\n    )", "output": "\u7c7b\u4f3c\u4e8epd.concat \u7528\u4e8e\u5408\u5e76\u4e00\u4e2alist\u91cc\u9762\u7684\u591a\u4e2aDataStruct,\u4f1a\u81ea\u52a8\u53bb\u91cd\n\n\n\n    Arguments:\n        lists {[type]} -- [DataStruct1,DataStruct2,....,DataStructN]\n\n    Returns:\n        [type] -- new DataStruct", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_diff(changes):\n    '''\n    \n    '''\n    for conf_dict in changes:\n        if conf_dict == 'Networks':\n            continue\n        for item in changes[conf_dict]:\n            if changes[conf_dict][item]['new'] is None:\n                old = changes[conf_dict][item]['old']\n                if old == '':\n                    return True\n                else:\n                    try:\n                        if all(x == '' for x in old):\n                            return True\n                    except TypeError:\n                        # Old value is not an iterable type\n                        pass\n    return False", "output": "Check the diff for signs of incorrect argument handling in previous\n    releases, as discovered here:\n\n    https://github.com/saltstack/salt/pull/39996#issuecomment-288025200", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_dict(cls, json_object):\n        \"\"\"\"\"\"\n        config = OpenAIGPTConfig(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config", "output": "Constructs a `OpenAIGPTConfig` from a Python dictionary of parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_changes(rsync_out):\n    '''\n    \n    '''\n    copied = list()\n    deleted = list()\n\n    for line in rsync_out.split(\"\\n\\n\")[0].split(\"\\n\")[1:]:\n        if line.startswith(\"deleting \"):\n            deleted.append(line.split(\" \", 1)[-1])\n        else:\n            copied.append(line)\n\n    ret = {\n        'copied': os.linesep.join(sorted(copied)) or \"N/A\",\n        'deleted': os.linesep.join(sorted(deleted)) or \"N/A\",\n    }\n\n    # Return whether anything really changed\n    ret['changed'] = not ((ret['copied'] == 'N/A') and (ret['deleted'] == 'N/A'))\n\n    return ret", "output": "Get changes from the rsync successful output.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_reshape(self, name, input_name, output_name, target_shape, mode):\n        \"\"\"\n        \n        \"\"\"\n\n        spec = self.spec\n        nn_spec = self.nn_spec\n\n        # Add a new layer\n        spec_layer = nn_spec.layers.add()\n        spec_layer.name = name\n        spec_layer.input.append(input_name)\n        spec_layer.output.append(output_name)\n\n        spec_layer_params = spec_layer.reshape\n        spec_layer_params.targetShape.extend(target_shape)\n        if mode == 0:\n            spec_layer_params.mode = \\\n                    _NeuralNetwork_pb2.ReshapeLayerParams.ReshapeOrder.Value('CHANNEL_FIRST')\n        else:\n            spec_layer_params.mode = \\\n                    _NeuralNetwork_pb2.ReshapeLayerParams.ReshapeOrder.Value('CHANNEL_LAST')\n\n        if len(target_shape) != 4 and len(target_shape) != 3:\n            raise ValueError(\"Length of the 'target-shape' parameter must be equal to 3 or 4\")", "output": "Add a reshape layer. Kindly refer to NeuralNetwork.proto for details.\n\n        Parameters\n        ----------\n        name: str\n            The name of this layer.\n        target_shape: tuple\n            Shape of the output blob. The product of target_shape must be equal\n            to the shape of the input blob.\n            Can be either length 3 (C,H,W) or length 4 (Seq,C,H,W).\n        mode: int\n\n            - If mode == 0, the reshape layer is in CHANNEL_FIRST mode.\n            - If mode == 1, the reshape layer is in CHANNEL_LAST mode.\n\n        input_name: str\n            The input blob name of this layer.\n        output_name: str\n            The output blob name of this layer.\n\n        See Also\n        --------\n        add_flatten, add_permute", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(name):\n    '''\n    \n    '''\n    if _service_is_upstart(name):\n        cmd = 'start {0}'.format(name)\n    else:\n        cmd = '/sbin/service {0} start'.format(name)\n    return not __salt__['cmd.retcode'](cmd, python_shell=False)", "output": "Start the specified service\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.start <service name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_progress(opts, out, progress):\n    '''\n    \n    '''\n    return salt.loader.raw_mod(opts,\n                                out,\n                                'rawmodule',\n                                mod='output')['{0}.progress_iter'.format(out)](progress)", "output": "Get the progress bar from the given outputter", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def progress_string(self):\n        \"\"\"\"\"\"\n\n        if not self.last_result:\n            return self._status_string()\n\n        def location_string(hostname, pid):\n            if hostname == os.uname()[1]:\n                return \"pid={}\".format(pid)\n            else:\n                return \"{} pid={}\".format(hostname, pid)\n\n        pieces = [\n            \"{}\".format(self._status_string()), \"[{}]\".format(\n                self.resources.summary_string()), \"[{}]\".format(\n                    location_string(\n                        self.last_result.get(HOSTNAME),\n                        self.last_result.get(PID))), \"{} s\".format(\n                            int(self.last_result.get(TIME_TOTAL_S)))\n        ]\n\n        if self.last_result.get(TRAINING_ITERATION) is not None:\n            pieces.append(\"{} iter\".format(\n                self.last_result[TRAINING_ITERATION]))\n\n        if self.last_result.get(TIMESTEPS_TOTAL) is not None:\n            pieces.append(\"{} ts\".format(self.last_result[TIMESTEPS_TOTAL]))\n\n        if self.last_result.get(EPISODE_REWARD_MEAN) is not None:\n            pieces.append(\"{} rew\".format(\n                format(self.last_result[EPISODE_REWARD_MEAN], \".3g\")))\n\n        if self.last_result.get(MEAN_LOSS) is not None:\n            pieces.append(\"{} loss\".format(\n                format(self.last_result[MEAN_LOSS], \".3g\")))\n\n        if self.last_result.get(MEAN_ACCURACY) is not None:\n            pieces.append(\"{} acc\".format(\n                format(self.last_result[MEAN_ACCURACY], \".3g\")))\n\n        return \", \".join(pieces)", "output": "Returns a progress message for printing out to the console.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def db_list(user=None, password=None, host=None, port=None, authdb=None):\n    '''\n    \n    '''\n    conn = _connect(user, password, host, port, authdb=authdb)\n    if not conn:\n        return 'Failed to connect to mongo database'\n\n    try:\n        log.info('Listing databases')\n        return conn.database_names()\n    except pymongo.errors.PyMongoError as err:\n        log.error(err)\n        return six.text_type(err)", "output": "List all MongoDB databases\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mongodb.db_list <user> <password> <host> <port>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_conf(conf, path=MAIN_CF):\n    '''\n    \n    '''\n    with salt.utils.files.fopen(path, 'w') as fh_:\n        for line in conf:\n            line = salt.utils.stringutils.to_str(line)\n            if isinstance(line, dict):\n                fh_.write(' '.join(line))\n            else:\n                fh_.write(line)\n            fh_.write('\\n')", "output": "Write out configuration file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _config_list(conf_tuples=None, only_net=False, **kwargs):\n    '''\n    \n\n    '''\n    # explicit cast\n    only_net = bool(only_net)\n    if not conf_tuples:\n        conf_tuples = []\n    kwargs = copy.deepcopy(kwargs)\n    ret = []\n    if not only_net:\n        default_data = _get_lxc_default_data(**kwargs)\n        for k, val in six.iteritems(default_data):\n            ret.append({k: val})\n    net_datas = _network_conf(conf_tuples=conf_tuples, **kwargs)\n    ret.extend(net_datas)\n    return ret", "output": "Return a list of dicts from the salt level configurations\n\n    conf_tuples\n        _LXCConfig compatible list of entries which can contain\n\n            - string line\n            - tuple (lxc config param,value)\n            - dict of one entry: {lxc config param: value)\n\n    only_net\n        by default we add to the tuples a reflection of both\n        the real config if avalaible and a certain amount of\n        default values like the cpu parameters, the memory\n        and etc.\n        On the other hand, we also no matter the case reflect\n        the network configuration computed from the actual config if\n        available and given values.\n        if no_default_loads is set, we will only\n        reflect the network configuration back to the conf tuples\n        list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_cols(self, sid, dts, cols, invalid_data_behavior='warn'):\n        \"\"\"\n        \n        \"\"\"\n        if not all(len(dts) == len(cols[name]) for name in self.COL_NAMES):\n            raise BcolzMinuteWriterColumnMismatch(\n                \"Length of dts={0} should match cols: {1}\".format(\n                    len(dts),\n                    \" \".join(\"{0}={1}\".format(name, len(cols[name]))\n                             for name in self.COL_NAMES)))\n        self._write_cols(sid, dts, cols, invalid_data_behavior)", "output": "Write the OHLCV data for the given sid.\n        If there is no bcolz ctable yet created for the sid, create it.\n        If the length of the bcolz ctable is not exactly to the date before\n        the first day provided, fill the ctable with 0s up to that date.\n\n        Parameters\n        ----------\n        sid : int\n            The asset identifier for the data being written.\n        dts : datetime64 array\n            The dts corresponding to values in cols.\n        cols : dict of str -> np.array\n            dict of market data with the following characteristics.\n            keys are ('open', 'high', 'low', 'close', 'volume')\n            open : float64\n            high : float64\n            low  : float64\n            close : float64\n            volume : float64|int64", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def average_gradients(model):\n    \"\"\"  \"\"\"\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM, group=0)\n        param.grad.data /= size", "output": "Gradient averaging.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def container_config_delete(name, config_key, remote_addr=None,\n                            cert=None, key=None, verify_cert=True):\n    '''\n    \n    '''\n    container = container_get(\n        name, remote_addr, cert, key, verify_cert, _raw=True\n    )\n\n    return _delete_property_dict_item(\n        container, 'config', config_key\n    )", "output": "Delete a container config value\n\n    name :\n        Name of the container\n\n    config_key :\n        The config key to delete\n\n    remote_addr :\n        An URL to a remote Server, you also have to give cert and key if\n        you provide remote_addr and its a TCP Address!\n\n        Examples:\n            https://myserver.lan:8443\n            /var/lib/mysocket.sock\n\n    cert :\n        PEM Formatted SSL Certificate.\n\n        Examples:\n            ~/.config/lxc/client.crt\n\n    key :\n        PEM Formatted SSL Key.\n\n        Examples:\n            ~/.config/lxc/client.key\n\n    verify_cert : True\n        Wherever to verify the cert, this is by default True\n        but in the most cases you want to set it off as LXD\n        normaly uses self-signed certificates.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _invalid(m, comment=INVALID_RESPONSE, out=None):\n    '''\n    \n    '''\n    return _set_status(m, status=False, comment=comment, out=out)", "output": "Return invalid status.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cbday_roll(self):\n        \"\"\"\n        \n        \"\"\"\n        cbday = CustomBusinessDay(n=self.n, normalize=False, **self.kwds)\n\n        if self._prefix.endswith('S'):\n            # MonthBegin\n            roll_func = cbday.rollforward\n        else:\n            # MonthEnd\n            roll_func = cbday.rollback\n        return roll_func", "output": "Define default roll function to be called in apply method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _load_obj(name):\n        \"\"\"\n        \n        \"\"\"\n        for maxsplit in range(1, name.count('.') + 1):\n            # TODO when py3 only replace by: module, *func_parts = ...\n            func_name_split = name.rsplit('.', maxsplit)\n            module = func_name_split[0]\n            func_parts = func_name_split[1:]\n            try:\n                obj = importlib.import_module(module)\n            except ImportError:\n                pass\n            else:\n                continue\n\n        if 'obj' not in locals():\n            raise ImportError('No module can be imported '\n                              'from \"{}\"'.format(name))\n\n        for part in func_parts:\n            obj = getattr(obj, part)\n        return obj", "output": "Import Python object from its name as string.\n\n        Parameters\n        ----------\n        name : str\n            Object name to import (e.g. pandas.Series.str.upper)\n\n        Returns\n        -------\n        object\n            Python object that can be a class, method, function...\n\n        Examples\n        --------\n        >>> Docstring._load_obj('pandas.Series')\n        <class 'pandas.core.series.Series'>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_state(subset=None, show_ip=False, show_ipv4=None):\n    '''\n    \n    '''\n    show_ip = _show_ip_migration(show_ip, show_ipv4)\n\n    # Always return 'present' for 0MQ for now\n    # TODO: implement other states support for 0MQ\n    ckminions = salt.utils.minions.CkMinions(__opts__)\n    minions = ckminions.connected_ids(show_ip=show_ip, subset=subset)\n\n    connected = dict(minions) if show_ip else sorted(minions)\n\n    return connected", "output": ".. versionadded:: 2015.8.0\n    .. versionchanged:: 2019.2.0\n        The 'show_ipv4' argument has been renamed to 'show_ip' as it now\n        includes IPv6 addresses for IPv6-connected minions.\n\n    Print a list of all minions that are up according to Salt's presence\n    detection (no commands will be sent to minions)\n\n    subset : None\n        Pass in a CIDR range to filter minions by IP address.\n\n    show_ip : False\n        Also show the IP address each minion is connecting from.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run manage.list_state", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_response_data_type(self, response_data):\n        \"\"\"\n        \"\"\"\n        if isinstance(response_data, list) and not isinstance(\n            response_data, str\n        ):\n            return response_data\n\n        int_match_str = \"|\".join(self.config[\"response_format\"][\"int\"])\n        float_match_str = \"|\".join(self.config[\"response_format\"][\"float\"])\n        for item in response_data:\n            for key in item:\n                try:\n                    if re.search(int_match_str, key) is not None:\n                        item[key] = helpers.str2num(item[key], \"int\")\n                    elif re.search(float_match_str, key) is not None:\n                        item[key] = helpers.str2num(item[key], \"float\")\n                except ValueError:\n                    continue\n        return response_data", "output": "\u683c\u5f0f\u5316\u8fd4\u56de\u7684\u503c\u4e3a\u6b63\u786e\u7684\u7c7b\u578b\n        :param response_data: \u8fd4\u56de\u7684\u6570\u636e", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prettify_exc(error):\n    \"\"\"\"\"\"\n    matched_exceptions = [k for k in KNOWN_EXCEPTIONS.keys() if k in error]\n    if not matched_exceptions:\n        return \"{}\".format(vistir.misc.decode_for_output(error))\n    errors = []\n    for match in matched_exceptions:\n        _, error, info = error.rpartition(KNOWN_EXCEPTIONS[match])\n        errors.append(\"{} {}\".format(error, info))\n\n    return \"\\n\".join(errors)", "output": "Catch known errors and prettify them instead of showing the\n    entire traceback, for better UX", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ConvertMapFieldValue(self, value, message, field):\n    \"\"\"\n    \"\"\"\n    if not isinstance(value, dict):\n      raise ParseError(\n          'Map field {0} must be in a dict which is {1}.'.format(\n              field.name, value))\n    key_field = field.message_type.fields_by_name['key']\n    value_field = field.message_type.fields_by_name['value']\n    for key in value:\n      key_value = _ConvertScalarFieldValue(key, key_field, True)\n      if value_field.cpp_type == descriptor.FieldDescriptor.CPPTYPE_MESSAGE:\n        self.ConvertMessage(value[key], getattr(\n            message, field.name)[key_value])\n      else:\n        getattr(message, field.name)[key_value] = _ConvertScalarFieldValue(\n            value[key], value_field)", "output": "Convert map field value for a message map field.\n\n    Args:\n      value: A JSON object to convert the map field value.\n      message: A protocol message to record the converted data.\n      field: The descriptor of the map field to be converted.\n\n    Raises:\n      ParseError: In case of convert problems.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def disabled(name, **kwargs):\n    '''\n    \n\n    '''\n\n    ret = {'name': name,\n           'result': True,\n           'changes': {},\n           'comment': []}\n\n    current_schedule = __salt__['schedule.list'](show_all=True, return_yaml=False)\n    if name in current_schedule:\n        if 'test' in __opts__ and __opts__['test']:\n            kwargs['test'] = True\n            result = __salt__['schedule.disable_job'](name, **kwargs)\n            ret['comment'].append(result['comment'])\n        else:\n            result = __salt__['schedule.disable_job'](name, **kwargs)\n            if not result['result']:\n                ret['result'] = result['result']\n                ret['comment'] = result['comment']\n                return ret\n            else:\n                ret['comment'].append('Disabled job {0} from schedule'.format(name))\n    else:\n        ret['comment'].append('Job {0} not present in schedule'.format(name))\n\n    ret['comment'] = '\\n'.join(ret['comment'])\n    return ret", "output": "Ensure a job is disabled in the schedule\n\n    name\n        The unique name that is given to the scheduled job.\n\n    persist\n        Whether the job should persist between minion restarts, defaults to True.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resize(src, size, interpolation=cv2.INTER_LINEAR):\n    \"\"\"\n    \"\"\"\n    hdl = NDArrayHandle()\n    check_call(_LIB.MXCVResize(src.handle, mx_uint(size[0]), mx_uint(size[1]),\n                               interpolation, ctypes.byref(hdl)))\n    return mx.nd.NDArray(hdl)", "output": "Decode image from str buffer.\n    Wrapper for cv2.imresize that uses mx.nd.NDArray\n\n    Parameters\n    ----------\n    src : NDArray\n        image in (width, height, channels)\n    size : tuple\n        target size in (width, height)\n    interpolation : int\n        same as interpolation for cv2.imresize\n\n    Returns\n    -------\n    img : NDArray\n        resized image", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_model(model_path):\n    \"\"\"\n    \"\"\"\n    if not(HAS_LIBSVM):\n        raise RuntimeError('libsvm not found. libsvm conversion API is disabled.')\n\n    from svmutil import svm_load_model # From libsvm\n    import os\n    if (not os.path.exists(model_path)):\n        raise IOError(\"Expected a valid file path. %s does not exist\" % model_path)\n    return svm_load_model(model_path)", "output": "Load a libsvm model from a path on disk.\n\n    This currently supports:\n      * C-SVC\n      * NU-SVC\n      * Epsilon-SVR\n      * NU-SVR\n\n    Parameters\n    ----------\n    model_path: str\n        Path on disk where the libsvm model representation is.\n\n    Returns\n    -------\n    model: libsvm_model\n        A model of the libsvm format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, name, **kwargs):\n        \"\"\"\n        \"\"\"\n        name = self._prefix + name\n        if name not in self._params:\n            self._params[name] = symbol.Variable(name, **kwargs)\n        return self._params[name]", "output": "Get the variable given a name if one exists or create a new one if missing.\n\n        Parameters\n        ----------\n        name : str\n            name of the variable\n        **kwargs :\n            more arguments that's passed to symbol.Variable", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_value(self, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        warnings.warn(\"get_value is deprecated and will be removed \"\n                      \"in a future release. Please use \"\n                      \".at[] or .iat[] accessors instead\", FutureWarning,\n                      stacklevel=2)\n        return self._get_value(*args, **kwargs)", "output": "Quickly retrieve single value at (item, major, minor) location.\n\n        .. deprecated:: 0.21.0\n\n        Please use .at[] or .iat[] accessors.\n\n        Parameters\n        ----------\n        item : item label (panel item)\n        major : major axis label (panel item row)\n        minor : minor axis label (panel item column)\n        takeable : interpret the passed labels as indexers, default False\n\n        Returns\n        -------\n        value : scalar value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_excel(self, path, na_rep='', engine=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        from pandas.io.excel import ExcelWriter\n\n        if isinstance(path, str):\n            writer = ExcelWriter(path, engine=engine)\n        else:\n            writer = path\n        kwargs['na_rep'] = na_rep\n\n        for item, df in self.iteritems():\n            name = str(item)\n            df.to_excel(writer, name, **kwargs)\n        writer.save()", "output": "Write each DataFrame in Panel to a separate excel sheet.\n\n        Parameters\n        ----------\n        path : string or ExcelWriter object\n            File path or existing ExcelWriter\n        na_rep : string, default ''\n            Missing data representation\n        engine : string, default None\n            write engine to use - you can also set this via the options\n            ``io.excel.xlsx.writer``, ``io.excel.xls.writer``, and\n            ``io.excel.xlsm.writer``.\n\n        Other Parameters\n        ----------------\n        float_format : string, default None\n            Format string for floating point numbers\n        cols : sequence, optional\n            Columns to write\n        header : boolean or list of string, default True\n            Write out column names. If a list of string is given it is\n            assumed to be aliases for the column names\n        index : boolean, default True\n            Write row names (index)\n        index_label : string or sequence, default None\n            Column label for index column(s) if desired. If None is given, and\n            `header` and `index` are True, then the index names are used. A\n            sequence should be given if the DataFrame uses MultiIndex.\n        startrow : upper left cell row to dump data frame\n        startcol : upper left cell column to dump data frame\n\n        Notes\n        -----\n        Keyword arguments (and na_rep) are passed to the ``to_excel`` method\n        for each DataFrame written.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_pax_info(self, pax_headers, encoding, errors):\n        \"\"\"\n        \"\"\"\n        for keyword, value in pax_headers.items():\n            if keyword == \"GNU.sparse.name\":\n                setattr(self, \"path\", value)\n            elif keyword == \"GNU.sparse.size\":\n                setattr(self, \"size\", int(value))\n            elif keyword == \"GNU.sparse.realsize\":\n                setattr(self, \"size\", int(value))\n            elif keyword in PAX_FIELDS:\n                if keyword in PAX_NUMBER_FIELDS:\n                    try:\n                        value = PAX_NUMBER_FIELDS[keyword](value)\n                    except ValueError:\n                        value = 0\n                if keyword == \"path\":\n                    value = value.rstrip(\"/\")\n                setattr(self, keyword, value)\n\n        self.pax_headers = pax_headers.copy()", "output": "Replace fields with supplemental information from a previous\n           pax extended or global header.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _step_envs(self, action):\n    \"\"\"\"\"\"\n    self._frame_counter += 1\n    real_env_step_tuple = self.real_env.step(action)\n    sim_env_step_tuple = self.sim_env.step(action)\n    self.sim_env.add_to_initial_stack(real_env_step_tuple[0])\n    return self._pack_step_tuples(real_env_step_tuple, sim_env_step_tuple)", "output": "Perform step(action) on environments and update initial_frame_stack.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_room(name, api_key=None):\n    '''\n    \n    '''\n    if not api_key:\n        api_key = _get_api_key()\n\n    # search results don't include the name of the\n    # channel with a hash, if the passed channel name\n    # has a hash we remove it.\n    if name.startswith('#'):\n        name = name[1:]\n    ret = list_rooms(api_key)\n    if ret['res']:\n        rooms = ret['message']\n        if rooms:\n            for room in range(0, len(rooms)):\n                if rooms[room]['name'] == name:\n                    return rooms[room]\n    return False", "output": "Find a room by name and return it.\n\n    :param name:    The room name.\n    :param api_key: The Slack admin api key.\n    :return:        The room object.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' slack.find_room name=\"random\"\n\n        salt '*' slack.find_room name=\"random\" api_key=peWcBiMOS9HrZG15peWcBiMOS9HrZG15", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_file_iface(iface, data, folder, pattern):\n    '''\n    \n    '''\n    filename = os.path.join(folder, pattern.format(iface))\n    if not os.path.exists(folder):\n        msg = '{0} cannot be written. {1} does not exist'\n        msg = msg.format(filename, folder)\n        log.error(msg)\n        raise AttributeError(msg)\n    with salt.utils.files.fopen(filename, 'w') as fp_:\n        fp_.write(salt.utils.stringutils.to_str(data))", "output": "Writes a file to disk", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_format(self, format, kwargs):\n        \"\"\"  \"\"\"\n        kwargs = kwargs.copy()\n\n        # validate\n        try:\n            kwargs['format'] = _FORMAT_MAP[format.lower()]\n        except KeyError:\n            raise TypeError(\"invalid HDFStore format specified [{0}]\"\n                            .format(format))\n\n        return kwargs", "output": "validate / deprecate formats; return the new kwargs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read_tsv(cls, input_file, quotechar=None):\n        \"\"\"\"\"\"\n        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n            lines = []\n            for line in reader:\n                if sys.version_info[0] == 2:\n                    line = list(unicode(cell, 'utf-8') for cell in line)\n                lines.append(line)\n            return lines", "output": "Reads a tab separated value file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_floating(self):\n        \"\"\"\"\"\"\n        return (\n            self.is_numpy_compatible and np.issubdtype(self.as_numpy_dtype, np.floating)\n        ) or self.base_dtype == bfloat16", "output": "Returns whether this is a (non-quantized, real) floating point type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_range_vector(size: int, device: int) -> torch.Tensor:\n    \"\"\"\n    \n    \"\"\"\n    if device > -1:\n        return torch.cuda.LongTensor(size, device=device).fill_(1).cumsum(0) - 1\n    else:\n        return torch.arange(0, size, dtype=torch.long)", "output": "Returns a range vector with the desired size, starting at 0. The CUDA implementation\n    is meant to avoid copy data from CPU to GPU.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_layout(self, obj, place='center'):\n        ''' \n\n        '''\n        valid_places = ['left', 'right', 'above', 'below', 'center']\n        if place not in valid_places:\n            raise ValueError(\n                \"Invalid place '%s' specified. Valid place values are: %s\" % (place, nice_join(valid_places))\n            )\n\n        getattr(self, place).append(obj)", "output": "Adds an object to the plot in a specified place.\n\n        Args:\n            obj (Renderer) : the object to add to the Plot\n            place (str, optional) : where to add the object (default: 'center')\n                Valid places are: 'left', 'right', 'above', 'below', 'center'.\n\n        Returns:\n            None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def queryone(self, stmt, *args):\n        \"\"\"\n        \"\"\"\n        results = await self.query(stmt, *args)\n        if len(results) == 0:\n            raise NoResultError()\n        elif len(results) > 1:\n            raise ValueError(\"Expected 1 result, got %d\" % len(results))\n        return results[0]", "output": "Query for exactly one result.\n\n        Raises NoResultError if there are no results, or ValueError if\n        there are more than one.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def count(self, **kwargs):\n        \"\"\"\n        \"\"\"\n        if self._is_transposed:\n            kwargs[\"axis\"] = kwargs.get(\"axis\", 0) ^ 1\n            return self.transpose().count(**kwargs)\n        axis = kwargs.get(\"axis\", 0)\n        map_func = self._build_mapreduce_func(pandas.DataFrame.count, **kwargs)\n        reduce_func = self._build_mapreduce_func(pandas.DataFrame.sum, **kwargs)\n        return self._full_reduce(axis, map_func, reduce_func)", "output": "Counts the number of non-NaN objects for each column or row.\n\n        Return:\n            A new QueryCompiler object containing counts of non-NaN objects from each\n            column or row.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def salt_ip_verify_tool():\n    '''\n    \n    '''\n    # This is overly cumbersome and crude,\n    # But, it's also safe... ish...\n    salt_config = cherrypy.config.get('saltopts', None)\n    if salt_config:\n        cherrypy_conf = salt_config.get('rest_cherrypy', None)\n        if cherrypy_conf:\n            auth_ip_list = cherrypy_conf.get('authorized_ips', None)\n            if auth_ip_list:\n                logger.debug('Found IP list: %s', auth_ip_list)\n                rem_ip = cherrypy.request.headers.get('Remote-Addr', None)\n                logger.debug('Request from IP: %s', rem_ip)\n                if rem_ip not in auth_ip_list:\n                    logger.error('Blocked IP: %s', rem_ip)\n                    raise cherrypy.HTTPError(403, 'Bad IP')", "output": "If there is a list of restricted IPs, verify current\n    client is coming from one of those IPs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_embedding(builder, layer, input_names, output_names, keras_layer):\n    \"\"\"\n    \"\"\"\n    # Get input and output names\n    input_name, output_name = (input_names[0], output_names[0])\n\n    # Get the weights from keras\n    W = keras_layer.get_weights ()[0].T\n\n    # assuming keras embedding layers don't have biases\n    builder.add_embedding(name = layer,\n                          W = W,\n                          b = None,\n                          input_dim = keras_layer.input_dim,\n                          output_channels = keras_layer.output_dim,\n                          has_bias = False,\n                          input_name = input_name,\n                          output_name = output_name)", "output": "Convert a dense layer from keras to coreml.\n\n    Parameters\n    keras_layer: layer\n    ----------\n        A keras layer object.\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def polynet(num_classes=1000, pretrained='imagenet'):\n    \"\"\"\n    \"\"\"\n    if pretrained:\n        settings = pretrained_settings['polynet'][pretrained]\n        assert num_classes == settings['num_classes'], \\\n            'num_classes should be {}, but is {}'.format(\n                settings['num_classes'], num_classes)\n        model = PolyNet(num_classes=num_classes)\n        model.load_state_dict(model_zoo.load_url(settings['url']))\n        model.input_space = settings['input_space']\n        model.input_size = settings['input_size']\n        model.input_range = settings['input_range']\n        model.mean = settings['mean']\n        model.std = settings['std']\n    else:\n        model = PolyNet(num_classes=num_classes)\n    return model", "output": "PolyNet architecture from the paper\n    'PolyNet: A Pursuit of Structural Diversity in Very Deep Networks'\n    https://arxiv.org/abs/1611.05725", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def serialize(obj, **options):\n    '''\n    \n    '''\n\n    try:\n        if not isinstance(obj, dict):\n            raise TypeError(\"configparser can only serialize dictionaries, not {0}\".format(type(obj)))\n        fp = options.pop('fp', None)\n        if six.PY3:\n            cp = configparser.ConfigParser(**options)\n        else:\n            cp = configparser.SafeConfigParser(**options)\n        _read_dict(cp, obj)\n\n        if fp:\n            return cp.write(fp)\n        else:\n            s = six.moves.StringIO()\n            cp.write(s)\n            return s.getvalue()\n    except Exception as error:\n        raise SerializationError(error)", "output": "Serialize Python data to a configparser formatted string or file.\n\n    :param obj: the data structure to serialize\n    :param options: options given to lower configparser module.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def iterrows(self):\n        \"\"\"\n        \n        \"\"\"\n        columns = self.columns\n        klass = self._constructor_sliced\n        for k, v in zip(self.index, self.values):\n            s = klass(v, index=columns, name=k)\n            yield k, s", "output": "Iterate over DataFrame rows as (index, Series) pairs.\n\n        Yields\n        ------\n        index : label or tuple of label\n            The index of the row. A tuple for a `MultiIndex`.\n        data : Series\n            The data of the row as a Series.\n\n        it : generator\n            A generator that iterates over the rows of the frame.\n\n        See Also\n        --------\n        itertuples : Iterate over DataFrame rows as namedtuples of the values.\n        iteritems : Iterate over (column name, Series) pairs.\n\n        Notes\n        -----\n\n        1. Because ``iterrows`` returns a Series for each row,\n           it does **not** preserve dtypes across the rows (dtypes are\n           preserved across columns for DataFrames). For example,\n\n           >>> df = pd.DataFrame([[1, 1.5]], columns=['int', 'float'])\n           >>> row = next(df.iterrows())[1]\n           >>> row\n           int      1.0\n           float    1.5\n           Name: 0, dtype: float64\n           >>> print(row['int'].dtype)\n           float64\n           >>> print(df['int'].dtype)\n           int64\n\n           To preserve dtypes while iterating over the rows, it is better\n           to use :meth:`itertuples` which returns namedtuples of the values\n           and which is generally faster than ``iterrows``.\n\n        2. You should **never modify** something you are iterating over.\n           This is not guaranteed to work in all cases. Depending on the\n           data types, the iterator returns a copy and not a view, and writing\n           to it will have no effect.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_params(self, initializer=mx.init.Uniform(0.01), **kwargs):\n        \"\"\"\n        \"\"\"\n        self._module.init_params(initializer=initializer, **kwargs)", "output": "Initializes the parameters and auxiliary states.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download_urls(self):\n    \"\"\"\"\"\"\n    urls = {\n        tfds.Split.TRAIN: [\"train_clean100\"],\n        tfds.Split.VALIDATION: [\"dev_clean\"],\n        tfds.Split.TEST: [\"test_clean\"],\n    }\n    if self.data in [\"all\", \"clean360\"]:\n      urls[tfds.Split.TRAIN].append(\"train_clean360\")\n    if self.data == \"all\":\n      urls[tfds.Split.TRAIN].extend([\"train_clean360\", \"train_other500\"])\n      urls[tfds.Split.VALIDATION].append(\"dev_other\")\n      urls[tfds.Split.TEST].append(\"test_other\")\n\n    urls = {\n        split: [_DL_URLS[name] for name in names\n               ] for split, names in urls.items()\n    }\n    return urls", "output": "Returns download urls for this config.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_folder(name, location='\\\\'):\n    \n    '''\n    # Check for existing folder\n    if name in list_folders(location):\n        # Connect to an existing task definition\n        return '{0} already exists'.format(name)\n\n    # Create the task service object\n    with salt.utils.winapi.Com():\n        task_service = win32com.client.Dispatch(\"Schedule.Service\")\n    task_service.Connect()\n\n    # Get the folder to list folders from\n    task_folder = task_service.GetFolder(location)\n    task_folder.CreateFolder(name)\n\n    # Verify creation\n    if name in list_folders(location):\n        return True\n    else:\n        return False", "output": "r'''\n    Create a folder in which to create tasks.\n\n    :param str name: The name of the folder. This will be displayed in the task\n        scheduler.\n\n    :param str location: A string value representing the location in which to\n        create the folder. Default is '\\\\' which is the root for the task\n        scheduler (C:\\Windows\\System32\\tasks).\n\n    :return: True if successful, False if unsuccessful\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'minion-id' task.create_folder <folder_name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def augment_usage_errors(ctx, param=None):\n    \"\"\"\n    \"\"\"\n    try:\n        yield\n    except BadParameter as e:\n        if e.ctx is None:\n            e.ctx = ctx\n        if param is not None and e.param is None:\n            e.param = param\n        raise\n    except UsageError as e:\n        if e.ctx is None:\n            e.ctx = ctx\n        raise", "output": "Context manager that attaches extra information to exceptions that\n    fly.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, api_repr):\n        \"\"\"\n        \"\"\"\n        # Handle optional properties with default values\n        mode = api_repr.get(\"mode\", \"NULLABLE\")\n        description = api_repr.get(\"description\")\n        fields = api_repr.get(\"fields\", ())\n        return cls(\n            field_type=api_repr[\"type\"].upper(),\n            fields=[cls.from_api_repr(f) for f in fields],\n            mode=mode.upper(),\n            description=description,\n            name=api_repr[\"name\"],\n        )", "output": "Return a ``SchemaField`` object deserialized from a dictionary.\n\n        Args:\n            api_repr (Mapping[str, str]): The serialized representation\n                of the SchemaField, such as what is output by\n                :meth:`to_api_repr`.\n\n        Returns:\n            google.cloud.biquery.schema.SchemaField:\n                The ``SchemaField`` object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_quote_position(text):\r\n        \"\"\"\"\"\"\r\n        pos = {}\r\n        is_found_left_quote = False\r\n\r\n        for idx, character in enumerate(text):\r\n            if is_found_left_quote is False:\r\n                if character == \"'\" or character == '\"':\r\n                    is_found_left_quote = True\r\n                    quote = character\r\n                    left_pos = idx\r\n            else:\r\n                if character == quote and text[idx - 1] != '\\\\':\r\n                    pos[left_pos] = idx\r\n                    is_found_left_quote = False\r\n\r\n        if is_found_left_quote:\r\n            raise IndexError(\"No matching close quote at: \" + str(left_pos))\r\n\r\n        return pos", "output": "Return the start and end position of pairs of quotes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def truncate_selection(self, position_from):\r\n        \"\"\"\"\"\"\r\n        position_from = self.get_position(position_from)\r\n        cursor = self.textCursor()\r\n        start, end = cursor.selectionStart(), cursor.selectionEnd()\r\n        if start < end:\r\n            start = max([position_from, start])\r\n        else:\r\n            end = max([position_from, end])\r\n        self.set_selection(start, end)", "output": "Unselect read-only parts in shell, like prompt", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ProcessContent(self, strip_expansion=False):\n    \"\"\"\"\"\"\n    self._ParseFile()\n    if strip_expansion:\n      # Without a collection the expansions become blank, removing them.\n      collection = None\n    else:\n      collection = MacroCollection()\n    for section in self._sections:\n      section.BindMacroCollection(collection)\n    result = ''\n    for section in self._sections:\n      result += section.text\n    self._processed_content = result", "output": "Processes the file contents.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def display_rows(self,\n                     rows,\n                     labels,\n                     indent):\n\n        ''''''\n\n        out = []\n\n        if not rows:\n            return out\n\n        first_row_type = type(rows[0])\n        # all rows must have the same datatype\n        consistent = True\n        for row in rows[1:]:\n            if type(row) != first_row_type:\n                consistent = False\n\n        if not consistent:\n            return out\n\n        if isinstance(labels, dict):\n            labels_temp = []\n            for key in sorted(labels):\n                labels_temp.append(labels[key])\n            labels = labels_temp\n\n        if first_row_type is dict:  # and all the others\n            temp_rows = []\n            if not labels:\n                labels = [six.text_type(label).replace('_', ' ').title() for label in sorted(rows[0])]\n            for row in rows:\n                temp_row = []\n                for key in sorted(row):\n                    temp_row.append(six.text_type(row[key]))\n                temp_rows.append(temp_row)\n            rows = temp_rows\n        elif isinstance(rows[0], six.string_types):\n            rows = [[row] for row in rows]  # encapsulate each row in a single-element list\n\n        labels_and_rows = [labels] + rows if labels else rows\n        has_header = self.has_header and labels\n\n        return self.prepare_rows(labels_and_rows, indent + 4, has_header)", "output": "Prepares row content and displays.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _suppress_distutils_logs():\n    \"\"\"\n    \"\"\"\n    f = distutils.log.Log._log\n\n    def _log(log, level, msg, args):\n        if level >= distutils.log.ERROR:\n            f(log, level, msg, args)\n\n    distutils.log.Log._log = _log\n    yield\n    distutils.log.Log._log = f", "output": "Hack to hide noise generated by `setup.py develop`.\n\n    There isn't a good way to suppress them now, so let's monky-patch.\n    See https://bugs.python.org/issue25392.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def analyzer(self, *args, **kwargs):\n        \"\"\"\n        \n\n        \"\"\"\n        analyzer = analysis.analyzer(*args, **kwargs)\n        d = analyzer.get_analysis_definition()\n        # empty custom analyzer, probably already defined out of our control\n        if not d:\n            return\n\n        # merge the definition\n        merge(self._analysis, d, True)", "output": "Explicitly add an analyzer to an index. Note that all custom analyzers\n        defined in mappings will also be created. This is useful for search analyzers.\n\n        Example::\n\n            from elasticsearch_dsl import analyzer, tokenizer\n\n            my_analyzer = analyzer('my_analyzer',\n                tokenizer=tokenizer('trigram', 'nGram', min_gram=3, max_gram=3),\n                filter=['lowercase']\n            )\n\n            i = Index('blog')\n            i.analyzer(my_analyzer)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deselect_by_visible_text(self, text):\n        \"\"\"\n        \"\"\"\n        if not self.is_multiple:\n            raise NotImplementedError(\"You may only deselect options of a multi-select\")\n        matched = False\n        xpath = \".//option[normalize-space(.) = %s]\" % self._escapeString(text)\n        opts = self._el.find_elements(By.XPATH, xpath)\n        for opt in opts:\n            self._unsetSelected(opt)\n            matched = True\n        if not matched:\n            raise NoSuchElementException(\"Could not locate element with visible text: %s\" % text)", "output": "Deselect all options that display text matching the argument. That is, when given \"Bar\" this\n           would deselect an option like:\n\n           <option value=\"foo\">Bar</option>\n\n           :Args:\n            - text - The visible text to match against", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _import_module_from_path(module_name, plugin_path):\r\n    \"\"\"\r\n    \"\"\"\r\n    module = None\r\n    try:\r\n        if PY2:\r\n            info = imp.find_module(module_name, [plugin_path])\r\n            if info:\r\n                module = imp.load_module(module_name, *info)\r\n        else:  # Python 3.4+\r\n            spec = importlib.machinery.PathFinder.find_spec(\r\n                module_name,\r\n                [plugin_path])\r\n            if spec:\r\n                module = spec.loader.load_module(module_name)\r\n    except Exception:\r\n        pass\r\n\r\n    return module", "output": "Imports `module_name` from `plugin_path`.\r\n\r\n    Return None if no module is found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shake_shake_block(x, output_filters, stride, hparams):\n  \"\"\"\"\"\"\n  is_training = hparams.mode == tf.estimator.ModeKeys.TRAIN\n  batch_size = common_layers.shape_list(x)[0]\n\n  # Generate random numbers for scaling the branches.\n  rand_forward = [\n      tf.random_uniform(\n          [batch_size, 1, 1, 1], minval=0, maxval=1, dtype=tf.float32)\n      for _ in range(hparams.shake_shake_num_branches)\n  ]\n  rand_backward = [\n      tf.random_uniform(\n          [batch_size, 1, 1, 1], minval=0, maxval=1, dtype=tf.float32)\n      for _ in range(hparams.shake_shake_num_branches)\n  ]\n  # Normalize so that all sum to 1.\n  total_forward = tf.add_n(rand_forward)\n  total_backward = tf.add_n(rand_backward)\n  rand_forward = [samp / total_forward for samp in rand_forward]\n  rand_backward = [samp / total_backward for samp in rand_backward]\n  zipped_rand = zip(rand_forward, rand_backward)\n\n  branches = []\n  for branch, (r_forward, r_backward) in enumerate(zipped_rand):\n    with tf.variable_scope(\"branch_{}\".format(branch)):\n      b = shake_shake_branch(x, output_filters, stride, r_forward, r_backward,\n                             hparams)\n      b = tf.nn.dropout(b, 1.0 - hparams.layer_prepostprocess_dropout)\n      branches.append(b)\n  res = shake_shake_skip_connection(x, output_filters, stride, is_training)\n  if hparams.shake_shake_concat:\n    concat_values = [res] + branches\n    concat_output = tf.concat(values=concat_values, axis=-1)\n    concat_output = tf.nn.relu(concat_output)\n    concat_output = tf.layers.conv2d(\n        concat_output, output_filters, (1, 1), name=\"concat_1x1\")\n    concat_output = tf.layers.batch_normalization(\n        concat_output, training=is_training, name=\"concat_bn\")\n    return concat_output\n  else:\n    return res + tf.add_n(branches)", "output": "Builds a full shake-shake sub layer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_custom_interpreters_list(self, value):\r\n        \"\"\"\"\"\"\r\n        custom_list = self.get_option('custom_interpreters_list')\r\n        if value not in custom_list and value != get_python_executable():\r\n            custom_list.append(value)\r\n            self.set_option('custom_interpreters_list', custom_list)", "output": "Update the list of interpreters used and the current one.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_arrays(cls, path, trn, val, bs=64, tfms=(None,None), classes=None, num_workers=4, test=None, continuous=False):\n        \"\"\" \n        \"\"\"\n        f = ArraysIndexRegressionDataset if continuous else ArraysIndexDataset\n        datasets = cls.get_ds(f, trn, val, tfms, test=test)\n        return cls(path, datasets, bs, num_workers, classes=classes)", "output": "Read in images and their labels given as numpy arrays\n\n        Arguments:\n            path: a root path of the data (used for storing trained models, precomputed values, etc)\n            trn: a tuple of training data matrix and target label/classification array (e.g. `trn=(x,y)` where `x` has the\n                shape of `(5000, 784)` and `y` has the shape of `(5000,)`)\n            val: a tuple of validation data matrix and target label/classification array.\n            bs: batch size\n            tfms: transformations (for data augmentations). e.g. output of `tfms_from_model`\n            classes: a list of all labels/classifications\n            num_workers: a number of workers\n            test: a matrix of test data (the shape should match `trn[0]`)\n\n        Returns:\n            ImageClassifierData", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def switch(self, gen_mode:bool=None):\n        \"\"\n        self.gen_mode = (not self.gen_mode) if gen_mode is None else gen_mode", "output": "Put the model in generator mode if `gen_mode`, in critic mode otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tell(self):\n        \"\"\"\n        \"\"\"\n        assert self.writable\n        pos = ctypes.c_size_t()\n        check_call(_LIB.MXRecordIOWriterTell(self.handle, ctypes.byref(pos)))\n        return pos.value", "output": "Returns the current position of write head.\n\n        Examples\n        ---------\n        >>> record = mx.recordio.MXIndexedRecordIO('tmp.idx', 'tmp.rec', 'w')\n        >>> print(record.tell())\n        0\n        >>> for i in range(5):\n        ...     record.write_idx(i, 'record_%d'%i)\n        ...     print(record.tell())\n        16\n        32\n        48\n        64\n        80", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compute_mask_offsets(shard_id2num_examples):\n  \"\"\"\n  \"\"\"\n  total_num_examples = sum(shard_id2num_examples)\n\n  mask_offsets = []\n  total_num_examples = 0\n  for num_examples_in_shard in shard_id2num_examples:\n    # The offset (nb of examples to skip in the next shard) correspond to the\n    # number of examples remaining in the current shard\n    mask_offsets.append(total_num_examples % 100)\n    total_num_examples += num_examples_in_shard\n\n  return mask_offsets", "output": "Return the list of offsets associated with each shards.\n\n  Args:\n    shard_id2num_examples: `list[int]`, mapping shard_id=>num_examples\n\n  Returns:\n    mask_offsets: `list[int]`, offset to skip for each of the shard", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _save_current_cell(self):\n        \"\"\"\"\"\"\n        row, cell = self._row, self._cell\n        family = row._cells.setdefault(cell.family_name, {})\n        qualified = family.setdefault(cell.qualifier, [])\n        complete = Cell.from_pb(cell)\n        qualified.append(complete)\n        self._cell, self._previous_cell = None, cell", "output": "Helper for :meth:`consume_next`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def change_last_focused_widget(self, old, now):\r\n        \"\"\"\"\"\"\r\n        if (now is None and QApplication.activeWindow() is not None):\r\n            QApplication.activeWindow().setFocus()\r\n            self.last_focused_widget = QApplication.focusWidget()\r\n        elif now is not None:\r\n            self.last_focused_widget = now\r\n\r\n        self.previous_focused_widget =  old", "output": "To keep track of to the last focused widget", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_title(self, docname):\n        \"\"\"\n        \"\"\"\n        env = self.document.settings.env\n        title = self.titles.get(docname)\n        if title is None:\n            fname = os.path.join(env.srcdir, docname+'.rst')\n            try:\n                f = open(fname, 'r')\n            except IOError:\n                title = False\n            else:\n                for line in f:\n                    if len(line) > 0 and (line[0].isalnum() or line[0] == '<'):\n                        title = line.rstrip()\n                        break\n                f.close()\n                if title is None:\n                    title = os.path.basename(docname)\n            self.titles[docname] = title\n        return title", "output": "Parse a document title as the first line starting in [A-Za-z0-9<]\n           or fall back to the document basename if no such line exists.\n           The cmake --help-*-list commands also depend on this convention.\n           Return the title or False if the document file does not exist.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_app(name, site):\n    '''\n    \n    '''\n    current_apps = list_apps(site)\n\n    if name not in current_apps:\n        log.debug('Application already absent: %s', name)\n        return True\n\n    ps_cmd = ['Remove-WebApplication',\n              '-Name', \"'{0}'\".format(name),\n              '-Site', \"'{0}'\".format(site)]\n\n    cmd_ret = _srvmgr(ps_cmd)\n\n    if cmd_ret['retcode'] != 0:\n        msg = 'Unable to remove application: {0}\\nError: {1}' \\\n              ''.format(name, cmd_ret['stderr'])\n        raise CommandExecutionError(msg)\n\n    new_apps = list_apps(site)\n\n    if name not in new_apps:\n        log.debug('Application removed successfully: %s', name)\n        return True\n\n    log.error('Unable to remove application: %s', name)\n    return False", "output": "Remove an IIS application.\n\n    Args:\n        name (str): The application name.\n        site (str): The IIS site name.\n\n    Returns:\n        bool: True if successful, otherwise False\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' win_iis.remove_app name='app0' site='site0'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_combine_stderr(self, combine):\n        \"\"\"\n        \n        \"\"\"\n        data = bytes()\n        self.lock.acquire()\n        try:\n            old = self.combine_stderr\n            self.combine_stderr = combine\n            if combine and not old:\n                # copy old stderr buffer into primary buffer\n                data = self.in_stderr_buffer.empty()\n        finally:\n            self.lock.release()\n        if len(data) > 0:\n            self._feed(data)\n        return old", "output": "Set whether stderr should be combined into stdout on this channel.\n        The default is ``False``, but in some cases it may be convenient to\n        have both streams combined.\n\n        If this is ``False``, and `exec_command` is called (or ``invoke_shell``\n        with no pty), output to stderr will not show up through the `recv`\n        and `recv_ready` calls.  You will have to use `recv_stderr` and\n        `recv_stderr_ready` to get stderr output.\n\n        If this is ``True``, data will never show up via `recv_stderr` or\n        `recv_stderr_ready`.\n\n        :param bool combine:\n            ``True`` if stderr output should be combined into stdout on this\n            channel.\n        :return: the previous setting (a `bool`).\n\n        .. versionadded:: 1.1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validatePopElement(self, doc, elem, qname):\n        \"\"\" \"\"\"\n        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        ret = libxml2mod.xmlValidatePopElement(self._o, doc__o, elem__o, qname)\n        return ret", "output": "Pop the element end from the validation stack.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_appoptics(options):\n    '''\n    \n    '''\n    conn = appoptics_metrics.connect(\n        options.get('api_token'),\n        sanitizer=appoptics_metrics.sanitize_metric_name,\n        hostname=options.get('api_url'))\n    log.info(\"Connected to appoptics.\")\n    return conn", "output": "Return an appoptics connection object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_number(self, rows: List[Row], column: NumberColumn) -> Number:\n        \"\"\"\n        \n        \"\"\"\n        numbers: List[float] = []\n        for row in rows:\n            cell_value = row.values[column.name]\n            if isinstance(cell_value, float):\n                numbers.append(cell_value)\n\n        return numbers[0] if numbers else -1", "output": "Select function takes a row (as a list) and a column name and returns the number in that\n        column. If multiple rows are given, will return the first number that is not None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _flatten_dict(original_dict):\n  \"\"\"\n  \"\"\"\n  flat_dict = {}\n  for key, value in original_dict.items():\n    if isinstance(value, dict):\n      for name, tensor in value.items():\n        if isinstance(tensor, dict):\n          raise ValueError(\"flatten_dict only handles 2 levels of nesting.\")\n        flat_key = \"__\" + key + \"_\" + name\n        flat_dict[flat_key] = tensor\n    else:\n      flat_dict[key] = value\n\n  return flat_dict", "output": "Flatten dict of dicts into a single dict with appropriate prefixes.\n\n  Handles only 2 levels of nesting in the original dict.\n\n  Args:\n    original_dict: Dict which may contain one or more dicts.\n  Returns:\n    flat_dict: Dict without any nesting. Any dicts in the original dict have\n      their keys as prefixes in the new dict.\n  Raises:\n    ValueError if the original dict has more than two levels of nesting.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy(self, conn, ins_rows, table_bound):\n        \"\"\"\n        \n        \"\"\"\n        bound_cols = dict((c, sqlalchemy.bindparam(\"_\" + c.key)) for c in table_bound.columns)\n        ins = table_bound.insert().values(bound_cols)\n        conn.execute(ins, ins_rows)", "output": "This method does the actual insertion of the rows of data given by ins_rows into the\n        database. A task that needs row updates instead of insertions should overload this method.\n        :param conn: The sqlalchemy connection object\n        :param ins_rows: The dictionary of rows with the keys in the format _<column_name>. For example\n        if you have a table with a column name \"property\", then the key in the dictionary\n        would be \"_property\". This format is consistent with the bindparam usage in sqlalchemy.\n        :param table_bound: The object referring to the table\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def colorize(img, heatmap):\n    \"\"\" \n    \"\"\"\n    heatmap = viz.intensity_to_rgb(heatmap, cmap='jet')[:, :, ::-1]\n    return img * 0.5 + heatmap * 0.5", "output": "img: bgr, [0,255]\n        heatmap: [0,1]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_installation_order(self, req_set):\n        # type: (RequirementSet) -> List[InstallRequirement]\n        \"\"\"\n        \"\"\"\n        # The current implementation, which we may change at any point\n        # installs the user specified things in the order given, except when\n        # dependencies must come earlier to achieve topological order.\n        order = []\n        ordered_reqs = set()  # type: Set[InstallRequirement]\n\n        def schedule(req):\n            if req.satisfied_by or req in ordered_reqs:\n                return\n            if req.constraint:\n                return\n            ordered_reqs.add(req)\n            for dep in self._discovered_dependencies[req.name]:\n                schedule(dep)\n            order.append(req)\n\n        for install_req in req_set.requirements.values():\n            schedule(install_req)\n        return order", "output": "Create the installation order.\n\n        The installation order is topological - requirements are installed\n        before the requiring thing. We break cycles at an arbitrary point,\n        and make no other guarantees.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def grad_and_loss(func, argnum=None):\n    \"\"\"\n    \"\"\"\n    @functools.wraps(func)\n    def wrapped(*args):\n        \"\"\"Wrapped function.\"\"\"\n        variables = args\n        if argnum is not None:\n            argnum_ = argnum if isinstance(argnum, list) else [argnum]\n            variables = [args[i] for i in argnum_]\n        for x in variables:\n            assert isinstance(x, NDArray), \"type of autograd input should NDArray.\"\n        grads = [zeros_like(x) for x in variables]\n        mark_variables(variables, grads)\n        with train_section():\n            outputs = func(*args)\n        compute_gradient([outputs] if isinstance(outputs, NDArray) else outputs)\n        return grads, outputs\n    return wrapped", "output": "Return function that computes both gradient of arguments and loss value.\n\n    Parameters\n    ----------\n    func: a python function\n        The forward (loss) function.\n    argnum: an int or a list of int\n        The index of argument to calculate gradient for.\n\n    Returns\n    -------\n    grad_and_loss_func: a python function\n        A function that would compute both the gradient of arguments and loss value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_configurations(self, experiments):\n        \"\"\"\n        \"\"\"\n        experiment_list = convert_to_experiment_list(experiments)\n        for experiment in experiment_list:\n            self._trial_generator = itertools.chain(\n                self._trial_generator,\n                self._generate_trials(experiment.spec, experiment.name))", "output": "Chains generator given experiment specifications.\n\n        Arguments:\n            experiments (Experiment | list | dict): Experiments to run.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def htmlDocContentDumpFormatOutput(self, buf, encoding, format):\n        \"\"\" \"\"\"\n        if buf is None: buf__o = None\n        else: buf__o = buf._o\n        libxml2mod.htmlDocContentDumpFormatOutput(buf__o, self._o, encoding, format)", "output": "Dump an HTML document.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def append(self, key, _item):  # type: (Union[Key, str], Any) -> Table\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(_item, Item):\n            _item = item(_item)\n\n        self._value.append(key, _item)\n\n        if isinstance(key, Key):\n            key = key.key\n\n        if key is not None:\n            super(Table, self).__setitem__(key, _item)\n\n        m = re.match(\"(?s)^[^ ]*([ ]+).*$\", self._trivia.indent)\n        if not m:\n            return self\n\n        indent = m.group(1)\n\n        if not isinstance(_item, Whitespace):\n            m = re.match(\"(?s)^([^ ]*)(.*)$\", _item.trivia.indent)\n            if not m:\n                _item.trivia.indent = indent\n            else:\n                _item.trivia.indent = m.group(1) + indent + m.group(2)\n\n        return self", "output": "Appends a (key, item) to the table.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_graphs(self, run_key, debug=False):\n    \"\"\"\n    \"\"\"\n    graph_dict = (self._run_key_to_debug_graphs if debug else\n                  self._run_key_to_original_graphs)\n    graph_wrappers = graph_dict.get(run_key, {})\n    graph_defs = dict()\n    for device_name, wrapper in graph_wrappers.items():\n      graph_defs[device_name] = wrapper.graph_def\n    return graph_defs", "output": "Get the runtime GraphDef protos associated with a run key.\n\n    Args:\n      run_key: A Session.run kay.\n      debug: Whether the debugger-decoratedgraph is to be retrieved.\n\n    Returns:\n      A `dict` mapping device name to `GraphDef` protos.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def modulo(lhs, rhs):\n    \"\"\"\n    \"\"\"\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_mod,\n        operator.mod,\n        _internal._mod_scalar,\n        _internal._rmod_scalar)", "output": "Returns element-wise modulo of the input arrays with broadcasting.\n\n    Equivalent to ``lhs % rhs`` and ``mx.nd.broadcast_mod(lhs, rhs)``.\n\n    .. note::\n\n       If the corresponding dimensions of two arrays have the same size or one of them has size 1,\n       then the arrays are broadcastable to a common shape.\n\n    Parameters\n    ----------\n    lhs : scalar or mxnet.ndarray.array\n        First array in modulo.\n    rhs : scalar or mxnet.ndarray.array\n         Second array in modulo.\n        The arrays to be taken modulo. If ``lhs.shape != rhs.shape``, they must be\n        broadcastable to a common shape.\n\n    Returns\n    -------\n    NDArray\n        The element-wise modulo of the input arrays.\n\n    Examples\n    --------\n    >>> x = mx.nd.ones((2,3))*6\n    >>> y = mx.nd.ones((2,1))*4\n    >>> x.asnumpy()\n    array([[ 6.,  6.,  6.],\n           [ 6.,  6.,  6.]], dtype=float32)\n    >>> y.asnumpy()\n    array([[ 4.],\n           [ 4.]], dtype=float32)\n    >>> x%5\n    <NDArray 2x3 @cpu(0)>\n    >>> (x%5).asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> (x%y).asnumpy()\n    array([[ 2.,  2.,  2.],\n           [ 2.,  2.,  2.]], dtype=float32)\n    >>> mx.nd.modulo(x,y).asnumpy()\n    array([[ 2.,  2.,  2.],\n           [ 2.,  2.,  2.]], dtype=float32)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_value(self, sid, dt, field):\n        \"\"\"\n        \n        \"\"\"\n        ix = self.sid_day_index(sid, dt)\n        price = self._spot_col(field)[ix]\n        if field != 'volume':\n            if price == 0:\n                return nan\n            else:\n                return price * 0.001\n        else:\n            return price", "output": "Parameters\n        ----------\n        sid : int\n            The asset identifier.\n        day : datetime64-like\n            Midnight of the day for which data is requested.\n        colname : string\n            The price field. e.g. ('open', 'high', 'low', 'close', 'volume')\n\n        Returns\n        -------\n        float\n            The spot price for colname of the given sid on the given day.\n            Raises a NoDataOnDate exception if the given day and sid is before\n            or after the date range of the equity.\n            Returns -1 if the day is within the date range, but the price is\n            0.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def policy_exists(vhost, name, runas=None):\n    '''\n    \n    '''\n    if runas is None and not salt.utils.platform.is_windows():\n        runas = salt.utils.user.get_user()\n    policies = list_policies(runas=runas)\n    return bool(vhost in policies and name in policies[vhost])", "output": "Return whether the policy exists based on rabbitmqctl list_policies.\n\n    Reference: http://www.rabbitmq.com/ha.html\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rabbitmq.policy_exists / HA", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_pretrained_file_names(cls, pretrained_file_name):\n        \"\"\"\n        \"\"\"\n\n        embedding_name = cls.__name__.lower()\n        if pretrained_file_name not in cls.pretrained_file_name_sha1:\n            raise KeyError('Cannot find pretrained file %s for token embedding %s. Valid '\n                           'pretrained files for embedding %s: %s' %\n                           (pretrained_file_name, embedding_name, embedding_name,\n                            ', '.join(cls.pretrained_file_name_sha1.keys())))", "output": "Checks if a pre-trained token embedding file name is valid.\n\n\n        Parameters\n        ----------\n        pretrained_file_name : str\n            The pre-trained token embedding file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_exported_entries(self, category, name=None):\n        \"\"\"\n        \n        \"\"\"\n        for dist in self.get_distributions():\n            r = dist.exports\n            if category in r:\n                d = r[category]\n                if name is not None:\n                    if name in d:\n                        yield d[name]\n                else:\n                    for v in d.values():\n                        yield v", "output": "Return all of the exported entries in a particular category.\n\n        :param category: The category to search for entries.\n        :param name: If specified, only entries with that name are returned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_target_variable (self, targets, variable, value, append=0):\n        \"\"\" \n        \"\"\"\n        if isinstance (targets, str):\n            targets = [targets]\n        if isinstance(value, str):\n            value = [value]\n\n        assert is_iterable(targets)\n        assert isinstance(variable, basestring)\n        assert is_iterable(value)\n\n        if targets:\n            if append:\n                bjam_interface.call(\"set-target-variable\", targets, variable, value, \"true\")\n            else:\n                bjam_interface.call(\"set-target-variable\", targets, variable, value)", "output": "Sets a target variable.\n\n        The 'variable' will be available to bjam when it decides\n        where to generate targets, and will also be available to\n        updating rule for that 'taret'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def proxy_type(v):\n    \"\"\"  \"\"\"\n    proxies = []\n    if re.match(r\"((http|socks5):\\/\\/.)?(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}):(\\d{1,5})\", v):\n        proxies.append({\"http\": v,\n                        \"https\": v})\n        return proxies\n    elif re.match(r\"((http|socks5):\\/\\/.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}:(\\d{1,5})\", v):\n        proxies.append({\"http\": v,\n                        \"https\": v})\n        return proxies\n    elif is_proxy_list(v, proxies):\n        return proxies\n    else:\n        raise argparse.ArgumentTypeError(\n            \"Proxy should follow IP:PORT or DOMAIN:PORT format\")", "output": "Match IP:PORT or DOMAIN:PORT in a losse manner", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(self, client=None):\n        \"\"\"\n        \"\"\"\n        client = self._require_client(client)\n        client.metrics_api.metric_create(\n            self.project, self.name, self.filter_, self.description\n        )", "output": "API call:  create the metric via a PUT request\n\n        See\n        https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.metrics/create\n\n        :type client: :class:`~google.cloud.logging.client.Client` or\n                      ``NoneType``\n        :param client: the client to use.  If not passed, falls back to the\n                       ``client`` stored on the current metric.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install_missing(name, version=None, source=None):\n    '''\n    \n    '''\n    choc_path = _find_chocolatey(__context__, __salt__)\n    if _LooseVersion(chocolatey_version()) >= _LooseVersion('0.9.8.24'):\n        log.warning('installmissing is deprecated, using install')\n        return install(name, version=version)\n\n    # chocolatey helpfully only supports a single package argument\n    cmd = [choc_path, 'installmissing', name]\n    if version:\n        cmd.extend(['--version', version])\n    if source:\n        cmd.extend(['--source', source])\n    # Shouldn't need this as this code should never run on v0.9.9 and newer\n    cmd.extend(_yes(__context__))\n    result = __salt__['cmd.run_all'](cmd, python_shell=False)\n\n    if result['retcode'] != 0:\n        raise CommandExecutionError(\n            'Running chocolatey failed: {0}'.format(result['stdout'])\n        )\n\n    return result['stdout']", "output": "Instructs Chocolatey to install a package if it doesn't already exist.\n\n    .. versionchanged:: 2014.7.0\n        If the minion has Chocolatey >= 0.9.8.24 installed, this function calls\n        :mod:`chocolatey.install <salt.modules.chocolatey.install>` instead, as\n        ``installmissing`` is deprecated as of that version and will be removed\n        in Chocolatey 1.0.\n\n    name\n        The name of the package to be installed. Only accepts a single argument.\n\n    version\n        Install a specific version of the package. Defaults to latest version\n        available.\n\n    source\n        Chocolatey repository (directory, share or remote URL feed) the package\n        comes from. Defaults to the official Chocolatey feed.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' chocolatey.install_missing <package name>\n        salt '*' chocolatey.install_missing <package name> version=<package version>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _HasOOOWrite(self, path):\n    \"\"\"\"\"\"\n    # Check the sizes of each path before the current one.\n    size = tf.io.gfile.stat(path).length\n    old_size = self._finalized_sizes.get(path, None)\n    if size != old_size:\n      if old_size is None:\n        logger.error('File %s created after file %s even though it\\'s '\n                         'lexicographically earlier', path, self._path)\n      else:\n        logger.error('File %s updated even though the current file is %s',\n                         path, self._path)\n      return True\n    else:\n      return False", "output": "Returns whether the path has had an out-of-order write.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_parent(self):\n        \"\"\"\n        \"\"\"\n        if self.is_partial:\n            parent_args = self.flat_path[:-1]\n        else:\n            parent_args = self.flat_path[:-2]\n        if parent_args:\n            return self.__class__(\n                *parent_args, project=self.project, namespace=self.namespace\n            )", "output": "Creates a parent key for the current path.\n\n        Extracts all but the last element in the key path and creates a new\n        key, while still matching the namespace and the project.\n\n        :rtype: :class:`google.cloud.datastore.key.Key` or :class:`NoneType`\n        :returns: A new ``Key`` instance, whose path consists of all but the\n                  last element of current path. If the current key has only\n                  one path element, returns ``None``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def local_attention1d_spatial_decoder(x, kv_dim, heads_dim,\n                                      feedforward_dim, hparams):\n  \"\"\"\"\"\"\n  batch_dim, length_dim, model_dim = x.shape.dims\n  blocks_w_dim = mtf.Dimension(\"blocksw\", hparams.block_length)\n  num_w_blocks_dim = mtf.Dimension(\"num_wblocks\",\n                                   length_dim.size // blocks_w_dim.size)\n  x = mtf.reshape(\n      x, mtf.Shape([batch_dim, num_w_blocks_dim, blocks_w_dim, model_dim]))\n  # [ self attention - ffn - residual + dropout] x n\n  for layer in range(hparams.num_decoder_layers):\n    layer_name = \"decoder_layer_%d\" % layer\n    with tf.variable_scope(layer_name):\n      # Self attention layer\n      x += layer_prepostprocess_dropout(\n          mtf.layers.local_self_attention_spatial_blocks(\n              mtf.layers.layer_norm(x, model_dim, name=\"layer_norm_att\"),\n              kv_dim,\n              heads_dim,\n              memory_w_dim=blocks_w_dim,\n              mask_right=True,\n              name=\"self_att\"), hparams)\n      # ffn layer\n      x += layer_prepostprocess_dropout(\n          mtf.layers.dense_relu_dense(\n              mtf.layers.layer_norm(x, model_dim, name=\"layer_norm_ffn\"),\n              feedforward_dim,\n              hparams.dropout,\n              dropout_broadcast_dims=[length_dim]), hparams)\n\n  output = mtf.layers.layer_norm(x, model_dim, name=\"final_layer_norm\")\n  return output", "output": "Image Transformer decoder with local1D spatial layers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_hparam(self, name, value):\n    \"\"\"\n    \"\"\"\n    param_type, is_list = self._hparam_types[name]\n    if isinstance(value, list):\n      if not is_list:\n        raise ValueError(\n            'Must not pass a list for single-valued parameter: %s' % name)\n      setattr(self, name, [\n          _cast_to_type_if_compatible(name, param_type, v) for v in value])\n    else:\n      if is_list:\n        raise ValueError(\n            'Must pass a list for multi-valued parameter: %s.' % name)\n      setattr(self, name, _cast_to_type_if_compatible(name, param_type, value))", "output": "Set the value of an existing hyperparameter.\n\n    This function verifies that the type of the value matches the type of the\n    existing hyperparameter.\n\n    Args:\n      name: Name of the hyperparameter.\n      value: New value of the hyperparameter.\n\n    Raises:\n      KeyError: If the hyperparameter doesn't exist.\n      ValueError: If there is a type mismatch.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode(x, x_space, hparams, name):\n  \"\"\"\"\"\"\n  with tf.variable_scope(name):\n    (encoder_input, encoder_self_attention_bias,\n     ed) = transformer.transformer_prepare_encoder(x, x_space, hparams)\n    encoder_input = tf.nn.dropout(encoder_input, 1.0 - hparams.dropout)\n    return transformer.transformer_encoder(\n        encoder_input, encoder_self_attention_bias, hparams), ed", "output": "Transformer preparations and encoder.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_environment(self, environment):\n        \"\"\"\"\"\"\n        todo = deque([self])\n        while todo:\n            node = todo.popleft()\n            node.environment = environment\n            todo.extend(node.iter_child_nodes())\n        return self", "output": "Set the environment for all nodes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bestExponent(self, seriesLenght):\n        '''\n        \n        '''\n        i = 0\n        cont = True\n        while(cont):\n            if(int(seriesLenght/int(math.pow(2, i))) <= 1):\n                cont = False\n            else:\n                i += 1\n        return int(i-1)", "output": ":type seriesLenght: int\n        :rtype: int", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def owners(self):\n        \"\"\"\"\"\"\n        result = set()\n        for role in self._OWNER_ROLES:\n            for member in self._bindings.get(role, ()):\n                result.add(member)\n        return frozenset(result)", "output": "Legacy access to owner role.\n\n        DEPRECATED:  use ``policy[\"roles/owners\"]`` instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_file_network(data, filename, create=False):\n    '''\n    \n    '''\n    if not os.path.exists(filename) and not create:\n        msg = '{0} cannot be written. {0} does not exist\\\n                and create is set to False'\n        msg = msg.format(filename)\n        log.error(msg)\n        raise AttributeError(msg)\n    with salt.utils.files.flopen(filename, 'w') as fout:\n        fout.write(salt.utils.stringutils.to_str(data))", "output": "Writes a file to disk\n    If file does not exist, only create if create\n    argument is True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_input_buffer(self, text):\r\n        \"\"\"\"\"\"\r\n        if self.current_prompt_pos is not None:\r\n            self.replace_text(self.current_prompt_pos, 'eol', text)\r\n        else:\r\n            self.insert(text)\r\n        self.set_cursor_position('eof')", "output": "Set input buffer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sys_path(self):\n        \"\"\"\n        \n        \"\"\"\n\n        from .vendor.vistir.compat import JSONDecodeError\n        current_executable = vistir.compat.Path(sys.executable).as_posix()\n        if not self.python or self.python == current_executable:\n            return sys.path\n        elif any([sys.prefix == self.prefix, not self.is_venv]):\n            return sys.path\n        cmd_args = [self.python, \"-c\", \"import json, sys; print(json.dumps(sys.path))\"]\n        path, _ = vistir.misc.run(cmd_args, return_object=False, nospin=True, block=True, combine_stderr=False, write_to_stdout=False)\n        try:\n            path = json.loads(path.strip())\n        except JSONDecodeError:\n            path = sys.path\n        return path", "output": "The system path inside the environment\n\n        :return: The :data:`sys.path` from the environment\n        :rtype: list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def usergroup_update(usrgrpid, **kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'usergroup.update'\n            params = {\"usrgrpid\": usrgrpid}\n            params = _params_extend(params, **kwargs)\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result']['usrgrpids']\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": ".. versionadded:: 2016.3.0\n\n    Update existing user group\n\n    .. note::\n        This function accepts all standard user group properties: keyword\n        argument names differ depending on your zabbix version, see here__.\n\n        .. __: https://www.zabbix.com/documentation/2.4/manual/api/reference/usergroup/object#user_group\n\n    :param usrgrpid: ID of the user group to update.\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    :return: IDs of the updated user group, False on failure.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.usergroup_update 8 name=guestsRenamed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_examples_validation(self, archive, labels):\n    \"\"\"\n    \"\"\"\n    # Get the current random seeds.\n    numpy_st0 = np.random.get_state()\n    # Set new random seeds.\n    np.random.seed(135)\n    logging.warning('Overwriting cv2 RNG seed.')\n    tfds.core.lazy_imports.cv2.setRNGSeed(357)\n\n    for example in super(Imagenet2012Corrupted,\n                         self)._generate_examples_validation(archive, labels):\n      with tf.Graph().as_default():\n        tf_img = tf.image.decode_jpeg(example['image'].read(), channels=3)\n        image_np = tfds.as_numpy(tf_img)\n      example['image'] = self._get_corrupted_example(image_np)\n      yield example\n    # Reset the seeds back to their original values.\n    np.random.set_state(numpy_st0)", "output": "Generate corrupted imagenet validation data.\n\n    Apply corruptions to the raw images according to self.corruption_type.\n\n    Args:\n      archive: an iterator for the raw dataset.\n      labels: a dictionary that maps the file names to imagenet labels.\n\n    Yields:\n      dictionary with the file name, an image file objective, and label of each\n      imagenet validation data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_metadata(self, existing):\n        \"\"\"  \"\"\"\n        self.metadata = [\n            c.name for c in self.values_axes if c.metadata is not None]", "output": "create / validate metadata", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put_lifecycle_configuration(Bucket,\n           Rules,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        if Rules is not None and isinstance(Rules, six.string_types):\n            Rules = salt.utils.json.loads(Rules)\n        conn.put_bucket_lifecycle_configuration(Bucket=Bucket, LifecycleConfiguration={'Rules': Rules})\n        return {'updated': True, 'name': Bucket}\n    except ClientError as e:\n        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Given a valid config, update the Lifecycle rules for a bucket.\n\n    Returns {updated: true} if Lifecycle was updated and returns\n    {updated: False} if Lifecycle was not updated.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.put_lifecycle_configuration my_bucket '[{\\\\\n              \"Expiration\": {...},\\\\\n              \"ID\": \"idstring\",\\\\\n              \"Prefix\": \"prefixstring\",\\\\\n              \"Status\": \"enabled\",\\\\\n              \"Transitions\": [{...},],\\\\\n              \"NoncurrentVersionTransitions\": [{...},],\\\\\n              \"NoncurrentVersionExpiration\": {...},\\\\\n        }]'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_lenet():\n    \"\"\"\n    \"\"\"\n    data = mx.symbol.Variable('data')\n\n    # first conv\n    conv1 = mx.symbol.CaffeOp(data_0=data, num_weight=2,\n                              prototxt=\"layer{type:\\\"Convolution\\\" \"\n                                       \"convolution_param { num_output: 20 kernel_size: 5 stride: 1} }\")\n    act1 = mx.symbol.CaffeOp(data_0=conv1, prototxt=\"layer{type:\\\"TanH\\\"}\")\n    pool1 = mx.symbol.CaffeOp(data_0=act1,\n                              prototxt=\"layer{type:\\\"Pooling\\\" pooling_param { pool: MAX kernel_size: 2 stride: 2}}\")\n\n    # second conv\n    conv2 = mx.symbol.CaffeOp(data_0=pool1, num_weight=2,\n                              prototxt=\"layer{type:\\\"Convolution\\\" \"\n                                       \"convolution_param { num_output: 50 kernel_size: 5 stride: 1} }\")\n    act2 = mx.symbol.CaffeOp(data_0=conv2, prototxt=\"layer{type:\\\"TanH\\\"}\")\n    pool2 = mx.symbol.CaffeOp(data_0=act2,\n                              prototxt=\"layer{type:\\\"Pooling\\\" pooling_param { pool: MAX kernel_size: 2 stride: 2}}\")\n\n    fc1 = mx.symbol.CaffeOp(data_0=pool2, num_weight=2,\n                            prototxt=\"layer{type:\\\"InnerProduct\\\" inner_product_param{num_output: 500} }\")\n    act3 = mx.symbol.CaffeOp(data_0=fc1, prototxt=\"layer{type:\\\"TanH\\\"}\")\n\n    # second fullc\n    fc2 = mx.symbol.CaffeOp(data_0=act3, num_weight=2,\n                            prototxt=\"layer{type:\\\"InnerProduct\\\"inner_product_param{num_output: 10} }\")\n    if use_caffe_loss:\n        label = mx.symbol.Variable('softmax_label')\n        lenet = mx.symbol.CaffeLoss(data=fc2, label=label, grad_scale=1, name='softmax',\n                                    prototxt=\"layer{type:\\\"SoftmaxWithLoss\\\"}\")\n    else:\n        lenet = mx.symbol.SoftmaxOutput(data=fc2, name='softmax')\n    return lenet", "output": "LeCun, Yann, Leon Bottou, Yoshua Bengio, and Patrick\n    Haffner. \"Gradient-based learning applied to document recognition.\"\n    Proceedings of the IEEE (1998)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extract_war_version(war):\n    '''\n    \n    '''\n    basename = os.path.basename(war)\n    war_package = os.path.splitext(basename)[0]  # remove '.war'\n    version = re.findall(\"-([\\\\d.-]+)$\", war_package)  # try semver\n    return version[0] if version and len(version) == 1 else None", "output": "Extract the version from the war file name. There does not seem to be a\n    standard for encoding the version into the `war file name`_\n\n    .. _`war file name`: https://tomcat.apache.org/tomcat-6.0-doc/deployer-howto.html\n\n    Examples:\n\n    .. code-block:: bash\n\n        /path/salt-2015.8.6.war -> 2015.8.6\n        /path/V6R2013xD5.war -> None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hybrid_forward(self, F, feature_a):\n        \"\"\"\n        \n        \"\"\"\n        tilde_a = self.intra_attn_emb(feature_a)\n        e_matrix = F.batch_dot(tilde_a, tilde_a, transpose_b=True)\n        alpha = F.batch_dot(e_matrix.softmax(), tilde_a)\n        return alpha", "output": "Compute intra-sentence attention given embedded words.\n\n        Parameters\n        ----------\n        feature_a : NDArray\n            Shape (batch_size, length, hidden_size)\n\n        Returns\n        -------\n        alpha : NDArray\n            Shape (batch_size, length, hidden_size)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_transform(self, model_output):\n        \"\"\" \n        \"\"\"\n        if model_output == \"margin\":\n            transform = \"identity\"\n        elif model_output == \"probability\":\n            if self.tree_output == \"log_odds\":\n                transform = \"logistic\"\n            elif self.tree_output == \"probability\":\n                transform = \"identity\"\n            else:\n                raise Exception(\"model_output = \\\"probability\\\" is not yet supported when model.tree_output = \\\"\" + self.tree_output + \"\\\"!\")\n        elif model_output == \"logloss\":\n\n            if self.objective == \"squared_error\":\n                transform = \"squared_loss\"\n            elif self.objective == \"binary_crossentropy\":\n                transform = \"logistic_nlogloss\"\n            else:\n                raise Exception(\"model_output = \\\"logloss\\\" is not yet supported when model.objective = \\\"\" + self.objective + \"\\\"!\")\n        return transform", "output": "A consistent interface to make predictions from this model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def scheduled_sample_prob(ground_truth_x,\n                          generated_x,\n                          batch_size,\n                          scheduled_sample_var):\n  \"\"\"\n  \"\"\"\n  probability_threshold = scheduled_sample_var\n  probability_of_generated = tf.random_uniform([batch_size])\n  return tf.where(probability_of_generated > probability_threshold,\n                  generated_x, ground_truth_x)", "output": "Probability based scheduled sampling.\n\n  Args:\n    ground_truth_x: tensor of ground-truth data points.\n    generated_x: tensor of generated data points.\n    batch_size: batch size\n    scheduled_sample_var: probability of choosing from ground_truth.\n  Returns:\n    New batch with randomly selected data points.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lengths_to_area_mask(feature_length, length, max_area_size):\n  \"\"\"\n  \"\"\"\n\n  paddings = tf.cast(tf.expand_dims(\n      tf.logical_not(\n          tf.sequence_mask(feature_length, maxlen=length)), 2), tf.float32)\n  _, _, area_sum, _, _ = compute_area_features(paddings,\n                                               max_area_width=max_area_size)\n  mask = tf.squeeze(tf.logical_not(tf.cast(area_sum, tf.bool)), [2])\n  return mask", "output": "Generates a non-padding mask for areas based on lengths.\n\n  Args:\n    feature_length: a tensor of [batch_size]\n    length: the length of the batch\n    max_area_size: the maximum area size considered\n  Returns:\n    mask: a tensor in shape of [batch_size, num_areas]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_name(name, maxlen=80):\n    \"\"\"\n    \"\"\"\n    if not isinstance(name, basestring):\n        msg = \"Name must be of type string\"\n        raise InvalidAwsLambdaName(msg)\n    if len(name) > maxlen:\n        msg = \"Name is longer than {maxlen} characters.\"\n        raise InvalidAwsLambdaName(msg.format(maxlen=maxlen))\n    if len(name) == 0:\n        msg = \"Name must not be empty string.\"\n        raise InvalidAwsLambdaName(msg)\n    if not re.match(\"^[a-zA-Z0-9-_]+$\", name):\n        msg = \"Name can only contain characters from a-z, A-Z, 0-9, _ and -\"\n        raise InvalidAwsLambdaName(msg)\n    return name", "output": "Validate name for AWS Lambda function.\n    name: actual name (without `arn:aws:lambda:...:` prefix and without\n        `:$LATEST`, alias or version suffix.\n    maxlen: max allowed length for name without prefix and suffix.\n\n    The value 80 was calculated from prefix with longest known region name\n    and assuming that no alias or version would be longer than `$LATEST`.\n\n    Based on AWS Lambda spec\n    http://docs.aws.amazon.com/lambda/latest/dg/API_CreateFunction.html\n\n    Return: the name\n    Raise: InvalidAwsLambdaName, if the name is invalid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rotate_v1(array, k):\n    \"\"\"\n    \n    \"\"\"\n    array = array[:]\n    n = len(array)\n    for i in range(k):      # unused variable is not a problem\n        temp = array[n - 1]\n        for j in range(n-1, 0, -1):\n            array[j] = array[j - 1]\n        array[0] = temp\n    return array", "output": "Rotate the entire array 'k' times\n    T(n)- O(nk)\n\n    :type array: List[int]\n    :type k: int\n    :rtype: void Do not return anything, modify array in-place instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render_embed_css(self, css_embed: Iterable[bytes]) -> bytes:\n        \"\"\"\n        \"\"\"\n        return b'<style type=\"text/css\">\\n' + b\"\\n\".join(css_embed) + b\"\\n</style>\"", "output": "Default method used to render the final embedded css for the\n        rendered webpage.\n\n        Override this method in a sub-classed controller to change the output.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def returner(load):\n    '''\n    \n    '''\n    cb_ = _get_connection()\n\n    hn_key = '{0}/{1}'.format(load['jid'], load['id'])\n    try:\n        ret_doc = {'return': load['return'],\n                   'full_ret': salt.utils.json.dumps(load)}\n\n        cb_.add(hn_key,\n               ret_doc,\n               ttl=_get_ttl(),\n               )\n    except couchbase.exceptions.KeyExistsError:\n        log.error(\n            'An extra return was detected from minion %s, please verify '\n            'the minion, this could be a replay attack', load['id']\n        )\n        return False", "output": "Return data to couchbase bucket", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mknod_chrdev(name,\n                 major,\n                 minor,\n                 user=None,\n                 group=None,\n                 mode='0660'):\n    '''\n    \n    '''\n    name = os.path.expanduser(name)\n\n    ret = {'name': name,\n           'changes': {},\n           'comment': '',\n           'result': False}\n    log.debug('Creating character device name:%s major:%s minor:%s mode:%s',\n              name, major, minor, mode)\n    try:\n        if __opts__['test']:\n            ret['changes'] = {'new': 'Character device {0} created.'.format(name)}\n            ret['result'] = None\n        else:\n            if os.mknod(name,\n                        int(six.text_type(mode).lstrip('0Oo'), 8) | stat.S_IFCHR,\n                        os.makedev(major, minor)) is None:\n                ret['changes'] = {'new': 'Character device {0} created.'.format(name)}\n                ret['result'] = True\n    except OSError as exc:\n        # be happy it is already there....however, if you are trying to change the\n        # major/minor, you will need to unlink it first as os.mknod will not overwrite\n        if exc.errno != errno.EEXIST:\n            raise\n        else:\n            ret['comment'] = 'File {0} exists and cannot be overwritten'.format(name)\n    # quick pass at verifying the permissions of the newly created character device\n    check_perms(name,\n                None,\n                user,\n                group,\n                int('{0}'.format(mode)) if mode else None)\n    return ret", "output": ".. versionadded:: 0.17.0\n\n    Create a character device.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n       salt '*' file.mknod_chrdev /dev/chr 180 31", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_delete(self, path: str, handler: _WebHandler,\n                   **kwargs: Any) -> AbstractRoute:\n        \"\"\"\n        \n        \"\"\"\n        return self.add_route(hdrs.METH_DELETE, path, handler, **kwargs)", "output": "Shortcut for add_route with method DELETE", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def network_io_counters(interface=None):\n    '''\n    \n    '''\n    if not interface:\n        return dict(psutil.net_io_counters()._asdict())\n    else:\n        stats = psutil.net_io_counters(pernic=True)\n        if interface in stats:\n            return dict(stats[interface]._asdict())\n        else:\n            return False", "output": "Return network I/O statistics.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.network_io_counters\n\n        salt '*' ps.network_io_counters interface=eth0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def smoothing_cross_entropy_factored(a, b, labels, confidence):\n  \"\"\"\n  \"\"\"\n  num_splits = 16\n  vocab_size = shape_list(b)[0]\n  labels = approximate_split(labels, num_splits)\n  a = approximate_split(a, num_splits)\n  parts = []\n  for part in range(num_splits):\n    with tf.control_dependencies(parts[-1:]):\n      logits = tf.matmul(a[part], b, transpose_b=True)\n      parts.append(\n          smoothing_cross_entropy(logits, labels[part], vocab_size, confidence))\n  return tf.concat(parts, 0)", "output": "Memory-efficient computation of smoothing cross-entropy.\n\n  Avoids realizing the entire logits matrix at once.\n\n  Args:\n    a: a Tensor with shape [batch, inner_dim]\n    b: a Tensor with shape [vocab_size, inner_dim]\n    labels: an integer Tensor with shape [batch]\n    confidence: a float\n\n  Returns:\n    A Tensor with shape [batch]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(name, profile=\"splunk\"):\n    '''\n    \n    '''\n    client = _get_splunk(profile)\n    try:\n        client.saved_searches.delete(name)\n        return True\n    except KeyError:\n        return None", "output": "Delete a splunk search\n\n    CLI Example:\n\n       splunk_search.delete 'my search name'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_proc_file(path, opts):\n    '''\n    \n    '''\n    serial = salt.payload.Serial(opts)\n    with salt.utils.files.fopen(path, 'rb') as fp_:\n        try:\n            data = serial.load(fp_)\n        except Exception as err:\n            # need to add serial exception here\n            # Could not read proc file\n            log.warning(\"Issue deserializing data: %s\", err)\n            return None\n\n    if not isinstance(data, dict):\n        # Invalid serial object\n        log.warning(\"Data is not a dict: %s\", data)\n        return None\n\n    pid = data.get('pid', None)\n    if not pid:\n        # No pid, not a salt proc file\n        log.warning(\"No PID found in data\")\n        return None\n\n    return data", "output": "Return a dict of JID metadata, or None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wm_preferences(name,\n                   user=None,\n                   action_double_click_titlebar=None,\n                   action_middle_click_titlebar=None,\n                   action_right_click_titlebar=None,\n                   application_based=None,\n                   audible_bell=None,\n                   auto_raise=None,\n                   auto_raise_delay=None,\n                   button_layout=None,\n                   disable_workarounds=None,\n                   focus_mode=None,\n                   focus_new_windows=None,\n                   mouse_button_modifier=None,\n                   num_workspaces=None,\n                   raise_on_click=None,\n                   resize_with_right_button=None,\n                   theme=None,\n                   titlebar_font=None,\n                   titlebar_uses_system_font=None,\n                   visual_bell=None,\n                   visual_bell_type=None,\n                   workspace_names=None,\n                   **kwargs):\n    '''\n    \n    '''\n    gnome_kwargs = {\n        'user': user,\n        'schema': 'org.gnome.desktop.wm.preferences'\n    }\n\n    preferences = ['action_double_click_titlebar',\n            'action_middle_click_titlebar', 'action_right_click_titlebar',\n            'application_based', 'audible_bell', 'auto_raise',\n            'auto_raise_delay', 'button_layout', 'disable_workarounds',\n            'focus_mode', 'focus_new_windows', 'mouse_button_modifier',\n            'num_workspaces', 'raise_on_click', 'resize_with_right_button',\n            'theme', 'titlebar_font', 'titlebar_uses_system_font',\n            'visual_bell', 'visual_bell_type', 'workspace_names']\n\n    preferences_hash = {}\n    for pref in preferences:\n        if pref in locals() and locals()[pref] is not None:\n            key = re.sub('_', '-', pref)\n            preferences_hash[key] = locals()[pref]\n\n    return _do(name, gnome_kwargs, preferences_hash)", "output": "wm_preferences: sets values in the org.gnome.desktop.wm.preferences schema", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_column_families(self):\n        \"\"\"\n        \"\"\"\n        table_client = self._instance._client.table_admin_client\n        table_pb = table_client.get_table(self.name)\n\n        result = {}\n        for column_family_id, value_pb in table_pb.column_families.items():\n            gc_rule = _gc_rule_from_pb(value_pb.gc_rule)\n            column_family = self.column_family(column_family_id, gc_rule=gc_rule)\n            result[column_family_id] = column_family\n        return result", "output": "List the column families owned by this table.\n\n        For example:\n\n        .. literalinclude:: snippets_table.py\n            :start-after: [START bigtable_list_column_families]\n            :end-before: [END bigtable_list_column_families]\n\n        :rtype: dict\n        :returns: Dictionary of column families attached to this table. Keys\n                  are strings (column family names) and values are\n                  :class:`.ColumnFamily` instances.\n        :raises: :class:`ValueError <exceptions.ValueError>` if the column\n                 family name from the response does not agree with the computed\n                 name from the column family ID.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reverse_url(self, name: str, *args: Any) -> str:\n        \"\"\"\n        \"\"\"\n        reversed_url = self.default_router.reverse_url(name, *args)\n        if reversed_url is not None:\n            return reversed_url\n\n        raise KeyError(\"%s not found in named urls\" % name)", "output": "Returns a URL path for handler named ``name``\n\n        The handler must be added to the application as a named `URLSpec`.\n\n        Args will be substituted for capturing groups in the `URLSpec` regex.\n        They will be converted to strings if necessary, encoded as utf8,\n        and url-escaped.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_layer_converter_fn(layer, add_custom_layers = False):\n    \"\"\"\n    \"\"\"\n    layer_type = type(layer)\n    if layer_type in _KERAS_LAYER_REGISTRY:\n        convert_func = _KERAS_LAYER_REGISTRY[layer_type]\n        if convert_func is _layers2.convert_activation:\n            act_name = _layers2._get_activation_name_from_keras_layer(layer)\n            if act_name == 'CUSTOM':\n                return None\n        return convert_func\n    elif add_custom_layers:\n        return None\n    else:\n        raise TypeError(\"Keras layer of type %s is not supported.\" % type(layer))", "output": "Get the right converter function for Keras", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def service_delete(service_id=None, **kwargs):\n    '''\n    \n    '''\n\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'service.delete'\n            if not service_id:\n                return {'result': False, 'comment': 'service_id param is required'}\n            params = [str(service_id)]\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result'] if ret['result'] else False\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": ".. versionadded:: Fluorine\n\n    Delete service specified by id.\n\n    .. note::\n        https://www.zabbix.com/documentation/3.4/manual/api/reference/service/delete\n\n    :param service_id: ID of service which should be deleted\n\n    .. note::\n        Service can't be deleted if it has any children.\n\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    :return: ID of deleted service, False if service could not be deleted or on failure.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.service_delete 10", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close(self, reason=None):\n        \"\"\"\n        \"\"\"\n        with self._closing:\n            if self._closed:\n                return\n\n            # Stop consuming messages.\n            if self.is_active:\n                _LOGGER.debug(\"Stopping consumer.\")\n                self._consumer.stop()\n            self._consumer = None\n\n            self._rpc.close()\n            self._rpc = None\n            self._closed = True\n            _LOGGER.debug(\"Finished stopping manager.\")\n\n        if reason:\n            # Raise an exception if a reason is provided\n            _LOGGER.debug(\"reason for closing: %s\" % reason)\n            if isinstance(reason, Exception):\n                raise reason\n            raise RuntimeError(reason)", "output": "Stop consuming messages and shutdown all helper threads.\n\n        This method is idempotent. Additional calls will have no effect.\n\n        Args:\n            reason (Any): The reason to close this. If None, this is considered\n                an \"intentional\" shutdown.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _remove_double_brackets(text):\n  \"\"\"\n  \"\"\"\n  def replacement_fn(s):\n    if u\":\" in s:\n      # this is probably a category or something like that.\n      return \"\"\n    # keep the part after the bar.\n    bar_pos = s.find(u\"|\")\n    if bar_pos == -1:\n      return s\n    return s[bar_pos + 1:]\n  return _find_and_replace(text, u\"[[\", u\"]]\", replacement_fn)", "output": "Remove double brackets (internal links) but leave the viewable text.\n\n  Args:\n    text: a unicode string\n  Returns:\n    a unicode string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decompress_decoder(inputs,\n                       hparams,\n                       strides=(2, 2),\n                       kernel=(3, 3),\n                       name=None):\n  \"\"\"\n  \"\"\"\n  with tf.variable_scope(name, default_name=\"decompress\"):\n    x = inputs\n    x = tf.layers.dense(x, hparams.hidden_size, name=name + \"_dense\")\n    x = residual_block_layer(x, hparams)\n    for i in range(hparams.num_compress_steps // 2):\n      j = hparams.num_compress_steps // 2 - i - 1\n      with tf.variable_scope(name + \"_%d\" % j):\n        if hparams.do_decompress_attend:\n          y = compress_self_attention_layer(\n              x, hparams, name=\"decompress_selfatt\")\n          x += y\n        y = tf.layers.conv2d_transpose(\n            x,\n            hparams.hidden_size,\n            kernel,\n            strides=strides,\n            padding=\"SAME\",\n            activation=tf.nn.relu if i > 0 else None,\n            name=\"decompress_conv\")\n        x = y\n    return x", "output": "Decoder that decompresses 2-D inputs by 2**num_compress_steps.\n\n  Args:\n    inputs: Tensor of shape [batch, compress_height, compress_width, channels].\n    hparams: HParams.\n    strides: Tuple, strides for conv block.\n    kernel: Tuple, kernel window size for conv block.\n    name: string, variable scope.\n\n  Returns:\n    Tensor of shape [batch, height, width, hparams.hidden_size].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_powercfg_minute_values(scheme, guid, subguid, safe_name):\n    '''\n    \n    '''\n    if scheme is None:\n        scheme = _get_current_scheme()\n\n    if __grains__['osrelease'] == '7':\n        cmd = 'powercfg /q {0} {1}'.format(scheme, guid)\n    else:\n        cmd = 'powercfg /q {0} {1} {2}'.format(scheme, guid, subguid)\n    out = __salt__['cmd.run'](cmd, python_shell=False)\n\n    split = out.split('\\r\\n\\r\\n')\n    if len(split) > 1:\n        for s in split:\n            if safe_name in s or subguid in s:\n                out = s\n                break\n    else:\n        out = split[0]\n\n    raw_settings = re.findall(r'Power Setting Index: ([0-9a-fx]+)', out)\n    return {'ac': int(raw_settings[0], 0) / 60,\n            'dc': int(raw_settings[1], 0) / 60}", "output": "Returns the AC/DC values in an dict for a guid and subguid for a the given\n    scheme", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_toolbar(self):\r\n        \"\"\"\"\"\"\r\n        load_button = create_toolbutton(self, text=_('Import data'),\r\n                                        icon=ima.icon('fileimport'),\r\n                                        triggered=lambda: self.import_data())\r\n        self.save_button = create_toolbutton(self, text=_(\"Save data\"),\r\n                            icon=ima.icon('filesave'),\r\n                            triggered=lambda: self.save_data(self.filename))\r\n        self.save_button.setEnabled(False)\r\n        save_as_button = create_toolbutton(self,\r\n                                           text=_(\"Save data as...\"),\r\n                                           icon=ima.icon('filesaveas'),\r\n                                           triggered=self.save_data)\r\n        reset_namespace_button = create_toolbutton(\r\n                self, text=_(\"Remove all variables\"),\r\n                icon=ima.icon('editdelete'), triggered=self.reset_namespace)\r\n\r\n        return [load_button, self.save_button, save_as_button,\r\n                reset_namespace_button]", "output": "Setup toolbar", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def post_data(self, job_id, body, params=None):\n        \"\"\"\n        \n        \"\"\"\n        for param in (job_id, body):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"POST\",\n            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_data\"),\n            params=params,\n            body=self._bulk_body(body),\n        )", "output": "`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-post-data.html>`_\n\n        :arg job_id: The name of the job receiving the data\n        :arg body: The data to process\n        :arg reset_end: Optional parameter to specify the end of the bucket\n            resetting range\n        :arg reset_start: Optional parameter to specify the start of the bucket\n            resetting range", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_types(self) -> None:\n        \"\"\"\n        \n        \"\"\"\n        all_instance_fields_and_types: List[Dict[str, str]] = [{k: v.__class__.__name__\n                                                                for k, v in x.fields.items()}\n                                                               for x in self.instances]\n        # Check all the field names and Field types are the same for every instance.\n        if not all([all_instance_fields_and_types[0] == x for x in all_instance_fields_and_types]):\n            raise ConfigurationError(\"You cannot construct a Batch with non-homogeneous Instances.\")", "output": "Check that all the instances have the same types.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_object_attrs(obj):\n    \"\"\"\n    \n\n    \"\"\"\n    attrs = []\n    if hasattr(obj, 'dtype'):\n        attrs.append(('dtype', \"'{}'\".format(obj.dtype)))\n    if getattr(obj, 'name', None) is not None:\n        attrs.append(('name', default_pprint(obj.name)))\n    max_seq_items = get_option('display.max_seq_items') or len(obj)\n    if len(obj) > max_seq_items:\n        attrs.append(('length', len(obj)))\n    return attrs", "output": "Return a list of tuples of the (attr, formatted_value)\n    for common attrs, including dtype, name, length\n\n    Parameters\n    ----------\n    obj : object\n        must be iterable\n\n    Returns\n    -------\n    list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def has_substring(self, substring):\n        \"\"\"\n        \n        \"\"\"\n        return ArrayPredicate(\n            term=self,\n            op=LabelArray.has_substring,\n            opargs=(substring,),\n        )", "output": "Construct a Filter matching values containing ``substring``.\n\n        Parameters\n        ----------\n        substring : str\n            Sub-string against which to compare values produced by ``self``.\n\n        Returns\n        -------\n        matches : Filter\n            Filter returning True for all sid/date pairs for which ``self``\n            produces a string containing ``substring``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_dvs_link_discovery_protocol(dvs_name, dvs_link_disc_protocol):\n    '''\n    \n    '''\n    log.trace('Building the dict of the DVS \\'%s\\' link discovery protocol', dvs_name)\n    return {'operation': dvs_link_disc_protocol.operation,\n            'protocol': dvs_link_disc_protocol.protocol}", "output": "Returns the dict representation of the DVS link discovery protocol\n\n    dvs_name\n        The name of the DVS\n\n    dvs_link_disc_protocl\n        The DVS link discovery protocol", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _recurse_config_to_dict(t_data):\n    '''\n    \n    '''\n    if not isinstance(t_data, type(None)):\n        if isinstance(t_data, list):\n            t_list = []\n            for i in t_data:\n                t_list.append(_recurse_config_to_dict(i))\n            return t_list\n        elif isinstance(t_data, dict):\n            t_dict = {}\n            for k, v in six.iteritems(t_data):\n                t_dict[k] = _recurse_config_to_dict(v)\n            return t_dict\n        else:\n            if hasattr(t_data, '__dict__'):\n                return _recurse_config_to_dict(t_data.__dict__)\n            else:\n                return _serializer(t_data)", "output": "helper function to recurse through a vim object and attempt to return all child objects", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shutdown(vm_):\n    '''\n    \n    '''\n    with _get_xapi_session() as xapi:\n        vm_uuid = _get_label_uuid(xapi, 'VM', vm_)\n        if vm_uuid is False:\n            return False\n        try:\n            xapi.VM.clean_shutdown(vm_uuid)\n            return True\n        except Exception:\n            return False", "output": "Send a soft shutdown signal to the named vm\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' virt.shutdown <vm name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_model_stats(self, iteration:int)->None:\n        \"\"\n        # We don't want to write stats when model is not iterated on and hence has zeroed out gradients\n        gen_mode = self.learn.gan_trainer.gen_mode\n        if gen_mode and not self.gen_stats_updated: self._write_gen_model_stats(iteration=iteration)\n        if not gen_mode and not self.crit_stats_updated: self._write_critic_model_stats(iteration=iteration)", "output": "Writes gradient statistics to Tensorboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ustring(self,\n                indent,\n                color,\n                msg,\n                prefix='',\n                suffix='',\n                endc=None):\n        ''''''\n        if endc is None:\n            endc = self.ENDC  # pylint: disable=no-member\n\n        indent *= ' '\n        fmt = u'{0}{1}{2}{3}{4}{5}'\n\n        try:\n            return fmt.format(indent, color, prefix, msg, endc, suffix)\n        except UnicodeDecodeError:\n            return fmt.format(indent, color, prefix, salt.utils.data.decode(msg), endc, suffix)", "output": "Build the unicode string to be displayed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def declare_backward_dependency(self, out_grad, in_data, out_data):\n        \"\"\"\n        \"\"\"\n        deps = []\n        if self.need_top_grad_:\n            deps.extend(out_grad)\n        deps.extend(in_data)\n        deps.extend(out_data)\n        return deps", "output": "Declare dependencies of this operator for backward pass.\n\n        Parameters\n        ----------\n        out_grad : list of int\n            ids of out_grad blobs.\n        in_data : list of int\n            ids of in_data blobs.\n        out_data: list of int\n            ids of out_data blobs.\n\n        Returns\n        -------\n        deps : list of int\n            ids of the needed blobs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parameter_init(self, name, shape, init):\n        \"\"\"\n        \"\"\"\n        p = self.params.get(name, shape=shape, init=init)\n        return p", "output": "Create parameter given name, shape and initiator\n\n        Parameters\n        ----------\n        name : str\n            parameter name\n        shape : tuple\n            parameter shape\n        init : mxnet.initializer\n            an initializer\n\n        Returns\n        -------\n        mxnet.gluon.parameter\n            a parameter object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def argmin(self, rows: List[Row], column: ComparableColumn) -> List[Row]:\n        \"\"\"\n        \n        \"\"\"\n        if not rows:\n            return []\n        value_row_pairs = [(row.values[column.name], row) for row in rows]\n        if not value_row_pairs:\n            return []\n        # Returns a list containing the row with the max cell value.\n        return [sorted(value_row_pairs, key=lambda x: x[0])[0][1]]", "output": "Takes a list of rows and a column and returns a list containing a single row (dict from\n        columns to cells) that has the minimum numerical value in the given column. We return a list\n        instead of a single dict to be consistent with the return type of ``select`` and\n        ``all_rows``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def spacetodepth(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'blocksize':'block_size'})\n\n    return \"space_to_depth\", new_attrs, inputs", "output": "Rearranges blocks of spatial data into depth.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_business_hours_by_sec(self):\n        \"\"\"\n        \n        \"\"\"\n        if self._get_daytime_flag:\n            # create dummy datetime to calculate businesshours in a day\n            dtstart = datetime(2014, 4, 1, self.start.hour, self.start.minute)\n            until = datetime(2014, 4, 1, self.end.hour, self.end.minute)\n            return (until - dtstart).total_seconds()\n        else:\n            dtstart = datetime(2014, 4, 1, self.start.hour, self.start.minute)\n            until = datetime(2014, 4, 2, self.end.hour, self.end.minute)\n            return (until - dtstart).total_seconds()", "output": "Return business hours in a day by seconds.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ext_pillar(minion_id, pillar, function, **kwargs):\n    '''\n    \n    '''\n    if function.startswith('_') or function not in globals():\n        return {}\n    # Call specified function to pull redis data\n    return globals()[function](minion_id, pillar, **kwargs)", "output": "Grabs external pillar data based on configured function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_gid(path, follow_symlinks=True):\n    '''\n    \n    '''\n    return stats(os.path.expanduser(path), follow_symlinks=follow_symlinks).get('gid', -1)", "output": "Return the id of the group that owns a given file\n\n    path\n        file or directory of which to get the gid\n\n    follow_symlinks\n        indicated if symlinks should be followed\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.get_gid /etc/passwd\n\n    .. versionchanged:: 0.16.4\n        ``follow_symlinks`` option added", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reaped(subset=None, show_ip=False, show_ipv4=None):\n    '''\n    \n    '''\n    show_ip = _show_ip_migration(show_ip, show_ipv4)\n    return list_state(subset=subset, show_ip=show_ip)", "output": ".. versionadded:: 2015.8.0\n    .. versionchanged:: 2019.2.0\n        The 'show_ipv4' argument has been renamed to 'show_ip' as it now\n        includes IPv6 addresses for IPv6-connected minions.\n\n    Print a list of all minions that are up according to Salt's presence\n    detection (no commands will be sent to minions)\n\n    subset : None\n        Pass in a CIDR range to filter minions by IP address.\n\n    show_ip : False\n        Also show the IP address each minion is connecting from.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run manage.reaped", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render_svg(self, render_id, words, arcs):\n        \"\"\"\n        \"\"\"\n        self.levels = self.get_levels(arcs)\n        self.highest_level = len(self.levels)\n        self.offset_y = self.distance / 2 * self.highest_level + self.arrow_stroke\n        self.width = self.offset_x + len(words) * self.distance\n        self.height = self.offset_y + 3 * self.word_spacing\n        self.id = render_id\n        words = [self.render_word(w[\"text\"], w[\"tag\"], i) for i, w in enumerate(words)]\n        arcs = [\n            self.render_arrow(a[\"label\"], a[\"start\"], a[\"end\"], a[\"dir\"], i)\n            for i, a in enumerate(arcs)\n        ]\n        content = \"\".join(words) + \"\".join(arcs)\n        return TPL_DEP_SVG.format(\n            id=self.id,\n            width=self.width,\n            height=self.height,\n            color=self.color,\n            bg=self.bg,\n            font=self.font,\n            content=content,\n            dir=self.direction,\n            lang=self.lang,\n        )", "output": "Render SVG.\n\n        render_id (int): Unique ID, typically index of document.\n        words (list): Individual words and their tags.\n        arcs (list): Individual arcs and their start, end, direction and label.\n        RETURNS (unicode): Rendered SVG markup.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait_for_compute_global_operation(project_name, operation):\n    \"\"\"\"\"\"\n    logger.info(\"wait_for_compute_global_operation: \"\n                \"Waiting for operation {} to finish...\".format(\n                    operation[\"name\"]))\n\n    for _ in range(MAX_POLLS):\n        result = compute.globalOperations().get(\n            project=project_name,\n            operation=operation[\"name\"],\n        ).execute()\n        if \"error\" in result:\n            raise Exception(result[\"error\"])\n\n        if result[\"status\"] == \"DONE\":\n            logger.info(\"wait_for_compute_global_operation: \"\n                        \"Operation done.\")\n            break\n\n        time.sleep(POLL_INTERVAL)\n\n    return result", "output": "Poll for global compute operation until finished.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _compute_type_url(klass, prefix=_GOOGLE_APIS_PREFIX):\n    \"\"\"\n    \"\"\"\n    name = klass.DESCRIPTOR.full_name\n    return \"%s/%s\" % (prefix, name)", "output": "Compute a type URL for a klass.\n\n    :type klass: type\n    :param klass: class to be used as a factory for the given type\n\n    :type prefix: str\n    :param prefix: URL prefix for the type\n\n    :rtype: str\n    :returns: the URL, prefixed as appropriate", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sysctl():\n    '''\n    \n    '''\n    ret = {}\n    sysctl_jail = __salt__['cmd.run']('sysctl security.jail')\n    for line in sysctl_jail.splitlines():\n        key, value = line.split(':', 1)\n        ret[key.strip()] = value.strip()\n    return ret", "output": "Dump all jail related kernel states (sysctl)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' jail.sysctl", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convertToDatetime(fmt=\"%Y-%m-%dT%H:%M:%S.%f\"):\n        \"\"\"\n        \"\"\"\n        def cvt_fn(s,l,t):\n            try:\n                return datetime.strptime(t[0], fmt)\n            except ValueError as ve:\n                raise ParseException(s, l, str(ve))\n        return cvt_fn", "output": "Helper to create a parse action for converting parsed\n        datetime string to Python datetime.datetime\n\n        Params -\n         - fmt - format to be passed to datetime.strptime (default= ``\"%Y-%m-%dT%H:%M:%S.%f\"``)\n\n        Example::\n\n            dt_expr = pyparsing_common.iso8601_datetime.copy()\n            dt_expr.setParseAction(pyparsing_common.convertToDatetime())\n            print(dt_expr.parseString(\"1999-12-31T23:59:59.999\"))\n\n        prints::\n\n            [datetime.datetime(1999, 12, 31, 23, 59, 59, 999000)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_time_server():\n    '''\n    \n    '''\n    ret = salt.utils.mac_utils.execute_return_result(\n        'systemsetup -getnetworktimeserver')\n    return salt.utils.mac_utils.parse_return(ret)", "output": "Display the currently set network time server.\n\n    :return: the network time server\n    :rtype: str\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' timezone.get_time_server", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_compatible_with(\n            self,\n            other: Union[Period, Timestamp, Timedelta, NaTType],\n    ) -> None:\n        \"\"\"\n        \n        \"\"\"\n        raise AbstractMethodError(self)", "output": "Verify that `self` and `other` are compatible.\n\n        * DatetimeArray verifies that the timezones (if any) match\n        * PeriodArray verifies that the freq matches\n        * Timedelta has no verification\n\n        In each case, NaT is considered compatible.\n\n        Parameters\n        ----------\n        other\n\n        Raises\n        ------\n        Exception", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _netinfo_freebsd_netbsd():\n    '''\n    \n    '''\n    ret = {}\n    # NetBSD requires '-n' to disable port-to-service resolution\n    out = __salt__['cmd.run'](\n        'sockstat -46 {0} | tail -n+2'.format(\n            '-n' if __grains__['kernel'] == 'NetBSD' else ''\n        ), python_shell=True\n    )\n    for line in out.splitlines():\n        user, cmd, pid, _, proto, local_addr, remote_addr = line.split()\n        local_addr = '.'.join(local_addr.rsplit(':', 1))\n        remote_addr = '.'.join(remote_addr.rsplit(':', 1))\n        ret.setdefault(\n            local_addr, {}).setdefault(\n                remote_addr, {}).setdefault(\n                    proto, {}).setdefault(\n                        pid, {})['user'] = user\n        ret[local_addr][remote_addr][proto][pid]['cmd'] = cmd\n    return ret", "output": "Get process information for network connections using sockstat", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def append_metrics(self, metrics, df_name):\n        \"\"\"\n        \"\"\"\n        dataframe = self._dataframes[df_name]\n        _add_new_columns(dataframe, metrics)\n        dataframe.loc[len(dataframe)] = metrics", "output": "Append new metrics to selected dataframes.\n\n        Parameters\n        ----------\n        metrics : metric.EvalMetric\n            New metrics to be added.\n        df_name : str\n            Name of the dataframe to be modified.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pkgdb(opts):\n    '''\n    \n    '''\n    return LazyLoader(\n        _module_dirs(\n            opts,\n            'pkgdb',\n            base_path=os.path.join(SALT_BASE_PATH, 'spm')\n        ),\n        opts,\n        tag='pkgdb'\n    )", "output": "Return modules for SPM's package database\n\n    .. versionadded:: 2015.8.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _canonize_content_input(self, dataset, single_style):\n        \"\"\"\n        \n        \"\"\"\n        unpack = lambda x: x\n        if isinstance(dataset, _tc.SArray):\n            dataset = _tc.SFrame({self.content_feature: dataset})\n            if single_style:\n                unpack = lambda sf: sf['stylized_' + self.content_feature]\n        elif isinstance(dataset, _tc.Image):\n            dataset = _tc.SFrame({self.content_feature: [dataset]})\n            if single_style:\n                unpack = lambda sf: sf['stylized_' + self.content_feature][0]\n        return dataset, unpack", "output": "Takes input and returns tuple of the input in canonical form (SFrame)\n        along with an unpack callback function that can be applied to\n        prediction results to \"undo\" the canonization.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close_error_dlg(self):\r\n        \"\"\"\"\"\"\r\n        if self.error_dlg.dismiss_box.isChecked():\r\n            self.dismiss_error = True\r\n        self.error_dlg.reject()", "output": "Close error dialog.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def verify_hash_type(self):\n        '''\n        \n        '''\n        if self.config['hash_type'].lower() in ['md5', 'sha1']:\n            log.warning(\n                'IMPORTANT: Do not use %s hashing algorithm! Please set '\n                '\"hash_type\" to sha256 in Salt %s config!',\n                self.config['hash_type'], self.__class__.__name__\n            )", "output": "Verify and display a nag-messsage to the log if vulnerable hash-type is used.\n\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def PrependSOffsetTRelative(self, off):\n        \"\"\"\n        \n        \"\"\"\n\n        # Ensure alignment is already done:\n        self.Prep(N.SOffsetTFlags.bytewidth, 0)\n        if not (off <= self.Offset()):\n            msg = \"flatbuffers: Offset arithmetic error.\"\n            raise OffsetArithmeticError(msg)\n        off2 = self.Offset() - off + N.SOffsetTFlags.bytewidth\n        self.PlaceSOffsetT(off2)", "output": "PrependSOffsetTRelative prepends an SOffsetT, relative to where it\n        will be written.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_process_worker(self, cmd_list, environ=None):\n        \"\"\"\"\"\"\n        worker = ProcessWorker(cmd_list, environ=environ)\n        self._create_worker(worker)\n        return worker", "output": "Create a new process worker instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ceafe(clusters, gold_clusters):\n        \"\"\"\n        \n        \"\"\"\n        clusters = [cluster for cluster in clusters if len(cluster) != 1]\n        scores = np.zeros((len(gold_clusters), len(clusters)))\n        for i, gold_cluster in enumerate(gold_clusters):\n            for j, cluster in enumerate(clusters):\n                scores[i, j] = Scorer.phi4(gold_cluster, cluster)\n        matching = linear_assignment(-scores)\n        similarity = sum(scores[matching[:, 0], matching[:, 1]])\n        return similarity, len(clusters), similarity, len(gold_clusters)", "output": "Computes the  Constrained EntityAlignment F-Measure (CEAF) for evaluating coreference.\n        Gold and predicted mentions are aligned into clusterings which maximise a metric - in\n        this case, the F measure between gold and predicted clusters.\n\n        <https://www.semanticscholar.org/paper/On-Coreference-Resolution-Performance-Metrics-Luo/de133c1f22d0dfe12539e25dda70f28672459b99>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_anyof(self, definitions, field, value):\n        \"\"\"  \"\"\"\n        valids, _errors = \\\n            self.__validate_logical('anyof', definitions, field, value)\n        if valids < 1:\n            self._error(field, errors.ANYOF, _errors,\n                        valids, len(definitions))", "output": "{'type': 'list', 'logical': 'anyof'}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathEval(self, str):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlXPathEval(str, self._o)\n        if ret is None:raise xpathError('xmlXPathEval() failed')\n        return xpathObjectRet(ret)", "output": "Evaluate the XPath Location Path in the given context.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_return_text(self, data, function, **kwargs):  # pylint: disable=unused-argument\n        '''\n        \n        '''\n        # emulate the yaml_out output formatter. It relies on a global __opts__ object which\n        # we can't obviously pass in\n        try:\n            try:\n                outputter = data[next(iter(data))].get('out')\n            except (StopIteration, AttributeError):\n                outputter = None\n            return salt.output.string_format(\n                {x: y['return'] for x, y in six.iteritems(data)},\n                out=outputter,\n                opts=__opts__,\n            )\n        except Exception as exc:\n            import pprint\n            log.exception(\n                'Exception encountered when trying to serialize %s',\n                pprint.pformat(data)\n            )\n            return 'Got an error trying to serialze/clean up the response'", "output": "Print out YAML using the block mode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uncombine_initial_dims(tensor: torch.Tensor, original_size: torch.Size) -> torch.Tensor:\n    \"\"\"\n    \n    \"\"\"\n    if len(original_size) <= 2:\n        return tensor\n    else:\n        view_args = list(original_size) + [tensor.size(-1)]\n        return tensor.view(*view_args)", "output": "Given a tensor of embeddings with shape\n    (d1 * ... * dn, sequence_length, embedding_dim)\n    and the original shape\n    (d1, ..., dn, sequence_length),\n    return the reshaped tensor of embeddings with shape\n    (d1, ..., dn, sequence_length, embedding_dim).\n    If original size is 1-d or 2-d, return it as is.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_value_list(original_strings, corenlp_values=None):\n    \"\"\"\n    \"\"\"\n    assert isinstance(original_strings, (list, tuple, set))\n    if corenlp_values is not None:\n        assert isinstance(corenlp_values, (list, tuple, set))\n        assert len(original_strings) == len(corenlp_values)\n        return list(set(to_value(x, y) for (x, y)\n                        in zip(original_strings, corenlp_values)))\n    else:\n        return list(set(to_value(x) for x in original_strings))", "output": "Convert a list of strings to a list of Values\n\n    Args:\n        original_strings (list[basestring])\n        corenlp_values (list[basestring or None])\n    Returns:\n        list[Value]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_uid(path, follow_symlinks=True):\n    '''\n    \n    '''\n    return stats(os.path.expanduser(path), follow_symlinks=follow_symlinks).get('uid', -1)", "output": "Return the id of the user that owns a given file\n\n    path\n        file or directory of which to get the uid\n\n    follow_symlinks\n        indicated if symlinks should be followed\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.get_uid /etc/passwd\n\n    .. versionchanged:: 0.16.4\n        ``follow_symlinks`` option added", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_dict(self, get_value=None):\n        \"\"\"\"\"\"\n        self.trim()\n        result = {}\n        for key, value in iteritems(self.counters):\n            if get_value is not None:\n                value = getattr(value, get_value)\n            r = result\n            for _key in key[:-1]:\n                r = r.setdefault(_key, {})\n            r[key[-1]] = value\n        return result", "output": "Dump counters as a dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_params(self, deep=False):\n        \"\"\"\"\"\"\n        params = super(XGBModel, self).get_params(deep=deep)\n        if isinstance(self.kwargs, dict):  # if kwargs is a dict, update params accordingly\n            params.update(self.kwargs)\n        if params['missing'] is np.nan:\n            params['missing'] = None  # sklearn doesn't handle nan. see #4725\n        if not params.get('eval_metric', True):\n            del params['eval_metric']  # don't give as None param to Booster\n        return params", "output": "Get parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_secret(\n        name,\n        namespace='default',\n        data=None,\n        source=None,\n        template=None,\n        saltenv='base',\n        **kwargs):\n    '''\n    \n    '''\n    if source:\n        data = __read_and_render_yaml_file(source, template, saltenv)\n    elif data is None:\n        data = {}\n\n    data = __enforce_only_strings_dict(data)\n\n    # encode the secrets using base64 as required by kubernetes\n    for key in data:\n        data[key] = base64.b64encode(data[key])\n\n    body = kubernetes.client.V1Secret(\n        metadata=__dict_to_object_meta(name, namespace, {}),\n        data=data)\n\n    cfg = _setup_conn(**kwargs)\n\n    try:\n        api_instance = kubernetes.client.CoreV1Api()\n        api_response = api_instance.create_namespaced_secret(\n            namespace, body)\n\n        return api_response.to_dict()\n    except (ApiException, HTTPError) as exc:\n        if isinstance(exc, ApiException) and exc.status == 404:\n            return None\n        else:\n            log.exception(\n                'Exception when calling '\n                'CoreV1Api->create_namespaced_secret'\n            )\n            raise CommandExecutionError(exc)\n    finally:\n        _cleanup(**cfg)", "output": "Creates the kubernetes secret as defined by the user.\n\n    CLI Examples::\n\n        salt 'minion1' kubernetes.create_secret \\\n            passwords default '{\"db\": \"letmein\"}'\n\n        salt 'minion2' kubernetes.create_secret \\\n            name=passwords namespace=default data='{\"db\": \"letmein\"}'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_inference_graph(model_path):\n    \"\"\" \"\"\"\n    pred_config = PredictConfig(\n        session_init=get_model_loader(model_path),\n        model=InferenceOnlyModel(),\n        input_names=['input_img_bytes'],\n        output_names=['prediction_img_bytes'])\n\n    pred = OfflinePredictor(pred_config)\n    buf = open('lena.png', 'rb').read()\n    prediction = pred([buf])[0]\n    with open('applied_inference_graph.png', 'wb') as f:\n        f.write(prediction[0])", "output": "Run inference from a different graph, which receives encoded images buffers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def predict(parameters_value, regressor_gp):\n    '''\n    \n    '''\n    parameters_value = numpy.array(parameters_value).reshape(-1, len(parameters_value))\n    mu, sigma = regressor_gp.predict(parameters_value, return_std=True)\n\n    return mu[0], sigma[0]", "output": "Predict by Gaussian Process Model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_(key, value, profile=None):\n    '''\n    \n    '''\n    if not profile:\n        return False\n    conn, cur, table = _connect(profile)\n    if six.PY2:\n        value = buffer(salt.utils.msgpack.packb(value))\n    else:\n        value = memoryview(salt.utils.msgpack.packb(value))\n    q = profile.get('set_query', ('INSERT OR REPLACE INTO {0} VALUES '\n                                  '(:key, :value)').format(table))\n    conn.execute(q, {'key': key, 'value': value})\n    conn.commit()\n    return True", "output": "Set a key/value pair in sqlite3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate(self, v):\n        \"\"\"  \"\"\"\n        val = v.tostring(self.encoding)\n        return \"({lhs} {op} {val})\".format(lhs=self.lhs, op=self.op, val=val)", "output": "create and return the op string for this TermValue", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_capability_definitions(profile_manager):\n    '''\n    \n    '''\n    res_type = pbm.profile.ResourceType(\n        resourceType=pbm.profile.ResourceTypeEnum.STORAGE)\n    try:\n        cap_categories = profile_manager.FetchCapabilityMetadata(res_type)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise VMwareApiError('Not enough permissions. Required privilege: '\n                             '{0}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise VMwareRuntimeError(exc.msg)\n    cap_definitions = []\n    for cat in cap_categories:\n        cap_definitions.extend(cat.capabilityMetadata)\n    return cap_definitions", "output": "Returns a list of all capability definitions.\n\n    profile_manager\n        Reference to the profile manager.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dump_profiles(self, path):\n        \"\"\" \n        \"\"\"\n        if self.profiler_collector is not None:\n            self.profiler_collector.dump_profiles(path)\n        else:\n            raise RuntimeError(\"'spark.python.profile' configuration must be set \"\n                               \"to 'true' to enable Python profile.\")", "output": "Dump the profile stats into directory `path`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attr(self, key):\n        \"\"\"\n        \"\"\"\n        ret = ctypes.c_char_p()\n        success = ctypes.c_int()\n        _check_call(_LIB.XGBoosterGetAttr(\n            self.handle, c_str(key), ctypes.byref(ret), ctypes.byref(success)))\n        if success.value != 0:\n            return py_str(ret.value)\n        return None", "output": "Get attribute string from the Booster.\n\n        Parameters\n        ----------\n        key : str\n            The key to get attribute from.\n\n        Returns\n        -------\n        value : str\n            The attribute value of the key, returns None if attribute do not exist.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unregister_file(path, pkg=None, conn=None):  # pylint: disable=W0612\n    '''\n    \n    '''\n    close = False\n    if conn is None:\n        close = True\n        conn = init()\n\n    conn.execute('DELETE FROM files WHERE path=?', (path, ))\n    if close:\n        conn.close()", "output": "Unregister a file from the package database", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_executable(script_path):\n    \"\"\"\n    \"\"\"\n    status = os.stat(script_path)\n    os.chmod(script_path, status.st_mode | stat.S_IEXEC)", "output": "Make `script_path` executable.\n\n    :param script_path: The file to change", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dump(self):\n        \"\"\"\"\"\"\n        assert self.database is not None\n\n        cmd = \"SELECT count from {} WHERE rowid={}\"\n        self._execute(cmd.format(self.STATE_INFO_TABLE, self.STATE_INFO_ROW))\n        ret = self._fetchall()\n        assert len(ret) == 1\n        assert len(ret[0]) == 1\n        count = self._from_sqlite(ret[0][0]) + self.inserts\n\n        if count > self.row_limit:\n            msg = \"cleaning up state, this might take a while.\"\n            logger.warning(msg)\n\n            delete = count - self.row_limit\n            delete += int(self.row_limit * (self.row_cleanup_quota / 100.0))\n            cmd = (\n                \"DELETE FROM {} WHERE timestamp IN (\"\n                \"SELECT timestamp FROM {} ORDER BY timestamp ASC LIMIT {});\"\n            )\n            self._execute(\n                cmd.format(self.STATE_TABLE, self.STATE_TABLE, delete)\n            )\n\n            self._vacuum()\n\n            cmd = \"SELECT COUNT(*) FROM {}\"\n\n            self._execute(cmd.format(self.STATE_TABLE))\n            ret = self._fetchall()\n            assert len(ret) == 1\n            assert len(ret[0]) == 1\n            count = ret[0][0]\n\n        cmd = \"UPDATE {} SET count = {} WHERE rowid = {}\"\n        self._execute(\n            cmd.format(\n                self.STATE_INFO_TABLE,\n                self._to_sqlite(count),\n                self.STATE_INFO_ROW,\n            )\n        )\n\n        self._update_cache_directory_state()\n\n        self.database.commit()\n        self.cursor.close()\n        self.database.close()\n        self.database = None\n        self.cursor = None\n        self.inserts = 0", "output": "Saves state database.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_disk_from_distro(vm_, linode_id, swap_size=None):\n    \n\n    '''\n    kwargs = {}\n\n    if swap_size is None:\n        swap_size = get_swap_size(vm_)\n\n    pub_key = get_pub_key(vm_)\n    root_password = get_password(vm_)\n\n    if pub_key:\n        kwargs.update({'rootSSHKey': pub_key})\n    if root_password:\n        kwargs.update({'rootPass': root_password})\n    else:\n        raise SaltCloudConfigError(\n            'The Linode driver requires a password.'\n        )\n\n    kwargs.update({'LinodeID': linode_id,\n                   'DistributionID': get_distribution_id(vm_),\n                   'Label': vm_['name'],\n                   'Size': get_disk_size(vm_, swap_size, linode_id)})\n\n    result = _query('linode', 'disk.createfromdistribution', args=kwargs)\n\n    return _clean_data(result)", "output": "r'''\n    Creates the disk for the Linode from the distribution.\n\n    vm\\_\n        The VM profile to create the disk for.\n\n    linode_id\n        The ID of the Linode to create the distribution disk for. Required.\n\n    swap_size\n        The size of the disk, in MB.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def print_docs(self):\n        '''\n        \n        '''\n        arg = self.opts.get('fun', None)\n        docs = super(Runner, self).get_docs(arg)\n        for fun in sorted(docs):\n            display_output('{0}:'.format(fun), 'text', self.opts)\n            print(docs[fun])", "output": "Print out the documentation!", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_input_matrix(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.f.isQuant():\n            raise ValueError(\"Can't get quantized Matrix\")\n        return np.array(self.f.getInputMatrix())", "output": "Get a copy of the full input matrix of a Model. This only\n        works if the model is not quantized.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def description_of(lines, name='stdin'):\n    \"\"\"\n    \n    \"\"\"\n    u = UniversalDetector()\n    for line in lines:\n        line = bytearray(line)\n        u.feed(line)\n        # shortcut out of the loop to save reading further - particularly useful if we read a BOM.\n        if u.done:\n            break\n    u.close()\n    result = u.result\n    if PY2:\n        name = name.decode(sys.getfilesystemencoding(), 'ignore')\n    if result['encoding']:\n        return '{0}: {1} with confidence {2}'.format(name, result['encoding'],\n                                                     result['confidence'])\n    else:\n        return '{0}: no result'.format(name)", "output": "Return a string describing the probable encoding of a file or\n    list of strings.\n\n    :param lines: The lines to get the encoding of.\n    :type lines: Iterable of bytes\n    :param name: Name of file or collection of lines\n    :type name: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def move_to_device(obj, cuda_device: int):\n    \"\"\"\n    \n    \"\"\"\n    if cuda_device < 0 or not has_tensor(obj):\n        return obj\n    elif isinstance(obj, torch.Tensor):\n        return obj.cuda(cuda_device)\n    elif isinstance(obj, dict):\n        return {key: move_to_device(value, cuda_device) for key, value in obj.items()}\n    elif isinstance(obj, list):\n        return [move_to_device(item, cuda_device) for item in obj]\n    elif isinstance(obj, tuple):\n        return tuple([move_to_device(item, cuda_device) for item in obj])\n    else:\n        return obj", "output": "Given a structure (possibly) containing Tensors on the CPU,\n    move all the Tensors to the specified GPU (or do nothing, if they should be on the CPU).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def guided_relu():\n    \"\"\"\n    \n    \"\"\"\n    from tensorflow.python.ops import gen_nn_ops   # noqa\n\n    @tf.RegisterGradient(\"GuidedReLU\")\n    def GuidedReluGrad(op, grad):\n        return tf.where(0. < grad,\n                        gen_nn_ops._relu_grad(grad, op.outputs[0]),\n                        tf.zeros(grad.get_shape()))\n\n    g = tf.get_default_graph()\n    with g.gradient_override_map({'Relu': 'GuidedReLU'}):\n        yield", "output": "Returns:\n        A context where the gradient of :meth:`tf.nn.relu` is replaced by\n        guided back-propagation, as described in the paper:\n        `Striving for Simplicity: The All Convolutional Net\n        <https://arxiv.org/abs/1412.6806>`_", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ls(bank):\n    '''\n    \n    '''\n    _init_client()\n    query = \"SELECT etcd_key FROM {0} WHERE bank='{1}'\".format(\n        _table_name, bank)\n    cur, _ = run_query(client, query)\n    out = [row[0] for row in cur.fetchall()]\n    cur.close()\n    return out", "output": "Return an iterable object containing all entries stored in the specified\n    bank.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _rows_differ(row, _row):\n    '''\n    \n    '''\n    row_copy = copy.deepcopy(row)\n    _row_copy = copy.deepcopy(_row)\n    # Strip id from all panels in both rows, since they are always generated.\n    for panel in row_copy['panels']:\n        if 'id' in panel:\n            del panel['id']\n    for _panel in _row_copy['panels']:\n        if 'id' in _panel:\n            del _panel['id']\n    diff = DictDiffer(row_copy, _row_copy)\n    return diff.changed() or diff.added() or diff.removed()", "output": "Check if grafana dashboard row and _row differ", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_variables(self, page_size=None, page_token=None, client=None):\n        \"\"\"\n        \"\"\"\n        path = \"%s/variables\" % (self.path,)\n        client = self._require_client(client)\n        iterator = page_iterator.HTTPIterator(\n            client=client,\n            api_request=client._connection.api_request,\n            path=path,\n            item_to_value=_item_to_variable,\n            items_key=\"variables\",\n            page_token=page_token,\n            max_results=page_size,\n        )\n        iterator._MAX_RESULTS = \"pageSize\"\n        iterator.config = self\n        return iterator", "output": "API call:  list variables for this config.\n\n        This only lists variable names, not the values.\n\n        See\n        https://cloud.google.com/deployment-manager/runtime-configurator/reference/rest/v1beta1/projects.configs.variables/list\n\n        :type page_size: int\n        :param page_size:\n            Optional. The maximum number of variables in each page of results\n            from this request. Non-positive values are ignored. Defaults\n            to a sensible value set by the API.\n\n        :type page_token: str\n        :param page_token:\n            Optional. If present, return the next batch of variables, using\n            the value, which must correspond to the ``nextPageToken`` value\n            returned in the previous response.  Deprecated: use the ``pages``\n            property of the returned iterator instead of manually passing\n            the token.\n\n        :type client: :class:`~google.cloud.runtimeconfig.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the current config.\n\n        :rtype: :class:`~google.api_core.page_iterator.Iterator`\n        :returns:\n            Iterator of :class:`~google.cloud.runtimeconfig.variable.Variable`\n            belonging to this project.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_argsort_with_ascending(ascending, args, kwargs):\n    \"\"\"\n    \n    \"\"\"\n\n    if is_integer(ascending) or ascending is None:\n        args = (ascending,) + args\n        ascending = True\n\n    validate_argsort_kind(args, kwargs, max_fname_arg_count=3)\n    return ascending", "output": "If 'Categorical.argsort' is called via the 'numpy' library, the\n    first parameter in its signature is 'axis', which takes either\n    an integer or 'None', so check if the 'ascending' parameter has\n    either integer type or is None, since 'ascending' itself should\n    be a boolean", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reshape_like(a, b):\n  \"\"\"\"\"\"\n  ret = tf.reshape(a, tf.concat([tf.shape(b)[:-1], tf.shape(a)[-1:]], 0))\n  if not tf.executing_eagerly():\n    ret.set_shape(b.get_shape().as_list()[:-1] + a.get_shape().as_list()[-1:])\n  return ret", "output": "Reshapes a to match the shape of b in all but the last dimension.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def abbreviate(s):\n    \"\"\"\n    \"\"\"\n    if not s:\n        return ''\n    # check the cache\n    if s in abbreviate.abbreviations:\n        return abbreviate.abbreviations[s]\n    # anything less than 4 characters doesn't need\n    # an abbreviation\n    if len(s) < 4:\n        # update cache\n        abbreviate.abbreviations[s] = s\n        return s\n    # save the first character in case it's a vowel\n    s1 = s[0]\n    s2 = s[1:]\n    if s.endswith('ing'):\n        # strip off the 'ing'\n        s2 = s2[:-3]\n    # reduce all doubled characters to one\n    s2 = ''.join(c for c, _ in groupby(s2))\n    # remove all vowels\n    s2 = s2.translate(None, \"AEIOUaeiou\")\n    # shorten remaining consonants to 4 characters\n    # and add the first char back to the front\n    s2 = s1 + s2[:4]\n    # update cache\n    abbreviate.abbreviations[s] = s2\n    return s2", "output": "Apply a set of standard transformations to string to produce an\n    abbreviation no more than 4 characters long.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_icon(name, default=None, resample=False):\n    \"\"\"\n    \"\"\"\n\n    icon_path = get_image_path(name, default=None)\n    if icon_path is not None:\n        icon = QIcon(icon_path)\n    elif isinstance(default, QIcon):\n        icon = default\n    elif default is None:\n        try:\n            icon = get_std_icon(name[:-4])\n        except AttributeError:\n            icon = QIcon(get_image_path(name, default))\n    else:\n        icon = QIcon(get_image_path(name, default))\n    if resample:\n        icon0 = QIcon()\n        for size in (16, 24, 32, 48, 96, 128, 256, 512):\n            icon0.addPixmap(icon.pixmap(size, size))\n        return icon0\n    else:\n        return icon", "output": "Return image inside a QIcon object.\n\n    default: default image name or icon\n    resample: if True, manually resample icon pixmaps for usual sizes\n    (16, 24, 32, 48, 96, 128, 256). This is recommended for QMainWindow icons\n    created from SVG images on non-Windows platforms due to a Qt bug (see\n    Issue 1314).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_wdhistory(self):\r\n        \"\"\"\"\"\"\r\n        text = [ to_text_string( self.pathedit.itemText(index) ) \\\r\n                 for index in range(self.pathedit.count()) ]\r\n        try:\r\n            encoding.writelines(text, self.LOG_PATH)\r\n        except EnvironmentError:\r\n            pass", "output": "Save history to a text file in user home directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_get_tdxtraderecord(file):\n    \"\"\"\n    \n    \"\"\"\n    try:\n        with open('./20180606.csv', 'r') as f:\n            l = csv.reader(f)\n            data = [item for item in l]\n\n        res = pd.DataFrame(data[1:], columns=data[0])\n        return res\n    except:\n        raise IOError('QA CANNOT READ THIS RECORD')", "output": "QUANTAXIS \u8bfb\u53d6\u5386\u53f2\u4ea4\u6613\u8bb0\u5f55 \u901a\u8fbe\u4fe1 \u5386\u53f2\u6210\u4ea4-\u8f93\u51fa-xlsfile--\u8f6c\u6362csvfile", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_key_event(self, event_data):\n        '''\n        \n        '''\n\n        tag = event_data['tag']\n        event_info = event_data['data']\n\n        if event_info['act'] == 'delete':\n            self.minions.pop(event_info['id'], None)\n        elif event_info['act'] == 'accept':\n            self.minions.setdefault(event_info['id'], {})\n\n        self.publish_minions()", "output": "Tag: salt/key\n        Data:\n        {'_stamp': '2014-05-20T22:45:04.345583',\n         'act': 'delete',\n         'id': 'compute.home',\n         'result': True}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_ruby(ret, ruby, user=None):\n    '''\n    \n    '''\n    match_version = True\n    match_micro_version = False\n    micro_version_regex = re.compile(r'-([0-9]{4}\\.[0-9]{2}|p[0-9]+)$')\n    if micro_version_regex.search(ruby):\n        match_micro_version = True\n    if re.search('^[a-z]+$', ruby):\n        match_version = False\n    ruby = re.sub('^ruby-', '', ruby)\n\n    for impl, version, default in __salt__['rvm.list'](runas=user):\n        if impl != 'ruby':\n            version = '{impl}-{version}'.format(impl=impl, version=version)\n        if not match_micro_version:\n            version = micro_version_regex.sub('', version)\n        if not match_version:\n            version = re.sub('-.*', '', version)\n        if version == ruby:\n            ret['result'] = True\n            ret['comment'] = 'Requested ruby exists.'\n            ret['default'] = default\n            break\n    return ret", "output": "Check that ruby is installed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_resource(resource, name=None, resource_id=None, region=None,\n                  key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n\n    if not _exactly_one((name, resource_id)):\n        raise SaltInvocationError('One (but not both) of name or id must be '\n                                  'provided.')\n\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    f = 'get_all_{0}'.format(resource)\n    if not f.endswith('s'):\n        f = f + 's'\n    get_resources = getattr(conn, f)\n    filter_parameters = {}\n\n    if name:\n        filter_parameters['filters'] = {'tag:Name': name}\n    if resource_id:\n        filter_parameters['{0}_ids'.format(resource)] = resource_id\n\n    try:\n        r = get_resources(**filter_parameters)\n    except BotoServerError as e:\n        if e.code.endswith('.NotFound'):\n            return None\n        raise\n\n    if r:\n        if len(r) == 1:\n            if name:\n                _cache_id(name, sub_resource=resource,\n                          resource_id=r[0].id,\n                          region=region,\n                          key=key, keyid=keyid,\n                          profile=profile)\n            return r[0]\n        else:\n            raise CommandExecutionError('Found more than one '\n                                        '{0} named \"{1}\"'.format(\n                                            resource, name))\n    else:\n        return None", "output": "Get a VPC resource based on resource type and name or id.\n    Cache the id if name was provided.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_requirements(file_):\n    \"\"\"\n    \"\"\"\n    modules = []\n    delim = [\"<\", \">\", \"=\", \"!\", \"~\"]  # https://www.python.org/dev/peps/pep-0508/#complete-grammar\n\n    try:\n        f = open_func(file_, \"r\")\n    except OSError:\n        logging.error(\"Failed on file: {}\".format(file_))\n        raise\n    else:\n        data = [x.strip() for x in f.readlines() if x != \"\\n\"]\n    finally:\n        f.close()\n\n    data = [x for x in data if x[0].isalpha()]\n\n    for x in data:\n        if not any([y in x for y in delim]):  # Check for modules w/o a specifier.\n            modules.append({\"name\": x, \"version\": None})\n        for y in x:\n            if y in delim:\n                module = x.split(y)\n                module_name = module[0]\n                module_version = module[-1].replace(\"=\", \"\")\n                module = {\"name\": module_name, \"version\": module_version}\n\n                if module not in modules:\n                    modules.append(module)\n\n                break\n\n    return modules", "output": "Parse a requirements formatted file.\n\n    Traverse a string until a delimiter is detected, then split at said\n    delimiter, get module name by element index, create a dict consisting of\n    module:version, and add dict to list of parsed modules.\n\n    Args:\n        file_: File to parse.\n\n    Raises:\n        OSerror: If there's any issues accessing the file.\n\n    Returns:\n        tuple: The contents of the file, excluding comments.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uts46_remap(domain, std3_rules=True, transitional=False):\n    \"\"\"\"\"\"\n    from .uts46data import uts46data\n    output = u\"\"\n    try:\n        for pos, char in enumerate(domain):\n            code_point = ord(char)\n            uts46row = uts46data[code_point if code_point < 256 else\n                bisect.bisect_left(uts46data, (code_point, \"Z\")) - 1]\n            status = uts46row[1]\n            replacement = uts46row[2] if len(uts46row) == 3 else None\n            if (status == \"V\" or\n                    (status == \"D\" and not transitional) or\n                    (status == \"3\" and not std3_rules and replacement is None)):\n                output += char\n            elif replacement is not None and (status == \"M\" or\n                    (status == \"3\" and not std3_rules) or\n                    (status == \"D\" and transitional)):\n                output += replacement\n            elif status != \"I\":\n                raise IndexError()\n        return unicodedata.normalize(\"NFC\", output)\n    except IndexError:\n        raise InvalidCodepoint(\n            \"Codepoint {0} not allowed at position {1} in {2}\".format(\n            _unot(code_point), pos + 1, repr(domain)))", "output": "Re-map the characters in the string according to UTS46 processing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def netmiko_call(method, *args, **kwargs):\n    '''\n    \n    '''\n    netmiko_kwargs = netmiko_args()\n    kwargs.update(netmiko_kwargs)\n    return __salt__['netmiko.call'](method, *args, **kwargs)", "output": ".. versionadded:: 2019.2.0\n\n    Execute an arbitrary Netmiko method, passing the authentication details from\n    the existing NAPALM connection.\n\n    method\n        The name of the Netmiko method to execute.\n\n    args\n        List of arguments to send to the Netmiko method specified in ``method``.\n\n    kwargs\n        Key-value arguments to send to the execution function specified in\n        ``method``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm.netmiko_call send_command 'show version'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_diet_var_getter(params):\n  \"\"\"\"\"\"\n\n  def diet_var_initializer(shape, dtype, partition_info=None):\n    \"\"\"Initializer for a diet variable.\"\"\"\n    del dtype\n    del partition_info\n\n    with common_layers.fn_device_dependency(\"diet_init\") as out_deps:\n      float_range = math.sqrt(3)\n      ret = tf.random_uniform(shape, -float_range, float_range)\n      if params.quantize:\n        ret = _quantize(ret, params, randomize=False)\n      out_deps.append(ret)\n      return ret\n\n  def diet_var_getter(getter, **kwargs):\n    \"\"\"Get diet variable and return it dequantized.\"\"\"\n    if params.quantize:\n      kwargs[\"dtype\"] = tf.float16\n    kwargs[\"initializer\"] = diet_var_initializer\n    kwargs[\"trainable\"] = False\n\n    base_var = getter(**kwargs)\n\n    dequantized = _dequantize(base_var, params)\n\n    if not hasattr(params, \"dequantized\"):\n      params.dequantized = defaultdict(list)\n    params.dequantized[base_var.name].append(dequantized)\n\n    return dequantized\n\n  return diet_var_getter", "output": "Create a custom variable getter for diet variables according to params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def event(self, coro):\n        \"\"\"\n        \"\"\"\n\n        if not asyncio.iscoroutinefunction(coro):\n            raise TypeError('event registered must be a coroutine function')\n\n        setattr(self, coro.__name__, coro)\n        log.debug('%s has successfully been registered as an event', coro.__name__)\n        return coro", "output": "A decorator that registers an event to listen to.\n\n        You can find more info about the events on the :ref:`documentation below <discord-api-events>`.\n\n        The events must be a |corourl|_, if not, :exc:`TypeError` is raised.\n\n        Example\n        ---------\n\n        .. code-block:: python3\n\n            @client.event\n            async def on_ready():\n                print('Ready!')\n\n        Raises\n        --------\n        TypeError\n            The coroutine passed is not actually a coroutine.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(self):\n        \"\"\"\n        \"\"\"\n        api = self._client.instance_admin_api\n        instance_pb = admin_v1_pb2.Instance(\n            name=self.name,\n            config=self.configuration_name,\n            display_name=self.display_name,\n            node_count=self.node_count,\n        )\n        metadata = _metadata_with_prefix(self.name)\n\n        future = api.create_instance(\n            parent=self._client.project_name,\n            instance_id=self.instance_id,\n            instance=instance_pb,\n            metadata=metadata,\n        )\n\n        return future", "output": "Create this instance.\n\n        See\n        https://cloud.google.com/spanner/reference/rpc/google.spanner.admin.instance.v1#google.spanner.admin.instance.v1.InstanceAdmin.CreateInstance\n\n        .. note::\n\n           Uses the ``project`` and ``instance_id`` on the current\n           :class:`Instance` in addition to the ``display_name``.\n           To change them before creating, reset the values via\n\n           .. code:: python\n\n              instance.display_name = 'New display name'\n              instance.instance_id = 'i-changed-my-mind'\n\n           before calling :meth:`create`.\n\n        :rtype: :class:`google.api_core.operation.Operation`\n        :returns: an operation instance\n        :raises Conflict: if the instance already exists", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_loginclass(name):\n    '''\n    \n    '''\n    if __grains__['kernel'] != 'OpenBSD':\n        return False\n    userinfo = __salt__['cmd.run_stdout'](\n        ['userinfo', name],\n        python_shell=False)\n    for line in userinfo.splitlines():\n        if line.startswith('class'):\n            try:\n                ret = line.split(None, 1)[1]\n                break\n            except (ValueError, IndexError):\n                continue\n    else:\n        ret = ''\n    return ret", "output": "Get the login class of the user\n\n    name\n        User to get the information\n\n    .. note::\n        This function only applies to OpenBSD systems.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.get_loginclass foo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_fqdn(hostname):\n    \"\"\"\n    \n    \"\"\"\n\n    compliant = re.compile(r\"(?!-)[A-Z\\d\\-\\_]{1,63}(?<!-)$\", re.IGNORECASE)\n    return \".\" in hostname and len(hostname) < 0xff and all(compliant.match(x) for x in hostname.rstrip(\".\").split(\".\"))", "output": "Verify if hostname conforms to be a FQDN.\n\n    :param hostname: text string with the name of the host\n    :return: bool, True if hostname is correct FQDN, False otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert(values):\n    \"\"\"  \"\"\"\n\n    dtype = values.dtype\n\n    if is_categorical_dtype(values):\n        return values\n\n    elif is_object_dtype(dtype):\n        return values.ravel().tolist()\n\n    if needs_i8_conversion(dtype):\n        values = values.view('i8')\n    v = values.ravel()\n\n    if compressor == 'zlib':\n        _check_zlib()\n\n        # return string arrays like they are\n        if dtype == np.object_:\n            return v.tolist()\n\n        # convert to a bytes array\n        v = v.tostring()\n        return ExtType(0, zlib.compress(v))\n\n    elif compressor == 'blosc':\n        _check_blosc()\n\n        # return string arrays like they are\n        if dtype == np.object_:\n            return v.tolist()\n\n        # convert to a bytes array\n        v = v.tostring()\n        return ExtType(0, blosc.compress(v, typesize=dtype.itemsize))\n\n    # ndarray (on original dtype)\n    return ExtType(0, v.tostring())", "output": "convert the numpy values to a list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _explicit_close(napalm_device):\n    '''\n    \n    '''\n    if salt.utils.napalm.not_always_alive(__opts__):\n        # force closing the configuration session\n        # when running in a non-always-alive proxy\n        # or regular minion\n        try:\n            napalm_device['DRIVER'].close()\n        except Exception as err:\n            log.error('Unable to close the temp connection with the device:')\n            log.error(err)\n            log.error('Please report.')", "output": "Will explicily close the config session with the network device,\n    when running in a now-always-alive proxy minion or regular minion.\n    This helper must be used in configuration-related functions,\n    as the session is preserved and not closed before making any changes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_stream(self, max_buffer_size, af, addr, **kwargs):  # pylint: disable=unused-argument\n        '''\n        \n        '''\n        # Always connect in plaintext; we'll convert to ssl if necessary\n        # after one connection has completed.\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        _set_tcp_keepalive(sock, self.opts)\n        stream = tornado.iostream.IOStream(\n            sock,\n            max_buffer_size=max_buffer_size)\n        if tornado.version_info < (5,):\n            return stream.connect(addr)\n        return stream, stream.connect(addr)", "output": "Override _create_stream() in TCPClient.\n\n        Tornado 4.5 added the kwargs 'source_ip' and 'source_port'.\n        Due to this, use **kwargs to swallow these and any future\n        kwargs to maintain compatibility.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def override(cls):\n    \"\"\"\n    \"\"\"\n\n    def check_override(method):\n        if method.__name__ not in dir(cls):\n            raise NameError(\"{} does not override any method of {}\".format(\n                method, cls))\n        return method\n\n    return check_override", "output": "Annotation for documenting method overrides.\n\n    Arguments:\n        cls (type): The superclass that provides the overriden method. If this\n            cls does not actually have the method, an error is raised.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_aligned_activations(layer):\n    \"\"\"\"\"\"\n    activation_paths = [\n        PATH_TEMPLATE.format(\n            sanitize(layer.model_class.name), sanitize(layer.name), page\n        )\n        for page in range(NUMBER_OF_PAGES)\n    ]\n    activations = np.vstack([load(path) for path in activation_paths])\n    assert np.all(np.isfinite(activations))\n    return activations", "output": "Downloads 100k activations of the specified layer sampled from iterating over\n    ImageNet. Activations of all layers where sampled at the same spatial positions for\n    each image, allowing the calculation of correlations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _chunk_actions(actions, chunk_size, max_chunk_bytes, serializer):\n    \"\"\"\n    \n    \"\"\"\n    bulk_actions, bulk_data = [], []\n    size, action_count = 0, 0\n    for action, data in actions:\n        raw_data, raw_action = data, action\n        action = serializer.dumps(action)\n        # +1 to account for the trailing new line character\n        cur_size = len(action.encode(\"utf-8\")) + 1\n\n        if data is not None:\n            data = serializer.dumps(data)\n            cur_size += len(data.encode(\"utf-8\")) + 1\n\n        # full chunk, send it and start a new one\n        if bulk_actions and (\n            size + cur_size > max_chunk_bytes or action_count == chunk_size\n        ):\n            yield bulk_data, bulk_actions\n            bulk_actions, bulk_data = [], []\n            size, action_count = 0, 0\n\n        bulk_actions.append(action)\n        if data is not None:\n            bulk_actions.append(data)\n            bulk_data.append((raw_action, raw_data))\n        else:\n            bulk_data.append((raw_action,))\n\n        size += cur_size\n        action_count += 1\n\n    if bulk_actions:\n        yield bulk_data, bulk_actions", "output": "Split actions into chunks by number or size, serialize them into strings in\n    the process.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _split_symbol_mappings(df, exchanges):\n    \"\"\"\n    \"\"\"\n    mappings = df[list(mapping_columns)]\n    with pd.option_context('mode.chained_assignment', None):\n        mappings['sid'] = mappings.index\n    mappings.reset_index(drop=True, inplace=True)\n\n    # take the most recent sid->exchange mapping based on end date\n    asset_exchange = df[\n        ['exchange', 'end_date']\n    ].sort_values('end_date').groupby(level=0)['exchange'].nth(-1)\n\n    _check_symbol_mappings(mappings, exchanges, asset_exchange)\n    return (\n        df.groupby(level=0).apply(_check_asset_group),\n        mappings,\n    )", "output": "Split out the symbol: sid mappings from the raw data.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The dataframe with multiple rows for each symbol: sid pair.\n    exchanges : pd.DataFrame\n        The exchanges table.\n\n    Returns\n    -------\n    asset_info : pd.DataFrame\n        The asset info with one row per asset.\n    symbol_mappings : pd.DataFrame\n        The dataframe of just symbol: sid mappings. The index will be\n        the sid, then there will be three columns: symbol, start_date, and\n        end_date.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def visibility_changed(self, enable):\r\n        \"\"\"\"\"\"\r\n        SpyderPluginWidget.visibility_changed(self, enable)\r\n        if self.dockwidget is None:\r\n            return\r\n        if self.dockwidget.isWindow():\r\n            self.dock_toolbar.show()\r\n        else:\r\n            self.dock_toolbar.hide()\r\n        if enable:\r\n            self.refresh_plugin()\r\n        self.sig_update_plugin_title.emit()", "output": "DockWidget visibility has changed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def scalars_impl(self, run, tag_regex_string):\n    \"\"\"\n    \"\"\"\n    if not tag_regex_string:\n      # The user provided no regex.\n      return {\n          _REGEX_VALID_PROPERTY: False,\n          _TAG_TO_EVENTS_PROPERTY: {},\n      }\n\n    # Construct the regex.\n    try:\n      regex = re.compile(tag_regex_string)\n    except re.error:\n      return {\n          _REGEX_VALID_PROPERTY: False,\n          _TAG_TO_EVENTS_PROPERTY: {},\n      }\n\n    # Fetch the tags for the run. Filter for tags that match the regex.\n    run_to_data = self._multiplexer.PluginRunToTagToContent(\n        scalars_metadata.PLUGIN_NAME)\n\n    tag_to_data = None\n    try:\n      tag_to_data = run_to_data[run]\n    except KeyError:\n      # The run could not be found. Perhaps a configuration specified a run that\n      # TensorBoard has not read from disk yet.\n      payload = {}\n\n    if tag_to_data:\n      scalars_plugin_instance = self._get_scalars_plugin()\n      if not scalars_plugin_instance:\n        raise ValueError(('Failed to respond to request for /scalars. '\n                          'The scalars plugin is oddly not registered.'))\n\n      form = scalars_plugin.OutputFormat.JSON\n      payload = {\n        tag: scalars_plugin_instance.scalars_impl(tag, run, None, form)[0]\n            for tag in tag_to_data.keys()\n            if regex.match(tag)\n      }\n\n    return {\n        _REGEX_VALID_PROPERTY: True,\n        _TAG_TO_EVENTS_PROPERTY: payload,\n    }", "output": "Given a tag regex and single run, return ScalarEvents.\n\n    Args:\n      run: A run string.\n      tag_regex_string: A regular expression that captures portions of tags.\n\n    Raises:\n      ValueError: if the scalars plugin is not registered.\n\n    Returns:\n      A dictionary that is the JSON-able response.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def include_revision(revision_num, skip_factor=1.1):\n  \"\"\"\n  \"\"\"\n  if skip_factor <= 1.0:\n    return True\n  return (int(math.log1p(revision_num) / math.log(skip_factor)) != int(\n      math.log(revision_num + 2.0) / math.log(skip_factor)))", "output": "Decide whether to include a revision.\n\n  If the number of revisions is large, we exclude some revisions to avoid\n  a quadratic blowup in runtime, since the article is likely also large.\n\n  We make the ratio between consecutive included revision numbers\n  appproximately equal to \"factor\".\n\n  Args:\n    revision_num: an integer\n    skip_factor: a floating point number >= 1.0\n\n  Returns:\n    a boolean", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save(self, file:PathLikeOrBinaryStream= 'data_save.pkl')->None:\n        \"\"\n        if not getattr(self, 'label_list', False):\n            warn(\"Serializing the `DataBunch` only works when you created it using the data block API.\")\n            return\n        try_save(self.label_list, self.path, file)", "output": "Save the `DataBunch` in `self.path/file`. `file` can be file-like (file or buffer)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expand(tmpl, *args, **kwargs):\n    \"\"\"\n    \"\"\"\n    replacer = functools.partial(_expand_variable_match, list(args), kwargs)\n    return _VARIABLE_RE.sub(replacer, tmpl)", "output": "Expand a path template with the given variables.\n\n    ..code-block:: python\n\n        >>> expand('users/*/messages/*', 'me', '123')\n        users/me/messages/123\n        >>> expand('/v1/{name=shelves/*/books/*}', name='shelves/1/books/3')\n        /v1/shelves/1/books/3\n\n    Args:\n        tmpl (str): The path template.\n        args: The positional variables for the path.\n        kwargs: The named variables for the path.\n\n    Returns:\n        str: The expanded path\n\n    Raises:\n        ValueError: If a positional or named variable is required by the\n            template but not specified or if an unexpected template expression\n            is encountered.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _setattr_wrapper(mod, key, value):\n    \"\"\"\n    \n    \"\"\"\n    setattr(mod, key, value)\n    if mod == _thismodule:\n        setattr(_sys.modules[__name__], key, value)", "output": "A setattr wrapper call used only by _publish(). This ensures that anything\n    published into this module is also published into tc.extensions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def profile(model, inputs=None, n_texts=10000):\n    \"\"\"\n    \n    \"\"\"\n    msg = Printer()\n    if inputs is not None:\n        inputs = _read_inputs(inputs, msg)\n    if inputs is None:\n        n_inputs = 25000\n        with msg.loading(\"Loading IMDB dataset via Thinc...\"):\n            imdb_train, _ = thinc.extra.datasets.imdb()\n            inputs, _ = zip(*imdb_train)\n        msg.info(\"Loaded IMDB dataset and using {} examples\".format(n_inputs))\n        inputs = inputs[:n_inputs]\n    with msg.loading(\"Loading model '{}'...\".format(model)):\n        nlp = load_model(model)\n    msg.good(\"Loaded model '{}'\".format(model))\n    texts = list(itertools.islice(inputs, n_texts))\n    cProfile.runctx(\"parse_texts(nlp, texts)\", globals(), locals(), \"Profile.prof\")\n    s = pstats.Stats(\"Profile.prof\")\n    msg.divider(\"Profile stats\")\n    s.strip_dirs().sort_stats(\"time\").print_stats()", "output": "Profile a spaCy pipeline, to find out which functions take the most time.\n    Input should be formatted as one JSON object per line with a key \"text\".\n    It can either be provided as a JSONL file, or be read from sys.sytdin.\n    If no input file is specified, the IMDB dataset is loaded via Thinc.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cert_array_from_pem(pem_bundle):\n    \"\"\"\n    \n    \"\"\"\n    # Normalize the PEM bundle's line endings.\n    pem_bundle = pem_bundle.replace(b\"\\r\\n\", b\"\\n\")\n\n    der_certs = [\n        base64.b64decode(match.group(1))\n        for match in _PEM_CERTS_RE.finditer(pem_bundle)\n    ]\n    if not der_certs:\n        raise ssl.SSLError(\"No root certificates specified\")\n\n    cert_array = CoreFoundation.CFArrayCreateMutable(\n        CoreFoundation.kCFAllocatorDefault,\n        0,\n        ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks)\n    )\n    if not cert_array:\n        raise ssl.SSLError(\"Unable to allocate memory!\")\n\n    try:\n        for der_bytes in der_certs:\n            certdata = _cf_data_from_bytes(der_bytes)\n            if not certdata:\n                raise ssl.SSLError(\"Unable to allocate memory!\")\n            cert = Security.SecCertificateCreateWithData(\n                CoreFoundation.kCFAllocatorDefault, certdata\n            )\n            CoreFoundation.CFRelease(certdata)\n            if not cert:\n                raise ssl.SSLError(\"Unable to build cert object!\")\n\n            CoreFoundation.CFArrayAppendValue(cert_array, cert)\n            CoreFoundation.CFRelease(cert)\n    except Exception:\n        # We need to free the array before the exception bubbles further.\n        # We only want to do that if an error occurs: otherwise, the caller\n        # should free.\n        CoreFoundation.CFRelease(cert_array)\n\n    return cert_array", "output": "Given a bundle of certs in PEM format, turns them into a CFArray of certs\n    that can be used to validate a cert chain.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode_images_as_png(images):\n  \"\"\"\"\"\"\n  if tf.executing_eagerly():\n    for image in images:\n      yield tf.image.encode_png(image).numpy()\n  else:\n    (height, width, channels) = images[0].shape\n    with tf.Graph().as_default():\n      image_t = tf.placeholder(dtype=tf.uint8, shape=(height, width, channels))\n      encoded_image_t = tf.image.encode_png(image_t)\n      with tf.Session() as sess:\n        for image in images:\n          enc_string = sess.run(encoded_image_t, feed_dict={image_t: image})\n          yield enc_string", "output": "Yield images encoded as pngs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_extra_packages(self, repo):\n        \"\"\"\n        \n        \"\"\"\n        if self._update:\n            extras = {k: [d.name for d in v] for k, v in self._package.extras.items()}\n        else:\n            extras = self._locker.lock_data.get(\"extras\", {})\n\n        extra_packages = []\n        for extra_name, packages in extras.items():\n            if extra_name not in self._extras:\n                continue\n\n            extra_packages += [Dependency(p, \"*\") for p in packages]\n\n        def _extra_packages(packages):\n            pkgs = []\n            for package in packages:\n                for pkg in repo.packages:\n                    if pkg.name == package.name:\n                        pkgs.append(package)\n                        pkgs += _extra_packages(pkg.requires)\n\n                        break\n\n            return pkgs\n\n        return _extra_packages(extra_packages)", "output": "Returns all packages required by extras.\n\n        Maybe we just let the solver handle it?", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathNextChild(self, ctxt):\n        \"\"\" \"\"\"\n        if ctxt is None: ctxt__o = None\n        else: ctxt__o = ctxt._o\n        ret = libxml2mod.xmlXPathNextChild(ctxt__o, self._o)\n        if ret is None:raise xpathError('xmlXPathNextChild() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Traversal function for the \"child\" direction The child axis\n          contains the children of the context node in document order.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def device(self, idx):\n        \"\"\"\n        \"\"\"\n\n        class GpuDevice(Structure):\n            pass\n\n        c_nvmlDevice_t = POINTER(GpuDevice)\n\n        c_index = c_uint(idx)\n        device = c_nvmlDevice_t()\n        _check_return(_NVML.get_function(\n            \"nvmlDeviceGetHandleByIndex_v2\")(c_index, byref(device)))\n        return NvidiaDevice(device)", "output": "Get a specific GPU device\n\n        Args:\n            idx: index of device\n\n        Returns:\n            NvidiaDevice: single GPU device", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, id):\n        \"\"\"\n        \"\"\"\n        self.cur.execute(\"SELECT * FROM jobs WHERE hash=?\", (id,))\n        item = self.cur.fetchone()\n        if item:\n            return dict(zip(\n                (\"id\", \"description\", \"last-run\", \"next-run\", \"last-run-result\"),\n                item))\n\n        return None", "output": "Retrieves the job with the selected ID.\n        :param str id: The ID of the job\n        :returns: The dictionary of the job if found, None otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cursor_position_changed(self):\n        \"\"\" \n        \"\"\"\n        cursor = self._text_edit.textCursor()\n        position = cursor.position()\n        document = self._text_edit.document()\n        char = to_text_string(document.characterAt(position - 1))\n        if position <= self._start_position:\n            self.hide()\n        elif char == ')':\n            pos, _ = self._find_parenthesis(position - 1, forward=False)\n            if pos == -1:\n                self.hide()", "output": "Updates the tip based on user cursor movement.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_metadata(self, data_dir, feature_name=None):\n    \"\"\"\"\"\"\n    # Recursively save all child features\n    for feature_key, feature in six.iteritems(self._feature_dict):\n      if feature_name:\n        feature_key = '-'.join((feature_name, feature_key))\n      feature.save_metadata(data_dir, feature_name=feature_key)", "output": "See base class for details.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_ssh(opts):\n    '''\n    \n    '''\n    if opts is None:\n        opts = __opts__\n    try:\n        this_prompt = None\n        if 'prompt_regex' in opts['proxy']:\n            this_prompt = opts['proxy']['prompt_regex']\n        elif 'prompt_name' in opts['proxy']:\n            this_prompt = '{0}.*#'.format(opts['proxy']['prompt_name'])\n        else:\n            log.warning('nxos proxy configuration does not specify a prompt match.')\n            this_prompt = '.+#$'\n\n        DEVICE_DETAILS[_worker_name()] = SSHConnection(\n            host=opts['proxy']['host'],\n            username=opts['proxy']['username'],\n            password=opts['proxy']['password'],\n            key_accept=opts['proxy'].get('key_accept', False),\n            ssh_args=opts['proxy'].get('ssh_args', ''),\n            prompt=this_prompt)\n        out, err = DEVICE_DETAILS[_worker_name()].sendline('terminal length 0')\n        log.info('SSH session establised for process %s', _worker_name())\n    except Exception as ex:\n        log.error('Unable to connect to %s', opts['proxy']['host'])\n        log.error('Please check the following:\\n')\n        log.error('-- Verify that \"feature ssh\" is enabled on your NX-OS device: %s', opts['proxy']['host'])\n        log.error('-- Exception Generated: %s', ex)\n        log.error(ex)\n        exit()\n    DEVICE_DETAILS['initialized'] = True\n    DEVICE_DETAILS['no_save_config'] = opts['proxy'].get('no_save_config', False)", "output": "Open a connection to the NX-OS switch over SSH.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_POST(self, environ, start_response):\n        \"\"\"\n        \"\"\"\n\n        try:\n            # Get arguments by reading body of request.\n            # We read this in chunks to avoid straining\n            # socket.read(); around the 10 or 15Mb mark, some platforms\n            # begin to have problems (bug #792570).\n\n            length = int(environ['CONTENT_LENGTH'])\n            data = environ['wsgi.input'].read(length)\n\n            # In previous versions of SimpleXMLRPCServer, _dispatch\n            # could be overridden in this class, instead of in\n            # SimpleXMLRPCDispatcher. To maintain backwards compatibility,\n            # check to see if a subclass implements _dispatch and\n            # using that method if present.\n            response = self.dispatcher._marshaled_dispatch(\n                data, getattr(self.dispatcher, '_dispatch', None)\n            )\n            response += b'\\n'\n        except Exception as e:  # This should only happen if the module is buggy\n            # internal error, report as HTTP server error\n            logger.exception(e)\n            start_response(\"500 Server error\", [('Content-Type', 'text/plain')])\n            return []\n        else:\n            # got a valid XML RPC response\n            start_response(\"200 OK\", [('Content-Type', 'text/xml'), ('Content-Length', str(len(response)),)])\n            return [response]", "output": "Handles the HTTP POST request.\n\n        Attempts to interpret all HTTP POST requests as XML-RPC calls,\n        which are forwarded to the server's _dispatch method for handling.\n\n        Most code taken from SimpleXMLRPCServer with modifications for wsgi and my custom dispatcher.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_batch_size(batch: Union[Dict, torch.Tensor]) -> int:\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(batch, torch.Tensor):\n        return batch.size(0) # type: ignore\n    elif isinstance(batch, Dict):\n        return get_batch_size(next(iter(batch.values())))\n    else:\n        return 0", "output": "Returns the size of the batch dimension. Assumes a well-formed batch,\n    returns 0 otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cancel_http(api_request, operation_name):\n    \"\"\"\n    \"\"\"\n    path = \"operations/{}:cancel\".format(operation_name)\n    api_request(method=\"POST\", path=path)", "output": "Cancel an operation using a JSON/HTTP client.\n\n    Args:\n        api_request (Callable): A callable used to make an API request. This\n            should generally be\n            :meth:`google.cloud._http.Connection.api_request`.\n        operation_name (str): The name of the operation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_shortcuts(self):\n        \"\"\"\"\"\"\n        inspect = config_shortcut(self._control.inspect_current_object,\n                                  context='Console',\n                                  name='Inspect current object', parent=self)\n        clear_console = config_shortcut(self.clear_console, context='Console',\n                                        name='Clear shell', parent=self)\n        restart_kernel = config_shortcut(self.ipyclient.restart_kernel,\n                                         context='ipython_console',\n                                         name='Restart kernel', parent=self)\n        new_tab = config_shortcut(lambda: self.new_client.emit(),\n                                  context='ipython_console', name='new tab',\n                                  parent=self)\n        reset_namespace = config_shortcut(lambda: self._reset_namespace(),\n                                          context='ipython_console',\n                                          name='reset namespace', parent=self)\n        array_inline = config_shortcut(self._control.enter_array_inline,\n                                       context='array_builder',\n                                       name='enter array inline', parent=self)\n        array_table = config_shortcut(self._control.enter_array_table,\n                                      context='array_builder',\n                                      name='enter array table', parent=self)\n        clear_line = config_shortcut(self.ipyclient.clear_line,\n                                     context='console', name='clear line',\n                                     parent=self)\n\n        return [inspect, clear_console, restart_kernel, new_tab,\n                reset_namespace, array_inline, array_table, clear_line]", "output": "Create shortcuts for ipyconsole.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pool(self):\n        \"\"\"\n        \"\"\"\n        if self._pool is None:\n            self._pool = ThreadPool(self.pool_threads)\n        return self._pool", "output": "Create thread pool on first request\n         avoids instantiating unused threadpool for blocking clients.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_xml(xml_str):\n    '''\n    \n    '''\n    try:\n        xml_data = etree.XML(xml_str)\n    # XMLSyntaxError seems to be only available from lxml, but that is the xml\n    # library loaded by this module\n    except etree.XMLSyntaxError as err:\n        # opennebula returned invalid XML, which could be an error message, so\n        # log it\n        raise SaltCloudSystemExit('opennebula returned: {0}'.format(xml_str))\n    return xml_data", "output": "Intrepret the data coming from opennebula and raise if it's not XML.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_signature(self, content):\n        \"\"\"\"\"\"\n        data = content.get('data', {})\n        text = data.get('text/plain', '')\n        if text:\n            text = ANSI_OR_SPECIAL_PATTERN.sub('', text)\n            self._control.current_prompt_pos = self._prompt_pos\n            line = self._control.get_current_line_to_cursor()\n            name = line[:-1].split('(')[-1]   # Take last token after a (\n            name = name.split('.')[-1]        # Then take last token after a .\n            # Clean name from invalid chars\n            try:\n                name = self.clean_invalid_var_chars(name).split('_')[-1]\n            except:\n                pass\n            argspec = getargspecfromtext(text)\n            if argspec:\n                # This covers cases like np.abs, whose docstring is\n                # the same as np.absolute and because of that a proper\n                # signature can't be obtained correctly\n                signature = name + argspec\n            else:\n                signature = getsignaturefromtext(text, name)\n\n            # Remove docstring for uniformity with editor\n            signature = signature.split('Docstring:')[0]\n\n            return signature\n        else:\n            return ''", "output": "Get signature from inspect reply content", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _aux_data(self, i):\n        \"\"\" \n        \"\"\"\n        self.wait_to_read()\n        hdl = NDArrayHandle()\n        check_call(_LIB.MXNDArrayGetAuxNDArray(self.handle, i, ctypes.byref(hdl)))\n        return NDArray(hdl)", "output": "Get a deep copy NDArray of the i-th aux data array associated with the\n        BaseSparseNDArray.\n\n        This function blocks. Do not use it in performance critical code.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _arith_method_SPARSE_SERIES(cls, op, special):\n    \"\"\"\n    \n    \"\"\"\n    op_name = _get_op_name(op, special)\n\n    def wrapper(self, other):\n        if isinstance(other, ABCDataFrame):\n            return NotImplemented\n        elif isinstance(other, ABCSeries):\n            if not isinstance(other, ABCSparseSeries):\n                other = other.to_sparse(fill_value=self.fill_value)\n            return _sparse_series_op(self, other, op, op_name)\n        elif is_scalar(other):\n            with np.errstate(all='ignore'):\n                new_values = op(self.values, other)\n            return self._constructor(new_values,\n                                     index=self.index,\n                                     name=self.name)\n        else:  # pragma: no cover\n            raise TypeError('operation with {other} not supported'\n                            .format(other=type(other)))\n\n    wrapper.__name__ = op_name\n    return wrapper", "output": "Wrapper function for Series arithmetic operations, to avoid\n    code duplication.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_name_mapping(self, name: str, translated_name: str, name_type: Type = None):\n        \"\"\"\n        \n        \"\"\"\n        self.local_name_mapping[name] = translated_name\n        self.reverse_name_mapping[translated_name] = name\n        if name_type:\n            self.local_type_signatures[translated_name] = name_type", "output": "Utility method to add a name and its translation to the local name mapping, and the corresponding\n        signature, if available to the local type signatures. This method also updates the reverse name\n        mapping.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nts(s, encoding, errors):\n    \"\"\"\n    \"\"\"\n    p = s.find(b\"\\0\")\n    if p != -1:\n        s = s[:p]\n    return s.decode(encoding, errors)", "output": "Convert a null-terminated bytes object to a string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def joined(name, host, user='rabbit', ram_node=None, runas='root'):\n    '''\n    \n    '''\n\n    ret = {'name': name, 'result': True, 'comment': '', 'changes': {}}\n\n    status = __salt__['rabbitmq.cluster_status']()\n    if '{0}@{1}'.format(user, host) in status:\n        ret['comment'] = 'Already in cluster'\n        return ret\n\n    if not __opts__['test']:\n        result = __salt__['rabbitmq.join_cluster'](host,\n                                                   user,\n                                                   ram_node,\n                                                   runas=runas)\n        if 'Error' in result:\n            ret['result'] = False\n            ret['comment'] = result['Error']\n            return ret\n        elif 'Join' in result:\n            ret['comment'] = result['Join']\n\n    # If we've reached this far before returning, we have changes.\n    ret['changes'] = {'old': '', 'new': '{0}@{1}'.format(user, host)}\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Node is set to join cluster {0}@{1}'.format(\n            user, host)\n\n    return ret", "output": "Ensure the current node joined to a cluster with node user@host\n\n    name\n        Irrelevant, not used (recommended: user@host)\n    user\n        The user of node to join to (default: rabbit)\n    host\n        The host of node to join to\n    ram_node\n        Join node as a RAM node\n    runas\n        The user to run the rabbitmq command as", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ipv6_host(host, scheme):\n    \"\"\"\n    \n    \"\"\"\n\n    # httplib doesn't like it when we include brackets in IPv6 addresses\n    # Specifically, if we include brackets but also pass the port then\n    # httplib crazily doubles up the square brackets on the Host header.\n    # Instead, we need to make sure we never pass ``None`` as the port.\n    # However, for backward compatibility reasons we can't actually\n    # *assert* that.  See http://bugs.python.org/issue28539\n    #\n    # Also if an IPv6 address literal has a zone identifier, the\n    # percent sign might be URIencoded, convert it back into ASCII\n    if host.startswith('[') and host.endswith(']'):\n        host = host.replace('%25', '%').strip('[]')\n    if scheme in NORMALIZABLE_SCHEMES:\n        host = host.lower()\n    return host", "output": "Process IPv6 address literals", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_and_replace(text, start_string, end_string, replace_fn):\n  \"\"\"\n  \"\"\"\n  ret = u\"\"\n  current_pos = 0\n  while True:\n    start_pos = text.find(start_string, current_pos)\n    if start_pos == -1:\n      ret += text[current_pos:]\n      break\n    ret += text[current_pos:start_pos]\n    end_pos = text.find(end_string, start_pos + len(start_string))\n    if end_pos == -1:\n      break\n    ret += replace_fn(text[start_pos + len(start_string):end_pos])\n    current_pos = end_pos + len(end_string)\n  return ret", "output": "Remove everything found between instances of start_string and end_string.\n\n  Replace each such instance with replace_fn(removed_text)\n\n  e.g. _find_and_replace(u\"the [[fat]] cat [[sat]]\", u\"[[\", u\"]]\", lambda x: x)\n    = u\"the fat cat sat\"\n\n  Args:\n    text: a unicode string\n    start_string: a unicode string\n    end_string: a unicode string\n    replace_fn: a unary function from unicode string to unicode string\n\n  Returns:\n    a string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_resource_config(cls, resource):\n        \"\"\"\n        \"\"\"\n        if \"jobReference\" not in resource or \"jobId\" not in resource[\"jobReference\"]:\n            raise KeyError(\n                \"Resource lacks required identity information: \"\n                '[\"jobReference\"][\"jobId\"]'\n            )\n        job_id = resource[\"jobReference\"][\"jobId\"]\n        if (\n            \"configuration\" not in resource\n            or cls._JOB_TYPE not in resource[\"configuration\"]\n        ):\n            raise KeyError(\n                \"Resource lacks required configuration: \"\n                '[\"configuration\"][\"%s\"]' % cls._JOB_TYPE\n            )\n        return job_id, resource[\"configuration\"]", "output": "Helper for :meth:`from_api_repr`\n\n        :type resource: dict\n        :param resource: resource for the job\n\n        :rtype: dict\n        :returns: tuple (string, dict), where the first element is the\n                  job ID and the second contains job-specific configuration.\n        :raises: :class:`KeyError` if the resource has no identifier, or\n                 is missing the appropriate configuration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def str_count(arr, pat, flags=0):\n    \"\"\"\n    \n    \"\"\"\n    regex = re.compile(pat, flags=flags)\n    f = lambda x: len(regex.findall(x))\n    return _na_map(f, arr, dtype=int)", "output": "Count occurrences of pattern in each string of the Series/Index.\n\n    This function is used to count the number of times a particular regex\n    pattern is repeated in each of the string elements of the\n    :class:`~pandas.Series`.\n\n    Parameters\n    ----------\n    pat : str\n        Valid regular expression.\n    flags : int, default 0, meaning no flags\n        Flags for the `re` module. For a complete list, `see here\n        <https://docs.python.org/3/howto/regex.html#compilation-flags>`_.\n    **kwargs\n        For compatibility with other string methods. Not used.\n\n    Returns\n    -------\n    Series or Index\n        Same type as the calling object containing the integer counts.\n\n    See Also\n    --------\n    re : Standard library module for regular expressions.\n    str.count : Standard library version, without regular expression support.\n\n    Notes\n    -----\n    Some characters need to be escaped when passing in `pat`.\n    eg. ``'$'`` has a special meaning in regex and must be escaped when\n    finding this literal character.\n\n    Examples\n    --------\n    >>> s = pd.Series(['A', 'B', 'Aaba', 'Baca', np.nan, 'CABA', 'cat'])\n    >>> s.str.count('a')\n    0    0.0\n    1    0.0\n    2    2.0\n    3    2.0\n    4    NaN\n    5    0.0\n    6    1.0\n    dtype: float64\n\n    Escape ``'$'`` to find the literal dollar sign.\n\n    >>> s = pd.Series(['$', 'B', 'Aab$', '$$ca', 'C$B$', 'cat'])\n    >>> s.str.count('\\\\$')\n    0    1\n    1    0\n    2    1\n    3    2\n    4    2\n    5    0\n    dtype: int64\n\n    This is also available on Index\n\n    >>> pd.Index(['A', 'A', 'Aaba', 'cat']).str.count('a')\n    Int64Index([0, 0, 2, 1], dtype='int64')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_temp_export_dir(timestamped_export_dir):\n  \"\"\"\n  \"\"\"\n  (dirname, basename) = os.path.split(timestamped_export_dir)\n  temp_export_dir = os.path.join(\n      tf.compat.as_bytes(dirname),\n      tf.compat.as_bytes(\"temp-{}\".format(basename)))\n  return temp_export_dir", "output": "Builds a directory name based on the argument but starting with 'temp-'.\n\n  This relies on the fact that TensorFlow Serving ignores subdirectories of\n  the base directory that can't be parsed as integers.\n\n  Args:\n    timestamped_export_dir: the name of the eventual export directory, e.g.\n      /foo/bar/<timestamp>\n\n  Returns:\n    A sister directory prefixed with 'temp-', e.g. /foo/bar/temp-<timestamp>.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_with_uniform(uf, arg_shapes, dim=None, npuf=None, rmin=-10, type_list=[np.float32]):\n    \"\"\"\"\"\"\n    if isinstance(arg_shapes, int):\n        assert dim\n        shape = tuple(np.random.randint(1, int(1000**(1.0/dim)), size=dim))\n        arg_shapes = [shape] * arg_shapes\n    for dtype in type_list:\n        ndarray_arg = []\n        numpy_arg = []\n        for s in arg_shapes:\n            npy = np.random.uniform(rmin, 10, s).astype(dtype)\n            narr = mx.nd.array(npy, dtype=dtype)\n            ndarray_arg.append(narr)\n            numpy_arg.append(npy)\n        out1 = uf(*ndarray_arg)\n        if npuf is None:\n            out2 = uf(*numpy_arg).astype(dtype)\n        else:\n            out2 = npuf(*numpy_arg).astype(dtype)\n\n        assert out1.shape == out2.shape\n        if isinstance(out1, mx.nd.NDArray):\n            out1 = out1.asnumpy()\n        if dtype == np.float16:\n            assert reldiff(out1, out2) < 2e-3\n        else:\n            assert reldiff(out1, out2) < 1e-6", "output": "check function consistency with uniform random numbers", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wrap(txt, width=80, ident=0):\n    '''\n    \n    '''\n    ident = ' ' * ident\n    txt = (txt or '').replace(os.linesep, ' ').strip()\n\n    wrapper = textwrap.TextWrapper()\n    wrapper.fix_sentence_endings = False\n    wrapper.initial_indent = wrapper.subsequent_indent = ident\n\n    return wrapper.wrap(txt)", "output": "Wrap text to the required dimensions and clean it up, prepare for display.\n\n    :param txt:\n    :param width:\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_proc_name(proc):\n    '''\n    \n    '''\n    try:\n        return salt.utils.data.decode(proc.name() if PSUTIL2 else proc.name)\n    except (psutil.NoSuchProcess, psutil.AccessDenied):\n        return []", "output": "Returns the name of a Process instance.\n\n    It's backward compatible with < 2.0 versions of psutil.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absolute_hinge_difference(arr1, arr2, min_diff=10, dtype=np.uint8):\n  \"\"\"\n  \"\"\"\n  diff = np.abs(arr1.astype(np.int) - arr2, dtype=np.int)\n  return np.maximum(diff - min_diff, 0).astype(dtype)", "output": "Point-wise, hinge loss-like, difference between arrays.\n\n  Args:\n    arr1: integer array to compare.\n    arr2: integer array to compare.\n    min_diff: minimal difference taken into consideration.\n    dtype: dtype of returned array.\n\n  Returns:\n    array", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy_submission_to_destination(self, src_filename, dst_subdir,\n                                     submission_id):\n    \"\"\"\n    \"\"\"\n\n    extension = [e for e in ALLOWED_EXTENSIONS if src_filename.endswith(e)]\n    if len(extension) != 1:\n      logging.error('Invalid submission extension: %s', src_filename)\n      return\n    dst_filename = os.path.join(self.target_dir, dst_subdir,\n                                submission_id + extension[0])\n    cmd = ['gsutil', 'cp', src_filename, dst_filename]\n    if subprocess.call(cmd) != 0:\n      logging.error('Can\\'t copy submission to destination')\n    else:\n      logging.info('Submission copied to: %s', dst_filename)", "output": "Copies submission to target directory.\n\n    Args:\n      src_filename: source filename of the submission\n      dst_subdir: subdirectory of the target directory where submission should\n        be copied to\n      submission_id: ID of the submission, will be used as a new\n        submission filename (before extension)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_tkinter_size_to_Qt(size):\n    \"\"\"\n    \n    \"\"\"\n    qtsize = size\n    if size[1] is not None and size[1] < DEFAULT_PIXEL_TO_CHARS_CUTOFF:        # change from character based size to pixels (roughly)\n        qtsize = size[0]*DEFAULT_PIXELS_TO_CHARS_SCALING[0], size[1]*DEFAULT_PIXELS_TO_CHARS_SCALING[1]\n    return qtsize", "output": "Converts size in characters to size in pixels\n    :param size:  size in characters, rows\n    :return: size in pixels, pixels", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _list_fields(self):\n        \"\"\"\n        \n        \"\"\"\n\n        response = self.__proxy__.list_fields()\n        return [s for s in response['value'] if not s.startswith(\"_\")]", "output": "Get the current settings of the model. The keys depend on the type of\n        model.\n\n        Returns\n        -------\n        out : list\n            A list of fields that can be queried using the ``get`` method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _split_into_groups(n, max_group_size, mesh_dim_size):\n  \"\"\"\n  \"\"\"\n  if n % mesh_dim_size != 0:\n    raise ValueError(\n        \"n=%d is not a multiple of mesh_dim_size=%d\" % (n, mesh_dim_size))\n  num_groups = max(1, n // max_group_size)\n  while (num_groups % mesh_dim_size != 0 or n % num_groups != 0):\n    num_groups += 1\n  group_size = n // num_groups\n  tf.logging.info(\n      \"_split_into_groups(n=%d, max_group_size=%d, mesh_dim_size=%d)\"\n      \" = (num_groups=%d group_size=%d)\" %\n      (n, max_group_size, mesh_dim_size, num_groups, group_size))\n  return num_groups, group_size", "output": "Helper function for figuring out how to split a dimensino into groups.\n\n  We have a dimension with size n and we want to split it into\n  two dimensions: n = num_groups * group_size\n\n  group_size should be the largest possible value meeting the constraints:\n    group_size <= max_group_size\n    (num_groups = n/group_size) is a multiple of mesh_dim_size\n\n  Args:\n    n: an integer\n    max_group_size: an integer\n    mesh_dim_size: an integer\n\n  Returns:\n    num_groups: an integer\n    group_size: an integer\n\n  Raises:\n    ValueError: if n is not a multiple of mesh_dim_size", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ea_passthrough(array_method):\n    \"\"\"\n    \n    \"\"\"\n\n    def method(self, *args, **kwargs):\n        return array_method(self._data, *args, **kwargs)\n\n    method.__name__ = array_method.__name__\n    method.__doc__ = array_method.__doc__\n    return method", "output": "Make an alias for a method of the underlying ExtensionArray.\n\n    Parameters\n    ----------\n    array_method : method on an Array class\n\n    Returns\n    -------\n    method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _firmware_update(firmwarefile='', host='',\n                     directory=''):\n    '''\n    \n    '''\n    dest = os.path.join(directory, firmwarefile[7:])\n\n    __salt__['cp.get_file'](firmwarefile, dest)\n\n    username = __pillar__['proxy']['admin_user']\n    password = __pillar__['proxy']['admin_password']\n    __salt__['dracr.update_firmware'](dest,\n                                      host=host,\n                                      admin_username=username,\n                                      admin_password=password)", "output": "Update firmware for a single host", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log_internal(args, filetype):\n    ''''''\n    file_name = get_config_filename(args)\n    if filetype == 'stdout':\n        file_full_path = os.path.join(NNICTL_HOME_DIR, file_name, 'stdout')\n    else:\n        file_full_path = os.path.join(NNICTL_HOME_DIR, file_name, 'stderr')\n    print(check_output_command(file_full_path, head=args.head, tail=args.tail))", "output": "internal function to call get_log_content", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shutdown(self):\n        \"\"\"\n        \"\"\"\n        # Drop all pending item from the executor. Without this, the executor\n        # will block until all pending items are complete, which is\n        # undesirable.\n        try:\n            while True:\n                self._executor._work_queue.get(block=False)\n        except queue.Empty:\n            pass\n        self._executor.shutdown()", "output": "Shuts down the scheduler and immediately end all pending callbacks.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_new_job_event(self, event_data):\n        '''\n        \n\n        '''\n        job = None\n        tag = event_data['tag']\n        event_info = event_data['data']\n        minions = {}\n        for mid in event_info['minions']:\n            minions[mid] = {'success': False}\n\n        job = {\n            'jid': event_info['jid'],\n            'start_time': event_info['_stamp'],\n            'minions': minions,  # is a dictionary keyed by mids\n            'fun': event_info['fun'],\n            'tgt': event_info['tgt'],\n            'tgt_type': event_info['tgt_type'],\n            'state': 'running',\n        }\n        self.jobs[event_info['jid']] = job\n        self.publish('jobs', self.jobs)", "output": "Creates a new job with properties from the event data\n        like jid, function, args, timestamp.\n\n        Also sets the initial state to started.\n\n        Minions that are participating in this job are also noted.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put_task(self, task):\n        ''''''\n        _schedule = task.get('schedule', self.default_schedule)\n        self.projects[task['project']].task_queue.put(\n            task['taskid'],\n            priority=_schedule.get('priority', self.default_schedule['priority']),\n            exetime=_schedule.get('exetime', self.default_schedule['exetime'])\n        )", "output": "put task to task queue", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wite_to_json(self, dir_path=\"\", file_name=\"\"):\n        \"\"\"\"\"\"\n        # \u63d0\u53d6\u6570\u636e\n        data = {\n            \"plot_data\": self.record_thread.profile_data,\n            \"method_exec_info\": self.method_exec_info,\n            \"search_file\": self.search_file,\n            \"source_file\": self.source_file}\n        # \u5199\u5165\u6587\u4ef6\n        file_path = os.path.join(dir_path, file_name)\n        if not os.path.exists(dir_path):\n            os.makedirs(dir_path)\n        json.dump(data, open(file_path, \"w+\"), indent=4)", "output": "\u5c06\u6027\u80fd\u6570\u636e\u5199\u5165\u6587\u4ef6.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _execute_cmd(plugin, args='', run_type='cmd.retcode'):\n    '''\n    \n    '''\n    data = {}\n\n    all_plugins = list_plugins()\n    if plugin in all_plugins:\n        data = __salt__[run_type](\n                '{0}{1} {2}'.format(PLUGINDIR, plugin, args),\n                python_shell=False)\n\n    return data", "output": "Execute nagios plugin if it's in the directory with salt command specified in run_type", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dump(self, filename):\n        \"\"\"\"\"\"\n        try:\n            with open(filename, 'wb') as fp:\n                cPickle.dump(self.counters, fp)\n        except Exception as e:\n            logging.warning(\"can't dump counter to file %s: %s\", filename, e)\n            return False\n        return True", "output": "Dump counters to file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_distinguished_name_list(list_name):\n    '''\n    \n\n    '''\n    payload = {\"jsonrpc\": \"2.0\",\n               \"id\": \"ID0\",\n               \"method\": \"add_policy_distinguished_names_list\",\n               \"params\": [{\"list_name\": list_name}]}\n\n    response = __proxy__['bluecoat_sslv.call'](payload, True)\n\n    return _validate_change_result(response)", "output": "Add a list of policy distinguished names.\n\n    list_name(str): The name of the specific policy distinguished name list to add.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bluecoat_sslv.add_distinguished_name_list MyDistinguishedList", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_jid(jid):\n    '''\n    \n    '''\n    conn, mdb = _get_conn(ret=None)\n    ret = {}\n    rdata = mdb.saltReturns.find({'jid': jid}, {'_id': 0})\n    if rdata:\n        for data in rdata:\n            minion = data['minion']\n            # return data in the format {<minion>: { <unformatted full return data>}}\n            ret[minion] = data['full_ret']\n    return ret", "output": "Return the return information associated with a jid", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_or_set_hash(name,\n        length=8,\n        chars='abcdefghijklmnopqrstuvwxyz0123456789!@#$%^&*(-_=+)'):\n    '''\n    \n    '''\n    ret = get(name, None)\n\n    if ret is None:\n        val = ''.join([random.SystemRandom().choice(chars) for _ in range(length)])\n\n        if DEFAULT_TARGET_DELIM in name:\n            root, rest = name.split(DEFAULT_TARGET_DELIM, 1)\n            curr = get(root, _infinitedict())\n            val = _dict_from_path(rest, val)\n            curr.update(val)\n            setval(root, curr)\n        else:\n            setval(name, val)\n\n    return get(name)", "output": "Perform a one-time generation of a hash and write it to the local grains.\n    If that grain has already been set return the value instead.\n\n    This is useful for generating passwords or keys that are specific to a\n    single minion that don't need to be stored somewhere centrally.\n\n    State Example:\n\n    .. code-block:: yaml\n\n        some_mysql_user:\n          mysql_user:\n            - present\n            - host: localhost\n            - password: {{ salt['grains.get_or_set_hash']('mysql:some_mysql_user') }}\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' grains.get_or_set_hash 'django:SECRET_KEY' 50\n\n    .. warning::\n\n        This function could return strings which may contain characters which are reserved\n        as directives by the YAML parser, such as strings beginning with ``%``. To avoid\n        issues when using the output of this function in an SLS file containing YAML+Jinja,\n        surround the call with single quotes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _tta_only(learn:Learner, ds_type:DatasetType=DatasetType.Valid, scale:float=1.35) -> Iterator[List[Tensor]]:\n    \"\"\n    dl = learn.dl(ds_type)\n    ds = dl.dataset\n    old = ds.tfms\n    augm_tfm = [o for o in learn.data.train_ds.tfms if o.tfm not in\n               (crop_pad, flip_lr, dihedral, zoom)]\n    try:\n        pbar = master_bar(range(8))\n        for i in pbar:\n            row = 1 if i&1 else 0\n            col = 1 if i&2 else 0\n            flip = i&4\n            d = {'row_pct':row, 'col_pct':col, 'is_random':False}\n            tfm = [*augm_tfm, zoom(scale=scale, **d), crop_pad(**d)]\n            if flip: tfm.append(flip_lr(p=1.))\n            ds.tfms = tfm\n            yield get_preds(learn.model, dl, pbar=pbar, activ=_loss_func2activ(learn.loss_func))[0]\n    finally: ds.tfms = old", "output": "Computes the outputs for several augmented inputs for TTA", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pull_doc(self, document):\n        ''' \n\n        '''\n        msg = self._protocol.create('PULL-DOC-REQ')\n        reply = self._send_message_wait_for_reply(msg)\n        if reply is None:\n            raise RuntimeError(\"Connection to server was lost\")\n        elif reply.header['msgtype'] == 'ERROR':\n            raise RuntimeError(\"Failed to pull document: \" + reply.content['text'])\n        else:\n            reply.push_to_document(document)", "output": "Pull a document from the server, overwriting the passed-in document\n\n        Args:\n            document : (Document)\n              The document to overwrite with server content.\n\n        Returns:\n            None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_random_string(length=44,\n                       allowed_chars='abcdefghijklmnopqrstuvwxyz'\n                       'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789',\n                       secret_key=settings.secret_key_bytes()):\n    \"\"\"\n    \n    \"\"\"\n    secret_key = _ensure_bytes(secret_key)\n    _reseed_if_needed(using_sysrandom, secret_key)\n    return ''.join(random.choice(allowed_chars) for i in range(length))", "output": "Return a securely generated random string.\n    With the a-z, A-Z, 0-9 character set:\n    Length 12 is a 71-bit value. log_2((26+26+10)^12) =~ 71\n    Length 44 is a 261-bit value. log_2((26+26+10)^44) = 261", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_file(name):\n    '''\n    \n    '''\n    out = ''\n    try:\n        with salt.utils.files.fopen(name, 'r') as f:\n            out = salt.utils.stringutils.to_unicode(f.read())\n    except Exception as ex:\n        log.error(ex)\n        return None\n    return out", "output": "output the contents of a file:\n\n    this is a workaround if the cp.push module does not work.\n    https://github.com/saltstack/salt/issues/37133\n\n    help the master output the contents of a document\n    that might be saved on the minions filesystem.\n\n    .. code-block:: python\n\n        #!/bin/python\n        import os\n        import salt.client\n        s = salt.client.LocalClient()\n        o = s.cmd('*', 'highstate_doc.read_file', ['/root/README.md'])\n        for m in o:\n            d = o.get(m)\n            if d and not d.endswith('is not available.'):\n                # mkdir m\n                #directory = os.path.dirname(file_path)\n                if not os.path.exists(m):\n                    os.makedirs(m)\n                with open(m + '/README.md','wb') as fin:\n                    fin.write(d)\n                print('ADDED: ' + m + '/README.md')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def trial(request):\n    \"\"\"\"\"\"\n    job_id = request.GET.get(\"job_id\")\n    trial_id = request.GET.get(\"trial_id\")\n    recent_trials = TrialRecord.objects \\\n        .filter(job_id=job_id) \\\n        .order_by(\"-start_time\")\n    recent_results = ResultRecord.objects \\\n        .filter(trial_id=trial_id) \\\n        .order_by(\"-date\")[0:2000]\n    current_trial = TrialRecord.objects \\\n        .filter(trial_id=trial_id) \\\n        .order_by(\"-start_time\")[0]\n    context = {\n        \"job_id\": job_id,\n        \"trial_id\": trial_id,\n        \"current_trial\": current_trial,\n        \"recent_results\": recent_results,\n        \"recent_trials\": recent_trials\n    }\n    return render(request, \"trial.html\", context)", "output": "View for a single trial.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transform(self, tfms:Optional[Tuple[TfmList,TfmList]]=(None,None), **kwargs):\n        \"\"\n        if not tfms: tfms=(None,None)\n        assert is_listy(tfms) and len(tfms) == 2, \"Please pass a list of two lists of transforms (train and valid).\"\n        self.train.transform(tfms[0], **kwargs)\n        self.valid.transform(tfms[1], **kwargs)\n        if self.test: self.test.transform(tfms[1], **kwargs)\n        return self", "output": "Set `tfms` to be applied to the xs of the train and validation set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def route_filter_delete(name, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    result = False\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        route_filter = netconn.route_filters.delete(\n            route_filter_name=name,\n            resource_group_name=resource_group\n        )\n        route_filter.wait()\n        result = True\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Delete a route filter.\n\n    :param name: The name of the route filter to delete.\n\n    :param resource_group: The resource group name assigned to the\n        route filter.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.route_filter_delete test-filter testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install_given_reqs(\n    to_install,  # type: List[InstallRequirement]\n    install_options,  # type: List[str]\n    global_options=(),  # type: Sequence[str]\n    *args, **kwargs\n):\n    # type: (...) -> List[InstallRequirement]\n    \"\"\"\n    \n    \"\"\"\n\n    if to_install:\n        logger.info(\n            'Installing collected packages: %s',\n            ', '.join([req.name for req in to_install]),\n        )\n\n    with indent_log():\n        for requirement in to_install:\n            if requirement.conflicts_with:\n                logger.info(\n                    'Found existing installation: %s',\n                    requirement.conflicts_with,\n                )\n                with indent_log():\n                    uninstalled_pathset = requirement.uninstall(\n                        auto_confirm=True\n                    )\n            try:\n                requirement.install(\n                    install_options,\n                    global_options,\n                    *args,\n                    **kwargs\n                )\n            except Exception:\n                should_rollback = (\n                    requirement.conflicts_with and\n                    not requirement.install_succeeded\n                )\n                # if install did not succeed, rollback previous uninstall\n                if should_rollback:\n                    uninstalled_pathset.rollback()\n                raise\n            else:\n                should_commit = (\n                    requirement.conflicts_with and\n                    requirement.install_succeeded\n                )\n                if should_commit:\n                    uninstalled_pathset.commit()\n            requirement.remove_temporary_source()\n\n    return to_install", "output": "Install everything in the given list.\n\n    (to be called after having downloaded and unpacked the packages)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _new_conn(self):\n        \"\"\" \n        \"\"\"\n        extra_kw = {}\n        if self.source_address:\n            extra_kw['source_address'] = self.source_address\n\n        if self.socket_options:\n            extra_kw['socket_options'] = self.socket_options\n\n        try:\n            conn = connection.create_connection(\n                (self._dns_host, self.port), self.timeout, **extra_kw)\n\n        except SocketTimeout as e:\n            raise ConnectTimeoutError(\n                self, \"Connection to %s timed out. (connect timeout=%s)\" %\n                (self.host, self.timeout))\n\n        except SocketError as e:\n            raise NewConnectionError(\n                self, \"Failed to establish a new connection: %s\" % e)\n\n        return conn", "output": "Establish a socket connection and set nodelay settings on it.\n\n        :return: New socket connection.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def score_url(self, url):\n        \"\"\"\n        \n        \"\"\"\n        t = urlparse(url)\n        basename = posixpath.basename(t.path)\n        compatible = True\n        is_wheel = basename.endswith('.whl')\n        is_downloadable = basename.endswith(self.downloadable_extensions)\n        if is_wheel:\n            compatible = is_compatible(Wheel(basename), self.wheel_tags)\n        return (t.scheme == 'https', 'pypi.python.org' in t.netloc,\n                is_downloadable, is_wheel, compatible, basename)", "output": "Give an url a score which can be used to choose preferred URLs\n        for a given project release.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def srv_name(svc, proto='tcp', domain=None):\n    '''\n    \n    '''\n    proto = RFC.validate(proto, RFC.SRV_PROTO)\n    if isinstance(svc, int) or svc.isdigit():\n        svc = _to_port(svc)\n\n    if domain:\n        domain = '.' + domain\n    return '_{0}._{1}{2}'.format(svc, proto, domain)", "output": "Generate SRV record name\n    :param svc: ldap, 389 etc\n    :param proto: tcp, udp, sctp etc.\n    :param domain: name to append\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def change_format(self):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        format, valid = QInputDialog.getText(self, _('Format'),\r\n                                             _(\"Float formatting\"),\r\n                                             QLineEdit.Normal,\r\n                                             self.dataModel.get_format())\r\n        if valid:\r\n            format = str(format)\r\n            try:\r\n                format % 1.1\r\n            except:\r\n                msg = _(\"Format ({}) is incorrect\").format(format)\r\n                QMessageBox.critical(self, _(\"Error\"), msg)\r\n                return\r\n            if not format.startswith('%'):\r\n                msg = _(\"Format ({}) should start with '%'\").format(format)\r\n                QMessageBox.critical(self, _(\"Error\"), msg)\r\n                return\r\n            self.dataModel.set_format(format)\r\n            self.sig_option_changed.emit('dataframe_format', format)", "output": "Ask user for display format for floats and use it.\r\n\r\n        This function also checks whether the format is valid and emits\r\n        `sig_option_changed`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extract(self, path, extract_method, to_path):\n    \"\"\"\"\"\"\n    self._pbar_path.update_total(1)\n    if extract_method not in _EXTRACT_METHODS:\n      raise ValueError('Unknown extraction method \"%s\".' % extract_method)\n    future = self._executor.submit(self._sync_extract,\n                                   path, extract_method, to_path)\n    return promise.Promise.resolve(future)", "output": "Returns `promise.Promise` => to_path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __float(value):\n    ''''''\n    valid, _value = False, value\n    try:\n        _value = float(value)\n        valid = True\n    except ValueError:\n        pass\n    return (valid, _value, 'float')", "output": "validate a float", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_params(self, deep=True):\n        \"\"\"\n        \"\"\"\n        params = super(LGBMModel, self).get_params(deep=deep)\n        params.update(self._other_params)\n        return params", "output": "Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, optional (default=True)\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : dict\n            Parameter names mapped to their values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_exports(exports, edict):\n    '''\n    \n    '''\n    with salt.utils.files.fopen(exports, 'w') as efh:\n        for export in edict:\n            line = salt.utils.stringutils.to_str(export)\n            for perms in edict[export]:\n                hosts = perms['hosts']\n                options = ','.join(perms['options'])\n                line += ' {0}({1})'.format(hosts, options)\n            efh.write('{0}\\n'.format(line))", "output": "Write an exports file to disk\n\n    If multiple shares were initially configured per line, like:\n\n        /media/storage /media/data *(ro,sync,no_subtree_check)\n\n    ...then they will be saved to disk with only one share per line:\n\n        /media/storage *(ro,sync,no_subtree_check)\n        /media/data *(ro,sync,no_subtree_check)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_route_table_association(association_id, route_table_id, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        association_id = conn.replace_route_table_association_with_assoc(association_id, route_table_id)\n        log.info('Route table %s was reassociated with association id %s',\n                 route_table_id, association_id)\n        return {'replaced': True, 'association_id': association_id}\n    except BotoServerError as e:\n        return {'replaced': False, 'error': __utils__['boto.get_error'](e)}", "output": "Replaces a route table association.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_vpc.replace_route_table_association 'rtbassoc-d8ccddba' 'rtb-1f382e7d'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wider_next_conv(layer, start_dim, total_dim, n_add, weighted=True):\n    '''\n    '''\n    n_dim = get_n_dim(layer)\n    if not weighted:\n        return get_conv_class(n_dim)(layer.input_channel + n_add,\n                                     layer.filters,\n                                     kernel_size=layer.kernel_size,\n                                     stride=layer.stride)\n    n_filters = layer.filters\n    teacher_w, teacher_b = layer.get_weights()\n\n    new_weight_shape = list(teacher_w.shape)\n    new_weight_shape[1] = n_add\n    new_weight = np.zeros(tuple(new_weight_shape))\n\n    student_w = np.concatenate((teacher_w[:, :start_dim, ...].copy(),\n                                add_noise(new_weight, teacher_w),\n                                teacher_w[:, start_dim:total_dim, ...].copy()), axis=1)\n    new_layer = get_conv_class(n_dim)(layer.input_channel + n_add,\n                                      n_filters,\n                                      kernel_size=layer.kernel_size,\n                                      stride=layer.stride)\n    new_layer.set_weights((student_w, teacher_b))\n    return new_layer", "output": "wider next conv layer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpointerNewContext(self, here, origin):\n        \"\"\" \"\"\"\n        if here is None: here__o = None\n        else: here__o = here._o\n        if origin is None: origin__o = None\n        else: origin__o = origin._o\n        ret = libxml2mod.xmlXPtrNewContext(self._o, here__o, origin__o)\n        if ret is None:raise treeError('xmlXPtrNewContext() failed')\n        __tmp = xpathContext(_obj=ret)\n        return __tmp", "output": "Create a new XPointer context", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_table(self):\r\n        \"\"\"\"\"\"\r\n        self.horizontalHeader().setStretchLastSection(True)\r\n        self.adjust_columns()\r\n        self.columnAt(0)\r\n        # Sorting columns\r\n        self.setSortingEnabled(False)\r\n        self.sortByColumn(0, Qt.DescendingOrder)", "output": "Setup table", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cmd_sync(self, low):\n        '''\n        \n        '''\n\n        kwargs = copy.deepcopy(low)\n\n        for ignore in ['tgt', 'fun', 'arg', 'timeout', 'tgt_type', 'kwarg']:\n            if ignore in kwargs:\n                del kwargs[ignore]\n\n        return self.cmd(low['tgt'],\n                        low['fun'],\n                        low.get('arg', []),\n                        low.get('timeout'),\n                        low.get('tgt_type'),\n                        low.get('kwarg'),\n                        **kwargs)", "output": "Execute a salt-ssh call synchronously.\n\n        .. versionadded:: 2015.5.0\n\n        WARNING: Eauth is **NOT** respected\n\n        .. code-block:: python\n\n            client.cmd_sync({\n                'tgt': 'silver',\n                'fun': 'test.ping',\n                'arg': (),\n                'tgt_type'='glob',\n                'kwarg'={}\n                })\n            {'silver': {'fun_args': [], 'jid': '20141202152721523072', 'return': True, 'retcode': 0, 'success': True, 'fun': 'test.ping', 'id': 'silver'}}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _serve_experiments(self, request):\n    \"\"\"\n    \"\"\"\n    results = self.list_experiments_impl()\n    return http_util.Respond(request, results, 'application/json')", "output": "Serve a JSON array of experiments. Experiments are ordered by experiment\n    started time (aka first event time) with empty times sorted last, and then\n    ties are broken by sorting on the experiment name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def server_enabled(s_name, **connection_args):\n    '''\n    \n    '''\n    server = _server_get(s_name, **connection_args)\n    return server is not None and server.get_state() == 'ENABLED'", "output": "Check if a server is enabled globally\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' netscaler.server_enabled 'serverName'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepend(exception, message, end=': '):\n    \"\"\"\n    \"\"\"\n    exception.args = exception.args or ('',)\n    exception.args = (message + end + exception.args[0], ) + exception.args[1:]\n    return exception", "output": "Prepends the first argument (i.e., the exception message) of the a BaseException with the provided message.\n    Useful for reraising exceptions with additional information.\n\n    :param BaseException exception: the exception to prepend\n    :param str message: the message to prepend\n    :param str end: the separator to add to the end of the provided message\n    :returns: the exception", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mpool(self,\n              k_height,\n              k_width,\n              d_height=2,\n              d_width=2,\n              mode=\"VALID\",\n              input_layer=None,\n              num_channels_in=None):\n        \"\"\"\"\"\"\n        return self._pool(\"mpool\", pooling_layers.max_pooling2d, k_height,\n                          k_width, d_height, d_width, mode, input_layer,\n                          num_channels_in)", "output": "Construct a max pooling layer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install(app_id, enable=True):\n    '''\n    \n    '''\n    ge_el_capitan = True if _LooseVersion(__grains__['osrelease']) >= salt.utils.stringutils.to_str('10.11') else False\n    client_type = _client_type(app_id)\n    enable_str = '1' if enable else '0'\n    cmd = 'sqlite3 \"/Library/Application Support/com.apple.TCC/TCC.db\" ' \\\n          '\"INSERT or REPLACE INTO access VALUES(\\'kTCCServiceAccessibility\\',\\'{0}\\',{1},{2},1,NULL{3})\"'.\\\n        format(app_id, client_type, enable_str, ',NULL' if ge_el_capitan else '')\n\n    call = __salt__['cmd.run_all'](\n        cmd,\n        output_loglevel='debug',\n        python_shell=False\n    )\n    if call['retcode'] != 0:\n        comment = ''\n        if 'stderr' in call:\n            comment += call['stderr']\n        if 'stdout' in call:\n            comment += call['stdout']\n\n        raise CommandExecutionError('Error installing app: {0}'.format(comment))\n\n    return True", "output": "Install a bundle ID or command as being allowed to use\n    assistive access.\n\n    app_id\n        The bundle ID or command to install for assistive access.\n\n    enabled\n        Sets enabled or disabled status. Default is ``True``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' assistive.install /usr/bin/osascript\n        salt '*' assistive.install com.smileonmymac.textexpander", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def url_escape(value: Union[str, bytes], plus: bool = True) -> str:\n    \"\"\"\n    \"\"\"\n    quote = urllib.parse.quote_plus if plus else urllib.parse.quote\n    return quote(utf8(value))", "output": "Returns a URL-encoded version of the given value.\n\n    If ``plus`` is true (the default), spaces will be represented\n    as \"+\" instead of \"%20\".  This is appropriate for query strings\n    but not for the path component of a URL.  Note that this default\n    is the reverse of Python's urllib module.\n\n    .. versionadded:: 3.1\n        The ``plus`` argument", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_namespaced_cron_job(self, name, namespace, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_cron_job_with_http_info(name, namespace, **kwargs)\n        else:\n            (data) = self.read_namespaced_cron_job_with_http_info(name, namespace, **kwargs)\n            return data", "output": "read the specified CronJob\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_cron_job(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the CronJob (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'. Deprecated. Planned for removal in 1.18.\n        :param bool export: Should this value be exported.  Export strips fields that a user can not specify. Deprecated. Planned for removal in 1.18.\n        :return: V2alpha1CronJob\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validateElement(self, doc, elem):\n        \"\"\" \"\"\"\n        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        ret = libxml2mod.xmlValidateElement(self._o, doc__o, elem__o)\n        return ret", "output": "Try to validate the subtree under an element", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compare_checkpoints(self, attr_mean):\n        \"\"\"\n        \"\"\"\n        if self._cmp_greater and attr_mean > self.best_checkpoint_attr_value:\n            return True\n        elif (not self._cmp_greater\n              and attr_mean < self.best_checkpoint_attr_value):\n            return True\n        return False", "output": "Compares two checkpoints based on the attribute attr_mean param.\n        Greater than is used by default. If  command-line parameter\n        checkpoint_score_attr starts with \"min-\" less than is used.\n\n        Arguments:\n            attr_mean: mean of attribute value for the current checkpoint\n\n        Returns:\n            True: when attr_mean is greater than previous checkpoint attr_mean\n                  and greater than function is selected\n                  when attr_mean is less than previous checkpoint attr_mean and\n                  less than function is selected\n            False: when attr_mean is not in alignment with selected cmp fn", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def item_selection_changed(self):\r\n        \"\"\"\"\"\"\r\n        is_selection = len(self.selectedItems()) > 0\r\n        self.expand_selection_action.setEnabled(is_selection)\r\n        self.collapse_selection_action.setEnabled(is_selection)", "output": "Item selection has changed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_periodic_callback(self, callback, period_milliseconds):\n        ''' \n\n        '''\n        from ..server.callbacks import PeriodicCallback\n        cb = PeriodicCallback(self,\n                              None,\n                              period_milliseconds)\n        return self._add_session_callback(cb, callback, one_shot=False, originator=self.add_periodic_callback)", "output": "Add a callback to be invoked on a session periodically.\n\n        Args:\n            callback (callable) :\n                A callback function to execute periodically\n\n            period_milliseconds (int) :\n                Number of milliseconds between each callback execution.\n\n        Returns:\n            PeriodicCallback : can be used with ``remove_periodic_callback``\n\n        .. note::\n            Periodic callbacks only work within the context of a Bokeh server\n            session. This function will no effect when Bokeh outputs to\n            standalone HTML or Jupyter notebook cells.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ConvertWrapperMessage(self, value, message):\n    \"\"\"\"\"\"\n    field = message.DESCRIPTOR.fields_by_name['value']\n    setattr(message, 'value', _ConvertScalarFieldValue(value, field))", "output": "Convert a JSON representation into Wrapper message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_ipv4(value):\n    '''\n    \n    '''\n    if len(value) == 3:\n        if not salt.utils.validate.net.ipv4_addr(value[0].strip()):\n            return False, 'Invalid ip address: {0} for ipv4 option'.format(value[0])\n        if not salt.utils.validate.net.netmask(value[1].strip()):\n            return False, 'Invalid netmask: {0} for ipv4 option'.format(value[1])\n        if not salt.utils.validate.net.ipv4_addr(value[2].strip()):\n            return False, 'Invalid gateway: {0} for ipv4 option'.format(value[2])\n    else:\n        return False, 'Invalid value: {0} for ipv4 option'.format(value)\n    return True, ''", "output": "validate ipv4 values", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_hash(cls, attrs):\n    \"\"\"\n    \n    \"\"\"\n    cls.__hash__ = _make_hash(attrs, frozen=False, cache_hash=False)\n    return cls", "output": "Add a hash method to *cls*.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def adjust_properties (self, prop_set):\n        \"\"\" \n        \"\"\"\n        assert isinstance(prop_set, property_set.PropertySet)\n        s = self.targets () [0].creating_subvariant ()\n\n        return prop_set.add_raw (s.implicit_includes ('include', 'H'))", "output": "For all virtual targets for the same dependency graph as self,\n            i.e. which belong to the same main target, add their directories\n            to include path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def equals(self, rhs):\n    \"\"\"\n    \"\"\"\n\n    for comparator in self._comparators:\n      if comparator.equals(rhs):\n        return True\n\n    return False", "output": "Checks whether any Comparator is equal to rhs.\n\n    Args:\n      # rhs: can be anything\n\n    Returns:\n      bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sort(self, search):\n        \"\"\"\n        \n        \"\"\"\n        if self._sort:\n            search = search.sort(*self._sort)\n        return search", "output": "Add sorting information to the request.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def token(self):\n        '''\n        \n        '''\n        # find the token (cookie or headers)\n        if AUTH_TOKEN_HEADER in self.request.headers:\n            return self.request.headers[AUTH_TOKEN_HEADER]\n        else:\n            return self.get_cookie(AUTH_COOKIE_NAME)", "output": "The token used for the request", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy(self, deep=True):\n        \"\"\"\n        \n        \"\"\"\n        # TODO: https://github.com/pandas-dev/pandas/issues/22314\n        # We skip the block manager till that is resolved.\n        new_data = self.values.copy(deep=deep)\n        return self._constructor(new_data, sparse_index=self.sp_index,\n                                 fill_value=self.fill_value,\n                                 index=self.index.copy(),\n                                 name=self.name).__finalize__(self)", "output": "Make a copy of the SparseSeries. Only the actual sparse values need to\n        be copied", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_expand_dims(node, **kwargs):\n    \"\"\"\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    axis = int(attrs.get(\"axis\"))\n\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        input_nodes,\n        [name],\n        axes=[axis],\n        name=name,\n    )\n    return [node]", "output": "Map MXNet's expand_dims operator attributes to onnx's Unsqueeze operator\n    and return the created node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def report_error_event(self, error_report):\n        \"\"\"\n        \"\"\"\n        project_name = self._gapic_api.project_path(self._project)\n        error_report_payload = report_errors_service_pb2.ReportedErrorEvent()\n        ParseDict(error_report, error_report_payload)\n        self._gapic_api.report_error_event(project_name, error_report_payload)", "output": "Uses the gapic client to report the error.\n\n        :type error_report: dict\n        :param error_report:\n            payload of the error report formatted according to\n            https://cloud.google.com/error-reporting/docs/formatting-error-messages\n            This object should be built using\n            Use\n            :meth:~`google.cloud.error_reporting.client._build_error_report`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_mapper(self, stdin=sys.stdin, stdout=sys.stdout):\n        \"\"\"\n        \n        \"\"\"\n        self.init_hadoop()\n        self.init_mapper()\n        outputs = self._map_input((line[:-1] for line in stdin))\n        if self.reducer == NotImplemented:\n            self.writer(outputs, stdout)\n        else:\n            self.internal_writer(outputs, stdout)", "output": "Run the mapper on the hadoop node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def import_submodules(package_name: str) -> None:\n    \"\"\"\n    \n    \"\"\"\n    importlib.invalidate_caches()\n\n    # For some reason, python doesn't always add this by default to your path, but you pretty much\n    # always want it when using `--include-package`.  And if it's already there, adding it again at\n    # the end won't hurt anything.\n    sys.path.append('.')\n\n    # Import at top level\n    module = importlib.import_module(package_name)\n    path = getattr(module, '__path__', [])\n    path_string = '' if not path else path[0]\n\n    # walk_packages only finds immediate children, so need to recurse.\n    for module_finder, name, _ in pkgutil.walk_packages(path):\n        # Sometimes when you import third-party libraries that are on your path,\n        # `pkgutil.walk_packages` returns those too, so we need to skip them.\n        if path_string and module_finder.path != path_string:\n            continue\n        subpackage = f\"{package_name}.{name}\"\n        import_submodules(subpackage)", "output": "Import all submodules under the given package.\n    Primarily useful so that people using AllenNLP as a library\n    can specify their own custom packages and have their custom\n    classes get loaded and registered.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def focusInEvent(self, event):\n        \"\"\"\"\"\"\n        self.focus_changed.emit()\n        return super(ControlWidget, self).focusInEvent(event)", "output": "Reimplement Qt method to send focus change notification", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def collect(self):\n        \"\"\"\n        \"\"\"\n        with SCCallSiteSync(self._sc) as css:\n            sock_info = self._jdf.collectToPython()\n        return list(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))", "output": "Returns all the records as a list of :class:`Row`.\n\n        >>> df.collect()\n        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gather_bootstrap_script(bootstrap=None):\n    '''\n    \n    '''\n    if not HAS_CLOUD:\n        return False, 'config.gather_bootstrap_script is unavailable'\n    ret = salt.utils.cloud.update_bootstrap(__opts__, url=bootstrap)\n    if 'Success' in ret and ret['Success']['Files updated']:\n        return ret['Success']['Files updated'][0]", "output": "Download the salt-bootstrap script, and return its location\n\n    bootstrap\n        URL of alternate bootstrap script\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' config.gather_bootstrap_script", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_session_groups(self):\n    \"\"\"\"\"\"\n\n    # Algorithm: We keep a dict 'groups_by_name' mapping a SessionGroup name\n    # (str) to a SessionGroup protobuffer. We traverse the runs associated with\n    # the plugin--each representing a single session. We form a Session\n    # protobuffer from each run and add it to the relevant SessionGroup object\n    # in the 'groups_by_name' dict. We create the SessionGroup object, if this\n    # is the first session of that group we encounter.\n    groups_by_name = {}\n    run_to_tag_to_content = self._context.multiplexer.PluginRunToTagToContent(\n        metadata.PLUGIN_NAME)\n    for (run, tag_to_content) in six.iteritems(run_to_tag_to_content):\n      if metadata.SESSION_START_INFO_TAG not in tag_to_content:\n        continue\n      start_info = metadata.parse_session_start_info_plugin_data(\n          tag_to_content[metadata.SESSION_START_INFO_TAG])\n      end_info = None\n      if metadata.SESSION_END_INFO_TAG in tag_to_content:\n        end_info = metadata.parse_session_end_info_plugin_data(\n            tag_to_content[metadata.SESSION_END_INFO_TAG])\n      session = self._build_session(run, start_info, end_info)\n      if session.status in self._request.allowed_statuses:\n        self._add_session(session, start_info, groups_by_name)\n\n    # Compute the session group's aggregated metrics for each group.\n    groups = groups_by_name.values()\n    for group in groups:\n      # We sort the sessions in a group so that the order is deterministic.\n      group.sessions.sort(key=operator.attrgetter('name'))\n      self._aggregate_metrics(group)\n    return groups", "output": "Returns a list of SessionGroups protobuffers from the summary data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def utils(opts, whitelist=None, context=None, proxy=proxy):\n    '''\n    \n    '''\n    return LazyLoader(\n        _module_dirs(opts, 'utils', ext_type_dirs='utils_dirs'),\n        opts,\n        tag='utils',\n        whitelist=whitelist,\n        pack={'__context__': context, '__proxy__': proxy or {}},\n    )", "output": "Returns the utility modules", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_focus_widget(self):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        self.pydocbrowser.url_combo.lineEdit().selectAll()\r\n        return self.pydocbrowser.url_combo", "output": "Return the widget to give focus to when\r\n        this plugin's dockwidget is raised on top-level", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_input_grads(self, merge_multi_context=True):\n        \"\"\"\n        \"\"\"\n        assert self.binded and self.params_initialized and self.inputs_need_grad\n        return self._exec_group.get_input_grads(merge_multi_context=merge_multi_context)", "output": "Gets the gradients with respect to the inputs of the module.\n\n        If ``merge_multi_context`` is ``True``, it is like ``[grad1, grad2]``. Otherwise, it\n        is like ``[[grad1_dev1, grad1_dev2], [grad2_dev1, grad2_dev2]]``. All the output\n        elements are `NDArray`.\n\n        Parameters\n        ----------\n        merge_multi_context : bool\n            Default is ``True``. In the case when data-parallelism is used, the outputs\n            will be collected from multiple devices. A ``True`` value indicate that we\n            should merge the collected results so that they look like from a single\n            executor.\n\n        Returns\n        -------\n        list of NDArray or list of list of NDArray\n              Input gradients", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_metadata(self, handler):\n        \"\"\"  \"\"\"\n        if self.meta == 'category':\n            new_metadata = self.metadata\n            cur_metadata = handler.read_metadata(self.cname)\n            if (new_metadata is not None and cur_metadata is not None and\n                    not array_equivalent(new_metadata, cur_metadata)):\n                raise ValueError(\"cannot append a categorical with \"\n                                 \"different categories to the existing\")", "output": "validate that kind=category does not change the categories", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reduce_to_2d(arr):\n  \"\"\"\n  \"\"\"\n  if not isinstance(arr, np.ndarray):\n    raise ValueError('reduce_to_2d requires a numpy.ndarray')\n\n  ndims = len(arr.shape)\n  if ndims < 2:\n    raise ValueError('reduce_to_2d requires an array of dimensionality >=2')\n  # slice(None) is equivalent to `:`, so we take arr[0,0,...0,:,:]\n  slices = ([0] * (ndims - 2)) + [slice(None), slice(None)]\n  return arr[slices]", "output": "Given a np.npdarray with nDims > 2, reduce it to 2d.\n\n  It does this by selecting the zeroth coordinate for every dimension greater\n  than two.\n\n  Args:\n    arr: a numpy ndarray of dimension at least 2.\n\n  Returns:\n    A two-dimensional subarray from the input array.\n\n  Raises:\n    ValueError: If the argument is not a numpy ndarray, or the dimensionality\n      is too low.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def op(scalars_layout, collections=None):\n  \"\"\"\n  \"\"\"\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n\n  assert isinstance(scalars_layout, layout_pb2.Layout)\n  summary_metadata = metadata.create_summary_metadata()\n  return tf.summary.tensor_summary(name=metadata.CONFIG_SUMMARY_TAG,\n                                   tensor=tf.constant(\n                                       scalars_layout.SerializeToString(),\n                                       dtype=tf.string),\n                                   collections=collections,\n                                   summary_metadata=summary_metadata)", "output": "Creates a summary that contains a layout.\n\n  When users navigate to the custom scalars dashboard, they will see a layout\n  based on the proto provided to this function.\n\n  Args:\n    scalars_layout: The scalars_layout_pb2.Layout proto that specifies the\n        layout.\n    collections: Optional list of graph collections keys. The new\n        summary op is added to these collections. Defaults to\n        `[Graph Keys.SUMMARIES]`.\n\n  Returns:\n    A tensor summary op that writes the layout to disk.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def argspec_report(functions, module=''):\n    '''\n    \n    '''\n    ret = {}\n    if '*' in module or '.' in module:\n        for fun in fnmatch.filter(functions, module):\n            try:\n                aspec = get_function_argspec(functions[fun])\n            except TypeError:\n                # this happens if not callable\n                continue\n\n            args, varargs, kwargs, defaults = aspec\n\n            ret[fun] = {}\n            ret[fun]['args'] = args if args else None\n            ret[fun]['defaults'] = defaults if defaults else None\n            ret[fun]['varargs'] = True if varargs else None\n            ret[fun]['kwargs'] = True if kwargs else None\n\n    else:\n        # \"sys\" should just match sys without also matching sysctl\n        module_dot = module + '.'\n\n        for fun in functions:\n            if fun.startswith(module_dot):\n                try:\n                    aspec = get_function_argspec(functions[fun])\n                except TypeError:\n                    # this happens if not callable\n                    continue\n\n                args, varargs, kwargs, defaults = aspec\n\n                ret[fun] = {}\n                ret[fun]['args'] = args if args else None\n                ret[fun]['defaults'] = defaults if defaults else None\n                ret[fun]['varargs'] = True if varargs else None\n                ret[fun]['kwargs'] = True if kwargs else None\n\n    return ret", "output": "Pass in a functions dict as it is returned from the loader and return the\n    argspec function signatures", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_parameter(name, parameter, path=None):\n    '''\n    \n    '''\n    _ensure_exists(name, path=path)\n    cmd = 'lxc-cgroup'\n    if path:\n        cmd += ' -P {0}'.format(pipes.quote(path))\n    cmd += ' -n {0} {1}'.format(name, parameter)\n    ret = __salt__['cmd.run_all'](cmd, python_shell=False)\n    if ret['retcode'] != 0:\n        raise CommandExecutionError(\n            'Unable to retrieve value for \\'{0}\\''.format(parameter)\n        )\n    return ret['stdout'].strip()", "output": "Returns the value of a cgroup parameter for a container\n\n    path\n        path to the container parent directory\n        default: /var/lib/lxc (system)\n\n        .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' lxc.get_parameter container_name memory.limit_in_bytes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_request_range(\n    range_header: str\n) -> Optional[Tuple[Optional[int], Optional[int]]]:\n    \"\"\"\n    \"\"\"\n    unit, _, value = range_header.partition(\"=\")\n    unit, value = unit.strip(), value.strip()\n    if unit != \"bytes\":\n        return None\n    start_b, _, end_b = value.partition(\"-\")\n    try:\n        start = _int_or_none(start_b)\n        end = _int_or_none(end_b)\n    except ValueError:\n        return None\n    if end is not None:\n        if start is None:\n            if end != 0:\n                start = -end\n                end = None\n        else:\n            end += 1\n    return (start, end)", "output": "Parses a Range header.\n\n    Returns either ``None`` or tuple ``(start, end)``.\n    Note that while the HTTP headers use inclusive byte positions,\n    this method returns indexes suitable for use in slices.\n\n    >>> start, end = _parse_request_range(\"bytes=1-2\")\n    >>> start, end\n    (1, 3)\n    >>> [0, 1, 2, 3, 4][start:end]\n    [1, 2]\n    >>> _parse_request_range(\"bytes=6-\")\n    (6, None)\n    >>> _parse_request_range(\"bytes=-6\")\n    (-6, None)\n    >>> _parse_request_range(\"bytes=-0\")\n    (None, 0)\n    >>> _parse_request_range(\"bytes=\")\n    (None, None)\n    >>> _parse_request_range(\"foo=42\")\n    >>> _parse_request_range(\"bytes=1-2,6-10\")\n\n    Note: only supports one range (ex, ``bytes=1-2,6-10`` is not allowed).\n\n    See [0] for the details of the range header.\n\n    [0]: http://greenbytes.de/tech/webdav/draft-ietf-httpbis-p5-range-latest.html#byte.ranges", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cli(cls, opts):\n        \"\"\"\n        \"\"\"\n        if opts.background:\n            logging.getLogger().setLevel(logging.INFO)\n            return True\n\n        if opts.logdir:\n            logging.basicConfig(\n                level=logging.INFO,\n                format=cls._log_format,\n                filename=os.path.join(opts.logdir, \"luigi-server.log\"))\n            return True\n\n        return False", "output": "Setup logging via CLI options\n\n        If `--background` -- set INFO level for root logger.\n        If `--logdir` -- set logging with next params:\n            default Luigi's formatter,\n            INFO level,\n            output in logdir in `luigi-server.log` file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_num_special_tokens(self, num_special_tokens):\n        \"  \"\n        if self.config.n_special == num_special_tokens:\n            return\n        # Update config\n        self.config.n_special = num_special_tokens\n        # Build new embeddings and initialize all new embeddings (in particular the special tokens)\n        old_embed = self.tokens_embed\n        self.tokens_embed = nn.Embedding(self.config.total_tokens_embeddings, self.config.n_embd)\n        self.tokens_embed.to(old_embed.weight.device)\n        self.init_weights(self.tokens_embed)\n        # Copy word embeddings from the previous weights\n        self.tokens_embed.weight.data[:self.config.vocab_size, :] = old_embed.weight.data[:self.config.vocab_size, :]", "output": "Update input embeddings with new embedding matrice if needed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_model(model_name, dataset_name, model_func):\n    \"\"\"\"\"\"\n    model_map = _get_model_map(dataset_name)\n    if model_name in model_map:\n        raise ValueError(\"Model \\\"%s\\\" is already registered for dataset\"\n                         \"\\\"%s\\\"\" % (model_name, dataset_name))\n    model_map[model_name] = model_func", "output": "Register a new model that can be obtained with `get_model_config`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attrs(self) -> _Attrs:\n        \"\"\"\n        \"\"\"\n        if self._attrs is None:\n            self._attrs = {k: v for k, v in self.element.items()}\n\n            # Split class and rel up, as there are ussually many of them:\n            for attr in ['class', 'rel']:\n                if attr in self._attrs:\n                    self._attrs[attr] = tuple(self._attrs[attr].split())\n\n        return self._attrs", "output": "Returns a dictionary of the attributes of the :class:`Element <Element>`\n        (`learn more <https://www.w3schools.com/tags/ref_attributes.asp>`_).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_capability(capability,\n                   source=None,\n                   limit_access=False,\n                   image=None,\n                   restart=False):\n    '''\n    \n    '''\n    if salt.utils.versions.version_cmp(__grains__['osversion'], '10') == -1:\n        raise NotImplementedError(\n            '`install_capability` is not available on this version of Windows: '\n            '{0}'.format(__grains__['osversion']))\n\n    cmd = ['DISM',\n           '/Quiet',\n           '/Image:{0}'.format(image) if image else '/Online',\n           '/Add-Capability',\n           '/CapabilityName:{0}'.format(capability)]\n\n    if source:\n        cmd.append('/Source:{0}'.format(source))\n    if limit_access:\n        cmd.append('/LimitAccess')\n    if not restart:\n        cmd.append('/NoRestart')\n\n    return __salt__['cmd.run_all'](cmd)", "output": "Install a capability\n\n    Args:\n        capability (str): The capability to install\n        source (Optional[str]): The optional source of the capability. Default\n            is set by group policy and can be Windows Update.\n        limit_access (Optional[bool]): Prevent DISM from contacting Windows\n            Update for the source package\n        image (Optional[str]): The path to the root directory of an offline\n            Windows image. If `None` is passed, the running operating system is\n            targeted. Default is None.\n        restart (Optional[bool]): Reboot the machine if required by the install\n\n    Raises:\n        NotImplementedError: For all versions of Windows that are not Windows 10\n        and later. Server editions of Windows use ServerManager instead.\n\n    Returns:\n        dict: A dictionary containing the results of the command\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' dism.add_capability Tools.Graphics.DirectX~~~~0.0.1.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(key, value=None, conf_file=_DEFAULT_CONF):\n    '''\n    \n    '''\n    current_conf = _parse_conf(conf_file)\n    stanza = current_conf.get(key, False)\n\n    if value:\n        if stanza:\n            return stanza.get(value, False)\n        _LOG.warning(\"Block '%s' not present or empty.\", key)\n    return stanza", "output": "Get the value for a specific configuration line.\n\n    :param str key: The command or stanza block to configure.\n    :param str value: The command value or command of the block specified by the key parameter.\n    :param str conf_file: The logrotate configuration file.\n\n    :return: The value for a specific configuration line.\n    :rtype: bool|int|str\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' logrotate.get rotate\n\n        salt '*' logrotate.get /var/log/wtmp rotate /etc/logrotate.conf", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def adjust_cells(self):\r\n        \"\"\"\"\"\"\r\n        self.resizeColumnsToContents()\r\n        fm = self.horizontalHeader().fontMetrics()\r\n        names = [fm.width(s.name + ' '*9) for s in self.source_model.shortcuts]\r\n        self.setColumnWidth(NAME, max(names))\r\n        self.horizontalHeader().setStretchLastSection(True)", "output": "Adjust column size based on contents.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def FindEnumTypeByName(self, full_name):\n    \"\"\"\n    \"\"\"\n\n    full_name = _NormalizeFullyQualifiedName(full_name)\n    if full_name not in self._enum_descriptors:\n      self._FindFileContainingSymbolInDb(full_name)\n    return self._enum_descriptors[full_name]", "output": "Loads the named enum descriptor from the pool.\n\n    Args:\n      full_name: The full name of the enum descriptor to load.\n\n    Returns:\n      The enum descriptor for the named type.\n\n    Raises:\n      KeyError: if the enum cannot be found in the pool.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_line_text(self, line_nbr, new_text):\n        \"\"\"\n        \n\n        \"\"\"\n        editor = self._editor\n        text_cursor = self._move_cursor_to(line_nbr)\n        text_cursor.select(text_cursor.LineUnderCursor)\n        text_cursor.insertText(new_text)\n        editor.setTextCursor(text_cursor)", "output": "Replace an entire line with ``new_text``.\n\n        :param line_nbr: line number of the line to change.\n        :param new_text: The replacement text.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_auto_dict(values, source='auto'):\n    '''\n    \n\n    '''\n    for name, value in values.items():\n        values[name] = from_auto(name, value, source)\n\n    return values", "output": "Pass an entire dictionary to from_auto\n\n    .. note::\n        The key will be passed as the name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def asset_security_marks_path(cls, organization, asset):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"organizations/{organization}/assets/{asset}/securityMarks\",\n            organization=organization,\n            asset=asset,\n        )", "output": "Return a fully-qualified asset_security_marks string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_data_disk_size(vm_, swap, linode_id):\n    '''\n    \n    '''\n    disk_size = get_linode(kwargs={'linode_id': linode_id})['TOTALHD']\n    root_disk_size = config.get_cloud_config_value(\n        'disk_size', vm_, __opts__, default=disk_size - swap\n    )\n    return disk_size - root_disk_size - swap", "output": "Return the size of of the data disk in MB\n\n    .. versionadded:: 2016.3.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_cors_allowed_methods_for_path(self, path):\n        \"\"\"\n        \n        \"\"\"\n\n        # https://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html\n        all_http_methods = [\"OPTIONS\", \"GET\", \"HEAD\", \"POST\", \"PUT\", \"DELETE\", \"PATCH\"]\n\n        if not self.has_path(path):\n            return \"\"\n\n        # At this point, value of Swagger path should be a dictionary with method names being the keys\n        methods = list(self.get_path(path).keys())\n\n        if self._X_ANY_METHOD in methods:\n            # API Gateway's ANY method is not a real HTTP method but a wildcard representing all HTTP methods\n            allow_methods = all_http_methods\n        else:\n            allow_methods = methods\n            allow_methods.append(\"options\")  # Always add Options to the CORS methods response\n\n        # Clean up the result:\n        #\n        # - HTTP Methods **must** be upper case and they are case sensitive.\n        #   (https://tools.ietf.org/html/rfc7231#section-4.1)\n        # - Convert to set to remove any duplicates\n        # - Sort to keep this list stable because it could be constructed from dictionary keys which are *not* ordered.\n        #   Therefore we might get back a different list each time the code runs. To prevent any unnecessary\n        #   regression, we sort the list so the returned value is stable.\n        allow_methods = list({m.upper() for m in allow_methods})\n        allow_methods.sort()\n\n        # Allow-Methods is comma separated string\n        return ','.join(allow_methods)", "output": "Creates the value for Access-Control-Allow-Methods header for given path. All HTTP methods defined for this\n        path will be included in the result. If the path contains \"ANY\" method, then *all available* HTTP methods will\n        be returned as result.\n\n        :param string path: Path to generate AllowMethods value for\n        :return string: String containing the value of AllowMethods, if the path contains any methods.\n                        Empty string, otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text):\n    \"\"\"\"\"\"\n\n    # The SQuAD annotations are character based. We first project them to\n    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n    # often find a \"better match\". For example:\n    #\n    #   Question: What year was John Smith born?\n    #   Context: The leader was John Smith (1895-1943).\n    #   Answer: 1895\n    #\n    # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n    # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n    # the exact answer, 1895.\n    #\n    # However, this is not always possible. Consider the following:\n    #\n    #   Question: What country is the top exporter of electornics?\n    #   Context: The Japanese electronics industry is the lagest in the world.\n    #   Answer: Japan\n    #\n    # In this case, the annotator chose \"Japan\" as a character sub-span of\n    # the word \"Japanese\". Since our WordPiece tokenizer does not split\n    # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n    # in SQuAD, but does happen.\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)", "output": "Returns tokenized answer spans that better match the annotated answer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_params(self, **params):\n        \"\"\"\n        \"\"\"\n        if not params:\n            # Simple optimization to gain speed (inspect is slow)\n            return self\n\n        for key, value in params.items():\n            if hasattr(self, key):\n                setattr(self, key, value)\n            else:\n                self.kwargs[key] = value\n\n        return self", "output": "Set the parameters of this estimator.\n        Modification of the sklearn method to allow unknown kwargs. This allows using\n        the full range of xgboost parameters that are not defined as member variables\n        in sklearn grid search.\n        Returns\n        -------\n        self", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def query_properties_with_values(self, query, include_defaults=True):\n        ''' \n\n        '''\n        themed_keys = set()\n        result = dict()\n        if include_defaults:\n            keys = self.properties()\n        else:\n            # TODO (bev) For now, include unstable default values. Things rely on Instances\n            # always getting serialized, even defaults, and adding unstable defaults here\n            # accomplishes that. Unmodified defaults for property value containers will be\n            # weeded out below.\n            keys = set(self._property_values.keys()) | set(self._unstable_default_values.keys())\n            if self.themed_values():\n                themed_keys = set(self.themed_values().keys())\n                keys |= themed_keys\n\n        for key in keys:\n            descriptor = self.lookup(key)\n            if not query(descriptor):\n                continue\n\n            value = descriptor.serializable_value(self)\n            if not include_defaults and key not in themed_keys:\n                if isinstance(value, PropertyValueContainer) and key in self._unstable_default_values:\n                    continue\n            result[key] = value\n\n        return result", "output": "Query the properties values of |HasProps| instances with a\n        predicate.\n\n        Args:\n            query (callable) :\n                A callable that accepts property descriptors and returns True\n                or False\n\n            include_defaults (bool, optional) :\n                Whether to include properties that have not been explicitly\n                set by a user (default: True)\n\n        Returns:\n            dict : mapping of property names and values for matching properties", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mtr_lm_dense(sz):\n  \"\"\"\n  \"\"\"\n  n = 2 ** sz\n  hparams = mtf_unitransformer_base()\n  hparams.d_model = 1024\n  hparams.max_length = 1024\n  hparams.batch_size = 128\n  # Parameters for my_layer_stack()\n  hparams.num_hidden_layers = 6\n  hparams.d_ff = 8192 * n\n  hparams.d_kv = 256\n  hparams.num_heads = 8 * n\n  hparams.learning_rate_decay_steps = 65536\n  hparams.layout = \"batch:batch;vocab:model;d_ff:model;heads:model\"\n  hparams.mesh_shape = \"batch:32\"\n  return hparams", "output": "Series of architectures for language modeling.\n\n  We assume infinite training data, so no dropout necessary.\n\n  You can use languagemodel_wiki_noref_v32k_l1k.\n  (1 epoch = ~46000 steps).\n  TODO(noam): find a large enough dataset for these experiments.\n\n  Args:\n    sz: an integer\n\n  Returns:\n    a hparams", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _wipe(dev):\n    '''\n    \n    '''\n    endres = 0\n    dev = _devbase(dev)\n\n    size, block, discard = _sizes(dev)\n\n    if discard is None:\n        log.error('Unable to read SysFS props for %s', dev)\n        return None\n    elif not discard:\n        log.warning('%s seems unable to discard', dev)\n        wiper = 'dd'\n    elif not HAS_BLKDISCARD:\n        log.warning('blkdiscard binary not available, properly wipe the dev manually for optimal results')\n        wiper = 'dd'\n    else:\n        wiper = 'blkdiscard'\n\n    wipe_failmsg = 'Error wiping {0}: %s'.format(dev)\n    if wiper == 'dd':\n        blocks = 4\n        cmd = 'dd if=/dev/zero of=/dev/{0} bs=1M count={1}'.format(dev, blocks)\n        endres += _run_all(cmd, 'warn', wipe_failmsg)\n\n        # Some stuff (<cough>GPT</cough>) writes stuff at the end of a dev as well\n        cmd += ' seek={0}'.format((size/1024**2) - blocks)\n        endres += _run_all(cmd, 'warn', wipe_failmsg)\n\n    elif wiper == 'blkdiscard':\n        cmd = 'blkdiscard /dev/{0}'.format(dev)\n        endres += _run_all(cmd, 'warn', wipe_failmsg)\n        # TODO: fix annoying bug failing blkdiscard by trying to discard 1 sector past blkdev\n        endres = 1\n\n    return endres > 0", "output": "REALLY DESTRUCTIVE STUFF RIGHT AHEAD", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def try_to_create_directory(directory_path):\n    \"\"\"\n    \"\"\"\n    logger = logging.getLogger(\"ray\")\n    directory_path = os.path.expanduser(directory_path)\n    if not os.path.exists(directory_path):\n        try:\n            os.makedirs(directory_path)\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise e\n            logger.warning(\n                \"Attempted to create '{}', but the directory already \"\n                \"exists.\".format(directory_path))\n        # Change the log directory permissions so others can use it. This is\n        # important when multiple people are using the same machine.\n    try:\n        os.chmod(directory_path, 0o0777)\n    except OSError as e:\n        # Silently suppress the PermissionError that is thrown by the chmod.\n        # This is done because the user attempting to change the permissions\n        # on a directory may not own it. The chmod is attempted whether the\n        # directory is new or not to avoid race conditions.\n        # ray-project/ray/#3591\n        if e.errno in [errno.EACCES, errno.EPERM]:\n            pass\n        else:\n            raise", "output": "Attempt to create a directory that is globally readable/writable.\n\n    Args:\n        directory_path: The path of the directory to create.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode_for_output(output, target_stream=None, translation_map=None):\n    \"\"\"\n    \"\"\"\n\n    if not isinstance(output, six.string_types):\n        return output\n    encoding = None\n    if target_stream is not None:\n        encoding = getattr(target_stream, \"encoding\", None)\n    encoding = get_output_encoding(encoding)\n    try:\n        output = _encode(output, encoding=encoding, translation_map=translation_map)\n    except (UnicodeDecodeError, UnicodeEncodeError):\n        output = to_native_string(output)\n        output = _encode(\n            output, encoding=encoding, errors=\"replace\", translation_map=translation_map\n        )\n    return to_text(output, encoding=encoding, errors=\"replace\")", "output": "Given a string, decode it for output to a terminal\n\n    :param str output: A string to print to a terminal\n    :param target_stream: A stream to write to, we will encode to target this stream if possible.\n    :param dict translation_map: A mapping of unicode character ordinals to replacement strings.\n    :return: A re-encoded string using the preferred encoding\n    :rtype: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cumprod(self, axis=0, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        nv.validate_groupby_func('cumprod', args, kwargs,\n                                 ['numeric_only', 'skipna'])\n        if axis != 0:\n            return self.apply(lambda x: x.cumprod(axis=axis, **kwargs))\n\n        return self._cython_transform('cumprod', **kwargs)", "output": "Cumulative product for each group.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def aggregate_gradients_using_copy_with_device_selection(\n        tower_grads, avail_devices, use_mean=True, check_inf_nan=False):\n    \"\"\"\n  \"\"\"\n    agg_grads = []\n    has_nan_or_inf_list = []\n    for i, single_grads in enumerate(zip(*tower_grads)):\n        with tf.device(avail_devices[i % len(avail_devices)]):\n            grad_and_var, has_nan_or_inf = aggregate_single_gradient(\n                single_grads, use_mean, check_inf_nan)\n            agg_grads.append(grad_and_var)\n            has_nan_or_inf_list.append(has_nan_or_inf)\n    return agg_grads", "output": "Aggregate gradients, controlling device for the aggregation.\n\n  Args:\n    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n      is over towers. The inner list is over individual gradients.\n    use_mean: if True, mean is taken, else sum of gradients is taken.\n    check_inf_nan: If true, check grads for nans and infs.\n\n  Returns:\n    The tuple ([(average_gradient, variable),], has_nan_or_inf) where the\n      gradient has been averaged across all towers. The variable is chosen from\n      the first tower. The has_nan_or_inf indicates the grads has nan or inf.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def defragment(path):\n    '''\n    \n    '''\n    is_device = salt.utils.fsutils._is_device(path)\n    mounts = salt.utils.fsutils._get_mounts(\"btrfs\")\n    if is_device and not mounts.get(path):\n        raise CommandExecutionError(\"Device \\\"{0}\\\" is not mounted\".format(path))\n\n    result = []\n    if is_device:\n        for mount_point in mounts[path]:\n            result.append(_defragment_mountpoint(mount_point['mount_point']))\n    else:\n        is_mountpoint = False\n        for mountpoints in six.itervalues(mounts):\n            for mpnt in mountpoints:\n                if path == mpnt['mount_point']:\n                    is_mountpoint = True\n                    break\n        d_res = _defragment_mountpoint(path)\n        if not is_mountpoint and not d_res['passed'] and \"range ioctl not supported\" in d_res['log']:\n            d_res['log'] = \"Range ioctl defragmentation is not supported in this kernel.\"\n\n        if not is_mountpoint:\n            d_res['mount_point'] = False\n            d_res['range'] = os.path.exists(path) and path or False\n\n        result.append(d_res)\n\n    return result", "output": "Defragment mounted BTRFS filesystem.\n    In order to defragment a filesystem, device should be properly mounted and writable.\n\n    If passed a device name, then defragmented whole filesystem, mounted on in.\n    If passed a moun tpoint of the filesystem, then only this mount point is defragmented.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' btrfs.defragment /dev/sda1\n        salt '*' btrfs.defragment /path/on/filesystem", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_training_name(name):\n    \"\"\"\n    \n    \"\"\"\n    # TODO: maybe simply check against TRAINABLE_VARIABLES and MODEL_VARIABLES?\n    # TODO or use get_slot_names()\n    name = get_op_tensor_name(name)[0]\n    if name.endswith('/Adam') or name.endswith('/Adam_1'):\n        return True\n    if name.endswith('/Momentum'):\n        return True\n    if name.endswith('/Adadelta') or name.endswith('/Adadelta_1'):\n        return True\n    if name.endswith('/RMSProp') or name.endswith('/RMSProp_1'):\n        return True\n    if name.endswith('/Adagrad'):\n        return True\n    if name.startswith('EMA/') or '/EMA/' in name:  # all the moving average summaries\n        return True\n    if name.startswith('AccumGrad') or name.endswith('/AccumGrad'):\n        return True\n    if name.startswith('apply_gradients'):\n        return True\n    return False", "output": "**Guess** if this variable is only used in training.\n    Only used internally to avoid too many logging. Do not use it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def isemhash_unbottleneck(x, hidden_size, isemhash_filter_size_multiplier=1.0):\n  \"\"\"\"\"\"\n  filter_size = int(hidden_size * isemhash_filter_size_multiplier)\n  x = 0.5 * (x - 1.0)  # Move from [-1, 1] to [0, 1].\n  with tf.variable_scope(\"isemhash_unbottleneck\"):\n    h1a = tf.layers.dense(x, filter_size, name=\"hidden1a\")\n    h1b = tf.layers.dense(1.0 - x, filter_size, name=\"hidden1b\")\n    h2 = tf.layers.dense(tf.nn.relu(h1a + h1b), filter_size, name=\"hidden2\")\n    return tf.layers.dense(tf.nn.relu(h2), hidden_size, name=\"final\")", "output": "Improved semantic hashing un-bottleneck.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mtf_bitransformer_tiny():\n  \"\"\"\"\"\"\n  hparams = mtf_bitransformer_base()\n  hparams.batch_size = 2\n  hparams.mesh_shape = \"\"\n  hparams.d_model = 128\n  hparams.encoder_layers = [\"self_att\", \"drd\"] * 2\n  hparams.decoder_layers = [\"self_att\", \"enc_att\", \"drd\"] * 2\n  hparams.num_heads = 4\n  hparams.d_ff = 512\n  return hparams", "output": "Small encoder-decoder model for testing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def datasets_create_version_by_id(self, id, dataset_new_version_request, **kwargs):  # noqa: E501\n        \"\"\"\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.datasets_create_version_by_id_with_http_info(id, dataset_new_version_request, **kwargs)  # noqa: E501\n        else:\n            (data) = self.datasets_create_version_by_id_with_http_info(id, dataset_new_version_request, **kwargs)  # noqa: E501\n            return data", "output": "Create a new dataset version by id  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.datasets_create_version_by_id(id, dataset_new_version_request, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param int id: Dataset ID (required)\n        :param DatasetNewVersionRequest dataset_new_version_request: Information for creating a new dataset version (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def source_add(source, source_type='imgapi'):\n    '''\n    \n    '''\n    ret = {}\n\n    # NOTE: there are some undocumented deprecated source types\n    #       so we just warn instead of error on those\n    if source_type not in ['imgapi', 'docker']:\n        log.warning('Possible unsupported imgage source type specified!')\n\n    cmd = 'imgadm sources -a {0} -t {1}'.format(source, source_type)\n    res = __salt__['cmd.run_all'](cmd)\n    retcode = res['retcode']\n    if retcode != 0:\n        ret['Error'] = _exit_status(retcode, res['stderr'])\n        return ret\n\n    return sources(False)", "output": "Add a new source\n\n    source : string\n        source url to add\n    source_trype : string (imgapi)\n        source type, either imgapi or docker\n\n    .. versionadded:: 2019.2.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' imgadm.source_add https://updates.joyent.com\n        salt '*' imgadm.source_add https://docker.io docker", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_shortcuts(self):\r\n        \"\"\"\"\"\"\r\n        toberemoved = []\r\n        for index, (qobject, context, name,\r\n                    add_sc_to_tip) in enumerate(self.shortcut_data):\r\n            keyseq = QKeySequence( get_shortcut(context, name) )\r\n            try:\r\n                if isinstance(qobject, QAction):\r\n                    if sys.platform == 'darwin' and \\\r\n                      qobject._shown_shortcut == 'missing':\r\n                        qobject._shown_shortcut = keyseq\r\n                    else:\r\n                        qobject.setShortcut(keyseq)\r\n                    if add_sc_to_tip:\r\n                        add_shortcut_to_tooltip(qobject, context, name)\r\n                elif isinstance(qobject, QShortcut):\r\n                    qobject.setKey(keyseq)\r\n            except RuntimeError:\r\n                # Object has been deleted\r\n                toberemoved.append(index)\r\n        for index in sorted(toberemoved, reverse=True):\r\n            self.shortcut_data.pop(index)", "output": "Apply shortcuts settings to all widgets/plugins", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def submit(cluster_config_file, docker, screen, tmux, stop, start,\n           cluster_name, port_forward, script, script_args):\n    \"\"\"\n    \"\"\"\n    assert not (screen and tmux), \"Can specify only one of `screen` or `tmux`.\"\n\n    if start:\n        create_or_update_cluster(cluster_config_file, None, None, False, False,\n                                 True, cluster_name)\n\n    target = os.path.join(\"~\", os.path.basename(script))\n    rsync(cluster_config_file, script, target, cluster_name, down=False)\n\n    cmd = \" \".join([\"python\", target] + list(script_args))\n    exec_cluster(cluster_config_file, cmd, docker, screen, tmux, stop, False,\n                 cluster_name, port_forward)", "output": "Uploads and runs a script on the specified cluster.\n\n    The script is automatically synced to the following location:\n\n        os.path.join(\"~\", os.path.basename(script))", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_number_matches(self, pattern, source_text='', case=False,\r\n                           regexp=False):\r\n        \"\"\"\"\"\"\r\n        pattern = to_text_string(pattern)\r\n        if not pattern:\r\n            return 0\r\n\r\n        if not regexp:\r\n            pattern = re.escape(pattern)\r\n\r\n        if not source_text:\r\n            source_text = to_text_string(self.toPlainText())\r\n\r\n        try:\r\n            if case:\r\n                regobj = re.compile(pattern)\r\n            else:\r\n                regobj = re.compile(pattern, re.IGNORECASE)\r\n        except sre_constants.error:\r\n            return None\r\n\r\n        number_matches = 0\r\n        for match in regobj.finditer(source_text):\r\n            number_matches += 1\r\n\r\n        return number_matches", "output": "Get the number of matches for the searched text.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_output(decoder_output, rows, cols, targets, hparams):\n  \"\"\"\n  \"\"\"\n  del targets  # unused arg\n  decoded_image = postprocess_image(decoder_output, rows, cols, hparams)\n  batch = common_layers.shape_list(decoded_image)[0]\n  depth = common_layers.shape_list(decoded_image)[-1]\n  likelihood = getattr(hparams, \"likelihood\", DistributionType.CAT)\n  if hparams.mode == tf.estimator.ModeKeys.PREDICT:\n    y = tf.reshape(decoded_image, [batch, -1, 1, 1, depth])\n    output = y[:, :rows, :, :, :]\n  elif likelihood == DistributionType.CAT:\n    # Unpack the cols dimension of the Categorical.\n    channels = hparams.num_channels\n    output = tf.reshape(decoded_image,\n                        [batch, rows, cols // channels, channels, depth])\n  else:\n    output = decoded_image\n  return output", "output": "Creates output from decoder output and vars.\n\n  Args:\n    decoder_output: Tensor of shape [batch, ...], where ... can be any rank such\n      that the number of elements is batch * rows * cols * hparams.hidden_size.\n    rows: Integer representing number of rows in a 2-D data point.\n    cols: Integer representing number of columns in a 2-D data point.\n    targets: Tensor of shape [batch, hparams.img_len, hparams.img_len,\n      hparams.num_channels].\n    hparams: HParams set.\n\n  Returns:\n    Tensor of shape [batch, hparams.img_len, hparams.img_len,\n    hparams.num_mixtures * 10] if hparams.likelihood is DMOL, otherwise\n    [batch, hparams.img_len, hparams.img_len, hparams.num_channels, 256].\n    In the special case of predict mode, it is a Tensor of rank 5.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_permute(builder, layer, input_names, output_names, keras_layer):\n    \"\"\"\n    \"\"\"\n    input_name, output_name = (input_names[0], output_names[0])\n\n    keras_dims = keras_layer.dims\n    # Keras permute layer index begins at 1\n    if len(keras_dims) == 3:\n        # Keras input tensor interpret as (H,W,C)\n        x = list(np.array(keras_dims))\n        i1, i2, i3 = x.index(1), x.index(2), x.index(3)\n        x[i1], x[i2], x[i3] = 2, 3, 1\n        # add a sequence axis\n        x = [0] + x\n        dim = tuple(x)\n    elif len(keras_dims) == 4:\n        # Here we use Keras converter as a place holder for inserting\n        # permutations - the values here are not valid Keras dim parameters\n        # but parameters we need to use to convert to CoreML model\n        dim = keras_dims\n    else:\n        raise NotImplementedError('Supports only 3d permutation.')\n\n    builder.add_permute(name = layer, dim=dim, input_name = input_name,\n            output_name = output_name)", "output": "Convert a softmax layer from keras to coreml.\n\n    Parameters\n    keras_layer: layer\n    ----------\n        A keras layer object.\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def aggregate(self, zeroValue, seqOp, combOp):\n        \"\"\"\n        \n        \"\"\"\n        seqOp = fail_on_stopiteration(seqOp)\n        combOp = fail_on_stopiteration(combOp)\n\n        def func(iterator):\n            acc = zeroValue\n            for obj in iterator:\n                acc = seqOp(acc, obj)\n            yield acc\n        # collecting result of mapPartitions here ensures that the copy of\n        # zeroValue provided to each partition is unique from the one provided\n        # to the final reduce call\n        vals = self.mapPartitions(func).collect()\n        return reduce(combOp, vals, zeroValue)", "output": "Aggregate the elements of each partition, and then the results for all\n        the partitions, using a given combine functions and a neutral \"zero\n        value.\"\n\n        The functions C{op(t1, t2)} is allowed to modify C{t1} and return it\n        as its result value to avoid object allocation; however, it should not\n        modify C{t2}.\n\n        The first function (seqOp) can return a different result type, U, than\n        the type of this RDD. Thus, we need one operation for merging a T into\n        an U and one operation for merging two U\n\n        >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n        >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n        >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n        (10, 4)\n        >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n        (0, 0)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_util_getBetweenQuarter(begin_date, end_date):\n    \"\"\"\n    \n    \"\"\"\n    quarter_list = {}\n    month_list = QA_util_getBetweenMonth(begin_date, end_date)\n    for value in month_list:\n        tempvalue = value.split(\"-\")\n        year = tempvalue[0]\n        if tempvalue[1] in ['01', '02', '03']:\n            quarter_list[year + \"Q1\"] = ['%s-01-01' % year, '%s-03-31' % year]\n        elif tempvalue[1] in ['04', '05', '06']:\n            quarter_list[year + \"Q2\"] = ['%s-04-01' % year, '%s-06-30' % year]\n        elif tempvalue[1] in ['07', '08', '09']:\n            quarter_list[year + \"Q3\"] = ['%s-07-31' % year, '%s-09-30' % year]\n        elif tempvalue[1] in ['10', '11', '12']:\n            quarter_list[year + \"Q4\"] = ['%s-10-01' % year, '%s-12-31' % year]\n    return(quarter_list)", "output": "#\u52a0\u4e0a\u6bcf\u5b63\u5ea6\u7684\u8d77\u59cb\u65e5\u671f\u3001\u7ed3\u675f\u65e5\u671f", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _rehydrate_skeleton_class(skeleton_class, class_dict):\n    \"\"\"\n    \"\"\"\n    for attrname, attr in class_dict.items():\n        setattr(skeleton_class, attrname, attr)\n    return skeleton_class", "output": "Put attributes from `class_dict` back on `skeleton_class`.\n\n    See CloudPickler.save_dynamic_class for more info.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def state(name):\n    '''\n    \n    '''\n    try:\n        cmd = 'show {0} --property=State'.format(name)\n        return _machinectl(cmd, ignore_retcode=True)['stdout'].split('=')[-1]\n    except IndexError:\n        return 'stopped'", "output": "Return state of container (running or stopped)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion nspawn.state <name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_gpu(x, *args, **kwargs):\n    ''' '''\n    return x.cuda(*args, **kwargs) if USE_GPU else x", "output": "puts pytorch variable to gpu, if cuda is available and USE_GPU is set to true.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unzip(seq, elem_len=None):\n    \"\"\"\n    \"\"\"\n    ret = tuple(zip(*_gen_unzip(map(tuple, seq), elem_len)))\n    if ret:\n        return ret\n\n    if elem_len is None:\n        raise ValueError(\"cannot unzip empty sequence without 'elem_len'\")\n    return ((),) * elem_len", "output": "Unzip a length n sequence of length m sequences into m seperate length\n    n sequences.\n    Parameters\n    ----------\n    seq : iterable[iterable]\n        The sequence to unzip.\n    elem_len : int, optional\n        The expected length of each element of ``seq``. If not provided this\n        will be infered from the length of the first element of ``seq``. This\n        can be used to ensure that code like: ``a, b = unzip(seq)`` does not\n        fail even when ``seq`` is empty.\n    Returns\n    -------\n    seqs : iterable[iterable]\n        The new sequences pulled out of the first iterable.\n    Raises\n    ------\n    ValueError\n        Raised when ``seq`` is empty and ``elem_len`` is not provided.\n        Raised when elements of ``seq`` do not match the given ``elem_len`` or\n        the length of the first element of ``seq``.\n    Examples\n    --------\n    >>> seq = [('a', 1), ('b', 2), ('c', 3)]\n    >>> cs, ns = unzip(seq)\n    >>> cs\n    ('a', 'b', 'c')\n    >>> ns\n    (1, 2, 3)\n\n    # checks that the elements are the same length\n    >>> seq = [('a', 1), ('b', 2), ('c', 3, 'extra')]\n    >>> cs, ns = unzip(seq)\n    Traceback (most recent call last):\n       ...\n    ValueError: element at index 2 was length 3, expected 2\n\n    # allows an explicit element length instead of infering\n    >>> seq = [('a', 1, 'extra'), ('b', 2), ('c', 3)]\n    >>> cs, ns = unzip(seq, 2)\n    Traceback (most recent call last):\n      ...\n    ValueError: element at index 0 was length 3, expected 2\n\n    # handles empty sequences when a length is given\n    >>> cs, ns = unzip([], elem_len=2)\n    >>> cs == ns == ()\n    True\n\n    Notes\n    -----\n    This function will force ``seq`` to completion.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ports_open(name, ports, proto='tcp', direction='in'):\n    '''\n    \n    '''\n\n    ports = list(six.moves.map(six.text_type, ports))\n    diff = False\n    ret = {'name': ','.join(ports),\n           'changes': {},\n           'result': True,\n           'comment': 'Ports open.'}\n\n    current_ports = __salt__['csf.get_ports'](proto=proto, direction=direction)\n    direction = direction.upper()\n    directions = __salt__['csf.build_directions'](direction)\n    for direction in directions:\n        log.trace('current_ports[direction]: %s', current_ports[direction])\n        log.trace('ports: %s', ports)\n        if current_ports[direction] != ports:\n            diff = True\n    if diff:\n        result = __salt__['csf.allow_ports'](ports, proto=proto, direction=direction)\n        ret['changes']['Ports'] = 'Changed'\n        ret['comment'] = result\n    return ret", "output": "Ensure ports are open for a protocol, in a direction.\n    e.g. - proto='tcp', direction='in' would set the values\n    for TCP_IN in the csf.conf file.\n\n    ports\n        A list of ports that should be open.\n\n    proto\n        The protocol. May be one of 'tcp', 'udp',\n        'tcp6', or 'udp6'.\n\n    direction\n        Choose 'in', 'out', or both to indicate the port\n        should be opened for inbound traffic, outbound\n        traffic, or both.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_object_parser(self, json):\n        \"\"\"\n        \n        \"\"\"\n        typ = self.typ\n        dtype = self.dtype\n        kwargs = {\n            \"orient\": self.orient, \"dtype\": self.dtype,\n            \"convert_axes\": self.convert_axes,\n            \"convert_dates\": self.convert_dates,\n            \"keep_default_dates\": self.keep_default_dates, \"numpy\": self.numpy,\n            \"precise_float\": self.precise_float, \"date_unit\": self.date_unit\n        }\n        obj = None\n        if typ == 'frame':\n            obj = FrameParser(json, **kwargs).parse()\n\n        if typ == 'series' or obj is None:\n            if not isinstance(dtype, bool):\n                kwargs['dtype'] = dtype\n            obj = SeriesParser(json, **kwargs).parse()\n\n        return obj", "output": "Parses a json document into a pandas object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def train_valid_split(dataset, valid_ratio=0.05):\n    \"\"\"\n    \"\"\"\n    if not 0.0 <= valid_ratio <= 1.0:\n        raise ValueError('valid_ratio should be in [0, 1]')\n\n    num_train = len(dataset)\n    num_valid = np.ceil(num_train * valid_ratio).astype('int')\n    indices = np.arange(num_train)\n\n    np.random.shuffle(indices)\n    valid = SimpleDataset([dataset[indices[i]] for i in range(num_valid)])\n    train = SimpleDataset([dataset[indices[i + num_valid]] for i in range(num_train - num_valid)])\n    return train, valid", "output": "Split the dataset into training and validation sets.\n\n    Parameters\n    ----------\n    dataset : list\n        A list of training samples.\n    valid_ratio : float, default 0.05\n        Proportion of training samples to use for validation set\n        range: [0, 1]\n\n    Returns\n    -------\n    train : SimpleDataset\n    valid : SimpleDataset", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getattrs(value, attrs, default=_no_default):\n    \"\"\"\n    \n    \"\"\"\n    try:\n        for attr in attrs:\n            value = getattr(value, attr)\n    except AttributeError:\n        if default is _no_default:\n            raise\n        value = default\n    return value", "output": "Perform a chained application of ``getattr`` on ``value`` with the values\n    in ``attrs``.\n\n    If ``default`` is supplied, return it if any of the attribute lookups fail.\n\n    Parameters\n    ----------\n    value : object\n        Root of the lookup chain.\n    attrs : iterable[str]\n        Sequence of attributes to look up.\n    default : object, optional\n        Value to return if any of the lookups fail.\n\n    Returns\n    -------\n    result : object\n        Result of the lookup sequence.\n\n    Examples\n    --------\n    >>> class EmptyObject(object):\n    ...     pass\n    ...\n    >>> obj = EmptyObject()\n    >>> obj.foo = EmptyObject()\n    >>> obj.foo.bar = \"value\"\n    >>> getattrs(obj, ('foo', 'bar'))\n    'value'\n\n    >>> getattrs(obj, ('foo', 'buzz'))\n    Traceback (most recent call last):\n       ...\n    AttributeError: 'EmptyObject' object has no attribute 'buzz'\n\n    >>> getattrs(obj, ('foo', 'buzz'), 'default')\n    'default'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_pb(cls, database_pb, instance, pool=None):\n        \"\"\"\n        \"\"\"\n        match = _DATABASE_NAME_RE.match(database_pb.name)\n        if match is None:\n            raise ValueError(\n                \"Database protobuf name was not in the \" \"expected format.\",\n                database_pb.name,\n            )\n        if match.group(\"project\") != instance._client.project:\n            raise ValueError(\n                \"Project ID on database does not match the \"\n                \"project ID on the instance's client\"\n            )\n        instance_id = match.group(\"instance_id\")\n        if instance_id != instance.instance_id:\n            raise ValueError(\n                \"Instance ID on database does not match the \"\n                \"Instance ID on the instance\"\n            )\n        database_id = match.group(\"database_id\")\n\n        return cls(database_id, instance, pool=pool)", "output": "Creates an instance of this class from a protobuf.\n\n        :type database_pb:\n            :class:`google.spanner.v2.spanner_instance_admin_pb2.Instance`\n        :param database_pb: A instance protobuf object.\n\n        :type instance: :class:`~google.cloud.spanner_v1.instance.Instance`\n        :param instance: The instance that owns the database.\n\n        :type pool: concrete subclass of\n                    :class:`~google.cloud.spanner_v1.pool.AbstractSessionPool`.\n        :param pool: (Optional) session pool to be used by database.\n\n        :rtype: :class:`Database`\n        :returns: The database parsed from the protobuf response.\n        :raises ValueError:\n            if the instance name does not match the expected format\n            or if the parsed project ID does not match the project ID\n            on the instance's client, or if the parsed instance ID does\n            not match the instance's ID.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _AddPropertiesForExtensions(descriptor, cls):\n  \"\"\"\"\"\"\n  extension_dict = descriptor.extensions_by_name\n  for extension_name, extension_field in extension_dict.items():\n    constant_name = extension_name.upper() + \"_FIELD_NUMBER\"\n    setattr(cls, constant_name, extension_field.number)\n\n  # TODO(amauryfa): Migrate all users of these attributes to functions like\n  #   pool.FindExtensionByNumber(descriptor).\n  if descriptor.file is not None:\n    # TODO(amauryfa): Use cls.MESSAGE_FACTORY.pool when available.\n    pool = descriptor.file.pool\n    cls._extensions_by_number = pool._extensions_by_number[descriptor]\n    cls._extensions_by_name = pool._extensions_by_name[descriptor]", "output": "Adds properties for all fields in this protocol message type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def conv_lstm(x,\n              kernel_size,\n              filters,\n              padding=\"SAME\",\n              dilation_rate=(1, 1),\n              name=None,\n              reuse=None):\n  \"\"\"\"\"\"\n  with tf.variable_scope(\n      name, default_name=\"conv_lstm\", values=[x], reuse=reuse):\n    gates = conv(\n        x,\n        4 * filters,\n        kernel_size,\n        padding=padding,\n        dilation_rate=dilation_rate)\n    g = tf.split(layer_norm(gates, 4 * filters), 4, axis=3)\n    new_cell = tf.sigmoid(g[0]) * x + tf.sigmoid(g[1]) * tf.tanh(g[3])\n    return tf.sigmoid(g[2]) * tf.tanh(new_cell)", "output": "Convolutional LSTM in 1 dimension.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _reset_syslog_config_params(host, username, password, cmd, resets, valid_resets,\n                                protocol=None, port=None, esxi_host=None, credstore=None):\n    '''\n    \n    '''\n    ret_dict = {}\n    all_success = True\n\n    if not isinstance(resets, list):\n        resets = [resets]\n\n    for reset_param in resets:\n        if reset_param in valid_resets:\n            ret = salt.utils.vmware.esxcli(host, username, password, cmd + reset_param,\n                                           protocol=protocol, port=port,\n                                           esxi_host=esxi_host, credstore=credstore)\n            ret_dict[reset_param] = {}\n            ret_dict[reset_param]['success'] = ret['retcode'] == 0\n            if ret['retcode'] != 0:\n                all_success = False\n                ret_dict[reset_param]['message'] = ret['stdout']\n        else:\n            all_success = False\n            ret_dict[reset_param] = {}\n            ret_dict[reset_param]['success'] = False\n            ret_dict[reset_param]['message'] = 'Invalid syslog ' \\\n                                               'configuration parameter'\n\n    ret_dict['success'] = all_success\n\n    return ret_dict", "output": "Helper function for reset_syslog_config that resets the config and populates the return dictionary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def faulty():\n    '''\n    \n    '''\n    fmadm = _check_fmadm()\n    cmd = '{cmd} faulty'.format(\n        cmd=fmadm,\n    )\n    res = __salt__['cmd.run_all'](cmd)\n    result = {}\n    if res['stdout'] == '':\n        result = False\n    else:\n        result = _parse_fmadm_faulty(res['stdout'])\n\n    return result", "output": "Display list of faulty resources\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' fmadm.faulty", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace(s, pattern, replacement):\n    \"\"\"\n    \"\"\"\n    # the replacement string may contain invalid backreferences (like \\1 or \\g)\n    # which will cause python's regex to blow up. Since this should emulate\n    # the jam version exactly and the jam version didn't support\n    # backreferences, this version shouldn't either. re.sub\n    # allows replacement to be a callable; this is being used\n    # to simply return the replacement string and avoid the hassle\n    # of worrying about backreferences within the string.\n    def _replacement(matchobj):\n        return replacement\n    return re.sub(pattern, _replacement, s)", "output": "Replaces occurrences of a match string in a given\n    string and returns the new string. The match string\n    can be a regex expression.\n\n    Args:\n        s (str):           the string to modify\n        pattern (str):     the search expression\n        replacement (str): the string to replace each match with", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _shuffle_tfrecord(path, random_gen):\n  \"\"\"\"\"\"\n  # Read all records\n  record_iter = tf.compat.v1.io.tf_record_iterator(path)\n  all_records = [\n      r for r in utils.tqdm(\n          record_iter, desc=\"Reading...\", unit=\" examples\", leave=False)\n  ]\n  # Shuffling in memory\n  random_gen.shuffle(all_records)\n  # Write all record back\n  with tf.io.TFRecordWriter(path) as writer:\n    for record in utils.tqdm(\n        all_records, desc=\"Writing...\", unit=\" examples\", leave=False):\n      writer.write(record)", "output": "Shuffle a single record file in memory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def groups(self):\n        \"\"\"\n        \"\"\"\n        _tables()\n        self._check_if_open()\n        return [\n            g for g in self._handle.walk_groups()\n            if (not isinstance(g, _table_mod.link.Link) and\n                (getattr(g._v_attrs, 'pandas_type', None) or\n                 getattr(g, 'table', None) or\n                (isinstance(g, _table_mod.table.Table) and\n                 g._v_name != 'table')))\n        ]", "output": "return a list of all the top-level nodes (that are not themselves a\n        pandas storage object)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_padding_lengths(self) -> Dict[str, Dict[str, int]]:\n        \"\"\"\n        \n        \"\"\"\n        lengths = {}\n        for field_name, field in self.fields.items():\n            lengths[field_name] = field.get_padding_lengths()\n        return lengths", "output": "Returns a dictionary of padding lengths, keyed by field name.  Each ``Field`` returns a\n        mapping from padding keys to actual lengths, and we just key that dictionary by field name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __mgmt(name, _type, action):\n    '''\n    \n    '''\n    # It's permanent because the 4 concerned functions need the permanent option, it's wrong without\n    cmd = '--{0}-{1}={2} --permanent'.format(action, _type, name)\n\n    return __firewall_cmd(cmd)", "output": "Perform zone management", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def complain(distribution_name):\n    \"\"\"\n    \"\"\"\n    try:\n        pkg_resources.get_distribution(distribution_name)\n        warnings.warn(\n            \"The {pkg} distribution is now obsolete. \"\n            \"Please `pip uninstall {pkg}`. \"\n            \"In the future, this warning will become an ImportError.\".format(\n                pkg=distribution_name\n            ),\n            DeprecationWarning,\n        )\n    except pkg_resources.DistributionNotFound:\n        pass", "output": "Issue a warning if `distribution_name` is installed.\n\n    In a future release, this method will be updated to raise ImportError\n    rather than just send a warning.\n\n    Args:\n        distribution_name (str): The name of the obsolete distribution.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_git_repos(clean=False):\n    '''\n    \n    '''\n    if not salt.utils.path.which('git'):\n        raise CommandExecutionError(\n            'Git for Windows is not installed, or not configured to be '\n            'accessible from the Command Prompt'\n        )\n    return _update_git_repos(opts=__opts__, clean=clean, masterless=True)", "output": "Checkout git repos containing :ref:`Windows Software Package Definitions\n    <windows-package-manager>`.\n\n    .. important::\n        This function requires `Git for Windows`_ to be installed in order to\n        work. When installing, make sure to select an installation option which\n        permits the git executable to be run from the Command Prompt.\n\n    .. _`Git for Windows`: https://git-for-windows.github.io/\n\n    clean : False\n        Clean repo cachedirs which are not configured under\n        :conf_minion:`winrepo_remotes`.\n\n        .. note::\n            This option only applies if either pygit2_ or GitPython_ is\n            installed into Salt's bundled Python.\n\n        .. warning::\n            This argument should not be set to ``True`` if a mix of git and\n            non-git repo definitions are being used, as it will result in the\n            non-git repo definitions being removed.\n\n        .. versionadded:: 2015.8.0\n\n        .. _GitPython: https://github.com/gitpython-developers/GitPython\n        .. _pygit2: https://github.com/libgit2/pygit2\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call winrepo.update_git_repos", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def new_context(self, vars=None, shared=False, locals=None):\n        \"\"\"\n        \"\"\"\n        return new_context(self.environment, self.name, self.blocks,\n                           vars, shared, self.globals, locals)", "output": "Create a new :class:`Context` for this template.  The vars\n        provided will be passed to the template.  Per default the globals\n        are added to the context.  If shared is set to `True` the data\n        is passed as it to the context without adding the globals.\n\n        `locals` can be a dict of local variables for internal usage.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copyto(self, other):\n        \"\"\"\n        \"\"\"\n        if isinstance(other, Context):\n            return super(RowSparseNDArray, self).copyto(other)\n        elif isinstance(other, NDArray):\n            stype = other.stype\n            if stype in ('default', 'row_sparse'):\n                return super(RowSparseNDArray, self).copyto(other)\n            else:\n                raise TypeError('copyto does not support destination NDArray stype ' + str(stype))\n        else:\n            raise TypeError('copyto does not support type ' + str(type(other)))", "output": "Copies the value of this array to another array.\n\n        If ``other`` is a ``NDArray`` or ``RowSparseNDArray`` object, then ``other.shape``\n        and ``self.shape`` should be the same. This function copies the value from\n        ``self`` to ``other``.\n\n        If ``other`` is a context, a new ``RowSparseNDArray`` will be first created on\n        the target context, and the value of ``self`` is copied.\n\n        Parameters\n        ----------\n        other : NDArray or RowSparseNDArray or Context\n            The destination array or context.\n\n        Returns\n        -------\n        NDArray or RowSparseNDArray\n            The copied array. If ``other`` is an ``NDArray`` or ``RowSparseNDArray``, then the\n            return value and ``other`` will point to the same ``NDArray`` or ``RowSparseNDArray``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_atom_coltype(self, kind=None):\n        \"\"\"  \"\"\"\n        if kind is None:\n            kind = self.kind\n        if self.kind.startswith('uint'):\n            col_name = \"UInt{name}Col\".format(name=kind[4:])\n        else:\n            col_name = \"{name}Col\".format(name=kind.capitalize())\n\n        return getattr(_tables(), col_name)", "output": "return the PyTables column class for this column", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_install_conflicts(to_install):\n    # type: (List[InstallRequirement]) -> Tuple[PackageSet, CheckResult]\n    \"\"\"\n    \"\"\"\n    # Start from the current state\n    package_set, _ = create_package_set_from_installed()\n    # Install packages\n    would_be_installed = _simulate_installation_of(to_install, package_set)\n\n    # Only warn about directly-dependent packages; create a whitelist of them\n    whitelist = _create_whitelist(would_be_installed, package_set)\n\n    return (\n        package_set,\n        check_package_set(\n            package_set, should_ignore=lambda name: name not in whitelist\n        )\n    )", "output": "For checking if the dependency graph would be consistent after \\\n    installing given requirements", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sync_matchers(saltenv=None, refresh=False, extmod_whitelist=None, extmod_blacklist=None):\n    '''\n    \n    '''\n    ret = _sync('matchers', saltenv, extmod_whitelist, extmod_blacklist)\n    if refresh:\n        refresh_modules()\n    return ret", "output": ".. versionadded:: 2019.2.0\n\n    Sync engine modules from ``salt://_matchers`` to the minion\n\n    saltenv\n        The fileserver environment from which to sync. To sync from more than\n        one environment, pass a comma-separated list.\n\n        If not passed, then all environments configured in the :ref:`top files\n        <states-top>` will be checked for engines to sync. If no top files are\n        found, then the ``base`` environment will be synced.\n\n    refresh : True\n        If ``True``, refresh the available execution modules on the minion.\n        This refresh will be performed even if no new matcher modules are synced.\n        Set to ``False`` to prevent this refresh.\n\n    extmod_whitelist : None\n        comma-separated list of modules to sync\n\n    extmod_blacklist : None\n        comma-separated list of modules to blacklist based on type\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.sync_matchers\n        salt '*' saltutil.sync_matchers saltenv=base,dev", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def profile_methods(self, method_list):\n        \"\"\"\"\"\"\n        self.method_exec_info = []\n        # \u5f00\u59cb\u6570\u636e\u8bb0\u5f55\u8fdb\u7a0b\n        self.record_thread.stop_flag = False\n        self.record_thread.start()\n\n        for name in method_list:\n            if name not in self.check_macthing_object.MATCHING_METHODS.keys():\n                continue\n            time.sleep(3)  # \u7559\u51fa\u7ed8\u56fe\u7a7a\u767d\u533a\n            start_time = time.time()  # \u8bb0\u5f55\u5f00\u59cb\u65f6\u95f4\n            print(\"--->>> start '%s' matching:\\n\" % name)\n            kp_sch, kp_src, good, result = self.check_macthing_object.get_and_plot_keypoints(name)  # \u6839\u636e\u65b9\u6cd5\u540d\u7ed8\u5236\u5bf9\u5e94\u7684\u8bc6\u522b\u7ed3\u679c\n            print(\"\\n\\n\\n\")\n            end_time = time.time()  # \u8bb0\u5f55\u7ed3\u675f\u65f6\u95f4\n            time.sleep(3)  # \u7559\u51fa\u7ed8\u56fe\u7a7a\u767d\u533a\n            # \u8bb0\u5f55\u672c\u6b21\u5339\u914d\u7684\u76f8\u5173\u6570\u636e\n            ret_info = {\n                \"name\": name,\n                \"start_time\": start_time,\n                \"end_time\": end_time,\n                \"result\": result,\n                \"kp_sch\": len(kp_sch),\n                \"kp_src\": len(kp_src),\n                \"good\": len(good)}\n            self.method_exec_info.append(ret_info)\n\n        self.record_thread.stop_flag = True", "output": "\u5e2e\u52a9\u51fd\u6570\u6267\u884c\u65f6\u8bb0\u5f55\u6570\u636e.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_objects(config=None, config_path=None, regex=None, saltenv='base'):\n    '''\n    \n    '''\n    ccp = _get_ccp(config=config, config_path=config_path, saltenv=saltenv)\n    lines = ccp.find_objects(regex)\n    return lines", "output": "Return all the line objects that match the expression in the ``regex``\n    argument.\n\n    .. warning::\n        This function is mostly valuable when invoked from other Salt\n        components (i.e., execution modules, states, templates etc.). For CLI\n        usage, please consider using\n        :py:func:`ciscoconfparse.find_lines <salt.ciscoconfparse_mod.find_lines>`\n\n    config\n        The configuration sent as text.\n\n        .. note::\n            This argument is ignored when ``config_path`` is specified.\n\n    config_path\n        The absolute or remote path to the file with the configuration to be\n        parsed. This argument supports the usual Salt filesystem URIs, e.g.,\n        ``salt://``, ``https://``, ``ftp://``, ``s3://``, etc.\n\n    regex\n        The regular expression to match the lines against.\n\n    saltenv: ``base``\n        Salt fileserver environment from which to retrieve the file. This\n        argument is ignored when ``config_path`` is not a ``salt://`` URL.\n\n    Usage example:\n\n    .. code-block:: python\n\n        objects = __salt__['ciscoconfparse.find_objects'](config_path='salt://path/to/config.txt',\n                                                          regex='Gigabit')\n        for obj in objects:\n            print(obj.text)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def port_add_policy(name, sel_type=None, protocol=None, port=None, sel_range=None):\n    '''\n    \n    '''\n    return _port_add_or_delete_policy('add', name, sel_type, protocol, port, sel_range)", "output": ".. versionadded:: 2019.2.0\n\n    Adds the SELinux policy for a given protocol and port.\n\n    Returns the result of the call to semanage.\n\n    name\n        The protocol and port spec. Can be formatted as ``(tcp|udp)/(port|port-range)``.\n\n    sel_type\n        The SELinux Type. Required.\n\n    protocol\n        The protocol for the port, ``tcp`` or ``udp``. Required if name is not formatted.\n\n    port\n        The port or port range. Required if name is not formatted.\n\n    sel_range\n        The SELinux MLS/MCS Security Range.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' selinux.port_add_policy add tcp/8080 http_port_t\n        salt '*' selinux.port_add_policy add foobar http_port_t protocol=tcp port=8091", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def schema_org(builder):\n  # pylint: disable=line-too-long\n  \"\"\"\n  \"\"\"\n  # pylint: enable=line-too-long\n\n  properties = [\n      (lambda x: x.name, SCHEMA_ORG_NAME),\n      (lambda x: x.description, SCHEMA_ORG_DESC),\n      (lambda x: x.name, SCHEMA_ORG_URL),\n      (lambda x: (x.urls and x.urls[0]) or \"\", SCHEMA_ORG_SAMEAS)\n  ]\n\n  info = builder.info\n  out_str = SCHEMA_ORG_PRE\n  for extractor, template in properties:\n    val = extractor(info)\n    if val:\n      # We are using cgi module instead of html due to Python 2 compatibility\n      out_str += template.format(val=cgi.escape(val, quote=True).strip())\n  out_str += SCHEMA_ORG_POST\n\n  return out_str", "output": "Builds schema.org microdata for DatasetSearch from DatasetBuilder.\n\n  Markup spec: https://developers.google.com/search/docs/data-types/dataset#dataset\n  Testing tool: https://search.google.com/structured-data/testing-tool\n  For Google Dataset Search: https://toolbox.google.com/datasetsearch\n\n  Microdata format was chosen over JSON-LD due to the fact that Markdown\n  rendering engines remove all <script> tags.\n\n  Args:\n    builder: `tfds.core.DatasetBuilder`\n\n  Returns:\n    HTML string with microdata", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def broadcast_to(self, shape):\n        \"\"\"\n        \"\"\"\n        cur_shape = self.shape\n        err_str = 'operands could not be broadcast together with remapped shapes' \\\n                  '[original->remapped]: {} and requested shape {}'.format(cur_shape, shape)\n        if len(shape) < len(cur_shape):\n            raise ValueError(err_str)\n        cur_shape = (1,) * (len(shape) - len(cur_shape)) + cur_shape\n        cur_shape_arr = np.array(cur_shape)\n        broadcasting_axes = np.nonzero(cur_shape_arr != np.array(shape))\n        if (cur_shape_arr[broadcasting_axes] != 1).any():\n            raise ValueError(err_str)\n        if cur_shape != self.shape:\n            return op.broadcast_to(self.reshape(cur_shape), shape=shape)\n        else:\n            return op.broadcast_to(self, shape=tuple(shape))", "output": "Broadcasts the input array to a new shape.\n\n        Broadcasting is only allowed on axes with size 1. The new shape cannot change\n        the number of dimensions.\n        For example, you could broadcast from shape (2, 1) to (2, 3), but not from\n        shape (2, 3) to (2, 3, 3).\n\n        Parameters\n        ----------\n        shape : tuple of int\n            The shape of the desired array.\n\n        Returns\n        -------\n        NDArray\n            A NDArray with the desired shape that is not sharing data with this\n            array, even if the new shape is the same as ``self.shape``.\n\n        Examples\n        --------\n        >>> x = mx.nd.arange(0,3).reshape((1,3,1))\n        >>> x.asnumpy()\n        array([[[ 0.],\n                [ 1.],\n                [ 2.]]], dtype=float32)\n        >>> y = x.broadcast_to((2,3,3))\n        >>> y.asnumpy()\n        array([[[ 0.,  0.,  0.],\n                [ 1.,  1.,  1.],\n                [ 2.,  2.,  2.]],\n        <BLANKLINE>\n               [[ 0.,  0.,  0.],\n                [ 1.,  1.,  1.],\n                [ 2.,  2.,  2.]]], dtype=float32)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def any_path_path(cls, project, database, document, any_path):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/databases/{database}/documents/{document}/{any_path=**}\",\n            project=project,\n            database=database,\n            document=document,\n            any_path=any_path,\n        )", "output": "Return a fully-qualified any_path string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_bucket(self, bucket_name):\n        \"\"\"\n        \"\"\"\n        bucket = Bucket(self, name=bucket_name)\n        bucket.reload(client=self)\n        return bucket", "output": "Get a bucket by name.\n\n        If the bucket isn't found, this will raise a\n        :class:`google.cloud.exceptions.NotFound`.\n\n        For example:\n\n        .. literalinclude:: snippets.py\n            :start-after: [START get_bucket]\n            :end-before: [END get_bucket]\n\n        This implements \"storage.buckets.get\".\n\n        :type bucket_name: str\n        :param bucket_name: The name of the bucket to get.\n\n        :rtype: :class:`google.cloud.storage.bucket.Bucket`\n        :returns: The bucket matching the name provided.\n        :raises: :class:`google.cloud.exceptions.NotFound`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def free(object_ids, local_only=False, delete_creating_tasks=False):\n    \"\"\"\n    \"\"\"\n    worker = ray.worker.get_global_worker()\n\n    if ray.worker._mode() == ray.worker.LOCAL_MODE:\n        return\n\n    if isinstance(object_ids, ray.ObjectID):\n        object_ids = [object_ids]\n\n    if not isinstance(object_ids, list):\n        raise TypeError(\"free() expects a list of ObjectID, got {}\".format(\n            type(object_ids)))\n\n    # Make sure that the values are object IDs.\n    for object_id in object_ids:\n        if not isinstance(object_id, ray.ObjectID):\n            raise TypeError(\"Attempting to call `free` on the value {}, \"\n                            \"which is not an ray.ObjectID.\".format(object_id))\n\n    worker.check_connected()\n    with profiling.profile(\"ray.free\"):\n        if len(object_ids) == 0:\n            return\n\n        worker.raylet_client.free_objects(object_ids, local_only,\n                                          delete_creating_tasks)", "output": "Free a list of IDs from object stores.\n\n    This function is a low-level API which should be used in restricted\n    scenarios.\n\n    If local_only is false, the request will be send to all object stores.\n\n    This method will not return any value to indicate whether the deletion is\n    successful or not. This function is an instruction to object store. If\n    the some of the objects are in use, object stores will delete them later\n    when the ref count is down to 0.\n\n    Args:\n        object_ids (List[ObjectID]): List of object IDs to delete.\n        local_only (bool): Whether only deleting the list of objects in local\n            object store or all object stores.\n        delete_creating_tasks (bool): Whether also delete the object creating\n            tasks.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def columnCount(self, index=QModelIndex()):\r\n        \"\"\"\"\"\"\r\n        # Avoid a \"Qt exception in virtual methods\" generated in our\r\n        # tests on Windows/Python 3.7\r\n        # See PR 8910\r\n        try:\r\n            # This is done to implement series\r\n            if len(self.df.shape) == 1:\r\n                return 2\r\n            elif self.total_cols <= self.cols_loaded:\r\n                return self.total_cols\r\n            else:\r\n                return self.cols_loaded\r\n        except AttributeError:\r\n            return 0", "output": "DataFrame column number", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def GetRemainder(self):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlTextReaderGetRemainder(self._o)\n        if ret is None:raise treeError('xmlTextReaderGetRemainder() failed')\n        __tmp = inputBuffer(_obj=ret)\n        return __tmp", "output": "Method to get the remainder of the buffered XML. this\n          method stops the parser, set its state to End Of File and\n          return the input stream with what is left that the parser\n          did not use.  The implementation is not good, the parser\n          certainly procgressed past what's left in reader->input,\n          and there is an allocation problem. Best would be to\n           rewrite it differently.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_set_info(set):\n    '''\n    \n    '''\n\n    cmd = '{0} list -t {1}'.format(_ipset_cmd(), set)\n    out = __salt__['cmd.run_all'](cmd, python_shell=False)\n\n    if out['retcode'] > 0:\n        # Set doesn't exist return false\n        return False\n\n    setinfo = {}\n    _tmp = out['stdout'].split('\\n')\n    for item in _tmp:\n        # Only split if item has a colon\n        if ':' in item:\n            key, value = item.split(':', 1)\n            setinfo[key] = value[1:]\n    return setinfo", "output": "Return information about the set", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_matches(self, match):\n        \"\"\"\n        \n        \"\"\"\n        response = Statement(text='')\n\n        from_parsed = match.group(\"from\")\n        target_parsed = match.group(\"target\")\n        n_statement = match.group(\"number\")\n\n        if n_statement == 'a' or n_statement == 'an':\n            n_statement = '1.0'\n\n        n = mathparse.parse(n_statement, self.language.ISO_639.upper())\n\n        ureg = UnitRegistry()\n        from_parsed, target_parsed = self.get_valid_units(ureg, from_parsed, target_parsed)\n\n        if from_parsed is None or target_parsed is None:\n            response.confidence = 0.0\n        else:\n            from_value = ureg.Quantity(float(n), from_parsed)\n            target_value = from_value.to(target_parsed)\n            response.confidence = 1.0\n            response.text = str(target_value.magnitude)\n\n        return response", "output": "Returns a response statement from a matched input statement.\n\n        :param match: It is a valid matched pattern from the input statement\n        :type: `_sre.SRE_Match`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validCtxtNormalizeAttributeValue(self, ctxt, elem, name, value):\n        \"\"\" \"\"\"\n        if ctxt is None: ctxt__o = None\n        else: ctxt__o = ctxt._o\n        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        ret = libxml2mod.xmlValidCtxtNormalizeAttributeValue(ctxt__o, self._o, elem__o, name, value)\n        return ret", "output": "Does the validation related extra step of the normalization\n          of attribute values:  If the declared value is not CDATA,\n          then the XML processor must further process the normalized\n          attribute value by discarding any leading and trailing\n          space (#x20) characters, and by replacing sequences of\n          space (#x20) characters by single space (#x20) character.\n          Also  check VC: Standalone Document Declaration in P32, and\n           update ctxt->valid accordingly", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_group(self):\n        \"\"\"\n        \"\"\"\n        if self.group is None:\n            self.group = self.get_field('group')\n            if self.group is not None:\n                # group data from LightGBM is boundaries data, need to convert to group size\n                self.group = np.diff(self.group)\n        return self.group", "output": "Get the group of the Dataset.\n\n        Returns\n        -------\n        group : numpy array or None\n            Group size of each group.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode(self, input_str):\n    \"\"\"\"\"\"\n    inputs = self.encoders[\"inputs\"].encode(input_str) + [EOS_ID]\n    batch_inputs = np.reshape(inputs, [1, -1, 1, 1])  # Make it 3D.\n    return batch_inputs", "output": "Input str to features dict, ready for inference.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _copy_default(cls, default):\n        ''' \n\n        '''\n        if not isinstance(default, types.FunctionType):\n            return copy(default)\n        else:\n            return default()", "output": "Return a copy of the default, or a new value if the default\n        is specified by a function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _delay_until_retry(exc, deadline):\n    \"\"\"\n    \"\"\"\n    cause = exc.errors[0]\n\n    now = time.time()\n\n    if now >= deadline:\n        raise\n\n    delay = _get_retry_delay(cause)\n    if delay is not None:\n\n        if now + delay > deadline:\n            raise\n\n        time.sleep(delay)", "output": "Helper for :meth:`Session.run_in_transaction`.\n\n    Detect retryable abort, and impose server-supplied delay.\n\n    :type exc: :class:`google.api_core.exceptions.Aborted`\n    :param exc: exception for aborted transaction\n\n    :type deadline: float\n    :param deadline: maximum timestamp to continue retrying the transaction.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_rich_text_html(self, html_text, base_url):\r\n        \"\"\"\"\"\"\r\n        self.rich_text.set_html(html_text, base_url)\r\n        self.save_text([self.rich_text.set_html, html_text, base_url])", "output": "Set rich text", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def connect(self, exe_path=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        connect_path = exe_path or self._config.DEFAULT_EXE_PATH\n        if connect_path is None:\n            raise ValueError(\n                \"\u53c2\u6570 exe_path \u672a\u8bbe\u7f6e\uff0c\u8bf7\u8bbe\u7f6e\u5ba2\u6237\u7aef\u5bf9\u5e94\u7684 exe \u5730\u5740,\u7c7b\u4f3c C:\\\\\u5ba2\u6237\u7aef\u5b89\u88c5\u76ee\u5f55\\\\xiadan.exe\"\n            )\n\n        self._app = pywinauto.Application().connect(\n            path=connect_path, timeout=10\n        )\n        self._close_prompt_windows()\n        self._main = self._app.top_window()", "output": "\u76f4\u63a5\u8fde\u63a5\u767b\u9646\u540e\u7684\u5ba2\u6237\u7aef\n        :param exe_path: \u5ba2\u6237\u7aef\u8def\u5f84\u7c7b\u4f3c r'C:\\\\htzqzyb2\\\\xiadan.exe', \u9ed8\u8ba4 r'C:\\\\htzqzyb2\\\\xiadan.exe'\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_pillar(kwargs, pillar=None):\n    '''\n    \n    '''\n    if kwargs.get('force'):\n        return True\n    pillar_dict = pillar if pillar is not None else __pillar__\n    if '_errors' in pillar_dict:\n        return False\n    return True", "output": "Check the pillar for errors, refuse to run the state if there are errors\n    in the pillar and return the pillar errors", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cache(self, bank, key, fun, loop_fun=None, **kwargs):\n        '''\n        \n        '''\n        expire_seconds = kwargs.get('expire', 86400)  # 1 day\n\n        updated = self.updated(bank, key)\n        update_cache = False\n        if updated is None:\n            update_cache = True\n        else:\n            if int(time.time()) - updated > expire_seconds:\n                update_cache = True\n\n        data = self.fetch(bank, key)\n\n        if not data or update_cache is True:\n            if loop_fun is not None:\n                data = []\n                items = fun(**kwargs)\n                for item in items:\n                    data.append(loop_fun(item))\n            else:\n                data = fun(**kwargs)\n            self.store(bank, key, data)\n\n        return data", "output": "Check cache for the data. If it is there, check to see if it needs to\n        be refreshed.\n\n        If the data is not there, or it needs to be refreshed, then call the\n        callback function (``fun``) with any given ``**kwargs``.\n\n        In some cases, the callback function returns a list of objects which\n        need to be processed by a second function. If that is the case, then\n        the second function is passed in as ``loop_fun``. Each item in the\n        return list from the first function will be the only argument for the\n        second function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def as_pyplot_figure(self, label=1, **kwargs):\n        \"\"\"\n        \"\"\"\n        import matplotlib.pyplot as plt\n        exp = self.as_list(label=label, **kwargs)\n        fig = plt.figure()\n        vals = [x[1] for x in exp]\n        names = [x[0] for x in exp]\n        vals.reverse()\n        names.reverse()\n        colors = ['green' if x > 0 else 'red' for x in vals]\n        pos = np.arange(len(exp)) + .5\n        plt.barh(pos, vals, align='center', color=colors)\n        plt.yticks(pos, names)\n        if self.mode == \"classification\":\n            title = 'Local explanation for class %s' % self.class_names[label]\n        else:\n            title = 'Local explanation'\n        plt.title(title)\n        return fig", "output": "Returns the explanation as a pyplot figure.\n\n        Will throw an error if you don't have matplotlib installed\n        Args:\n            label: desired label. If you ask for a label for which an\n                   explanation wasn't computed, will throw an exception.\n                   Will be ignored for regression explanations.\n            kwargs: keyword arguments, passed to domain_mapper\n\n        Returns:\n            pyplot figure (barchart).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forward(self, is_train, req, in_data, out_data, aux):\n        \"\"\"\n        \"\"\"\n        data = in_data[0]\n        label = in_data[1]\n        pred = mx.nd.SoftmaxOutput(data, label)\n        self.assign(out_data[0], req[0], pred)", "output": "Implements forward computation.\n\n        is_train : bool, whether forwarding for training or testing.\n        req : list of {'null', 'write', 'inplace', 'add'}, how to assign to out_data. 'null' means skip assignment, etc.\n        in_data : list of NDArray, input data.\n        out_data : list of NDArray, pre-allocated output buffers.\n        aux : list of NDArray, mutable auxiliary states. Usually not used.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_rpn_anchor_input(im, boxes, is_crowd):\n    \"\"\"\n    \n    \"\"\"\n    boxes = boxes.copy()\n    all_anchors = np.copy(get_all_anchors())\n    # fHxfWxAx4 -> (-1, 4)\n    featuremap_anchors_flatten = all_anchors.reshape((-1, 4))\n\n    # only use anchors inside the image\n    inside_ind, inside_anchors = filter_boxes_inside_shape(featuremap_anchors_flatten, im.shape[:2])\n    # obtain anchor labels and their corresponding gt boxes\n    anchor_labels, anchor_gt_boxes = get_anchor_labels(inside_anchors, boxes[is_crowd == 0], boxes[is_crowd == 1])\n\n    # Fill them back to original size: fHxfWx1, fHxfWx4\n    anchorH, anchorW = all_anchors.shape[:2]\n    featuremap_labels = -np.ones((anchorH * anchorW * cfg.RPN.NUM_ANCHOR, ), dtype='int32')\n    featuremap_labels[inside_ind] = anchor_labels\n    featuremap_labels = featuremap_labels.reshape((anchorH, anchorW, cfg.RPN.NUM_ANCHOR))\n    featuremap_boxes = np.zeros((anchorH * anchorW * cfg.RPN.NUM_ANCHOR, 4), dtype='float32')\n    featuremap_boxes[inside_ind, :] = anchor_gt_boxes\n    featuremap_boxes = featuremap_boxes.reshape((anchorH, anchorW, cfg.RPN.NUM_ANCHOR, 4))\n    return featuremap_labels, featuremap_boxes", "output": "Args:\n        im: an image\n        boxes: nx4, floatbox, gt. shoudn't be changed\n        is_crowd: n,\n\n    Returns:\n        The anchor labels and target boxes for each pixel in the featuremap.\n        fm_labels: fHxfWxNA\n        fm_boxes: fHxfWxNAx4\n        NA will be NUM_ANCHOR_SIZES x NUM_ANCHOR_RATIOS", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_json_string(self, indent=None):\n        ''' \n\n        '''\n        root_ids = []\n        for r in self._roots:\n            root_ids.append(r.id)\n\n        root_references = self._all_models.values()\n\n        json = {\n            'title' : self.title,\n            'roots' : {\n                'root_ids' : root_ids,\n                'references' : references_json(root_references)\n            },\n            'version' : __version__\n        }\n\n        return serialize_json(json, indent=indent)", "output": "Convert the document to a JSON string.\n\n        Args:\n            indent (int or None, optional) : number of spaces to indent, or\n                None to suppress all newlines and indentation (default: None)\n\n        Returns:\n            str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _toggle_checkable_action(self, checked, editorstack_method, conf_name):\r\n        \"\"\"\r\n        \"\"\"\r\n        if self.editorstacks:\r\n            for editorstack in self.editorstacks:\r\n                try:\r\n                    editorstack.__getattribute__(editorstack_method)(checked)\r\n                except AttributeError as e:\r\n                    logger.error(e, exc_info=True)\r\n                # Run code analysis when `set_pep8_enabled` is toggled\r\n                if editorstack_method == 'set_pep8_enabled':\r\n                    # TODO: Connect this to the LSP\r\n                    #for finfo in editorstack.data:\r\n                    #    finfo.run_code_analysis(\r\n                    #            self.get_option('code_analysis/pyflakes'),\r\n                    #            checked)\r\n                    pass\r\n        CONF.set('editor', conf_name, checked)", "output": "Handle the toogle of a checkable action.\r\n\r\n        Update editorstacks and the configuration.\r\n\r\n        Args:\r\n            checked (bool): State of the action.\r\n            editorstack_method (str): name of EditorStack class that will be\r\n                used to update the changes in each editorstack.\r\n            conf_name (str): configuration setting associated with the action.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_suffix(self, suffix):\n        \"\"\"\n        \n        \"\"\"\n        f = functools.partial('{}{suffix}'.format, suffix=suffix)\n\n        mapper = {self._info_axis_name: f}\n        return self.rename(**mapper)", "output": "Suffix labels with string `suffix`.\n\n        For Series, the row labels are suffixed.\n        For DataFrame, the column labels are suffixed.\n\n        Parameters\n        ----------\n        suffix : str\n            The string to add after each label.\n\n        Returns\n        -------\n        Series or DataFrame\n            New Series or DataFrame with updated labels.\n\n        See Also\n        --------\n        Series.add_prefix: Prefix row labels with string `prefix`.\n        DataFrame.add_prefix: Prefix column labels with string `prefix`.\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s\n        0    1\n        1    2\n        2    3\n        3    4\n        dtype: int64\n\n        >>> s.add_suffix('_item')\n        0_item    1\n        1_item    2\n        2_item    3\n        3_item    4\n        dtype: int64\n\n        >>> df = pd.DataFrame({'A': [1, 2, 3, 4],  'B': [3, 4, 5, 6]})\n        >>> df\n           A  B\n        0  1  3\n        1  2  4\n        2  3  5\n        3  4  6\n\n        >>> df.add_suffix('_col')\n             A_col  B_col\n        0       1       3\n        1       2       4\n        2       3       5\n        3       4       6", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_network_adapter_mapping(domain, gateway, ip_addr, subnet_mask, mac):\n    '''\n    \n    '''\n    adapter_mapping = vim.vm.customization.AdapterMapping()\n    adapter_mapping.macAddress = mac\n    adapter_mapping.adapter = vim.vm.customization.IPSettings()\n    if domain:\n        adapter_mapping.adapter.dnsDomain = domain\n    if gateway:\n        adapter_mapping.adapter.gateway = gateway\n    if ip_addr:\n        adapter_mapping.adapter.ip = \\\n            vim.vm.customization.FixedIp(ipAddress=ip_addr)\n        adapter_mapping.adapter.subnetMask = subnet_mask\n    else:\n        adapter_mapping.adapter.ip = vim.vm.customization.DhcpIpGenerator()\n    return adapter_mapping", "output": "Returns a vim.vm.customization.AdapterMapping object containing the IP\n    properties of a network adapter card\n\n    domain\n        Domain of the host\n\n    gateway\n        Gateway address\n\n    ip_addr\n        IP address\n\n    subnet_mask\n        Subnet mask\n\n    mac\n        MAC address of the guest", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_presence_events(self, salt_data, token, opts):\n        '''\n        \n        '''\n        log.debug('In presence')\n        changed = False\n\n        # check if any connections were dropped\n        if set(salt_data['data'].get('lost', [])):\n            dropped_minions = set(salt_data['data'].get('lost', []))\n        else:\n            dropped_minions = set(self.minions) - set(salt_data['data'].get('present', []))\n\n        for minion in dropped_minions:\n            changed = True\n            log.debug('Popping %s', minion)\n            self.minions.pop(minion, None)\n\n        # check if any new connections were made\n        if set(salt_data['data'].get('new', [])):\n            log.debug('got new minions')\n            new_minions = set(salt_data['data'].get('new', []))\n            changed = True\n        elif set(salt_data['data'].get('present', [])) - set(self.minions):\n            log.debug('detected new minions')\n            new_minions = set(salt_data['data'].get('present', [])) - set(self.minions)\n            changed = True\n        else:\n            new_minions = []\n\n        tgt = ','.join(new_minions)\n        for mid in new_minions:\n            log.debug('Adding minion')\n            self.minions[mid] = {}\n\n        if tgt:\n            changed = True\n            client = salt.netapi.NetapiClient(opts)\n            client.run(\n                {\n                    'fun': 'grains.items',\n                    'tgt': tgt,\n                    'expr_type': 'list',\n                    'mode': 'client',\n                    'client': 'local',\n                    'asynchronous': 'local_async',\n                    'token': token,\n                })\n\n        if changed:\n            self.publish_minions()", "output": "Check if any minions have connected or dropped.\n        Send a message to the client if they have.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_valid_cidr(string_network):\n    \"\"\"\n    \n    \"\"\"\n    if string_network.count('/') == 1:\n        try:\n            mask = int(string_network.split('/')[1])\n        except ValueError:\n            return False\n\n        if mask < 1 or mask > 32:\n            return False\n\n        try:\n            socket.inet_aton(string_network.split('/')[0])\n        except socket.error:\n            return False\n    else:\n        return False\n    return True", "output": "Very simple check of the cidr format in no_proxy variable.\n\n    :rtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_archive_name(self, archname=None):\n        '''\n        \n        '''\n        archname = re.sub('[^a-z0-9]', '', (archname or '').lower()) or 'support'\n        for grain in ['fqdn', 'host', 'localhost', 'nodename']:\n            host = __grains__.get(grain)\n            if host:\n                break\n        if not host:\n            host = 'localhost'\n\n        return os.path.join(tempfile.gettempdir(),\n                            '{hostname}-{archname}-{date}-{time}.bz2'.format(archname=archname,\n                                                                             hostname=host,\n                                                                             date=time.strftime('%Y%m%d'),\n                                                                             time=time.strftime('%H%M%S')))", "output": "Create default archive name.\n\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def verify_signature(pubkey_path, message, signature):\n    '''\n    \n    '''\n    log.debug('salt.crypt.verify_signature: Loading public key')\n    pubkey = get_rsa_pub_key(pubkey_path)\n    log.debug('salt.crypt.verify_signature: Verifying signature')\n    if HAS_M2:\n        md = EVP.MessageDigest('sha1')\n        md.update(salt.utils.stringutils.to_bytes(message))\n        digest = md.final()\n        return pubkey.verify(digest, signature)\n    else:\n        verifier = PKCS1_v1_5.new(pubkey)\n        return verifier.verify(SHA.new(salt.utils.stringutils.to_bytes(message)), signature)", "output": "Use Crypto.Signature.PKCS1_v1_5 to verify the signature on a message.\n    Returns True for valid signature.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_param_doc(arg_names, arg_types, arg_descs, remove_dup=True):\n    \"\"\"\n    \"\"\"\n    param_keys = set()\n    param_str = []\n    for key, type_info, desc in zip(arg_names, arg_types, arg_descs):\n        if key in param_keys and remove_dup:\n            continue\n        if key == 'num_args':\n            continue\n        param_keys.add(key)\n        ret = '%s : %s' % (key, type_info)\n        if len(desc) != 0:\n            ret += '\\n    ' + desc\n        param_str.append(ret)\n    doc_str = ('Parameters\\n' +\n               '----------\\n' +\n               '%s\\n')\n    doc_str = doc_str % ('\\n'.join(param_str))\n    return doc_str", "output": "Build argument docs in python style.\n\n    arg_names : list of str\n        Argument names.\n\n    arg_types : list of str\n        Argument type information.\n\n    arg_descs : list of str\n        Argument description information.\n\n    remove_dup : boolean, optional\n        Whether remove duplication or not.\n\n    Returns\n    -------\n    docstr : str\n        Python docstring of parameter sections.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wollist(maclist, bcast='255.255.255.255', destport=9):\n    '''\n    \n    '''\n    ret = []\n    try:\n        with salt.utils.files.fopen(maclist, 'r') as ifile:\n            for mac in ifile:\n                mac = salt.utils.stringutils.to_unicode(mac).strip()\n                wol(mac, bcast, destport)\n                print('Waking up {0}'.format(mac))\n                ret.append(mac)\n    except Exception as err:\n        __jid_event__.fire_event({'error': 'Failed to open the MAC file. Error: {0}'.format(err)}, 'progress')\n        return []\n    return ret", "output": "Send a \"Magic Packet\" to wake up a list of Minions.\n    This list must contain one MAC hardware address per line\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run network.wollist '/path/to/maclist'\n        salt-run network.wollist '/path/to/maclist' 255.255.255.255 7\n        salt-run network.wollist '/path/to/maclist' 255.255.255.255 7", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy(self, names=None, dtype=None, levels=None, codes=None,\n             deep=False, _set_identity=False, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        name = kwargs.get('name')\n        names = self._validate_names(name=name, names=names, deep=deep)\n\n        if deep:\n            from copy import deepcopy\n            if levels is None:\n                levels = deepcopy(self.levels)\n            if codes is None:\n                codes = deepcopy(self.codes)\n        else:\n            if levels is None:\n                levels = self.levels\n            if codes is None:\n                codes = self.codes\n        return MultiIndex(levels=levels, codes=codes, names=names,\n                          sortorder=self.sortorder, verify_integrity=False,\n                          _set_identity=_set_identity)", "output": "Make a copy of this object. Names, dtype, levels and codes can be\n        passed and will be set on new copy.\n\n        Parameters\n        ----------\n        names : sequence, optional\n        dtype : numpy dtype or pandas type, optional\n        levels : sequence, optional\n        codes : sequence, optional\n\n        Returns\n        -------\n        copy : MultiIndex\n\n        Notes\n        -----\n        In most cases, there should be no functional difference from using\n        ``deep``, but if ``deep`` is passed it will attempt to deepcopy.\n        This could be potentially expensive on large MultiIndex objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_code_lines(lines):\n    \"\"\"\n    \"\"\"\n    in_code = False\n    lang = None\n    indent = None\n    for l in lines:\n        m = _CODE_MARK.match(l)\n        if m is not None:\n            if not in_code:\n                if m.groups()[1].lower() in _LANGS:\n                    lang = m.groups()[1].lower()\n                    indent = len(m.groups()[0])\n                    in_code = True\n                yield (l, in_code, lang, indent)\n            else:\n                yield (l, in_code, lang, indent)\n                lang = None\n                indent = None\n                in_code = False\n        else:\n            yield (l, in_code, lang, indent)", "output": "A iterator that returns if a line is within a code block\n\n    Returns\n    -------\n    iterator of (str, bool, str, int)\n        - line: the line\n        - in_code: if this line is in a code block\n        - lang: the code block langunage\n        - indent: the code indent", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_virtualserver(self, name):\n        '''\n        \n        '''\n        vs = self.bigIP.LocalLB.VirtualServer\n        for v in vs.get_list():\n            if v.split('/')[-1] == name:\n                return True\n        return False", "output": "Check to see if a virtual server exists", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_SU_save_stock_info_tushare(client=DATABASE):\n    '''\n        \n    '''\n    df = QATs.get_stock_basics()\n    print(\" Get stock info from tushare,stock count is %d\" % len(df))\n    coll = client.stock_info_tushare\n    client.drop_collection(coll)\n    json_data = json.loads(df.reset_index().to_json(orient='records'))\n    coll.insert(json_data)\n    print(\" Save data to stock_info_tushare collection\uff0c OK\")", "output": "\u83b7\u53d6 \u80a1\u7968\u7684 \u57fa\u672c\u4fe1\u606f\uff0c\u5305\u542b\u80a1\u7968\u7684\u5982\u4e0b\u4fe1\u606f\n\n        code,\u4ee3\u7801\n        name,\u540d\u79f0\n        industry,\u6240\u5c5e\u884c\u4e1a\n        area,\u5730\u533a\n        pe,\u5e02\u76c8\u7387\n        outstanding,\u6d41\u901a\u80a1\u672c(\u4ebf)\n        totals,\u603b\u80a1\u672c(\u4ebf)\n        totalAssets,\u603b\u8d44\u4ea7(\u4e07)\n        liquidAssets,\u6d41\u52a8\u8d44\u4ea7\n        fixedAssets,\u56fa\u5b9a\u8d44\u4ea7\n        reserved,\u516c\u79ef\u91d1\n        reservedPerShare,\u6bcf\u80a1\u516c\u79ef\u91d1\n        esp,\u6bcf\u80a1\u6536\u76ca\n        bvps,\u6bcf\u80a1\u51c0\u8d44\n        pb,\u5e02\u51c0\u7387\n        timeToMarket,\u4e0a\u5e02\u65e5\u671f\n        undp,\u672a\u5206\u5229\u6da6\n        perundp, \u6bcf\u80a1\u672a\u5206\u914d\n        rev,\u6536\u5165\u540c\u6bd4(%)\n        profit,\u5229\u6da6\u540c\u6bd4(%)\n        gpr,\u6bdb\u5229\u7387(%)\n        npr,\u51c0\u5229\u6da6\u7387(%)\n        holders,\u80a1\u4e1c\u4eba\u6570\n\n        add by tauruswang\n\n    \u5728\u547d\u4ee4\u884c\u5de5\u5177 quantaxis \u4e2d\u8f93\u5165 save stock_info_tushare \u4e2d\u7684\u547d\u4ee4\n    :param client:\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download_url(url:str, dest:str, overwrite:bool=False, pbar:ProgressBar=None,\n                 show_progress=True, chunk_size=1024*1024, timeout=4, retries=5)->None:\n    \"\"\n    if os.path.exists(dest) and not overwrite: return\n\n    s = requests.Session()\n    s.mount('http://',requests.adapters.HTTPAdapter(max_retries=retries))\n    u = s.get(url, stream=True, timeout=timeout)\n    try: file_size = int(u.headers[\"Content-Length\"])\n    except: show_progress = False\n\n    with open(dest, 'wb') as f:\n        nbytes = 0\n        if show_progress: pbar = progress_bar(range(file_size), auto_update=False, leave=False, parent=pbar)\n        try:\n            for chunk in u.iter_content(chunk_size=chunk_size):\n                nbytes += len(chunk)\n                if show_progress: pbar.update(nbytes)\n                f.write(chunk)\n        except requests.exceptions.ConnectionError as e:\n            fname = url.split('/')[-1]\n            from fastai.datasets import Config\n            data_dir = Config().data_path()\n            timeout_txt =(f'\\n Download of {url} has failed after {retries} retries\\n'\n                          f' Fix the download manually:\\n'\n                          f'$ mkdir -p {data_dir}\\n'\n                          f'$ cd {data_dir}\\n'\n                          f'$ wget -c {url}\\n'\n                          f'$ tar -zxvf {fname}\\n\\n'\n                          f'And re-run your code once the download is successful\\n')\n            print(timeout_txt)\n            import sys;sys.exit(1)", "output": "Download `url` to `dest` unless it exists and not `overwrite`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def freeze_bn(self):\n        ''''''\n        for layer in self.modules():\n            if isinstance(layer, nn.BatchNorm2d):\n                layer.eval()", "output": "Freeze BatchNorm layers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __ip_addr(addr, address_family=socket.AF_INET):\n    '''\n    \n    '''\n    mask_max = '32'\n    if address_family == socket.AF_INET6:\n        mask_max = '128'\n\n    try:\n        if '/' not in addr:\n            addr = '{addr}/{mask_max}'.format(addr=addr, mask_max=mask_max)\n    except TypeError:\n        return False\n\n    ip, mask = addr.rsplit('/', 1)\n\n    # Verify that IP address is valid\n    try:\n        socket.inet_pton(address_family, ip)\n    except socket.error:\n        return False\n\n    # Verify that mask is valid\n    try:\n        mask = int(mask)\n    except ValueError:\n        return False\n    else:\n        if not 1 <= mask <= int(mask_max):\n            return False\n\n    return True", "output": "Returns True if the IP address (and optional subnet) are valid, otherwise\n    returns False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_timedeltalike_scalar(self, other):\n        \"\"\"\n        \n        \"\"\"\n        assert isinstance(self.freq, Tick)  # checked by calling function\n        assert isinstance(other, (timedelta, np.timedelta64, Tick))\n\n        if notna(other):\n            # special handling for np.timedelta64(\"NaT\"), avoid calling\n            #  _check_timedeltalike_freq_compat as that would raise TypeError\n            other = self._check_timedeltalike_freq_compat(other)\n\n        # Note: when calling parent class's _add_timedeltalike_scalar,\n        #  it will call delta_to_nanoseconds(delta).  Because delta here\n        #  is an integer, delta_to_nanoseconds will return it unchanged.\n        ordinals = super()._add_timedeltalike_scalar(other)\n        return ordinals", "output": "Parameters\n        ----------\n        other : timedelta, Tick, np.timedelta64\n\n        Returns\n        -------\n        result : ndarray[int64]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_editor_cursor(self, editor, cursor):\n        \"\"\"\"\"\"\n        pos = cursor.position()\n        anchor = cursor.anchor()\n\n        new_cursor = QTextCursor()\n        if pos == anchor:\n            new_cursor.movePosition(pos)\n        else:\n            new_cursor.movePosition(anchor)\n            new_cursor.movePosition(pos, QTextCursor.KeepAnchor)\n        editor.setTextCursor(cursor)", "output": "Set the cursor of an editor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_data(train_path='./data/regression.train', test_path='./data/regression.test'):\n    '''\n    \n    '''\n    print('Load data...')\n    df_train = pd.read_csv(train_path, header=None, sep='\\t')\n    df_test = pd.read_csv(test_path, header=None, sep='\\t')\n    num = len(df_train)\n    split_num = int(0.9 * num)\n\n    y_train = df_train[0].values\n    y_test = df_test[0].values\n    y_eval = y_train[split_num:]\n    y_train = y_train[:split_num]\n\n    X_train = df_train.drop(0, axis=1).values\n    X_test = df_test.drop(0, axis=1).values\n    X_eval = X_train[split_num:, :]\n    X_train = X_train[:split_num, :]\n\n    # create dataset for lightgbm\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_eval, y_eval, reference=lgb_train)\n\n    return lgb_train, lgb_eval, X_test, y_test", "output": "Load or create dataset", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _quantize_symbol(sym, excluded_symbols=None, offline_params=None, quantized_dtype='int8'):\n    \"\"\"\n    \"\"\"\n    num_excluded_symbols = 0\n    if excluded_symbols is not None:\n        assert isinstance(excluded_symbols, list)\n        num_excluded_symbols = len(excluded_symbols)\n    else:\n        excluded_symbols = []\n\n    num_offline = 0\n    offline = []\n    if offline_params is not None:\n        num_offline = len(offline_params)\n        for k in offline_params:\n            offline.append(c_str(k))\n\n    out = SymbolHandle()\n    check_call(_LIB.MXQuantizeSymbol(sym.handle,\n                                     ctypes.byref(out),\n                                     mx_uint(num_excluded_symbols),\n                                     c_str_array(excluded_symbols),\n                                     mx_uint(num_offline),\n                                     c_array(ctypes.c_char_p, offline),\n                                     c_str(quantized_dtype),\n                                     ctypes.c_bool(True)))\n    return Symbol(out)", "output": "Given a symbol object representing a neural network of data type FP32,\n    quantize it into a INT8 network.\n\n    Parameters\n    ----------\n    sym : Symbol\n        FP32 neural network symbol.\n    excluded_sym_names : list of strings\n        A list of strings representing the names of the symbols that users want to excluding\n        from being quantized.\n    offline_params : list of strs\n        Names of the parameters that users want to quantize offline. It's always recommended to\n        quantize parameters offline so that quantizing parameters during the inference can be\n        avoided.\n    quantized_dtype: str\n        The quantized destination type for input data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_logging(level):\n    \"\"\"\n    \"\"\"\n    format = \"[%(asctime)s] %(levelname)s:%(name)s:%(message)s\"\n    logging.basicConfig(\n        level=level, stream=sys.stdout, format=format, datefmt=\"%Y-%m-%d %H:%M:%S\"\n    )", "output": "Setup basic logging\n    Args:\n      level (int): minimum log level for emitting messages", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_binary(data):\n    '''\n    \n    '''\n    if not data or not isinstance(data, (six.string_types, six.binary_type)):\n        return False\n\n    if isinstance(data, six.binary_type):\n        if b'\\0' in data:\n            return True\n    elif str('\\0') in data:\n        return True\n\n    text_characters = ''.join([chr(x) for x in range(32, 127)] + list('\\n\\r\\t\\b'))\n    # Get the non-text characters (map each character to itself then use the\n    # 'remove' option to get rid of the text characters.)\n    if six.PY3:\n        if isinstance(data, six.binary_type):\n            import salt.utils.data\n            nontext = data.translate(None, salt.utils.data.encode(text_characters))\n        else:\n            trans = ''.maketrans('', '', text_characters)\n            nontext = data.translate(trans)\n    else:\n        if isinstance(data, six.text_type):\n            trans_args = ({ord(x): None for x in text_characters},)\n        else:\n            trans_args = (None, str(text_characters))  # future lint: blacklisted-function\n        nontext = data.translate(*trans_args)\n\n    # If more than 30% non-text characters, then\n    # this is considered binary data\n    if float(len(nontext)) / len(data) > 0.30:\n        return True\n    return False", "output": "Detects if the passed string of data is binary or text", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_dataset_string(self, dataset):\n        \"\"\" \n        \"\"\"\n        if dataset:\n            if '/' not in dataset:\n                raise ValueError('Dataset must be specified in the form of '\n                                 '\\'{username}/{dataset-slug}\\'')\n\n            split = dataset.split('/')\n            if not split[0] or not split[1]:\n                raise ValueError('Invalid dataset specification ' + dataset)", "output": "determine if a dataset string is valid, meaning it is in the format\n            of {username}/{dataset-slug}.\n             Parameters\n            ==========\n            dataset: the dataset name to validate", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cric__random_forest():\n    \"\"\" \n    \"\"\"\n    model = sklearn.ensemble.RandomForestClassifier(100, random_state=0)\n\n    # we want to explain the raw probability outputs of the trees\n    model.predict = lambda X: model.predict_proba(X)[:,1]\n    \n    return model", "output": "Random Forest", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_harddisk_sleep(minutes):\n    '''\n    \n    '''\n    value = _validate_sleep(minutes)\n    cmd = 'systemsetup -setharddisksleep {0}'.format(value)\n    salt.utils.mac_utils.execute_return_success(cmd)\n\n    return salt.utils.mac_utils.confirm_updated(\n        str(value),\n        get_harddisk_sleep,\n    )", "output": "Set the amount of idle time until the harddisk sleeps. Pass \"Never\" of \"Off\"\n    to never sleep.\n\n    :param minutes: Can be an integer between 1 and 180 or \"Never\" or \"Off\"\n    :ptype: int, str\n\n    :return: True if successful, False if not\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' power.set_harddisk_sleep 120\n        salt '*' power.set_harddisk_sleep off", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_all_snapshots(name, kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The remove_all_snapshots action must be called with '\n            '-a or --action.'\n        )\n\n    vm_ref = salt.utils.vmware.get_mor_by_property(_get_si(),\n                                                   vim.VirtualMachine,\n                                                   name)\n\n    try:\n        task = vm_ref.RemoveAllSnapshots()\n        salt.utils.vmware.wait_for_task(task, name, 'remove snapshots', 5, 'info')\n    except Exception as exc:\n        log.error(\n            'Error while removing snapshots on VM %s: %s',\n            name, exc,\n            # Show the traceback if the debug logging level is enabled\n            exc_info_on_loglevel=logging.DEBUG\n        )\n        return 'Failed to remove snapshots'\n\n    return 'Removed all snapshots'", "output": "Remove all the snapshots present for the specified virtual machine.\n\n    .. note::\n\n        All the snapshots higher up in the hierarchy of the current snapshot tree\n        are consolidated and their virtual disks are merged. To override this\n        behavior and only remove all snapshots, set ``merge_snapshots=False``.\n        Default is ``merge_snapshots=True``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a remove_all_snapshots vmname [merge_snapshots=False]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_schema_resource(info):\n    \"\"\"\n    \"\"\"\n    if \"fields\" not in info:\n        return ()\n\n    schema = []\n    for r_field in info[\"fields\"]:\n        name = r_field[\"name\"]\n        field_type = r_field[\"type\"]\n        mode = r_field.get(\"mode\", \"NULLABLE\")\n        description = r_field.get(\"description\")\n        sub_fields = _parse_schema_resource(r_field)\n        schema.append(SchemaField(name, field_type, mode, description, sub_fields))\n    return schema", "output": "Parse a resource fragment into a schema field.\n\n    Args:\n        info: (Mapping[str->dict]): should contain a \"fields\" key to be parsed\n\n    Returns:\n        (Union[Sequence[:class:`google.cloud.bigquery.schema.SchemaField`],None])\n            a list of parsed fields, or ``None`` if no \"fields\" key found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_row_range_from_keys(\n        self, start_key=None, end_key=None, start_inclusive=True, end_inclusive=False\n    ):\n        \"\"\"\n        \"\"\"\n        row_range = RowRange(start_key, end_key, start_inclusive, end_inclusive)\n        self.row_ranges.append(row_range)", "output": "Add row range to row_ranges list from the row keys\n\n        For example:\n\n        .. literalinclude:: snippets_table.py\n            :start-after: [START bigtable_row_range_from_keys]\n            :end-before: [END bigtable_row_range_from_keys]\n\n        :type start_key: bytes\n        :param start_key: (Optional) Start key of the row range. If left empty,\n                          will be interpreted as the empty string.\n\n        :type end_key: bytes\n        :param end_key: (Optional) End key of the row range. If left empty,\n                        will be interpreted as the empty string and range will\n                        be unbounded on the high end.\n\n        :type start_inclusive: bool\n        :param start_inclusive: (Optional) Whether the ``start_key`` should be\n                        considered inclusive. The default is True (inclusive).\n\n        :type end_inclusive: bool\n        :param end_inclusive: (Optional) Whether the ``end_key`` should be\n                  considered inclusive. The default is False (exclusive).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop(dev=None):\n    '''\n    \n\n    '''\n    if dev is not None:\n        log.warning('Stopping %s, device will only reappear after reregistering!', dev)\n        if not _bcsys(dev, 'stop', 'goaway', 'error', 'Error stopping {0}'.format(dev)):\n            return False\n        return _wait(lambda: _sysfs_attr(_bcpath(dev)) is False, 'error', 'Device {0} did not stop'.format(dev), 300)\n    else:\n        cache = uuid()\n        if not cache:\n            log.warning('bcache already stopped?')\n            return None\n\n        if not _alltrue(detach()):\n            return False\n        elif not _fssys('stop', 'goaway', 'error', 'Error stopping cache'):\n            return False\n\n        return _wait(lambda: uuid() is False, 'error', 'Cache did not stop', 300)", "output": "Stop a bcache device\n    If no device is given, all backing devices will be detached from the cache, which will subsequently be stopped.\n\n    .. warning::\n        'Stop' on an individual backing device means hard-stop;\n        no attempt at flushing will be done and the bcache device will seemingly 'disappear' from the device lists\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt '*' bcache.stop", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_experiment(args):\n    ''''''\n    config_file_name = ''.join(random.sample(string.ascii_letters + string.digits, 8))\n    nni_config = Config(config_file_name)\n    config_path = os.path.abspath(args.config)\n    if not os.path.exists(config_path):\n        print_error('Please set correct config path!')\n        exit(1)\n    experiment_config = get_yml_content(config_path)\n    validate_all_content(experiment_config, config_path)\n\n    nni_config.set_config('experimentConfig', experiment_config)\n    launch_experiment(args, experiment_config, 'new', config_file_name)\n    nni_config.set_config('restServerPort', args.port)", "output": "start a new experiment", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_time(time):\n    '''\n    \n    '''\n    # time must be double quoted '\"17:46\"'\n    time_format = _get_date_time_format(time)\n    dt_obj = datetime.strptime(time, time_format)\n\n    cmd = 'systemsetup -settime {0}'.format(dt_obj.strftime('%H:%M:%S'))\n    return salt.utils.mac_utils.execute_return_success(cmd)", "output": "Sets the current time. Must be in 24 hour format.\n\n    :param str time: The time to set in 24 hour format.  The value must be\n        double quoted. ie: '\"17:46\"'\n\n    :return: True if successful, False if not\n    :rtype: bool\n\n    :raises: SaltInvocationError on Invalid Time format\n    :raises: CommandExecutionError on failure\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' timezone.set_time '\"17:34\"'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _jitter(c, magnitude:uniform):\n    \"\"\n    c.flow.add_((torch.rand_like(c.flow)-0.5)*magnitude*2)\n    return c", "output": "Replace pixels by random neighbors at `magnitude`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def info(host=None, port=None, db=None, password=None):\n    '''\n    \n    '''\n    server = _connect(host, port, db, password)\n    return server.info()", "output": "Get information and statistics about the server\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.info", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def with_suffix(self, suffix):\n        \"\"\"\n        \"\"\"\n        # XXX if suffix is None, should the current suffix be removed?\n        f = self._flavour\n        if f.sep in suffix or f.altsep and f.altsep in suffix:\n            raise ValueError(\"Invalid suffix %r\" % (suffix))\n        if suffix and not suffix.startswith('.') or suffix == '.':\n            raise ValueError(\"Invalid suffix %r\" % (suffix))\n        name = self.name\n        if not name:\n            raise ValueError(\"%r has an empty name\" % (self,))\n        old_suffix = self.suffix\n        if not old_suffix:\n            name = name + suffix\n        else:\n            name = name[:-len(old_suffix)] + suffix\n        return self._from_parsed_parts(self._drv, self._root,\n                                       self._parts[:-1] + [name])", "output": "Return a new path with the file suffix changed (or added, if\n        none).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Intersect(self, mask1, mask2):\n    \"\"\"\"\"\"\n    _CheckFieldMaskMessage(mask1)\n    _CheckFieldMaskMessage(mask2)\n    tree = _FieldMaskTree(mask1)\n    intersection = _FieldMaskTree()\n    for path in mask2.paths:\n      tree.IntersectPath(path, intersection)\n    intersection.ToFieldMask(self)", "output": "Intersects mask1 and mask2 into this FieldMask.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_sub_dirs(parent):\n    \"\"\"\"\"\"\n    return [child for child in os.listdir(parent) if os.path.isdir(os.path.join(parent, child))]", "output": "Returns a list of the child directories of the given parent directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def depends(name):\n    '''\n    \n    '''\n    # Resolve tag or short-SHA to full SHA\n    image_id = inspect_image(name)['Id']\n\n    container_depends = []\n    for container in six.itervalues(ps_(all=True, verbose=True)):\n        if container['Info']['Image'] == image_id:\n            container_depends.extend(\n                [x.lstrip('/') for x in container['Names']]\n            )\n\n    return {\n        'Containers': container_depends,\n        'Images': [x[:12] for x, y in six.iteritems(images(all=True))\n                   if y['ParentId'] == image_id]\n    }", "output": "Returns the containers and images, if any, which depend on the given image\n\n    name\n        Name or ID of image\n\n\n    **RETURN DATA**\n\n    A dictionary containing the following keys:\n\n    - ``Containers`` - A list of containers which depend on the specified image\n    - ``Images`` - A list of IDs of images which depend on the specified image\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion docker.depends myimage\n        salt myminion docker.depends 0123456789ab", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toggle_minmax(self, state):\r\n        \"\"\"\"\"\"\r\n        self.sig_option_changed.emit('minmax', state)\r\n        self.model.minmax = state", "output": "Toggle min/max display for numpy arrays", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_latent_pred_loss(latents_pred, latents_discrete_hot, hparams):\n  \"\"\"\"\"\"\n  latents_logits = tf.layers.dense(\n      latents_pred, 2**hparams.bottleneck_bits, name=\"extra_logits\")\n  loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n      labels=tf.stop_gradient(latents_discrete_hot), logits=latents_logits)\n  return loss", "output": "Latent prediction and loss.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate(template_dict, schema=None):\n        \"\"\"\n        \n        \"\"\"\n\n        if not schema:\n            schema = SamTemplateValidator._read_schema()\n\n        validation_errors = \"\"\n\n        try:\n            jsonschema.validate(template_dict, schema)\n        except ValidationError as ex:\n            # Stringifying the exception will give us useful error message\n            validation_errors = str(ex)\n            # Swallowing expected exception here as our caller is expecting validation errors and\n            # not the valiation exception itself\n            pass\n\n        return validation_errors", "output": "Is this a valid SAM template dictionary\n\n        :param dict template_dict: Data to be validated\n        :param dict schema: Optional, dictionary containing JSON Schema representing SAM template\n        :return: Empty string if there are no validation errors in template", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_directory_archive(path):\n    \"\"\"\n    \n    \"\"\"\n    if path is None:\n        return False\n\n    if not _os.path.isdir(path):\n        return False\n\n    ini_path = '/'.join([_convert_slashes(path), 'dir_archive.ini'])\n\n    if not _os.path.exists(ini_path):\n        return False\n\n    if _os.path.isfile(ini_path):\n        return True\n    return False", "output": "Utility function that returns True if the path provided is a directory that has an SFrame or SGraph in it.\n\n    SFrames are written to disk as a directory archive, this function identifies if a given directory is an archive\n    for an SFrame.\n\n    Parameters\n    ----------\n    path : string\n        Directory to evaluate.\n\n    Returns\n    -------\n    True if path provided is an archive location, False otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def minion_config(opts, vm_):\n    '''\n    \n    '''\n\n    # Don't start with a copy of the default minion opts; they're not always\n    # what we need. Some default options are Null, let's set a reasonable default\n    minion = {\n        'master': 'salt',\n        'log_level': 'info',\n        'hash_type': 'sha256',\n    }\n\n    # Now, let's update it to our needs\n    minion['id'] = vm_['name']\n    master_finger = salt.config.get_cloud_config_value('master_finger', vm_, opts)\n    if master_finger is not None:\n        minion['master_finger'] = master_finger\n    minion.update(\n        # Get ANY defined minion settings, merging data, in the following order\n        # 1. VM config\n        # 2. Profile config\n        # 3. Global configuration\n        salt.config.get_cloud_config_value(\n            'minion', vm_, opts, default={}, search_global=True\n        )\n    )\n\n    make_master = salt.config.get_cloud_config_value('make_master', vm_, opts)\n    if 'master' not in minion and make_master is not True:\n        raise SaltCloudConfigError(\n            'A master setting was not defined in the minion\\'s configuration.'\n        )\n\n    # Get ANY defined grains settings, merging data, in the following order\n    # 1. VM config\n    # 2. Profile config\n    # 3. Global configuration\n    minion.setdefault('grains', {}).update(\n        salt.config.get_cloud_config_value(\n            'grains', vm_, opts, default={}, search_global=True\n        )\n    )\n    return minion", "output": "Return a minion's configuration for the provided options and VM", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_query_info(sql, con, partition_column):\n    \"\"\" \n    \"\"\"\n    engine = create_engine(con)\n    if is_table(engine, sql):\n        table_metadata = get_table_metadata(engine, sql)\n        query = build_query_from_table(sql)\n        cols = get_table_columns(table_metadata)\n    else:\n        check_query(sql)\n        query = sql.replace(\";\", \"\")\n        cols = get_query_columns(engine, query)\n    # TODO allow validation that takes into account edge cases of pandas e.g. \"[index]\"\n    # check_partition_column(partition_column, cols)\n    cols_names = list(cols.keys())\n    return cols_names, query", "output": "Return a columns name list and the query string\n\n    Args:\n        sql: SQL query or table name\n        con: database connection or url string\n        partition_column: column used to share the data between the workers\n\n    Returns:\n        Columns name list and query string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_all(logdir, steps, thresholds, verbose=False):\n  \"\"\"\n  \"\"\"\n  # First, we generate data for a PR curve that assigns even weights for\n  # predictions of all classes.\n  run_name = 'colors'\n  if verbose:\n    print('--- Running: %s' % run_name)\n  start_runs(\n      logdir=logdir,\n      steps=steps,\n      run_name=run_name,\n      thresholds=thresholds)\n\n  # Next, we generate data for a PR curve that assigns arbitrary weights to\n  # predictions.\n  run_name = 'mask_every_other_prediction'\n  if verbose:\n    print('--- Running: %s' % run_name)\n  start_runs(\n      logdir=logdir,\n      steps=steps,\n      run_name=run_name,\n      thresholds=thresholds,\n      mask_every_other_prediction=True)", "output": "Generate PR curve summaries.\n\n  Arguments:\n    logdir: The directory into which to store all the runs' data.\n    steps: The number of steps to run for.\n    verbose: Whether to print the names of runs into stdout during execution.\n    thresholds: The number of thresholds to use for PR curves.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def print_table(self, items, fields):\n        \"\"\" \n        \"\"\"\n        formats = []\n        borders = []\n        for f in fields:\n            length = max(\n                len(f), max([len(self.string(getattr(i, f))) for i in items]))\n            justify = '>' if isinstance(getattr(\n                items[0], f), int) or f == 'size' or f == 'reward' else '<'\n            formats.append('{:' + justify + self.string(length + 2) + '}')\n            borders.append('-' * length + '  ')\n        row_format = u''.join(formats)\n        headers = [f + '  ' for f in fields]\n        print(row_format.format(*headers))\n        print(row_format.format(*borders))\n        for i in items:\n            i_fields = [self.string(getattr(i, f)) + '  ' for f in fields]\n            try:\n                print(row_format.format(*i_fields))\n            except UnicodeEncodeError:\n                print(row_format.format(*i_fields).encode('utf-8'))", "output": "print a table of items, for a set of fields defined\n\n            Parameters\n            ==========\n            items: a list of items to print\n            fields: a list of fields to select from items", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_dvportgroup_out_shaping(pg_name, out_shaping, out_shaping_conf):\n    '''\n    \n    '''\n    log.trace('Building portgroup\\'s \\'%s\\' out shaping policy', pg_name)\n    if out_shaping_conf.get('average_bandwidth'):\n        out_shaping.averageBandwidth = vim.LongPolicy()\n        out_shaping.averageBandwidth.value = \\\n                out_shaping_conf['average_bandwidth']\n    if out_shaping_conf.get('burst_size'):\n        out_shaping.burstSize = vim.LongPolicy()\n        out_shaping.burstSize.value = out_shaping_conf['burst_size']\n    if 'enabled' in out_shaping_conf:\n        out_shaping.enabled = vim.BoolPolicy()\n        out_shaping.enabled.value = out_shaping_conf['enabled']\n    if out_shaping_conf.get('peak_bandwidth'):\n        out_shaping.peakBandwidth = vim.LongPolicy()\n        out_shaping.peakBandwidth.value = out_shaping_conf['peak_bandwidth']", "output": "Applies the values in out_shaping_conf to an out_shaping object\n\n    pg_name\n        The name of the portgroup\n\n    out_shaping\n        The vim.DVSTrafficShapingPolicy to apply the config to\n\n    out_shaping_conf\n        The out shaping config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _lowest_mu(x, fun_prediction, fun_prediction_args,\n               x_bounds, x_types, minimize_constraints_fun):\n    '''\n    \n    '''\n    # This is only for step-wise optimization\n    x = lib_data.match_val_type(x, x_bounds, x_types)\n\n    mu = sys.maxsize\n    if (minimize_constraints_fun is None) or (minimize_constraints_fun(x) is True):\n        mu, _ = fun_prediction(x, *fun_prediction_args)\n    return mu", "output": "Calculate the lowest mu", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _copy(src_file, dest_path):\n  \"\"\"\"\"\"\n  tf.io.gfile.makedirs(os.path.dirname(dest_path))\n  with tf.io.gfile.GFile(dest_path, 'wb') as dest_file:\n    while True:\n      data = src_file.read(io.DEFAULT_BUFFER_SIZE)\n      if not data:\n        break\n      dest_file.write(data)", "output": "Copy data read from src file obj to new file in dest_path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def recent_activity(self, user_id):\n        \"\"\"\"\"\"\n        M = models  # noqa\n\n        if request.args.get('limit'):\n            limit = int(request.args.get('limit'))\n        else:\n            limit = 1000\n\n        qry = (\n            db.session.query(M.Log, M.Dashboard, M.Slice)\n            .outerjoin(\n                M.Dashboard,\n                M.Dashboard.id == M.Log.dashboard_id,\n            )\n            .outerjoin(\n                M.Slice,\n                M.Slice.id == M.Log.slice_id,\n            )\n            .filter(\n                sqla.and_(\n                    ~M.Log.action.in_(('queries', 'shortner', 'sql_json')),\n                    M.Log.user_id == user_id,\n                ),\n            )\n            .order_by(M.Log.dttm.desc())\n            .limit(limit)\n        )\n        payload = []\n        for log in qry.all():\n            item_url = None\n            item_title = None\n            if log.Dashboard:\n                item_url = log.Dashboard.url\n                item_title = log.Dashboard.dashboard_title\n            elif log.Slice:\n                item_url = log.Slice.slice_url\n                item_title = log.Slice.slice_name\n\n            payload.append({\n                'action': log.Log.action,\n                'item_url': item_url,\n                'item_title': item_title,\n                'time': log.Log.dttm,\n            })\n        return json_success(\n            json.dumps(payload, default=utils.json_int_dttm_ser))", "output": "Recent activity (actions) for a given user", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_int(b:Any)->Union[int,List[int]]:\n    \"\"\n    if is_listy(b): return [to_int(x) for x in b]\n    else:          return int(b)", "output": "Convert `b` to an int or list of ints (if `is_listy`); raises exception if not convertible", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _load_into_numpy(sf, np_array, start, end, strides=None, shape=None):\n    \"\"\"\"\"\"\n    np_array[:] = 0.0\n    np_array_2d = np_array.reshape((np_array.shape[0], np_array.shape[1] * np_array.shape[2]))\n    _extensions.sframe_load_to_numpy(sf, np_array.ctypes.data,\n                                     np_array_2d.strides, np_array_2d.shape,\n                                     start, end)", "output": "Loads into numpy array from SFrame, assuming SFrame stores data flattened", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dilated_attention_1d(x,\n                         hparams,\n                         attention_type=\"masked_dilated_1d\",\n                         q_padding=\"VALID\",\n                         kv_padding=\"VALID\",\n                         gap_size=2):\n  \"\"\"\"\"\"\n  # self-attention\n  x, x_shape, is_4d = maybe_reshape_4d_to_3d(x)\n  with tf.variable_scope(\"masked_dilated_1d\"):\n    y = common_attention.multihead_attention(\n        x,\n        None,\n        None,\n        hparams.attention_key_channels or hparams.hidden_size,\n        hparams.attention_value_channels or hparams.hidden_size,\n        hparams.hidden_size,\n        hparams.num_heads,\n        hparams.attention_dropout,\n        attention_type=attention_type,\n        block_width=hparams.block_width,\n        block_length=hparams.block_length,\n        q_padding=q_padding,\n        kv_padding=kv_padding,\n        q_filter_width=hparams.q_filter_width,\n        kv_filter_width=hparams.kv_filter_width,\n        gap_size=gap_size,\n        num_memory_blocks=hparams.num_memory_blocks,\n        name=\"self_attention\")\n    if is_4d:\n      y = tf.reshape(y, x_shape)\n      y.set_shape([None, None, None, hparams.hidden_size])\n    return y", "output": "Dilated 1d self attention.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def envs():\n    '''\n    \n    '''\n    saltenvs = []\n    for container in __opts__.get('azurefs', []):\n        saltenvs.append(container.get('saltenv', 'base'))\n    # Remove duplicates\n    return list(set(saltenvs))", "output": "Each container configuration can have an environment setting, or defaults\n    to base", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __assert_false(returned):\n        '''\n        \n        '''\n        result = \"Pass\"\n        if isinstance(returned, str):\n            try:\n                returned = bool(returned)\n            except ValueError:\n                raise\n        try:\n            assert (returned is False), \"{0} not False\".format(returned)\n        except AssertionError as err:\n            result = \"Fail: \" + six.text_type(err)\n        return result", "output": "Test if an boolean is False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset(self):\n        \"\"\"\"\"\"\n        if self.seq is not None and self.shuffle:\n            random.shuffle(self.seq)\n        if self.last_batch_handle != 'roll_over' or \\\n            self._cache_data is None:\n            if self.imgrec is not None:\n                self.imgrec.reset()\n            self.cur = 0\n            if self._allow_read is False:\n                self._allow_read = True", "output": "Resets the iterator to the beginning of the data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def issue_tags(issue):\n    \"\"\"\"\"\"\n    labels = issue.get('labels', [])\n    return [label['name'].replace('tag: ', '') for label in labels if label['name'].startswith('tag: ')]", "output": "Returns list of tags for this issue.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_from_template(zone, template):\n    '''\n    \n    '''\n    ret = {'status': True}\n\n    # create from template\n    _dump_cfg(template)\n    res = __salt__['cmd.run_all']('zonecfg -z {zone} create -t {tmpl} -F'.format(\n        zone=zone,\n        tmpl=template,\n    ))\n    ret['status'] = res['retcode'] == 0\n    ret['message'] = res['stdout'] if ret['status'] else res['stderr']\n    if ret['message'] == '':\n        del ret['message']\n    else:\n        ret['message'] = _clean_message(ret['message'])\n\n    return ret", "output": "Create an in-memory configuration from a template for the specified zone.\n\n    zone : string\n        name of zone\n    template : string\n        name of template\n\n    .. warning::\n        existing config will be overwritten!\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zonecfg.create_from_template leo tallgeese", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_path_to_flask(path):\n        \"\"\"\n        \n        \"\"\"\n        proxy_sub_path = APIGW_TO_FLASK_REGEX.sub(FLASK_CAPTURE_ALL_PATH, path)\n\n        # Replace the '{' and '}' with '<' and '>' respectively\n        return proxy_sub_path.replace(LEFT_BRACKET, LEFT_ANGLE_BRACKET).replace(RIGHT_BRACKET, RIGHT_ANGLE_BRACKET)", "output": "Converts a Path from an Api Gateway defined path to one that is accepted by Flask\n\n        Examples:\n\n        '/id/{id}' => '/id/<id>'\n        '/{proxy+}' => '/<path:proxy>'\n\n        :param str path: Path to convert to Flask defined path\n        :return str: Path representing a Flask path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_bbox_from_albumentations(bbox, target_format, rows, cols, check_validity=False):\n    \"\"\"\n\n    \"\"\"\n    if target_format not in {'coco', 'pascal_voc'}:\n        raise ValueError(\n            \"Unknown target_format {}. Supported formats are: 'coco' and 'pascal_voc'\".format(target_format)\n        )\n    if check_validity:\n        check_bbox(bbox)\n    bbox = denormalize_bbox(bbox, rows, cols)\n    if target_format == 'coco':\n        x_min, y_min, x_max, y_max = bbox[:4]\n        width = x_max - x_min\n        height = y_max - y_min\n        bbox = [x_min, y_min, width, height] + list(bbox[4:])\n    return bbox", "output": "Convert a bounding box from the format used by albumentations to a format, specified in `target_format`.\n\n    Args:\n        bbox (list): bounding box with coordinates in the format used by albumentations\n        target_format (str): required format of the output bounding box. Should be 'coco' or 'pascal_voc'.\n        rows (int): image height\n        cols (int): image width\n        check_validity (bool): check if all boxes are valid boxes\n\n    Note:\n        The `coco` format of a bounding box looks like `[x_min, y_min, width, height]`, e.g. [97, 12, 150, 200].\n        The `pascal_voc` format of a bounding box looks like `[x_min, y_min, x_max, y_max]`, e.g. [97, 12, 247, 212].\n\n    Raises:\n        ValueError: if `target_format` is not equal to `coco` or `pascal_voc`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image2np(image:Tensor)->np.ndarray:\n    \"\"\n    res = image.cpu().permute(1,2,0).numpy()\n    return res[...,0] if res.shape[2]==1 else res", "output": "Convert from torch style `image` to numpy/matplotlib style.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def text(self, paths, wholetext=False, lineSep=None):\n        \"\"\"\n        \n        \"\"\"\n        self._set_opts(wholetext=wholetext, lineSep=lineSep)\n        if isinstance(paths, basestring):\n            paths = [paths]\n        return self._df(self._jreader.text(self._spark._sc._jvm.PythonUtils.toSeq(paths)))", "output": "Loads text files and returns a :class:`DataFrame` whose schema starts with a\n        string column named \"value\", and followed by partitioned columns if there\n        are any.\n        The text files must be encoded as UTF-8.\n\n        By default, each line in the text file is a new row in the resulting DataFrame.\n\n        :param paths: string, or list of strings, for input path(s).\n        :param wholetext: if true, read each file from input path(s) as a single row.\n        :param lineSep: defines the line separator that should be used for parsing. If None is\n                        set, it covers all ``\\\\r``, ``\\\\r\\\\n`` and ``\\\\n``.\n\n        >>> df = spark.read.text('python/test_support/sql/text-test.txt')\n        >>> df.collect()\n        [Row(value=u'hello'), Row(value=u'this')]\n        >>> df = spark.read.text('python/test_support/sql/text-test.txt', wholetext=True)\n        >>> df.collect()\n        [Row(value=u'hello\\\\nthis')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def htmlSetMetaEncoding(self, encoding):\n        \"\"\" \"\"\"\n        ret = libxml2mod.htmlSetMetaEncoding(self._o, encoding)\n        return ret", "output": "Sets the current encoding in the Meta tags NOTE: this will\n          not change the document content encoding, just the META\n           flag associated.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write(self, data, show_progress=False, invalid_data_behavior='warn'):\n        \"\"\"\n        \"\"\"\n        ctx = maybe_show_progress(\n            data,\n            show_progress=show_progress,\n            item_show_func=lambda e: e if e is None else str(e[0]),\n            label=\"Merging minute equity files:\",\n        )\n        write_sid = self.write_sid\n        with ctx as it:\n            for e in it:\n                write_sid(*e, invalid_data_behavior=invalid_data_behavior)", "output": "Write a stream of minute data.\n\n        Parameters\n        ----------\n        data : iterable[(int, pd.DataFrame)]\n            The data to write. Each element should be a tuple of sid, data\n            where data has the following format:\n              columns : ('open', 'high', 'low', 'close', 'volume')\n                  open : float64\n                  high : float64\n                  low  : float64\n                  close : float64\n                  volume : float64|int64\n              index : DatetimeIndex of market minutes.\n            A given sid may appear more than once in ``data``; however,\n            the dates must be strictly increasing.\n        show_progress : bool, optional\n            Whether or not to show a progress bar while writing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_filename(filename, shorten=False):\n    \"\"\"\n    \"\"\"\n    if shorten:\n        filename = os.path.basename(filename)\n    return filename_to_ui(filename)", "output": "Formats a filename for user display.  The main purpose of this\n    function is to ensure that the filename can be displayed at all.  This\n    will decode the filename to unicode if necessary in a way that it will\n    not fail.  Optionally, it can shorten the filename to not include the\n    full path to the filename.\n\n    :param filename: formats a filename for UI display.  This will also convert\n                     the filename into unicode without failing.\n    :param shorten: this optionally shortens the filename to strip of the\n                    path that leads up to it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_url(url, _params):\n    \"\"\"\"\"\"\n\n    # Support for unicode domain names and paths.\n    scheme, netloc, path, params, query, fragment = urlparse(url)\n    netloc = netloc.encode('idna').decode('utf-8')\n    if not path:\n        path = '/'\n\n    if six.PY2:\n        if isinstance(scheme, six.text_type):\n            scheme = scheme.encode('utf-8')\n        if isinstance(netloc, six.text_type):\n            netloc = netloc.encode('utf-8')\n        if isinstance(path, six.text_type):\n            path = path.encode('utf-8')\n        if isinstance(params, six.text_type):\n            params = params.encode('utf-8')\n        if isinstance(query, six.text_type):\n            query = query.encode('utf-8')\n        if isinstance(fragment, six.text_type):\n            fragment = fragment.encode('utf-8')\n\n    enc_params = _encode_params(_params)\n    if enc_params:\n        if query:\n            query = '%s&%s' % (query, enc_params)\n        else:\n            query = enc_params\n    url = (urlunparse([scheme, netloc, path, params, query, fragment]))\n    return url", "output": "Build the actual URL to use.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _windows_wwns():\n    '''\n    \n    '''\n    ps_cmd = r'Get-WmiObject -ErrorAction Stop ' \\\n             r'-class MSFC_FibrePortHBAAttributes ' \\\n             r'-namespace \"root\\WMI\" | ' \\\n             r'Select -Expandproperty Attributes | ' \\\n             r'%{($_.PortWWN | % {\"{0:x2}\" -f $_}) -join \"\"}'\n    ret = []\n    cmd_ret = salt.modules.cmdmod.powershell(ps_cmd)\n    for line in cmd_ret:\n        ret.append(line.rstrip())\n    return ret", "output": "Return Fibre Channel port WWNs from a Windows host.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _clean_kwargs(keep_name=False, **kwargs):\n    '''\n    \n    '''\n    if 'name' in kwargs and not keep_name:\n        kwargs['name_or_id'] = kwargs.pop('name')\n\n    return __utils__['args.clean_kwargs'](**kwargs)", "output": "Sanatize the the arguments for use with shade", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_minion_id():\n    '''\n    \n    '''\n    try:\n        ret = salt.utils.stringutils.to_unicode(_generate_minion_id().first())\n    except TypeError:\n        ret = None\n    return ret or 'localhost'", "output": "Return only first element of the hostname from all possible list.\n\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _notify_change(self, model, attr, old, new, hint=None, setter=None, callback_invoker=None):\n        ''' \n\n        '''\n        # if name changes, update by-name index\n        if attr == 'name':\n            if old is not None:\n                self._all_models_by_name.remove_value(old, model)\n            if new is not None:\n                self._all_models_by_name.add_value(new, model)\n\n        if hint is None:\n            serializable_new = model.lookup(attr).serializable_value(model)\n        else:\n            serializable_new = None\n\n        event = ModelChangedEvent(self, model, attr, old, new, serializable_new, hint, setter, callback_invoker)\n        self._trigger_on_change(event)", "output": "Called by Model when it changes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_raw_xsrf_token(self) -> Tuple[Optional[int], bytes, float]:\n        \"\"\"\n        \"\"\"\n        if not hasattr(self, \"_raw_xsrf_token\"):\n            cookie = self.get_cookie(\"_xsrf\")\n            if cookie:\n                version, token, timestamp = self._decode_xsrf_token(cookie)\n            else:\n                version, token, timestamp = None, None, None\n            if token is None:\n                version = None\n                token = os.urandom(16)\n                timestamp = time.time()\n            assert token is not None\n            assert timestamp is not None\n            self._raw_xsrf_token = (version, token, timestamp)\n        return self._raw_xsrf_token", "output": "Read or generate the xsrf token in its raw form.\n\n        The raw_xsrf_token is a tuple containing:\n\n        * version: the version of the cookie from which this token was read,\n          or None if we generated a new token in this request.\n        * token: the raw token data; random (non-ascii) bytes.\n        * timestamp: the time this token was generated (will not be accurate\n          for version 1 cookies)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _nbytes(self, deep=False):\n        \"\"\"\n        \n\n        \"\"\"\n\n        # for implementations with no useful getsizeof (PyPy)\n        objsize = 24\n\n        level_nbytes = sum(i.memory_usage(deep=deep) for i in self.levels)\n        label_nbytes = sum(i.nbytes for i in self.codes)\n        names_nbytes = sum(getsizeof(i, objsize) for i in self.names)\n        result = level_nbytes + label_nbytes + names_nbytes\n\n        # include our engine hashtable\n        result += self._engine.sizeof(deep=deep)\n        return result", "output": "return the number of bytes in the underlying data\n        deeply introspect the level data if deep=True\n\n        include the engine hashtable\n\n        *this is in internal routine*", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_pb(self):\n        \"\"\"\n        \"\"\"\n        chain = data_v2_pb2.RowFilter.Chain(\n            filters=[row_filter.to_pb() for row_filter in self.filters]\n        )\n        return data_v2_pb2.RowFilter(chain=chain)", "output": "Converts the row filter to a protobuf.\n\n        :rtype: :class:`.data_v2_pb2.RowFilter`\n        :returns: The converted current object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ext_pillar(minion_id,\n               pillar,  # pylint: disable=W0613\n               url,\n               with_grains=False):\n    '''\n    \n    '''\n\n    url = url.replace('%s', _quote(minion_id))\n\n    grain_pattern = r'<(?P<grain_name>.*?)>'\n\n    if with_grains:\n        # Get the value of the grain and substitute each grain\n        # name for the url-encoded version of its grain value.\n        for match in re.finditer(grain_pattern, url):\n            grain_name = match.group('grain_name')\n            grain_value = __salt__['grains.get'](grain_name, None)\n\n            if not grain_value:\n                log.error(\"Unable to get minion '%s' grain: %s\", minion_id, grain_name)\n                return {}\n\n            grain_value = _quote(six.text_type(grain_value))\n            url = re.sub('<{0}>'.format(grain_name), grain_value, url)\n\n    log.debug('Getting url: %s', url)\n    data = __salt__['http.query'](url=url, decode=True, decode_type='json')\n\n    if 'dict' in data:\n        return data['dict']\n\n    log.error(\"Error on minion '%s' http query: %s\\nMore Info:\\n\", minion_id, url)\n\n    for key in data:\n        log.error('%s: %s', key, data[key])\n\n    return {}", "output": "Read pillar data from HTTP response.\n\n    :param str url: Url to request.\n    :param bool with_grains: Whether to substitute strings in the url with their grain values.\n\n    :return: A dictionary of the pillar data to add.\n    :rtype: dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def summarize_metrics(eval_metrics_writer, metrics, epoch):\n  \"\"\"\"\"\"\n  for (name, value) in six.iteritems(metrics):\n    summary = tf.Summary()\n    summary.value.add(tag=name, simple_value=value)\n    eval_metrics_writer.add_summary(summary, epoch)\n  eval_metrics_writer.flush()", "output": "Write metrics to summary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add (self, ps):\n        \"\"\" \n        \"\"\"\n        assert isinstance(ps, PropertySet)\n        if ps not in self.added_:\n            self.added_[ps] = create(self.all_ + ps.all())\n        return self.added_[ps]", "output": "Creates a new property set containing the properties in this one,\n            plus the ones of the property set passed as argument.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def disassociate_api_key_stagekeys(apiKey, stagekeyslist, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        pvlist = [('/stages', stagekey) for stagekey in stagekeyslist]\n        response = _api_key_patch_remove(conn, apiKey, pvlist)\n        return {'disassociated': True}\n    except ClientError as e:\n        return {'disassociated': False, 'error': __utils__['boto3.get_error'](e)}", "output": "disassociate the given stagekeyslist to the given apiKey.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_apigateway.disassociate_stagekeys_api_key \\\\\n                api_key '[\"restapi id/stage name\", ...]'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _snapper_post(opts, jid, pre_num):\n    '''\n    \n    '''\n    try:\n        if not opts['test'] and __opts__.get('snapper_states') and pre_num:\n            # Run the snapper pre snapshot\n            __salt__['snapper.create_snapshot'](\n                    config=__opts__.get('snapper_states_config', 'root'),\n                    snapshot_type='post',\n                    pre_number=pre_num,\n                    description='Salt State run for jid {0}'.format(jid),\n                    __pub_jid=jid)\n    except Exception:\n        log.error('Failed to create snapper pre snapshot for jid: %s', jid)", "output": "Create the post states snapshot", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_examples(self, num_examples, data_path, label_path):\n    \"\"\"\n    \"\"\"\n    images = _extract_mnist_images(data_path, num_examples)\n    labels = _extract_mnist_labels(label_path, num_examples)\n    data = list(zip(images, labels))\n\n    # Data is shuffled automatically to distribute classes uniformly.\n    for image, label in data:\n      yield {\n          \"image\": image,\n          \"label\": label,\n      }", "output": "Generate MNIST examples as dicts.\n\n    Args:\n      num_examples (int): The number of example.\n      data_path (str): Path to the data files\n      label_path (str): Path to the labels\n\n    Yields:\n      Generator yielding the next examples", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_list(profile=None, **connection_args):\n    '''\n    \n    '''\n    kstone = auth(profile, **connection_args)\n    ret = {}\n    for user in kstone.users.list():\n        ret[user.name] = dict((value, getattr(user, value, None)) for value in dir(user)\n                              if not value.startswith('_') and\n                              isinstance(getattr(user, value, None), (six.string_types, dict, bool)))\n        tenant_id = getattr(user, 'tenantId', None)\n        if tenant_id:\n            ret[user.name]['tenant_id'] = tenant_id\n    return ret", "output": "Return a list of available users (keystone user-list)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystone.user_list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_message(message, msg_type=None,\n        text_color=None, back_color=None, scroll_speed=0.1):\n    '''\n    \n    '''\n    text_color = text_color or [255, 255, 255]\n    back_color = back_color or [0, 0, 0]\n\n    color_by_type = {\n        'error': [255, 0, 0],\n        'warning': [255, 100, 0],\n        'success': [0, 255, 0],\n        'info': [0, 0, 255]\n    }\n\n    if msg_type in color_by_type:\n        text_color = color_by_type[msg_type]\n\n    _sensehat.show_message(message, scroll_speed, text_color, back_color)\n    return {'message': message}", "output": "Displays a message on the LED matrix.\n\n    message\n        The message to display\n    msg_type\n        The type of the message. Changes the appearance of the message.\n\n        Available types are::\n\n            error:      red text\n            warning:    orange text\n            success:    green text\n            info:       blue text\n\n    scroll_speed\n        The speed at which the message moves over the LED matrix.\n        This value represents the time paused for between shifting the text\n        to the left by one column of pixels. Defaults to '0.1'.\n    text_color\n        The color in which the message is shown. Defaults to '[255, 255, 255]' (white).\n    back_color\n        The background color of the display. Defaults to '[0, 0, 0]' (black).\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'raspberry' sensehat.show_message 'Status ok'\n        salt 'raspberry' sensehat.show_message 'Something went wrong' error\n        salt 'raspberry' sensehat.show_message 'Red' text_color='[255, 0, 0]'\n        salt 'raspberry' sensehat.show_message 'Hello world' None '[0, 0, 255]' '[255, 255, 0]' 0.2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_util_random_with_topic(topic='Acc', lens=8):\n    \"\"\"\n    \n\n    \"\"\"\n    _list = [chr(i) for i in range(65,\n                                   91)] + [chr(i) for i in range(97,\n                                                                 123)\n                                          ] + [str(i) for i in range(10)]\n\n    num = random.sample(_list, lens)\n    return '{}_{}'.format(topic, ''.join(num))", "output": "\u751f\u6210account\u968f\u673a\u503c\n\n    Acc+4\u6570\u5b57id+4\u4f4d\u5927\u5c0f\u5199\u968f\u673a", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def destroy(zpool, force=False):\n    '''\n    \n\n    '''\n    # destroy zpool\n    res = __salt__['cmd.run_all'](\n        __utils__['zfs.zpool_command'](\n            command='destroy',\n            flags=['-f'] if force else None,\n            target=zpool,\n        ),\n        python_shell=False,\n    )\n\n    return __utils__['zfs.parse_command_result'](res, 'destroyed')", "output": "Destroys a storage pool\n\n    zpool : string\n        Name of storage pool\n\n    force : boolean\n        Force destroy of pool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zpool.destroy myzpool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _nested_op(inputs, op):  # pylint: disable=invalid-name\n  \"\"\"\"\"\"\n  # First the simple non-nested case.\n  if not isinstance(inputs[0], (list, tuple)):\n    return op(inputs)\n  # In the nested case, sum on each axis separately.\n  result_list = []\n  for i in range(len(inputs[0])):\n    result_list.append(_nested_op([x[i] for x in inputs], op=op))\n  if isinstance(inputs[0], list):\n    return result_list\n  return tuple(result_list)", "output": "Helper: sum a list of arrays or nested arrays.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reload_(name):\n    '''\n    \n\n    '''\n    if __SYSLOG_NG_BINARY_PATH:\n        syslog_ng_ctl_binary = os.path.join(__SYSLOG_NG_BINARY_PATH,\n                                            'syslog-ng-ctl')\n        command = [syslog_ng_ctl_binary, 'reload']\n        result = __salt__['cmd.run_all'](command, python_shell=False)\n    else:\n        command = ['syslog-ng-ctl', 'reload']\n        result = __salt__['cmd.run_all'](command, python_shell=False)\n\n    succ = True if result['retcode'] == 0 else False\n    return _format_state_result(name, result=succ, comment=result['stdout'])", "output": "Reloads syslog-ng. This function is intended to be used from states.\n\n    If :mod:`syslog_ng.set_config_file\n    <salt.modules.syslog_ng.set_binary_path>`, is called before, this function\n    will use the set binary path.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' syslog_ng.reload", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download_dataset(storage_client, image_batches, target_dir,\n                     local_dataset_copy=None):\n  \"\"\"\n  \"\"\"\n  for batch_id, batch_value in iteritems(image_batches.data):\n    batch_dir = os.path.join(target_dir, batch_id)\n    os.mkdir(batch_dir)\n    for image_id, image_val in iteritems(batch_value['images']):\n      dst_filename = os.path.join(batch_dir, image_id + '.png')\n      # try to use local copy first\n      if local_dataset_copy:\n        local_filename = os.path.join(local_dataset_copy,\n                                      os.path.basename(image_val['image_path']))\n        if os.path.exists(local_filename):\n          shutil.copyfile(local_filename, dst_filename)\n          continue\n      # download image from cloud\n      cloud_path = ('gs://' + storage_client.bucket_name\n                    + '/' + image_val['image_path'])\n      if not os.path.exists(dst_filename):\n        subprocess.call(['gsutil', 'cp', cloud_path, dst_filename])", "output": "Downloads dataset, organize it by batches and rename images.\n\n  Args:\n    storage_client: instance of the CompetitionStorageClient\n    image_batches: subclass of ImageBatchesBase with data about images\n    target_dir: target directory, should exist and be empty\n    local_dataset_copy: directory with local dataset copy, if local copy is\n      available then images will be takes from there instead of Cloud Storage\n\n  Data in the target directory will be organized into subdirectories by batches,\n  thus path to each image will be \"target_dir/BATCH_ID/IMAGE_ID.png\"\n  where BATCH_ID - ID of the batch (key of image_batches.data),\n  IMAGE_ID - ID of the image (key of image_batches.data[batch_id]['images'])", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _infer_schema(row, names=None):\n    \"\"\"\"\"\"\n    if isinstance(row, dict):\n        items = sorted(row.items())\n\n    elif isinstance(row, (tuple, list)):\n        if hasattr(row, \"__fields__\"):  # Row\n            items = zip(row.__fields__, tuple(row))\n        elif hasattr(row, \"_fields\"):  # namedtuple\n            items = zip(row._fields, tuple(row))\n        else:\n            if names is None:\n                names = ['_%d' % i for i in range(1, len(row) + 1)]\n            elif len(names) < len(row):\n                names.extend('_%d' % i for i in range(len(names) + 1, len(row) + 1))\n            items = zip(names, row)\n\n    elif hasattr(row, \"__dict__\"):  # object\n        items = sorted(row.__dict__.items())\n\n    else:\n        raise TypeError(\"Can not infer schema for type: %s\" % type(row))\n\n    fields = [StructField(k, _infer_type(v), True) for k, v in items]\n    return StructType(fields)", "output": "Infer the schema from dict/namedtuple/object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unescape_unicode(string):\n    \"\"\"\n    \"\"\"\n    if string is None:\n        return string\n    # We only want to unescape the unicode, so we first must protect the other\n    # backslashes.\n    string = string.replace(\"\\\\\", \"\\\\\\\\\")\n    # Now we remove that protection for the unicode.\n    string = string.replace(\"\\\\\\\\u\", \"\\\\u\")\n    string = string.replace(\"\\\\\\\\U\", \"\\\\U\")\n    # Now we unescape by evaling the string with the AST. This can't execute\n    # code -- it only does the representational level.\n    return ast.literal_eval(\"u'''\" + string + \"'''\")", "output": "Python2.7's re module chokes when compiling patterns that have ranges\n    between escaped unicode codepoints if the two codepoints are unrecognised\n    in the unicode database. For instance:\n\n        re.compile('[\\\\uAA77-\\\\uAA79]').findall(\"hello\")\n\n    Ends up matching every character (on Python 2). This problem doesn't occur\n    if we're dealing with unicode literals.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ToJsonName(name):\n  \"\"\"\"\"\"\n  capitalize_next = False\n  result = []\n\n  for c in name:\n    if c == '_':\n      capitalize_next = True\n    elif capitalize_next:\n      result.append(c.upper())\n      capitalize_next = False\n    else:\n      result += c\n\n  return ''.join(result)", "output": "Converts name to Json name and returns it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_lists(cls, path:PathOrStr, fnames:FilePathList, labels:Collection[str], valid_pct:float=0.2,\n                   item_cls:Callable=None, **kwargs):\n        \"\"\n        item_cls = ifnone(item_cls, ImageList)\n        fname2label = {f:l for (f,l) in zip(fnames, labels)}\n        src = (item_cls(fnames, path=path).split_by_rand_pct(valid_pct)\n                                .label_from_func(lambda x:fname2label[x]))\n        return cls.create_from_ll(src, **kwargs)", "output": "Create from list of `fnames` in `path`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getent(refresh=False, root=None):\n    '''\n    \n    '''\n    if 'group.getent' in __context__ and not refresh:\n        return __context__['group.getent']\n\n    ret = []\n    if root is not None:\n        getgrall = functools.partial(_getgrall, root=root)\n    else:\n        getgrall = functools.partial(grp.getgrall)\n\n    for grinfo in getgrall():\n        ret.append(_format_info(grinfo))\n    __context__['group.getent'] = ret\n    return ret", "output": "Return info on all groups\n\n    refresh\n        Force a refresh of group information\n\n    root\n        Directory to chroot into\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' group.getent", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def saveAsTextFiles(self, prefix, suffix=None):\n        \"\"\"\n        \n        \"\"\"\n        def saveAsTextFile(t, rdd):\n            path = rddToFileName(prefix, suffix, t)\n            try:\n                rdd.saveAsTextFile(path)\n            except Py4JJavaError as e:\n                # after recovered from checkpointing, the foreachRDD may\n                # be called twice\n                if 'FileAlreadyExistsException' not in str(e):\n                    raise\n        return self.foreachRDD(saveAsTextFile)", "output": "Save each RDD in this DStream as at text file, using string\n        representation of elements.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resource_group_create_or_update(name, location, **kwargs):  # pylint: disable=invalid-name\n    '''\n    \n\n    '''\n    result = {}\n    resconn = __utils__['azurearm.get_client']('resource', **kwargs)\n    resource_group_params = {\n        'location': location,\n        'managed_by': kwargs.get('managed_by'),\n        'tags': kwargs.get('tags'),\n    }\n    try:\n        group = resconn.resource_groups.create_or_update(name, resource_group_params)\n        result = group.as_dict()\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('resource', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Create or update a resource group in a given location.\n\n    :param name: The name of the resource group to create or update.\n\n    :param location: The location of the resource group. This value\n        is not able to be updated once the resource group is created.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_resource.resource_group_create_or_update testgroup westus", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ensure_tf_install():  # pylint: disable=g-statement-before-imports\n  \"\"\"\n  \"\"\"\n  try:\n    import tensorflow as tf\n  except ImportError:\n    # Print more informative error message, then reraise.\n    print(\"\\n\\nFailed to import TensorFlow. Please note that TensorFlow is not \"\n          \"installed by default when you install TensorFlow Datasets. This is \"\n          \"so that users can decide whether to install the GPU-enabled \"\n          \"TensorFlow package. To use TensorFlow Datasets, please install the \"\n          \"most recent version of TensorFlow, by following instructions at \"\n          \"https://tensorflow.org/install.\\n\\n\")\n    raise\n\n  tf_version = distutils.version.LooseVersion(tf.__version__)\n  v_1_12 = distutils.version.LooseVersion(\"1.12.0\")\n  if tf_version < v_1_12:\n    raise ImportError(\n        \"This version of TensorFlow Datasets requires TensorFlow \"\n        \"version >= {required}; Detected an installation of version {present}. \"\n        \"Please upgrade TensorFlow to proceed.\".format(\n            required=\"1.12.0\",\n            present=tf.__version__))\n  _patch_tf(tf)", "output": "Attempt to import tensorflow, and ensure its version is sufficient.\n\n  Raises:\n    ImportError: if either tensorflow is not importable or its version is\n    inadequate.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def persistent_load(self, pid):\n        \"\"\"\n        \n        \"\"\"\n        if len(pid) == 2:\n            # Pre GLC-1.3 release behavior, without memorization\n            type_tag, filename = pid\n            abs_path = _os.path.join(self.gl_temp_storage_path, filename)\n            return  _get_gl_object_from_persistent_id(type_tag, abs_path)\n        else:\n            # Post GLC-1.3 release behavior, with memorization\n            type_tag, filename, object_id = pid\n            if object_id in self.gl_object_memo:\n                return self.gl_object_memo[object_id]\n            else:\n                abs_path = _os.path.join(self.gl_temp_storage_path, filename)\n                obj = _get_gl_object_from_persistent_id(type_tag, abs_path)\n                self.gl_object_memo[object_id] = obj\n                return obj", "output": "Reconstruct a GLC object using the persistent ID.\n\n        This method should not be used externally. It is required by the unpickler super class.\n\n        Parameters\n        ----------\n        pid      : The persistent ID used in pickle file to save the GLC object.\n\n        Returns\n        ----------\n        The GLC object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def components(self):\n        \"\"\"\n        \n        \"\"\"\n        from pandas import DataFrame\n\n        columns = ['days', 'hours', 'minutes', 'seconds',\n                   'milliseconds', 'microseconds', 'nanoseconds']\n        hasnans = self._hasnans\n        if hasnans:\n            def f(x):\n                if isna(x):\n                    return [np.nan] * len(columns)\n                return x.components\n        else:\n            def f(x):\n                return x.components\n\n        result = DataFrame([f(x) for x in self], columns=columns)\n        if not hasnans:\n            result = result.astype('int64')\n        return result", "output": "Return a dataframe of the components (days, hours, minutes,\n        seconds, milliseconds, microseconds, nanoseconds) of the Timedeltas.\n\n        Returns\n        -------\n        a DataFrame", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mtf_transformer_paper_lm(size):\n  \"\"\"\n  \"\"\"\n  n = 2 ** size\n  hparams = mtf_transformer_base_lm()\n  hparams.batch_size = 256\n  hparams.d_model = 1024\n  hparams.d_ff = int(8192 * n)\n  hparams.d_kv = 256\n  hparams.num_heads = int(8 * n)\n  hparams.shared_embedding_and_softmax_weights = False\n  # one epoch for languagemodel_lm1b32k_packed = 13600 steps\n  hparams.learning_rate_decay_steps = 13600\n  return hparams", "output": "Config for language-model experiments.\n\n  Train these on languagemodel_lm1b32k_packed for 136000 steps (10 epochs)\n\n  The size parameter is an integer that controls the number of heads and the\n  size of the size of the feedforward hidden layers.  Increasing size by 1\n  doubles each of these.\n\n  Results:\n  size   params/10^9  log-ppl(per-token)\n  -1     0.14         3.209\n  0      0.22         3.119\n  1      0.37         3.037\n  2      0.67         2.969\n  3      1.28         2.912\n  4      2.48         2.874\n  5      4.90         2.871\n\n  (to get word-level log-ppl, multiply by 1.1078)\n\n  Args:\n    size: an integer\n  Returns:\n    a hparams object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(mod, persist=False):\n    '''\n    \n    '''\n    pre_mods = lsmod()\n    res = __salt__['cmd.run_all']('modprobe {0}'.format(mod), python_shell=False)\n    if res['retcode'] == 0:\n        post_mods = lsmod()\n        mods = _new_mods(pre_mods, post_mods)\n        persist_mods = set()\n        if persist:\n            persist_mods = _set_persistent_module(mod)\n        return sorted(list(mods | persist_mods))\n    else:\n        return 'Error loading module {0}: {1}'.format(mod, res['stderr'])", "output": "Load the specified kernel module\n\n    mod\n        Name of module to add\n\n    persist\n        Write module to /etc/modules to make it load on system reboot\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' kmod.load kvm", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tops(opts):\n    '''\n    \n    '''\n    if 'master_tops' not in opts:\n        return {}\n    whitelist = list(opts['master_tops'].keys())\n    ret = LazyLoader(\n        _module_dirs(opts, 'tops', 'top'),\n        opts,\n        tag='top',\n        whitelist=whitelist,\n    )\n    return FilterDictWrapper(ret, '.top')", "output": "Returns the tops modules", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ext_pillar(minion_id, pillar, repo, branch='default', root=None):\n    '''\n    \n    '''\n    with Repo(repo) as repo:\n        repo.update(branch)\n    envname = 'base' if branch == 'default' else branch\n    if root:\n        path = os.path.normpath(os.path.join(repo.working_dir, root))\n    else:\n        path = repo.working_dir\n\n    opts = copy.deepcopy(__opts__)\n    opts['pillar_roots'][envname] = [path]\n    pil = salt.pillar.Pillar(opts, __grains__, minion_id, envname)\n    return pil.compile_pillar(ext=False)", "output": "Extract pillar from an hg repository", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_keepalive(self, interval, callback):\n        \"\"\"\n        \n        \"\"\"\n        self.__keepalive_interval = interval\n        self.__keepalive_callback = callback\n        self.__keepalive_last = time.time()", "output": "Turn on/off the callback keepalive.  If ``interval`` seconds pass with\n        no data read from or written to the socket, the callback will be\n        executed and the timer will be reset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _TTA(learn:Learner, beta:float=0.4, scale:float=1.35, ds_type:DatasetType=DatasetType.Valid, with_loss:bool=False) -> Tensors:\n    \"\"\n    preds,y = learn.get_preds(ds_type)\n    all_preds = list(learn.tta_only(scale=scale, ds_type=ds_type))\n    avg_preds = torch.stack(all_preds).mean(0)\n    if beta is None: return preds,avg_preds,y\n    else:            \n        final_preds = preds*beta + avg_preds*(1-beta)\n        if with_loss: \n            with NoneReduceOnCPU(learn.loss_func) as lf: loss = lf(final_preds, y)\n            return final_preds, y, loss\n        return final_preds, y", "output": "Applies TTA to predict on `ds_type` dataset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def text2class_txt_iterator(source_txt_path, label_txt_path, class_strs=None):\n  \"\"\"\n  \"\"\"\n  if class_strs:\n    class_strs = dict([(s, i) for i, s in enumerate(class_strs)])\n  for inputs, label in zip(\n      txt_line_iterator(source_txt_path), txt_line_iterator(label_txt_path)):\n    label = label.strip()\n    if class_strs:\n      label = class_strs[label]\n    else:\n      label = int(label)\n    yield {\"inputs\": inputs, \"label\": label}", "output": "Yield dicts for Text2ClassProblem.generate_samples from lines of files.\n\n  Args:\n    source_txt_path: txt file with record per line.\n    label_txt_path: txt file with label per line, either as int or str. If\n      string, must provide class_strs.\n    class_strs: list<str> of class label names. Must be in correct order (i.e.\n      [\"a\", \"b\", \"c\"] means that \"a\" will get class ID 0, \"b\" ID 1, etc.).\n\n  Yields:\n    {\"inputs\": inputs, \"label\": label}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def column_families(self):\n        \"\"\"\n        \"\"\"\n        prop = self._properties.get(\"columnFamilies\", [])\n        return [BigtableColumnFamily.from_api_repr(cf) for cf in prop]", "output": "List[:class:`~.external_config.BigtableColumnFamily`]: List of\n        column families to expose in the table schema along with their types.\n\n        See\n        https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.query.tableDefinitions.(key).bigtableOptions.columnFamilies\n        https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#externalDataConfiguration.bigtableOptions.columnFamilies", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_indicator(self, time, code, indicator_name=None):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            return self.data.loc[(pd.Timestamp(time), code), indicator_name]\n        except:\n            raise ValueError('CANNOT FOUND THIS DATE&CODE')", "output": "\u83b7\u53d6\u67d0\u4e00\u65f6\u95f4\u7684\u67d0\u4e00\u53ea\u80a1\u7968\u7684\u6307\u6807", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _unpack_batch_response(response):\n    \"\"\"\n    \"\"\"\n    parser = Parser()\n    message = _generate_faux_mime_message(parser, response)\n\n    if not isinstance(message._payload, list):\n        raise ValueError(\"Bad response:  not multi-part\")\n\n    for subrequest in message._payload:\n        status_line, rest = subrequest._payload.split(\"\\n\", 1)\n        _, status, _ = status_line.split(\" \", 2)\n        sub_message = parser.parsestr(rest)\n        payload = sub_message._payload\n        msg_headers = dict(sub_message._headers)\n        content_id = msg_headers.get(\"Content-ID\")\n\n        subresponse = requests.Response()\n        subresponse.request = requests.Request(\n            method=\"BATCH\", url=\"contentid://{}\".format(content_id)\n        ).prepare()\n        subresponse.status_code = int(status)\n        subresponse.headers.update(msg_headers)\n        subresponse._content = payload.encode(\"utf-8\")\n\n        yield subresponse", "output": "Convert requests.Response -> [(headers, payload)].\n\n    Creates a generator of tuples of emulating the responses to\n    :meth:`requests.Session.request`.\n\n    :type response: :class:`requests.Response`\n    :param response: HTTP response / headers from a request.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_current_route(self, flask_request):\n        \"\"\"\n        \n        \"\"\"\n        endpoint = flask_request.endpoint\n        method = flask_request.method\n\n        route_key = self._route_key(method, endpoint)\n        route = self._dict_of_routes.get(route_key, None)\n\n        if not route:\n            LOG.debug(\"Lambda function for the route not found. This should not happen because Flask is \"\n                      \"already configured to serve all path/methods given to the service. \"\n                      \"Path=%s Method=%s RouteKey=%s\", endpoint, method, route_key)\n            raise KeyError(\"Lambda function for the route not found\")\n\n        return route", "output": "Get the route (Route) based on the current request\n\n        :param request flask_request: Flask Request\n        :return: Route matching the endpoint and method of the request", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sync_with_handlers(self, handlers=()):\n        '''\n        \n        '''\n        if not handlers:\n            return\n\n        while self.__messages:\n            record = self.__messages.pop(0)\n            for handler in handlers:\n                if handler.level > record.levelno:\n                    # If the handler's level is higher than the log record one,\n                    # it should not handle the log record\n                    continue\n                handler.handle(record)", "output": "Sync the stored log records to the provided log handlers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def legal_graph(graph):\n    '''\n    '''\n\n    descriptor = graph.extract_descriptor()\n    skips = descriptor.skip_connections\n    if len(skips) != len(set(skips)):\n        return False\n    return True", "output": "judge if a graph is legal or not.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rows(self):\n        \"\"\"\n        \n        \"\"\"\n        # We use DataFrames for serialization of IndexedRows from\n        # Java, so we first convert the RDD of rows to a DataFrame\n        # on the Scala/Java side. Then we map each Row in the\n        # DataFrame back to an IndexedRow on this side.\n        rows_df = callMLlibFunc(\"getIndexedRows\", self._java_matrix_wrapper._java_model)\n        rows = rows_df.rdd.map(lambda row: IndexedRow(row[0], row[1]))\n        return rows", "output": "Rows of the IndexedRowMatrix stored as an RDD of IndexedRows.\n\n        >>> mat = IndexedRowMatrix(sc.parallelize([IndexedRow(0, [1, 2, 3]),\n        ...                                        IndexedRow(1, [4, 5, 6])]))\n        >>> rows = mat.rows\n        >>> rows.first()\n        IndexedRow(0, [1.0,2.0,3.0])", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def disk_set(device, flag, state):\n    '''\n    \n    '''\n    _validate_device(device)\n\n    if flag not in VALID_DISK_FLAGS:\n        raise CommandExecutionError('Invalid flag passed to partition.disk_set')\n\n    if state not in set(['on', 'off']):\n        raise CommandExecutionError('Invalid state passed to partition.disk_set')\n\n    cmd = ['parted', '-m', '-s', device, 'disk_set', flag, state]\n    out = __salt__['cmd.run'](cmd).splitlines()\n    return out", "output": "Changes a flag on selected device.\n\n    A flag can be either \"on\" or \"off\" (make sure to use proper\n    quoting, see :ref:`YAML Idiosyncrasies\n    <yaml-idiosyncrasies>`). Some or all of these flags will be\n    available, depending on what disk label you are using.\n\n    Valid flags are:\n      * cylinder_alignment\n      * pmbr_boot\n      * implicit_partition_table\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' partition.disk_set /dev/sda pmbr_boot '\"on\"'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_batches_if_needed(self)->None:\n        \"\"\n        if self.learn.data.valid_dl is None: return # Running learning rate finder, so return\n        update_batches = self.data is not self.learn.data\n        if not update_batches: return\n        self.data = self.learn.data\n        self.trn_batch = self._get_new_batch(ds_type=DatasetType.Train)\n        self.val_batch = self._get_new_batch(ds_type=DatasetType.Valid)", "output": "one_batch function is extremely slow with large datasets.  This is caching the result as an optimization.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_args(args):\n    \"\"\"\"\"\"\n    # Check correctness of similarity dataset names\n    for dataset_name in args.similarity_datasets:\n        if dataset_name.lower() not in map(\n                str.lower,\n                nlp.data.word_embedding_evaluation.word_similarity_datasets):\n            print('{} is not a supported dataset.'.format(dataset_name))\n            sys.exit(1)\n\n    # Check correctness of analogy dataset names\n    for dataset_name in args.analogy_datasets:\n        if dataset_name.lower() not in map(\n                str.lower,\n                nlp.data.word_embedding_evaluation.word_analogy_datasets):\n            print('{} is not a supported dataset.'.format(dataset_name))\n            sys.exit(1)", "output": "Validate provided arguments and act on --help.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register(self, plugin):\n        \"\"\"\n        \n        \"\"\"\n\n        if not plugin or not isinstance(plugin, BasePlugin):\n            raise ValueError(\"Plugin must be implemented as a subclass of BasePlugin class\")\n\n        if self.is_registered(plugin.name):\n            raise ValueError(\"Plugin with name {} is already registered\".format(plugin.name))\n\n        self._plugins.append(plugin)", "output": "Register a plugin. New plugins are added to the end of the plugins list.\n\n        :param samtranslator.plugins.BasePlugin plugin: Instance/subclass of BasePlugin class that implements hooks\n        :raises ValueError: If plugin is not an instance of samtranslator.plugins.BasePlugin or if it is already\n            registered\n        :return: None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reboot_node(node_id, profile, **libcloud_kwargs):\n    '''\n    \n    '''\n    conn = _get_driver(profile=profile)\n    node = _get_by_id(conn.list_nodes(**libcloud_kwargs), node_id)\n    return conn.reboot_node(node, **libcloud_kwargs)", "output": "Reboot a node in the cloud\n\n    :param node_id: Unique ID of the node to reboot\n    :type  node_id: ``str``\n\n    :param profile: The profile key\n    :type  profile: ``str``\n\n    :param libcloud_kwargs: Extra arguments for the driver's reboot_node method\n    :type  libcloud_kwargs: ``dict``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion libcloud_compute.reboot_node as-2346 profile1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def should_series_dispatch(left, right, op):\n    \"\"\"\n    \n    \"\"\"\n    if left._is_mixed_type or right._is_mixed_type:\n        return True\n\n    if not len(left.columns) or not len(right.columns):\n        # ensure obj.dtypes[0] exists for each obj\n        return False\n\n    ldtype = left.dtypes.iloc[0]\n    rdtype = right.dtypes.iloc[0]\n\n    if ((is_timedelta64_dtype(ldtype) and is_integer_dtype(rdtype)) or\n            (is_timedelta64_dtype(rdtype) and is_integer_dtype(ldtype))):\n        # numpy integer dtypes as timedelta64 dtypes in this scenario\n        return True\n\n    if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):\n        # in particular case where right is an array of DateOffsets\n        return True\n\n    return False", "output": "Identify cases where a DataFrame operation should dispatch to its\n    Series counterpart.\n\n    Parameters\n    ----------\n    left : DataFrame\n    right : DataFrame\n    op : binary operator\n\n    Returns\n    -------\n    override : bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def input_diff(orig_img):\n  \"\"\"\n  \"\"\"\n  def inner(T):\n    diff = T(\"input\") - orig_img\n    return tf.sqrt(tf.reduce_mean(diff**2))\n  return inner", "output": "Average L2 difference between optimized image and orig_img.\n\n  This objective is usually mutliplied by a negative number and used as a\n  penalty in making advarsarial counterexamples.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rescale_gradients(model: Model, grad_norm: Optional[float] = None) -> Optional[float]:\n    \"\"\"\n    \n    \"\"\"\n    if grad_norm:\n        parameters_to_clip = [p for p in model.parameters()\n                              if p.grad is not None]\n        return sparse_clip_norm(parameters_to_clip, grad_norm)\n    return None", "output": "Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def evaluate(data_source, batch_size, params_file_name, ctx=None):\n    \"\"\"\n    \"\"\"\n\n    total_L = 0.0\n    ntotal = 0\n\n    model_eval.load_parameters(params_file_name, context)\n\n    hidden = model_eval.begin_state(batch_size=batch_size, func=mx.nd.zeros, ctx=context[0])\n    i = 0\n    while i < len(data_source) - 1 - 1:\n        data, target = get_batch(data_source, i, seq_len=args.bptt)\n        data = data.as_in_context(ctx)\n        target = target.as_in_context(ctx)\n        output, hidden = model_eval(data, hidden)\n        hidden = detach(hidden)\n        L = loss(output.reshape(-3, -1),\n                 target.reshape(-1,))\n        total_L += mx.nd.sum(L).asscalar()\n        ntotal += L.size\n        i += args.bptt\n    return total_L / ntotal", "output": "Evaluate the model on the dataset.\n\n    Parameters\n    ----------\n    data_source : NDArray\n        The dataset is evaluated on.\n    batch_size : int\n        The size of the mini-batch.\n    params_file_name : str\n        The parameter file to use to evaluate,\n        e.g., val.params or args.save\n    ctx : mx.cpu() or mx.gpu()\n        The context of the computation.\n\n    Returns\n    -------\n    loss: float\n        The loss on the dataset", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_time(self, asc=False):\n        \"\"\"\n        \"\"\"\n        _time = time.localtime(time.time() + self.next())\n\n        if asc:\n            return time.asctime(_time)\n\n        return time.mktime(_time)", "output": "Get the local time of the next schedule time this job will run.\n        :param bool asc: Format the result with ``time.asctime()``\n        :returns: The epoch time or string representation of the epoch time that\n            the job should be run next", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_avg_session_metrics(session_group):\n  \"\"\"\n  \"\"\"\n  assert session_group.sessions, 'SessionGroup cannot be empty.'\n  # Algorithm: Iterate over all (session, metric) pairs and maintain a\n  # dict from _MetricIdentifier to _MetricStats objects.\n  # Then use the final dict state to compute the average for each metric.\n  metric_stats = collections.defaultdict(_MetricStats)\n  for session in session_group.sessions:\n    for metric_value in session.metric_values:\n      metric_name = _MetricIdentifier(group=metric_value.name.group,\n                                      tag=metric_value.name.tag)\n      stats = metric_stats[metric_name]\n      stats.total += metric_value.value\n      stats.count += 1\n      stats.total_step += metric_value.training_step\n      stats.total_wall_time_secs += metric_value.wall_time_secs\n\n  del session_group.metric_values[:]\n  for (metric_name, stats) in six.iteritems(metric_stats):\n    session_group.metric_values.add(\n        name=api_pb2.MetricName(group=metric_name.group, tag=metric_name.tag),\n        value=float(stats.total)/float(stats.count),\n        training_step=stats.total_step // stats.count,\n        wall_time_secs=stats.total_wall_time_secs / stats.count)", "output": "Sets the metrics for the group to be the average of its sessions.\n\n  The resulting session group metrics consist of the union of metrics across\n  the group's sessions. The value of each session group metric is the average\n  of that metric values across the sessions in the group. The 'step' and\n  'wall_time_secs' fields of the resulting MetricValue field in the session\n  group are populated with the corresponding averages (truncated for 'step')\n  as well.\n\n  Args:\n    session_group: A SessionGroup protobuffer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cmd(**kwargs):\n    '''\n    \n    '''\n    update_cmd = salt.utils.path.which('freebsd-update')\n    if not update_cmd:\n        raise CommandNotFoundError('\"freebsd-update\" command not found')\n\n    params = []\n    if 'basedir' in kwargs:\n        params.append('-b {0}'.format(kwargs['basedir']))\n    if 'workdir' in kwargs:\n        params.append('-d {0}'.format(kwargs['workdir']))\n    if 'conffile' in kwargs:\n        params.append('-f {0}'.format(kwargs['conffile']))\n    if 'force' in kwargs:\n        params.append('-F')\n    if 'key' in kwargs:\n        params.append('-k {0}'.format(kwargs['key']))\n    if 'newrelease' in kwargs:\n        params.append('-r {0}'.format(kwargs['newrelease']))\n    if 'server' in kwargs:\n        params.append('-s {0}'.format(kwargs['server']))\n    if 'address' in kwargs:\n        params.append('-t {0}'.format(kwargs['address']))\n\n    if params:\n        return '{0} {1}'.format(update_cmd, ' '.join(params))\n    return update_cmd", "output": ".. versionadded:: 2016.3.4\n\n    Private function that returns the freebsd-update command string to be\n    executed. It checks if any arguments are given to freebsd-update and appends\n    them accordingly.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_dns(ip, interface='Local Area Connection', index=1):\n    '''\n    \n    '''\n    servers = get_dns_servers(interface)\n\n    # Return False if could not find the interface\n    if servers is False:\n        return False\n\n    # Return true if configured\n    try:\n        if servers[index - 1] == ip:\n            return True\n    except IndexError:\n        pass\n\n    # If configured in the wrong order delete it\n    if ip in servers:\n        rm_dns(ip, interface)\n\n    cmd = ['netsh', 'interface', 'ip', 'add', 'dns',\n           interface, ip, 'index={0}'.format(index), 'validate=no']\n\n    return __salt__['cmd.retcode'](cmd, python_shell=False) == 0", "output": "Add the DNS server to the network interface\n    (index starts from 1)\n\n    Note: if the interface DNS is configured by DHCP, all the DNS servers will\n    be removed from the interface and the requested DNS will be the only one\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' win_dns_client.add_dns <ip> <interface> <index>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sidpath(self, sid):\n        \"\"\"\n        \n        \"\"\"\n        sid_subdir = _sid_subdir_path(sid)\n        return join(self._rootdir, sid_subdir)", "output": "Parameters\n        ----------\n        sid : int\n            Asset identifier.\n\n        Returns\n        -------\n        out : string\n            Full path to the bcolz rootdir for the given sid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_raw_text_to_files(all_files, urls_path, dataset_split, tmp_dir):\n  \"\"\"\"\"\"\n\n  def write_to_file(all_files, urls_path, tmp_dir, filename):\n    \"\"\"\"\"\"\n    with io.open(\n        os.path.join(tmp_dir, filename + \".source\"), \"w\",\n        encoding=\"utf-8\") as fstory:\n      with io.open(\n          os.path.join(tmp_dir, filename + \".target\"), \"w\",\n          encoding=\"utf-8\") as fsummary:\n        for example in example_generator(all_files, urls_path, sum_token=True):\n          story, summary = _story_summary_split(example)\n          fstory.write(story + \"\\n\")\n          fsummary.write(summary + \"\\n\")\n\n  if dataset_split == problem.DatasetSplit.TRAIN:\n    filename = \"cnndm.train\"\n  elif dataset_split == problem.DatasetSplit.EVAL:\n    filename = \"cnndm.dev\"\n  else:\n    filename = \"cnndm.test\"\n\n  tf.logging.info(\"Writing %s\" % filename)\n  write_to_file(all_files, urls_path, tmp_dir, filename)", "output": "Write text to files.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_env_cache(opts, env_cache):\n    '''\n    \n    '''\n    if not os.path.isfile(env_cache):\n        return None\n    try:\n        with salt.utils.files.fopen(env_cache, 'rb') as fp_:\n            log.trace('Returning env cache data from %s', env_cache)\n            serial = salt.payload.Serial(opts)\n            return salt.utils.data.decode(serial.load(fp_))\n    except (IOError, OSError):\n        pass\n    return None", "output": "Returns cached env names, if present. Otherwise returns None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def beacon(config):\n    '''\n    \n    '''\n    ret = []\n    min_default = {\n        'humidity': '0',\n        'pressure': '0',\n        'temperature': '-273.15'\n    }\n\n    _config = {}\n    list(map(_config.update, config))\n\n    for sensor in _config.get('sensors', {}):\n        sensor_function = 'sensehat.get_{0}'.format(sensor)\n        if sensor_function not in __salt__:\n            log.error('No sensor for meassuring %s. Skipping.', sensor)\n            continue\n\n        sensor_config = _config['sensors'][sensor]\n        if isinstance(sensor_config, list):\n            sensor_min = six.text_type(sensor_config[0])\n            sensor_max = six.text_type(sensor_config[1])\n        else:\n            sensor_min = min_default.get(sensor, '0')\n            sensor_max = six.text_type(sensor_config)\n\n        if '%' in sensor_min:\n            sensor_min = re.sub('%', '', sensor_min)\n        if '%' in sensor_max:\n            sensor_max = re.sub('%', '', sensor_max)\n        sensor_min = float(sensor_min)\n        sensor_max = float(sensor_max)\n\n        current_value = __salt__[sensor_function]()\n        if not sensor_min <= current_value <= sensor_max:\n            ret.append({\n                'tag': 'sensehat/{0}'.format(sensor),\n                sensor: current_value\n            })\n\n    return ret", "output": "Monitor the temperature, humidity and pressure using the SenseHat sensors.\n\n    You can either specify a threshold for each value and only emit a beacon\n    if it is exceeded or define a range and emit a beacon when the value is\n    out of range.\n\n    Units:\n    * humidity:                     percent\n    * temperature:                  degrees Celsius\n    * temperature_from_pressure:    degrees Celsius\n    * pressure:                     Millibars\n\n    .. code-block:: yaml\n\n        beacons:\n          sensehat:\n            - sensors:\n                humidity: 70%\n                temperature: [20, 40]\n                temperature_from_pressure: 40\n                pressure: 1500", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _draw_outline(o:Patch, lw:int):\n    \"\"\n    o.set_path_effects([patheffects.Stroke(\n        linewidth=lw, foreground='black'), patheffects.Normal()])", "output": "Outline bounding box onto image `Patch`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_sebool():\n    '''\n    \n    '''\n    bdata = __salt__['cmd.run']('semanage boolean -l').splitlines()\n    ret = {}\n    for line in bdata[1:]:\n        if not line.strip():\n            continue\n        comps = line.split()\n        ret[comps[0]] = {'State': comps[1][1:],\n                         'Default': comps[3][:-1],\n                         'Description': ' '.join(comps[4:])}\n    return ret", "output": "Return a structure listing all of the selinux booleans on the system and\n    what state they are in\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' selinux.list_sebool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_focus_client(self):\r\n        \"\"\"\"\"\"\r\n        widget = QApplication.focusWidget()\r\n        for client in self.get_clients():\r\n            if widget is client or widget is client.get_control():\r\n                return client", "output": "Return current client with focus, if any", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setsebools(pairs, persist=False):\n    '''\n    \n    '''\n    if not isinstance(pairs, dict):\n        return {}\n    if persist:\n        cmd = 'setsebool -P '\n    else:\n        cmd = 'setsebool '\n    for boolean, value in six.iteritems(pairs):\n        cmd = '{0} {1}={2}'.format(cmd, boolean, value)\n    return not __salt__['cmd.retcode'](cmd, python_shell=False)", "output": "Set the value of multiple booleans\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' selinux.setsebools '{virt_use_usb: on, squid_use_tproxy: off}'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main_target_usage_requirements (self, specification, project):\n        \"\"\" \n        \"\"\"\n        assert is_iterable_typed(specification, basestring)\n        assert isinstance(project, ProjectTarget)\n        project_usage_requirements = project.get ('usage-requirements')\n\n        # We don't use 'refine-from-user-input' because I'm not sure if:\n        # - removing of parent's usage requirements makes sense\n        # - refining of usage requirements is not needed, since usage requirements\n        #   are always free.\n        usage_requirements = property_set.create_from_user_input(\n            specification, project.project_module(), project.get(\"location\"))\n\n        return project_usage_requirements.add (usage_requirements)", "output": "Returns the use requirement to use when declaraing a main target,\n            which are obtained by\n            - translating all specified property paths, and\n            - adding project's usage requirements\n            specification:  Use-properties explicitly specified for a main target\n            project:        Project where the main target is to be declared", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_to_ascii(statement):\n    \"\"\"\n    \n    \"\"\"\n    import unicodedata\n\n    text = unicodedata.normalize('NFKD', statement.text)\n    text = text.encode('ascii', 'ignore').decode('utf-8')\n\n    statement.text = str(text)\n    return statement", "output": "Converts unicode characters to ASCII character equivalents.\n    For example: \"p\u00e5 f\u00e9d\u00e9ral\" becomes \"pa federal\".", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def execute(self, logical_form: str):\n        \"\"\"\"\"\"\n        if not hasattr(self, '_functions'):\n            raise RuntimeError(\"You must call super().__init__() in your Language constructor\")\n        logical_form = logical_form.replace(\",\", \" \")\n        expression = util.lisp_to_nested_expression(logical_form)\n        return self._execute_expression(expression)", "output": "Executes a logical form, using whatever predicates you have defined.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def request(mods=None,\n            **kwargs):\n    '''\n    \n    '''\n    kwargs['test'] = True\n    ret = apply_(mods, **kwargs)\n    notify_path = os.path.join(__opts__['cachedir'], 'req_state.p')\n    serial = salt.payload.Serial(__opts__)\n    req = check_request()\n    req.update({kwargs.get('name', 'default'): {\n            'test_run': ret,\n            'mods': mods,\n            'kwargs': kwargs\n            }\n        })\n    with salt.utils.files.set_umask(0o077):\n        try:\n            if salt.utils.platform.is_windows():\n                # Make sure cache file isn't read-only\n                __salt__['cmd.run']('attrib -R \"{0}\"'.format(notify_path))\n            with salt.utils.files.fopen(notify_path, 'w+b') as fp_:\n                serial.dump(req, fp_)\n        except (IOError, OSError):\n            log.error(\n                'Unable to write state request file %s. Check permission.',\n                notify_path\n            )\n    return ret", "output": ".. versionadded:: 2017.7.3\n\n    Request that the local admin execute a state run via\n    `salt-call state.run_request`\n    All arguments match state.apply\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.request\n        salt '*' state.request test\n        salt '*' state.request test,pkgs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_combination(list_of_sentences):\n  \"\"\"\n  \"\"\"\n  num_sentences = len(list_of_sentences) - 1\n  combinations = []\n  for i, _ in enumerate(list_of_sentences):\n    if i == num_sentences:\n      break\n    num_pairs = num_sentences - i\n    populated = num_pairs * [list_of_sentences[i]]\n    zipped = list(zip(populated, list_of_sentences[i + 1:]))\n    combinations += zipped\n  return combinations", "output": "Generates all possible pair combinations for the input list of sentences.\n\n  For example:\n\n  input = [\"paraphrase1\", \"paraphrase2\", \"paraphrase3\"]\n\n  output = [(\"paraphrase1\", \"paraphrase2\"),\n            (\"paraphrase1\", \"paraphrase3\"),\n            (\"paraphrase2\", \"paraphrase3\")]\n\n  Args:\n    list_of_sentences: the list of input sentences.\n  Returns:\n    the list of all possible sentence pairs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _decode(image_data):\n    \"\"\"\n    \n    \"\"\"\n    from ...data_structures.sarray import SArray as _SArray\n    from ... import extensions as _extensions\n    if type(image_data) is _SArray:\n        return _extensions.decode_image_sarray(image_data)\n    elif type(image_data) is _Image:\n        return _extensions.decode_image(image_data)", "output": "Internal helper function for decoding a single Image or an SArray of Images", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_vmfs_datastore(datastore_name, disk_id, vmfs_major_version,\n                          safety_checks=True, service_instance=None):\n    '''\n    \n    '''\n    log.debug('Validating vmfs datastore input')\n    schema = VmfsDatastoreSchema.serialize()\n    try:\n        jsonschema.validate(\n            {'datastore': {'name': datastore_name,\n                           'backing_disk_id': disk_id,\n                           'vmfs_version': vmfs_major_version}},\n            schema)\n    except jsonschema.exceptions.ValidationError as exc:\n        raise ArgumentValueError(exc)\n    host_ref = _get_proxy_target(service_instance)\n    hostname = __proxy__['esxi.get_details']()['esxi_host']\n    if safety_checks:\n        disks = salt.utils.vmware.get_disks(host_ref, disk_ids=[disk_id])\n        if not disks:\n            raise VMwareObjectRetrievalError(\n                'Disk \\'{0}\\' was not found in host \\'{1}\\''.format(disk_id,\n                                                                    hostname))\n    ds_ref = salt.utils.vmware.create_vmfs_datastore(\n        host_ref, datastore_name, disks[0], vmfs_major_version)\n    return True", "output": "Creates a ESXi host disk group with the specified cache and capacity disks.\n\n    datastore_name\n        The name of the datastore to be created.\n\n    disk_id\n        The disk id (canonical name) on which the datastore is created.\n\n    vmfs_major_version\n        The VMFS major version.\n\n    safety_checks\n        Specify whether to perform safety check or to skip the checks and try\n        performing the required task. Default is True.\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter/ESXi host.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.create_vmfs_datastore datastore_name=ds1 disk_id=\n            vmfs_major_version=5", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def status(name, sig=None):\n    '''\n    \n    '''\n\n    proxy_fn = 'dummy.service_status'\n    resp = __proxy__[proxy_fn](name)\n    if resp['comment'] == 'stopped':\n        return False\n    if resp['comment'] == 'running':\n        return True", "output": "Return the status for a service via dummy, returns a bool\n    whether the service is running.\n\n    .. versionadded:: 2016.11.3\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.status <service name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n    \"\"\"\n    \"\"\"\n    run, tag = metrics.run_tag_from_session_and_metric(\n        self._request.session_name, self._request.metric_name)\n    body, _ = self._scalars_plugin_instance.scalars_impl(\n        tag, run, None, scalars_plugin.OutputFormat.JSON)\n    return body", "output": "Executes the request.\n\n    Returns:\n       An array of tuples representing the metric evaluations--each of the form\n       (<wall time in secs>, <training step>, <metric value>).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def follower(platform, **kwargs):\n    \"\"\"\n    \"\"\"\n    if platform.lower() in [\"rq\", \"ricequant\", \"\u7c73\u7b50\"]:\n        return RiceQuantFollower()\n    if platform.lower() in [\"jq\", \"joinquant\", \"\u805a\u5bbd\"]:\n        return JoinQuantFollower()\n    if platform.lower() in [\"xq\", \"xueqiu\", \"\u96ea\u7403\"]:\n        return XueQiuFollower(**kwargs)\n    raise NotImplementedError", "output": "\u7528\u4e8e\u751f\u6210\u7279\u5b9a\u7684\u5238\u5546\u5bf9\u8c61\n    :param platform:\u5e73\u53f0\u652f\u6301 ['jq', 'joinquant', '\u805a\u5bbd\u2019]\n    :param initial_assets: [\u96ea\u7403\u53c2\u6570] \u63a7\u5236\u96ea\u7403\u521d\u59cb\u8d44\u91d1\uff0c\u9ed8\u8ba4\u4e3a\u4e00\u4e07,\n        \u603b\u8d44\u91d1\u7531 initial_assets * \u7ec4\u5408\u5f53\u524d\u51c0\u503c \u5f97\u51fa\n    :param total_assets: [\u96ea\u7403\u53c2\u6570] \u63a7\u5236\u96ea\u7403\u603b\u8d44\u91d1\uff0c\u65e0\u9ed8\u8ba4\u503c,\n        \u82e5\u8bbe\u7f6e\u5219\u8986\u76d6 initial_assets\n    :return the class of follower\n\n    Usage::\n\n        >>> import easytrader\n        >>> user = easytrader.use('xq')\n        >>> user.prepare('xq.json')\n        >>> jq = easytrader.follower('jq')\n        >>> jq.login(user='username', password='password')\n        >>> jq.follow(users=user, strategies=['strategies_link'])", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def union(self, rdds):\n        \"\"\"\n        \n        \"\"\"\n        first_jrdd_deserializer = rdds[0]._jrdd_deserializer\n        if any(x._jrdd_deserializer != first_jrdd_deserializer for x in rdds):\n            rdds = [x._reserialize() for x in rdds]\n        cls = SparkContext._jvm.org.apache.spark.api.java.JavaRDD\n        jrdds = SparkContext._gateway.new_array(cls, len(rdds))\n        for i in range(0, len(rdds)):\n            jrdds[i] = rdds[i]._jrdd\n        return RDD(self._jsc.union(jrdds), self, rdds[0]._jrdd_deserializer)", "output": "Build the union of a list of RDDs.\n\n        This supports unions() of RDDs with different serialized formats,\n        although this forces them to be reserialized using the default\n        serializer:\n\n        >>> path = os.path.join(tempdir, \"union-text.txt\")\n        >>> with open(path, \"w\") as testFile:\n        ...    _ = testFile.write(\"Hello\")\n        >>> textFile = sc.textFile(path)\n        >>> textFile.collect()\n        [u'Hello']\n        >>> parallelized = sc.parallelize([\"World!\"])\n        >>> sorted(sc.union([textFile, parallelized]).collect())\n        [u'Hello', 'World!']", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def num_col(self):\n        \"\"\"\n        \"\"\"\n        ret = ctypes.c_uint()\n        _check_call(_LIB.XGDMatrixNumCol(self.handle,\n                                         ctypes.byref(ret)))\n        return ret.value", "output": "Get the number of columns (features) in the DMatrix.\n\n        Returns\n        -------\n        number of columns : int", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_serving(self, delegate: httputil.HTTPServerConnectionDelegate) -> None:\n        \"\"\"\n        \"\"\"\n        assert isinstance(delegate, httputil.HTTPServerConnectionDelegate)\n        fut = gen.convert_yielded(self._server_request_loop(delegate))\n        self._serving_future = fut\n        # Register the future on the IOLoop so its errors get logged.\n        self.stream.io_loop.add_future(fut, lambda f: f.result())", "output": "Starts serving requests on this connection.\n\n        :arg delegate: a `.HTTPServerConnectionDelegate`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_post_agg(mconf):\n        \"\"\"\n        \n        \"\"\"\n        if mconf.get('type') == 'javascript':\n            return JavascriptPostAggregator(\n                name=mconf.get('name', ''),\n                field_names=mconf.get('fieldNames', []),\n                function=mconf.get('function', ''))\n        elif mconf.get('type') == 'quantile':\n            return Quantile(\n                mconf.get('name', ''),\n                mconf.get('probability', ''),\n            )\n        elif mconf.get('type') == 'quantiles':\n            return Quantiles(\n                mconf.get('name', ''),\n                mconf.get('probabilities', ''),\n            )\n        elif mconf.get('type') == 'fieldAccess':\n            return Field(mconf.get('name'))\n        elif mconf.get('type') == 'constant':\n            return Const(\n                mconf.get('value'),\n                output_name=mconf.get('name', ''),\n            )\n        elif mconf.get('type') == 'hyperUniqueCardinality':\n            return HyperUniqueCardinality(\n                mconf.get('name'),\n            )\n        elif mconf.get('type') == 'arithmetic':\n            return Postaggregator(\n                mconf.get('fn', '/'),\n                mconf.get('fields', []),\n                mconf.get('name', ''))\n        else:\n            return CustomPostAggregator(\n                mconf.get('name', ''),\n                mconf)", "output": "For a metric specified as `postagg` returns the\n        kind of post aggregation for pydruid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __is_subproperty_of (parent_property, p):\n    \"\"\" \n    \"\"\"\n    if __debug__:\n        from .property import Property\n        assert isinstance(parent_property, Property)\n        assert isinstance(p, Property)\n    return is_subfeature_of (parent_property, p.feature)", "output": "As is_subfeature_of, for subproperties.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_jids():\n    '''\n    \n    '''\n    log.debug('sdstack_etcd returner <get_jids> called')\n    ret = []\n    client, path = _get_conn(__opts__)\n    items = client.get('/'.join((path, 'jobs')))\n    for item in items.children:\n        if item.dir is True:\n            jid = str(item.key).split('/')[-1]\n            ret.append(jid)\n    return ret", "output": "Return a list of all job ids", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Pack(self, msg, type_url_prefix='type.googleapis.com/'):\n    \"\"\"\"\"\"\n    if len(type_url_prefix) < 1 or type_url_prefix[-1] != '/':\n      self.type_url = '%s/%s' % (type_url_prefix, msg.DESCRIPTOR.full_name)\n    else:\n      self.type_url = '%s%s' % (type_url_prefix, msg.DESCRIPTOR.full_name)\n    self.value = msg.SerializeToString()", "output": "Packs the specified message into current Any message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def iter_chunked(self, n: int) -> AsyncStreamIterator[bytes]:\n        \"\"\"\n        \"\"\"\n        return AsyncStreamIterator(lambda: self.read(n))", "output": "Returns an asynchronous iterator that yields chunks of size n.\n\n        Python-3.5 available for Python 3.5+ only", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def add(ctx, left: int, right: int):\n    \"\"\"\"\"\"\n    await ctx.send(left + right)", "output": "Adds two numbers together.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def abs(self):\n        \"\"\"\n        \n        \"\"\"\n        return self._constructor(np.abs(self.values),\n                                 index=self.index).__finalize__(self)", "output": "Return an object with absolute value taken. Only applicable to objects\n        that are all numeric\n\n        Returns\n        -------\n        abs: same type as caller", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_private(self):\n        \"\"\"\n\n        \"\"\"\n        return (self in IPv4Network('0.0.0.0/8') or\n                self in IPv4Network('10.0.0.0/8') or\n                self in IPv4Network('127.0.0.0/8') or\n                self in IPv4Network('169.254.0.0/16') or\n                self in IPv4Network('172.16.0.0/12') or\n                self in IPv4Network('192.0.0.0/29') or\n                self in IPv4Network('192.0.0.170/31') or\n                self in IPv4Network('192.0.2.0/24') or\n                self in IPv4Network('192.168.0.0/16') or\n                self in IPv4Network('198.18.0.0/15') or\n                self in IPv4Network('198.51.100.0/24') or\n                self in IPv4Network('203.0.113.0/24') or\n                self in IPv4Network('240.0.0.0/4') or\n                self in IPv4Network('255.255.255.255/32'))", "output": "Test if this address is allocated for private networks.\n\n        Returns:\n            A boolean, True if the address is reserved per\n            iana-ipv4-special-registry.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_combined_index(indexes, intersect=False, sort=False):\n    \"\"\"\n    \n    \"\"\"\n\n    # TODO: handle index names!\n    indexes = _get_distinct_objs(indexes)\n    if len(indexes) == 0:\n        index = Index([])\n    elif len(indexes) == 1:\n        index = indexes[0]\n    elif intersect:\n        index = indexes[0]\n        for other in indexes[1:]:\n            index = index.intersection(other)\n    else:\n        index = _union_indexes(indexes, sort=sort)\n        index = ensure_index(index)\n\n    if sort:\n        try:\n            index = index.sort_values()\n        except TypeError:\n            pass\n    return index", "output": "Return the union or intersection of indexes.\n\n    Parameters\n    ----------\n    indexes : list of Index or list objects\n        When intersect=True, do not accept list of lists.\n    intersect : bool, default False\n        If True, calculate the intersection between indexes. Otherwise,\n        calculate the union.\n    sort : bool, default False\n        Whether the result index should come out sorted or not.\n\n    Returns\n    -------\n    Index", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def split_model(model:nn.Module=None, splits:Collection[Union[nn.Module,ModuleList]]=None):\n    \"\"\n    splits = listify(splits)\n    if isinstance(splits[0], nn.Module):\n        layers = flatten_model(model)\n        idxs = [layers.index(first_layer(s)) for s in splits]\n        return split_model_idx(model, idxs)\n    return [nn.Sequential(*s) for s in splits]", "output": "Split `model` according to the layers in `splits`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_failure(self):\n        \"\"\"\n        \n        \"\"\"\n        failure_time = time.time()\n\n        if not self.first_failure_time:\n            self.first_failure_time = failure_time\n\n        self.failures.append(failure_time)", "output": "Add a failure event with the current timestamp.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def arp(interface='', ipaddr='', macaddr='', **kwargs):  # pylint: disable=unused-argument\n\n    '''\n    \n    '''\n\n    proxy_output = salt.utils.napalm.call(\n        napalm_device,  # pylint: disable=undefined-variable\n        'get_arp_table',\n        **{\n        }\n    )\n\n    if not proxy_output.get('result'):\n        return proxy_output\n\n    arp_table = proxy_output.get('out')\n\n    if interface:\n        arp_table = _filter_list(arp_table, 'interface', interface)\n\n    if ipaddr:\n        arp_table = _filter_list(arp_table, 'ip', ipaddr)\n\n    if macaddr:\n        arp_table = _filter_list(arp_table, 'mac', macaddr)\n\n    proxy_output.update({\n        'out': arp_table\n    })\n\n    return proxy_output", "output": "NAPALM returns a list of dictionaries with details of the ARP entries.\n\n    :param interface: interface name to filter on\n    :param ipaddr: IP address to filter on\n    :param macaddr: MAC address to filter on\n    :return: List of the entries in the ARP table\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' net.arp\n        salt '*' net.arp macaddr='5c:5e:ab:da:3c:f0'\n\n    Example output:\n\n    .. code-block:: python\n\n        [\n            {\n                'interface' : 'MgmtEth0/RSP0/CPU0/0',\n                'mac'       : '5c:5e:ab:da:3c:f0',\n                'ip'        : '172.17.17.1',\n                'age'       : 1454496274.84\n            },\n            {\n                'interface': 'MgmtEth0/RSP0/CPU0/0',\n                'mac'       : '66:0e:94:96:e0:ff',\n                'ip'        : '172.17.17.2',\n                'age'       : 1435641582.49\n            }\n        ]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transform_padding(pad_width):\n    \"\"\"\n    \"\"\"\n    num_pad_values = len(pad_width)\n    onnx_pad_width = [0]*num_pad_values\n\n    start_index = 0\n    # num_pad_values will always be multiple of 2\n    end_index = int(num_pad_values/2)\n    for idx in range(0, num_pad_values):\n        if idx % 2 == 0:\n            onnx_pad_width[start_index] = pad_width[idx]\n            start_index += 1\n        else:\n            onnx_pad_width[end_index] = pad_width[idx]\n            end_index += 1\n\n    return onnx_pad_width", "output": "Helper function to convert padding format for pad operator.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def returner(ret):\n    '''\n    \n    '''\n    write_profile = __opts__.get('etcd.returner_write_profile')\n    if write_profile:\n        ttl = __opts__.get(write_profile, {}).get('etcd.ttl')\n    else:\n        ttl = __opts__.get('etcd.ttl')\n\n    client, path = _get_conn(__opts__, write_profile)\n    # Make a note of this minion for the external job cache\n    client.set(\n        '/'.join((path, 'minions', ret['id'])),\n        ret['jid'],\n        ttl=ttl,\n    )\n\n    for field in ret:\n        # Not using os.path.join because we're not dealing with file paths\n        dest = '/'.join((\n            path,\n            'jobs',\n            ret['jid'],\n            ret['id'],\n            field\n        ))\n        client.set(dest, salt.utils.json.dumps(ret[field]), ttl=ttl)", "output": "Return data to an etcd server or cluster", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_np(self, x, **kwargs):\n    \"\"\"\n    \n\n    \"\"\"\n\n    x_adv = []\n\n    if 'image_target' in kwargs and kwargs['image_target'] is not None:\n      image_target = np.copy(kwargs['image_target'])\n    else:\n      image_target = None\n    if 'y_target' in kwargs and kwargs['y_target'] is not None:\n      y_target = np.copy(kwargs['y_target'])\n    else:\n      y_target = None\n\n    for i, x_single in enumerate(x):\n      img = np.expand_dims(x_single, axis=0)\n      if image_target is not None:\n        single_img_target = np.expand_dims(image_target[i], axis=0)\n        kwargs['image_target'] = single_img_target\n      if y_target is not None:\n        single_y_target = np.expand_dims(y_target[i], axis=0)\n        kwargs['y_target'] = single_y_target\n\n      adv_img = super(BoundaryAttackPlusPlus,\n                      self).generate_np(img, **kwargs)\n      x_adv.append(adv_img)\n\n    return np.concatenate(x_adv, axis=0)", "output": "Generate adversarial images in a for loop.\n    :param y: An array of shape (n, nb_classes) for true labels.\n    :param y_target:  An array of shape (n, nb_classes) for target labels.\n    Required for targeted attack.\n    :param image_target: An array of shape (n, **image shape) for initial\n    target images. Required for targeted attack.\n\n    See parse_params for other kwargs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _splitlines_preserving_trailing_newline(str):\n    '''\n    \n    '''\n    lines = str.splitlines()\n    if str.endswith('\\n') or str.endswith('\\r'):\n        lines.append('')\n    return lines", "output": "Returns a list of the lines in the string, breaking at line boundaries and\n    preserving a trailing newline (if present).\n\n    Essentially, this works like ``str.striplines(False)`` but preserves an\n    empty line at the end. This is equivalent to the following code:\n\n    .. code-block:: python\n\n        lines = str.splitlines()\n        if str.endswith('\\n') or str.endswith('\\r'):\n            lines.append('')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def partitionBy(*cols):\n        \"\"\"\n        \n        \"\"\"\n        sc = SparkContext._active_spark_context\n        jspec = sc._jvm.org.apache.spark.sql.expressions.Window.partitionBy(_to_java_cols(cols))\n        return WindowSpec(jspec)", "output": "Creates a :class:`WindowSpec` with the partitioning defined.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def container_device_delete(name, device_name, remote_addr=None,\n                            cert=None, key=None, verify_cert=True):\n    '''\n    \n    '''\n    container = container_get(\n        name, remote_addr, cert, key, verify_cert, _raw=True\n    )\n\n    return _delete_property_dict_item(\n        container, 'devices', device_name\n    )", "output": "Delete a container device\n\n    name :\n        Name of the container\n\n    device_name :\n        The device name to delete\n\n    remote_addr :\n        An URL to a remote Server, you also have to give cert and key if\n        you provide remote_addr and its a TCP Address!\n\n        Examples:\n            https://myserver.lan:8443\n            /var/lib/mysocket.sock\n\n    cert :\n        PEM Formatted SSL Certificate.\n\n        Examples:\n            ~/.config/lxc/client.crt\n\n    key :\n        PEM Formatted SSL Key.\n\n        Examples:\n            ~/.config/lxc/client.key\n\n    verify_cert : True\n        Wherever to verify the cert, this is by default True\n        but in the most cases you want to set it off as LXD\n        normaly uses self-signed certificates.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def define_task(name,\n                tick_script,\n                task_type='stream',\n                database=None,\n                retention_policy='default',\n                dbrps=None):\n    '''\n    \n    '''\n    if not database and not dbrps:\n        log.error(\"Providing database name or dbrps is mandatory.\")\n        return False\n\n    if version() < '0.13':\n        cmd = 'kapacitor define -name {0}'.format(name)\n    else:\n        cmd = 'kapacitor define {0}'.format(name)\n\n    if tick_script.startswith('salt://'):\n        tick_script = __salt__['cp.cache_file'](tick_script, __env__)\n\n    cmd += ' -tick {0}'.format(tick_script)\n\n    if task_type:\n        cmd += ' -type {0}'.format(task_type)\n\n    if not dbrps:\n        dbrps = []\n\n    if database and retention_policy:\n        dbrp = '{0}.{1}'.format(database, retention_policy)\n        dbrps.append(dbrp)\n\n    if dbrps:\n        for dbrp in dbrps:\n            cmd += ' -dbrp {0}'.format(dbrp)\n\n    return _run_cmd(cmd)", "output": "Define a task. Serves as both create/update.\n\n    name\n        Name of the task.\n\n    tick_script\n        Path to the TICK script for the task. Can be a salt:// source.\n\n    task_type\n        Task type. Defaults to 'stream'\n\n    dbrps\n        A list of databases and retention policies in \"dbname\".\"rpname\" format\n        to fetch data from. For backward compatibility, the value of\n        'database' and 'retention_policy' will be merged as part of dbrps.\n\n        .. versionadded:: 2019.2.0\n\n    database\n        Which database to fetch data from.\n\n    retention_policy\n        Which retention policy to fetch data from. Defaults to 'default'.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' kapacitor.define_task cpu salt://kapacitor/cpu.tick database=telegraf", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def BARLAST(cond, yes=True):\n    \"\"\"\n    \"\"\"\n    if isinstance(cond.index, pd.MultiIndex):\n        return len(cond)-cond.index.levels[0].tolist().index(cond[cond != yes].index[-1][0])-1\n    elif isinstance(cond.index, pd.DatetimeIndex):\n        return len(cond)-cond.index.tolist().index(cond[cond != yes].index[-1])-1", "output": "\u652f\u6301MultiIndex\u7684cond\u548cDateTimeIndex\u7684cond\n    \u6761\u4ef6\u6210\u7acb  yes= True \u6216\u8005 yes=1 \u6839\u636e\u4e0d\u540c\u7684\u6307\u6807\u81ea\u5df1\u5b9a\n\n    Arguments:\n        cond {[type]} -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def findSynonymsArray(self, word, num):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(word, basestring):\n            word = _convert_to_vector(word)\n        tuples = self._java_obj.findSynonymsArray(word, num)\n        return list(map(lambda st: (st._1(), st._2()), list(tuples)))", "output": "Find \"num\" number of words closest in similarity to \"word\".\n        word can be a string or vector representation.\n        Returns an array with two fields word and similarity (which\n        gives the cosine similarity).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_graph_run(self, run_args):\n    \"\"\"\"\"\"\n    # Could try to use tfe.py_func(fct) but this would require knowing\n    # information about the signature of the function.\n\n    # Create a new graph:\n    with tf.Graph().as_default() as g:\n      # Create placeholder\n      input_ = run_args.input\n      placeholder = tf.compat.v1.placeholder(\n          dtype=input_.dtype, shape=input_.shape)\n      output = run_args.fct(placeholder)\n    return GraphRun(\n        session=raw_nogpu_session(g),\n        graph=g,\n        placeholder=placeholder,\n        output=output,\n    )", "output": "Create a new graph for the given args.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(overlay):\n    '''\n    \n    '''\n    ret = list()\n    old_overlays = list_local()\n    cmd = 'layman --quietness=0 --delete {0}'.format(overlay)\n    delete_attempt = __salt__['cmd.run_all'](cmd, python_shell=False)\n    if delete_attempt['retcode'] != 0:\n        raise salt.exceptions.CommandExecutionError(delete_attempt['stdout'])\n    new_overlays = list_local()\n\n    # If we now have no overlays added, We need to ensure that the make.conf\n    # does not source layman's make.conf, as it will break emerge\n    if not new_overlays:\n        srcline = 'source /var/lib/layman/make.conf'\n        makeconf = _get_makeconf()\n        if __salt__['file.contains'](makeconf, 'layman'):\n            __salt__['file.sed'](makeconf, srcline, '')\n\n    ret = [overlay for overlay in old_overlays if overlay not in new_overlays]\n    return ret", "output": "Remove the given overlay from the your locally installed overlays.\n    Specify 'ALL' to remove all overlays.\n\n    Return a list of the overlays(s) that were removed:\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' layman.delete <overlay name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait_for_winexe(host, port, username, password, timeout=900):\n    '''\n    \n    '''\n    start = time.time()\n    log.debug(\n        'Attempting winexe connection to host %s on port %s',\n        host, port\n    )\n    try_count = 0\n    while True:\n        try_count += 1\n        try:\n            # Shell out to winexe to check %TEMP%\n            ret_code = run_winexe_command(\n                \"sc\", \"query winexesvc\", host, username, password, port\n            )\n            if ret_code == 0:\n                log.debug('winexe connected...')\n                return True\n            log.debug('Return code was %s', ret_code)\n        except socket.error as exc:\n            log.debug('Caught exception in wait_for_winexesvc: %s', exc)\n\n        if time.time() - start > timeout:\n            return False\n        time.sleep(1)", "output": "Wait until winexe connection can be established.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hypermedia_out():\n    '''\n    \n    '''\n    request = cherrypy.serving.request\n    request._hypermedia_inner_handler = request.handler\n\n    # If handler has been explicitly set to None, don't override.\n    if request.handler is not None:\n        request.handler = hypermedia_handler", "output": "Determine the best handler for the requested content type\n\n    Wrap the normal handler and transform the output from that handler into the\n    requested content type", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def StartVector(self, elemSize, numElems, alignment):\n        \"\"\"\n        \n        \"\"\"\n\n        self.assertNotNested()\n        self.nested = True\n        self.Prep(N.Uint32Flags.bytewidth, elemSize*numElems)\n        self.Prep(alignment, elemSize*numElems)  # In case alignment > int.\n        return self.Offset()", "output": "StartVector initializes bookkeeping for writing a new vector.\n\n        A vector has the following format:\n          - <UOffsetT: number of elements in this vector>\n          - <T: data>+, where T is the type of elements of this vector.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def walk(self, top, topdown=True, ignore_file_handler=None):\n        \"\"\"\n        \"\"\"\n\n        tree = self.git_object_by_path(top)\n        if tree is None:\n            raise IOError(errno.ENOENT, \"No such file\")\n\n        for x in self._walk(tree, topdown):\n            yield x", "output": "Directory tree generator.\n\n        See `os.walk` for the docs. Differences:\n        - no support for symlinks\n        - it could raise exceptions, there is no onerror argument", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_status(self, status_code: int, reason: str = None) -> None:\n        \"\"\"\n        \"\"\"\n        self._status_code = status_code\n        if reason is not None:\n            self._reason = escape.native_str(reason)\n        else:\n            self._reason = httputil.responses.get(status_code, \"Unknown\")", "output": "Sets the status code for our response.\n\n        :arg int status_code: Response status code.\n        :arg str reason: Human-readable reason phrase describing the status\n            code. If ``None``, it will be filled in from\n            `http.client.responses` or \"Unknown\".\n\n        .. versionchanged:: 5.0\n\n           No longer validates that the response code is in\n           `http.client.responses`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _rev(repo):\n    '''\n    \n    '''\n    try:\n        repo_info = dict(six.iteritems(CLIENT.info(repo['repo'])))\n    except (pysvn._pysvn.ClientError, TypeError,\n            KeyError, AttributeError) as exc:\n        log.error(\n            'Error retrieving revision ID for svnfs remote %s '\n            '(cachedir: %s): %s',\n            repo['url'], repo['repo'], exc\n        )\n    else:\n        return repo_info['revision'].number\n    return None", "output": "Returns revision ID of repo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deselect_by_index(self, index):\n        \"\"\"\n        \"\"\"\n        if not self.is_multiple:\n            raise NotImplementedError(\"You may only deselect options of a multi-select\")\n        for opt in self.options:\n            if opt.get_attribute(\"index\") == str(index):\n                self._unsetSelected(opt)\n                return\n        raise NoSuchElementException(\"Could not locate element with index %d\" % index)", "output": "Deselect the option at the given index. This is done by examing the \"index\" attribute of an\n           element, and not merely by counting.\n\n           :Args:\n            - index - The option at this index will be deselected\n\n            throws NoSuchElementException If there is no option with specified index in SELECT", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cast_values_for_fillna(values, dtype):\n    \"\"\"\n    \n    \"\"\"\n    # TODO: for int-dtypes we make a copy, but for everything else this\n    #  alters the values in-place.  Is this intentional?\n\n    if (is_datetime64_dtype(dtype) or is_datetime64tz_dtype(dtype) or\n            is_timedelta64_dtype(dtype)):\n        values = values.view(np.int64)\n\n    elif is_integer_dtype(values):\n        # NB: this check needs to come after the datetime64 check above\n        values = ensure_float64(values)\n\n    return values", "output": "Cast values to a dtype that algos.pad and algos.backfill can handle.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ppo_original_world_model_stochastic_discrete():\n  \"\"\"\"\"\"\n  hparams = ppo_original_params()\n  hparams.policy_network = \"next_frame_basic_stochastic_discrete\"\n  hparams_keys = hparams.values().keys()\n  video_hparams = basic_stochastic.next_frame_basic_stochastic_discrete()\n  for (name, value) in six.iteritems(video_hparams.values()):\n    if name in hparams_keys:\n      hparams.set_hparam(name, value)\n    else:\n      hparams.add_hparam(name, value)\n  # To avoid OOM. Probably way to small.\n  hparams.optimization_batch_size = 1\n  hparams.weight_decay = 0\n  return hparams", "output": "Atari parameters with stochastic discrete world model as policy.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_simple_rnn(builder, layer, input_names, output_names, keras_layer):\n    \"\"\"\n    \n    \"\"\"\n    # Get input and output names\n    hidden_size = keras_layer.units\n    input_size = keras_layer.input_shape[-1]\n\n    output_all = keras_layer.return_sequences\n    reverse_input = keras_layer.go_backwards\n\n    W_h = _np.zeros((hidden_size, hidden_size))\n    W_x = _np.zeros((hidden_size, input_size))\n    b = None\n\n    implementation = keras_layer.implementation if hasattr(keras_layer,\n            'implementation') else 0\n    if implementation == 0:\n        W_h = keras_layer.get_weights()[1].T\n        W_x = keras_layer.get_weights()[0].T\n        if keras_layer.use_bias:\n            b = keras_layer.get_weights()[2]\n\n    # Set actication type\n    activation_str = _get_recurrent_activation_name_from_keras(keras_layer.activation)\n\n    # Add to the network\n    builder.add_simple_rnn(\n        name = layer,\n        W_h = W_h, W_x = W_x, b = b,\n        hidden_size = hidden_size,\n        input_size = input_size,\n        activation = activation_str,\n        input_names = input_names,\n        output_names = output_names,\n        output_all=output_all,\n        reverse_input=reverse_input)", "output": "Convert an SimpleRNN layer from keras to coreml.\n\n    Parameters\n    ----------\n    keras_layer: layer\n        A keras layer object.\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_message(cls, message):\n        \"\"\"\n        \n        \"\"\"\n        type_ = message.get_text()\n        return cls(type_=type_, blob=message.asbytes())", "output": "Create a public blob from a network `.Message`.\n\n        Specifically, a cert-bearing pubkey auth packet, because by definition\n        OpenSSH-style certificates 'are' their own network representation.\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_tz_from_dtype(dtype, tz):\n    \"\"\"\n    \n    \"\"\"\n    if dtype is not None:\n        if isinstance(dtype, str):\n            try:\n                dtype = DatetimeTZDtype.construct_from_string(dtype)\n            except TypeError:\n                # Things like `datetime64[ns]`, which is OK for the\n                # constructors, but also nonsense, which should be validated\n                # but not by us. We *do* allow non-existent tz errors to\n                # go through\n                pass\n        dtz = getattr(dtype, 'tz', None)\n        if dtz is not None:\n            if tz is not None and not timezones.tz_compare(tz, dtz):\n                raise ValueError(\"cannot supply both a tz and a dtype\"\n                                 \" with a tz\")\n            tz = dtz\n\n        if tz is not None and is_datetime64_dtype(dtype):\n            # We also need to check for the case where the user passed a\n            #  tz-naive dtype (i.e. datetime64[ns])\n            if tz is not None and not timezones.tz_compare(tz, dtz):\n                raise ValueError(\"cannot supply both a tz and a \"\n                                 \"timezone-naive dtype (i.e. datetime64[ns])\")\n\n    return tz", "output": "If the given dtype is a DatetimeTZDtype, extract the implied\n    tzinfo object from it and check that it does not conflict with the given\n    tz.\n\n    Parameters\n    ----------\n    dtype : dtype, str\n    tz : None, tzinfo\n\n    Returns\n    -------\n    tz : consensus tzinfo\n\n    Raises\n    ------\n    ValueError : on tzinfo mismatch", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tableNames(self, dbName=None):\n        \"\"\"\n        \"\"\"\n        if dbName is None:\n            return [name for name in self._ssql_ctx.tableNames()]\n        else:\n            return [name for name in self._ssql_ctx.tableNames(dbName)]", "output": "Returns a list of names of tables in the database ``dbName``.\n\n        :param dbName: string, name of the database to use. Default to the current database.\n        :return: list of table names, in string\n\n        >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n        >>> \"table1\" in sqlContext.tableNames()\n        True\n        >>> \"table1\" in sqlContext.tableNames(\"default\")\n        True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_graph_metadata(self, graph):\n        \"\"\"\n        \n        \"\"\"\n        _params = set()\n        for tensor_vals in graph.initializer:\n            _params.add(tensor_vals.name)\n\n        input_data = []\n        for graph_input in graph.input:\n            if graph_input.name not in _params:\n                shape = [val.dim_value for val in graph_input.type.tensor_type.shape.dim]\n                input_data.append((graph_input.name, tuple(shape)))\n\n        output_data = []\n        for graph_out in graph.output:\n            shape = [val.dim_value for val in graph_out.type.tensor_type.shape.dim]\n            output_data.append((graph_out.name, tuple(shape)))\n        metadata = {'input_tensor_data' : input_data,\n                    'output_tensor_data' : output_data\n                   }\n        return metadata", "output": "Get the model metadata from a given onnx graph.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_outputs(sym, params, in_shape, in_label):\n        \"\"\" \n        \"\"\"\n        # remove any input listed in params from sym.list_inputs() and bind them to the input shapes provided\n        # by user. Also remove in_label, which is the name of the label symbol that may have been used\n        # as the label for loss during training.\n        inputs = {n: tuple(s) for n, s in zip([n for n in sym.list_inputs() if n not in params and n != in_label],\n                                              in_shape)}\n        # Add params and their shape to list of inputs\n        inputs.update({n: v.shape for n, v in params.items() if n in sym.list_inputs()})\n        # Provide input data as well as input params to infer_shape()\n        _, out_shapes, _ = sym.infer_shape(**inputs)\n\n        out_names = list()\n        for name in sym.list_outputs():\n            if name.endswith('_output'):\n                out_names.append(name[:-len('_output')])\n            else:\n                logging.info(\"output '%s' does not end with '_output'\", name)\n                out_names.append(name)\n\n        assert len(out_shapes) == len(out_names)\n        # bind output shapes with output names\n        graph_outputs = {n: s for n, s in zip(out_names, out_shapes)}\n\n        return graph_outputs", "output": "Infer output shapes and return dictionary of output name to shape\n\n        :param :class:`~mxnet.symbol.Symbol` sym: symbol to perform infer shape on\n        :param dic of (str, nd.NDArray) params:\n        :param list of tuple(int, ...) in_shape: list of all input shapes\n        :param  in_label: name of label typically used in loss that may be left in graph. This name is\n            removed from list of inputs required by symbol\n        :return: dictionary of output name to shape\n        :rtype: dict of (str, tuple(int, ...))", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def swapaxes(self, axis1, axis2, copy=True):\n        \"\"\"\n        \n        \"\"\"\n        i = self._get_axis_number(axis1)\n        j = self._get_axis_number(axis2)\n\n        if i == j:\n            if copy:\n                return self.copy()\n            return self\n\n        mapping = {i: j, j: i}\n\n        new_axes = (self._get_axis(mapping.get(k, k))\n                    for k in range(self._AXIS_LEN))\n        new_values = self.values.swapaxes(i, j)\n        if copy:\n            new_values = new_values.copy()\n\n        return self._constructor(new_values, *new_axes).__finalize__(self)", "output": "Interchange axes and swap values axes appropriately.\n\n        Returns\n        -------\n        y : same as input", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def value(self):\n        \"\"\" \n        \"\"\"\n        if not hasattr(self, \"_value\") and self._path is not None:\n            # we only need to decrypt it here when encryption is enabled and\n            # if its on the driver, since executor decryption is handled already\n            if self._sc is not None and self._sc._encryption_enabled:\n                port, auth_secret = self._python_broadcast.setupDecryptionServer()\n                (decrypted_sock_file, _) = local_connect_and_auth(port, auth_secret)\n                self._python_broadcast.waitTillBroadcastDataSent()\n                return self.load(decrypted_sock_file)\n            else:\n                self._value = self.load_from_path(self._path)\n        return self._value", "output": "Return the broadcasted value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_dependencies_from_json(ireq, sources):\n    \"\"\"\n    \"\"\"\n    if os.environ.get(\"PASSA_IGNORE_JSON_API\"):\n        return\n\n    # It is technically possible to parse extras out of the JSON API's\n    # requirement format, but it is such a chore let's just use the simple API.\n    if ireq.extras:\n        return\n\n    try:\n        version = get_pinned_version(ireq)\n    except ValueError:\n        return\n\n    url_prefixes = [\n        proc_url[:-7]   # Strip \"/simple\".\n        for proc_url in (\n            raw_url.rstrip(\"/\")\n            for raw_url in (source.get(\"url\", \"\") for source in sources)\n        )\n        if proc_url.endswith(\"/simple\")\n    ]\n\n    session = requests.session()\n\n    for prefix in url_prefixes:\n        url = \"{prefix}/pypi/{name}/{version}/json\".format(\n            prefix=prefix,\n            name=packaging.utils.canonicalize_name(ireq.name),\n            version=version,\n        )\n        try:\n            dependencies = _get_dependencies_from_json_url(url, session)\n            if dependencies is not None:\n                return dependencies\n        except Exception as e:\n            print(\"unable to read dependencies via {0} ({1})\".format(url, e))\n    session.close()\n    return", "output": "Retrieves dependencies for the install requirement from the JSON API.\n\n    :param ireq: A single InstallRequirement\n    :type ireq: :class:`~pip._internal.req.req_install.InstallRequirement`\n    :return: A set of dependency lines for generating new InstallRequirements.\n    :rtype: set(str) or None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def diffs(self):\n        '''\n        \n        '''\n        differences = []\n        for item in self._get_recursive_difference(type='all'):\n            if item.diffs:\n                if item.past_dict:\n                    differences.append({item.past_dict[self._key]: item.diffs})\n                elif item.current_dict:\n                    differences.append({item.current_dict[self._key]: item.diffs})\n        return differences", "output": "Returns a list of dictionaries with key value pairs.\n        The values are the differences between the items identified by the key.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_preferred_submodules():\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    # Path to the modules database\r\n    modules_path = get_conf_path('db')\r\n\r\n    # Modules database\r\n    modules_db = PickleShareDB(modules_path)\r\n\r\n    if 'submodules' in modules_db:\r\n        return modules_db['submodules']\r\n\r\n    submodules = []\r\n\r\n    for m in PREFERRED_MODULES:\r\n        submods = get_submodules(m)\r\n        submodules += submods\r\n    \r\n    modules_db['submodules'] = submodules\r\n    return submodules", "output": "Get all submodules of the main scientific modules and others of our\r\n    interest", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def AddServiceDescriptor(self, service_desc):\n    \"\"\"\n    \"\"\"\n\n    if not isinstance(service_desc, descriptor.ServiceDescriptor):\n      raise TypeError('Expected instance of descriptor.ServiceDescriptor.')\n\n    self._service_descriptors[service_desc.full_name] = service_desc", "output": "Adds a ServiceDescriptor to the pool.\n\n    Args:\n      service_desc: A ServiceDescriptor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None,\n                       partitionFunc=portable_hash):\n        \"\"\"\n        \n        \"\"\"\n        def createZero():\n            return copy.deepcopy(zeroValue)\n\n        return self.combineByKey(\n            lambda v: seqFunc(createZero(), v), seqFunc, combFunc, numPartitions, partitionFunc)", "output": "Aggregate the values of each key, using given combine functions and a neutral\n        \"zero value\". This function can return a different result type, U, than the type\n        of the values in this RDD, V. Thus, we need one operation for merging a V into\n        a U and one operation for merging two U's, The former operation is used for merging\n        values within a partition, and the latter is used for merging values between\n        partitions. To avoid memory allocation, both of these functions are\n        allowed to modify and return their first argument instead of creating a new U.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_tags(Name,\n           region=None, key=None, keyid=None, profile=None, **kwargs):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        tagslist = []\n        for k, v in six.iteritems(kwargs):\n            if six.text_type(k).startswith('__'):\n                continue\n            tagslist.append({'Key': six.text_type(k), 'Value': six.text_type(v)})\n        conn.add_tags(ResourceId=_get_trail_arn(Name,\n                      region=region, key=key, keyid=keyid,\n                      profile=profile), TagsList=tagslist)\n        return {'tagged': True}\n    except ClientError as e:\n        return {'tagged': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Add tags to a trail\n\n    Returns {tagged: true} if the trail was tagged and returns\n    {tagged: False} if the trail was not tagged.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_cloudtrail.add_tags my_trail tag_a=tag_value tag_b=tag_value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def worker_loop_v1(dataset, key_queue, data_queue, batchify_fn):\n    \"\"\"\"\"\"\n    while True:\n        idx, samples = key_queue.get()\n        if idx is None:\n            break\n        batch = batchify_fn([dataset[i] for i in samples])\n        data_queue.put((idx, batch))", "output": "Worker loop for multiprocessing DataLoader.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uninstall_python(python, runas=None):\n    '''\n    \n    '''\n    python = re.sub(r'^python-', '', python)\n\n    args = '--force {0}'.format(python)\n    _pyenv_exec('uninstall', args, runas=runas)\n    return True", "output": "Uninstall a python implementation.\n\n    python\n        The version of python to uninstall. Should match one of the versions\n        listed by :mod:`pyenv.versions <salt.modules.pyenv.versions>`\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pyenv.uninstall_python 2.0.0-p0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Setup(self, input, URL, encoding, options):\n        \"\"\" \"\"\"\n        if input is None: input__o = None\n        else: input__o = input._o\n        ret = libxml2mod.xmlTextReaderSetup(self._o, input__o, URL, encoding, options)\n        return ret", "output": "Setup an XML reader with new options", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hlen(key, host=None, port=None, db=None, password=None):\n    '''\n    \n    '''\n    server = _connect(host, port, db, password)\n    return server.hlen(key)", "output": "Returns number of fields of a hash.\n\n    .. versionadded:: 2017.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.hlen foo_hash", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def focusInEvent(self, event):\n        \"\"\"\"\"\"\n        self.focus_changed.emit()\n        return super(ShellWidget, self).focusInEvent(event)", "output": "Reimplement Qt method to send focus change notification", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_styles(self, style=None):\n        \"\"\"\n        \n        \"\"\"\n\n        style, _ = self._style_input_check(style)\n        return self.styles.filter_by(style, self._index_column)", "output": "Returns SFrame of style images used for training the model\n\n        Parameters\n        ----------\n        style: int or list, optional\n            The selected style or list of styles to return. If `None`, all\n            styles will be returned\n\n        See Also\n        --------\n        stylize\n\n        Examples\n        --------\n        >>>  model.get_styles()\n        Columns:\n            style   int\n            image   Image\n\n        Rows: 4\n\n        Data:\n        +-------+--------------------------+\n        | style |          image           |\n        +-------+--------------------------+\n        |  0    |  Height: 642 Width: 642  |\n        |  1    |  Height: 642 Width: 642  |\n        |  2    |  Height: 642 Width: 642  |\n        |  3    |  Height: 642 Width: 642  |\n        +-------+--------------------------+", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sub(self, repl):\n        \"\"\"\n        \n        \"\"\"\n        if self.asGroupList:\n            warnings.warn(\"cannot use sub() with Regex(asGroupList=True)\",\n                           SyntaxWarning, stacklevel=2)\n            raise SyntaxError()\n\n        if self.asMatch and callable(repl):\n            warnings.warn(\"cannot use sub() with a callable with Regex(asMatch=True)\",\n                           SyntaxWarning, stacklevel=2)\n            raise SyntaxError()\n\n        if self.asMatch:\n            def pa(tokens):\n                return tokens[0].expand(repl)\n        else:\n            def pa(tokens):\n                return self.re.sub(repl, tokens[0])\n        return self.addParseAction(pa)", "output": "Return Regex with an attached parse action to transform the parsed\n        result as if called using `re.sub(expr, repl, string) <https://docs.python.org/3/library/re.html#re.sub>`_.\n\n        Example::\n\n            make_html = Regex(r\"(\\w+):(.*?):\").sub(r\"<\\1>\\2</\\1>\")\n            print(make_html.transformString(\"h1:main title:\"))\n            # prints \"<h1>main title</h1>\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_additional_options(runtime, debug_options):\n        \"\"\"\n        \n        \"\"\"\n        if not debug_options:\n            return None\n\n        opts = {}\n\n        if runtime == Runtime.go1x.value:\n            # These options are required for delve to function properly inside a docker container on docker < 1.12\n            # See https://github.com/moby/moby/issues/21051\n            opts[\"security_opt\"] = [\"seccomp:unconfined\"]\n            opts[\"cap_add\"] = [\"SYS_PTRACE\"]\n\n        return opts", "output": "Return additional Docker container options. Used by container debug mode to enable certain container\n        security options.\n        :param DebugContext debug_options: DebugContext for the runtime of the container.\n        :return dict: Dictionary containing additional arguments to be passed to container creation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_custom_template(cls, searchpath, name):\n        \"\"\"\n        \n        \"\"\"\n        loader = ChoiceLoader([\n            FileSystemLoader(searchpath),\n            cls.loader,\n        ])\n\n        class MyStyler(cls):\n            env = Environment(loader=loader)\n            template = env.get_template(name)\n\n        return MyStyler", "output": "Factory function for creating a subclass of ``Styler``\n        with a custom template and Jinja environment.\n\n        Parameters\n        ----------\n        searchpath : str or list\n            Path or paths of directories containing the templates\n        name : str\n            Name of your custom template to use for rendering\n\n        Returns\n        -------\n        MyStyler : subclass of Styler\n            Has the correct ``env`` and ``template`` class attributes set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def block_widths(self):\n        \"\"\"\n        \"\"\"\n        if self._widths_cache is None:\n            try:\n                # The first column will have the correct lengths. We have an\n                # invariant that requires that all blocks be the same width in a\n                # column of blocks.\n                self._widths_cache = np.array(\n                    ray.get([obj.width().oid for obj in self._partitions_cache[0]])\n                    if len(self._partitions_cache) > 0\n                    else []\n                )\n            except RayTaskError as e:\n                handle_ray_task_error(e)\n        return self._widths_cache", "output": "Gets the widths of the blocks.\n\n        Note: This works with the property structure `_widths_cache` to avoid\n            having to recompute these values each time they are needed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_pod(name, namespace='default', **kwargs):\n    '''\n    \n    '''\n    cfg = _setup_conn(**kwargs)\n    body = kubernetes.client.V1DeleteOptions(orphan_dependents=True)\n\n    try:\n        api_instance = kubernetes.client.CoreV1Api()\n        api_response = api_instance.delete_namespaced_pod(\n            name=name,\n            namespace=namespace,\n            body=body)\n\n        return api_response.to_dict()\n    except (ApiException, HTTPError) as exc:\n        if isinstance(exc, ApiException) and exc.status == 404:\n            return None\n        else:\n            log.exception(\n                'Exception when calling '\n                'CoreV1Api->delete_namespaced_pod'\n            )\n            raise CommandExecutionError(exc)\n    finally:\n        _cleanup(**cfg)", "output": "Deletes the kubernetes pod defined by name and namespace\n\n    CLI Examples::\n\n        salt '*' kubernetes.delete_pod guestbook-708336848-5nl8c default\n        salt '*' kubernetes.delete_pod name=guestbook-708336848-5nl8c namespace=default", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def feature_names(self):\n        \"\"\"\n        \"\"\"\n        if self._feature_names is None:\n            self._feature_names = ['f{0}'.format(i) for i in range(self.num_col())]\n        return self._feature_names", "output": "Get feature names (column labels).\n\n        Returns\n        -------\n        feature_names : list or None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def categories(self):\n        \"\"\"\n        \"\"\"\n        r = [ch for ch in self._channels.values() if isinstance(ch, CategoryChannel)]\n        r.sort(key=lambda c: (c.position, c.id))\n        return r", "output": "List[:class:`CategoryChannel`]: A list of categories that belongs to this guild.\n\n        This is sorted by the position and are in UI order from top to bottom.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filter_by_rand(self, p:float, seed:int=None):\n        \"\"\n        if seed is not None: np.random.seed(seed)\n        return self.filter_by_func(lambda o: rand_bool(p))", "output": "Keep random sample of `items` with probability `p` and an optional `seed`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compose(self, sources, client=None):\n        \"\"\"\n        \"\"\"\n        client = self._require_client(client)\n        query_params = {}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        request = {\n            \"sourceObjects\": [{\"name\": source.name} for source in sources],\n            \"destination\": self._properties.copy(),\n        }\n        api_response = client._connection.api_request(\n            method=\"POST\",\n            path=self.path + \"/compose\",\n            query_params=query_params,\n            data=request,\n            _target_object=self,\n        )\n        self._set_properties(api_response)", "output": "Concatenate source blobs into this one.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type sources: list of :class:`Blob`\n        :param sources: blobs whose contents will be composed into this blob.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: Optional. The client to use.  If not passed, falls back\n                       to the ``client`` stored on the blob's bucket.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def present(name, auth=None, **kwargs):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    kwargs = __utils__['args.clean_kwargs'](**kwargs)\n\n    __salt__['keystoneng.setup_clouds'](auth)\n\n    kwargs['name'] = name\n    user = _common(kwargs)\n\n    if user is None:\n        if __opts__['test'] is True:\n            ret['result'] = None\n            ret['changes'] = kwargs\n            ret['comment'] = 'User will be created.'\n            return ret\n\n        user = __salt__['keystoneng.user_create'](**kwargs)\n        ret['changes'] = user\n        ret['comment'] = 'Created user'\n        return ret\n\n    changes = __salt__['keystoneng.compare_changes'](user, **kwargs)\n    if changes:\n        if __opts__['test'] is True:\n            ret['result'] = None\n            ret['changes'] = changes\n            ret['comment'] = 'User will be updated.'\n            return ret\n\n        kwargs['name'] = user\n        __salt__['keystoneng.user_update'](**kwargs)\n        ret['changes'].update(changes)\n        ret['comment'] = 'Updated user'\n\n    return ret", "output": "Ensure domain exists and is up-to-date\n\n    name\n        Name of the domain\n\n    domain\n        The name or id of the domain\n\n    enabled\n        Boolean to control if domain is enabled\n\n    description\n        An arbitrary description of the domain\n\n    password\n        The user password\n\n    email\n        The users email address", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def snapshot_name_to_id(name, snap_name, strict=False, runas=None):\n    '''\n    \n    '''\n    # Validate VM and snapshot names\n    name = salt.utils.data.decode(name)\n    snap_name = salt.utils.data.decode(snap_name)\n\n    # Get a multiline string containing all the snapshot GUIDs\n    info = prlctl('snapshot-list', name, runas=runas)\n\n    # Get a set of all snapshot GUIDs in the string\n    snap_ids = _find_guids(info)\n\n    # Try to match the snapshot name to an ID\n    named_ids = []\n    for snap_id in snap_ids:\n        if snapshot_id_to_name(name, snap_id, runas=runas) == snap_name:\n            named_ids.append(snap_id)\n\n    # Return one or more IDs having snap_name or raise an error upon\n    # non-singular names\n    if not named_ids:\n        raise SaltInvocationError(\n            'No snapshots for VM \"{0}\" have name \"{1}\"'.format(name, snap_name)\n        )\n    elif len(named_ids) == 1:\n        return named_ids[0]\n    else:\n        multi_msg = ('Multiple snapshots for VM \"{0}\" have name '\n                     '\"{1}\"'.format(name, snap_name))\n        if strict:\n            raise SaltInvocationError(multi_msg)\n        else:\n            log.warning(multi_msg)\n        return named_ids", "output": "Attempt to convert a snapshot name to a snapshot ID.  If the name is not\n    found an empty string is returned.  If multiple snapshots share the same\n    name, a list will be returned\n\n    :param str name:\n        Name/ID of VM whose snapshots are inspected\n\n    :param str snap_name:\n        Name of the snapshot\n\n    :param bool strict:\n        Raise an exception if multiple snapshot IDs are found\n\n    :param str runas:\n        The user that the prlctl command will be run as\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' parallels.snapshot_id_to_name macvm original runas=macdev", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _AddMessageMethods(message_descriptor, cls):\n  \"\"\"\"\"\"\n  _AddListFieldsMethod(message_descriptor, cls)\n  _AddHasFieldMethod(message_descriptor, cls)\n  _AddClearFieldMethod(message_descriptor, cls)\n  if message_descriptor.is_extendable:\n    _AddClearExtensionMethod(cls)\n    _AddHasExtensionMethod(cls)\n  _AddEqualsMethod(message_descriptor, cls)\n  _AddStrMethod(message_descriptor, cls)\n  _AddReprMethod(message_descriptor, cls)\n  _AddUnicodeMethod(message_descriptor, cls)\n  _AddByteSizeMethod(message_descriptor, cls)\n  _AddSerializeToStringMethod(message_descriptor, cls)\n  _AddSerializePartialToStringMethod(message_descriptor, cls)\n  _AddMergeFromStringMethod(message_descriptor, cls)\n  _AddIsInitializedMethod(message_descriptor, cls)\n  _AddMergeFromMethod(cls)\n  _AddWhichOneofMethod(message_descriptor, cls)\n  _AddReduceMethod(cls)\n  # Adds methods which do not depend on cls.\n  cls.Clear = _Clear\n  cls.DiscardUnknownFields = _DiscardUnknownFields\n  cls._SetListener = _SetListener", "output": "Adds implementations of all Message methods to cls.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def latents_to_frames(z_top_interp, level_eps_interp, hparams):\n  \"\"\"\"\"\"\n  # Decode [z^1_t, z^2_t .. z^l_t] to [X_t]\n  images, _, _, _ = glow_ops.encoder_decoder(\n      \"codec\", z_top_interp, hparams, eps=level_eps_interp, reverse=True)\n  images = glow_ops.postprocess(images)\n  return images", "output": "Decodes latents to frames.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _take_instances(self,\n                        instances: Iterable[Instance],\n                        max_instances: Optional[int] = None) -> Iterator[Instance]:\n        \"\"\"\n        \n        \"\"\"\n        # If max_instances isn't specified, just iterate once over the whole dataset\n        if max_instances is None:\n            yield from iter(instances)\n        else:\n            # If we don't have a cursor for this dataset, create one. We use ``id()``\n            # for the key because ``instances`` could be a list, which can't be used as a key.\n            key = id(instances)\n            iterator = self._cursors.get(key, iter(instances))\n\n            while max_instances > 0:\n                try:\n                    # If there are instances left on this iterator,\n                    # yield one and decrement max_instances.\n                    yield next(iterator)\n                    max_instances -= 1\n                except StopIteration:\n                    # None left, so start over again at the beginning of the dataset.\n                    iterator = iter(instances)\n\n            # We may have a new iterator, so update the cursor.\n            self._cursors[key] = iterator", "output": "Take the next `max_instances` instances from the given dataset.\n        If `max_instances` is `None`, then just take all instances from the dataset.\n        If `max_instances` is not `None`, each call resumes where the previous one\n        left off, and when you get to the end of the dataset you start again from the beginning.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def discount_rewards(r):\n    \"\"\"\"\"\"\n    discounted_r = np.zeros_like(r)\n    running_add = 0\n    for t in reversed(range(0, r.size)):\n        # Reset the sum, since this was a game boundary (pong specific!).\n        if r[t] != 0:\n            running_add = 0\n        running_add = running_add * gamma + r[t]\n        discounted_r[t] = running_add\n    return discounted_r", "output": "take 1D float array of rewards and compute discounted reward", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filter_by_func(self, func:Callable)->'ItemList':\n        \"\"\n        self.items = array([o for o in self.items if func(o)])\n        return self", "output": "Only keep elements for which `func` returns `True`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop_trial(self, trial_id):\n        \"\"\"\"\"\"\n        response = requests.put(\n            urljoin(self._path, \"trials/{}\".format(trial_id)))\n        return self._deserialize(response)", "output": "Requests to stop trial by trial_id.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toggle_view(self, checked):\n        \"\"\"\"\"\"\n        if not self.dockwidget:\n            return\n        if checked:\n            self.dockwidget.show()\n            self.dockwidget.raise_()\n        else:\n            self.dockwidget.hide()", "output": "Toggle view", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def request_server_info(self):\n        ''' \n\n        '''\n        if self._server_info is None:\n            self._server_info = self._send_request_server_info()\n        return self._server_info", "output": "Ask for information about the server.\n\n        Returns:\n            A dictionary of server attributes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_figure(self, fig, fmt):\n        \"\"\"\n        \n        \"\"\"\n        self.fig = fig\n        self.fmt = fmt\n\n        if fmt in ['image/png', 'image/jpeg']:\n            self._qpix_orig = QPixmap()\n            self._qpix_orig.loadFromData(fig, fmt.upper())\n        elif fmt == 'image/svg+xml':\n            self._qpix_orig = QPixmap(svg_to_image(fig))\n\n        self._qpix_buffer = [self._qpix_orig]\n        self.fwidth = self._qpix_orig.width()\n        self.fheight = self._qpix_orig.height()", "output": "Load the figure from a png, jpg, or svg image, convert it in\n        a QPixmap, and force a repaint of the widget.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def overwrites_for(self, obj):\n        \"\"\"\n        \"\"\"\n\n        if isinstance(obj, User):\n            predicate = lambda p: p.type == 'member'\n        elif isinstance(obj, Role):\n            predicate = lambda p: p.type == 'role'\n        else:\n            predicate = lambda p: True\n\n        for overwrite in filter(predicate, self._overwrites):\n            if overwrite.id == obj.id:\n                allow = Permissions(overwrite.allow)\n                deny = Permissions(overwrite.deny)\n                return PermissionOverwrite.from_pair(allow, deny)\n\n        return PermissionOverwrite()", "output": "Returns the channel-specific overwrites for a member or a role.\n\n        Parameters\n        -----------\n        obj\n            The :class:`Role` or :class:`abc.User` denoting\n            whose overwrite to get.\n\n        Returns\n        ---------\n        :class:`PermissionOverwrite`\n            The permission overwrites for this object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_value(self, key):\n        # type: (str) -> Any\n        \"\"\"\n        \"\"\"\n        try:\n            return self._dictionary[key]\n        except KeyError:\n            raise ConfigurationError(\"No such key - {}\".format(key))", "output": "Get a value from the configuration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def union(self, other):\n        \"\"\"\n        \n        \"\"\"\n        if self._jrdd_deserializer == other._jrdd_deserializer:\n            rdd = RDD(self._jrdd.union(other._jrdd), self.ctx,\n                      self._jrdd_deserializer)\n        else:\n            # These RDDs contain data in different serialized formats, so we\n            # must normalize them to the default serializer.\n            self_copy = self._reserialize()\n            other_copy = other._reserialize()\n            rdd = RDD(self_copy._jrdd.union(other_copy._jrdd), self.ctx,\n                      self.ctx.serializer)\n        if (self.partitioner == other.partitioner and\n                self.getNumPartitions() == rdd.getNumPartitions()):\n            rdd.partitioner = self.partitioner\n        return rdd", "output": "Return the union of this RDD and another one.\n\n        >>> rdd = sc.parallelize([1, 1, 2, 3])\n        >>> rdd.union(rdd).collect()\n        [1, 1, 2, 3, 1, 1, 2, 3]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def diff(*paths, **kwargs):\n    '''\n    \n    '''\n    ret = {}\n\n    pkg_to_paths = {}\n    for pth in paths:\n        pth_pkg = __salt__['lowpkg.owner'](pth)\n        if not pth_pkg:\n            ret[pth] = os.path.exists(pth) and 'Not managed' or 'N/A'\n        else:\n            if pkg_to_paths.get(pth_pkg) is None:\n                pkg_to_paths[pth_pkg] = []\n            pkg_to_paths[pth_pkg].append(pth)\n\n    if pkg_to_paths:\n        local_pkgs = __salt__['pkg.download'](*pkg_to_paths.keys())\n        for pkg, files in pkg_to_paths.items():\n            for path in files:\n                ret[path] = __salt__['lowpkg.diff'](\n                    local_pkgs[pkg]['path'], path) or 'Unchanged'\n\n    return ret", "output": "Return a formatted diff between current files and original in a package.\n    NOTE: this function includes all files (configuration and not), but does\n    not work on binary content.\n\n    :param path: Full path to the installed file\n    :return: Difference string or raises and exception if examined file is binary.\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.diff /etc/apache2/httpd.conf /etc/sudoers", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_binary(self, filename):\n        \"\"\"\n        \"\"\"\n        _safe_call(_LIB.LGBM_DatasetSaveBinary(\n            self.construct().handle,\n            c_str(filename)))\n        return self", "output": "Save Dataset to a binary file.\n\n        Parameters\n        ----------\n        filename : string\n            Name of the output file.\n\n        Returns\n        -------\n        self : Dataset\n            Returns self.", "category": "Python"}]