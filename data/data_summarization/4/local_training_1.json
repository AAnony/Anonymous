[{"instruction": "Summarizing the code.", "input": "def _do_names(names, fun, path=None):\n    '''\n    \n    '''\n    ret = {}\n    hosts = find_guests(names, path=path)\n    if not hosts:\n        return False\n\n    client = salt.client.get_local_client(__opts__['conf_file'])\n    for host, sub_names in six.iteritems(hosts):\n        cmds = []\n        for name in sub_names:\n            cmds.append(client.cmd_iter(\n                    host,\n                    'lxc.{0}'.format(fun),\n                    [name],\n                    kwarg={'path': path},\n                    timeout=60))\n        for cmd in cmds:\n            data = next(cmd)\n            data = data.get(host, {}).get('ret', None)\n            if data:\n                ret.update({host: data})\n    return ret", "output": "Invoke a function in the lxc module with no args\n\n    path\n        path to the container parent\n        default: /var/lib/lxc (system default)\n\n        .. versionadded:: 2015.8.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _concat(self, first_data, second_data):\n        \"\"\"\"\"\"\n        assert len(first_data) == len(\n            second_data), 'data source should contain the same size'\n        if first_data and second_data:\n            return [\n                concat(\n                    first_data[x],\n                    second_data[x],\n                    dim=0\n                ) for x in range(len(first_data))\n            ]\n        elif (not first_data) and (not second_data):\n            return []\n        else:\n            return [\n                first_data[0] if first_data else second_data[0]\n                for x in range(len(first_data))\n            ]", "output": "Helper function to concat two NDArrays.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def _oauth_get_user_future(\n        self, access_token: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"\n        \"\"\"\n        raise NotImplementedError()", "output": "Subclasses must override this to get basic information about the\n        user.\n\n        Should be a coroutine whose result is a dictionary\n        containing information about the user, which may have been\n        retrieved by using ``access_token`` to make a request to the\n        service.\n\n        The access token will be added to the returned dictionary to make\n        the result of `get_authenticated_user`.\n\n        .. versionchanged:: 5.1\n\n           Subclasses may also define this method with ``async def``.\n\n        .. versionchanged:: 6.0\n\n           A synchronous fallback to ``_oauth_get_user`` was removed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_view_on_selected(self, widget, selected_item_key):\n        \"\"\" \n        \"\"\"\n        self.lbl.set_text('List selection: ' + self.listView.children[selected_item_key].get_text())", "output": "The selection event of the listView, returns a key of the clicked event.\n            You can retrieve the item rapidly", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_top_states(saltenv='base'):\n    '''\n    \n    '''\n    alt_states = []\n    try:\n        returned = __salt__['state.show_top']()\n        for i in returned[saltenv]:\n            alt_states.append(i)\n    except Exception:\n        raise\n    # log.info(\"top states: %s\", alt_states)\n    return alt_states", "output": "Equivalent to a salt cli: salt web state.show_top", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parent_info(self):\n        \"\"\"\n        \"\"\"\n        parent_doc = self.parent\n        if parent_doc is None:\n            parent_path = _helpers.DOCUMENT_PATH_DELIMITER.join(\n                (self._client._database_string, \"documents\")\n            )\n        else:\n            parent_path = parent_doc._document_path\n\n        expected_prefix = _helpers.DOCUMENT_PATH_DELIMITER.join((parent_path, self.id))\n        return parent_path, expected_prefix", "output": "Get fully-qualified parent path and prefix for this collection.\n\n        Returns:\n            Tuple[str, str]: Pair of\n\n            * the fully-qualified (with database and project) path to the\n              parent of this collection (will either be the database path\n              or a document path).\n            * the prefix to a document in this collection.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(cls, src, dist=None):\n        \"\"\"\n        \"\"\"\n        m = cls.pattern.match(src)\n        if not m:\n            msg = \"EntryPoint must be in 'name=module:attrs [extras]' format\"\n            raise ValueError(msg, src)\n        res = m.groupdict()\n        extras = cls._parse_extras(res['extras'])\n        attrs = res['attr'].split('.') if res['attr'] else ()\n        return cls(res['name'], res['module'], attrs, extras, dist)", "output": "Parse a single entry point from string `src`\n\n        Entry point syntax follows the form::\n\n            name = some.module:some.attr [extra1, extra2]\n\n        The entry name and module name are required, but the ``:attrs`` and\n        ``[extras]`` parts are optional", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_insert_idx(pos_dict, name):\n    \"\"\n    keys,i = list(pos_dict.keys()),0\n    while i < len(keys) and str.lower(keys[i]) < str.lower(name): i+=1\n    if i == len(keys): return -1\n    else:              return pos_dict[keys[i]]", "output": "Return the position to insert a given function doc in a notebook.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def native_concat(nodes):\n    \"\"\"\n    \"\"\"\n    head = list(islice(nodes, 2))\n\n    if not head:\n        return None\n\n    if len(head) == 1:\n        out = head[0]\n    else:\n        out = u''.join([text_type(v) for v in chain(head, nodes)])\n\n    try:\n        return literal_eval(out)\n    except (ValueError, SyntaxError, MemoryError):\n        return out", "output": "Return a native Python type from the list of compiled nodes. If the\n    result is a single node, its value is returned. Otherwise, the nodes are\n    concatenated as strings. If the result can be parsed with\n    :func:`ast.literal_eval`, the parsed value is returned. Otherwise, the\n    string is returned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def droplevel(self, level, axis=0):\n        \"\"\"\n        \n        \"\"\"\n        labels = self._get_axis(axis)\n        new_labels = labels.droplevel(level)\n        result = self.set_axis(new_labels, axis=axis, inplace=False)\n        return result", "output": "Return DataFrame with requested index / column level(s) removed.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        level : int, str, or list-like\n            If a string is given, must be the name of a level\n            If list-like, elements must be names or positional indexes\n            of levels.\n\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n\n        Returns\n        -------\n        DataFrame.droplevel()\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([\n        ...     [1, 2, 3, 4],\n        ...     [5, 6, 7, 8],\n        ...     [9, 10, 11, 12]\n        ... ]).set_index([0, 1]).rename_axis(['a', 'b'])\n\n        >>> df.columns = pd.MultiIndex.from_tuples([\n        ...    ('c', 'e'), ('d', 'f')\n        ... ], names=['level_1', 'level_2'])\n\n        >>> df\n        level_1   c   d\n        level_2   e   f\n        a b\n        1 2      3   4\n        5 6      7   8\n        9 10    11  12\n\n        >>> df.droplevel('a')\n        level_1   c   d\n        level_2   e   f\n        b\n        2        3   4\n        6        7   8\n        10      11  12\n\n        >>> df.droplevel('level2', axis=1)\n        level_1   c   d\n        a b\n        1 2      3   4\n        5 6      7   8\n        9 10    11  12", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ppo_original_params():\n  \"\"\"\"\"\"\n  hparams = ppo_atari_base()\n  hparams.learning_rate_constant = 2.5e-4\n  hparams.gae_gamma = 0.99\n  hparams.gae_lambda = 0.95\n  hparams.clipping_coef = 0.1\n  hparams.value_loss_coef = 1\n  hparams.entropy_loss_coef = 0.01\n  hparams.eval_every_epochs = 200\n  hparams.dropout_ppo = 0.1\n  # The parameters below are modified to accommodate short epoch_length (which\n  # is needed for model based rollouts).\n  hparams.epoch_length = 50\n  hparams.optimization_batch_size = 20\n  return hparams", "output": "Parameters based on the original PPO paper.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_fun(fun):\n    '''\n    \n    '''\n    log.debug('sdstack_etcd returner <get_fun> called fun: %s', fun)\n    ret = {}\n    client, path = _get_conn(__opts__)\n    items = client.get('/'.join((path, 'minions')))\n    for item in items.children:\n        comps = str(item.key).split('/')\n        efun = salt.utils.json.loads(client.get('/'.join((path, 'jobs', str(item.value), comps[-1], 'fun'))).value)\n        if efun == fun:\n            ret[comps[-1]] = str(efun)\n    return ret", "output": "Return a dict of the last function called for all minions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def fetch_message(self, id):\n        \"\"\"\n        \"\"\"\n\n        channel = await self._get_channel()\n        data = await self._state.http.get_message(channel.id, id)\n        return self._state.create_message(channel=channel, data=data)", "output": "|coro|\n\n        Retrieves a single :class:`.Message` from the destination.\n\n        This can only be used by bot accounts.\n\n        Parameters\n        ------------\n        id: :class:`int`\n            The message ID to look for.\n\n        Raises\n        --------\n        :exc:`.NotFound`\n            The specified message was not found.\n        :exc:`.Forbidden`\n            You do not have the permissions required to get a message.\n        :exc:`.HTTPException`\n            Retrieving the message failed.\n\n        Returns\n        --------\n        :class:`.Message`\n            The message asked for.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attention_lm_moe_unscramble_base():\n  \"\"\"\"\"\"\n  hparams = attention_lm_no_moe_small()\n  hparams.use_inputs = True\n  hparams.min_length_bucket = 1024\n  hparams.max_length = 1024\n  hparams.batch_size = 5000\n  hparams.layer_prepostprocess_dropout = 0.0\n  hparams.layer_preprocess_sequence = \"n\"\n  hparams.layer_postprocess_sequence = \"da\"\n  return hparams", "output": "Version to use with languagemodel_wiki_scramble1k50.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _service_by_name(name):\n    '''\n    \n    '''\n    services = _available_services()\n    name = name.lower()\n\n    if name in services:\n        # Match on label\n        return services[name]\n\n    for service in six.itervalues(services):\n        if service['file_path'].lower() == name:\n            # Match on full path\n            return service\n        basename, ext = os.path.splitext(service['filename'])\n        if basename.lower() == name:\n            # Match on basename\n            return service\n\n    return False", "output": "Return the service info for a service by label, filename or path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _setup_user_dir(self):\n        \"\"\"\"\"\"\n        user_dir = self._get_user_dir_path()\n\n        rules_dir = user_dir.joinpath('rules')\n        if not rules_dir.is_dir():\n            rules_dir.mkdir(parents=True)\n        self.user_dir = user_dir", "output": "Returns user config dir, create it when it doesn't exist.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setCurrentIndex(self, y, x):\r\n        \"\"\"\"\"\"\r\n        self.dataTable.selectionModel().setCurrentIndex(\r\n            self.dataTable.model().index(y, x),\r\n            QItemSelectionModel.ClearAndSelect)", "output": "Set current selection.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def walk(self, where=\"/\"):\n        \"\"\" \n        \"\"\"\n        _tables()\n        self._check_if_open()\n        for g in self._handle.walk_groups(where):\n            if getattr(g._v_attrs, 'pandas_type', None) is not None:\n                continue\n\n            groups = []\n            leaves = []\n            for child in g._v_children.values():\n                pandas_type = getattr(child._v_attrs, 'pandas_type', None)\n                if pandas_type is None:\n                    if isinstance(child, _table_mod.group.Group):\n                        groups.append(child._v_name)\n                else:\n                    leaves.append(child._v_name)\n\n            yield (g._v_pathname.rstrip('/'), groups, leaves)", "output": "Walk the pytables group hierarchy for pandas objects\n\n        This generator will yield the group path, subgroups and pandas object\n        names for each group.\n        Any non-pandas PyTables objects that are not a group will be ignored.\n\n        The `where` group itself is listed first (preorder), then each of its\n        child groups (following an alphanumerical order) is also traversed,\n        following the same procedure.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        where : str, optional\n            Group where to start walking.\n            If not supplied, the root group is used.\n\n        Yields\n        ------\n        path : str\n            Full path to a group (without trailing '/')\n        groups : list of str\n            names of the groups contained in `path`\n        leaves : list of str\n            names of the pandas objects contained in `path`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_session(self, app_path, session_id):\n        ''' \n\n        '''\n        if app_path not in self._applications:\n            raise ValueError(\"Application %s does not exist on this server\" % app_path)\n        return self._applications[app_path].get_session(session_id)", "output": "Get an active a session by name application path and session ID.\n\n        Args:\n            app_path (str) :\n                The configured application path for the application to return\n                a session for.\n\n            session_id (str) :\n                The session ID of the session to retrieve.\n\n        Returns:\n            ServerSession", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def data_cifar10(train_start=0, train_end=50000, test_start=0, test_end=10000):\n  \"\"\"\n  \n  \"\"\"\n\n\n  # These values are specific to CIFAR10\n  img_rows = 32\n  img_cols = 32\n  nb_classes = 10\n\n  # the data, shuffled and split between train and test sets\n  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n  if tf.keras.backend.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n  else:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n  x_train = x_train.astype('float32')\n  x_test = x_test.astype('float32')\n  x_train /= 255\n  x_test /= 255\n  print('x_train shape:', x_train.shape)\n  print(x_train.shape[0], 'train samples')\n  print(x_test.shape[0], 'test samples')\n\n  # convert class vectors to binary class matrices\n  y_train = np_utils.to_categorical(y_train, nb_classes)\n  y_test = np_utils.to_categorical(y_test, nb_classes)\n\n  x_train = x_train[train_start:train_end, :, :, :]\n  y_train = y_train[train_start:train_end, :]\n  x_test = x_test[test_start:test_end, :]\n  y_test = y_test[test_start:test_end, :]\n\n  return x_train, y_train, x_test, y_test", "output": "Preprocess CIFAR10 dataset\n  :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_evaluation_functions(kind=None):\n    \"\"\"\n\n    \"\"\"\n\n    if kind is None:\n        kind = tuple(_REGSITRY_KIND_CLASS_MAP.keys())\n\n    if not isinstance(kind, tuple):\n        if kind not in _REGSITRY_KIND_CLASS_MAP.keys():\n            raise KeyError(\n                'Cannot find `kind` {}. Use '\n                '`list_evaluation_functions(kind=None).keys()` to get all the'\n                'valid kinds of evaluation functions.'.format(kind))\n\n        reg = registry.get_registry(_REGSITRY_KIND_CLASS_MAP[kind])\n        return list(reg.keys())\n    else:\n        return {name: list_evaluation_functions(kind=name) for name in kind}", "output": "Get valid word embedding functions names.\n\n    Parameters\n    ----------\n    kind : ['similarity', 'analogy', None]\n        Return only valid names for similarity, analogy or both kinds of functions.\n\n    Returns\n    -------\n    dict or list:\n        A list of all the valid evaluation function names for the specified\n        kind. If kind is set to None, returns a dict mapping each valid name to\n        its respective output list. The valid names can be plugged in\n        `gluonnlp.model.word_evaluation_model.create(name)`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_task_cls(cls, name):\n        \"\"\"\n        \n        \"\"\"\n        task_cls = cls._get_reg().get(name)\n        if not task_cls:\n            raise TaskClassNotFoundException(cls._missing_task_msg(name))\n\n        if task_cls == cls.AMBIGUOUS_CLASS:\n            raise TaskClassAmbigiousException('Task %r is ambiguous' % name)\n        return task_cls", "output": "Returns an unambiguous class or raises an exception.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_config(self):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        self.set_option('recent_projects', self.recent_projects)\r\n        self.set_option('expanded_state',\r\n                        self.explorer.treewidget.get_expanded_state())\r\n        self.set_option('scrollbar_position',\r\n                        self.explorer.treewidget.get_scrollbar_position())\r\n        if self.current_active_project and self.dockwidget:\r\n            self.set_option('visible_if_project_open',\r\n                            self.dockwidget.isVisible())", "output": "Save configuration: opened projects & tree widget state.\r\n\r\n        Also save whether dock widget is visible if a project is open.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def referenced_tables(self):\n        \"\"\"\n        \"\"\"\n        tables = []\n        datasets_by_project_name = {}\n\n        for table in self._job_statistics().get(\"referencedTables\", ()):\n\n            t_project = table[\"projectId\"]\n\n            ds_id = table[\"datasetId\"]\n            t_dataset = datasets_by_project_name.get((t_project, ds_id))\n            if t_dataset is None:\n                t_dataset = DatasetReference(t_project, ds_id)\n                datasets_by_project_name[(t_project, ds_id)] = t_dataset\n\n            t_name = table[\"tableId\"]\n            tables.append(t_dataset.table(t_name))\n\n        return tables", "output": "Return referenced tables from job statistics, if present.\n\n        See:\n        https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#statistics.query.referencedTables\n\n        :rtype: list of dict\n        :returns: mappings describing the query plan, or an empty list\n                  if the query has not yet completed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_s3_key():\n    '''\n    \n    '''\n\n    key = __opts__['s3.key'] if 's3.key' in __opts__ else None\n    keyid = __opts__['s3.keyid'] if 's3.keyid' in __opts__ else None\n    service_url = __opts__['s3.service_url'] \\\n        if 's3.service_url' in __opts__ \\\n        else None\n    verify_ssl = __opts__['s3.verify_ssl'] \\\n        if 's3.verify_ssl' in __opts__ \\\n        else None\n    kms_keyid = __opts__['aws.kmw.keyid'] if 'aws.kms.keyid' in __opts__ else None\n    location = __opts__['s3.location'] \\\n        if 's3.location' in __opts__ \\\n        else None\n    path_style = __opts__['s3.path_style'] \\\n        if 's3.path_style' in __opts__ \\\n        else None\n    https_enable = __opts__['s3.https_enable'] \\\n        if 's3.https_enable' in __opts__ \\\n        else None\n\n    return key, keyid, service_url, verify_ssl, kms_keyid, location, path_style, https_enable", "output": "Get AWS keys from pillar or config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_valid(self, data, label=None, weight=None, group=None,\n                     init_score=None, silent=False, params=None):\n        \"\"\"\n        \"\"\"\n        ret = Dataset(data, label=label, reference=self,\n                      weight=weight, group=group, init_score=init_score,\n                      silent=silent, params=params, free_raw_data=self.free_raw_data)\n        ret._predictor = self._predictor\n        ret.pandas_categorical = self.pandas_categorical\n        return ret", "output": "Create validation data align with current Dataset.\n\n        Parameters\n        ----------\n        data : string, numpy array, pandas DataFrame, H2O DataTable's Frame, scipy.sparse or list of numpy arrays\n            Data source of Dataset.\n            If string, it represents the path to txt file.\n        label : list, numpy 1-D array, pandas Series / one-column DataFrame or None, optional (default=None)\n            Label of the data.\n        weight : list, numpy 1-D array, pandas Series or None, optional (default=None)\n            Weight for each instance.\n        group : list, numpy 1-D array, pandas Series or None, optional (default=None)\n            Group/query size for Dataset.\n        init_score : list, numpy 1-D array, pandas Series or None, optional (default=None)\n            Init score for Dataset.\n        silent : bool, optional (default=False)\n            Whether to print messages during construction.\n        params : dict or None, optional (default=None)\n            Other parameters for validation Dataset.\n\n        Returns\n        -------\n        valid : Dataset\n            Validation Dataset with reference to self.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def import_and_get_task(task_path):\n    \"\"\"\n    \n    \"\"\"\n    module, function = task_path.rsplit('.', 1)\n    app_module = importlib.import_module(module)\n    app_function = getattr(app_module, function)\n    return app_function", "output": "Given a modular path to a function, import that module\n    and return the function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_to_datastore(self):\n    \"\"\"\"\"\"\n    client = self._datastore_client\n    with client.no_transact_batch() as client_batch:\n      for batch_id, batch_data in iteritems(self._data):\n        batch_key = client.key(self._entity_kind_batches, batch_id)\n        batch_entity = client.entity(batch_key)\n        for k, v in iteritems(batch_data):\n          if k != 'images':\n            batch_entity[k] = v\n        client_batch.put(batch_entity)\n        self._write_single_batch_images_internal(batch_id, client_batch)", "output": "Writes all image batches to the datastore.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def isID(self, doc, attr):\n        \"\"\" \"\"\"\n        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        if attr is None: attr__o = None\n        else: attr__o = attr._o\n        ret = libxml2mod.xmlIsID(doc__o, self._o, attr__o)\n        return ret", "output": "Determine whether an attribute is of type ID. In case we\n          have DTD(s) then this is done if DTD loading has been\n          requested. In the case of HTML documents parsed with the\n           HTML parser, then ID detection is done systematically.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def geometric_progression_for_stepsize(x, update, dist, decision_function,\n                                       current_iteration):\n  \"\"\" \n  \"\"\"\n  epsilon = dist / np.sqrt(current_iteration)\n  while True:\n    updated = x + epsilon * update\n    success = decision_function(updated[None])[0]\n    if success:\n      break\n    else:\n      epsilon = epsilon / 2.0\n\n  return epsilon", "output": "Geometric progression to search for stepsize.\n      Keep decreasing stepsize by half until reaching\n      the desired side of the boundary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expected_bar_values_2d(dates,\n                           assets,\n                           asset_info,\n                           colname,\n                           holes=None):\n    \"\"\"\n    \n    \"\"\"\n    if colname == 'volume':\n        dtype = uint32\n        missing = 0\n    else:\n        dtype = float64\n        missing = float('nan')\n\n    data = full((len(dates), len(assets)), missing, dtype=dtype)\n    for j, asset in enumerate(assets):\n        # Use missing values when asset_id is not contained in asset_info.\n        if asset not in asset_info.index:\n            continue\n\n        start = asset_start(asset_info, asset)\n        end = asset_end(asset_info, asset)\n        for i, date in enumerate(dates):\n            # No value expected for dates outside the asset's start/end\n            # date.\n            if not (start <= date <= end):\n                continue\n\n            if holes is not None:\n                expected = expected_bar_value_with_holes(\n                    asset,\n                    date,\n                    colname,\n                    holes,\n                    missing,\n                )\n            else:\n                expected = expected_bar_value(asset, date, colname)\n\n            data[i, j] = expected\n    return data", "output": "Return an 2D array containing cls.expected_value(asset_id, date,\n    colname) for each date/asset pair in the inputs.\n\n    Missing locs are filled with 0 for volume and NaN for price columns:\n\n        - Values before/after an asset's lifetime.\n        - Values for asset_ids not contained in asset_info.\n        - Locs defined in `holes`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def top(num_processes=5, interval=3):\n    '''\n    \n    '''\n    result = []\n    start_usage = {}\n    for pid in psutil.pids():\n        try:\n            process = psutil.Process(pid)\n            user, system = process.cpu_times()\n        except ValueError:\n            user, system, _, _ = process.cpu_times()\n        except psutil.NoSuchProcess:\n            continue\n        start_usage[process] = user + system\n    time.sleep(interval)\n    usage = set()\n    for process, start in six.iteritems(start_usage):\n        try:\n            user, system = process.cpu_times()\n        except ValueError:\n            user, system, _, _ = process.cpu_times()\n        except psutil.NoSuchProcess:\n            continue\n        now = user + system\n        diff = now - start\n        usage.add((diff, process))\n\n    for idx, (diff, process) in enumerate(reversed(sorted(usage))):\n        if num_processes and idx >= num_processes:\n            break\n        if not _get_proc_cmdline(process):\n            cmdline = _get_proc_name(process)\n        else:\n            cmdline = _get_proc_cmdline(process)\n        info = {'cmd': cmdline,\n                'user': _get_proc_username(process),\n                'status': _get_proc_status(process),\n                'pid': _get_proc_pid(process),\n                'create_time': _get_proc_create_time(process),\n                'cpu': {},\n                'mem': {},\n        }\n        for key, value in six.iteritems(process.cpu_times()._asdict()):\n            info['cpu'][key] = value\n        for key, value in six.iteritems(process.memory_info()._asdict()):\n            info['mem'][key] = value\n        result.append(info)\n\n    return result", "output": "Return a list of top CPU consuming processes during the interval.\n    num_processes = return the top N CPU consuming processes\n    interval = the number of seconds to sample CPU usage over\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' ps.top\n\n        salt '*' ps.top 5 10", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_basic_ngpu(nb_classes=10, input_shape=(None, 28, 28, 1), **kwargs):\n  \"\"\"\n  \n  \"\"\"\n  model = make_basic_cnn()\n  layers = model.layers\n\n  model = MLPnGPU(nb_classes, layers, input_shape)\n  return model", "output": "Create a multi-GPU model similar to the basic cnn in the tutorials.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def config_merge_text(source='running',\n                      merge_config=None,\n                      merge_path=None,\n                      saltenv='base'):\n    '''\n    \n    '''\n    config_txt = __salt__['net.config'](source=source)['out'][source]\n    return __salt__['iosconfig.merge_text'](initial_config=config_txt,\n                                            merge_config=merge_config,\n                                            merge_path=merge_path,\n                                            saltenv=saltenv)", "output": ".. versionadded:: 2019.2.0\n\n    Return the merge result of the configuration from ``source`` with the\n    merge configuration, as plain text (without loading the config on the\n    device).\n\n    source: ``running``\n        The configuration type to retrieve from the network device. Default:\n        ``running``. Available options: ``running``, ``startup``, ``candidate``.\n\n    merge_config\n        The config to be merged into the initial config, sent as text. This\n        argument is ignored when ``merge_path`` is set.\n\n    merge_path\n        Absolute or remote path from where to load the merge configuration\n        text. This argument allows any URI supported by\n        :py:func:`cp.get_url <salt.modules.cp.get_url>`), e.g., ``salt://``,\n        ``https://``, ``s3://``, ``ftp:/``, etc.\n\n    saltenv: ``base``\n        Salt fileserver environment from which to retrieve the file.\n        Ignored if ``merge_path`` is not a ``salt://`` URL.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm.config_merge_text merge_path=salt://path/to/merge.cfg", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def call(method, *args, **kwargs):\n    '''\n    \n    '''\n    kwargs = clean_kwargs(**kwargs)\n    if not netmiko_device['always_alive']:\n        connection = ConnectHandler(**netmiko_device['args'])\n        ret = getattr(connection, method)(*args, **kwargs)\n        connection.disconnect()\n        return ret\n    return getattr(netmiko_device['connection'], method)(*args, **kwargs)", "output": "Calls an arbitrary netmiko method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_status_code(code):\n    \"\"\"\n    \n    \"\"\"\n\n    def class_decorator(cls):\n        cls.status_code = code\n        _sanic_exceptions[code] = cls\n        return cls\n\n    return class_decorator", "output": "Decorator used for adding exceptions to :class:`SanicException`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def server_url_for_websocket_url(url):\n    ''' \n\n    '''\n    if url.startswith(\"ws:\"):\n        reprotocoled = \"http\" + url[2:]\n    elif url.startswith(\"wss:\"):\n        reprotocoled = \"https\" + url[3:]\n    else:\n        raise ValueError(\"URL has non-websocket protocol \" + url)\n    if not reprotocoled.endswith(\"/ws\"):\n        raise ValueError(\"websocket URL does not end in /ws\")\n    return reprotocoled[:-2]", "output": "Convert an ``ws(s)`` URL for a Bokeh server into the appropriate\n    ``http(s)`` URL for the websocket endpoint.\n\n    Args:\n        url (str):\n            An ``ws(s)`` URL ending in ``/ws``\n\n    Returns:\n        str:\n            The corresponding ``http(s)`` URL.\n\n    Raises:\n        ValueError:\n            If the input URL is not of the proper form.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def all_view_names_in_schema(self, schema, cache=False,\n                                 cache_timeout=None, force=False):\n        \"\"\"\n        \"\"\"\n        views = []\n        try:\n            views = self.db_engine_spec.get_view_names(\n                inspector=self.inspector, schema=schema)\n        except Exception as e:\n            logging.exception(e)\n        return views", "output": "Parameters need to be passed as keyword arguments.\n\n        For unused parameters, they are referenced in\n        cache_util.memoized_func decorator.\n\n        :param schema: schema name\n        :type schema: str\n        :param cache: whether cache is enabled for the function\n        :type cache: bool\n        :param cache_timeout: timeout in seconds for the cache\n        :type cache_timeout: int\n        :param force: whether to force refresh the cache\n        :type force: bool\n        :return: view list\n        :rtype: list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def psaux(name):\n    '''\n    \n    '''\n    sanitize_name = six.text_type(name)\n    pattern = re.compile(sanitize_name)\n    salt_exception_pattern = re.compile(\"salt.+ps.psaux.+\")\n    ps_aux = __salt__['cmd.run'](\"ps aux\")\n    found_infos = []\n    ret = []\n    nb_lines = 0\n    for info in ps_aux.splitlines():\n        found = pattern.search(info)\n        if found is not None:\n            # remove 'salt' command from results\n            if not salt_exception_pattern.search(info):\n                nb_lines += 1\n                found_infos.append(info)\n    pid_count = six.text_type(nb_lines) + \" occurence(s).\"\n    ret = []\n    ret.extend([sanitize_name, found_infos, pid_count])\n    return ret", "output": "Retrieve information corresponding to a \"ps aux\" filtered\n    with the given pattern. It could be just a name or a regular\n    expression (using python search from \"re\" module).\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.psaux www-data.+apache2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attention_lm_moe_small():\n  \"\"\"\n  \"\"\"\n  hparams = attention_lm_moe_base()\n  hparams.num_hidden_layers = 4\n  hparams.hidden_size = 512\n  hparams.filter_size = 2048\n  hparams.moe_num_experts = 128\n  hparams.moe_layers = \"2\"\n  return hparams", "output": "Cheap model for single-gpu training.\n\n  on lm1b_32k:\n     ~312M params\n     1.6 steps/sec on  [GeForce GTX TITAN X]\n     After 50K steps on 8 GPUs (synchronous):\n        eval_log_ppl_per_token = 3.31\n\n  Returns:\n    an hparams object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lvresize(size=None, lvpath=None, extents=None):\n    '''\n    \n\n    '''\n    if size and extents:\n        log.error('Error: Please specify only one of size or extents')\n        return {}\n\n    cmd = ['lvresize']\n\n    if size:\n        cmd.extend(['-L', '{0}'.format(size)])\n    elif extents:\n        cmd.extend(['-l', '{0}'.format(extents)])\n    else:\n        log.error('Error: Either size or extents must be specified')\n        return {}\n\n    cmd.append(lvpath)\n    cmd_ret = __salt__['cmd.run'](cmd, python_shell=False).splitlines()\n    return {'Output from lvresize': cmd_ret[0].strip()}", "output": "Return information about the logical volume(s)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n\n        salt '*' lvm.lvresize +12M /dev/mapper/vg1-test\n        salt '*' lvm.lvresize lvpath=/dev/mapper/vg1-test extents=+100%FREE", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode_list(data, encoding=None, errors='strict', keep=False,\n                preserve_dict_class=False, preserve_tuples=False):\n    '''\n    \n    '''\n    rv = []\n    for item in data:\n        if isinstance(item, list):\n            item = encode_list(item, encoding, errors, keep,\n                               preserve_dict_class, preserve_tuples)\n        elif isinstance(item, tuple):\n            item = encode_tuple(item, encoding, errors, keep, preserve_dict_class) \\\n                if preserve_tuples \\\n                else encode_list(item, encoding, errors, keep,\n                                 preserve_dict_class, preserve_tuples)\n        elif isinstance(item, Mapping):\n            item = encode_dict(item, encoding, errors, keep,\n                               preserve_dict_class, preserve_tuples)\n        else:\n            try:\n                item = salt.utils.stringutils.to_bytes(item, encoding, errors)\n            except TypeError:\n                # to_bytes raises a TypeError when input is not a\n                # string/bytestring/bytearray. This is expected and simply\n                # means we are going to leave the value as-is.\n                pass\n            except UnicodeEncodeError:\n                if not keep:\n                    raise\n\n        rv.append(item)\n    return rv", "output": "Encode all string values to bytes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_genshi(walker):\n    \"\"\"\n\n    \"\"\"\n    text = []\n    for token in walker:\n        type = token[\"type\"]\n        if type in (\"Characters\", \"SpaceCharacters\"):\n            text.append(token[\"data\"])\n        elif text:\n            yield TEXT, \"\".join(text), (None, -1, -1)\n            text = []\n\n        if type in (\"StartTag\", \"EmptyTag\"):\n            if token[\"namespace\"]:\n                name = \"{%s}%s\" % (token[\"namespace\"], token[\"name\"])\n            else:\n                name = token[\"name\"]\n            attrs = Attrs([(QName(\"{%s}%s\" % attr if attr[0] is not None else attr[1]), value)\n                           for attr, value in token[\"data\"].items()])\n            yield (START, (QName(name), attrs), (None, -1, -1))\n            if type == \"EmptyTag\":\n                type = \"EndTag\"\n\n        if type == \"EndTag\":\n            if token[\"namespace\"]:\n                name = \"{%s}%s\" % (token[\"namespace\"], token[\"name\"])\n            else:\n                name = token[\"name\"]\n\n            yield END, QName(name), (None, -1, -1)\n\n        elif type == \"Comment\":\n            yield COMMENT, token[\"data\"], (None, -1, -1)\n\n        elif type == \"Doctype\":\n            yield DOCTYPE, (token[\"name\"], token[\"publicId\"],\n                            token[\"systemId\"]), (None, -1, -1)\n\n        else:\n            pass  # FIXME: What to do?\n\n    if text:\n        yield TEXT, \"\".join(text), (None, -1, -1)", "output": "Convert a tree to a genshi tree\n\n    :arg walker: the treewalker to use to walk the tree to convert it\n\n    :returns: generator of genshi nodes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def identical(self, other):\n        \"\"\"\n        \n        \"\"\"\n        return (self.equals(other) and\n                all((getattr(self, c, None) == getattr(other, c, None)\n                     for c in self._comparables)) and\n                type(self) == type(other))", "output": "Similar to equals, but check that other comparable attributes are\n        also equal.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(kind, name, **kwargs):\n    \"\"\"\n\n    \"\"\"\n    if kind not in _REGSITRY_KIND_CLASS_MAP.keys():\n        raise KeyError(\n            'Cannot find `kind` {}. Use '\n            '`list_evaluation_functions(kind=None).keys()` to get'\n            'all the valid kinds of evaluation functions.'.format(kind))\n\n    create_ = registry.get_create_func(\n        _REGSITRY_KIND_CLASS_MAP[kind],\n        'word embedding {} evaluation function'.format(kind))\n\n    return create_(name, **kwargs)", "output": "Creates an instance of a registered word embedding evaluation function.\n\n    Parameters\n    ----------\n    kind : ['similarity', 'analogy']\n        Return only valid names for similarity, analogy or both kinds of\n        functions.\n    name : str\n        The evaluation function name (case-insensitive).\n\n\n    Returns\n    -------\n    An instance of\n    :class:`gluonnlp.embedding.evaluation.WordEmbeddingAnalogyFunction`:\n    or\n    :class:`gluonnlp.embedding.evaluation.WordEmbeddingSimilarityFunction`:\n        An instance of the specified evaluation function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ListDirectoryAbsolute(directory):\n  \"\"\"\"\"\"\n  return (os.path.join(directory, path)\n          for path in tf.io.gfile.listdir(directory))", "output": "Yields all files in the given directory. The paths are absolute.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def search_queries(self) -> Response:\n        \"\"\"\n        \n        \"\"\"\n        query = db.session.query(Query)\n        if security_manager.can_only_access_owned_queries():\n            search_user_id = g.user.get_user_id()\n        else:\n            search_user_id = request.args.get('user_id')\n        database_id = request.args.get('database_id')\n        search_text = request.args.get('search_text')\n        status = request.args.get('status')\n        # From and To time stamp should be Epoch timestamp in seconds\n        from_time = request.args.get('from')\n        to_time = request.args.get('to')\n\n        if search_user_id:\n            # Filter on user_id\n            query = query.filter(Query.user_id == search_user_id)\n\n        if database_id:\n            # Filter on db Id\n            query = query.filter(Query.database_id == database_id)\n\n        if status:\n            # Filter on status\n            query = query.filter(Query.status == status)\n\n        if search_text:\n            # Filter on search text\n            query = query \\\n                .filter(Query.sql.like('%{}%'.format(search_text)))\n\n        if from_time:\n            query = query.filter(Query.start_time > int(from_time))\n\n        if to_time:\n            query = query.filter(Query.start_time < int(to_time))\n\n        query_limit = config.get('QUERY_SEARCH_LIMIT', 1000)\n        sql_queries = (\n            query.order_by(Query.start_time.asc())\n            .limit(query_limit)\n            .all()\n        )\n\n        dict_queries = [q.to_dict() for q in sql_queries]\n\n        return Response(\n            json.dumps(dict_queries, default=utils.json_int_dttm_ser),\n            status=200,\n            mimetype='application/json')", "output": "Search for previously run sqllab queries. Used for Sqllab Query Search\n        page /superset/sqllab#search.\n\n        Custom permission can_only_search_queries_owned restricts queries\n        to only queries run by current user.\n\n        :returns: Response with list of sql query dicts", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _assert_all_loadable_terms_specialized_to(self, domain):\n        \"\"\"\n        \"\"\"\n        for term in self.graph.node:\n            if isinstance(term, LoadableTerm):\n                assert term.domain is domain", "output": "Make sure that we've specialized all loadable terms in the graph.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_plugin_apps(self):\n    \"\"\"\n    \"\"\"\n    return {\n        '/infer': self._infer,\n        '/update_example': self._update_example,\n        '/examples_from_path': self._examples_from_path_handler,\n        '/sprite': self._serve_sprite,\n        '/duplicate_example': self._duplicate_example,\n        '/delete_example': self._delete_example,\n        '/infer_mutants': self._infer_mutants_handler,\n        '/eligible_features': self._eligible_features_from_example_handler,\n    }", "output": "Obtains a mapping between routes and handlers. Stores the logdir.\n\n    Returns:\n      A mapping between routes and handlers (functions that respond to\n      requests).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_subnet_group(name, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    if not conn:\n        return False\n    try:\n        conn.delete_cache_subnet_group(name)\n        msg = 'Deleted ElastiCache subnet group {0}.'.format(name)\n        log.info(msg)\n        return True\n    except boto.exception.BotoServerError as e:\n        log.debug(e)\n        msg = 'Failed to delete ElastiCache subnet group {0}'.format(name)\n        log.error(msg)\n        return False", "output": "Delete an ElastiCache subnet group.\n\n    CLI example::\n\n        salt myminion boto_elasticache.delete_subnet_group my-subnet-group \\\n                region=us-east-1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_best_ip_by_real_data_fetch(_type='stock'):\n    \"\"\"\n    \n    \"\"\"\n    from QUANTAXIS.QAUtil.QADate import QA_util_today_str\n    import time\n    \n    #\u627e\u5230\u524d\u4e24\u5929\u7684\u6709\u6548\u4ea4\u6613\u65e5\u671f\n    pre_trade_date=QA_util_get_real_date(QA_util_today_str())\n    pre_trade_date=QA_util_get_real_date(pre_trade_date)\n    \n    # \u67d0\u4e2a\u51fd\u6570\u83b7\u53d6\u7684\u8017\u65f6\u6d4b\u8bd5\n    def get_stock_data_by_ip(ips):\n        start=time.time()\n        try:\n            QA_fetch_get_stock_transaction('000001',pre_trade_date,pre_trade_date,2,ips['ip'],ips['port'])\n            end=time.time()\n            return end-start\n        except:\n            return 9999\n\n    def get_future_data_by_ip(ips):\n        start=time.time()\n        try:\n            QA_fetch_get_future_transaction('RBL8',pre_trade_date,pre_trade_date,2,ips['ip'],ips['port'])\n            end=time.time()\n            return end-start\n        except:\n            return 9999\n\n    func,ip_list=0,0\n    if _type=='stock':\n        func,ip_list=get_stock_data_by_ip,stock_ip_list\n    else:\n        func,ip_list=get_future_data_by_ip,future_ip_list  \n    from pathos.multiprocessing import Pool\n    def multiMap(func,sequence):\n        res=[]\n        pool=Pool(4)\n        for i in sequence:\n            res.append(pool.apply_async(func,(i,)))\n        pool.close()\n        pool.join()\n        return list(map(lambda x:x.get(),res))\n   \n    res=multiMap(func,ip_list)\n    index=res.index(min(res))\n    return ip_list[index]", "output": "\u7528\u7279\u5b9a\u7684\u6570\u636e\u83b7\u53d6\u51fd\u6570\u6d4b\u8bd5\u6570\u636e\u83b7\u5f97\u7684\u65f6\u95f4,\u4ece\u800c\u9009\u62e9\u4e0b\u8f7d\u6570\u636e\u6700\u5feb\u7684\u670d\u52a1\u5668ip\n    \u9ed8\u8ba4\u4f7f\u7528\u7279\u5b9a\u54c1\u79cd1min\u7684\u65b9\u5f0f\u7684\u83b7\u53d6", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uninstall(pkg, dir=None, runas=None, env=None):\n    '''\n    \n\n    '''\n    # Protect against injection\n    if pkg:\n        pkg = _cmd_quote(pkg)\n\n    env = env or {}\n\n    if runas:\n        uid = salt.utils.user.get_uid(runas)\n        if uid:\n            env.update({'SUDO_UID': uid, 'SUDO_USER': ''})\n\n    cmd = ['npm', 'uninstall', '\"{0}\"'.format(pkg)]\n    if not dir:\n        cmd.append('--global')\n\n    cmd = ' '.join(cmd)\n\n    result = __salt__['cmd.run_all'](cmd, python_shell=True, cwd=dir, runas=runas, env=env)\n\n    if result['retcode'] != 0:\n        log.error(result['stderr'])\n        return False\n    return True", "output": "Uninstall an NPM package.\n\n    If no directory is specified, the package will be uninstalled globally.\n\n    pkg\n        A package name in any format accepted by NPM\n\n    dir\n        The target directory from which to uninstall the package, or None for\n        global installation\n\n    runas\n        The user to run NPM with\n\n    env\n        Environment variables to set when invoking npm. Uses the same ``env``\n        format as the :py:func:`cmd.run <salt.modules.cmdmod.run>` execution\n        function.\n\n        .. versionadded:: 2015.5.3\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' npm.uninstall coffee-script", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_cache(self, back=None):\n        '''\n        \n        '''\n        back = self.backends(back)\n        cleared = []\n        errors = []\n        for fsb in back:\n            fstr = '{0}.clear_cache'.format(fsb)\n            if fstr in self.servers:\n                log.debug('Clearing %s fileserver cache', fsb)\n                failed = self.servers[fstr]()\n                if failed:\n                    errors.extend(failed)\n                else:\n                    cleared.append(\n                        'The {0} fileserver cache was successfully cleared'\n                        .format(fsb)\n                    )\n        return cleared, errors", "output": "Clear the cache of all of the fileserver backends that support the\n        clear_cache function or the named backend(s) only.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hybrid_forward(self, F, a, b):\n        \"\"\"\n        \n        \"\"\"\n        # a.shape = [B, L1, H]\n        # b.shape = [B, L2, H]\n        # extract features\n        tilde_a = self.f(a)  # shape = [B, L1, H]\n        tilde_b = self.f(b)  # shape = [B, L2, H]\n        # attention\n        # e.shape = [B, L1, L2]\n        e = F.batch_dot(tilde_a, tilde_b, transpose_b=True)\n        # beta: b align to a, [B, L1, H]\n        beta = F.batch_dot(e.softmax(), tilde_b)\n        # alpha: a align to b, [B, L2, H]\n        alpha = F.batch_dot(e.transpose([0, 2, 1]).softmax(), tilde_a)\n        # compare\n        feature1 = self.g(F.concat(tilde_a, beta, dim=2))\n        feature2 = self.g(F.concat(tilde_b, alpha, dim=2))\n        feature1 = feature1.sum(axis=1)\n        feature2 = feature2.sum(axis=1)\n        yhat = self.h(F.concat(feature1, feature2, dim=1))\n        return yhat", "output": "Forward of Decomposable Attention layer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_project_view(self):\r\n        \"\"\"\"\"\"\r\n        for i in [1, 2, 3]:\r\n            self.hideColumn(i)\r\n        self.setHeaderHidden(True)\r\n        # Disable the view of .spyproject. \r\n        self.filter_directories()", "output": "Setup view for projects", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def categorical_df_concat(df_list, inplace=False):\n    \"\"\"\n    \n    \"\"\"\n\n    if not inplace:\n        df_list = deepcopy(df_list)\n\n    # Assert each dataframe has the same columns/dtypes\n    df = df_list[0]\n    if not all([(df.dtypes.equals(df_i.dtypes)) for df_i in df_list[1:]]):\n        raise ValueError(\"Input DataFrames must have the same columns/dtypes.\")\n\n    categorical_columns = df.columns[df.dtypes == 'category']\n\n    for col in categorical_columns:\n        new_categories = sorted(\n            set().union(\n                *(frame[col].cat.categories for frame in df_list)\n            )\n        )\n\n        with ignore_pandas_nan_categorical_warning():\n            for df in df_list:\n                df[col].cat.set_categories(new_categories, inplace=True)\n\n    return pd.concat(df_list)", "output": "Prepare list of pandas DataFrames to be used as input to pd.concat.\n    Ensure any columns of type 'category' have the same categories across each\n    dataframe.\n\n    Parameters\n    ----------\n    df_list : list\n        List of dataframes with same columns.\n    inplace : bool\n        True if input list can be modified. Default is False.\n\n    Returns\n    -------\n    concatenated : df\n        Dataframe of concatenated list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sync_job_info(self, job_name):\n        \"\"\"\n        \"\"\"\n        job_path = os.path.join(self._logdir, job_name)\n\n        if job_name not in self._monitored_jobs:\n            self._create_job_info(job_path)\n            self._monitored_jobs.add(job_name)\n        else:\n            self._update_job_info(job_path)\n\n        expr_dirs = filter(lambda d: os.path.isdir(os.path.join(job_path, d)),\n                           os.listdir(job_path))\n\n        for expr_dir_name in expr_dirs:\n            self.sync_trial_info(job_path, expr_dir_name)\n\n        self._update_job_info(job_path)", "output": "Load information of the job with the given job name.\n\n        1. Traverse each experiment sub-directory and sync information\n           for each trial.\n        2. Create or update the job information, together with the job\n           meta file.\n\n        Args:\n            job_name (str) name of the Tune experiment", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_loaded_rules(rules_paths):\n    \"\"\"\n\n    \"\"\"\n    for path in rules_paths:\n        if path.name != '__init__.py':\n            rule = Rule.from_path(path)\n            if rule.is_enabled:\n                yield rule", "output": "Yields all available rules.\n\n    :type rules_paths: [Path]\n    :rtype: Iterable[Rule]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def follow_info(self, index=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, \"_ccr\", \"info\"), params=params\n        )", "output": "`<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-follow-info.html>`_\n\n        :arg index: A comma-separated list of index patterns; use `_all` to\n            perform the operation on all indices", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _process_batch(self, param, df_name):\n        \"\"\"\n        \"\"\"\n        if param.eval_metric is not None:\n            metrics = dict(param.eval_metric.get_name_value())\n            param.eval_metric.reset()\n        else:\n            metrics = {}\n        metrics['elapsed'] = datetime.datetime.now() - self.start_time\n        for key, value in metrics.items():\n            if key not in self._data[df_name]:\n                self._data[df_name][key] = []\n            self._data[df_name][key].append(value)", "output": "Update selected dataframe after a completed batch\n        Parameters\n        ----------\n        df_name : str\n            Selected dataframe name needs to be modified.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_tokens_and_tags(parse_str):\n  \"\"\"\"\"\"\n  tokens = []\n  parse_split = parse_str.split(' ')\n  for p in parse_split:\n    assert p.startswith('(') or p.endswith(')')\n    if p.endswith(')'):\n      token = p.replace(')', '')\n      tokens.append(token)\n\n  return tokens", "output": "Parse str to tokens and pos tags.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_remote_config(experiment_config, port, config_file_name):\n    ''''''\n    #set machine_list\n    request_data = dict()\n    request_data['machine_list'] = experiment_config['machineList']\n    if request_data['machine_list']:\n        for i in range(len(request_data['machine_list'])):\n            if isinstance(request_data['machine_list'][i].get('gpuIndices'), int):\n                request_data['machine_list'][i]['gpuIndices'] = str(request_data['machine_list'][i].get('gpuIndices'))\n    response = rest_put(cluster_metadata_url(port), json.dumps(request_data), REST_TIME_OUT)\n    err_message = ''\n    if not response or not check_response(response):\n        if response is not None:\n            err_message = response.text\n            _, stderr_full_path = get_log_path(config_file_name)\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n        return False, err_message\n    result, message = setNNIManagerIp(experiment_config, port, config_file_name)\n    if not result:\n        return result, message\n    #set trial_config\n    return set_trial_config(experiment_config, port, config_file_name), err_message", "output": "Call setClusterMetadata to pass trial", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_sun_flare(img, flare_center_x, flare_center_y, src_radius, src_color, circles):\n    \"\"\"\n\n    \"\"\"\n    non_rgb_warning(img)\n\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype('uint8'))\n        needs_float = True\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError('Unexpected dtype {} for RandomSunFlareaugmentation'.format(input_dtype))\n\n    overlay = img.copy()\n    output = img.copy()\n\n    for (alpha, (x, y), rad3, (r_color, g_color, b_color)) in circles:\n        cv2.circle(overlay, (x, y), rad3, (r_color, g_color, b_color), -1)\n\n        cv2.addWeighted(overlay, alpha, output, 1 - alpha, 0, output)\n\n    point = (int(flare_center_x), int(flare_center_y))\n\n    overlay = output.copy()\n    num_times = src_radius // 10\n    alpha = np.linspace(0.0, 1, num=num_times)\n    rad = np.linspace(1, src_radius, num=num_times)\n    for i in range(num_times):\n        cv2.circle(overlay, point, int(rad[i]), src_color, -1)\n        alp = alpha[num_times - i - 1] * alpha[num_times - i - 1] * alpha[num_times - i - 1]\n        cv2.addWeighted(overlay, alp, output, 1 - alp, 0, output)\n\n    image_rgb = output\n\n    if needs_float:\n        image_rgb = to_float(image_rgb, max_value=255)\n\n    return image_rgb", "output": "Add sun flare.\n\n    From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n\n    Args:\n        img (np.array):\n        flare_center_x (float):\n        flare_center_y (float):\n        src_radius:\n        src_color (int, int, int):\n        circles (list):\n\n    Returns:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def permissions_for(self, user=None):\n        \"\"\"\n        \"\"\"\n\n        base = Permissions.text()\n        base.send_tts_messages = False\n        base.manage_messages = False\n        return base", "output": "Handles permission resolution for a :class:`User`.\n\n        This function is there for compatibility with other channel types.\n\n        Actual direct messages do not really have the concept of permissions.\n\n        This returns all the Text related permissions set to true except:\n\n        - send_tts_messages: You cannot send TTS messages in a DM.\n        - manage_messages: You cannot delete others messages in a DM.\n\n        Parameters\n        -----------\n        user: :class:`User`\n            The user to check permissions for. This parameter is ignored\n            but kept for compatibility.\n\n        Returns\n        --------\n        :class:`Permissions`\n            The resolved permissions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def key_from_protobuf(pb):\n    \"\"\"\n    \"\"\"\n    path_args = []\n    for element in pb.path:\n        path_args.append(element.kind)\n        if element.id:  # Simple field (int64)\n            path_args.append(element.id)\n        # This is safe: we expect proto objects returned will only have\n        # one of `name` or `id` set.\n        if element.name:  # Simple field (string)\n            path_args.append(element.name)\n\n    project = None\n    if pb.partition_id.project_id:  # Simple field (string)\n        project = pb.partition_id.project_id\n    namespace = None\n    if pb.partition_id.namespace_id:  # Simple field (string)\n        namespace = pb.partition_id.namespace_id\n\n    return Key(*path_args, namespace=namespace, project=project)", "output": "Factory method for creating a key based on a protobuf.\n\n    The protobuf should be one returned from the Cloud Datastore\n    Protobuf API.\n\n    :type pb: :class:`.entity_pb2.Key`\n    :param pb: The Protobuf representing the key.\n\n    :rtype: :class:`google.cloud.datastore.key.Key`\n    :returns: a new `Key` instance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pause(self, seconds):\n        \"\"\"  \"\"\"\n        if self._driver.w3c:\n            self.w3c_actions.pointer_action.pause(seconds)\n            self.w3c_actions.key_action.pause(seconds)\n        else:\n            self._actions.append(lambda: time.sleep(seconds))\n        return self", "output": "Pause all inputs for the specified duration in seconds", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_list_header(value):\n    \"\"\"\n    \"\"\"\n    result = []\n    for item in _parse_list_header(value):\n        if item[:1] == item[-1:] == '\"':\n            item = unquote_header_value(item[1:-1])\n        result.append(item)\n    return result", "output": "Parse lists as described by RFC 2068 Section 2.\n\n    In particular, parse comma-separated lists where the elements of\n    the list may include quoted-strings.  A quoted-string could\n    contain a comma.  A non-quoted string could have quotes in the\n    middle.  Quotes are removed automatically after parsing.\n\n    It basically works like :func:`parse_set_header` just that items\n    may appear multiple times and case sensitivity is preserved.\n\n    The return value is a standard :class:`list`:\n\n    >>> parse_list_header('token, \"quoted value\"')\n    ['token', 'quoted value']\n\n    To create a header from the :class:`list` again, use the\n    :func:`dump_header` function.\n\n    :param value: a string with a list header.\n    :return: :class:`list`\n    :rtype: list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self, **kwargs):\n        \"\"\"\n\n        \"\"\"\n        kwargs.setdefault('script', self.script)\n        kwargs.setdefault('output', self.output)\n        return Command(**kwargs)", "output": "Returns new command with replaced fields.\n\n        :rtype: Command", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait_for_psexecsvc(host, port, username, password, timeout=900):\n    '''\n    \n    '''\n    if has_winexe() and not HAS_PSEXEC:\n        return wait_for_winexe(host, port, username, password, timeout)\n    start = time.time()\n    try_count = 0\n    while True:\n        try_count += 1\n        ret_code = 1\n        try:\n            stdout, stderr, ret_code = run_psexec_command(\n                'cmd.exe', '/c hostname', host, username, password, port=port\n            )\n        except Exception as exc:\n            log.exception(\"Unable to execute command\")\n        if ret_code == 0:\n            log.debug('psexec connected...')\n            return True\n        if time.time() - start > timeout:\n            return False\n        log.debug(\n            'Retrying psexec connection to host %s on port %s (try %s)',\n            host, port, try_count\n        )\n        time.sleep(1)", "output": "Wait until psexec connection can be established.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_image(vm_):\n    '''\n    \n    '''\n    images = avail_images()\n    vm_image = config.get_cloud_config_value(\n        'image', vm_, __opts__, search_global=False\n    )\n    if not isinstance(vm_image, six.string_types):\n        vm_image = six.text_type(vm_image)\n\n    for image in images:\n        if vm_image in (images[image]['name'],\n                        images[image]['slug'],\n                        images[image]['id']):\n            if images[image]['slug'] is not None:\n                return images[image]['slug']\n            return int(images[image]['id'])\n    raise SaltCloudNotFound(\n        'The specified image, \\'{0}\\', could not be found.'.format(vm_image)\n    )", "output": "Return the image object to use", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dbg_exec_magic(self, magic, args=''):\n        \"\"\"\"\"\"\n        code = \"!get_ipython().kernel.shell.run_line_magic('{}', '{}')\".format(\n                    magic, args)\n        self.kernel_client.input(code)", "output": "Run an IPython magic while debugging.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_type(self, frames):\n        \"\"\"\n        \n        \"\"\"\n        if self.vtype == 'mouth':\n            self.process_frames_mouth(frames)\n        elif self.vtype == 'face':\n            self.process_frames_face(frames)\n        else:\n            raise Exception('Video type not found')", "output": "Config video types", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def distinfo_dirname(cls, name, version):\n        \"\"\"\n        \"\"\"\n        name = name.replace('-', '_')\n        return '-'.join([name, version]) + DISTINFO_EXT", "output": "The *name* and *version* parameters are converted into their\n        filename-escaped form, i.e. any ``'-'`` characters are replaced\n        with ``'_'`` other than the one in ``'dist-info'`` and the one\n        separating the name from the version number.\n\n        :parameter name: is converted to a standard distribution name by replacing\n                         any runs of non- alphanumeric characters with a single\n                         ``'-'``.\n        :type name: string\n        :parameter version: is converted to a standard version string. Spaces\n                            become dots, and all other non-alphanumeric characters\n                            (except dots) become dashes, with runs of multiple\n                            dashes condensed to a single dash.\n        :type version: string\n        :returns: directory name\n        :rtype: string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_proto(self):\n    \"\"\"\"\"\"\n    # Return the proto.SplitInfo, sorted by name\n    return sorted((s.get_proto() for s in self.values()), key=lambda s: s.name)", "output": "Returns a list of SplitInfo protos that we have.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _lcs(x, y):\n  \"\"\"\n  \"\"\"\n  n, m = len(x), len(y)\n  table = {}\n  for i in range(n + 1):\n    for j in range(m + 1):\n      if i == 0 or j == 0:\n        table[i, j] = 0\n      elif x[i - 1] == y[j - 1]:\n        table[i, j] = table[i - 1, j - 1] + 1\n      else:\n        table[i, j] = max(table[i - 1, j], table[i, j - 1])\n  return table", "output": "Computes the length of the LCS between two seqs.\n\n  The implementation below uses a DP programming algorithm and runs\n  in O(nm) time where n = len(x) and m = len(y).\n  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n  Args:\n    x: collection of words\n    y: collection of words\n\n  Returns:\n    Table of dictionary of coord and len lcs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_data(self, columns, type='ndarray', with_index=False):\n        \"\"\"\n        \"\"\"\n\n        res = self.select_columns(columns)\n        if type == 'ndarray':\n            if with_index:\n                return res.reset_index().values\n            else:\n                return res.values\n        elif type == 'list':\n            if with_index:\n                return res.reset_index().values.tolist()\n            else:\n                return res.values.tolist()\n        elif type == 'dataframe':\n            if with_index:\n                return res.reset_index()\n            else:\n                return res", "output": "\u83b7\u53d6\u4e0d\u540c\u683c\u5f0f\u7684\u6570\u636e\n\n        Arguments:\n            columns {[type]} -- [description]\n\n        Keyword Arguments:\n            type {str} -- [description] (default: {'ndarray'})\n            with_index {bool} -- [description] (default: {False})\n\n        Returns:\n            [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _chunks(l, ncols):\n    \"\"\"\"\"\"\n    assert isinstance(ncols, int), \"ncols must be an integer\"\n    for i in range(0, len(l), ncols):\n        yield l[i: i+ncols]", "output": "Yield successive n-sized chunks from list, l.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fileserver(opts, backends):\n    '''\n    \n    '''\n    return LazyLoader(_module_dirs(opts, 'fileserver'),\n                      opts,\n                      tag='fileserver',\n                      whitelist=backends,\n                      pack={'__utils__': utils(opts)})", "output": "Returns the file server modules", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def maybe_pause_consumer(self):\n        \"\"\"\"\"\"\n        if self.load >= 1.0:\n            if self._consumer is not None and not self._consumer.is_paused:\n                _LOGGER.debug(\"Message backlog over load at %.2f, pausing.\", self.load)\n                self._consumer.pause()", "output": "Check the current load and pause the consumer if needed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get(url, profile):\n    ''''''\n    request_url = \"{0}/api/dashboards/{1}\".format(profile.get('grafana_url'),\n                                                  url)\n    response = requests.get(\n        request_url,\n        headers={\n            \"Accept\": \"application/json\",\n            \"Authorization\": \"Bearer {0}\".format(profile.get('grafana_token'))\n        },\n        timeout=profile.get('grafana_timeout', 3),\n    )\n    data = response.json()\n    if data.get('message') == 'Not found':\n        return None\n    if 'dashboard' not in data:\n        return None\n    return data['dashboard']", "output": "Get a specific dashboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _scan_payload(self):\n        '''\n        \n        '''\n        # Get ignored points\n        allowed = list()\n        for allowed_dir in self.db.get(AllowedDir):\n            if os.path.exists(allowed_dir.path):\n                allowed.append(allowed_dir.path)\n\n        ignored = list()\n        if not allowed:\n            for ignored_dir in self.db.get(IgnoredDir):\n                if os.path.exists(ignored_dir.path):\n                    ignored.append(ignored_dir.path)\n\n        all_files = list()\n        all_dirs = list()\n        all_links = list()\n        for entry_path in [pth for pth in (allowed or os.listdir(\"/\")) if pth]:\n            if entry_path[0] != \"/\":\n                entry_path = \"/{0}\".format(entry_path)\n            if entry_path in ignored or os.path.islink(entry_path):\n                continue\n            e_files, e_dirs, e_links = self._get_all_files(entry_path, *ignored)\n            all_files.extend(e_files)\n            all_dirs.extend(e_dirs)\n            all_links.extend(e_links)\n\n        return self._get_unmanaged_files(self._get_managed_files(), (all_files, all_dirs, all_links,))", "output": "Scan the system.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate(self):\n        \"\"\"\n        \n        \"\"\"\n        if self._method not in _RANK_METHODS:\n            raise UnknownRankMethod(\n                method=self._method,\n                choices=set(_RANK_METHODS),\n            )\n        return super(Rank, self)._validate()", "output": "Verify that the stored rank method is valid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cache_provider_details(conn=None):\n    '''\n    \n    '''\n    DETAILS['avail_locations'] = {}\n    DETAILS['avail_sizes'] = {}\n    DETAILS['avail_images'] = {}\n    locations = avail_locations(conn)\n    images = avail_images(conn)\n    sizes = avail_sizes(conn)\n\n    for key, location in six.iteritems(locations):\n        DETAILS['avail_locations'][location['name']] = location\n        DETAILS['avail_locations'][key] = location\n\n    for key, image in six.iteritems(images):\n        DETAILS['avail_images'][image['name']] = image\n        DETAILS['avail_images'][key] = image\n\n    for key, vm_size in six.iteritems(sizes):\n        DETAILS['avail_sizes'][vm_size['name']] = vm_size\n        DETAILS['avail_sizes'][key] = vm_size", "output": "Provide a place to hang onto results of --list-[locations|sizes|images]\n    so we don't have to go out to the API and get them every time.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_conflicting(self):\n        \"\"\"\"\"\"\n        # unknown installed version is also considered conflicting\n        if self.installed_version == self.UNKNOWN_VERSION:\n            return True\n        ver_spec = (self.version_spec if self.version_spec else '')\n        req_version_str = '{0}{1}'.format(self.project_name, ver_spec)\n        req_obj = pkg_resources.Requirement.parse(req_version_str)\n        return self.installed_version not in req_obj", "output": "If installed version conflicts with required version", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy_id(self):\n        '''\n        \n        '''\n        stdout, stderr, retcode = self._run_cmd(self._copy_id_str_old())\n        if salt.defaults.exitcodes.EX_OK != retcode and 'Usage' in stderr:\n            stdout, stderr, retcode = self._run_cmd(self._copy_id_str_new())\n        return stdout, stderr, retcode", "output": "Execute ssh-copy-id to plant the id file on the target", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __with_argument(node, value):\n    \"\"\"\"\"\"\n    arguments = node.getElementsByTagName('Argument')\n\n    if arguments:\n        logging.debug('Found argument within %s', value['name'])\n        value['flags'] = vsflags(VSFlags.UserValueIgnored, VSFlags.Continue)", "output": "Modifies the flags in value if the node contains an Argument.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_inception_score(images, splits=10):\n    \"\"\"\n    \n    \"\"\"\n    assert (images.shape[1] == 3)\n\n    # load inception model\n    if inception_model is None:\n        _init_inception()\n\n    # resize images to adapt inception model(inceptionV3)\n    if images.shape[2] != 299:\n        images = resize(images, 299, 299)\n\n    preds = []\n    bs = 4\n    n_batches = int(math.ceil(float(images.shape[0])/float(bs)))\n\n    # to get the predictions/picture of inception model\n    for i in range(n_batches):\n        sys.stdout.write(\".\")\n        sys.stdout.flush()\n        inps = images[(i * bs):min((i + 1) * bs, len(images))]\n        # inps size. bs x 3 x 299 x 299\n        pred = nd.softmax(inception_model(inps))\n        # pred size. bs x 1000\n        preds.append(pred.asnumpy())\n\n    # list to array\n    preds = np.concatenate(preds, 0)\n    scores = []\n\n    # to calculate the inception_score each split.\n    for i in range(splits):\n        # extract per split image pred\n        part = preds[(i * preds.shape[0] // splits):((i + 1) * preds.shape[0] // splits), :]\n        kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n        kl = np.mean(np.sum(kl, 1))\n        scores.append(np.exp(kl))\n\n    return np.mean(scores), np.std(scores)", "output": "Inception_score function.\n        The images will be divided into 'splits' parts, and calculate each inception_score separately,\n        then return the mean and std of inception_scores of these parts.\n    :param images: Images(num x c x w x h) that needs to calculate inception_score.\n    :param splits:\n    :return: mean and std of inception_score", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ShowMessage(self, title, message, filename=None, data=None, data_base64=None, messageicon=None, time=10000):\n        '''\n        \n        '''\n        if messageicon is None:\n            self.TaskBarIcon.ShowBalloon(title, message, msec=time)\n        else:\n            self.TaskBarIcon.ShowBalloon(title, message, msec=time, flags=messageicon)\n\n        return self", "output": "Shows a balloon above icon in system tray\n        :param title:  Title shown in balloon\n        :param message: Message to be displayed\n        :param filename: Optional icon filename\n        :param data: Optional in-ram icon\n        :param data_base64: Optional base64 icon\n        :param time: How long to display message in milliseconds\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def upgrade_available(pkg,\n                      bin_env=None,\n                      user=None,\n                      cwd=None):\n    '''\n    \n    '''\n    return pkg in list_upgrades(bin_env=bin_env, user=user, cwd=cwd)", "output": ".. versionadded:: 2015.5.0\n\n    Check whether or not an upgrade is available for a given package\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pip.upgrade_available <package name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dragEnterEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        if mimedata2url(event.mimeData()):\r\n            event.accept()\r\n        else:\r\n            event.ignore()", "output": "Allow user to drag files", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lookup(search=None, order=None, one=False):\n    '''\n    \n    '''\n    ret = {}\n    # vmadm lookup [-j|-1] [-o field,...] [field=value ...]\n    cmd = 'vmadm lookup {one} {order} {search}'.format(\n        one='-1' if one else '-j',\n        order='-o {0}'.format(order) if order else '',\n        search=search if search else ''\n    )\n    res = __salt__['cmd.run_all'](cmd)\n    retcode = res['retcode']\n    result = []\n    if retcode != 0:\n        ret['Error'] = res['stderr'] if 'stderr' in res else _exit_status(retcode)\n        return ret\n\n    if one:\n        result = res['stdout']\n    else:\n        for vm in salt.utils.json.loads(res['stdout']):\n            result.append(vm)\n\n    return result", "output": "Return a list of VMs using lookup\n\n    search : string\n        vmadm filter property\n    order : string\n        vmadm order (-o) property -- Default: uuid,type,ram,state,alias\n    one : boolean\n        return only one result (vmadm's -1)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' vmadm.lookup search='state=running'\n        salt '*' vmadm.lookup search='state=running' order=uuid,alias,hostname\n        salt '*' vmadm.lookup search='alias=nacl' one=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_examples(self, archive, directory):\n    \"\"\"\"\"\"\n    reg = re.compile(os.path.join(\"^%s\" % directory, \"(?P<label>neg|pos)\", \"\"))\n    for path, imdb_f in archive:\n      res = reg.match(path)\n      if not res:\n        continue\n      text = imdb_f.read().strip()\n      yield {\n          \"text\": text,\n          \"label\": res.groupdict()[\"label\"],\n      }", "output": "Generate IMDB examples.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def release(self) -> None:\n        \"\"\"\"\"\"\n        if self._at_eof:\n            return\n        while not self._at_eof:\n            await self.read_chunk(self.chunk_size)", "output": "Like read(), but reads all the data to the void.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_symlink_ownership(path, user, group, win_owner):\n    '''\n    \n    '''\n    if salt.utils.platform.is_windows():\n        try:\n            salt.utils.win_dacl.set_owner(path, win_owner)\n        except CommandExecutionError:\n            pass\n    else:\n        try:\n            __salt__['file.lchown'](path, user, group)\n        except OSError:\n            pass\n    return _check_symlink_ownership(path, user, group, win_owner)", "output": "Set the ownership of a symlink and return a boolean indicating\n    success/failure", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_feature(feature,\n                package=None,\n                source=None,\n                limit_access=False,\n                enable_parent=False,\n                image=None,\n                restart=False):\n    '''\n    \n    '''\n    cmd = ['DISM',\n           '/Quiet',\n           '/Image:{0}'.format(image) if image else '/Online',\n           '/Enable-Feature',\n           '/FeatureName:{0}'.format(feature)]\n    if package:\n        cmd.append('/PackageName:{0}'.format(package))\n    if source:\n        cmd.append('/Source:{0}'.format(source))\n    if limit_access:\n        cmd.append('/LimitAccess')\n    if enable_parent:\n        cmd.append('/All')\n    if not restart:\n        cmd.append('/NoRestart')\n\n    return __salt__['cmd.run_all'](cmd)", "output": "Install a feature using DISM\n\n    Args:\n        feature (str): The feature to install\n        package (Optional[str]): The parent package for the feature. You do not\n            have to specify the package if it is the Windows Foundation Package.\n            Otherwise, use package to specify the parent package of the feature\n        source (Optional[str]): The optional source of the capability. Default\n            is set by group policy and can be Windows Update\n        limit_access (Optional[bool]): Prevent DISM from contacting Windows\n            Update for the source package\n        enable_parent (Optional[bool]): True will enable all parent features of\n            the specified feature\n        image (Optional[str]): The path to the root directory of an offline\n            Windows image. If `None` is passed, the running operating system is\n            targeted. Default is None.\n        restart (Optional[bool]): Reboot the machine if required by the install\n\n    Returns:\n        dict: A dictionary containing the results of the command\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' dism.add_feature NetFx3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show(self, qname):\n        '''\n        \n        '''\n        try:\n            # First if not exists() -> exit\n            if not self.conn.queue_exists(qname):\n                return {}\n            # If exist, search the queue to return the Queue Object\n            for queue in self.conn.list():\n                if queue.name == qname:\n                    return queue\n        except pyrax.exceptions as err_msg:\n            log.error('RackSpace API got some problems during existing'\n                      ' queue check: %s', err_msg)\n        return {}", "output": "Show information about Queue", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absent(name,\n           keys=None,\n           user=None,\n           gnupghome=None,\n           **kwargs):\n    '''\n    \n\n    '''\n\n    ret = {'name': name,\n           'result': True,\n           'changes': {},\n           'comment': []}\n\n    _current_keys = __salt__['gpg.list_keys']()\n\n    current_keys = []\n    for key in _current_keys:\n        current_keys.append(key['keyid'])\n\n    if not keys:\n        keys = name\n\n    if isinstance(keys, six.string_types):\n        keys = [keys]\n\n    for key in keys:\n        if key in current_keys:\n            result = __salt__['gpg.delete_key'](key,\n                                                user,\n                                                gnupghome,\n                                                )\n            if 'result' in result and not result['result']:\n                ret['result'] = result['result']\n                ret['comment'].append(result['comment'])\n            else:\n                ret['comment'].append('Deleting {0} from GPG keychain'.format(name))\n        else:\n            ret['comment'].append('{0} not found in GPG keychain'.format(name))\n    ret['comment'] = '\\n'.join(ret['comment'])\n    return ret", "output": "Ensure GPG public key is absent in keychain\n\n    name\n        The unique name or keyid for the GPG public key.\n\n    keys\n        The keyId or keyIds to add to the GPG keychain.\n\n    user\n        Remove GPG keys from the specified user's keychain\n\n    gnupghome\n        Override GNUPG Home directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_mount_cache(real_name):\n    '''\n    \n    '''\n    cache = salt.utils.mount.read_cache(__opts__)\n\n    if cache:\n        if 'mounts' in cache:\n            if real_name in cache['mounts']:\n                del cache['mounts'][real_name]\n                cache_write = salt.utils.mount.write_cache(cache, __opts__)\n                if not cache_write:\n                    raise CommandExecutionError('Unable to write mount cache.')\n    return True", "output": ".. versionadded:: 2018.3.0\n\n    Provide information if the path is mounted\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mount.delete_mount_cache /mnt/share", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def permissions_for(self, user):\n        \"\"\"\n        \"\"\"\n\n        base = Permissions.text()\n        base.send_tts_messages = False\n        base.manage_messages = False\n        base.mention_everyone = True\n\n        if user.id == self.owner.id:\n            base.kick_members = True\n\n        return base", "output": "Handles permission resolution for a :class:`User`.\n\n        This function is there for compatibility with other channel types.\n\n        Actual direct messages do not really have the concept of permissions.\n\n        This returns all the Text related permissions set to true except:\n\n        - send_tts_messages: You cannot send TTS messages in a DM.\n        - manage_messages: You cannot delete others messages in a DM.\n\n        This also checks the kick_members permission if the user is the owner.\n\n        Parameters\n        -----------\n        user: :class:`User`\n            The user to check permissions for.\n\n        Returns\n        --------\n        :class:`Permissions`\n            The resolved permissions for the user.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def role_get(user):\n    '''\n    \n    '''\n    user_roles = []\n\n    ## read user_attr file (user:qualifier:res1:res2:attr)\n    with salt.utils.files.fopen('/etc/user_attr', 'r') as user_attr:\n        for role in user_attr:\n            role = salt.utils.stringutils.to_unicode(role)\n            role = role.strip().strip().split(':')\n\n            # skip comments and non complaint lines\n            if len(role) != 5:\n                continue\n\n            # skip other users\n            if role[0] != user:\n                continue\n\n            # parse attr\n            attrs = {}\n            for attr in role[4].strip().split(';'):\n                attr_key, attr_val = attr.strip().split('=')\n                if attr_key in ['auths', 'profiles', 'roles']:\n                    attrs[attr_key] = attr_val.strip().split(',')\n                else:\n                    attrs[attr_key] = attr_val\n            if 'roles' in attrs:\n                user_roles.extend(attrs['roles'])\n\n    return list(set(user_roles))", "output": "List roles for user\n\n    user : string\n        username\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rbac.role_get leo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __translate(self, parameter_values):\n        \"\"\"\n        \n        \"\"\"\n\n        template_copy = self.template\n\n        sam_parser = Parser()\n        sam_translator = Translator(managed_policy_map=self.__managed_policy_map(),\n                                    sam_parser=sam_parser,\n                                    # Default plugins are already initialized within the Translator\n                                    plugins=self.extra_plugins)\n\n        return sam_translator.translate(sam_template=template_copy,\n                                        parameter_values=parameter_values)", "output": "This method is unused and a Work In Progress", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def scrub(zpool, stop=False, pause=False):\n    '''\n    \n\n    '''\n    ## select correct action\n    if stop:\n        action = ['-s']\n    elif pause:\n        action = ['-p']\n    else:\n        action = None\n\n    ## Scrub storage pool\n    res = __salt__['cmd.run_all'](\n        __utils__['zfs.zpool_command'](\n            command='scrub',\n            flags=action,\n            target=zpool,\n        ),\n        python_shell=False,\n    )\n\n    if res['retcode'] != 0:\n        return __utils__['zfs.parse_command_result'](res, 'scrubbing')\n\n    ret = OrderedDict()\n    if stop or pause:\n        ret['scrubbing'] = False\n    else:\n        ret['scrubbing'] = True\n    return ret", "output": "Scrub a storage pool\n\n    zpool : string\n        Name of storage pool\n\n    stop : boolean\n        If ``True``, cancel ongoing scrub\n\n    pause : boolean\n        If ``True``, pause ongoing scrub\n\n        .. versionadded:: 2018.3.0\n\n        .. note::\n\n            Pause is only available on recent versions of ZFS.\n\n            If both ``pause`` and ``stop`` are ``True``, then ``stop`` will\n            win.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zpool.scrub myzpool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def evaluate(data_file, pred_file):\n    '''\n    \n    '''\n    expected_version = '1.1'\n    with open(data_file) as dataset_file:\n        dataset_json = json.load(dataset_file)\n        if dataset_json['version'] != expected_version:\n            print('Evaluation expects v-' + expected_version +\n                  ', but got dataset with v-' + dataset_json['version'],\n                  file=sys.stderr)\n        dataset = dataset_json['data']\n    with open(pred_file) as prediction_file:\n        predictions = json.load(prediction_file)\n    # print(json.dumps(evaluate(dataset, predictions)))\n    result = _evaluate(dataset, predictions)\n    # print('em:', result['exact_match'], 'f1:', result['f1'])\n    return result['exact_match']", "output": "Evaluate.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send_ready(self):\n        \"\"\"\n        \n        \"\"\"\n        self.lock.acquire()\n        try:\n            if self.closed or self.eof_sent:\n                return True\n            return self.out_window_size > 0\n        finally:\n            self.lock.release()", "output": "Returns true if data can be written to this channel without blocking.\n        This means the channel is either closed (so any write attempt would\n        return immediately) or there is at least one byte of space in the\n        outbound buffer. If there is at least one byte of space in the\n        outbound buffer, a `send` call will succeed immediately and return\n        the number of bytes actually written.\n\n        :return:\n            ``True`` if a `send` call on this channel would immediately succeed\n            or fail", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def require_setting(self, name: str, feature: str = \"this feature\") -> None:\n        \"\"\"\"\"\"\n        if not self.application.settings.get(name):\n            raise Exception(\n                \"You must define the '%s' setting in your \"\n                \"application to use %s\" % (name, feature)\n            )", "output": "Raises an exception if the given app setting is not defined.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_columns(self, column_names, inplace=False):\n        \"\"\"\n        \n        \"\"\"\n        column_names = list(column_names)\n        existing_columns = dict((k, i) for i, k in enumerate(self.column_names()))\n\n        for name in column_names:\n            if name not in existing_columns:\n                raise KeyError('Cannot find column %s' % name)\n\n        # Delete it going backwards so we don't invalidate indices\n        deletion_indices = sorted(existing_columns[name] for name in column_names)\n\n        if inplace:\n            ret = self\n        else:\n            ret = self.copy()\n\n        for colid in reversed(deletion_indices):\n            with cython_context():\n                ret.__proxy__.remove_column(colid)\n\n        ret._cache = None\n        return ret", "output": "Returns an SFrame with one or more columns removed.\n\n        If inplace == False (default) this operation does not modify the\n        current SFrame, returning a new SFrame.\n\n        If inplace == True, this operation modifies the current\n        SFrame, returning self.\n\n        Parameters\n        ----------\n        column_names : list or iterable\n            A list or iterable of column names.\n\n        inplace : bool, optional. Defaults to False.\n            Whether the SFrame is modified in place.\n\n        Returns\n        -------\n        out : SFrame\n            The SFrame with given columns removed.\n\n        Examples\n        --------\n        >>> sf = turicreate.SFrame({'id': [1, 2, 3], 'val1': ['A', 'B', 'C'], 'val2' : [10, 11, 12]})\n        >>> res = sf.remove_columns(['val1', 'val2'])\n        >>> res\n        +----+\n        | id |\n        +----+\n        | 1  |\n        | 2  |\n        | 3  |\n        +----+\n        [3 rows x 1 columns]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathContextSetCache(self, active, value, options):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlXPathContextSetCache(self._o, active, value, options)\n        return ret", "output": "Creates/frees an object cache on the XPath context. If\n          activates XPath objects (xmlXPathObject) will be cached\n          internally to be reused. @options: 0: This will set the\n          XPath object caching: @value: This will set the maximum\n          number of XPath objects to be cached per slot There are 5\n          slots for: node-set, string, number, boolean, and misc\n          objects. Use <0 for the default number (100). Other values\n           for @options have currently no effect.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def declare_namespace(packageName):\n    \"\"\"\"\"\"\n\n    _imp.acquire_lock()\n    try:\n        if packageName in _namespace_packages:\n            return\n\n        path = sys.path\n        parent, _, _ = packageName.rpartition('.')\n\n        if parent:\n            declare_namespace(parent)\n            if parent not in _namespace_packages:\n                __import__(parent)\n            try:\n                path = sys.modules[parent].__path__\n            except AttributeError:\n                raise TypeError(\"Not a package:\", parent)\n\n        # Track what packages are namespaces, so when new path items are added,\n        # they can be updated\n        _namespace_packages.setdefault(parent or None, []).append(packageName)\n        _namespace_packages.setdefault(packageName, [])\n\n        for path_item in path:\n            # Ensure all the parent's path items are reflected in the child,\n            # if they apply\n            _handle_ns(packageName, path_item)\n\n    finally:\n        _imp.release_lock()", "output": "Declare that package 'packageName' is a namespace package", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cleaned(_dashboard):\n    ''''''\n    dashboard = copy.deepcopy(_dashboard)\n\n    for ignored_dashboard_field in _IGNORED_DASHBOARD_FIELDS:\n        dashboard.pop(ignored_dashboard_field, None)\n    for row in dashboard.get('rows', []):\n        for ignored_row_field in _IGNORED_ROW_FIELDS:\n            row.pop(ignored_row_field, None)\n        for i, panel in enumerate(row.get('panels', [])):\n            for ignored_panel_field in _IGNORED_PANEL_FIELDS:\n                panel.pop(ignored_panel_field, None)\n            for target in panel.get('targets', []):\n                for ignored_target_field in _IGNORED_TARGET_FIELDS:\n                    target.pop(ignored_target_field, None)\n            row['panels'][i] = _stripped(panel)\n\n    return dashboard", "output": "Return a copy without fields that can differ.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_current_venv():\n        \"\"\"\n        \n        \"\"\"\n        if 'VIRTUAL_ENV' in os.environ:\n            venv = os.environ['VIRTUAL_ENV']\n        elif os.path.exists('.python-version'):  # pragma: no cover\n            try:\n                subprocess.check_output(['pyenv', 'help'], stderr=subprocess.STDOUT)\n            except OSError:\n                print(\"This directory seems to have pyenv's local venv, \"\n                      \"but pyenv executable was not found.\")\n            with open('.python-version', 'r') as f:\n                # minor fix in how .python-version is read\n                # Related: https://github.com/Miserlou/Zappa/issues/921\n                env_name = f.readline().strip()\n            bin_path = subprocess.check_output(['pyenv', 'which', 'python']).decode('utf-8')\n            venv = bin_path[:bin_path.rfind(env_name)] + env_name\n        else:  # pragma: no cover\n            return None\n        return venv", "output": "Returns the path to the current virtualenv", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _round_hex(q, r):\n    ''' \n\n    '''\n    x = q\n    z = r\n    y = -x-z\n\n    rx = np.round(x)\n    ry = np.round(y)\n    rz = np.round(z)\n\n    dx = np.abs(rx - x)\n    dy = np.abs(ry - y)\n    dz = np.abs(rz - z)\n\n    cond = (dx > dy) & (dx > dz)\n    q = np.where(cond              , -(ry + rz), rx)\n    r = np.where(~cond & ~(dy > dz), -(rx + ry), rz)\n\n    return q.astype(int), r.astype(int)", "output": "Round floating point axial hex coordinates to integer *(q,r)*\n    coordinates.\n\n    This code was adapted from:\n\n        https://www.redblobgames.com/grids/hexagons/#rounding\n\n    Args:\n        q (array[float]) :\n            NumPy array of Floating point axial *q* coordinates to round\n\n        r (array[float]) :\n            NumPy array of Floating point axial *q* coordinates to round\n\n    Returns:\n        (array[int], array[int])", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next(self):\n        \"\"\"\"\"\"\n        batch = mx.nd.zeros((self.batch_size, self.size[1], self.size[0], 3))\n        i = self.cur\n        for i in range(self.cur, min(len(self.list), self.cur+self.batch_size)):\n            str_img = open(self.root+self.list[i]+'.jpg').read()\n            img = imdecode(str_img, 1)\n            img, _ = random_crop(img, self.size)\n            batch[i - self.cur] = img\n        batch = mx.nd.transpose(batch, axes=(0, 3, 1, 2))\n        ret = mx.io.DataBatch(data=[batch],\n                              label=[],\n                              pad=self.batch_size-(i-self.cur),\n                              index=None)\n        self.cur = i\n        return ret", "output": "Move iterator position forward", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_pr_entry(self, step, wall_time, data_array, thresholds):\n    \"\"\"\n    \"\"\"\n    # Trim entries for which TP + FP = 0 (precision is undefined) at the tail of\n    # the data.\n    true_positives = [int(v) for v in data_array[metadata.TRUE_POSITIVES_INDEX]]\n    false_positives = [\n        int(v) for v in data_array[metadata.FALSE_POSITIVES_INDEX]]\n    tp_index = metadata.TRUE_POSITIVES_INDEX\n    fp_index = metadata.FALSE_POSITIVES_INDEX\n    positives = data_array[[tp_index, fp_index], :].astype(int).sum(axis=0)\n    end_index_inclusive = len(positives) - 1\n    while end_index_inclusive > 0 and positives[end_index_inclusive] == 0:\n      end_index_inclusive -= 1\n    end_index = end_index_inclusive + 1\n\n    return {\n        'wall_time': wall_time,\n        'step': step,\n        'precision': data_array[metadata.PRECISION_INDEX, :end_index].tolist(),\n        'recall': data_array[metadata.RECALL_INDEX, :end_index].tolist(),\n        'true_positives': true_positives[:end_index],\n        'false_positives': false_positives[:end_index],\n        'true_negatives':\n            [int(v) for v in\n             data_array[metadata.TRUE_NEGATIVES_INDEX][:end_index]],\n        'false_negatives':\n            [int(v) for v in\n             data_array[metadata.FALSE_NEGATIVES_INDEX][:end_index]],\n        'thresholds': thresholds[:end_index],\n    }", "output": "Creates an entry for PR curve data. Each entry corresponds to 1 step.\n\n    Args:\n      step: The step.\n      wall_time: The wall time.\n      data_array: A numpy array of PR curve data stored in the summary format.\n      thresholds: An array of floating point thresholds.\n\n    Returns:\n      A PR curve entry.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_children(self):\n        '''\n        \n        '''\n        if self._restart_processes is True:\n            for pid, mapping in six.iteritems(self._process_map):\n                if not mapping['Process'].is_alive():\n                    log.trace('Process restart of %s', pid)\n                    self.restart_process(pid)", "output": "Check the children once", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_to_experiment_list(experiments):\n    \"\"\"\n    \"\"\"\n    exp_list = experiments\n\n    # Transform list if necessary\n    if experiments is None:\n        exp_list = []\n    elif isinstance(experiments, Experiment):\n        exp_list = [experiments]\n    elif type(experiments) is dict:\n        exp_list = [\n            Experiment.from_json(name, spec)\n            for name, spec in experiments.items()\n        ]\n\n    # Validate exp_list\n    if (type(exp_list) is list\n            and all(isinstance(exp, Experiment) for exp in exp_list)):\n        if len(exp_list) > 1:\n            logger.warning(\"All experiments will be \"\n                           \"using the same SearchAlgorithm.\")\n    else:\n        raise TuneError(\"Invalid argument: {}\".format(experiments))\n\n    return exp_list", "output": "Produces a list of Experiment objects.\n\n    Converts input from dict, single experiment, or list of\n    experiments to list of experiments. If input is None,\n    will return an empty list.\n\n    Arguments:\n        experiments (Experiment | list | dict): Experiments to run.\n\n    Returns:\n        List of experiments.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _fill_from_default(self, default_job_config):\n        \"\"\"\n        \"\"\"\n        if self._job_type != default_job_config._job_type:\n            raise TypeError(\n                \"attempted to merge two incompatible job types: \"\n                + repr(self._job_type)\n                + \", \"\n                + repr(default_job_config._job_type)\n            )\n\n        new_job_config = self.__class__()\n\n        default_job_properties = copy.deepcopy(default_job_config._properties)\n        for key in self._properties:\n            if key != self._job_type:\n                default_job_properties[key] = self._properties[key]\n\n        default_job_properties[self._job_type].update(self._properties[self._job_type])\n        new_job_config._properties = default_job_properties\n\n        return new_job_config", "output": "Merge this job config with a default job config.\n\n        The keys in this object take precedence over the keys in the default\n        config. The merge is done at the top-level as well as for keys one\n        level below the job type.\n\n        Arguments:\n            default_job_config (google.cloud.bigquery.job._JobConfig):\n                The default job config that will be used to fill in self.\n\n        Returns:\n            google.cloud.bigquery.job._JobConfig A new (merged) job config.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_numeric_methods_add_sub_disabled(cls):\n        \"\"\"\n        \n        \"\"\"\n        cls.__add__ = make_invalid_op('__add__')\n        cls.__radd__ = make_invalid_op('__radd__')\n        cls.__iadd__ = make_invalid_op('__iadd__')\n        cls.__sub__ = make_invalid_op('__sub__')\n        cls.__rsub__ = make_invalid_op('__rsub__')\n        cls.__isub__ = make_invalid_op('__isub__')", "output": "Add in the numeric add/sub methods to disable.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_top(**kwargs):\n    '''\n    \n    '''\n    __opts__['grains'] = __grains__\n    opts = salt.utils.state.get_sls_opts(__opts__, **kwargs)\n    st_ = salt.client.ssh.state.SSHHighState(\n            opts,\n            __pillar__,\n            __salt__,\n            __context__['fileclient'])\n    top_data = st_.get_top()\n    errors = []\n    errors += st_.verify_tops(top_data)\n    if errors:\n        return errors\n    matches = st_.top_matches(top_data)\n    return matches", "output": "Return the top data that the minion will use for a highstate\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.show_top", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(name, stop=False):\n    '''\n    \n    '''\n    if not stop and state(name) != 'stopped':\n        raise CommandExecutionError(\n            'Container \\'{0}\\' is not stopped'.format(name)\n        )\n\n    def _failed_remove(name, exc):\n        raise CommandExecutionError(\n            'Unable to remove container \\'{0}\\': {1}'.format(name, exc)\n        )\n\n    if _sd_version() >= 219:\n        ret = _machinectl('remove {0}'.format(name))\n        if ret['retcode'] != 0:\n            __context__['retcode'] = salt.defaults.exitcodes.EX_UNAVAILABLE\n            _failed_remove(name, ret['stderr'])\n    else:\n        try:\n            shutil.rmtree(os.path.join(_root(), name))\n        except OSError as exc:\n            _failed_remove(name, exc)\n    return True", "output": "Remove the named container\n\n    .. warning::\n\n        This function will remove all data associated with the container. It\n        will not, however, remove the btrfs subvolumes created by pulling\n        container images (:mod:`nspawn.pull_raw\n        <salt.modules.nspawn.pull_raw>`, :mod:`nspawn.pull_tar\n        <salt.modules.nspawn.pull_tar>`, :mod:`nspawn.pull_dkr\n        <salt.modules.nspawn.pull_dkr>`).\n\n    stop : False\n        If ``True``, the container will be destroyed even if it is\n        running/frozen.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' nspawn.remove foo\n        salt '*' nspawn.remove foo stop=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def memory_usage(self, index=True, deep=False):\n        \"\"\"\n        \n        \"\"\"\n        v = super().memory_usage(deep=deep)\n        if index:\n            v += self.index.memory_usage(deep=deep)\n        return v", "output": "Return the memory usage of the Series.\n\n        The memory usage can optionally include the contribution of\n        the index and of elements of `object` dtype.\n\n        Parameters\n        ----------\n        index : bool, default True\n            Specifies whether to include the memory usage of the Series index.\n        deep : bool, default False\n            If True, introspect the data deeply by interrogating\n            `object` dtypes for system-level memory consumption, and include\n            it in the returned value.\n\n        Returns\n        -------\n        int\n            Bytes of memory consumed.\n\n        See Also\n        --------\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of the\n            array.\n        DataFrame.memory_usage : Bytes consumed by a DataFrame.\n\n        Examples\n        --------\n        >>> s = pd.Series(range(3))\n        >>> s.memory_usage()\n        104\n\n        Not including the index gives the size of the rest of the data, which\n        is necessarily smaller:\n\n        >>> s.memory_usage(index=False)\n        24\n\n        The memory footprint of `object` values is ignored by default:\n\n        >>> s = pd.Series([\"a\", \"b\"])\n        >>> s.values\n        array(['a', 'b'], dtype=object)\n        >>> s.memory_usage()\n        96\n        >>> s.memory_usage(deep=True)\n        212", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_attr(self, **kwargs):\n        \"\"\"\n        \"\"\"\n        for key, value in kwargs.items():\n            if value is not None:\n                if not isinstance(value, STRING_TYPES):\n                    raise ValueError(\"Set Attr only accepts string values\")\n                value = c_str(str(value))\n            _check_call(_LIB.XGBoosterSetAttr(\n                self.handle, c_str(key), value))", "output": "Set the attribute of the Booster.\n\n        Parameters\n        ----------\n        **kwargs\n            The attributes to set. Setting a value to None deletes an attribute.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __generate_really (self, prop_set):\n        \"\"\" \n        \"\"\"\n        assert isinstance(prop_set, property_set.PropertySet)\n        best_alternative = self.__select_alternatives (prop_set, debug=0)\n        self.best_alternative = best_alternative\n\n        if not best_alternative:\n            # FIXME: revive.\n            # self.__select_alternatives(prop_set, debug=1)\n            self.manager_.errors()(\n                \"No best alternative for '%s'.\\n\"\n                  % (self.full_name(),))\n\n        result = best_alternative.generate (prop_set)\n\n        # Now return virtual targets for the only alternative\n        return result", "output": "Generates the main target with the given property set\n            and returns a list which first element is property_set object\n            containing usage_requirements of generated target and with\n            generated virtual target in other elements. It's possible\n            that no targets are generated.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_timing_signal_1d(x,\n                         min_timescale=1.0,\n                         max_timescale=1.0e4,\n                         start_index=0):\n  \"\"\"\n  \"\"\"\n  length = common_layers.shape_list(x)[1]\n  channels = common_layers.shape_list(x)[2]\n  signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale,\n                                start_index)\n  return x + common_layers.cast_like(signal, x)", "output": "Adds a bunch of sinusoids of different frequencies to a Tensor.\n\n  Each channel of the input Tensor is incremented by a sinusoid of a different\n  frequency and phase.\n\n  This allows attention to learn to use absolute and relative positions.\n  Timing signals should be added to some precursors of both the query and the\n  memory inputs to attention.\n\n  The use of relative position is possible because sin(x+y) and cos(x+y) can be\n  expressed in terms of y, sin(x) and cos(x).\n\n  In particular, we use a geometric sequence of timescales starting with\n  min_timescale and ending with max_timescale.  The number of different\n  timescales is equal to channels / 2. For each timescale, we\n  generate the two sinusoidal signals sin(timestep/timescale) and\n  cos(timestep/timescale).  All of these sinusoids are concatenated in\n  the channels dimension.\n\n  Args:\n    x: a Tensor with shape [batch, length, channels]\n    min_timescale: a float\n    max_timescale: a float\n    start_index: index of first position\n\n  Returns:\n    a Tensor the same shape as x.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reboot(name, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudException(\n            'The show_instance action must be called with -a or --action.'\n        )\n\n    node_id = get_linode_id_from_name(name)\n    response = _query('linode', 'reboot', args={'LinodeID': node_id})\n    data = _clean_data(response)\n    reboot_jid = data['JobID']\n\n    if not _wait_for_job(node_id, reboot_jid):\n        log.error('Reboot failed for %s.', name)\n        return False\n\n    return data", "output": "Reboot a linode.\n\n    .. versionadded:: 2015.8.0\n\n    name\n        The name of the VM to reboot.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a reboot vm_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zone_compare(timezone):\n    '''\n    \n    '''\n    # if it's one of the key's just use it\n    if timezone.lower() in mapper.win_to_unix:\n        check_zone = timezone\n\n    elif timezone.lower() in mapper.unix_to_win:\n        # if it's one of the values, use the key\n        check_zone = mapper.get_win(timezone)\n\n    else:\n        # Raise error because it's neither key nor value\n        raise CommandExecutionError('Invalid timezone passed: {0}'\n                                    ''.format(timezone))\n\n    return get_zone() == mapper.get_unix(check_zone, 'Unknown')", "output": "Compares the given timezone with the machine timezone. Mostly useful for\n    running state checks.\n\n    Args:\n        timezone (str):\n            The timezone to compare. This can be in Windows or Unix format. Can\n            be any of the values returned by the ``timezone.list`` function\n\n    Returns:\n        bool: ``True`` if they match, otherwise ``False``\n\n    Example:\n\n    .. code-block:: bash\n\n        salt '*' timezone.zone_compare 'America/Denver'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_module_or_package(path):\r\n    \"\"\"\"\"\"\r\n    is_module = osp.isfile(path) and osp.splitext(path)[1] in ('.py', '.pyw')\r\n    is_package = osp.isdir(path) and osp.isfile(osp.join(path, '__init__.py'))\r\n    return is_module or is_package", "output": "Return True if path is a Python module/package", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_value(self, label, value, takeable=False):\n        \"\"\"\n        \n        \"\"\"\n        warnings.warn(\"set_value is deprecated and will be removed \"\n                      \"in a future release. Please use \"\n                      \".at[] or .iat[] accessors instead\", FutureWarning,\n                      stacklevel=2)\n        return self._set_value(label, value, takeable=takeable)", "output": "Quickly set single value at passed label. If label is not contained, a\n        new object is created with the label placed at the end of the result\n        index\n\n        .. deprecated:: 0.21.0\n\n        Please use .at[] or .iat[] accessors.\n\n        Parameters\n        ----------\n        label : object\n            Partial indexing with MultiIndex not allowed\n        value : object\n            Scalar value\n        takeable : interpret the index as indexers, default False\n\n        Notes\n        -----\n        This method *always* returns a new object. It is not particularly\n        efficient but is provided for API compatibility with Series\n\n        Returns\n        -------\n        series : SparseSeries", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _finish_deferred_init(self):\n        \"\"\"\"\"\"\n        if not self._deferred_init:\n            return\n        init, ctx, default_init, data = self._deferred_init\n        self._deferred_init = ()\n        assert self.shape is not None and np.prod(self.shape) > 0, \\\n            \"Cannot initialize Parameter '%s' because it has \" \\\n            \"invalid shape: %s. Please specify in_units, \" \\\n            \"in_channels, etc for `Block`s.\"%(\n                self.name, str(self.shape))\n\n        with autograd.pause():\n            if data is None:\n                data = ndarray.zeros(shape=self.shape, dtype=self.dtype,\n                                     ctx=context.cpu(), stype=self._stype)\n                initializer.create(default_init)(\n                    initializer.InitDesc(self.name, {'__init__': init}), data)\n\n            self._init_impl(data, ctx)", "output": "Finishes deferred initialization.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sources (self):\n        \"\"\" \n        \"\"\"\n        if self.source_targets_ == None:\n            self.source_targets_ = []\n            for s in self.sources_:\n                self.source_targets_.append(resolve_reference(s, self.project_)[0])\n\n        return self.source_targets_", "output": "Returns the list of AbstractTargets which are used as sources.\n            The extra properties specified for sources are not represented.\n            The only used of this rule at the moment is the '--dump-tests'\n            feature of the test system.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def collab_learner(data, n_factors:int=None, use_nn:bool=False, emb_szs:Dict[str,int]=None, layers:Collection[int]=None, \n                   ps:Collection[float]=None, emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, \n                   bn_final:bool=False, **learn_kwargs)->Learner:\n    \"\"\n    emb_szs = data.get_emb_szs(ifnone(emb_szs, {}))\n    u,m = data.train_ds.x.classes.values()\n    if use_nn: model = EmbeddingNN(emb_szs=emb_szs, layers=layers, ps=ps, emb_drop=emb_drop, y_range=y_range, \n                                   use_bn=use_bn, bn_final=bn_final, **learn_kwargs)\n    else:      model = EmbeddingDotBias(n_factors, len(u), len(m), y_range=y_range)\n    return CollabLearner(data, model, **learn_kwargs)", "output": "Create a Learner for collaborative filtering on `data`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def profile(self, new_profile):\n        \"\"\"\n\n        \"\"\"\n        if not isinstance(new_profile, FirefoxProfile):\n            new_profile = FirefoxProfile(new_profile)\n        self._profile = new_profile", "output": "Sets location of the browser profile to use, either by string\n        or ``FirefoxProfile``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def buy_close_order_quantity(self):\n        \"\"\"\n        \n        \"\"\"\n        return sum(order.unfilled_quantity for order in self.open_orders if order.side == SIDE.BUY and\n                   order.position_effect in [POSITION_EFFECT.CLOSE, POSITION_EFFECT.CLOSE_TODAY])", "output": "[int] \u4e70\u65b9\u5411\u6302\u5355\u91cf", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_keypress(self):\r\n        \"\"\"\"\"\"\r\n        if self.numpress == 2:\r\n            self.sig_double_tab_pressed.emit(True)\r\n        self.numpress = 0", "output": "When hitting tab, it handles if single or double tab", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _prepare_threads(self):\n        \"\"\"\n        \n        \"\"\"\n        self._threads = []\n        for i in range(self.num_workers):\n            t = threading.Thread(target=self._fetch)\n            t.setDaemon(True)\n            t.start()\n            self._threads.append(t)", "output": "Threads are created only when get_project is called, and terminate\n        before it returns. They are there primarily to parallelise I/O (i.e.\n        fetching web pages).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear(self):\n        '''\n        \n        '''\n        with self._lock:\n            super(LazyLoader, self).clear()  # clear the lazy loader\n            self.loaded_files = set()\n            self.missing_modules = {}\n            self.loaded_modules = {}\n            # if we have been loaded before, lets clear the file mapping since\n            # we obviously want a re-do\n            if hasattr(self, 'opts'):\n                self._refresh_file_mapping()\n            self.initial_load = False", "output": "Clear the dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_padding(builder, layer, input_names, output_names, keras_layer):\n    \"\"\"\n    \"\"\"\n    # Get input and output names\n    input_name, output_name = (input_names[0], output_names[0])\n\n    if isinstance(keras_layer, keras.layers.convolutional.ZeroPadding1D):\n        left, right = keras_layer.padding\n        top, bottom = (0, 0)\n    else: # 2D\n        top, left = keras_layer.padding\n        bottom, right = keras_layer.padding\n\n    # Now add the layer\n    builder.add_padding(name = layer,\n        left = left, right=right, top=top, bottom=bottom, value = 0,\n        input_name = input_name, output_name=output_name\n        )", "output": "Convert padding layer from keras to coreml.\n    Keras only supports zero padding at this time.\n    Parameters\n    ----------\n    keras_layer: layer\n        A keras layer object.\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_as_string(self, s3_path, encoding='utf-8'):\n        \"\"\"\n        \n        \"\"\"\n        content = self.get_as_bytes(s3_path)\n        return content.decode(encoding)", "output": "Get the contents of an object stored in S3 as string.\n\n        :param s3_path: URL for target S3 location\n        :param encoding: Encoding to decode bytes to string\n        :return: File contents as a string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_dt64_dtype(dtype):\n    \"\"\"\n    \n    \"\"\"\n    if dtype is not None:\n        dtype = pandas_dtype(dtype)\n        if is_dtype_equal(dtype, np.dtype(\"M8\")):\n            # no precision, warn\n            dtype = _NS_DTYPE\n            msg = textwrap.dedent(\"\"\"\\\n                Passing in 'datetime64' dtype with no precision is deprecated\n                and will raise in a future version. Please pass in\n                'datetime64[ns]' instead.\"\"\")\n            warnings.warn(msg, FutureWarning, stacklevel=5)\n\n        if ((isinstance(dtype, np.dtype) and dtype != _NS_DTYPE)\n                or not isinstance(dtype, (np.dtype, DatetimeTZDtype))):\n            raise ValueError(\"Unexpected value for 'dtype': '{dtype}'. \"\n                             \"Must be 'datetime64[ns]' or DatetimeTZDtype'.\"\n                             .format(dtype=dtype))\n    return dtype", "output": "Check that a dtype, if passed, represents either a numpy datetime64[ns]\n    dtype or a pandas DatetimeTZDtype.\n\n    Parameters\n    ----------\n    dtype : object\n\n    Returns\n    -------\n    dtype : None, numpy.dtype, or DatetimeTZDtype\n\n    Raises\n    ------\n    ValueError : invalid dtype\n\n    Notes\n    -----\n    Unlike validate_tz_from_dtype, this does _not_ allow non-existent\n    tz errors to go through", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate(self):\n        \"\"\"\n        \n        \"\"\"\n        variable_names, _unused = getExprNames(self._expr, {})\n        expr_indices = []\n        for name in variable_names:\n            if name == 'inf':\n                continue\n            match = _VARIABLE_NAME_RE.match(name)\n            if not match:\n                raise ValueError(\"%r is not a valid variable name\" % name)\n            expr_indices.append(int(match.group(2)))\n\n        expr_indices.sort()\n        expected_indices = list(range(len(self.inputs)))\n        if expr_indices != expected_indices:\n            raise ValueError(\n                \"Expected %s for variable indices, but got %s\" % (\n                    expected_indices, expr_indices,\n                )\n            )\n        super(NumericalExpression, self)._validate()", "output": "Ensure that our expression string has variables of the form x_0, x_1,\n        ... x_(N - 1), where N is the length of our inputs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_suspended(order_book_id, count=1):\n    \"\"\"\n    \n    \"\"\"\n    dt = Environment.get_instance().calendar_dt.date()\n    order_book_id = assure_stock_order_book_id(order_book_id)\n    return Environment.get_instance().data_proxy.is_suspended(order_book_id, dt, count)", "output": "\u5224\u65ad\u67d0\u53ea\u80a1\u7968\u662f\u5426\u5168\u5929\u505c\u724c\u3002\n\n    :param str order_book_id: \u67d0\u53ea\u80a1\u7968\u7684\u4ee3\u7801\u6216\u80a1\u7968\u4ee3\u7801\uff0c\u53ef\u4f20\u5165\u5355\u53ea\u80a1\u7968\u7684order_book_id, symbol\n\n    :param int count: \u56de\u6eaf\u83b7\u53d6\u7684\u6570\u636e\u4e2a\u6570\u3002\u9ed8\u8ba4\u4e3a\u5f53\u524d\u80fd\u591f\u83b7\u53d6\u5230\u7684\u6700\u8fd1\u7684\u6570\u636e\n\n    :return: count\u4e3a1\u65f6 `bool`; count>1\u65f6 `pandas.DataFrame`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nanany(values, axis=None, skipna=True, mask=None):\n    \"\"\"\n    \n    \"\"\"\n    values, mask, dtype, _, _ = _get_values(values, skipna, False, copy=skipna,\n                                            mask=mask)\n    return values.any(axis)", "output": "Check if any elements along an axis evaluate to True.\n\n    Parameters\n    ----------\n    values : ndarray\n    axis : int, optional\n    skipna : bool, default True\n    mask : ndarray[bool], optional\n        nan-mask if known\n\n    Returns\n    -------\n    result : bool\n\n    Examples\n    --------\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1, 2])\n    >>> nanops.nanany(s)\n    True\n\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([np.nan])\n    >>> nanops.nanany(s)\n    False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_metrics(self, *args, **kwargs):\n        \"\"\"\n        \"\"\"\n        new_query = copy.deepcopy(self)\n        new_query._filter.select_metrics(*args, **kwargs)\n        return new_query", "output": "Copy the query and add filtering by metric labels.\n\n        Examples::\n\n            query = query.select_metrics(instance_name='myinstance')\n            query = query.select_metrics(instance_name_prefix='mycluster-')\n\n        A keyword argument ``<label>=<value>`` ordinarily generates a filter\n        expression of the form::\n\n            metric.label.<label> = \"<value>\"\n\n        However, by adding ``\"_prefix\"`` or ``\"_suffix\"`` to the keyword,\n        you can specify a partial match.\n\n        ``<label>_prefix=<value>`` generates::\n\n            metric.label.<label> = starts_with(\"<value>\")\n\n        ``<label>_suffix=<value>`` generates::\n\n            metric.label.<label> = ends_with(\"<value>\")\n\n        If the label's value type is ``INT64``, a similar notation can be\n        used to express inequalities:\n\n        ``<label>_less=<value>`` generates::\n\n            metric.label.<label> < <value>\n\n        ``<label>_lessequal=<value>`` generates::\n\n            metric.label.<label> <= <value>\n\n        ``<label>_greater=<value>`` generates::\n\n            metric.label.<label> > <value>\n\n        ``<label>_greaterequal=<value>`` generates::\n\n            metric.label.<label> >= <value>\n\n        :type args: tuple\n        :param args: Raw filter expression strings to include in the\n            conjunction. If just one is provided and no keyword arguments\n            are provided, it can be a disjunction.\n\n        :type kwargs: dict\n        :param kwargs: Label filters to include in the conjunction as\n            described above.\n\n        :rtype: :class:`Query`\n        :returns: The new query object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def comment(path,\n            regex,\n            char='#',\n            backup='.bak'):\n    '''\n    \n    '''\n    return comment_line(path=path,\n                        regex=regex,\n                        char=char,\n                        cmnt=True,\n                        backup=backup)", "output": ".. deprecated:: 0.17.0\n       Use :py:func:`~salt.modules.file.replace` instead.\n\n    Comment out specified lines in a file\n\n    path\n        The full path to the file to be edited\n    regex\n        A regular expression used to find the lines that are to be commented;\n        this pattern will be wrapped in parenthesis and will move any\n        preceding/trailing ``^`` or ``$`` characters outside the parenthesis\n        (e.g., the pattern ``^foo$`` will be rewritten as ``^(foo)$``)\n    char : ``#``\n        The character to be inserted at the beginning of a line in order to\n        comment it out\n    backup : ``.bak``\n        The file will be backed up before edit with this file extension\n\n        .. warning::\n\n            This backup will be overwritten each time ``sed`` / ``comment`` /\n            ``uncomment`` is called. Meaning the backup will only be useful\n            after the first invocation.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.comment /etc/modules pcspkr", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rmtree_errorhandler(func, path, exc_info):\n    \"\"\"\"\"\"\n    # if file type currently read only\n    if os.stat(path).st_mode & stat.S_IREAD:\n        # convert to read/write\n        os.chmod(path, stat.S_IWRITE)\n        # use the original function to repeat the operation\n        func(path)\n        return\n    else:\n        raise", "output": "On Windows, the files in .svn are read-only, so when rmtree() tries to\n    remove them, an exception is thrown.  We catch that here, remove the\n    read-only attribute, and hopefully continue without problems.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_json_string(self, include_defaults):\n        ''' \n\n        '''\n        json_like = self._to_json_like(include_defaults=include_defaults)\n        json_like['id'] = self.id\n        # serialize_json \"fixes\" the JSON from _to_json_like by converting\n        # all types into plain JSON types # (it converts Model into refs,\n        # for example).\n        return serialize_json(json_like)", "output": "Returns a JSON string encoding the attributes of this object.\n\n        References to other objects are serialized as references\n        (just the object ID and type info), so the deserializer\n        will need to separately have the full attributes of those\n        other objects.\n\n        There's no corresponding ``from_json_string()`` because to\n        deserialize an object is normally done in the context of a\n        Document (since the Document can resolve references).\n\n        For most purposes it's best to serialize and deserialize\n        entire documents.\n\n        Args:\n            include_defaults (bool) : whether to include attributes\n                that haven't been changed from the default", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_lookup(self, branched_action_space):\n        \"\"\"\n        \n        \"\"\"\n        possible_vals = [range(_num) for _num in branched_action_space]\n        all_actions = [list(_action) for _action in itertools.product(*possible_vals)]\n        # Dict should be faster than List for large action spaces\n        action_lookup = {_scalar: _action for (_scalar, _action) in enumerate(all_actions)}\n        return action_lookup", "output": "Creates a Dict that maps discrete actions (scalars) to branched actions (lists).\n        Each key in the Dict maps to one unique set of branched actions, and each value\n        contains the List of branched actions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_project_dir(self, directory):\r\n        \"\"\"\"\"\"\r\n        if directory is not None:\r\n            self.treewidget.set_root_path(osp.dirname(directory))\r\n            self.treewidget.set_folder_names([osp.basename(directory)])\r\n        self.treewidget.setup_project_view()\r\n        try:\r\n            self.treewidget.setExpanded(self.treewidget.get_index(directory),\r\n                                        True)\r\n        except TypeError:\r\n            pass", "output": "Set the project directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve_parameter_refs(self, input_dict, parameters):\n        \"\"\"\n        \n        \"\"\"\n        if not self.can_handle(input_dict):\n            return input_dict\n\n        value = input_dict[self.intrinsic_name]\n\n        # FindInMap expects an array with 3 values\n        if not isinstance(value, list) or len(value) != 3:\n            raise InvalidDocumentException(\n                [InvalidTemplateException('Invalid FindInMap value {}. FindInMap expects an array with 3 values.'\n                                          .format(value))])\n\n        map_name = self.resolve_parameter_refs(value[0], parameters)\n        top_level_key = self.resolve_parameter_refs(value[1], parameters)\n        second_level_key = self.resolve_parameter_refs(value[2], parameters)\n\n        if not isinstance(map_name, string_types) or \\\n                not isinstance(top_level_key, string_types) or \\\n                not isinstance(second_level_key, string_types):\n            return input_dict\n\n        if map_name not in parameters or \\\n                top_level_key not in parameters[map_name] or \\\n                second_level_key not in parameters[map_name][top_level_key]:\n            return input_dict\n\n        return parameters[map_name][top_level_key][second_level_key]", "output": "Recursively resolves \"Fn::FindInMap\"references that are present in the mappings and returns the value.\n        If it is not in mappings, this method simply returns the input unchanged.\n\n        :param input_dict: Dictionary representing the FindInMap function. Must contain only one key and it\n                           should be \"Fn::FindInMap\".\n\n        :param parameters: Dictionary of mappings from the SAM template", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def split_kwargs_by_func(kwargs, func):\n    \"\"\n    args = func_args(func)\n    func_kwargs = {a:kwargs.pop(a) for a in args if a in kwargs}\n    return func_kwargs, kwargs", "output": "Split `kwargs` between those expected by `func` and the others.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_main_chain_layers(self):\n        \"\"\"\"\"\"\n        main_chain = self.get_main_chain()\n        ret = []\n        for u in main_chain:\n            for v, layer_id in self.adj_list[u]:\n                if v in main_chain and u in main_chain:\n                    ret.append(layer_id)\n        return ret", "output": "Return a list of layer IDs in the main chain.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lock(\n    ctx,\n    state,\n    **kwargs\n):\n    \"\"\"\"\"\"\n    from ..core import ensure_project, do_init, do_lock\n\n    # Ensure that virtualenv is available.\n    ensure_project(three=state.three, python=state.python, pypi_mirror=state.pypi_mirror)\n    if state.installstate.requirementstxt:\n        do_init(\n            dev=state.installstate.dev,\n            requirements=state.installstate.requirementstxt,\n            pypi_mirror=state.pypi_mirror,\n            pre=state.installstate.pre,\n        )\n    do_lock(\n        ctx=ctx,\n        clear=state.clear,\n        pre=state.installstate.pre,\n        keep_outdated=state.installstate.keep_outdated,\n        pypi_mirror=state.pypi_mirror,\n    )", "output": "Generates Pipfile.lock.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_binary(filename):\n    \"\"\"\n    \n    \"\"\"\n    logger.debug('is_binary: %(filename)r', locals())\n\n    # Check if the file extension is in a list of known binary types\n    binary_extensions = ['pyc', 'iso', 'zip', 'pdf']\n    for ext in binary_extensions:\n        if filename.endswith(ext):\n            return True\n\n    # Check if the starting chunk is a binary string\n    chunk = get_starting_chunk(filename)\n    return is_binary_string(chunk)", "output": ":param filename: File to check.\n    :returns: True if it's a binary file, otherwise False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def random_expr(depth, vlist, ops):\n  \"\"\"\n  \"\"\"\n  if not depth:\n    return str(vlist[random.randrange(len(vlist))])\n\n  max_depth_side = random.randrange(2)\n  other_side_depth = random.randrange(depth)\n\n  left = random_expr(depth - 1\n                     if max_depth_side else other_side_depth, vlist, ops)\n  right = random_expr(depth - 1\n                      if not max_depth_side else other_side_depth, vlist, ops)\n\n  op = ops[random.randrange(len(ops))]\n  return ExprNode(left, right, op)", "output": "Generate a random expression tree.\n\n  Args:\n    depth: At least one leaf will be this many levels down from the top.\n    vlist: A list of chars. These chars are randomly selected as leaf values.\n    ops: A list of ExprOp instances.\n\n  Returns:\n    An ExprNode instance which is the root of the generated expression tree.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pcre(tgt, minion_id=None):\n    '''\n    \n    '''\n    if minion_id is not None:\n        opts = copy.copy(__opts__)\n        if not isinstance(minion_id, six.string_types):\n            minion_id = six.text_type(minion_id)\n        opts['id'] = minion_id\n    else:\n        opts = __opts__\n    matchers = salt.loader.matchers(opts)\n    try:\n        return matchers['pcre_match.match'](tgt, opts=__opts__)\n    except Exception as exc:\n        log.exception(exc)\n        return False", "output": "Return True if the minion ID matches the given pcre target\n\n    minion_id\n        Specify the minion ID to match against the target expression\n\n        .. versionadded:: 2014.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' match.pcre '.*'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_extracted(self, file_path):\n        \"\"\"\n        \n        \"\"\"\n\n        if os.path.isdir(file_path):\n            self.chatbot.logger.info('File is already extracted')\n            return True\n        return False", "output": "Check if the data file is already extracted.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def combine_tensors(combination: str, tensors: List[torch.Tensor]) -> torch.Tensor:\n    \"\"\"\n    \n    \"\"\"\n    if len(tensors) > 9:\n        raise ConfigurationError(\"Double-digit tensor lists not currently supported\")\n    combination = combination.replace('x', '1').replace('y', '2')\n    to_concatenate = [_get_combination(piece, tensors) for piece in combination.split(',')]\n    return torch.cat(to_concatenate, dim=-1)", "output": "Combines a list of tensors using element-wise operations and concatenation, specified by a\n    ``combination`` string.  The string refers to (1-indexed) positions in the input tensor list,\n    and looks like ``\"1,2,1+2,3-1\"``.\n\n    We allow the following kinds of combinations: ``x``, ``x*y``, ``x+y``, ``x-y``, and ``x/y``,\n    where ``x`` and ``y`` are positive integers less than or equal to ``len(tensors)``.  Each of\n    the binary operations is performed elementwise.  You can give as many combinations as you want\n    in the ``combination`` string.  For example, for the input string ``\"1,2,1*2\"``, the result\n    would be ``[1;2;1*2]``, as you would expect, where ``[;]`` is concatenation along the last\n    dimension.\n\n    If you have a fixed, known way to combine tensors that you use in a model, you should probably\n    just use something like ``torch.cat([x_tensor, y_tensor, x_tensor * y_tensor])``.  This\n    function adds some complexity that is only necessary if you want the specific combination used\n    to be `configurable`.\n\n    If you want to do any element-wise operations, the tensors involved in each element-wise\n    operation must have the same shape.\n\n    This function also accepts ``x`` and ``y`` in place of ``1`` and ``2`` in the combination\n    string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _active_mounts(ret):\n    '''\n    \n    '''\n    _list = _list_mounts()\n    filename = '/proc/self/mounts'\n    if not os.access(filename, os.R_OK):\n        msg = 'File not readable {0}'\n        raise CommandExecutionError(msg.format(filename))\n\n    with salt.utils.files.fopen(filename) as ifile:\n        for line in ifile:\n            comps = salt.utils.stringutils.to_unicode(line).split()\n            ret[comps[1]] = {'device': comps[0],\n                             'alt_device': _list.get(comps[1], None),\n                             'fstype': comps[2],\n                             'opts': _resolve_user_group_names(comps[3].split(','))}\n    return ret", "output": "List active mounts on Linux systems", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_deletion_score(source_counts, prediction_counts, target_counts, beta=0):\n  \"\"\"\"\"\"\n  source_not_prediction_counts = source_counts - prediction_counts\n  source_not_target_counts = source_counts - target_counts\n  true_positives = sum((source_not_prediction_counts &\n                        source_not_target_counts).values())\n  selected = sum(source_not_prediction_counts.values())\n  relevant = sum(source_not_target_counts.values())\n  return _get_fbeta_score(true_positives, selected, relevant, beta=beta)", "output": "Compute the deletion score (Equation 6 in the paper).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_epoch_end(self, epoch: int, smooth_loss: Tensor, last_metrics: MetricsList, **kwargs: Any) -> bool:\n        \"\"\n        last_metrics = ifnone(last_metrics, [])\n        stats = [str(stat) if isinstance(stat, int) else '#na#' if stat is None else f'{stat:.6f}'\n                 for name, stat in zip(self.learn.recorder.names, [epoch, smooth_loss] + last_metrics)]\n        if self.add_time: stats.append(format_time(time() - self.start_epoch))\n        str_stats = ','.join(stats)\n        self.file.write(str_stats + '\\n')", "output": "Add a line with `epoch` number, `smooth_loss` and `last_metrics`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def layers_distance(list_a, list_b):\n    \"\"\"\"\"\"\n    len_a = len(list_a)\n    len_b = len(list_b)\n    f = np.zeros((len_a + 1, len_b + 1))\n    f[-1][-1] = 0\n    for i in range(-1, len_a):\n        f[i][-1] = i + 1\n    for j in range(-1, len_b):\n        f[-1][j] = j + 1\n    for i in range(len_a):\n        for j in range(len_b):\n            f[i][j] = min(\n                f[i][j - 1] + 1,\n                f[i - 1][j] + 1,\n                f[i - 1][j - 1] + layer_distance(list_a[i], list_b[j]),\n            )\n    return f[len_a - 1][len_b - 1]", "output": "The distance between the layers of two neural networks.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_stored_cert_serials(store):\n    '''\n    \n    '''\n    cmd = \"certutil.exe -store {0}\".format(store)\n    out = __salt__['cmd.run'](cmd)\n    # match serial numbers by header position to work with multiple languages\n    matches = re.findall(r\"={16}\\r\\n.*:\\s*(\\w*)\\r\\n\", out)\n    return matches", "output": "Get all of the certificate serials in the specified store\n\n    store\n        The store to get all the certificate serials from\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' certutil.get_stored_cert_serials <store>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def data_parallelism_from_flags(daisy_chain_variables=True, all_workers=False):\n  \"\"\"\n  \"\"\"\n  dp_arg_names = inspect.getargspec(data_parallelism).args\n\n  blacklist = [\"daisy_chain_variables\", \"all_workers\"]\n\n  kwargs = {}\n  for arg in dp_arg_names:\n    if arg in blacklist:\n      continue\n    kwargs[arg] = getattr(tf.flags.FLAGS, arg)\n\n  return data_parallelism(\n      daisy_chain_variables=daisy_chain_variables,\n      all_workers=all_workers,\n      **kwargs)", "output": "Over which devices do we split each training batch.\n\n  In old-fashioned async mode, we split the batch over all GPUs on the\n  current worker.\n\n  In sync mode, we split the batch over all the parameter server GPUs.\n\n  This function returns an expert_utils.Parallelism object, which can be used\n  to build the model.  It is configured in a way that any variables created\n  by `tf.get_variable` will be assigned to the parameter servers and shared\n  between datashards.\n\n  Args:\n    daisy_chain_variables: whether to copy variables in a daisy chain on GPUs.\n    all_workers: whether the devices are all async workers or just this one.\n\n  Returns:\n    a expert_utils.Parallelism.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def end_date(self):\n        \"\"\"\n        \"\"\"\n        if self.start_==None:\n            if len(self.time_index_max) > 0:\n                return str(max(self.time_index_max))[0:10]\n            else:\n                print(\n                    RuntimeWarning(\n                        'QAACCOUNT: THIS ACCOUNT DOESNOT HAVE ANY TRADE'\n                    )\n                )\n        else:\n            return self.end_", "output": "\u8d26\u6237\u7684\u4ea4\u6613\u7ed3\u675f\u65e5\u671f(\u53ea\u5728\u56de\u6d4b\u4e2d\u4f7f\u7528)\n\n        Raises:\n            RuntimeWarning -- [description]\n\n        Returns:\n            [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_images(profile, location_id=None, **libcloud_kwargs):\n    '''\n    \n    '''\n    conn = _get_driver(profile=profile)\n    libcloud_kwargs = salt.utils.args.clean_kwargs(**libcloud_kwargs)\n    if location_id is not None:\n        location = _get_by_id(conn.list_locations(), location_id)\n    else:\n        location = None\n    images = conn.list_images(location=location, **libcloud_kwargs)\n\n    ret = []\n    for image in images:\n        ret.append(_simple_image(image))\n    return ret", "output": "Return a list of images for this cloud\n\n    :param profile: The profile key\n    :type  profile: ``str``\n\n    :param location_id: The location key, from list_locations\n    :type  location_id: ``str``\n\n    :param libcloud_kwargs: Extra arguments for the driver's list_images method\n    :type  libcloud_kwargs: ``dict``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion libcloud_compute.list_images profile1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_agent_settings():\n    '''\n    \n    '''\n    ret = dict()\n    sorted_types = sorted(_SERVICE_TYPES.items(), key=lambda x: (-x[1], x[0]))\n\n    ret['services'] = list()\n    ret['contact'] = (__utils__['reg.read_value'](\n        _HKEY, _AGENT_KEY, 'sysContact'))['vdata']\n\n    ret['location'] = (__utils__['reg.read_value'](\n        _HKEY, _AGENT_KEY, 'sysLocation'))['vdata']\n\n    current_bitmask = (__utils__['reg.read_value'](\n        _HKEY, _AGENT_KEY, 'sysServices'))['vdata']\n\n    if current_bitmask == 0:\n        ret['services'].append(sorted_types[-1][0])\n    else:\n        # sorted_types is sorted from greatest to least bitmask.\n        for service, bitmask in sorted_types:\n            if current_bitmask is not None and current_bitmask > 0:\n                remaining_bitmask = current_bitmask - bitmask\n\n                if remaining_bitmask >= 0:\n                    current_bitmask = remaining_bitmask\n                    ret['services'].append(service)\n            else:\n                break\n\n    ret['services'] = sorted(ret['services'])\n    return ret", "output": "Determine the value of the SNMP sysContact, sysLocation, and sysServices\n    settings.\n\n    Returns:\n        dict: A dictionary of the agent settings.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' win_snmp.get_agent_settings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _shallow_copy(self, obj=None, obj_type=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        if obj is None:\n            obj = self._selected_obj.copy()\n        if obj_type is None:\n            obj_type = self._constructor\n        if isinstance(obj, obj_type):\n            obj = obj.obj\n        for attr in self._attributes:\n            if attr not in kwargs:\n                kwargs[attr] = getattr(self, attr)\n        return obj_type(obj, **kwargs)", "output": "return a new object with the replacement attributes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_random_response(input_statement, response_list, storage=None):\n    \"\"\"\n    \n    \"\"\"\n    from random import choice\n    logger = logging.getLogger(__name__)\n    logger.info('Selecting a response from list of {} options.'.format(\n        len(response_list)\n    ))\n    return choice(response_list)", "output": ":param input_statement: A statement, that closely matches an input to the chat bot.\n    :type input_statement: Statement\n\n    :param response_list: A list of statement options to choose a response from.\n    :type response_list: list\n\n    :param storage: An instance of a storage adapter to allow the response selection\n                    method to access other statements if needed.\n    :type storage: StorageAdapter\n\n    :return: Choose a random response from the selection.\n    :rtype: Statement", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_metadata_impl(self, run, tag):\n    \"\"\"\"\"\"\n    try:\n      run_metadata = self._multiplexer.RunMetadata(run, tag)\n    except ValueError:\n      # TODO(stephanwlee): Should include whether FE is fetching for v1 or v2 RunMetadata\n      # so we can remove this try/except.\n      tensor_events = self._multiplexer.Tensors(run, tag)\n      if tensor_events is None:\n        return None\n      # Take the first event if there are multiple events written from different\n      # steps.\n      run_metadata = config_pb2.RunMetadata.FromString(\n          tensor_events[0].tensor_proto.string_val[0])\n    if run_metadata is None:\n      return None\n    return (str(run_metadata), 'text/x-protobuf')", "output": "Result of the form `(body, mime_type)`, or `None` if no data exists.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def detectEncodingMeta(self):\n        \"\"\"\n        \"\"\"\n        buffer = self.rawStream.read(self.numBytesMeta)\n        assert isinstance(buffer, bytes)\n        parser = EncodingParser(buffer)\n        self.rawStream.seek(0)\n        encoding = parser.getEncoding()\n\n        if encoding is not None and encoding.name in (\"utf-16be\", \"utf-16le\"):\n            encoding = lookupEncoding(\"utf-8\")\n\n        return encoding", "output": "Report the encoding declared by the meta element", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def post(self, body=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"PUT\", \"/_license\", params=params, body=body\n        )", "output": "`<https://www.elastic.co/guide/en/x-pack/current/license-management.html>`_\n\n        :arg body: licenses to be installed\n        :arg acknowledge: whether the user has acknowledged acknowledge messages\n            (default: false)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _gen_fill_zeros(name):\n    \"\"\"\n    \n    \"\"\"\n    name = name.strip('__')\n    if 'div' in name:\n        # truediv, floordiv, div, and reversed variants\n        fill_value = np.inf\n    elif 'mod' in name:\n        # mod, rmod\n        fill_value = np.nan\n    else:\n        fill_value = None\n    return fill_value", "output": "Find the appropriate fill value to use when filling in undefined values\n    in the results of the given operation caused by operating on\n    (generally dividing by) zero.\n\n    Parameters\n    ----------\n    name : str\n\n    Returns\n    -------\n    fill_value : {None, np.nan, np.inf}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def queue_get_stoppable(self, q):\n        \"\"\" \"\"\"\n        while not self.stopped():\n            try:\n                return q.get(timeout=5)\n            except queue.Empty:\n                pass", "output": "Take obj from queue, but will give up when the thread is stopped", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_value(self, name):\r\n        \"\"\"\"\"\"\r\n        value = self.shellwidget.get_value(name)\r\n        # Reset temporal variable where value is saved to\r\n        # save memory\r\n        self.shellwidget._kernel_value = None\r\n        return value", "output": "Get the value of a variable", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def plot_pnlmoney(self):\n        \"\"\"\n        \n        \"\"\"\n        plt.scatter(x=self.pnl.sell_date.apply(str), y=self.pnl.pnl_money)\n        plt.gcf().autofmt_xdate()\n        return plt", "output": "\u753b\u51fapnl\u76c8\u4e8f\u989d\u6563\u70b9\u56fe", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def CRPS(label, pred):\n    \"\"\" \n    \"\"\"\n    for i in range(pred.shape[0]):\n        for j in range(pred.shape[1] - 1):\n            if pred[i, j] > pred[i, j + 1]:\n                pred[i, j + 1] = pred[i, j]\n    return np.sum(np.square(label - pred)) / label.size", "output": "Custom evaluation metric on CRPS.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rewind_body(body, body_pos):\n    \"\"\"\n    \n    \"\"\"\n    body_seek = getattr(body, 'seek', None)\n    if body_seek is not None and isinstance(body_pos, integer_types):\n        try:\n            body_seek(body_pos)\n        except (IOError, OSError):\n            raise UnrewindableBodyError(\"An error occurred when rewinding request \"\n                                        \"body for redirect/retry.\")\n    elif body_pos is _FAILEDTELL:\n        raise UnrewindableBodyError(\"Unable to record file position for rewinding \"\n                                    \"request body during a redirect/retry.\")\n    else:\n        raise ValueError(\"body_pos must be of type integer, \"\n                         \"instead it was %s.\" % type(body_pos))", "output": "Attempt to rewind body to a certain position.\n    Primarily used for request redirects and retries.\n\n    :param body:\n        File-like object that supports seek.\n\n    :param int pos:\n        Position to seek to in file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nsProp(self, name, nameSpace):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlGetNsProp(self._o, name, nameSpace)\n        return ret", "output": "Search and get the value of an attribute associated to a\n          node This attribute has to be anchored in the namespace\n          specified. This does the entity substitution. This function\n          looks in DTD attribute declaration for #FIXED or default\n           declaration values unless DTD use has been turned off.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_config_file(conf, atom):\n    '''\n    \n    '''\n    if '*' in atom:\n        parts = portage.dep.Atom(atom, allow_wildcard=True)\n        if not parts:\n            return\n        if parts.cp == '*/*':\n            # parts.repo will be empty if there is no repo part\n            relative_path = parts.repo or \"gentoo\"\n        elif six.text_type(parts.cp).endswith('/*'):\n            relative_path = six.text_type(parts.cp).split(\"/\")[0] + \"_\"\n        else:\n            relative_path = os.path.join(*[x for x in os.path.split(parts.cp) if x != '*'])\n    else:\n        relative_path = _p_to_cp(atom)\n        if not relative_path:\n            return\n\n    complete_file_path = BASE_PATH.format(conf) + '/' + relative_path\n\n    return complete_file_path", "output": "Parse the given atom, allowing access to its parts\n    Success does not mean that the atom exists, just that it\n    is in the correct format.\n    Returns none if the atom is invalid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def restart_apppool(name):\n    '''\n    \n    '''\n    ps_cmd = ['Restart-WebAppPool', r\"'{0}'\".format(name)]\n\n    cmd_ret = _srvmgr(ps_cmd)\n\n    return cmd_ret['retcode'] == 0", "output": "Restart an IIS application pool.\n\n    .. versionadded:: 2016.11.0\n\n    Args:\n        name (str): The name of the IIS application pool.\n\n    Returns:\n        bool: True if successful, otherwise False\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' win_iis.restart_apppool name='MyTestPool'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def schemaValidCtxtGetParserCtxt(self):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlSchemaValidCtxtGetParserCtxt(self._o)\n        if ret is None:raise parserError('xmlSchemaValidCtxtGetParserCtxt() failed')\n        __tmp = parserCtxt(_obj=ret)\n        return __tmp", "output": "allow access to the parser context of the schema validation\n           context", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_backend_type(self, location):\n        # type: (str) -> Optional[Type[VersionControl]]\n        \"\"\"\n        \n        \"\"\"\n        for vc_type in self._registry.values():\n            if vc_type.controls_location(location):\n                logger.debug('Determine that %s uses VCS: %s',\n                             location, vc_type.name)\n                return vc_type\n        return None", "output": "Return the type of the version control backend if found at given\n        location, e.g. vcs.get_backend_type('/path/to/vcs/checkout')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_with_json(self, json):\n        ''' \n\n        '''\n        replacement = self.from_json(json)\n        replacement._destructively_move(self)", "output": "Overwrite everything in this document with the JSON-encoded\n        document.\n\n        json (JSON-data) :\n            A JSON-encoded document to overwrite this one.\n\n        Returns:\n            None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def EncoderLayer(feature_depth,\n                 feedforward_depth,\n                 num_heads,\n                 dropout,\n                 mode):\n  \"\"\"\n  \"\"\"\n  # The encoder block expects (activation, mask) as input and returns\n  # the new activations only, we add the mask back to output next.\n  encoder_block = layers.Serial(\n      layers.Residual(  # Attention block here.\n          layers.Parallel(layers.LayerNorm(), layers.Identity()),\n          layers.MultiHeadedAttention(feature_depth, num_heads=num_heads,\n                                      dropout=dropout, mode=mode),\n          layers.Dropout(rate=dropout, mode=mode),\n          shortcut=layers.FirstBranch()\n      ),\n      ResidualFeedForward(feature_depth, feedforward_depth, dropout, mode=mode)\n  )\n  # Now we add the mask back.\n  return layers.Serial(\n      layers.Reorder(output=((0, 1), 1)),  # (x, mask) --> ((x, mask), mask)\n      layers.Parallel(encoder_block, layers.Identity())\n  )", "output": "Transformer encoder layer.\n\n  The input to the encoder is a pair (embedded source, mask) where\n  the mask is created from the original source to prevent attending\n  to the padding part of the input.\n\n  Args:\n    feature_depth: int:  depth of embedding\n    feedforward_depth: int: depth of feed-forward layer\n    num_heads: int: number of attention heads\n    dropout: float: dropout rate (how much to drop out)\n    mode: str: 'train' or 'eval'\n\n  Returns:\n    the layer, returning a pair (actiavtions, mask).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def step(self, fetch_stats=False):\n        \"\"\"\n        \"\"\"\n        if self.strategy == \"ps\":\n            return _distributed_sgd_step(\n                self.workers,\n                self.ps_list,\n                write_timeline=False,\n                fetch_stats=fetch_stats)\n        else:\n            return _simple_sgd_step(self.workers)", "output": "Run a single SGD step.\n\n        Arguments:\n            fetch_stats (bool): Whether to return stats from the step. This can\n                slow down the computation by acting as a global barrier.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bool(self):\n        \"\"\"\n        \n        \"\"\"\n        v = self.squeeze()\n        if isinstance(v, (bool, np.bool_)):\n            return bool(v)\n        elif is_scalar(v):\n            raise ValueError(\"bool cannot act on a non-boolean single element \"\n                             \"{0}\".format(self.__class__.__name__))\n\n        self.__nonzero__()", "output": "Return the bool of a single element PandasObject.\n\n        This must be a boolean scalar value, either True or False.  Raise a\n        ValueError if the PandasObject does not have exactly 1 element, or that\n        element is not boolean", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_unclaimed(work):\n  \"\"\"\"\"\"\n  if work['is_completed']:\n    return False\n  cutoff_time = time.time() - MAX_PROCESSING_TIME\n  if (work['claimed_worker_id'] and\n      work['claimed_worker_start_time'] is not None\n      and work['claimed_worker_start_time'] >= cutoff_time):\n    return False\n  return True", "output": "Returns True if work piece is unclaimed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def activations(self):\n    \"\"\"\"\"\"\n    if self._activations is None:\n      self._activations = _get_aligned_activations(self)\n    return self._activations", "output": "Loads sampled activations, which requires network access.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _external_pillar_data(self, pillar, val, key):\n        '''\n        \n        '''\n        ext = None\n        args = salt.utils.args.get_function_argspec(self.ext_pillars[key]).args\n\n        if isinstance(val, dict):\n            if ('extra_minion_data' in args) and self.extra_minion_data:\n                ext = self.ext_pillars[key](\n                    self.minion_id, pillar,\n                    extra_minion_data=self.extra_minion_data, **val)\n            else:\n                ext = self.ext_pillars[key](self.minion_id, pillar, **val)\n        elif isinstance(val, list):\n            if ('extra_minion_data' in args) and self.extra_minion_data:\n                ext = self.ext_pillars[key](\n                    self.minion_id, pillar, *val,\n                    extra_minion_data=self.extra_minion_data)\n            else:\n                ext = self.ext_pillars[key](self.minion_id,\n                                            pillar,\n                                            *val)\n        else:\n            if ('extra_minion_data' in args) and self.extra_minion_data:\n                ext = self.ext_pillars[key](\n                    self.minion_id,\n                    pillar,\n                    val,\n                    extra_minion_data=self.extra_minion_data)\n            else:\n                ext = self.ext_pillars[key](self.minion_id,\n                                            pillar,\n                                            val)\n        return ext", "output": "Builds actual pillar data structure and updates the ``pillar`` variable", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def augmentation_transform(self, data, label):  # pylint: disable=arguments-differ\n        \"\"\"\"\"\"\n        for aug in self.auglist:\n            data, label = aug(data, label)\n        return (data, label)", "output": "Override Transforms input data with specified augmentations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_service_account_json(cls, json_credentials_path, *args, **kwargs):\n        \"\"\"\n        \"\"\"\n        if \"credentials\" in kwargs:\n            raise TypeError(\"credentials must not be in keyword arguments\")\n        with io.open(json_credentials_path, \"r\", encoding=\"utf-8\") as json_fi:\n            credentials_info = json.load(json_fi)\n        credentials = service_account.Credentials.from_service_account_info(\n            credentials_info\n        )\n        if cls._SET_PROJECT:\n            if \"project\" not in kwargs:\n                kwargs[\"project\"] = credentials_info.get(\"project_id\")\n\n        kwargs[\"credentials\"] = credentials\n        return cls(*args, **kwargs)", "output": "Factory to retrieve JSON credentials while creating client.\n\n        :type json_credentials_path: str\n        :param json_credentials_path: The path to a private key file (this file\n                                      was given to you when you created the\n                                      service account). This file must contain\n                                      a JSON object with a private key and\n                                      other credentials information (downloaded\n                                      from the Google APIs console).\n\n        :type args: tuple\n        :param args: Remaining positional arguments to pass to constructor.\n\n        :type kwargs: dict\n        :param kwargs: Remaining keyword arguments to pass to constructor.\n\n        :rtype: :class:`_ClientFactoryMixin`\n        :returns: The client created with the retrieved JSON credentials.\n        :raises TypeError: if there is a conflict with the kwargs\n                 and the credentials created by the factory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_python(self, path):\n        \"\"\"\"\"\"\n        (pylint_stdout, pylint_stderr) = epylint.py_run(\n            ' '.join([str(path)] + self.pylint_opts), return_std=True)\n        emap = {}\n        print(pylint_stderr.read())\n        for line in pylint_stdout:\n            sys.stderr.write(line)\n            key = line.split(':')[-1].split('(')[0].strip()\n            if key not in self.pylint_cats:\n                continue\n            if key not in emap:\n                emap[key] = 1\n            else:\n                emap[key] += 1\n        sys.stderr.write('\\n')\n        self.python_map[str(path)] = emap", "output": "Process a python file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace(\n        self, new_value: Any, args: Sequence[Any], kwargs: Dict[str, Any]\n    ) -> Tuple[Any, Sequence[Any], Dict[str, Any]]:\n        \"\"\"\n        \"\"\"\n        if self.arg_pos is not None and len(args) > self.arg_pos:\n            # The arg to replace is passed positionally\n            old_value = args[self.arg_pos]\n            args = list(args)  # *args is normally a tuple\n            args[self.arg_pos] = new_value\n        else:\n            # The arg to replace is either omitted or passed by keyword.\n            old_value = kwargs.get(self.name)\n            kwargs[self.name] = new_value\n        return old_value, args, kwargs", "output": "Replace the named argument in ``args, kwargs`` with ``new_value``.\n\n        Returns ``(old_value, args, kwargs)``.  The returned ``args`` and\n        ``kwargs`` objects may not be the same as the input objects, or\n        the input objects may be mutated.\n\n        If the named argument was not found, ``new_value`` will be added\n        to ``kwargs`` and None will be returned as ``old_value``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def htmlDocContentDumpOutput(self, buf, encoding):\n        \"\"\" \"\"\"\n        if buf is None: buf__o = None\n        else: buf__o = buf._o\n        libxml2mod.htmlDocContentDumpOutput(buf__o, self._o, encoding)", "output": "Dump an HTML document. Formating return/spaces are added.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_fun(fun):\n    '''\n    \n    '''\n    serv = _get_serv(ret=None)\n    minions = _get_list(serv, 'minions')\n    returns = serv.get_multi(minions, key_prefix='{0}:'.format(fun))\n    # returns = {minion: return, minion: return, ...}\n    ret = {}\n    for minion, data in six.iteritems(returns):\n        ret[minion] = salt.utils.json.loads(data)\n    return ret", "output": "Return a dict of the last function called for all minions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def equals(self, other):\n        \"\"\"\n        \n        \"\"\"\n        if self is other:\n            return True\n\n        if not isinstance(other, Index):\n            return False\n\n        # need to compare nans locations and make sure that they are the same\n        # since nans don't compare equal this is a bit tricky\n        try:\n            if not isinstance(other, Float64Index):\n                other = self._constructor(other)\n            if (not is_dtype_equal(self.dtype, other.dtype) or\n                    self.shape != other.shape):\n                return False\n            left, right = self._ndarray_values, other._ndarray_values\n            return ((left == right) | (self._isnan & other._isnan)).all()\n        except (TypeError, ValueError):\n            return False", "output": "Determines if two Index objects contain the same elements.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rotate_left(self):\n        \"\"\"\n        \n        \"\"\"\n        new_root = self.node.right.node\n        new_left_sub = new_root.left.node\n        old_root = self.node\n\n        self.node = new_root\n        old_root.right.node = new_left_sub\n        new_root.left.node = old_root", "output": "Left rotation", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extract_ranges(index_list, range_size_limit=32):\n    \"\"\"\n  \"\"\"\n    if not index_list:\n        return [], []\n    first = index_list[0]\n    last = first\n    ranges = []\n    singles = []\n    for i in index_list[1:]:\n        if i == last + 1 and (last - first) <= range_size_limit:\n            last = i\n        else:\n            if last > first:\n                ranges.append([first, last])\n            else:\n                singles.append(first)\n            first = i\n            last = i\n    if last > first:\n        ranges.append([first, last])\n    else:\n        singles.append(first)\n    return ranges, singles", "output": "Extract consecutive ranges and singles from index_list.\n\n  Args:\n    index_list: List of monotone increasing non-negative integers.\n    range_size_limit: Largest size range to return.  If a larger\n      consecutive range exists it will be returned as multiple\n      ranges.\n\n  Returns:\n   ranges, singles where ranges is a list of [first, last] pairs of\n     consecutive elements in index_list, and singles is all of the\n     other elements, in original order.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_location(self, text=''):\r\n        \"\"\"\"\"\"\r\n        self.text_project_name.setEnabled(self.radio_new_dir.isChecked())\r\n        name = self.text_project_name.text().strip()\r\n\r\n        if name and self.radio_new_dir.isChecked():\r\n            path = osp.join(self.location, name)\r\n            self.button_create.setDisabled(os.path.isdir(path))\r\n        elif self.radio_from_dir.isChecked():\r\n            self.button_create.setEnabled(True)\r\n            path = self.location\r\n        else:\r\n            self.button_create.setEnabled(False)\r\n            path = self.location\r\n        \r\n        self.text_location.setText(path)", "output": "Update text of location.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _retry_from_retry_config(retry_params, retry_codes):\n    \"\"\"\n    \"\"\"\n    exception_classes = [\n        _exception_class_for_grpc_status_name(code) for code in retry_codes\n    ]\n    return retry.Retry(\n        retry.if_exception_type(*exception_classes),\n        initial=(retry_params[\"initial_retry_delay_millis\"] / _MILLIS_PER_SECOND),\n        maximum=(retry_params[\"max_retry_delay_millis\"] / _MILLIS_PER_SECOND),\n        multiplier=retry_params[\"retry_delay_multiplier\"],\n        deadline=retry_params[\"total_timeout_millis\"] / _MILLIS_PER_SECOND,\n    )", "output": "Creates a Retry object given a gapic retry configuration.\n\n    Args:\n        retry_params (dict): The retry parameter values, for example::\n\n            {\n                \"initial_retry_delay_millis\": 1000,\n                \"retry_delay_multiplier\": 2.5,\n                \"max_retry_delay_millis\": 120000,\n                \"initial_rpc_timeout_millis\": 120000,\n                \"rpc_timeout_multiplier\": 1.0,\n                \"max_rpc_timeout_millis\": 120000,\n                \"total_timeout_millis\": 600000\n            }\n\n        retry_codes (sequence[str]): The list of retryable gRPC error code\n            names.\n\n    Returns:\n        google.api_core.retry.Retry: The default retry object for the method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encodeEntities(self, input):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlEncodeEntities(self._o, input)\n        return ret", "output": "TODO: remove xmlEncodeEntities, once we are not afraid of\n          breaking binary compatibility  People must migrate their\n          code to xmlEncodeEntitiesReentrant ! This routine will\n           issue a warning when encountered.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delimitedList( expr, delim=\",\", combine=False ):\n    \"\"\"\n    \"\"\"\n    dlName = _ustr(expr)+\" [\"+_ustr(delim)+\" \"+_ustr(expr)+\"]...\"\n    if combine:\n        return Combine( expr + ZeroOrMore( delim + expr ) ).setName(dlName)\n    else:\n        return ( expr + ZeroOrMore( Suppress( delim ) + expr ) ).setName(dlName)", "output": "Helper to define a delimited list of expressions - the delimiter\n    defaults to ','. By default, the list elements and delimiters can\n    have intervening whitespace, and comments, but this can be\n    overridden by passing ``combine=True`` in the constructor. If\n    ``combine`` is set to ``True``, the matching tokens are\n    returned as a single token string, with the delimiters included;\n    otherwise, the matching tokens are returned as a list of tokens,\n    with the delimiters suppressed.\n\n    Example::\n\n        delimitedList(Word(alphas)).parseString(\"aa,bb,cc\") # -> ['aa', 'bb', 'cc']\n        delimitedList(Word(hexnums), delim=':', combine=True).parseString(\"AA:BB:CC:DD:EE\") # -> ['AA:BB:CC:DD:EE']", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe_subnet(subnet_id=None, subnet_name=None, region=None,\n                    key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    try:\n        subnet = _get_resource('subnet', name=subnet_name, resource_id=subnet_id,\n                               region=region, key=key, keyid=keyid, profile=profile)\n    except BotoServerError as e:\n        return {'error': __utils__['boto.get_error'](e)}\n\n    if not subnet:\n        return {'subnet': None}\n    log.debug('Found subnet: %s', subnet.id)\n\n    keys = ('id', 'cidr_block', 'availability_zone', 'tags', 'vpc_id')\n    ret = {'subnet': dict((k, getattr(subnet, k)) for k in keys)}\n    explicit_route_table_assoc = _get_subnet_explicit_route_table(ret['subnet']['id'],\n                                                                  ret['subnet']['vpc_id'],\n                                                                  conn=None, region=region,\n                                                                  key=key, keyid=keyid, profile=profile)\n    if explicit_route_table_assoc:\n        ret['subnet']['explicit_route_table_association_id'] = explicit_route_table_assoc\n    return ret", "output": "Given a subnet id or name, describe its properties.\n\n    Returns a dictionary of interesting properties.\n\n    .. versionadded:: 2015.8.0\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt myminion boto_vpc.describe_subnet subnet_id=subnet-123456\n        salt myminion boto_vpc.describe_subnet subnet_name=mysubnet", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_params(self, arg_params, aux_params, allow_missing=False, force_init=True,\n                   allow_extra=False):\n        \"\"\"\n        \"\"\"\n        self.init_params(initializer=None, arg_params=arg_params, aux_params=aux_params,\n                         allow_missing=allow_missing, force_init=force_init,\n                         allow_extra=allow_extra)", "output": "Assigns parameter and aux state values.\n\n        Parameters\n        ----------\n        arg_params : dict\n            Dictionary of name to value (`NDArray`) mapping.\n        aux_params : dict\n            Dictionary of name to value (`NDArray`) mapping.\n        allow_missing : bool\n            If ``True``, params could contain missing values, and the initializer will be\n            called to fill those missing params.\n        force_init : bool\n            If ``True``, will force re-initialize even if already initialized.\n        allow_extra : boolean, optional\n            Whether allow extra parameters that are not needed by symbol.\n            If this is True, no error will be thrown when arg_params or aux_params\n            contain extra parameters that is not needed by the executor.\n\n        Examples\n        --------\n        >>> # An example of setting module parameters.\n        >>> sym, arg_params, aux_params = mx.model.load_checkpoint(model_prefix, n_epoch_load)\n        >>> mod.set_params(arg_params=arg_params, aux_params=aux_params)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def AvgPooling(\n        inputs,\n        pool_size,\n        strides=None,\n        padding='valid',\n        data_format='channels_last'):\n    \"\"\"\n    \n    \"\"\"\n    if strides is None:\n        strides = pool_size\n    layer = tf.layers.AveragePooling2D(pool_size, strides, padding=padding, data_format=data_format)\n    ret = layer.apply(inputs, scope=tf.get_variable_scope())\n    return tf.identity(ret, name='output')", "output": "Same as `tf.layers.AveragePooling2D`. Default strides is equal to pool_size.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lstm_init_states(batch_size):\n    \"\"\" \"\"\"\n    hp = Hyperparams()\n    init_shapes = lstm.init_states(batch_size=batch_size, num_lstm_layer=hp.num_lstm_layer, num_hidden=hp.num_hidden)\n    init_names = [s[0] for s in init_shapes]\n    init_arrays = [mx.nd.zeros(x[1]) for x in init_shapes]\n    return init_names, init_arrays", "output": "Returns a tuple of names and zero arrays for LSTM init states", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def poll_open_file_languages(self):\r\n        \"\"\"\"\"\"\r\n        languages = []\r\n        for index in range(self.get_stack_count()):\r\n            languages.append(\r\n                self.tabs.widget(index).language.lower())\r\n        return set(languages)", "output": "Get list of current opened files' languages", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _op_maker(op_class, op_symbol):\n    \"\"\"\n    \"\"\"\n\n    def f(self, node, *args, **kwargs):\n        \"\"\"Return a partial function with an Op subclass with an operator\n        already passed.\n\n        Returns\n        -------\n        f : callable\n        \"\"\"\n        return partial(op_class, op_symbol, *args, **kwargs)\n    return f", "output": "Return a function to create an op class with its symbol already passed.\n\n    Returns\n    -------\n    f : callable", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def contextMenuEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        self.opt_menu.popup(event.globalPos())\r\n        event.accept()", "output": "Reimplement Qt method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shakeshake2_py(x, y, equal=False, individual=False):\n  \"\"\"\"\"\"\n  if equal:\n    alpha = 0.5\n  elif individual:\n    alpha = tf.random_uniform(tf.get_shape(x)[:1])\n  else:\n    alpha = tf.random_uniform([])\n\n  return alpha * x + (1.0 - alpha) * y", "output": "The shake-shake sum of 2 tensors, python version.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getUsage(api_key=None, api_version=None):\n    '''\n    \n    '''\n    ret = {'res': True}\n\n    if not api_key or not api_version:\n        try:\n            options = __salt__['config.option']('random_org')\n            if not api_key:\n                api_key = options.get('api_key')\n            if not api_version:\n                api_version = options.get('api_version')\n        except (NameError, KeyError, AttributeError):\n            log.error('No Random.org api key found.')\n            ret['message'] = 'No Random.org api key or api version found.'\n            ret['res'] = False\n            return ret\n\n    if isinstance(api_version, int):\n        api_version = six.text_type(api_version)\n\n    _function = RANDOM_ORG_FUNCTIONS.get(api_version).get('getUsage').get('method')\n    data = {}\n    data['id'] = 1911220\n    data['jsonrpc'] = '2.0'\n    data['method'] = _function\n    data['params'] = {'apiKey': api_key}\n\n    result = _query(api_version=api_version, data=data)\n\n    if result:\n        ret['bitsLeft'] = result.get('bitsLeft')\n        ret['requestsLeft'] = result.get('requestsLeft')\n        ret['totalBits'] = result.get('totalBits')\n        ret['totalRequests'] = result.get('totalRequests')\n    else:\n        ret['res'] = False\n        ret['message'] = result['message']\n    return ret", "output": "Show current usages statistics\n\n    :param api_key: The Random.org api key.\n    :param api_version: The Random.org api version.\n    :return: The current usage statistics.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' random_org.getUsage\n\n        salt '*' random_org.getUsage api_key=peWcBiMOS9HrZG15peWcBiMOS9HrZG15 api_version=1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read(path, attribute, **kwargs):\n    '''\n    \n    '''\n    kwargs = salt.utils.args.clean_kwargs(**kwargs)\n    hex_ = kwargs.pop('hex', False)\n    if kwargs:\n        salt.utils.args.invalid_kwargs(kwargs)\n\n    cmd = ['xattr', '-p']\n    if hex_:\n        cmd.append('-x')\n    cmd.extend([attribute, path])\n\n    try:\n        ret = salt.utils.mac_utils.execute_return_result(cmd)\n    except CommandExecutionError as exc:\n        if 'No such file' in exc.strerror:\n            raise CommandExecutionError('File not found: {0}'.format(path))\n        if 'No such xattr' in exc.strerror:\n            raise CommandExecutionError('Attribute not found: {0}'.format(attribute))\n        raise CommandExecutionError('Unknown Error: {0}'.format(exc.strerror))\n\n    return ret", "output": "Read the given attributes on the given file/directory\n\n    :param str path: The file to get attributes from\n\n    :param str attribute: The attribute to read\n\n    :param bool hex: Return the values with forced hexadecimal values\n\n    :return: A string containing the value of the named attribute\n    :rtype: str\n\n    :raises: CommandExecutionError on file not found, attribute not found, and\n        any other unknown error\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' xattr.read /path/to/file com.test.attr\n        salt '*' xattr.read /path/to/file com.test.attr hex=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop_site(name):\n    '''\n    \n    '''\n    ps_cmd = ['Stop-WebSite', r\"'{0}'\".format(name)]\n\n    cmd_ret = _srvmgr(ps_cmd)\n\n    return cmd_ret['retcode'] == 0", "output": "Stop a Web Site in IIS.\n\n    .. versionadded:: 2017.7.0\n\n    Args:\n        name (str): The name of the website to stop.\n\n    Returns:\n        bool: True if successful, otherwise False\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' win_iis.stop_site name='My Test Site'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_python_command(line):\n    \"\"\"\n    \n    \"\"\"\n\n    if not isinstance(line, six.string_types):\n        raise TypeError(\"Not a valid command to check: {0!r}\".format(line))\n\n    from pipenv.vendor.pythonfinder.utils import PYTHON_IMPLEMENTATIONS\n    is_version = re.match(r'[\\d\\.]+', line)\n    if (line.startswith(\"python\") or is_version or\n            any(line.startswith(v) for v in PYTHON_IMPLEMENTATIONS)):\n        return True\n    # we are less sure about this but we can guess\n    if line.startswith(\"py\"):\n        return True\n    return False", "output": "Given an input, checks whether the input is a request for python or notself.\n\n    This can be a version, a python runtime name, or a generic 'python' or 'pythonX.Y'\n\n    :param str line: A potential request to find python\n    :returns: Whether the line is a python lookup\n    :rtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self, client=None):\n        \"\"\"\n        \"\"\"\n        client = self._require_client(client)\n        query_params = self._query_params\n        query_params[\"projection\"] = \"full\"\n        api_response = client._connection.api_request(\n            method=\"PUT\",\n            path=self.path,\n            data=self._properties,\n            query_params=query_params,\n            _target_object=self,\n        )\n        self._set_properties(api_response)", "output": "Sends all properties in a PUT request.\n\n        Updates the ``_properties`` with the response from the backend.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use.  If not passed, falls back to the\n                       ``client`` stored on the current object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_metric_names(self, names):\n        \"\"\n        if hasattr(self, '_added_met_names'): self._added_met_names += names\n        else:                                 self._added_met_names  = names", "output": "Add `names` to the inner metric names.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def segments(self, using=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        return self._get_connection(using).indices.segments(index=self._name, **kwargs)", "output": "Provide low level segments information that a Lucene index (shard\n        level) is built with.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.segments`` unchanged.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rename(self, names, inplace=False):\n        \"\"\"\n        \n        \"\"\"\n        if (type(names) is not dict):\n            raise TypeError('names must be a dictionary: oldname -> newname')\n        all_columns = set(self.column_names())\n        for k in names:\n            if not k in all_columns:\n                raise ValueError('Cannot find column %s in the SFrame' % k)\n\n        if inplace:\n            ret = self\n        else:\n            ret = self.copy()\n\n        with cython_context():\n            for k in names:\n                colid = ret.column_names().index(k)\n                ret.__proxy__.set_column_name(colid, names[k])\n        ret._cache = None\n        return ret", "output": "Returns an SFrame with columns renamed. ``names`` is expected to be a\n        dict specifying the old and new names. This changes the names of the\n        columns given as the keys and replaces them with the names given as the\n        values.\n\n        If inplace == False (default) this operation does not modify the\n        current SFrame, returning a new SFrame.\n\n        If inplace == True, this operation modifies the current\n        SFrame, returning self.\n\n        Parameters\n        ----------\n        names : dict [string, string]\n            Dictionary of [old_name, new_name]\n\n        inplace : bool, optional. Defaults to False.\n            Whether the SFrame is modified in place.\n\n        Returns\n        -------\n        out : SFrame\n            The current SFrame.\n\n        See Also\n        --------\n        column_names\n\n        Examples\n        --------\n        >>> sf = SFrame({'X1': ['Alice','Bob'],\n        ...              'X2': ['123 Fake Street','456 Fake Street']})\n        >>> res = sf.rename({'X1': 'name', 'X2':'address'})\n        >>> res\n        +-------+-----------------+\n        |  name |     address     |\n        +-------+-----------------+\n        | Alice | 123 Fake Street |\n        |  Bob  | 456 Fake Street |\n        +-------+-----------------+\n        [2 rows x 2 columns]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_hash(self, data, hasher=None):\n        \"\"\"\n        \n        \"\"\"\n        if hasher is None:\n            hasher = self.hasher\n        if hasher is None:\n            hasher = hashlib.md5\n            prefix = ''\n        else:\n            hasher = getattr(hashlib, hasher)\n            prefix = '%s=' % self.hasher\n        digest = hasher(data).digest()\n        digest = base64.urlsafe_b64encode(digest).rstrip(b'=').decode('ascii')\n        return '%s%s' % (prefix, digest)", "output": "Get the hash of some data, using a particular hash algorithm, if\n        specified.\n\n        :param data: The data to be hashed.\n        :type data: bytes\n        :param hasher: The name of a hash implementation, supported by hashlib,\n                       or ``None``. Examples of valid values are ``'sha1'``,\n                       ``'sha224'``, ``'sha384'``, '``sha256'``, ``'md5'`` and\n                       ``'sha512'``. If no hasher is specified, the ``hasher``\n                       attribute of the :class:`InstalledDistribution` instance\n                       is used. If the hasher is determined to be ``None``, MD5\n                       is used as the hashing algorithm.\n        :returns: The hash of the data. If a hasher was explicitly specified,\n                  the returned hash will be prefixed with the specified hasher\n                  followed by '='.\n        :rtype: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bb2hw(a:Collection[int])->np.ndarray:\n    \"\"\n    return np.array([a[1],a[0],a[3]-a[1],a[2]-a[0]])", "output": "Convert bounding box points from (width,height,center) to (height,width,top,left).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chgroups(name, groups, append=True):\n    '''\n    \n    '''\n    if six.PY2:\n        name = _to_unicode(name)\n\n    if isinstance(groups, string_types):\n        groups = groups.split(',')\n\n    groups = [x.strip(' *') for x in groups]\n    if six.PY2:\n        groups = [_to_unicode(x) for x in groups]\n\n    ugrps = set(list_groups(name))\n    if ugrps == set(groups):\n        return True\n\n    name = _cmd_quote(name)\n\n    if not append:\n        for group in ugrps:\n            group = _cmd_quote(group).lstrip('\\'').rstrip('\\'')\n            if group not in groups:\n                cmd = 'net localgroup \"{0}\" {1} /delete'.format(group, name)\n                __salt__['cmd.run_all'](cmd, python_shell=True)\n\n    for group in groups:\n        if group in ugrps:\n            continue\n        group = _cmd_quote(group).lstrip('\\'').rstrip('\\'')\n        cmd = 'net localgroup \"{0}\" {1} /add'.format(group, name)\n        out = __salt__['cmd.run_all'](cmd, python_shell=True)\n        if out['retcode'] != 0:\n            log.error(out['stdout'])\n            return False\n\n    agrps = set(list_groups(name))\n    return len(ugrps - agrps) == 0", "output": "Change the groups this user belongs to, add append=False to make the user a\n    member of only the specified groups\n\n    Args:\n        name (str): The user name for which to change groups\n\n        groups (str, list): A single group or a list of groups to assign to the\n            user. For multiple groups this can be a comma delimited string or a\n            list.\n\n        append (bool, optional): True adds the passed groups to the user's\n            current groups. False sets the user's groups to the passed groups\n            only. Default is True.\n\n    Returns:\n        bool: True if successful, otherwise False\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.chgroups jsnuffy Administrators,Users True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def maybe_resume_consumer(self):\n        \"\"\"\"\"\"\n        # If we have been paused by flow control, check and see if we are\n        # back within our limits.\n        #\n        # In order to not thrash too much, require us to have passed below\n        # the resume threshold (80% by default) of each flow control setting\n        # before restarting.\n        if self._consumer is None or not self._consumer.is_paused:\n            return\n\n        if self.load < self.flow_control.resume_threshold:\n            self._consumer.resume()\n        else:\n            _LOGGER.debug(\"Did not resume, current load is %s\", self.load)", "output": "Check the current load and resume the consumer if needed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def join_lines(lines_enum):\n    # type: (ReqFileLines) -> ReqFileLines\n    \"\"\"\n    \"\"\"\n    primary_line_number = None\n    new_line = []  # type: List[Text]\n    for line_number, line in lines_enum:\n        if not line.endswith('\\\\') or COMMENT_RE.match(line):\n            if COMMENT_RE.match(line):\n                # this ensures comments are always matched later\n                line = ' ' + line\n            if new_line:\n                new_line.append(line)\n                yield primary_line_number, ''.join(new_line)\n                new_line = []\n            else:\n                yield line_number, line\n        else:\n            if not new_line:\n                primary_line_number = line_number\n            new_line.append(line.strip('\\\\'))\n\n    # last line contains \\\n    if new_line:\n        yield primary_line_number, ''.join(new_line)", "output": "Joins a line ending in '\\' with the previous line (except when following\n    comments).  The joined line takes on the index of the first line.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_typed_parameter(param):\n    '''\n    \n    '''\n    global _current_parameter_value\n    type_, value = _expand_one_key_dictionary(param)\n    _current_parameter.type = type_\n\n    if _is_simple_type(value) and value != '':\n        _current_parameter_value = SimpleParameterValue(value)\n        _current_parameter.add_value(_current_parameter_value)\n    elif isinstance(value, list):\n        for i in value:\n            if _is_simple_type(i):\n                _current_parameter_value = SimpleParameterValue(i)\n                _current_parameter.add_value(_current_parameter_value)\n            elif isinstance(i, dict):\n                _current_parameter_value = TypedParameterValue()\n                _parse_typed_parameter_typed_value(i)\n                _current_parameter.add_value(_current_parameter_value)", "output": "Parses a TypedParameter and fills it with values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filter_bboxes_by_visibility(original_shape, bboxes, transformed_shape, transformed_bboxes,\n                                threshold=0., min_area=0.):\n    \"\"\"\n    \"\"\"\n    img_height, img_width = original_shape[:2]\n    transformed_img_height, transformed_img_width = transformed_shape[:2]\n\n    visible_bboxes = []\n    for bbox, transformed_bbox in zip(bboxes, transformed_bboxes):\n        if not all(0.0 <= value <= 1.0 for value in transformed_bbox[:4]):\n            continue\n        bbox_area = calculate_bbox_area(bbox, img_height, img_width)\n        transformed_bbox_area = calculate_bbox_area(transformed_bbox, transformed_img_height, transformed_img_width)\n        if transformed_bbox_area < min_area:\n            continue\n        visibility = transformed_bbox_area / bbox_area\n        if visibility >= threshold:\n            visible_bboxes.append(transformed_bbox)\n    return visible_bboxes", "output": "Filter bounding boxes and return only those boxes whose visibility after transformation is above\n    the threshold and minimal area of bounding box in pixels is more then min_area.\n\n    Args:\n        original_shape (tuple): original image shape\n        bboxes (list): original bounding boxes\n        transformed_shape(tuple): transformed image\n        transformed_bboxes (list): transformed bounding boxes\n        threshold (float): visibility threshold. Should be a value in the range [0.0, 1.0].\n        min_area (float): Minimal area threshold.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forward(self, inputs, begin_state=None): # pylint: disable=arguments-differ\n        \"\"\"\n        \"\"\"\n        encoded = self.embedding(inputs)\n        if begin_state is None:\n            begin_state = self.begin_state(batch_size=inputs.shape[1])\n        out_states = []\n        for i, (e, s) in enumerate(zip(self.encoder, begin_state)):\n            encoded, state = e(encoded, s)\n            out_states.append(state)\n            if self._drop_h and i != len(self.encoder)-1:\n                encoded = nd.Dropout(encoded, p=self._drop_h, axes=(0,))\n        if self._dropout:\n            encoded = nd.Dropout(encoded, p=self._dropout, axes=(0,))\n        with autograd.predict_mode():\n            out = self.decoder(encoded)\n        return out, out_states", "output": "Implement forward computation.\n\n        Parameters\n        -----------\n        inputs : NDArray\n            input tensor with shape `(sequence_length, batch_size)`\n            when `layout` is \"TNC\".\n        begin_state : list\n            initial recurrent state tensor with length equals to num_layers.\n            the initial state with shape `(1, batch_size, num_hidden)`\n\n        Returns\n        --------\n        out: NDArray\n            output tensor with shape `(sequence_length, batch_size, input_size)`\n            when `layout` is \"TNC\".\n        out_states: list\n            output recurrent state tensor with length equals to num_layers.\n            the state with shape `(1, batch_size, num_hidden)`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send_one_ping(mySocket, destIP, myID, mySeqNumber, packet_size):\n    \"\"\"\n    \n    \"\"\"\n    #destIP  =  socket.gethostbyname(destIP)\n\n    # Header is type (8), code (8), checksum (16), id (16), sequence (16)\n    # (packet_size - 8) - Remove header size from packet size\n    myChecksum = 0\n\n    # Make a dummy heder with a 0 checksum.\n    header = struct.pack(\n        \"!BBHHH\", ICMP_ECHO, 0, myChecksum, myID, mySeqNumber\n    )\n\n    padBytes = []\n    startVal = 0x42\n    # 'cose of the string/byte changes in python 2/3 we have\n    # to build the data differnely for different version\n    # or it will make packets with unexpected size.\n    if sys.version[:1] == '2':\n        bytes = struct.calcsize(\"d\")\n        data = ((packet_size - 8) - bytes) * \"Q\"\n        data = struct.pack(\"d\", default_timer()) + data\n    else:\n        for i in range(startVal, startVal + (packet_size-8)):\n            padBytes += [(i & 0xff)]  # Keep chars in the 0-255 range\n        #data = bytes(padBytes)\n        data = bytearray(padBytes)\n\n\n    # Calculate the checksum on the data and the dummy header.\n    myChecksum = checksum(header + data) # Checksum is in network order\n\n    # Now that we have the right checksum, we put that in. It's just easier\n    # to make up a new header than to stuff it into the dummy.\n    header = struct.pack(\n        \"!BBHHH\", ICMP_ECHO, 0, myChecksum, myID, mySeqNumber\n    )\n\n    packet = header + data\n\n    sendTime = default_timer()\n\n    try:\n        mySocket.sendto(packet, (destIP, 1)) # Port number is irrelevant for ICMP\n    except socket.error as e:\n        print(\"General failure (%s)\" % (e.args[1]))\n        return\n\n    return sendTime", "output": "Send one ping to the given >destIP<.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_snapshot(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The create_snapshot function must be called with -f or --function.'\n        )\n\n    if not kwargs or 'name' not in kwargs:\n        log.error(\n            'A name must be specified when creating a snapshot.'\n        )\n        return False\n\n    if 'disk_name' not in kwargs:\n        log.error(\n            'A disk_name must be specified when creating a snapshot.'\n        )\n        return False\n\n    conn = get_conn()\n\n    name = kwargs.get('name')\n    disk_name = kwargs.get('disk_name')\n\n    try:\n        disk = conn.ex_get_volume(disk_name)\n    except ResourceNotFoundError as exc:\n        log.error(\n            'Disk %s was not found. Exception was: %s',\n            disk_name, exc, exc_info_on_loglevel=logging.DEBUG\n        )\n        return False\n\n    __utils__['cloud.fire_event'](\n        'event',\n        'create snapshot',\n        'salt/cloud/snapshot/creating',\n        args={\n            'name': name,\n            'disk_name': disk_name,\n        },\n        sock_dir=__opts__['sock_dir'],\n        transport=__opts__['transport']\n    )\n\n    snapshot = conn.create_volume_snapshot(disk, name)\n\n    __utils__['cloud.fire_event'](\n        'event',\n        'created snapshot',\n        'salt/cloud/snapshot/created',\n        args={\n            'name': name,\n            'disk_name': disk_name,\n        },\n        sock_dir=__opts__['sock_dir'],\n        transport=__opts__['transport']\n    )\n    return _expand_item(snapshot)", "output": "Create a new disk snapshot. Must specify `name` and  `disk_name`.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f create_snapshot gce name=snap1 disk_name=pd", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_entry(self, entry):\n        \"\"\"\n        \"\"\"\n        self.entry_keys.setdefault(entry, [])\n        self.entries.append(entry)\n        for dist in find_distributions(entry, True):\n            self.add(dist, entry, False)", "output": "Add a path item to ``.entries``, finding any distributions on it\n\n        ``find_distributions(entry, True)`` is used to find distributions\n        corresponding to the path entry, and they are added.  `entry` is\n        always appended to ``.entries``, even if it is already present.\n        (This is because ``sys.path`` can contain the same value more than\n        once, and the ``.entries`` of the ``sys.path`` WorkingSet should always\n        equal ``sys.path``.)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def virtual_interface_list(provider, names, **kwargs):\n    '''\n    \n\n    '''\n    client = _get_client()\n    return client.extra_action(provider=provider, names=names, action='virtual_interface_list', **kwargs)", "output": "List virtual interfaces on a server\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minionname cloud.virtual_interface_list my-nova names=['salt-master']", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_item(self):\r\n        \"\"\"\"\"\"\r\n        indexes = self.selectedIndexes()\r\n        if not indexes:\r\n            return\r\n        for index in indexes:\r\n            if not index.isValid():\r\n                return\r\n        one = _(\"Do you want to remove the selected item?\")\r\n        more = _(\"Do you want to remove all selected items?\")\r\n        answer = QMessageBox.question(self, _( \"Remove\"),\r\n                                      one if len(indexes) == 1 else more,\r\n                                      QMessageBox.Yes | QMessageBox.No)\r\n        if answer == QMessageBox.Yes:\r\n            idx_rows = unsorted_unique([idx.row() for idx in indexes])\r\n            keys = [ self.model.keys[idx_row] for idx_row in idx_rows ]\r\n            self.remove_values(keys)", "output": "Remove item", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deleteAllSystems(server):\n    '''\n    \n    '''\n\n    try:\n        client, key = _get_session(server)\n    except Exception as exc:\n        err_msg = 'Exception raised when connecting to spacewalk server ({0}): {1}'.format(server, exc)\n        log.error(err_msg)\n        return {'Error': err_msg}\n\n    systems = client.system.listSystems(key)\n\n    ids = []\n    names = []\n    for system in systems:\n        ids.append(system['id'])\n        names.append(system['name'])\n\n    if client.system.deleteSystems(key, ids) == 1:\n        return {'deleted': names}\n    else:\n        return {'Error': 'Failed to delete all systems'}", "output": "Delete all systems from Spacewalk\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run spacewalk.deleteAllSystems spacewalk01.domain.com", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extra_reading_spec(self):\n    \"\"\"\"\"\"\n    field_names = (\"frame_number\", \"action\", \"reward\", \"done\")\n    data_fields = {\n        name: tf.FixedLenFeature([1], tf.int64) for name in field_names\n    }\n    decoders = {\n        name: tf.contrib.slim.tfexample_decoder.Tensor(tensor_key=name)\n        for name in field_names\n    }\n    return (data_fields, decoders)", "output": "Additional data fields to store on disk and their decoders.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_scipy_sparse(arr):\n    \"\"\"\n    \n    \"\"\"\n\n    global _is_scipy_sparse\n\n    if _is_scipy_sparse is None:\n        try:\n            from scipy.sparse import issparse as _is_scipy_sparse\n        except ImportError:\n            _is_scipy_sparse = lambda _: False\n\n    return _is_scipy_sparse(arr)", "output": "Check whether an array-like is a scipy.sparse.spmatrix instance.\n\n    Parameters\n    ----------\n    arr : array-like\n        The array-like to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array-like is a scipy.sparse.spmatrix instance.\n\n    Notes\n    -----\n    If scipy is not installed, this function will always return False.\n\n    Examples\n    --------\n    >>> from scipy.sparse import bsr_matrix\n    >>> is_scipy_sparse(bsr_matrix([1, 2, 3]))\n    True\n    >>> is_scipy_sparse(pd.SparseArray([1, 2, 3]))\n    False\n    >>> is_scipy_sparse(pd.SparseSeries([1, 2, 3]))\n    False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_indicator_BIAS(DataFrame, N1, N2, N3):\n    ''\n    CLOSE = DataFrame['close']\n    BIAS1 = (CLOSE - MA(CLOSE, N1)) / MA(CLOSE, N1) * 100\n    BIAS2 = (CLOSE - MA(CLOSE, N2)) / MA(CLOSE, N2) * 100\n    BIAS3 = (CLOSE - MA(CLOSE, N3)) / MA(CLOSE, N3) * 100\n    DICT = {'BIAS1': BIAS1, 'BIAS2': BIAS2, 'BIAS3': BIAS3}\n\n    return pd.DataFrame(DICT)", "output": "\u4e56\u79bb\u7387", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extract_scalar_reward(value, scalar_key='default'):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(value, float) or isinstance(value, int):\n        reward = value\n    elif isinstance(value, dict) and scalar_key in value and isinstance(value[scalar_key], (float, int)):\n        reward = value[scalar_key]\n    else:\n        raise RuntimeError('Incorrect final result: the final result should be float/int, or a dict which has a key named \"default\" whose value is float/int.')\n    return reward", "output": "Extract scalar reward from trial result.\n\n    Raises\n    ------\n    RuntimeError\n        Incorrect final result: the final result should be float/int,\n        or a dict which has a key named \"default\" whose value is float/int.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dispatch(self, receiver):\n        ''' \n\n        '''\n        super(SessionCallbackRemoved, self).dispatch(receiver)\n        if hasattr(receiver, '_session_callback_removed'):\n            receiver._session_callback_removed(self)", "output": "Dispatch handling of this event to a receiver.\n\n        This method will invoke ``receiver._session_callback_removed`` if\n        it exists.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _remove_double_brackets(text):\n  \"\"\"\n  \"\"\"\n\n  def replacement_fn(s):\n    if \":\" in s:\n      # this is probably a category or something like that.\n      return \"\"\n    # keep the part after the bar.\n    bar_pos = s.find(\"|\")\n    if bar_pos == -1:\n      return s\n    return s[bar_pos + 1:]\n\n  return _find_and_replace(text, \"[[\", \"]]\", replacement_fn)", "output": "Remove double brackets, but leave the viewable text.\n\n  Args:\n    text: a string\n  Returns:\n    a string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_bool_indexer(key: Any) -> bool:\n    \"\"\"\n    \n    \"\"\"\n    na_msg = 'cannot index with vector containing NA / NaN values'\n    if (isinstance(key, (ABCSeries, np.ndarray, ABCIndex)) or\n            (is_array_like(key) and is_extension_array_dtype(key.dtype))):\n        if key.dtype == np.object_:\n            key = np.asarray(values_from_object(key))\n\n            if not lib.is_bool_array(key):\n                if isna(key).any():\n                    raise ValueError(na_msg)\n                return False\n            return True\n        elif is_bool_dtype(key.dtype):\n            # an ndarray with bool-dtype by definition has no missing values.\n            # So we only need to check for NAs in ExtensionArrays\n            if is_extension_array_dtype(key.dtype):\n                if np.any(key.isna()):\n                    raise ValueError(na_msg)\n            return True\n    elif isinstance(key, list):\n        try:\n            arr = np.asarray(key)\n            return arr.dtype == np.bool_ and len(arr) == len(key)\n        except TypeError:  # pragma: no cover\n            return False\n\n    return False", "output": "Check whether `key` is a valid boolean indexer.\n\n    Parameters\n    ----------\n    key : Any\n        Only list-likes may be considered boolean indexers.\n        All other types are not considered a boolean indexer.\n        For array-like input, boolean ndarrays or ExtensionArrays\n        with ``_is_boolean`` set are considered boolean indexers.\n\n    Returns\n    -------\n    bool\n\n    Raises\n    ------\n    ValueError\n        When the array is an object-dtype ndarray or ExtensionArray\n        and contains missing values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def borrow_optimizer(self, shared_module):\n        \"\"\"\n        \"\"\"\n        assert shared_module.optimizer_initialized\n        self._optimizer = shared_module._optimizer\n        self._kvstore = shared_module._kvstore\n        self._update_on_kvstore = shared_module._update_on_kvstore\n        self._updater = shared_module._updater\n        self.optimizer_initialized = True", "output": "Borrows optimizer from a shared module. Used in bucketing, where exactly the same\n        optimizer (esp. kvstore) is used.\n\n        Parameters\n        ----------\n        shared_module : Module", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dictupdate(dest, upd, recursive_update=True, merge_lists=False):\n    '''\n    \n    '''\n    return salt.utils.dictupdate.update(dest, upd, recursive_update=recursive_update, merge_lists=merge_lists)", "output": "Recursive version of the default dict.update\n\n    Merges upd recursively into dest\n\n    If recursive_update=False, will use the classic dict.update, or fall back\n    on a manual merge (helpful for non-dict types like ``FunctionWrapper``).\n\n    If ``merge_lists=True``, will aggregate list object types instead of replace.\n    The list in ``upd`` is added to the list in ``dest``, so the resulting list\n    is ``dest[key] + upd[key]``. This behaviour is only activated when\n    ``recursive_update=True``. By default ``merge_lists=False``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setListDoc(self, list):\n        \"\"\" \"\"\"\n        if list is None: list__o = None\n        else: list__o = list._o\n        libxml2mod.xmlSetListDoc(list__o, self._o)", "output": "update all nodes in the list to point to the right document", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_model(proto, f, format=None):  # type: (Union[ModelProto, bytes], Union[IO[bytes], Text], Optional[Any]) -> None\n    '''\n    \n    '''\n    if isinstance(proto, bytes):\n        proto = _deserialize(proto, ModelProto())\n\n    model_filepath = _get_file_path(f)\n    if model_filepath:\n        basepath = os.path.dirname(model_filepath)\n        proto = write_external_data_tensors(proto, basepath)\n\n    s = _serialize(proto)\n    _save_bytes(s, f)", "output": "Saves the ModelProto to the specified path.\n\n    @params\n    proto should be a in-memory ModelProto\n    f can be a file-like object (has \"write\" function) or a string containing a file name\n    format is for future use", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _remove_action_from_type(valid_actions: Dict[str, List[str]],\n                                 type_: str,\n                                 filter_function: Callable[[str], bool]) -> None:\n        \"\"\"\n        \n        \"\"\"\n        action_list = valid_actions[type_]\n        matching_action_index = [i for i, action in enumerate(action_list) if filter_function(action)]\n        assert len(matching_action_index) == 1, \"Filter function didn't find one action\"\n        action_list.pop(matching_action_index[0])", "output": "Finds the production rule matching the filter function in the given type's valid action\n        list, and removes it.  If there is more than one matching function, we crash.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_document_events(events, use_buffers=True):\n    ''' \n\n    '''\n\n    json_events = []\n    references = set()\n\n    buffers = [] if use_buffers else None\n\n    for event in events:\n        json_events.append(event.generate(references, buffers))\n\n    json = {\n        'events'     : json_events,\n        'references' : references_json(references),\n    }\n\n    return serialize_json(json), buffers if use_buffers else []", "output": "Create a JSON string describing a patch to be applied as well as\n    any optional buffers.\n\n    Args:\n      events : list of events to be translated into patches\n\n    Returns:\n      str, list :\n        JSON string which can be applied to make the given updates to obj\n        as well as any optional buffers", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_config_environment(self, config_data=None, quiet=False):\n        \"\"\"\n        \"\"\"\n\n        # Add all variables that start with KAGGLE_ to config data\n\n        if config_data is None:\n            config_data = {}\n        for key, val in os.environ.items():\n            if key.startswith('KAGGLE_'):\n                config_key = key.replace('KAGGLE_', '', 1).lower()\n                config_data[config_key] = val\n\n        return config_data", "output": "read_config_environment is the second effort to get a username\n           and key to authenticate to the Kaggle API. The environment keys\n           are equivalent to the kaggle.json file, but with \"KAGGLE_\" prefix\n           to define a unique namespace.\n\n           Parameters\n           ==========\n           config_data: a partially loaded configuration dictionary (optional)\n           quiet: suppress verbose print of output (default is False)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find(path, saltenv='base'):\n    '''\n    \n    '''\n    # Return a list of paths + text or bin\n    ret = []\n    if saltenv not in __opts__['pillar_roots']:\n        return ret\n    for root in __opts__['pillar_roots'][saltenv]:\n        full = os.path.join(root, path)\n        if os.path.isfile(full):\n            # Add it to the dict\n            with salt.utils.files.fopen(full, 'rb') as fp_:\n                if salt.utils.files.is_text(fp_):\n                    ret.append({full: 'txt'})\n                else:\n                    ret.append({full: 'bin'})\n    return ret", "output": "Return a dict of the files located with the given path and environment", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_unused_links(self, used):\n        \"\"\"\n        \"\"\"\n        unused = []\n\n        self._execute(\"SELECT * FROM {}\".format(self.LINK_STATE_TABLE))\n        for row in self.cursor:\n            relpath, inode, mtime = row\n            inode = self._from_sqlite(inode)\n            path = os.path.join(self.root_dir, relpath)\n\n            if path in used:\n                continue\n\n            if not os.path.exists(path):\n                continue\n\n            actual_inode = get_inode(path)\n            actual_mtime, _ = get_mtime_and_size(path)\n\n            if inode == actual_inode and mtime == actual_mtime:\n                logger.debug(\"Removing '{}' as unused link.\".format(path))\n                remove(path)\n                unused.append(relpath)\n\n        for relpath in unused:\n            cmd = 'DELETE FROM {} WHERE path = \"{}\"'\n            self._execute(cmd.format(self.LINK_STATE_TABLE, relpath))", "output": "Removes all saved links except the ones that are used.\n\n        Args:\n            used (list): list of used links that should not be removed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dmi_data(dmi_raw, clean, fields):\n    '''\n    \n    '''\n    dmi_data = {}\n\n    key = None\n    key_data = [None, []]\n    for line in dmi_raw:\n        if re.match(r'\\t[^\\s]+', line):\n            # Finish previous key\n            if key is not None:\n                # log.debug('Evaluating DMI key {0}: {1}'.format(key, key_data))\n                value, vlist = key_data\n                if vlist:\n                    if value is not None:\n                        # On the rare occasion\n                        # (I counted 1 on all systems we have)\n                        # that there's both a value <and> a list\n                        # just insert the value on top of the list\n                        vlist.insert(0, value)\n                    dmi_data[key] = vlist\n                elif value is not None:\n                    dmi_data[key] = value\n\n            # Family: Core i5\n            # Keyboard Password Status: Not Implemented\n            key, val = line.split(':', 1)\n            key = key.strip().lower().replace(' ', '_')\n            if (clean and key == 'header_and_data') \\\n                    or (fields and key not in fields):\n                key = None\n                continue\n            else:\n                key_data = [_dmi_cast(key, val.strip(), clean), []]\n        elif key is None:\n            continue\n        elif re.match(r'\\t\\t[^\\s]+', line):\n            # Installable Languages: 1\n            #        en-US\n            # Characteristics:\n            #        PCI is supported\n            #        PNP is supported\n            val = _dmi_cast(key, line.strip(), clean)\n            if val is not None:\n                # log.debug('DMI key %s gained list item %s', key, val)\n                key_data[1].append(val)\n\n    return dmi_data", "output": "Parse the raw DMIdecode output of a single handle\n    into a nice dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resize(image, target_size, **kwargs):\n    \"\"\"\"\"\"\n\n    if isinstance(target_size, int):\n        target_size = (target_size, target_size)\n\n    if not isinstance(target_size, (list, tuple, np.ndarray)):\n        message = (\n            \"`target_size` should be a single number (width) or a list\"\n            \"/tuple/ndarray (width, height), not {}.\".format(type(target_size))\n        )\n        raise ValueError(message)\n\n    rank = len(image.shape)\n    assert 3 <= rank <= 4\n\n    original_size = image.shape[-3:-1]\n\n    if original_size == target_size:\n        return image  # noop return because ndimage.zoom doesn't check itself\n\n    # TODO: maybe allow -1 in target_size to signify aspect-ratio preserving resize?\n    ratios = [t / o for t, o in zip(target_size, original_size)]\n    zoom = [1] * rank\n    zoom[-3:-1] = ratios\n\n    roughly_resized = ndimage.zoom(image, zoom, **kwargs)\n    return roughly_resized[..., : target_size[0], : target_size[1], :]", "output": "Resize an ndarray image of rank 3 or 4.\n    target_size can be a tuple `(width, height)` or scalar `width`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def readlines(filename, encoding='utf-8'):\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    text, encoding = read(filename, encoding)\r\n    return text.split(os.linesep), encoding", "output": "Read lines from file ('filename')\r\n    Return lines and encoding", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_reference(proxy, new_reference):\n        '''\n        \n        '''\n        # If the new reference is itself a proxy, we have to ensure that it does\n        # not refer to this proxy. If it does, we simply return because updating\n        # the reference would result in an inifite loop when trying to use the\n        # proxy.\n        possible_proxy = new_reference\n        while isinstance(possible_proxy, ThreadLocalProxy):\n            if possible_proxy is proxy:\n                return\n            possible_proxy = ThreadLocalProxy.get_reference(possible_proxy)\n        thread_local = object.__getattribute__(proxy, '_thread_local')\n        thread_local.reference = new_reference\n        object.__setattr__(proxy, '_last_reference', new_reference)", "output": "Set the reference to be used the current thread of execution.\n\n        After calling this function, the specified proxy will act like it was\n        the referenced object.\n\n        proxy:\n            proxy object for which the reference shall be set. If the specified\n            object is not an instance of `ThreadLocalProxy`, the behavior is\n            unspecified. Typically, an ``AttributeError`` is going to be\n            raised.\n\n        new_reference:\n            reference the proxy should point to for the current thread after\n            calling this function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def present(profile='pagerduty', subdomain=None, api_key=None, **kwargs):\n    '''\n    \n    '''\n    return __salt__['pagerduty_util.resource_present']('users',\n                                                       ['email', 'name', 'id'],\n                                                       None,\n                                                       profile,\n                                                       subdomain,\n                                                       api_key,\n                                                       **kwargs)", "output": "Ensure pagerduty user exists.\n    Arguments match those supported by\n    https://developer.pagerduty.com/documentation/rest/users/create.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gcs_dataset_info_files(dataset_dir):\n  \"\"\"\"\"\"\n  prefix = posixpath.join(GCS_DATASET_INFO_DIR, dataset_dir, \"\")\n  # Filter for this dataset\n  filenames = [el for el in gcs_files(prefix_filter=prefix)\n               if el.startswith(prefix) and len(el) > len(prefix)]\n  return filenames", "output": "Return paths to GCS files in the given dataset directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dict_to_string(dictionary):\n    '''\n    \n    '''\n    ret = ''\n    for key, val in sorted(dictionary.items()):\n        if isinstance(val, dict):\n            for line in _dict_to_string(val):\n                ret += six.text_type(key) + '-' + line + '\\n'\n        elif isinstance(val, list):\n            text = ' '.join([six.text_type(item) for item in val])\n            ret += six.text_type(key) + ': ' + text + '\\n'\n        else:\n            ret += six.text_type(key) + ': ' + six.text_type(val) + '\\n'\n    return ret.splitlines()", "output": "converts a dictionary object into a list of strings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_flatten(builder, layer, input_names, output_names, keras_layer):\n    \"\"\"\n    \"\"\"\n    input_name, output_name = (input_names[0], output_names[0])\n\n    # blob_order == 0 if the input blob needs not be rearranged\n    # blob_order == 1 if the input blob needs to be rearranged\n    blob_order = 0\n\n    # using keras_layer.input.shape have a \"?\" (Dimension[None] at the front),\n    # making a 3D tensor with unknown batch size 4D\n    if len(keras_layer.input.shape) == 4:\n        blob_order = 1\n\n    builder.add_flatten(name=layer, mode=blob_order, input_name=input_name, output_name=output_name)", "output": "Convert a flatten layer from keras to coreml.\n\n    Parameters\n    keras_layer: layer\n    ----------\n        A keras layer object.\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _extract_key_val(kv, delimiter='='):\n    '''\n    '''\n    pieces = kv.split(delimiter)\n    key = pieces[0]\n    val = delimiter.join(pieces[1:])\n    return key, val", "output": "Extract key and value from key=val string.\n\n    Example:\n    >>> _extract_key_val('foo=bar')\n    ('foo', 'bar')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def edit(self, **fields):\n        \"\"\"\n        \"\"\"\n\n        try:\n            content = fields['content']\n        except KeyError:\n            pass\n        else:\n            if content is not None:\n                fields['content'] = str(content)\n\n        try:\n            embed = fields['embed']\n        except KeyError:\n            pass\n        else:\n            if embed is not None:\n                fields['embed'] = embed.to_dict()\n\n        data = await self._state.http.edit_message(self.channel.id, self.id, **fields)\n        self._update(channel=self.channel, data=data)\n\n        try:\n            delete_after = fields['delete_after']\n        except KeyError:\n            pass\n        else:\n            if delete_after is not None:\n                await self.delete(delay=delete_after)", "output": "|coro|\n\n        Edits the message.\n\n        The content must be able to be transformed into a string via ``str(content)``.\n\n        Parameters\n        -----------\n        content: Optional[:class:`str`]\n            The new content to replace the message with.\n            Could be ``None`` to remove the content.\n        embed: Optional[:class:`Embed`]\n            The new embed to replace the original with.\n            Could be ``None`` to remove the embed.\n        delete_after: Optional[:class:`float`]\n            If provided, the number of seconds to wait in the background\n            before deleting the message we just edited. If the deletion fails,\n            then it is silently ignored.\n\n        Raises\n        -------\n        HTTPException\n            Editing the message failed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _fully_connected(self, x, out_dim):\n    \"\"\"\"\"\"\n    if self.init_layers:\n      fc = LinearnGPU(out_dim, w_name='DW')\n      fc.name = 'logits'\n      self.layers += [fc]\n    else:\n      fc = self.layers[self.layer_idx]\n      self.layer_idx += 1\n    fc.device_name = self.device_name\n    fc.set_training(self.training)\n    return fc.fprop(x)", "output": "FullyConnected layer for final output.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_delete(uid, channel=14, **kwargs):\n    '''\n    \n    '''\n    with _IpmiCommand(**kwargs) as c:\n        return c.user_delete(uid, channel)", "output": "Delete user (helper)\n\n    :param uid: user number [1:16]\n    :param channel: number [1:7]\n    :param kwargs:\n        - api_host=127.0.0.1\n        - api_user=admin\n        - api_pass=example\n        - api_port=623\n        - api_kg=None\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-call ipmi.user_delete uid=2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_run_configuration(fname):\r\n    \"\"\"\"\"\"\r\n    configurations = _get_run_configurations()\r\n    for filename, options in configurations:\r\n        if fname == filename:\r\n            runconf = RunConfiguration()\r\n            runconf.set(options)\r\n            return runconf", "output": "Return script *fname* run configuration", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def local_self_attention_layer(hparams, prefix):\n  \"\"\"\"\"\"\n  return transformer_layers.LocalSelfAttention(\n      num_heads=hparams.get(prefix + \"num_heads\"),\n      num_memory_heads=hparams.get(prefix + \"num_memory_heads\"),\n      radius=hparams.local_attention_radius,\n      key_value_size=hparams.d_kv,\n      shared_kv=hparams.get(prefix + \"shared_kv\", False),\n      attention_kwargs=attention_kwargs_from_hparams(hparams))", "output": "Create self-attention layer based on hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_settings(self):\r\n        \"\"\"\"\"\"\r\n        qapp = QApplication.instance()\r\n        # Set 'gtk+' as the default theme in Gtk-based desktops\r\n        # Fixes Issue 2036\r\n        if is_gtk_desktop() and ('GTK+' in QStyleFactory.keys()):\r\n            try:\r\n                qapp.setStyle('gtk+')\r\n            except:\r\n                pass\r\n        else:\r\n            style_name = CONF.get('appearance', 'windows_style',\r\n                                  self.default_style)\r\n            style = QStyleFactory.create(style_name)\r\n            if style is not None:\r\n                style.setProperty('name', style_name)\r\n                qapp.setStyle(style)\r\n\r\n        default = self.DOCKOPTIONS\r\n        if CONF.get('main', 'vertical_tabs'):\r\n            default = default|QMainWindow.VerticalTabs\r\n        if CONF.get('main', 'animated_docks'):\r\n            default = default|QMainWindow.AnimatedDocks\r\n        self.setDockOptions(default)\r\n\r\n        self.apply_panes_settings()\r\n        self.apply_statusbar_settings()\r\n\r\n        if CONF.get('main', 'use_custom_cursor_blinking'):\r\n            qapp.setCursorFlashTime(CONF.get('main', 'custom_cursor_blinking'))\r\n        else:\r\n            qapp.setCursorFlashTime(self.CURSORBLINK_OSDEFAULT)", "output": "Apply settings changed in 'Preferences' dialog box", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _service_is_sysv(name):\n    '''\n    \n    '''\n    try:\n        # Look for user-execute bit in file mode.\n        return bool(os.stat(\n            os.path.join('/etc/init.d', name)).st_mode & stat.S_IXUSR)\n    except OSError:\n        return False", "output": "Return True if the service is a System V service (includes those managed by\n    chkconfig); otherwise return False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_aliases_formatting(self, aliases):\n        \"\"\"\n        \"\"\"\n        self.paginator.add_line('**%s** %s' % (self.aliases_heading, ', '.join(aliases)), empty=True)", "output": "Adds the formatting information on a command's aliases.\n\n        The formatting should be added to the :attr:`paginator`.\n\n        The default implementation is the :attr:`aliases_heading` bolded\n        followed by a comma separated list of aliases.\n\n        This is not called if there are no aliases to format.\n\n        Parameters\n        -----------\n        aliases: Sequence[:class:`str`]\n            A list of aliases to format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _which_git_config(global_, cwd, user, password, output_encoding=None):\n    '''\n    \n    '''\n    if global_:\n        return ['--global']\n    version_ = _LooseVersion(version(versioninfo=False))\n    if version_ >= _LooseVersion('1.7.10.2'):\n        # --local added in 1.7.10.2\n        return ['--local']\n    else:\n        # For earlier versions, need to specify the path to the git config file\n        return ['--file', _git_config(cwd, user, password,\n                                      output_encoding=output_encoding)]", "output": "Based on whether global or local config is desired, return a list of CLI\n    args to include in the git config command.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def discriminator(self, frames):\n    \"\"\"\n    \"\"\"\n    ndf = self.hparams.num_discriminator_filters\n    frames = tf.stack(frames)\n\n    # Switch from time-major axis to batch-major axis.\n    frames = common_video.swap_time_and_batch_axes(frames)\n\n    # 3-D Conv-net mapping inputs to activations.\n    num_outputs = [ndf, ndf*2, ndf*2, ndf*4, ndf*4, ndf*8, ndf*8]\n    kernel_sizes = [3, 4, 3, 4, 3, 4, 3]\n    strides = [[1, 1, 1], [1, 2, 2], [1, 1, 1], [1, 2, 2], [1, 1, 1],\n               [2, 2, 2], [1, 1, 1]]\n\n    names = [\"video_sn_conv0_0\", \"video_sn_conv0_1\", \"video_sn_conv1_0\",\n             \"video_sn_conv1_1\", \"video_sn_conv2_0\", \"video_sn_conv2_1\",\n             \"video_sn_conv3_0\"]\n    iterable = zip(num_outputs, kernel_sizes, strides, names)\n    activations = frames\n    for num_filters, kernel_size, stride, name in iterable:\n      activations = self.pad_conv3d_lrelu(activations, num_filters, kernel_size,\n                                          stride, name)\n    num_fc_dimensions = self.get_fc_dimensions(strides, kernel_sizes)\n    activations = tf.reshape(activations, (-1, num_fc_dimensions))\n    return tf.squeeze(tf.layers.dense(activations, 1))", "output": "3-D SNGAN discriminator.\n\n    Args:\n      frames: a list of batch-major tensors indexed by time.\n\n    Returns:\n      logits: 1-D Tensor with shape=batch_size.\n              Positive logits imply that the discriminator thinks that it\n              belongs to the true class.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_sentence(sentence_blob: str) -> Tuple[List[Dict[str, str]], List[Tuple[int, int]], List[str]]:\n    \"\"\"\n    \n    \"\"\"\n    annotated_sentence = []\n    arc_indices = []\n    arc_tags = []\n    predicates = []\n\n    lines = [line.split(\"\\t\") for line in sentence_blob.split(\"\\n\")\n             if line and not line.strip().startswith(\"#\")]\n    for line_idx, line in enumerate(lines):\n        annotated_token = {k:v for k, v in zip(FIELDS, line)}\n        if annotated_token['pred'] == \"+\":\n            predicates.append(line_idx)\n        annotated_sentence.append(annotated_token)\n\n    for line_idx, line in enumerate(lines):\n        for predicate_idx, arg in enumerate(line[len(FIELDS):]):\n            if arg != \"_\":\n                arc_indices.append((line_idx, predicates[predicate_idx]))\n                arc_tags.append(arg)\n    return annotated_sentence, arc_indices, arc_tags", "output": "Parses a chunk of text in the SemEval SDP format.\n\n    Each word in the sentence is returned as a dictionary with the following\n    format:\n    'id': '1',\n    'form': 'Pierre',\n    'lemma': 'Pierre',\n    'pos': 'NNP',\n    'head': '2',   # Note that this is the `syntactic` head.\n    'deprel': 'nn',\n    'top': '-',\n    'pred': '+',\n    'frame': 'named:x-c'\n\n    Along with a list of arcs and their corresponding tags. Note that\n    in semantic dependency parsing words can have more than one head\n    (it is not a tree), meaning that the list of arcs and tags are\n    not tied to the length of the sentence.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _format_multirow(self, row, ilevels, i, rows):\n        \n        \"\"\"\n        for j in range(ilevels):\n            if row[j].strip():\n                nrow = 1\n                for r in rows[i + 1:]:\n                    if not r[j].strip():\n                        nrow += 1\n                    else:\n                        break\n                if nrow > 1:\n                    # overwrite non-multirow entry\n                    row[j] = '\\\\multirow{{{nrow:d}}}{{*}}{{{row:s}}}'.format(\n                        nrow=nrow, row=row[j].strip())\n                    # save when to end the current block with \\cline\n                    self.clinebuf.append([i + nrow - 1, j + 1])\n        return row", "output": "r\"\"\"\n        Check following rows, whether row should be a multirow\n\n        e.g.:     becomes:\n        a & 0 &   \\multirow{2}{*}{a} & 0 &\n          & 1 &     & 1 &\n        b & 0 &   \\cline{1-2}\n                  b & 0 &", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_fn_link(ft)->str:\n    \"\"\n    ft = getattr(ft, '__func__', ft)\n    anchor = strip_fastai(get_anchor(ft))\n    module_name = strip_fastai(get_module_name(ft))\n    base = '' if use_relative_links else FASTAI_DOCS\n    return f'{base}/{module_name}.html#{anchor}'", "output": "Return function link to notebook documentation of `ft`. Private functions link to source code", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, pos):\n    \"\"\"\n    \"\"\"\n    if pos <= 0:\n      raise ValueError('Invalid pos %d: pos must be > 0' % pos)\n    with self._outgoing_lock:\n      if self._outgoing_counter >= pos:\n        # If the stack already has the requested position, return the value\n        # immediately.\n        return self._outgoing[pos - 1], self._outgoing_counter\n      else:\n        # If the stack has not reached the requested position yet, create a\n        # queue and block on get().\n        if pos not in self._outgoing_pending_queues:\n          self._outgoing_pending_queues[pos] = []\n        q = queue.Queue(maxsize=1)\n        self._outgoing_pending_queues[pos].append(q)\n\n    value = q.get()\n    with self._outgoing_lock:\n      return value, self._outgoing_counter", "output": "Get message(s) from the outgoing message stack.\n\n    Blocks until an item at stack position pos becomes available.\n    This method is thread safe.\n\n    Args:\n       pos: An int specifying the top position of the message stack to access.\n         For example, if the stack counter is at 3 and pos == 2, then the 2nd\n         item on the stack will be returned, together with an int that indicates\n         the current stack heigh (3 in this case).\n\n    Returns:\n      1. The item at stack position pos.\n      2. The height of the stack when the retun values are generated.\n\n    Raises:\n      ValueError: If input `pos` is zero or negative.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_recent_files(self):\r\n        \"\"\"\"\"\"\r\n        try:\r\n            recent_files = self.CONF[WORKSPACE].get('main', 'recent_files',\r\n                                                    default=[])\r\n        except EnvironmentError:\r\n            return []\r\n\r\n        for recent_file in recent_files[:]:\r\n            if not os.path.isfile(recent_file):\r\n                recent_files.remove(recent_file)\r\n        return list(OrderedDict.fromkeys(recent_files))", "output": "Return a list of files opened by the project.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def GetVersion():\n  \"\"\"\"\"\"\n\n  with open(os.path.join('google', 'protobuf', '__init__.py')) as version_file:\n    exec(version_file.read(), globals())\n    return __version__", "output": "Gets the version from google/protobuf/__init__.py\n\n  Do not import google.protobuf.__init__ directly, because an installed\n  protobuf library may be loaded instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flick_element(self, on_element, xoffset, yoffset, speed):\n        \"\"\"\n        \n        \"\"\"\n        self._actions.append(lambda: self._driver.execute(\n            Command.FLICK, {\n                'element': on_element.id,\n                'xoffset': int(xoffset),\n                'yoffset': int(yoffset),\n                'speed': int(speed)}))\n        return self", "output": "Flick starting at on_element, and moving by the xoffset and yoffset\n        with specified speed.\n\n        :Args:\n         - on_element: Flick will start at center of element.\n         - xoffset: X offset to flick to.\n         - yoffset: Y offset to flick to.\n         - speed: Pixels per second to flick.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_postaggs_for(postagg_names, metrics_dict):\n        \"\"\"\"\"\"\n        postagg_metrics = [\n            metrics_dict[name] for name in postagg_names\n            if metrics_dict[name].metric_type == POST_AGG_TYPE\n        ]\n        # Remove post aggregations that were found\n        for postagg in postagg_metrics:\n            postagg_names.remove(postagg.metric_name)\n        return postagg_metrics", "output": "Return a list of metrics that are post aggregations", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge_dicts(d1, d2):\n    \"\"\"\"\"\"\n    merged = copy.deepcopy(d1)\n    deep_update(merged, d2, True, [])\n    return merged", "output": "Returns a new dict that is d1 and d2 deep merged.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_queue(queue, kwargs):\n    '''\n    \n    '''\n    if queue:\n        _wait(kwargs.get('__pub_jid'))\n    else:\n        conflict = running(concurrent=kwargs.get('concurrent', False))\n        if conflict:\n            __context__['retcode'] = salt.defaults.exitcodes.EX_STATE_COMPILER_ERROR\n            return conflict", "output": "Utility function to queue the state run if requested\n    and to check for conflicts in currently running states", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_history(self):\r\n        \"\"\"\"\"\"\r\n        open(self.LOG_PATH, 'w').write(\"\\n\".join( \\\r\n                [to_text_string(self.pydocbrowser.url_combo.itemText(index))\r\n                 for index in range(self.pydocbrowser.url_combo.count())]))", "output": "Save history to a text file in user home directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_image(image_id, profile, **libcloud_kwargs):\n    '''\n    \n    '''\n    conn = _get_driver(profile=profile)\n    libcloud_kwargs = salt.utils.args.clean_kwargs(**libcloud_kwargs)\n    image = conn.get_image(image_id, **libcloud_kwargs)\n    return _simple_image(image)", "output": "Get an image of a node\n\n    :param image_id: Image to fetch\n    :type image_id: ``str``\n\n    :param profile: The profile key\n    :type  profile: ``str``\n\n    :param libcloud_kwargs: Extra arguments for the driver's delete_image method\n    :type  libcloud_kwargs: ``dict``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion libcloud_compute.get_image image1 profile1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_labels(self, func):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(self.data, LabelArray):\n            raise TypeError(\n                'update_labels only supported if data is of type LabelArray.'\n            )\n\n        # Map the baseline values.\n        self._data = self._data.map(func)\n\n        # Map each of the adjustments.\n        for _, row_adjustments in iteritems(self.adjustments):\n            for adjustment in row_adjustments:\n                adjustment.value = func(adjustment.value)", "output": "Map a function over baseline and adjustment values in place.\n\n        Note that the baseline data values must be a LabelArray.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def data_orientation(self):\n        \"\"\"\"\"\"\n        return tuple(itertools.chain([int(a[0]) for a in self.non_index_axes],\n                                     [int(a.axis) for a in self.index_axes]))", "output": "return a tuple of my permutated axes, non_indexable at the front", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cidr_broadcast(cidr):\n    '''\n    \n    '''\n    ips = netaddr.IPNetwork(cidr)\n    return six.text_type(ips.broadcast)", "output": "Get the broadcast address associated with a CIDR address.\n\n    CLI example::\n\n        salt myminion netaddress.cidr_netmask 192.168.0.0/20", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _pretty_print_event(event, colored):\n        \"\"\"\n        \n        \"\"\"\n        event.timestamp = colored.yellow(event.timestamp)\n        event.log_stream_name = colored.cyan(event.log_stream_name)\n\n        return ' '.join([event.log_stream_name, event.timestamp, event.message])", "output": "Basic formatter to convert an event object to string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_sizes(conn=None):\n    '''\n    \n    '''\n    if not conn:\n        conn = get_conn()\n    raw_sizes = conn.list_sizes('all')  # get *all* the machine types!\n    sizes = []\n    for size in raw_sizes:\n        zone = size.extra['zone']\n        size.extra['zone'] = {}\n        size.extra['zone'].update(zone.__dict__)\n        mtype = {}\n        mtype.update(size.__dict__)\n        sizes.append(mtype)\n    return sizes", "output": "Return a dict of available instances sizes (a.k.a machine types) and\n    convert them to something more serializable.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def items_lower(self):\n        '''\n        \n        '''\n        return ((key, val[1]) for key, val in six.iteritems(self._data))", "output": "Returns a generator iterating over keys and values, with the keys all\n        being lowercase.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reload(self):\n        \"\"\"\n        \"\"\"\n        instance_pb = self._client.instance_admin_client.get_instance(self.name)\n\n        # NOTE: _update_from_pb does not check that the project and\n        #       instance ID on the response match the request.\n        self._update_from_pb(instance_pb)", "output": "Reload the metadata for this instance.\n\n        For example:\n\n        .. literalinclude:: snippets.py\n            :start-after: [START bigtable_reload_instance]\n            :end-before: [END bigtable_reload_instance]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wrap(self, stream, name=None, filename=None):\n        \"\"\"\n        \"\"\"\n        for lineno, token, value in stream:\n            if token in ignored_tokens:\n                continue\n            elif token == 'linestatement_begin':\n                token = 'block_begin'\n            elif token == 'linestatement_end':\n                token = 'block_end'\n            # we are not interested in those tokens in the parser\n            elif token in ('raw_begin', 'raw_end'):\n                continue\n            elif token == 'data':\n                value = self._normalize_newlines(value)\n            elif token == 'keyword':\n                token = value\n            elif token == 'name':\n                value = str(value)\n                if check_ident and not value.isidentifier():\n                    raise TemplateSyntaxError(\n                        'Invalid character in identifier',\n                        lineno, name, filename)\n            elif token == 'string':\n                # try to unescape string\n                try:\n                    value = self._normalize_newlines(value[1:-1]) \\\n                        .encode('ascii', 'backslashreplace') \\\n                        .decode('unicode-escape')\n                except Exception as e:\n                    msg = str(e).split(':')[-1].strip()\n                    raise TemplateSyntaxError(msg, lineno, name, filename)\n            elif token == 'integer':\n                value = int(value)\n            elif token == 'float':\n                value = float(value)\n            elif token == 'operator':\n                token = operators[value]\n            yield Token(lineno, token, value)", "output": "This is called with the stream as returned by `tokenize` and wraps\n        every token in a :class:`Token` and converts the value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_plugin(self):\r\n        \"\"\"\"\"\"\r\n        self.focus_changed.connect(self.main.plugin_focus_changed)\r\n        self.main.add_dockwidget(self)\r\n#        self.main.console.set_historylog(self)\r\n        self.main.console.shell.refresh.connect(self.refresh_plugin)", "output": "Register plugin in Spyder's main window", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _remove_layer(self, layer):\n        \"\"\"\n        \n        \"\"\"\n        successors = self.get_successors(layer)\n        predecessors = self.get_predecessors(layer)\n        # remove all edges\n        for succ in successors:\n            self._remove_edge(layer, succ)\n        for pred in predecessors:\n            self._remove_edge(pred, layer)\n        # remove layer in the data structures\n        self.keras_layer_map.pop(layer)\n        self.layer_list.remove(layer)", "output": "remove the layer and its input/output edges", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_plain_text(self, text, is_code):\r\n        \"\"\"\"\"\"\r\n\r\n        # text is coming from utils.dochelpers.getdoc\r\n        if type(text) is dict:\r\n            name = text['name']\r\n            if name:\r\n                rst_title = ''.join(['='*len(name), '\\n', name, '\\n',\r\n                                    '='*len(name), '\\n\\n'])\r\n            else:\r\n                rst_title = ''\r\n\r\n            if text['argspec']:\r\n                definition = ''.join(['Definition: ', name, text['argspec'],\r\n                                      '\\n'])\r\n            else:\r\n                definition = ''\r\n\r\n            if text['note']:\r\n                note = ''.join(['Type: ', text['note'], '\\n\\n----\\n\\n'])\r\n            else:\r\n                note = ''\r\n\r\n            full_text = ''.join([rst_title, definition, note,\r\n                                 text['docstring']])\r\n        else:\r\n            full_text = text\r\n\r\n        self.plain_text.set_text(full_text, is_code)\r\n        self.save_text([self.plain_text.set_text, full_text, is_code])", "output": "Set plain text docs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, name, hint):\n        \"\"\"\n        \"\"\"\n        if name:\n            return name\n        if hint not in self._counter:\n            self._counter[hint] = 0\n        name = '%s%d' % (hint, self._counter[hint])\n        self._counter[hint] += 1\n        return name", "output": "Get the canonical name for a symbol.\n\n        This is the default implementation.\n        If the user specifies a name,\n        the user-specified name will be used.\n\n        When user does not specify a name, we automatically generate a\n        name based on the hint string.\n\n        Parameters\n        ----------\n        name : str or None\n            The name specified by the user.\n\n        hint : str\n            A hint string, which can be used to generate name.\n\n        Returns\n        -------\n        full_name : str\n            A canonical name for the symbol.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def takeSample(self, withReplacement, num, seed=None):\n        \"\"\"\n        \n        \"\"\"\n        numStDev = 10.0\n\n        if num < 0:\n            raise ValueError(\"Sample size cannot be negative.\")\n        elif num == 0:\n            return []\n\n        initialCount = self.count()\n        if initialCount == 0:\n            return []\n\n        rand = random.Random(seed)\n\n        if (not withReplacement) and num >= initialCount:\n            # shuffle current RDD and return\n            samples = self.collect()\n            rand.shuffle(samples)\n            return samples\n\n        maxSampleSize = sys.maxsize - int(numStDev * sqrt(sys.maxsize))\n        if num > maxSampleSize:\n            raise ValueError(\n                \"Sample size cannot be greater than %d.\" % maxSampleSize)\n\n        fraction = RDD._computeFractionForSampleSize(\n            num, initialCount, withReplacement)\n        samples = self.sample(withReplacement, fraction, seed).collect()\n\n        # If the first sample didn't turn out large enough, keep trying to take samples;\n        # this shouldn't happen often because we use a big multiplier for their initial size.\n        # See: scala/spark/RDD.scala\n        while len(samples) < num:\n            # TODO: add log warning for when more than one iteration was run\n            seed = rand.randint(0, sys.maxsize)\n            samples = self.sample(withReplacement, fraction, seed).collect()\n\n        rand.shuffle(samples)\n\n        return samples[0:num]", "output": "Return a fixed-size sampled subset of this RDD.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        >>> rdd = sc.parallelize(range(0, 10))\n        >>> len(rdd.takeSample(True, 20, 1))\n        20\n        >>> len(rdd.takeSample(False, 5, 2))\n        5\n        >>> len(rdd.takeSample(False, 15, 3))\n        10", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zeroize():\n    '''\n    \n    '''\n    conn = __proxy__['junos.conn']()\n    ret = {}\n    ret['out'] = True\n    try:\n        conn.cli('request system zeroize')\n        ret['message'] = 'Completed zeroize and rebooted'\n    except Exception as exception:\n        ret['message'] = 'Could not zeroize due to : \"{0}\"'.format(exception)\n        ret['out'] = False\n\n    return ret", "output": "Resets the device to default factory settings\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'device_name' junos.zeroize", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ctypes2numpy(cptr, length, dtype):\n    \"\"\"\n    \"\"\"\n    NUMPY_TO_CTYPES_MAPPING = {\n        np.float32: ctypes.c_float,\n        np.uint32: ctypes.c_uint,\n    }\n    if dtype not in NUMPY_TO_CTYPES_MAPPING:\n        raise RuntimeError('Supported types: {}'.format(NUMPY_TO_CTYPES_MAPPING.keys()))\n    ctype = NUMPY_TO_CTYPES_MAPPING[dtype]\n    if not isinstance(cptr, ctypes.POINTER(ctype)):\n        raise RuntimeError('expected {} pointer'.format(ctype))\n    res = np.zeros(length, dtype=dtype)\n    if not ctypes.memmove(res.ctypes.data, cptr, length * res.strides[0]):\n        raise RuntimeError('memmove failed')\n    return res", "output": "Convert a ctypes pointer array to a numpy array.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_query(query_id, session, retry_count=5):\n    \"\"\"\"\"\"\n    query = None\n    attempt = 0\n    while not query and attempt < retry_count:\n        try:\n            query = session.query(Query).filter_by(id=query_id).one()\n        except Exception:\n            attempt += 1\n            logging.error(\n                'Query with id `{}` could not be retrieved'.format(query_id))\n            stats_logger.incr('error_attempting_orm_query_' + str(attempt))\n            logging.error('Sleeping for a sec before retrying...')\n            sleep(1)\n    if not query:\n        stats_logger.incr('error_failed_at_getting_orm_query')\n        raise SqlLabException('Failed at getting query')\n    return query", "output": "attemps to get the query and retry if it cannot", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def SetParserProp(self, prop, value):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlTextReaderSetParserProp(self._o, prop, value)\n        return ret", "output": "Change the parser processing behaviour by changing some of\n          its internal properties. Note that some properties can only\n           be changed before any read has been done.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dot(self, other):\n        \"\"\"\n        \n        \"\"\"\n        if type(other) == np.ndarray:\n            if other.ndim > 1:\n                assert len(self) == other.shape[0], \"dimension mismatch\"\n            return np.dot(self.array, other)\n        elif _have_scipy and scipy.sparse.issparse(other):\n            assert len(self) == other.shape[0], \"dimension mismatch\"\n            return other.transpose().dot(self.toArray())\n        else:\n            assert len(self) == _vector_size(other), \"dimension mismatch\"\n            if isinstance(other, SparseVector):\n                return other.dot(self)\n            elif isinstance(other, Vector):\n                return np.dot(self.toArray(), other.toArray())\n            else:\n                return np.dot(self.toArray(), other)", "output": "Compute the dot product of two Vectors. We support\n        (Numpy array, list, SparseVector, or SciPy sparse)\n        and a target NumPy array that is either 1- or 2-dimensional.\n        Equivalent to calling numpy.dot of the two vectors.\n\n        >>> dense = DenseVector(array.array('d', [1., 2.]))\n        >>> dense.dot(dense)\n        5.0\n        >>> dense.dot(SparseVector(2, [0, 1], [2., 1.]))\n        4.0\n        >>> dense.dot(range(1, 3))\n        5.0\n        >>> dense.dot(np.array(range(1, 3)))\n        5.0\n        >>> dense.dot([1.,])\n        Traceback (most recent call last):\n            ...\n        AssertionError: dimension mismatch\n        >>> dense.dot(np.reshape([1., 2., 3., 4.], (2, 2), order='F'))\n        array([  5.,  11.])\n        >>> dense.dot(np.reshape([1., 2., 3.], (3, 1), order='F'))\n        Traceback (most recent call last):\n            ...\n        AssertionError: dimension mismatch", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_jobs(self, id=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_rollup\", \"job\", id), params=params\n        )", "output": "`<>`_\n\n        :arg id: The ID of the job(s) to fetch. Accepts glob patterns, or left\n            blank for all jobs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetcher(ctx, xmlrpc, xmlrpc_host, xmlrpc_port, poolsize, proxy, user_agent,\n            timeout, phantomjs_endpoint, puppeteer_endpoint, splash_endpoint, fetcher_cls,\n            async_mode=True, get_object=False, no_input=False):\n    \"\"\"\n    \n    \"\"\"\n    g = ctx.obj\n    Fetcher = load_cls(None, None, fetcher_cls)\n\n    if no_input:\n        inqueue = None\n        outqueue = None\n    else:\n        inqueue = g.scheduler2fetcher\n        outqueue = g.fetcher2processor\n    fetcher = Fetcher(inqueue=inqueue, outqueue=outqueue,\n                      poolsize=poolsize, proxy=proxy, async_mode=async_mode)\n    fetcher.phantomjs_proxy = phantomjs_endpoint or g.phantomjs_proxy\n    fetcher.puppeteer_proxy = puppeteer_endpoint or g.puppeteer_proxy\n    fetcher.splash_endpoint = splash_endpoint\n    if user_agent:\n        fetcher.user_agent = user_agent\n    if timeout:\n        fetcher.default_options = copy.deepcopy(fetcher.default_options)\n        fetcher.default_options['timeout'] = timeout\n\n    g.instances.append(fetcher)\n    if g.get('testing_mode') or get_object:\n        return fetcher\n\n    if xmlrpc:\n        utils.run_in_thread(fetcher.xmlrpc_run, port=xmlrpc_port, bind=xmlrpc_host)\n    fetcher.run()", "output": "Run Fetcher.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_value(self, key, value):\n        # type: (str, Any) -> None\n        \"\"\"\n        \"\"\"\n        self._ensure_have_load_only()\n\n        fname, parser = self._get_parser_to_modify()\n\n        if parser is not None:\n            section, name = _disassemble_key(key)\n\n            # Modify the parser and the configuration\n            if not parser.has_section(section):\n                parser.add_section(section)\n            parser.set(section, name, value)\n\n        self._config[self.load_only][key] = value\n        self._mark_as_modified(fname, parser)", "output": "Modify a value in the configuration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_financial_report_adv(code, start, end=None, ltype='EN'):\n    \"\"\"\n    \"\"\"\n\n    if end is None:\n\n        return QA_DataStruct_Financial(QA_fetch_financial_report(code, start, ltype=ltype))\n    else:\n        series = pd.Series(\n            data=month_data, index=pd.to_datetime(month_data), name='date')\n        timerange = series.loc[start:end].tolist()\n        return QA_DataStruct_Financial(QA_fetch_financial_report(code, timerange, ltype=ltype))", "output": "\u9ad8\u7ea7\u8d22\u52a1\u67e5\u8be2\u63a5\u53e3\n    Arguments:\n        code {[type]} -- [description]\n        start {[type]} -- [description]\n    Keyword Arguments:\n        end {[type]} -- [description] (default: {None})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_files(metadata):\n    '''\n    \n    '''\n\n    ret = []\n    found = {}\n\n    for bucket_dict in metadata:\n        for bucket_name, data in six.iteritems(bucket_dict):\n            filepaths = [k['Key'] for k in data]\n            filepaths = [k for k in filepaths if not k.endswith('/')]\n            if bucket_name not in found:\n                found[bucket_name] = True\n                ret.append({bucket_name: filepaths})\n            else:\n                for bucket in ret:\n                    if bucket_name in bucket:\n                        bucket[bucket_name] += filepaths\n                        break\n    return ret", "output": "Looks for all the files in the S3 bucket cache metadata", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def above(self, objects: Set[Object]) -> Set[Object]:\n        \"\"\"\n        \n        \"\"\"\n        objects_per_box = self._separate_objects_by_boxes(objects)\n        return_set = set()\n        for box in objects_per_box:\n            # min_y_loc corresponds to the top-most object.\n            min_y_loc = min([obj.y_loc for obj in objects_per_box[box]])\n            for candidate_obj in box.objects:\n                if candidate_obj.y_loc < min_y_loc:\n                    return_set.add(candidate_obj)\n        return return_set", "output": "Returns the set of objects in the same boxes that are above the given objects. That is, if\n        the input is a set of two objects, one in each box, we will return a union of the objects\n        above the first object in the first box, and those above the second object in the second box.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write(self)->None:\n        \"\"\n        orig_images, gen_images, real_images = self._get_image_tensors()\n        self._write_images(name='orig images', images=orig_images)\n        self._write_images(name='gen images',  images=gen_images)\n        self._write_images(name='real images', images=real_images)", "output": "Writes original, generated and real(target) images to Tensorboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_output_compare(self, key, left, right):\n        \"\"\"\"\"\"\n        if isinstance(left, six.string_types):\n            left = _trim_base64(left)\n        if isinstance(right, six.string_types):\n            right = _trim_base64(right)\n\n        cc = self.colors\n\n        self.comparison_traceback.append(\n            cc.OKBLUE\n            + \" mismatch '%s'\" % key\n            + cc.FAIL)\n\n        # Use comparison repr from pytest:\n        hook_result = self.ihook.pytest_assertrepr_compare(\n            config=self.config, op='==', left=left, right=right)\n        for new_expl in hook_result:\n            if new_expl:\n                new_expl = ['  %s' % line.replace(\"\\n\", \"\\\\n\") for line in new_expl]\n                self.comparison_traceback.append(\"\\n assert reference_output == test_output failed:\\n\")\n                self.comparison_traceback.extend(new_expl)\n                break\n        else:\n            # Fallback repr:\n            self.comparison_traceback.append(\n                \"  <<<<<<<<<<<< Reference output from ipynb file:\"\n                + cc.ENDC)\n            self.comparison_traceback.append(_indent(left))\n            self.comparison_traceback.append(\n                cc.FAIL\n                + '  ============ disagrees with newly computed (test) output:'\n                + cc.ENDC)\n            self.comparison_traceback.append(_indent(right))\n            self.comparison_traceback.append(\n                cc.FAIL\n                + '  >>>>>>>>>>>>')\n        self.comparison_traceback.append(cc.ENDC)", "output": "Format an output for printing", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_write_gz(self):\n        \"\"\"\n        \"\"\"\n        self.cmp = self.zlib.compressobj(9, self.zlib.DEFLATED,\n                                            -self.zlib.MAX_WBITS,\n                                            self.zlib.DEF_MEM_LEVEL,\n                                            0)\n        timestamp = struct.pack(\"<L\", int(time.time()))\n        self.__write(b\"\\037\\213\\010\\010\" + timestamp + b\"\\002\\377\")\n        if self.name.endswith(\".gz\"):\n            self.name = self.name[:-3]\n        # RFC1952 says we must use ISO-8859-1 for the FNAME field.\n        self.__write(self.name.encode(\"iso-8859-1\", \"replace\") + NUL)", "output": "Initialize for writing with gzip compression.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def PlaceUOffsetT(self, x):\n        \"\"\"\n        \"\"\"\n        N.enforce_number(x, N.UOffsetTFlags)\n        self.head = self.head - N.UOffsetTFlags.bytewidth\n        encode.Write(packer.uoffset, self.Bytes, self.Head(), x)", "output": "PlaceUOffsetT prepends a UOffsetT to the Builder, without checking\n        for space.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_function(self, function_name, codeuri, runtime):\n        \"\"\"\n        \n        \"\"\"\n\n        # Create the arguments to pass to the builder\n        # Code is always relative to the given base directory.\n        code_dir = str(pathlib.Path(self._base_dir, codeuri).resolve())\n\n        config = get_workflow_config(runtime, code_dir, self._base_dir)\n\n        # artifacts directory will be created by the builder\n        artifacts_dir = str(pathlib.Path(self._build_dir, function_name))\n\n        with osutils.mkdir_temp() as scratch_dir:\n            manifest_path = self._manifest_path_override or os.path.join(code_dir, config.manifest_name)\n\n            # By default prefer to build in-process for speed\n            build_method = self._build_function_in_process\n            if self._container_manager:\n                build_method = self._build_function_on_container\n\n            return build_method(config,\n                                code_dir,\n                                artifacts_dir,\n                                scratch_dir,\n                                manifest_path,\n                                runtime)", "output": "Given the function information, this method will build the Lambda function. Depending on the configuration\n        it will either build the function in process or by spinning up a Docker container.\n\n        Parameters\n        ----------\n        function_name : str\n            Name or LogicalId of the function\n\n        codeuri : str\n            Path to where the code lives\n\n        runtime : str\n            AWS Lambda function runtime\n\n        Returns\n        -------\n        str\n            Path to the location where built artifacts are available", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_dynamic(module):\n    \"\"\"\n    \n    \"\"\"\n    # Quick check: module that have __file__ attribute are not dynamic modules.\n    if hasattr(module, '__file__'):\n        return False\n\n    if hasattr(module, '__spec__'):\n        return module.__spec__ is None\n    else:\n        # Backward compat for Python 2\n        import imp\n        try:\n            path = None\n            for part in module.__name__.split('.'):\n                if path is not None:\n                    path = [path]\n                f, path, description = imp.find_module(part, path)\n                if f is not None:\n                    f.close()\n        except ImportError:\n            return True\n        return False", "output": "Return True if the module is special module that cannot be imported by its\n    name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _prelu(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    new_attrs = translation_utils._add_extra_attributes(attrs, {'act_type': 'prelu'})\n    return 'LeakyReLU', new_attrs, inputs", "output": "PRelu function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nlargest(n, iterable, key=None):\n    \"\"\"\n    \"\"\"\n\n    # Short-cut for n==1 is to use max()\n    if n == 1:\n        it = iter(iterable)\n        sentinel = object()\n        if key is None:\n            result = max(it, default=sentinel)\n        else:\n            result = max(it, default=sentinel, key=key)\n        return [] if result is sentinel else [result]\n\n    # When n>=size, it's faster to use sorted()\n    try:\n        size = len(iterable)\n    except (TypeError, AttributeError):\n        pass\n    else:\n        if n >= size:\n            return sorted(iterable, key=key, reverse=True)[:n]\n\n    # When key is none, use simpler decoration\n    if key is None:\n        it = iter(iterable)\n        result = [(elem, i) for i, elem in zip(range(0, -n, -1), it)]\n        if not result:\n            return result\n        heapify(result)\n        top = result[0][0]\n        order = -n\n        _heapreplace = heapreplace\n        for elem in it:\n            if top < elem:\n                _heapreplace(result, (elem, order))\n                top = result[0][0]\n                order -= 1\n        result.sort(reverse=True)\n        return [r[0] for r in result]\n\n    # General case, slowest method\n    it = iter(iterable)\n    result = [(key(elem), i, elem) for i, elem in zip(range(0, -n, -1), it)]\n    if not result:\n        return result\n    heapify(result)\n    top = result[0][0]\n    order = -n\n    _heapreplace = heapreplace\n    for elem in it:\n        k = key(elem)\n        if top < k:\n            _heapreplace(result, (k, order, elem))\n            top = result[0][0]\n            order -= 1\n    result.sort(reverse=True)\n    return [r[2] for r in result]", "output": "Find the n largest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key, reverse=True)[:n]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_version(self, subject, avro_schema):\n        \"\"\"\n        \n        \"\"\"\n        schemas_to_version = self.subject_to_schema_versions[subject]\n        version = schemas_to_version.get(avro_schema, None)\n        if version is not None:\n            return version\n\n        url = '/'.join([self.url, 'subjects', subject])\n        body = {'schema': json.dumps(avro_schema.to_json())}\n\n        result, code = self._send_request(url, method='POST', body=body)\n        if code == 404:\n            log.error(\"Not found:\" + str(code))\n            return None\n        elif not (code >= 200 and code <= 299):\n            log.error(\"Unable to get version of a schema:\" + str(code))\n            return None\n        schema_id = result['id']\n        version = result['version']\n        self._cache_schema(avro_schema, schema_id, subject, version)\n        return version", "output": "POST /subjects/(string: subject)\n\n        Get the version of a schema for a given subject.\n\n        Returns None if not found.\n        :param str subject: subject name\n        :param: schema avro_schema: Avro schema\n        :returns: version\n        :rtype: int", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_avail(cmd):\n    '''\n    \n    '''\n    if isinstance(cmd, list):\n        cmd = ' '.join([six.text_type(x) if not isinstance(x, six.string_types) else x\n                        for x in cmd])\n    bret = True\n    wret = False\n    if __salt__['config.get']('cmd_blacklist_glob'):\n        blist = __salt__['config.get']('cmd_blacklist_glob', [])\n        for comp in blist:\n            if fnmatch.fnmatch(cmd, comp):\n                # BAD! you are blacklisted\n                bret = False\n    if __salt__['config.get']('cmd_whitelist_glob', []):\n        blist = __salt__['config.get']('cmd_whitelist_glob', [])\n        for comp in blist:\n            if fnmatch.fnmatch(cmd, comp):\n                # GOOD! You are whitelisted\n                wret = True\n                break\n    else:\n        # If no whitelist set then alls good!\n        wret = True\n    return bret and wret", "output": "Check to see if the given command can be run", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def domain_update(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    if 'new_name' in kwargs:\n        kwargs['name'] = kwargs.pop('new_name')\n    return cloud.update_domain(**kwargs)", "output": "Update a domain\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystoneng.domain_update name=domain1 new_name=newdomain\n        salt '*' keystoneng.domain_update name=domain1 enabled=True description='new description'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def route_delete(name, route_table, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    result = False\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        route = netconn.routes.delete(\n            resource_group_name=resource_group,\n            route_table_name=route_table,\n            route_name=name\n        )\n        route.wait()\n        result = True\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Delete a route from a route table.\n\n    :param name: The route to delete.\n\n    :param route_table: The route table containing the route.\n\n    :param resource_group: The resource group name assigned to the\n        route table.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.route_delete test-rt test-rt-table testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def datasets_view(self, owner_slug, dataset_slug, **kwargs):  # noqa: E501\n        \"\"\"\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.datasets_view_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501\n        else:\n            (data) = self.datasets_view_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501\n            return data", "output": "Show details about a dataset  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.datasets_view(owner_slug, dataset_slug, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str owner_slug: Dataset owner (required)\n        :param str dataset_slug: Dataset name (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None):\n        \"\"\"\n        \n        \"\"\"\n        jconf = self.ctx._dictToJavaMap(conf)\n        pickledRDD = self._pickled()\n        self.ctx._jvm.PythonRDD.saveAsHadoopDataset(pickledRDD._jrdd, True, jconf,\n                                                    keyConverter, valueConverter, True)", "output": "Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n        system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n        converted for output using either user specified converters or, by default,\n        L{org.apache.spark.api.python.JavaToWritableConverter}.\n\n        :param conf: Hadoop job configuration, passed in as a dict\n        :param keyConverter: (None by default)\n        :param valueConverter: (None by default)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_native_types(self, slicer=None, na_rep='nan', quoting=None,\n                        **kwargs):\n        \"\"\"  \"\"\"\n\n        values = self.get_values()\n\n        if slicer is not None:\n            values = values[:, slicer]\n        mask = isna(values)\n\n        if not self.is_object and not quoting:\n            values = values.astype(str)\n        else:\n            values = np.array(values, dtype='object')\n\n        values[mask] = na_rep\n        return values", "output": "convert to our native types format, slicing if desired", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(self, term):\n        \"\"\"\"\"\"\n        self._value = self.accum_param.addInPlace(self._value, term)", "output": "Adds a term to this accumulator's value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getUserSid(username):\n    '''\n    \n    '''\n    if six.PY2:\n        username = _to_unicode(username)\n\n    domain = win32api.GetComputerName()\n    if username.find('\\\\') != -1:\n        domain = username.split('\\\\')[0]\n        username = username.split('\\\\')[-1]\n    domain = domain.upper()\n    return win32security.ConvertSidToStringSid(\n        win32security.LookupAccountName(None, domain + '\\\\' + username)[0])", "output": "Get the Security ID for the user\n\n    Args:\n        username (str): The user name for which to look up the SID\n\n    Returns:\n        str: The user SID\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.getUserSid jsnuffy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_vn_id(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The get_vn_id function must be called with -f or --function.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    name = kwargs.get('name', None)\n    if name is None:\n        raise SaltCloudSystemExit(\n            'The get_vn_id function requires a name.'\n        )\n\n    try:\n        ret = list_vns()[name]['id']\n    except KeyError:\n        raise SaltCloudSystemExit(\n            'The VN \\'{0}\\' could not be found.'.format(name)\n        )\n\n    return ret", "output": "Returns a virtual network's ID from the given virtual network's name.\n\n    .. versionadded:: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f get_vn_id opennebula name=my-vn-name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _process_converter(self, f, filt=None):\n        \"\"\"\n        \n        \"\"\"\n\n        if filt is None:\n            filt = lambda col, c: True\n\n        needs_new_obj = False\n        new_obj = dict()\n        for i, (col, c) in enumerate(self.obj.iteritems()):\n            if filt(col, c):\n                new_data, result = f(col, c)\n                if result:\n                    c = new_data\n                    needs_new_obj = True\n            new_obj[i] = c\n\n        if needs_new_obj:\n\n            # possibly handle dup columns\n            new_obj = DataFrame(new_obj, index=self.obj.index)\n            new_obj.columns = self.obj.columns\n            self.obj = new_obj", "output": "Take a conversion function and possibly recreate the frame.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_report_error_api(client):\n    \"\"\"\n    \"\"\"\n    gax_client = report_errors_service_client.ReportErrorsServiceClient(\n        credentials=client._credentials, client_info=_CLIENT_INFO\n    )\n    return _ErrorReportingGapicApi(gax_client, client.project)", "output": "Create an instance of the gapic Logging API.\n\n    :type client::class:`google.cloud.error_reporting.Client`\n    :param client: Error Reporting client.\n\n    :rtype: :class:_ErrorReportingGapicApi\n    :returns: An Error Reporting API instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def defaults(features):\n    \"\"\" \n    \"\"\"\n    assert is_iterable_typed(features, Feature)\n    # FIXME: should merge feature and property modules.\n    from . import property\n\n    result = []\n    for f in features:\n        if not f.free and not f.optional and f.default:\n            result.append(property.Property(f, f.default))\n\n    return result", "output": "Returns the default property values for the given features.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def policy_definition_get(name, **kwargs):\n    '''\n    \n\n    '''\n    polconn = __utils__['azurearm.get_client']('policy', **kwargs)\n    try:\n        policy_def = polconn.policy_definitions.get(\n            policy_definition_name=name\n        )\n        result = policy_def.as_dict()\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('resource', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Get details about a specific policy definition.\n\n    :param name: The name of the policy definition to query.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_resource.policy_definition_get testpolicy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def min(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        nv.validate_min(args, kwargs)\n        nv.validate_minmax_axis(axis)\n\n        result = nanops.nanmin(self.asi8, skipna=skipna, mask=self.isna())\n        if isna(result):\n            # Period._from_ordinal does not handle np.nan gracefully\n            return NaT\n        return self._box_func(result)", "output": "Return the minimum value of the Array or minimum along\n        an axis.\n\n        See Also\n        --------\n        numpy.ndarray.min\n        Index.min : Return the minimum value in an Index.\n        Series.min : Return the minimum value in a Series.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wheel():\n    '''\n    \n    '''\n    client = salt.wheel.Wheel(__opts__)\n    ret = client.get_docs()\n    return ret", "output": "Return all inline documentation for wheel modules\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run doc.wheel", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prompt(self, timeout=-1):\n        '''\n        '''\n\n        if timeout == -1:\n            timeout = self.timeout\n        i = self.expect([self.PROMPT, TIMEOUT], timeout=timeout)\n        if i==1:\n            return False\n        return True", "output": "Match the next shell prompt.\n\n        This is little more than a short-cut to the :meth:`~pexpect.spawn.expect`\n        method. Note that if you called :meth:`login` with\n        ``auto_prompt_reset=False``, then before calling :meth:`prompt` you must\n        set the :attr:`PROMPT` attribute to a regex that it will use for\n        matching the prompt.\n\n        Calling :meth:`prompt` will erase the contents of the :attr:`before`\n        attribute even if no prompt is ever matched. If timeout is not given or\n        it is set to -1 then self.timeout is used.\n\n        :return: True if the shell prompt was matched, False if the timeout was\n                 reached.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_splits(self, split_dict):\n    \"\"\"\"\"\"\n    # Update the dictionary representation.\n    # Use from/to proto for a clean copy\n    self._splits = split_dict.copy()\n\n    # Update the proto\n    del self.as_proto.splits[:]  # Clear previous\n    for split_info in split_dict.to_proto():\n      self.as_proto.splits.add().CopyFrom(split_info)", "output": "Split setter (private method).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_action (self, action_name, command='', bound_list = [], flags = [],\n                         function = None):\n        \"\"\"\n        \"\"\"\n        assert isinstance(action_name, basestring)\n        assert isinstance(command, basestring)\n        assert is_iterable(bound_list)\n        assert is_iterable(flags)\n        assert function is None or callable(function)\n\n        bjam_flags = reduce(operator.or_,\n                            (action_modifiers[flag] for flag in flags), 0)\n\n        # We allow command to be empty so that we can define 'action' as pure\n        # python function that would do some conditional logic and then relay\n        # to other actions.\n        assert command or function\n        if command:\n            bjam_interface.define_action(action_name, command, bound_list, bjam_flags)\n\n        self.actions[action_name] = BjamAction(\n            action_name, function, has_command=bool(command))", "output": "Creates a new build engine action.\n\n        Creates on bjam side an action named 'action_name', with\n        'command' as the command to be executed, 'bound_variables'\n        naming the list of variables bound when the command is executed\n        and specified flag.\n        If 'function' is not None, it should be a callable taking three\n        parameters:\n            - targets\n            - sources\n            - instance of the property_set class\n        This function will be called by set_update_action, and can\n        set additional target variables.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(name, mac, mtu=1500):\n    '''\n    \n    '''\n    ret = {}\n\n    if mtu > 9000 or mtu < 1500:\n        return {'Error': 'mtu must be a value between 1500 and 9000.'}\n    if mac != 'etherstub':\n        cmd = 'dladm show-phys -m -p -o address'\n        res = __salt__['cmd.run_all'](cmd)\n        # dladm prints '00' as '0', so account for that.\n        if mac.replace('00', '0') not in res['stdout'].splitlines():\n            return {'Error': '{0} is not present on this system.'.format(mac)}\n\n    if mac == 'etherstub':\n        cmd = 'nictagadm add -l {0}'.format(name)\n        res = __salt__['cmd.run_all'](cmd)\n    else:\n        cmd = 'nictagadm add -p mtu={0},mac={1} {2}'.format(mtu, mac, name)\n        res = __salt__['cmd.run_all'](cmd)\n\n    if res['retcode'] == 0:\n        return True\n    else:\n        return {'Error': 'failed to create nictag.' if 'stderr' not in res and res['stderr'] == '' else res['stderr']}", "output": "Add a new nictag\n\n    name : string\n        name of new nictag\n    mac : string\n        mac of parent interface or 'etherstub' to create a ether stub\n    mtu : int\n        MTU (ignored for etherstubs)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' nictagadm.add storage0 etherstub\n        salt '*' nictagadm.add trunk0 'DE:AD:OO:OO:BE:EF' 9000", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def when(condition, value):\n    \"\"\"\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    if not isinstance(condition, Column):\n        raise TypeError(\"condition should be a Column\")\n    v = value._jc if isinstance(value, Column) else value\n    jc = sc._jvm.functions.when(condition._jc, v)\n    return Column(jc)", "output": "Evaluates a list of conditions and returns one of multiple possible result expressions.\n    If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n\n    :param condition: a boolean :class:`Column` expression.\n    :param value: a literal value, or a :class:`Column` expression.\n\n    >>> df.select(when(df['age'] == 2, 3).otherwise(4).alias(\"age\")).collect()\n    [Row(age=3), Row(age=4)]\n\n    >>> df.select(when(df.age == 2, df.age + 1).alias(\"age\")).collect()\n    [Row(age=3), Row(age=None)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chosen_probabs(probab_observations, actions):\n  \"\"\"\n  \"\"\"\n  B, T = actions.shape  # pylint: disable=invalid-name\n  assert (B, T + 1) == probab_observations.shape[:2]\n  return probab_observations[np.arange(B)[:, None], np.arange(T), actions]", "output": "Picks out the probabilities of the actions along batch and time-steps.\n\n  Args:\n    probab_observations: ndarray of shape `[B, T+1, A]`, where\n      probab_observations[b, t, i] contains the log-probability of action = i at\n      the t^th time-step in the b^th trajectory.\n    actions: ndarray of shape `[B, T]`, with each entry in [0, A) denoting which\n      action was chosen in the b^th trajectory's t^th time-step.\n\n  Returns:\n    `[B, T]` ndarray with the log-probabilities of the chosen actions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def searchsorted(self, value, side=\"left\", sorter=None):\n        \"\"\"\n        \n        \"\"\"\n        # Note: the base tests provided by pandas only test the basics.\n        # We do not test\n        # 1. Values outside the range of the `data_for_sorting` fixture\n        # 2. Values between the values in the `data_for_sorting` fixture\n        # 3. Missing values.\n        arr = self.astype(object)\n        return arr.searchsorted(value, side=side, sorter=sorter)", "output": "Find indices where elements should be inserted to maintain order.\n\n        .. versionadded:: 0.24.0\n\n        Find the indices into a sorted array `self` (a) such that, if the\n        corresponding elements in `value` were inserted before the indices,\n        the order of `self` would be preserved.\n\n        Assuming that `self` is sorted:\n\n        ======  ================================\n        `side`  returned index `i` satisfies\n        ======  ================================\n        left    ``self[i-1] < value <= self[i]``\n        right   ``self[i-1] <= value < self[i]``\n        ======  ================================\n\n        Parameters\n        ----------\n        value : array_like\n            Values to insert into `self`.\n        side : {'left', 'right'}, optional\n            If 'left', the index of the first suitable location found is given.\n            If 'right', return the last such index.  If there is no suitable\n            index, return either 0 or N (where N is the length of `self`).\n        sorter : 1-D array_like, optional\n            Optional array of integer indices that sort array a into ascending\n            order. They are typically the result of argsort.\n\n        Returns\n        -------\n        array of ints\n            Array of insertion points with the same shape as `value`.\n\n        See Also\n        --------\n        numpy.searchsorted : Similar method from NumPy.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def evaluate(self, dataset, metric='auto', missing_value_action='auto'):\n        \"\"\"\n        \n\n        \"\"\"\n        _raise_error_evaluation_metric_is_valid(\n                metric, ['auto', 'rmse', 'max_error'])\n        return super(BoostedTreesRegression, self).evaluate(dataset,\n                                 missing_value_action=missing_value_action,\n                                 metric=metric)", "output": "Evaluate the model on the given dataset.\n\n        Parameters\n        ----------\n        dataset : SFrame\n            Dataset in the same format used for training. The columns names and\n            types of the dataset must be the same as that used in training.\n\n        metric : str, optional\n            Name of the evaluation metric.  Can be one of:\n\n            - 'auto': Compute all metrics.\n            - 'rmse': Rooted mean squared error.\n            - 'max_error': Maximum error.\n\n        missing_value_action : str, optional\n            Action to perform when missing values are encountered. Can be\n            one of:\n\n            - 'auto': By default the model will treat missing value as is.\n            - 'impute': Proceed with evaluation by filling in the missing\n              values with the mean of the training data. Missing\n              values are also imputed if an entire column of data is\n              missing during evaluation.\n            - 'error': Do not proceed with evaluation and terminate with\n              an error message.\n\n        Returns\n        -------\n        out : dict\n            A dictionary containing the evaluation result.\n\n        See Also\n        ----------\n        create, predict\n\n        Examples\n        --------\n        ..sourcecode:: python\n\n          >>> results = model.evaluate(test_data, 'rmse')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cancel(self):\n        \"\"\"\n        \"\"\"\n\n        self.cancel_amount = self.amount - self.trade_amount\n        if self.trade_amount == 0:\n            # \u672a\u4ea4\u6613  \u76f4\u63a5\u8ba2\u5355\u5168\u64a4\n            self._status = ORDER_STATUS.CANCEL_ALL\n        else:\n            # \u90e8\u5206\u4ea4\u6613 \u5269\u4f59\u8ba2\u5355\u5168\u64a4\n            self._status = ORDER_STATUS.CANCEL_PART", "output": "\u64a4\u5355\n\n        Arguments:\n            amount {int} -- \u64a4\u5355\u6570\u91cf", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write(self, model:nn.Module, iteration:int, tbwriter:SummaryWriter, name:str='model')->None:\n        \"\"\n        request = HistogramTBRequest(model=model, iteration=iteration, tbwriter=tbwriter, name=name)\n        asyncTBWriter.request_write(request)", "output": "Writes model histograms to Tensorboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_template(vm_):\n    \n    '''\n\n    vm_template = six.text_type(config.get_cloud_config_value(\n        'template', vm_, __opts__, search_global=False\n    ))\n    try:\n        return list_templates()[vm_template]['id']\n    except KeyError:\n        raise SaltCloudNotFound(\n            'The specified template, \\'{0}\\', could not be found.'.format(vm_template)\n        )", "output": "r'''\n    Return the template id for a VM.\n\n    .. versionadded:: 2016.11.0\n\n    vm\\_\n        The VM dictionary for which to obtain a template.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_sub_prop(container, keys, value):\n    \"\"\"\n    \"\"\"\n    sub_val = container\n    for key in keys[:-1]:\n        if key not in sub_val:\n            sub_val[key] = {}\n        sub_val = sub_val[key]\n    sub_val[keys[-1]] = value", "output": "Set a nested value in a dictionary.\n\n    Arguments:\n        container (dict):\n            A dictionary which may contain other dictionaries as values.\n        keys (iterable):\n            A sequence of keys to attempt to set the value for. Each item in\n            the sequence represents a deeper nesting. The first key is for\n            the top level. If there is a dictionary there, the second key\n            attempts to get the value within that, and so on.\n        value (object): Value to set within the container.\n\n    Examples:\n        Set a top-level value (equivalent to ``container['key'] = 'value'``).\n\n        >>> container = {}\n        >>> _set_sub_prop(container, ['key'], 'value')\n        >>> container\n        {'key': 'value'}\n\n        Set a nested value.\n\n        >>> container = {}\n        >>> _set_sub_prop(container, ['key', 'subkey'], 'value')\n        >>> container\n        {'key': {'subkey': 'value'}}\n\n        Replace a nested value.\n\n        >>> container = {'key': {'subkey': 'prev'}}\n        >>> _set_sub_prop(container, ['key', 'subkey'], 'new')\n        >>> container\n        {'key': {'subkey': 'new'}}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sine(w, A=1, phi=0, offset=0):\n    ''' \n\n    '''\n    from math import sin\n    def f(i):\n        return A * sin(w*i + phi) + offset\n    return partial(force, sequence=_advance(f))", "output": "Return a driver function that can advance a sequence of sine values.\n\n    .. code-block:: none\n\n        value = A * sin(w*i + phi) + offset\n\n    Args:\n        w (float) : a frequency for the sine driver\n        A (float) : an amplitude for the sine driver\n        phi (float) : a phase offset to start the sine driver with\n        offset (float) : a global offset to add to the driver values", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _save_cfg_packages(self, data):\n        '''\n        \n        '''\n        pkg_id = 0\n        pkg_cfg_id = 0\n        for pkg_name, pkg_configs in data.items():\n            pkg = Package()\n            pkg.id = pkg_id\n            pkg.name = pkg_name\n            self.db.store(pkg)\n\n            for pkg_config in pkg_configs:\n                cfg = PackageCfgFile()\n                cfg.id = pkg_cfg_id\n                cfg.pkgid = pkg_id\n                cfg.path = pkg_config\n                self.db.store(cfg)\n                pkg_cfg_id += 1\n\n            pkg_id += 1", "output": "Save configuration packages. (NG)\n\n        :param data:\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_cidr(cidr):\n    '''\n    \n    '''\n    ret = {'network': None,\n           'netmask': None,\n           'broadcast': None}\n    cidr = calc_net(cidr)\n    network_info = ipaddress.ip_network(cidr)\n    ret['network'] = six.text_type(network_info.network_address)\n    ret['netmask'] = six.text_type(network_info.netmask)\n    ret['broadcast'] = six.text_type(network_info.broadcast_address)\n    return ret", "output": "returns the network address, subnet mask and broadcast address of a cidr address\n\n    .. versionadded:: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' network.convert_cidr 172.31.0.0/16", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mapPartitions(self, f, preservesPartitioning=False):\n        \"\"\"\n        \n        \"\"\"\n        def func(s, iterator):\n            return f(iterator)\n        return PipelinedRDD(self.rdd, func, preservesPartitioning, isFromBarrier=True)", "output": ".. note:: Experimental\n\n        Returns a new RDD by applying a function to each partition of the wrapped RDD,\n        where tasks are launched together in a barrier stage.\n        The interface is the same as :func:`RDD.mapPartitions`.\n        Please see the API doc there.\n\n        .. versionadded:: 2.4.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def execute(self, ignore_cache=False, raise_on_error=True):\n        \"\"\"\n        \n        \"\"\"\n        if ignore_cache or not hasattr(self, '_response'):\n            es = connections.get_connection(self._using)\n\n            responses = es.msearch(\n                index=self._index,\n                body=self.to_dict(),\n                **self._params\n            )\n\n            out = []\n            for s, r in zip(self._searches, responses['responses']):\n                if r.get('error', False):\n                    if raise_on_error:\n                        raise TransportError('N/A', r['error']['type'], r['error'])\n                    r = None\n                else:\n                    r = Response(s, r)\n                out.append(r)\n\n            self._response = out\n\n        return self._response", "output": "Execute the multi search request and return a list of search results.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_dependencies(self):\r\n        \"\"\"\"\"\"\r\n        from spyder.widgets.dependencies import DependenciesDialog\r\n        dlg = DependenciesDialog(self)\r\n        dlg.set_data(dependencies.DEPENDENCIES)\r\n        dlg.exec_()", "output": "Show Spyder's Dependencies dialog box", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def infer_devices(devices=None):\n  \"\"\"\n  \n  \"\"\"\n  if devices is None:\n    devices = get_available_gpus()\n    if len(devices) == 0:\n      warnings.warn(\"No GPUS, running on CPU\")\n      # Set device to empy string, tf will figure out whether to use\n      # XLA or not, etc., automatically\n      devices = [\"\"]\n  else:\n    assert len(devices) > 0\n    for device in devices:\n      assert isinstance(device, six.string_types), type(device)\n  return devices", "output": "Returns the list of devices that multi-replica code should use.\n  :param devices: list of string device names, e.g. [\"/GPU:0\"]\n      If the user specifies this, `infer_devices` checks that it is\n      valid, and then uses this user-specified list.\n      If the user does not specify this, infer_devices uses:\n          - All available GPUs, if there are any\n          - CPU otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def diff(name):\n    '''\n    \n    '''\n    changes = _client_wrapper('diff', name)\n    kind_map = {0: 'Changed', 1: 'Added', 2: 'Deleted'}\n    ret = {}\n    for change in changes:\n        key = kind_map.get(change['Kind'], 'Unknown')\n        ret.setdefault(key, []).append(change['Path'])\n    if 'Unknown' in ret:\n        log.error(\n            'Unknown changes detected in docker.diff of container %s. '\n            'This is probably due to a change in the Docker API. Please '\n            'report this to the SaltStack developers', name\n        )\n    return ret", "output": "Get information on changes made to container's filesystem since it was\n    created. Equivalent to running the ``docker diff`` Docker CLI command.\n\n    name\n        Container name or ID\n\n\n    **RETURN DATA**\n\n    A dictionary containing any of the following keys:\n\n    - ``Added`` - A list of paths that were added.\n    - ``Changed`` - A list of paths that were changed.\n    - ``Deleted`` - A list of paths that were deleted.\n\n    These keys will only be present if there were changes, so if the container\n    has no differences the return dict will be empty.\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion docker.diff mycontainer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ensure_decoded(s):\n    \"\"\"  \"\"\"\n    if isinstance(s, np.bytes_):\n        s = s.decode('UTF-8')\n    return s", "output": "if we have bytes, decode them to unicode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _wrap_unary_errors(callable_):\n    \"\"\"\"\"\"\n    _patch_callable_name(callable_)\n\n    @six.wraps(callable_)\n    def error_remapped_callable(*args, **kwargs):\n        try:\n            return callable_(*args, **kwargs)\n        except grpc.RpcError as exc:\n            six.raise_from(exceptions.from_grpc_error(exc), exc)\n\n    return error_remapped_callable", "output": "Map errors for Unary-Unary and Stream-Unary gRPC callables.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _conn(queue):\n    '''\n    \n    '''\n    queue_dir = __opts__['sqlite_queue_dir']\n    db = os.path.join(queue_dir, '{0}.db'.format(queue))\n    log.debug('Connecting to: %s', db)\n\n    con = sqlite3.connect(db)\n    tables = _list_tables(con)\n    if queue not in tables:\n        _create_table(con, queue)\n    return con", "output": "Return an sqlite connection", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fillna(self, column_name, value):\n        \"\"\"\n        \n        \"\"\"\n        # Normal error checking\n        if type(column_name) is not str:\n            raise TypeError(\"column_name must be a str\")\n        ret = self[self.column_names()]\n        ret[column_name] = ret[column_name].fillna(value)\n        return ret", "output": "Fill all missing values with a given value in a given column. If the\n        ``value`` is not the same type as the values in ``column_name``, this method\n        attempts to convert the value to the original column's type. If this\n        fails, an error is raised.\n\n        Parameters\n        ----------\n        column_name : str\n            The name of the column to modify.\n\n        value : type convertible to SArray's type\n            The value used to replace all missing values.\n\n        Returns\n        -------\n        out : SFrame\n            A new SFrame with the specified value in place of missing values.\n\n        See Also\n        --------\n        dropna\n\n        Examples\n        --------\n        >>> sf = turicreate.SFrame({'a':[1, None, None],\n        ...                       'b':['13.1', '17.2', None]})\n        >>> sf = sf.fillna('a', 0)\n        >>> sf\n        +---+------+\n        | a |  b   |\n        +---+------+\n        | 1 | 13.1 |\n        | 0 | 17.2 |\n        | 0 | None |\n        +---+------+\n        [3 rows x 2 columns]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _auth(uri):\n    '''\n    \n    '''\n\n    user, password = _get_credentials()\n    if user is False or password is False:\n        return False\n\n    basic = _HTTPBasicAuthHandler()\n    basic.add_password(realm='Tomcat Manager Application', uri=uri,\n                       user=user, passwd=password)\n    digest = _HTTPDigestAuthHandler()\n    digest.add_password(realm='Tomcat Manager Application', uri=uri,\n                        user=user, passwd=password)\n    return _build_opener(basic, digest)", "output": "returns a authentication handler.\n    Get user & password from grains, if are not set default to\n    modules.config.option\n\n    If user & pass are missing return False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def human_size_to_bytes(human_size):\n    '''\n    \n    '''\n    size_exp_map = {'K': 1, 'M': 2, 'G': 3, 'T': 4, 'P': 5}\n    human_size_str = six.text_type(human_size)\n    match = re.match(r'^(\\d+)([KMGTP])?$', human_size_str)\n    if not match:\n        raise ValueError(\n            'Size must be all digits, with an optional unit type '\n            '(K, M, G, T, or P)'\n        )\n    size_num = int(match.group(1))\n    unit_multiplier = 1024 ** size_exp_map.get(match.group(2), 0)\n    return size_num * unit_multiplier", "output": "Convert human-readable units to bytes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_file_position(body, pos):\n    \"\"\"\n    \n    \"\"\"\n    if pos is not None:\n        rewind_body(body, pos)\n    elif getattr(body, 'tell', None) is not None:\n        try:\n            pos = body.tell()\n        except (IOError, OSError):\n            # This differentiates from None, allowing us to catch\n            # a failed `tell()` later when trying to rewind the body.\n            pos = _FAILEDTELL\n\n    return pos", "output": "If a position is provided, move file to that point.\n    Otherwise, we'll attempt to record a position for future use.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_instance(name, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The show_instance action must be called with -a or --action.'\n        )\n    try:\n        node = list_nodes_full('function')[name]\n    except KeyError:\n        log.debug('Failed to get data for node \\'%s\\'', name)\n        node = {}\n\n    __utils__['cloud.cache_node'](node, __active_provider_name__, __opts__)\n\n    return node", "output": "Show the details from AzureARM concerning an instance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare(self):\n        '''\n        \n        '''\n        # Find an acceptable content-type\n        accept_header = self.request.headers.get('Accept', '*/*')\n        # Ignore any parameter, including q (quality) one\n        parsed_accept_header = [cgi.parse_header(h)[0] for h in accept_header.split(',')]\n\n        def find_acceptable_content_type(parsed_accept_header):\n            for media_range in parsed_accept_header:\n                for content_type, dumper in self.ct_out_map:\n                    if fnmatch.fnmatch(content_type, media_range):\n                        return content_type, dumper\n            return None, None\n\n        content_type, dumper = find_acceptable_content_type(parsed_accept_header)\n\n        # better return message?\n        if not content_type:\n            self.send_error(406)\n\n        self.content_type = content_type\n        self.dumper = dumper\n\n        # do the common parts\n        self.start = time.time()\n        self.connected = True\n\n        self.lowstate = self._get_lowstate()", "output": "Run before get/posts etc. Pre-flight checks:\n            - verify that we can speak back to them (compatible accept header)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_util_get_trade_range(start, end):\n    ''\n    start, end = QA_util_get_real_datelist(start, end)\n    if start is not None:\n        return trade_date_sse[trade_date_sse\n                              .index(start):trade_date_sse.index(end) + 1:1]\n    else:\n        return None", "output": "\u7ed9\u51fa\u4ea4\u6613\u5177\u4f53\u65f6\u95f4", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_ctx(self, ctx):\n        \"\"\"\n        \"\"\"\n        todo = deque([self])\n        while todo:\n            node = todo.popleft()\n            if 'ctx' in node.fields:\n                node.ctx = ctx\n            todo.extend(node.iter_child_nodes())\n        return self", "output": "Reset the context of a node and all child nodes.  Per default the\n        parser will all generate nodes that have a 'load' context as it's the\n        most common one.  This method is used in the parser to set assignment\n        targets and other nodes to a store context.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absent(name, auth=None, **kwargs):\n    '''\n    \n\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    kwargs = __utils__['args.clean_kwargs'](**kwargs)\n\n    __salt__['neutronng.setup_clouds'](auth)\n\n    kwargs['name'] = name\n    network = __salt__['neutronng.network_get'](name=name)\n\n    if network:\n        if __opts__['test'] is True:\n            ret['result'] = None\n            ret['changes'] = {'id': network.id}\n            ret['comment'] = 'Network will be deleted.'\n            return ret\n\n        __salt__['neutronng.network_delete'](name=network)\n        ret['changes']['id'] = network.id\n        ret['comment'] = 'Deleted network'\n\n    return ret", "output": "Ensure a network does not exists\n\n    name\n        Name of the network", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def import_mod(mod_name:str, ignore_errors=False):\n    \"\"\n    splits = str.split(mod_name, '.')\n    try:\n        if len(splits) > 1 : mod = importlib.import_module('.' + '.'.join(splits[1:]), splits[0])\n        else: mod = importlib.import_module(mod_name)\n        return mod\n    except:\n        if not ignore_errors: print(f\"Module {mod_name} doesn't exist.\")", "output": "Return module from `mod_name`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_epytext(line):\n    \"\"\"\n    \n    \"\"\"\n    line = line.replace('@', ':')\n    for p, sub in RULES:\n        line = re.sub(p, sub, line)\n    return line", "output": ">>> _convert_epytext(\"L{A}\")\n    :class:`A`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_interface(device_name, interface_name, **kwargs):\n    '''\n    \n    '''\n    nb_device = get_('dcim', 'devices', name=device_name)\n    nb_interface = _get('dcim', 'interfaces', auth_required=True, device_id=nb_device['id'], name=interface_name)\n    if not nb_device:\n        return False\n    if not nb_interface:\n        return False\n    else:\n        for k, v in __utils__['args.clean_kwargs'](**kwargs).items():\n            setattr(nb_interface, k, v)\n        try:\n            nb_interface.save()\n            return {'dcim': {'interfaces': {nb_interface.id: dict(nb_interface)}}}\n        except RequestError as e:\n            log.error('%s, %s, %s', e.req.request.headers, e.request_body, e.error)\n            return False", "output": ".. versionadded:: 2019.2.0\n\n    Update an existing interface with new attributes.\n\n    device_name\n        The name of the device, e.g., ``edge_router``\n    interface_name\n        The name of the interface, e.g., ``ae13``\n    kwargs\n        Arguments to change in interface, e.g., ``mac_address=50:87:69:53:32:D0``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion netbox.update_interface edge_router ae13 mac_address=50:87:69:53:32:D0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_args_to_dict(call, with_lambda=False):\n    \"\"\"\n    \"\"\"\n    keys, values = list(), list()\n    for arg in call.args:\n        if type(arg) in [ast.Str, ast.Num]:\n            arg_value = arg\n        else:\n        # if arg is not a string or a number, we use its source code as the key\n            arg_value = astor.to_source(arg).strip('\\n\"')\n            arg_value = ast.Str(str(arg_value))\n        arg = make_lambda(arg) if with_lambda else arg\n        keys.append(arg_value)\n        values.append(arg)\n    del call.args[:]\n    call.args.append(ast.Dict(keys=keys, values=values))\n\n    return call", "output": "Convert all args to a dict such that every key and value in the dict is the same as the value of the arg.\n    Return the AST Call node with only one arg that is the dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_dict(template_name, template_values_dict):\n        \"\"\"\n        \n        \"\"\"\n\n        parameters = template_values_dict.get(\"Parameters\", {})\n        definition = template_values_dict.get(\"Definition\", {})\n\n        return Template(template_name, parameters, definition)", "output": "Parses the input and returns an instance of this class.\n\n        :param string template_name: Name of the template\n        :param dict template_values_dict: Dictionary containing the value of the template. This dict must have passed\n            the JSON Schema validation.\n        :return Template: Instance of this class containing the values provided in this dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_label_vocab(vocab_path):\n  \"\"\"\"\"\"\n  if vocab_path:\n    try:\n      with tf.io.gfile.GFile(vocab_path, 'r') as f:\n        return [line.rstrip('\\n') for line in f]\n    except tf.errors.NotFoundError as err:\n      tf.logging.error('error reading vocab file: %s', err)\n  return []", "output": "Returns a list of label strings loaded from the provided path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absent(\n        name,\n        region=None,\n        key=None,\n        keyid=None,\n        profile=None,\n):\n    '''\n    \n    '''\n    ret = {'name': name, 'result': True, 'comment': '', 'changes': {}}\n\n    r = __salt__['boto_sqs.exists'](\n        name,\n        region=region,\n        key=key,\n        keyid=keyid,\n        profile=profile,\n    )\n    if 'error' in r:\n        ret['result'] = False\n        ret['comment'] = six.text_type(r['error'])\n        return ret\n\n    if not r['result']:\n        ret['comment'] = 'SQS queue {0} does not exist in {1}.'.format(\n            name,\n            region,\n        )\n        return ret\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'SQS queue {0} is set to be removed.'.format(name)\n        ret['changes'] = {'old': name, 'new': None}\n        return ret\n\n    r = __salt__['boto_sqs.delete'](\n        name,\n        region=region,\n        key=key,\n        keyid=keyid,\n        profile=profile,\n    )\n    if 'error' in r:\n        ret['result'] = False\n        ret['comment'] = six.text_type(r['error'])\n        return ret\n\n    ret['comment'] = 'SQS queue {0} was deleted.'.format(name)\n    ret['changes']['old'] = name\n    ret['changes']['new'] = None\n    return ret", "output": "Ensure the named sqs queue is deleted.\n\n    name\n        Name of the SQS queue.\n\n    region\n        Region to connect to.\n\n    key\n        Secret key to be used.\n\n    keyid\n        Access key to be used.\n\n    profile\n        A dict with region, key and keyid, or a pillar key (string)\n        that contains a dict with region, key and keyid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _analyse_status_type(line):\n    '''\n    \n    '''\n    spaces = _count_spaces_startswith(line)\n\n    if spaces is None:\n        return ''\n\n    switch = {\n        0: 'RESOURCE',\n        2: {' disk:': 'LOCALDISK', ' role:': 'PEERNODE', ' connection:': 'PEERNODE'},\n        4: {' peer-disk:': 'PEERDISK'}\n    }\n\n    ret = switch.get(spaces, 'UNKNOWN')\n\n    # isinstance(ret, str) only works when run directly, calling need unicode(six)\n    if isinstance(ret, six.text_type):\n        return ret\n\n    for x in ret:\n        if x in line:\n            return ret[x]\n\n    return 'UNKNOWN'", "output": "Figure out the sections in drbdadm status", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def slice_to_percent_mask(slice_value):\n  \"\"\"\"\"\"\n  if slice_value is None:\n    slice_value = slice(None)\n  # Select only the elements of the slice\n  selected = set(list(range(100))[slice_value])\n  # Create the binary mask\n  return [i in selected for i in range(100)]", "output": "Convert a python slice [15:50] into a list[bool] mask of 100 elements.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_graph_copy(self, graph, tags=None):\n    \"\"\"\"\"\"\n    with graph.as_default():\n      # Remove default attrs so that Modules created by a tensorflow version\n      # with ops that have new attrs that are left to their default values can\n      # still be loaded by older versions unware of those attributes.\n      meta_graph = tf_v1.train.export_meta_graph(strip_default_attrs=True)\n      _export_tags(meta_graph, tags)\n      _export_signatures(meta_graph)\n      _export_module_attachments(meta_graph)\n    self._proto.meta_graphs.extend([meta_graph])", "output": "Adds a copy of Graph with the specified set of tags.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extract_fields(document_data, prefix_path, expand_dots=False):\n    \"\"\"\"\"\"\n    if not document_data:\n        yield prefix_path, _EmptyDict\n    else:\n        for key, value in sorted(six.iteritems(document_data)):\n\n            if expand_dots:\n                sub_key = FieldPath.from_string(key)\n            else:\n                sub_key = FieldPath(key)\n\n            field_path = FieldPath(*(prefix_path.parts + sub_key.parts))\n\n            if isinstance(value, dict):\n                for s_path, s_value in extract_fields(value, field_path):\n                    yield s_path, s_value\n            else:\n                yield field_path, value", "output": "Do depth-first walk of tree, yielding field_path, value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_resource_dict(self):\n        \"\"\"\n        \"\"\"\n        resource_dict = {}\n\n        resource_dict['Type'] = self.resource_type\n\n        if self.depends_on:\n            resource_dict['DependsOn'] = self.depends_on\n\n        resource_dict.update(self.resource_attributes)\n\n        properties_dict = {}\n        for name in self.property_types:\n            value = getattr(self, name)\n            if value is not None:\n                properties_dict[name] = value\n\n        resource_dict['Properties'] = properties_dict\n\n        return resource_dict", "output": "Generates the resource dict for this Resource, the value associated with the logical id in a CloudFormation\n        template's Resources section.\n\n        :returns: the resource dict for this Resource\n        :rtype: dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def plot(self, n_skip=10, n_skip_end=5):\n        '''\n         \n        '''\n        plt.ylabel(\"validation loss\")\n        plt.xlabel(\"learning rate (log scale)\")\n        plt.plot(self.lrs[n_skip:-(n_skip_end+1)], self.losses[n_skip:-(n_skip_end+1)])\n        plt.xscale('log')", "output": "Plots the loss function with respect to learning rate, in log scale.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_initialize(self, data):\n        \"\"\"\n        \"\"\"\n        logger.info('start to handle_initialize')\n        # convert search space jason to ConfigSpace\n        self.handle_update_search_space(data)\n\n        # generate BOHB config_generator using Bayesian optimization\n        if self.search_space:\n            self.cg = CG_BOHB(configspace=self.search_space,\n                              min_points_in_model=self.min_points_in_model,\n                              top_n_percent=self.top_n_percent,\n                              num_samples=self.num_samples,\n                              random_fraction=self.random_fraction,\n                              bandwidth_factor=self.bandwidth_factor,\n                              min_bandwidth=self.min_bandwidth)\n        else:\n            raise ValueError('Error: Search space is None')\n        # generate first brackets\n        self.generate_new_bracket()\n        send(CommandType.Initialized, '')", "output": "Initialize Tuner, including creating Bayesian optimization-based parametric models \n        and search space formations\n\n        Parameters\n        ----------\n        data: search space\n            search space of this experiment\n\n        Raises\n        ------\n        ValueError\n            Error: Search space is None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_all_groups(region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    retries = 30\n    while True:\n        try:\n            next_token = ''\n            asgs = []\n            while next_token is not None:\n                ret = conn.get_all_groups(next_token=next_token)\n                asgs += [a for a in ret]\n                next_token = ret.next_token\n            return asgs\n        except boto.exception.BotoServerError as e:\n            if retries and e.code == 'Throttling':\n                log.debug('Throttled by AWS API, retrying in 5 seconds...')\n                time.sleep(5)\n                retries -= 1\n                continue\n            log.error(e)\n            return []", "output": "Return all AutoScale Groups visible in the account\n    (as a list of boto.ec2.autoscale.group.AutoScalingGroup).\n\n    .. versionadded:: 2016.11.0\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt-call boto_asg.get_all_groups region=us-east-1 --output yaml", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _store_token(self, token, remember=False):\n        \"\"\"\"\"\"\n        if token and remember:\n            try:\n                keyring.set_password('github', 'token', token)\n            except Exception:\n                if self._show_msgbox:\n                    QMessageBox.warning(self.parent_widget,\n                                        _('Failed to store token'),\n                                        _('It was not possible to securely '\n                                          'save your token. You will be '\n                                          'prompted for your Github token '\n                                          'next time you want to report '\n                                          'an issue.'))\n                remember = False\n        CONF.set('main', 'report_error/remember_token', remember)", "output": "Store token for future use.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_hosted_zone(Id, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    args = {'Id': Id}\n    return _collect_results(conn.get_hosted_zone, None, args)", "output": "Return detailed info about the given zone.\n\n    Id\n        The unique Zone Identifier for the Hosted Zone.\n\n    region\n        Region to connect to.\n\n    key\n        Secret key to be used.\n\n    keyid\n        Access key to be used.\n\n    profile\n        Dict, or pillar key pointing to a dict, containing AWS region/key/keyid.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto3_route53.get_hosted_zone Z1234567690 \\\n                profile='{\"region\": \"us-east-1\", \"keyid\": \"A12345678AB\", \"key\": \"xblahblahblah\"}'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _main_thread_terminated(self):\n        \"\"\"\"\"\"\n        if not self.is_alive:\n            return\n\n        if not self._queue.empty():\n            print(\n                \"Program shutting down, attempting to send %d queued log \"\n                \"entries to Stackdriver Logging...\" % (self._queue.qsize(),),\n                file=sys.stderr,\n            )\n\n        if self.stop(self._grace_period):\n            print(\"Sent all pending logs.\", file=sys.stderr)\n        else:\n            print(\n                \"Failed to send %d pending logs.\" % (self._queue.qsize(),),\n                file=sys.stderr,\n            )", "output": "Callback that attempts to send pending logs before termination.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compress_encoder_1d(x, hparams, name=None):\n  \"\"\"\n  \"\"\"\n  x = tf.expand_dims(x, axis=2)\n  return compress_encoder(x,\n                          hparams,\n                          strides=(2, 1),\n                          kernel_size=(hparams.kernel_size, 1),\n                          name=name)", "output": "Encoder that compresses 1-D inputs by 2**num_compress_steps.\n\n  Args:\n    x: Tensor of shape [batch, length, channels].\n    hparams: HParams.\n    name: string, variable scope.\n\n  Returns:\n    Tensor of shape [batch, latent_length, hparams.hidden_size], where\n      latent_length is\n      hparams.num_latents * length / 2**hparams.num_compress_steps.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_zones(verbose=True, installed=False, configured=False, hide_global=True):\n    '''\n    \n    '''\n    zones = {}\n\n    ## fetch zones\n    header = 'zoneid:zonename:state:zonepath:uuid:brand:ip-type'.split(':')\n    zone_data = __salt__['cmd.run_all']('zoneadm list -p -c')\n    if zone_data['retcode'] == 0:\n        for zone in zone_data['stdout'].splitlines():\n            zone = zone.split(':')\n\n            # create zone_t\n            zone_t = {}\n            for i in range(0, len(header)):\n                zone_t[header[i]] = zone[i]\n\n            # skip if global and hide_global\n            if hide_global and zone_t['zonename'] == 'global':\n                continue\n\n            # skip installed and configured\n            if not installed and zone_t['state'] == 'installed':\n                continue\n            if not configured and zone_t['state'] == 'configured':\n                continue\n\n            # update dict\n            zones[zone_t['zonename']] = zone_t\n            del zones[zone_t['zonename']]['zonename']\n\n    return zones if verbose else sorted(zones.keys())", "output": "List all zones\n\n    verbose : boolean\n        display additional zone information\n    installed : boolean\n        include installed zones in output\n    configured : boolean\n        include configured zones in output\n    hide_global : boolean\n        do not include global zone\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zoneadm.list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_error(\n        self, exception=ParseError, *args\n    ):  # type: (ParseError.__class__, ...) -> ParseError\n        \"\"\"\n        \n        \"\"\"\n        line, col = self._to_linecol()\n\n        return exception(line, col, *args)", "output": "Creates a generic \"parse error\" at the current position.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare_outputs(self, job):\n        \"\"\"\n        \n        \"\"\"\n        outputs = flatten(job.output())\n        for o in outputs:\n            if isinstance(o, FileSystemTarget):\n                parent_dir = os.path.dirname(o.path)\n                if parent_dir and not o.fs.exists(parent_dir):\n                    logger.info(\"Creating parent directory %r\", parent_dir)\n                    try:\n                        # there is a possible race condition\n                        # which needs to be handled here\n                        o.fs.mkdir(parent_dir)\n                    except FileAlreadyExists:\n                        pass", "output": "Called before job is started.\n\n        If output is a `FileSystemTarget`, create parent directories so the hive command won't fail", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_table(self):\r\n        \"\"\"\"\"\"\r\n        self.horizontalHeader().setStretchLastSection(True)\r\n        self.adjust_columns()\r\n        # Sorting columns\r\n        self.setSortingEnabled(True)\r\n        self.sortByColumn(0, Qt.AscendingOrder)", "output": "Setup table", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hybrid_forward(self, F, inputs, token_types, valid_length=None, masked_positions=None):\n        # pylint: disable=arguments-differ\n        # pylint: disable=unused-argument\n        \"\"\"\n        \"\"\"\n        outputs = []\n        seq_out, attention_out = self._encode_sequence(F, inputs, token_types, valid_length)\n        outputs.append(seq_out)\n\n        if self.encoder._output_all_encodings:\n            assert isinstance(seq_out, list)\n            output = seq_out[-1]\n        else:\n            output = seq_out\n\n        if attention_out:\n            outputs.append(attention_out)\n\n        if self._use_pooler:\n            pooled_out = self._apply_pooling(output)\n            outputs.append(pooled_out)\n            if self._use_classifier:\n                next_sentence_classifier_out = self.classifier(pooled_out)\n                outputs.append(next_sentence_classifier_out)\n        if self._use_decoder:\n            assert masked_positions is not None, \\\n                'masked_positions tensor is required for decoding masked language model'\n            decoder_out = self._decode(output, masked_positions)\n            outputs.append(decoder_out)\n        return tuple(outputs) if len(outputs) > 1 else outputs[0]", "output": "Generate the representation given the inputs.\n\n        This is used in training or fine-tuning a static (hybridized) BERT model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send_request(self, req):\n        \"\"\"\n        \n        \"\"\"\n        handlers = []\n        if self.password_handler:\n            handlers.append(self.password_handler)\n        if self.ssl_verifier:\n            handlers.append(self.ssl_verifier)\n        opener = build_opener(*handlers)\n        return opener.open(req)", "output": "Send a standard library :class:`Request` to PyPI and return its\n        response.\n\n        :param req: The request to send.\n        :return: The HTTP response from PyPI (a standard library HTTPResponse).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply(self, func, **kwargs):\n        \"\"\" \n        \"\"\"\n        with np.errstate(all='ignore'):\n            result = func(self.values, **kwargs)\n        if not isinstance(result, Block):\n            result = self.make_block(values=_block_shape(result,\n                                                         ndim=self.ndim))\n\n        return result", "output": "apply the function to my values; return a block if we are not\n        one", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compute(self, today, assets, out, *arrays):\n        \"\"\"\n        \n        \"\"\"\n        raise NotImplementedError(\n            \"{name} must define a compute method\".format(\n                name=type(self).__name__\n            )\n        )", "output": "Override this method with a function that writes a value into `out`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _revs_equal(rev1, rev2, rev_type):\n    '''\n    \n    '''\n    if (rev1 is None and rev2 is not None) \\\n            or (rev2 is None and rev1 is not None):\n        return False\n    elif rev1 is rev2 is None:\n        return True\n    elif rev_type == 'sha1':\n        return rev1.startswith(rev2)\n    else:\n        return rev1 == rev2", "output": "Shorthand helper function for comparing SHA1s. If rev_type == 'sha1' then\n    the comparison will be done using str.startwith() to allow short SHA1s to\n    compare successfully.\n\n    NOTE: This means that rev2 must be the short rev.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def info(name, root=None):\n    '''\n    \n    '''\n    # If root is provided, we use a less portable solution that\n    # depends on analyzing /etc/passwd manually. Of course we cannot\n    # find users from NIS nor LDAP, but in those cases do not makes\n    # sense to provide a root parameter.\n    #\n    # Please, note that if the non-root /etc/passwd file is long the\n    # iteration can be slow.\n    if root is not None and __grains__['kernel'] != 'AIX':\n        getpwnam = functools.partial(_getpwnam, root=root)\n    else:\n        getpwnam = functools.partial(pwd.getpwnam)\n\n    try:\n        data = getpwnam(_quote_username(name))\n    except KeyError:\n        return {}\n    else:\n        return _format_info(data)", "output": "Return user information\n\n    name\n        User to get the information\n\n    root\n        Directory to chroot into\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.info root", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chordialDiagram(fileStr, SM, Threshold, names, namesCategories):\n    '''\n    \n    '''\n    colors = text_list_to_colors_simple(namesCategories)\n    SM2 = SM.copy()\n    SM2 = (SM2 + SM2.T) / 2.0\n    for i in range(SM2.shape[0]):\n        M = Threshold\n#        a = np.sort(SM2[i,:])[::-1]\n#        M = np.mean(a[0:int(SM2.shape[1]/3+1)])\n        SM2[i, SM2[i, :] < M] = 0;\n    dirChordial = fileStr + \"_Chordial\"\n    if not os.path.isdir(dirChordial):\n        os.mkdir(dirChordial)\n    jsonPath         = dirChordial + os.sep + \"matrix.json\"\n    namesPath        = dirChordial + os.sep + \"Names.csv\"\n \n    jsonSMMatrix = simplejson.dumps(SM2.tolist())\n    f = open(jsonPath,'w'); f.write(jsonSMMatrix);  f.close()\n    f = open(namesPath,'w'); f.write(\"name,color\\n\"); \n    for i, n in enumerate(names):\n        f.write(\"{0:s},{1:s}\\n\".format(n,\"#\"+str(colors[i])))\n    f.close()\n\n    shutil.copyfile(os.path.join(os.path.dirname(os.path.realpath(__file__)),\n                                 \"data\", \"similarities.html\"),\n                    dirChordial+os.sep+\"similarities.html\")\n    shutil.copyfile(os.path.join(os.path.dirname(os.path.realpath(__file__)),\n                                 \"data\",\n                                 \"style.css\"),\n                    dirChordial+os.sep+\"style.css\")", "output": "Generates a d3js chordial diagram that illustrates similarites", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def soft_triplet_loss(anchor, positive, negative, extra=True, scope=\"soft_triplet_loss\"):\n    \n    \"\"\"\n\n    eps = 1e-10\n    with tf.name_scope(scope):\n        d_pos = tf.sqrt(tf.reduce_sum(tf.square(anchor - positive), 1) + eps)\n        d_neg = tf.sqrt(tf.reduce_sum(tf.square(anchor - negative), 1) + eps)\n\n        logits = tf.stack([d_pos, d_neg], axis=1)\n        ones = tf.ones_like(tf.squeeze(d_pos), dtype=\"int32\")\n\n        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=ones))\n\n        if extra:\n            pos_dist = tf.reduce_mean(d_pos, name='pos-dist')\n            neg_dist = tf.reduce_mean(d_neg, name='neg-dist')\n            return loss, pos_dist, neg_dist\n        else:\n            return loss", "output": "r\"\"\"Loss for triplet networks as described in the paper:\n    `Deep Metric Learning using Triplet Network\n    <https://arxiv.org/abs/1412.6622>`_ by Hoffer et al.\n\n    It is a softmax loss using :math:`(anchor-positive)^2` and\n    :math:`(anchor-negative)^2` as logits.\n\n    Args:\n        anchor (tf.Tensor): anchor feature vectors of shape [Batch, N].\n        positive (tf.Tensor): features of positive match of the same shape.\n        negative (tf.Tensor): features of negative match of the same shape.\n        extra (bool): also return distances for pos and neg.\n\n    Returns:\n        tf.Tensor: triplet-loss as scalar (and optionally average_pos_dist, average_neg_dist)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=portable_hash,\n                                           ascending=True, keyfunc=lambda x: x):\n        \"\"\"\n        \n        \"\"\"\n        if numPartitions is None:\n            numPartitions = self._defaultReducePartitions()\n\n        memory = _parse_memory(self.ctx._conf.get(\"spark.python.worker.memory\", \"512m\"))\n        serializer = self._jrdd_deserializer\n\n        def sortPartition(iterator):\n            sort = ExternalSorter(memory * 0.9, serializer).sorted\n            return iter(sort(iterator, key=lambda k_v: keyfunc(k_v[0]), reverse=(not ascending)))\n\n        return self.partitionBy(numPartitions, partitionFunc).mapPartitions(sortPartition, True)", "output": "Repartition the RDD according to the given partitioner and, within each resulting partition,\n        sort records by their keys.\n\n        >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n        >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n        >>> rdd2.glom().collect()\n        [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def receive_trial_result(self, parameter_id, parameters, value):\n        '''\n        '''\n        reward = extract_scalar_reward(value)\n        if parameter_id not in self.total_data:\n            raise RuntimeError('Received parameter_id not in total_data.')\n        # restore the paramsters contains \"_index\"\n        params = self.total_data[parameter_id]\n\n        if self.optimize_mode == OptimizeMode.Minimize:\n            reward = -reward\n\n        indiv = Individual(config=params, result=reward)\n        self.population.append(indiv)", "output": "Record the result from a trial\n\n        Parameters\n        ----------\n        parameters: dict\n        value : dict/float\n            if value is dict, it should have \"default\" key.\n            value is final metrics of the trial.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def friendly_number(self, value: int) -> str:\n        \"\"\"\"\"\"\n        if self.code not in (\"en\", \"en_US\"):\n            return str(value)\n        s = str(value)\n        parts = []\n        while s:\n            parts.append(s[-3:])\n            s = s[:-3]\n        return \",\".join(reversed(parts))", "output": "Returns a comma-separated number for the given integer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_market_close(self, dt, data_portal):\n        \"\"\"\n        \"\"\"\n        completed_session = self._current_session\n\n        if self.emission_rate == 'daily':\n            # this method is called for both minutely and daily emissions, but\n            # this chunk of code here only applies for daily emissions. (since\n            # it's done every minute, elsewhere, for minutely emission).\n            self.sync_last_sale_prices(dt, data_portal)\n\n        session_ix = self._session_count\n        # increment the day counter before we move markers forward.\n        self._session_count += 1\n\n        packet = {\n            'period_start': self._first_session,\n            'period_end': self._last_session,\n            'capital_base': self._capital_base,\n            'daily_perf': {\n                'period_open': self._market_open,\n                'period_close': dt,\n            },\n            'cumulative_perf': {\n                'period_open': self._first_session,\n                'period_close': self._last_session,\n            },\n            'progress': self._progress(self),\n            'cumulative_risk_metrics': {},\n        }\n        ledger = self._ledger\n        ledger.end_of_session(session_ix)\n        self.end_of_session(\n            packet,\n            ledger,\n            completed_session,\n            session_ix,\n            data_portal,\n        )\n\n        return packet", "output": "Handles the close of the given day.\n\n        Parameters\n        ----------\n        dt : Timestamp\n            The most recently completed simulation datetime.\n        data_portal : DataPortal\n            The current data portal.\n\n        Returns\n        -------\n        A daily perf packet.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_blocks(lines):\n    \"\"\"\n    \"\"\"\n    cur_block = []\n    pre_lang = None\n    pre_in_code = None\n    for (l, in_code, cur_lang, _) in _parse_code_lines(lines):\n        if in_code != pre_in_code:\n            if pre_in_code and len(cur_block) >= 2:\n                cur_block = cur_block[1:-1] # remove ```\n            # remove empty lines at head\n            while len(cur_block) > 0:\n                if len(cur_block[0]) == 0:\n                    cur_block.pop(0)\n                else:\n                    break\n            # remove empty lines at tail\n            while len(cur_block) > 0:\n                if len(cur_block[-1]) == 0:\n                    cur_block.pop()\n                else:\n                    break\n            if len(cur_block):\n                yield (pre_in_code, pre_lang, cur_block)\n            cur_block = []\n        cur_block.append(l)\n        pre_lang = cur_lang\n        pre_in_code = in_code\n    if len(cur_block):\n        yield (pre_in_code, pre_lang, cur_block)", "output": "split lines into code and non-code blocks\n\n    Returns\n    -------\n    iterator of (bool, str, list of str)\n      - if it is a code block\n      - source language\n      - lines of source", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def host_impl(self, run, tool):\n    \"\"\"\n    \"\"\"\n    hosts = {}\n    run_dir = self._run_dir(run)\n    if not run_dir:\n      logger.warn(\"Cannot find asset directory for: %s\", run)\n      return hosts\n    tool_pattern = '*' + TOOLS[tool]\n    try:\n      files = tf.io.gfile.glob(os.path.join(run_dir, tool_pattern))\n      hosts = [os.path.basename(f).replace(TOOLS[tool], '') for f in files]\n    except tf.errors.OpError as e:\n      logger.warn(\"Cannot read asset directory: %s, OpError %s\",\n                      run_dir, e)\n    return hosts", "output": "Returns available hosts for the run and tool in the log directory.\n\n    In the plugin log directory, each directory contains profile data for a\n    single run (identified by the directory name), and files in the run\n    directory contains data for different tools and hosts. The file that\n    contains profile for a specific tool \"x\" will have a prefix name TOOLS[\"x\"].\n\n    Example:\n      log/\n        run1/\n          plugins/\n            profile/\n              host1.trace\n              host2.trace\n        run2/\n          plugins/\n            profile/\n              host1.trace\n              host2.trace\n\n    Returns:\n      A list of host names e.g.\n        {\"host1\", \"host2\", \"host3\"} for the example.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_rabit():\n    \"\"\"\"\"\"\n    if _LIB is not None:\n        _LIB.RabitGetRank.restype = ctypes.c_int\n        _LIB.RabitGetWorldSize.restype = ctypes.c_int\n        _LIB.RabitIsDistributed.restype = ctypes.c_int\n        _LIB.RabitVersionNumber.restype = ctypes.c_int", "output": "internal library initializer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_indicator_CHO(DataFrame, N1=10, N2=20, M=6):\n    \"\"\"\n    \n    \"\"\"\n    HIGH = DataFrame.high\n    LOW = DataFrame.low\n    CLOSE = DataFrame.close\n    VOL = DataFrame.volume\n    MID = SUM(VOL*(2*CLOSE-HIGH-LOW)/(HIGH+LOW), 0)\n    CHO = MA(MID, N1)-MA(MID, N2)\n    MACHO = MA(CHO, M)\n    return pd.DataFrame({\n        'CHO': CHO, 'MACHO': MACHO\n    })", "output": "\u4f73\u5e86\u6307\u6807 CHO", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_block_manager_axis(cls, axis):\n        \"\"\"\"\"\"\n        axis = cls._get_axis_number(axis)\n        if cls._AXIS_REVERSED:\n            m = cls._AXIS_LEN - 1\n            return m - axis\n        return axis", "output": "Map the axis to the block_manager axis.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extend( self, itemseq ):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(itemseq, ParseResults):\n            self += itemseq\n        else:\n            self.__toklist.extend(itemseq)", "output": "Add sequence of elements to end of ParseResults list of elements.\n\n        Example::\n\n            patt = OneOrMore(Word(alphas))\n\n            # use a parse action to append the reverse of the matched strings, to make a palindrome\n            def make_palindrome(tokens):\n                tokens.extend(reversed([t[::-1] for t in tokens]))\n                return ''.join(tokens)\n            print(patt.addParseAction(make_palindrome).parseString(\"lskdj sdlkjf lksd\")) # -> 'lskdjsdlkjflksddsklfjkldsjdksl'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def monkeypatch_method(cls, patch_name):\r\n    # This function's code was inspired from the following thread:\r\n    # \"[Python-Dev] Monkeypatching idioms -- elegant or ugly?\"\r\n    # by Robert Brewer <fumanchu at aminus.org>\r\n    # (Tue Jan 15 19:13:25 CET 2008)\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    def decorator(func):\r\n        fname = func.__name__\r\n        old_func = getattr(cls, fname, None)\r\n        if old_func is not None:\r\n            # Add the old func to a list of old funcs.\r\n            old_ref = \"_old_%s_%s\" % (patch_name, fname)\r\n\r\n            old_attr = getattr(cls, old_ref, None)\r\n            if old_attr is None:\r\n                setattr(cls, old_ref, old_func)\r\n            else:\r\n                raise KeyError(\"%s.%s already exists.\"\r\n                               % (cls.__name__, old_ref))\r\n        setattr(cls, fname, func)\r\n        return func\r\n    return decorator", "output": "Add the decorated method to the given class; replace as needed.\r\n\r\n    If the named method already exists on the given class, it will\r\n    be replaced, and a reference to the old method is created as\r\n    cls._old<patch_name><name>. If the \"_old_<patch_name>_<name>\" attribute\r\n    already exists, KeyError is raised.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_default_sess_config(mem_fraction=0.99):\n    \"\"\"\n    \n    \"\"\"\n    conf = tfv1.ConfigProto()\n\n    conf.allow_soft_placement = True\n    # conf.log_device_placement = True\n\n    conf.intra_op_parallelism_threads = 1\n    conf.inter_op_parallelism_threads = 0\n    # TF benchmark use cpu_count() - gpu_thread_count(), e.g. 80 - 8 * 2\n    # Didn't see much difference.\n\n    conf.gpu_options.per_process_gpu_memory_fraction = mem_fraction\n\n    # This hurt performance of large data pipeline:\n    # https://github.com/tensorflow/benchmarks/commit/1528c46499cdcff669b5d7c006b7b971884ad0e6\n    # conf.gpu_options.force_gpu_compatible = True\n\n    conf.gpu_options.allow_growth = True\n\n    # from tensorflow.core.protobuf import rewriter_config_pb2 as rwc\n    # conf.graph_options.rewrite_options.memory_optimization = \\\n    #     rwc.RewriterConfig.HEURISTICS\n\n    # May hurt performance?\n    # conf.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n    # conf.graph_options.place_pruned_graph = True\n    return conf", "output": "Return a tf.ConfigProto to use as default session config.\n    You can modify the returned config to fit your needs.\n\n    Args:\n        mem_fraction(float): see the `per_process_gpu_memory_fraction` option\n            in TensorFlow's GPUOptions protobuf:\n            https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto\n\n    Returns:\n        tf.ConfigProto: the config to use.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _compute_key(self, id, nbytes):\n        \"\"\"\"\"\"\n        m = Message()\n        m.add_mpint(self.K)\n        m.add_bytes(self.H)\n        m.add_byte(b(id))\n        m.add_bytes(self.session_id)\n        # Fallback to SHA1 for kex engines that fail to specify a hex\n        # algorithm, or for e.g. transport tests that don't run kexinit.\n        hash_algo = getattr(self.kex_engine, \"hash_algo\", None)\n        hash_select_msg = \"kex engine {} specified hash_algo {!r}\".format(\n            self.kex_engine.__class__.__name__, hash_algo\n        )\n        if hash_algo is None:\n            hash_algo = sha1\n            hash_select_msg += \", falling back to sha1\"\n        if not hasattr(self, \"_logged_hash_selection\"):\n            self._log(DEBUG, hash_select_msg)\n            setattr(self, \"_logged_hash_selection\", True)\n        out = sofar = hash_algo(m.asbytes()).digest()\n        while len(out) < nbytes:\n            m = Message()\n            m.add_mpint(self.K)\n            m.add_bytes(self.H)\n            m.add_bytes(sofar)\n            digest = hash_algo(m.asbytes()).digest()\n            out += digest\n            sofar += digest\n        return out[:nbytes]", "output": "id is 'A' - 'F' for the various keys used by ssh", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ssh_build_mic(self, session_id, username, service, auth_method):\n        \"\"\"\n        \n        \"\"\"\n        mic = self._make_uint32(len(session_id))\n        mic += session_id\n        mic += struct.pack(\"B\", MSG_USERAUTH_REQUEST)\n        mic += self._make_uint32(len(username))\n        mic += username.encode()\n        mic += self._make_uint32(len(service))\n        mic += service.encode()\n        mic += self._make_uint32(len(auth_method))\n        mic += auth_method.encode()\n        return mic", "output": "Create the SSH2 MIC filed for gssapi-with-mic.\n\n        :param str session_id: The SSH session ID\n        :param str username: The name of the user who attempts to login\n        :param str service: The requested SSH service\n        :param str auth_method: The requested SSH authentication mechanism\n        :return: The MIC as defined in RFC 4462. The contents of the\n                 MIC field are:\n                 string    session_identifier,\n                 byte      SSH_MSG_USERAUTH_REQUEST,\n                 string    user-name,\n                 string    service (ssh-connection),\n                 string    authentication-method\n                           (gssapi-with-mic or gssapi-keyex)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_nodes_full(call=None, for_output=True):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The list_nodes_full function must be called with -f or --function.'\n        )\n    creds = get_creds()\n    clc.v1.SetCredentials(creds[\"token\"], creds[\"token_pass\"])\n    servers_raw = clc.v1.Server.GetServers(location=None)\n    servers_raw = salt.utils.json.dumps(servers_raw)\n    servers = salt.utils.json.loads(servers_raw)\n    return servers", "output": "Return a list of the VMs that are on the provider", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _strip_consts(graph_def, max_const_size=32):\n    \"\"\"\n    \"\"\"\n    strip_def = tf.GraphDef()\n    for n0 in graph_def.node:\n        n = strip_def.node.add()\n        n.MergeFrom(n0)\n        if n.op == 'Const':\n            tensor = n.attr['value'].tensor\n            size = len(tensor.tensor_content)\n            if size > max_const_size:\n                tensor.tensor_content = tf.compat.as_bytes(\"<stripped %d bytes>\"%size)\n    return strip_def", "output": "Strip large constant values from graph_def.\n\n    This is mostly a utility function for graph(), and also originates here:\n    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve_resource_id_refs(self, input_dict, supported_resource_id_refs):\n        \"\"\"\n        \n        \"\"\"\n\n        if not self.can_handle(input_dict):\n            return input_dict\n\n        key = self.intrinsic_name\n        value = input_dict[key]\n\n        # Value must be an array with *at least* two elements. If not, this is invalid GetAtt syntax. We just pass along\n        # the input to CFN for it to do the \"official\" validation.\n        if not isinstance(value, list) or len(value) < 2:\n            return input_dict\n\n        value_str = self._resource_ref_separator.join(value)\n        splits = value_str.split(self._resource_ref_separator)\n        logical_id = splits[0]\n        remaining = splits[1:]  # if any\n\n        resolved_value = supported_resource_id_refs.get(logical_id)\n        return self._get_resolved_dictionary(input_dict, key, resolved_value, remaining)", "output": "Resolve resource references within a GetAtt dict.\n\n        Example:\n            { \"Fn::GetAtt\": [\"LogicalId\", \"Arn\"] }  =>  {\"Fn::GetAtt\":  [\"ResolvedLogicalId\", \"Arn\"]}\n\n\n        Theoretically, only the first element of the array can contain reference to SAM resources. The second element\n        is name of an attribute (like Arn) of the resource.\n\n        However tools like AWS CLI apply the assumption that first element of the array is a LogicalId and cannot\n        contain a 'dot'. So they break at the first dot to convert YAML tag to JSON map like this:\n\n             `!GetAtt LogicalId.Arn` => {\"Fn::GetAtt\": [ \"LogicalId\", \"Arn\" ] }\n\n        Therefore to resolve the reference, we join the array into a string, break it back up to check if it contains\n        a known reference, and resolve it if we can.\n\n        :param input_dict: Dictionary to be resolved\n        :param dict supported_resource_id_refs: Dictionary that maps old logical ids to new ones.\n        :return: Resolved dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def previous_friday(dt):\n    \"\"\"\n    \n    \"\"\"\n    if dt.weekday() == 5:\n        return dt - timedelta(1)\n    elif dt.weekday() == 6:\n        return dt - timedelta(2)\n    return dt", "output": "If holiday falls on Saturday or Sunday, use previous Friday instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _settings_from_env(self):\n        \"\"\"\"\"\"\n        return {attr: self._val_from_env(env, attr)\n                for env, attr in const.ENV_TO_ATTR.items()\n                if env in os.environ}", "output": "Loads settings from env.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_vocab(dataset):\n    \"\"\"\n    \n    \"\"\"\n    counter = nlp.data.count_tokens([w for e in dataset for s in e[:2] for w in s],\n                                    to_lower=True)\n    vocab = nlp.Vocab(counter)\n    return vocab", "output": "Build vocab given a dataset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decide_slices(self, data_shapes):\n        \"\"\"\n        \"\"\"\n        assert len(data_shapes) > 0\n        major_axis = [DataDesc.get_batch_axis(x.layout) for x in data_shapes]\n\n        for (name, shape), axis in zip(data_shapes, major_axis):\n            if axis == -1:\n                continue\n\n            batch_size = shape[axis]\n            if self.batch_size is not None:\n                assert batch_size == self.batch_size, (\"all data must have the same batch size: \"\n                                                       + (\"batch_size = %d, but \" % self.batch_size)\n                                                       + (\"%s has shape %s\" % (name, shape)))\n            else:\n                self.batch_size = batch_size\n                self.slices = _split_input_slice(self.batch_size, self.workload)\n\n        return major_axis", "output": "Decide the slices for each context according to the workload.\n\n        Parameters\n        ----------\n        data_shapes : list\n            list of (name, shape) specifying the shapes for the input data or label.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def profile_get(user, default_hidden=True):\n    '''\n    \n    '''\n    user_profiles = []\n\n    ## read user_attr file (user:qualifier:res1:res2:attr)\n    with salt.utils.files.fopen('/etc/user_attr', 'r') as user_attr:\n        for profile in user_attr:\n            profile = salt.utils.stringutils.to_unicode(profile)\n            profile = profile.strip().split(':')\n\n            # skip comments and non complaint lines\n            if len(profile) != 5:\n                continue\n\n            # skip other users\n            if profile[0] != user:\n                continue\n\n            # parse attr\n            attrs = {}\n            for attr in profile[4].strip().split(';'):\n                attr_key, attr_val = attr.strip().split('=')\n                if attr_key in ['auths', 'profiles', 'roles']:\n                    attrs[attr_key] = attr_val.strip().split(',')\n                else:\n                    attrs[attr_key] = attr_val\n            if 'profiles' in attrs:\n                user_profiles.extend(attrs['profiles'])\n\n    ## remove default profiles\n    if default_hidden:\n        for profile in profile_list(default_only=True):\n            if profile in user_profiles:\n                user_profiles.remove(profile)\n\n    return list(set(user_profiles))", "output": "List profiles for user\n\n    user : string\n        username\n    default_hidden : boolean\n        hide default profiles\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rbac.profile_get leo\n        salt '*' rbac.profile_get leo default_hidden=False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _reverse_dependencies(self, cache_keys):\n        \"\"\"\n        \n\n        \"\"\"\n        # First, collect all the dependencies into a sequence of (parent, child) tuples, like [('flake8', 'pep8'),\n        # ('flake8', 'mccabe'), ...]\n        return lookup_table((key_from_req(Requirement(dep_name)), name)\n                            for name, version_and_extras in cache_keys\n                            for dep_name in self.cache[name][version_and_extras])", "output": "Returns a lookup table of reverse dependencies for all the given cache keys.\n\n        Example input:\n\n            [('pep8', '1.5.7'),\n             ('flake8', '2.4.0'),\n             ('mccabe', '0.3'),\n             ('pyflakes', '0.8.1')]\n\n        Example output:\n\n            {'pep8': ['flake8'],\n             'flake8': [],\n             'mccabe': ['flake8'],\n             'pyflakes': ['flake8']}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def connect_post_namespaced_pod_portforward(self, name, namespace, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_post_namespaced_pod_portforward_with_http_info(name, namespace, **kwargs)\n        else:\n            (data) = self.connect_post_namespaced_pod_portforward_with_http_info(name, namespace, **kwargs)\n            return data", "output": "connect POST requests to portforward of Pod\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_post_namespaced_pod_portforward(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodPortForwardOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param int ports: List of ports to forward Required when using WebSockets\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compare(self, statement_a, statement_b):\n        \"\"\"\n        \n        \"\"\"\n        document_a = self.nlp(statement_a.text)\n        document_b = self.nlp(statement_b.text)\n\n        return document_a.similarity(document_b)", "output": "Compare the two input statements.\n\n        :return: The percent of similarity between the closest synset distance.\n        :rtype: float", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_blob(storage_conn=None, **kwargs):\n    '''\n    \n    '''\n    if not storage_conn:\n        storage_conn = get_storage_conn(opts=kwargs)\n\n    if 'container' not in kwargs:\n        raise SaltSystemExit(code=42, msg='The blob container name must be specified as \"container\"')\n\n    if 'name' not in kwargs:\n        raise SaltSystemExit(code=42, msg='The blob name must be specified as \"name\"')\n\n    if 'local_path' not in kwargs and 'return_content' not in kwargs:\n        raise SaltSystemExit(\n            code=42,\n            msg='Either a local path needs to be passed in as \"local_path\", '\n            'or \"return_content\" to return the blob contents directly'\n        )\n\n    blob_kwargs = {\n        'container_name': kwargs['container'],\n        'blob_name': kwargs['name'],\n        'snapshot': kwargs.get('snapshot', None),\n        'x_ms_lease_id': kwargs.get('lease_id', None),\n        'progress_callback': kwargs.get('progress_callback', None),\n        'max_connections': kwargs.get('max_connections', 1),\n        'max_retries': kwargs.get('max_retries', 5),\n        'retry_wait': kwargs.get('retry_wait', 1),\n    }\n\n    if 'local_path' in kwargs:\n        data = storage_conn.get_blob_to_path(\n            file_path=kwargs['local_path'],\n            open_mode=kwargs.get('open_mode', 'wb'),\n            **blob_kwargs\n        )\n    elif 'return_content' in kwargs:\n        data = storage_conn.get_blob_to_bytes(\n            **blob_kwargs\n        )\n\n    return data", "output": ".. versionadded:: 2015.8.0\n\n    Download a blob", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_transfer_encoding(self) -> None:\n        \"\"\"\"\"\"\n        te = self.headers.get(hdrs.TRANSFER_ENCODING, '').lower()\n\n        if 'chunked' in te:\n            if self.chunked:\n                raise ValueError(\n                    'chunked can not be set '\n                    'if \"Transfer-Encoding: chunked\" header is set')\n\n        elif self.chunked:\n            if hdrs.CONTENT_LENGTH in self.headers:\n                raise ValueError(\n                    'chunked can not be set '\n                    'if Content-Length header is set')\n\n            self.headers[hdrs.TRANSFER_ENCODING] = 'chunked'\n        else:\n            if hdrs.CONTENT_LENGTH not in self.headers:\n                self.headers[hdrs.CONTENT_LENGTH] = str(len(self.body))", "output": "Analyze transfer-encoding header.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _walk_omniglot_dir(directory):\n  \"\"\"\"\"\"\n  directory = os.path.join(directory, tf.io.gfile.listdir(directory)[0])\n  alphabets = sorted(tf.io.gfile.listdir(directory))\n  for alphabet in alphabets:\n    alphabet_dir = os.path.join(directory, alphabet)\n    characters = sorted(tf.io.gfile.listdir(alphabet_dir))\n    for character in characters:\n      character_id = int(character[len(\"character\"):]) - 1\n      character_dir = os.path.join(alphabet_dir, character)\n      images = tf.io.gfile.listdir(character_dir)\n      for image in images:\n        label, _ = image.split(\"_\")\n        label = int(label) - 1\n        image_path = os.path.join(character_dir, image)\n        yield alphabet, character_id, label, image_path", "output": "Walk an Omniglot directory and yield examples.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nack(self):\n        \"\"\"\n        \"\"\"\n        self._request_queue.put(\n            requests.NackRequest(ack_id=self._ack_id, byte_size=self.size)\n        )", "output": "Decline to acknowldge the given message.\n\n        This will cause the message to be re-delivered to the subscription.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_logs(optimizer, logs):\n    \"\"\"\n\n    \"\"\"\n    import json\n\n    if isinstance(logs, str):\n        logs = [logs]\n\n    for log in logs:\n        with open(log, \"r\") as j:\n            while True:\n                try:\n                    iteration = next(j)\n                except StopIteration:\n                    break\n\n                iteration = json.loads(iteration)\n                try:\n                    optimizer.register(\n                        params=iteration[\"params\"],\n                        target=iteration[\"target\"],\n                    )\n                except KeyError:\n                    pass\n\n    return optimizer", "output": "Load previous ...", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lvdisplay(lvname='', quiet=False):\n    '''\n    \n    '''\n    ret = {}\n    cmd = ['lvdisplay', '-c']\n    if lvname:\n        cmd.append(lvname)\n    cmd_ret = __salt__['cmd.run_all'](cmd, python_shell=False,\n                                      ignore_retcode=quiet)\n\n    if cmd_ret['retcode'] != 0:\n        return {}\n\n    out = cmd_ret['stdout'].splitlines()\n    for line in out:\n        comps = line.strip().split(':')\n        ret[comps[0]] = {\n            'Logical Volume Name': comps[0],\n            'Volume Group Name': comps[1],\n            'Logical Volume Access': comps[2],\n            'Logical Volume Status': comps[3],\n            'Internal Logical Volume Number': comps[4],\n            'Open Logical Volumes': comps[5],\n            'Logical Volume Size': comps[6],\n            'Current Logical Extents Associated': comps[7],\n            'Allocated Logical Extents': comps[8],\n            'Allocation Policy': comps[9],\n            'Read Ahead Sectors': comps[10],\n            'Major Device Number': comps[11],\n            'Minor Device Number': comps[12],\n            }\n    return ret", "output": "Return information about the logical volume(s)\n\n    lvname\n        logical device name\n\n    quiet\n        if the logical volume is not present, do not show any error\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' lvm.lvdisplay\n        salt '*' lvm.lvdisplay /dev/vg_myserver/root", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mean(self, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        nv.validate_groupby_func('mean', args, kwargs, ['numeric_only'])\n        try:\n            return self._cython_agg_general('mean', **kwargs)\n        except GroupByError:\n            raise\n        except Exception:  # pragma: no cover\n            with _group_selection_context(self):\n                f = lambda x: x.mean(axis=self.axis, **kwargs)\n                return self._python_agg_general(f)", "output": "Compute mean of groups, excluding missing values.\n\n        Returns\n        -------\n        pandas.Series or pandas.DataFrame\n        %(see_also)s\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n        ...                    'B': [np.nan, 2, 3, 4, 5],\n        ...                    'C': [1, 2, 1, 1, 2]}, columns=['A', 'B', 'C'])\n\n        Groupby one column and return the mean of the remaining columns in\n        each group.\n\n        >>> df.groupby('A').mean()\n        >>>\n             B         C\n        A\n        1  3.0  1.333333\n        2  4.0  1.500000\n\n        Groupby two columns and return the mean of the remaining column.\n\n        >>> df.groupby(['A', 'B']).mean()\n        >>>\n               C\n        A B\n        1 2.0  2\n          4.0  1\n        2 3.0  1\n          5.0  2\n\n        Groupby one column and return the mean of only particular column in\n        the group.\n\n        >>> df.groupby('A')['B'].mean()\n        >>>\n        A\n        1    3.0\n        2    4.0\n        Name: B, dtype: float64", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_elementwise_add(net, node, module, builder):\n    \"\"\"\n    \"\"\"\n\n    input_names, output_name = _get_input_output_name(net, node, [0, 1])\n    name = node['name']\n\n    builder.add_elementwise(name, input_names, output_name, 'ADD')", "output": "Convert an elementwise add layer from mxnet to coreml.\n\n    Parameters\n    ----------\n    network: net\n        A mxnet network object.\n\n    layer: node\n        Node to convert.\n\n    module: module\n        An module for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def all_derived (type):\n    \"\"\" \n    \"\"\"\n    assert isinstance(type, basestring)\n    result = [type]\n    for d in __types [type]['derived']:\n        result.extend (all_derived (d))\n\n    return result", "output": "Returns type and all classes that derive from it, in the order of their distance from type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _index_resized(self, col, old_width, new_width):\r\n        \"\"\"\"\"\"\r\n        self.table_index.setColumnWidth(col, new_width)\r\n        self._update_layout()", "output": "Resize the corresponding column of the index section selected.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _gen_md5_filehash(fname, *args):\n    '''\n    \n    '''\n    _hash = hashlib.md5()\n    with salt.utils.files.fopen(fname, 'rb') as f:\n        for chunk in iter(lambda: f.read(4096), b''):\n            _hash.update(chunk)\n\n    for extra_arg in args:\n        _hash.update(six.b(str(extra_arg)))\n    return _hash.hexdigest()", "output": "helper function to generate a md5 hash of the swagger definition file\n    any extra argument passed to the function is converted to a string\n    and participates in the hash calculation", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def default(self, obj):\n        ''' \n\n        '''\n\n        from ..model import Model\n        from ..colors import Color\n        from .has_props import HasProps\n\n        # array types -- use force_list here, only binary\n        # encoding CDS columns for now\n        if pd and isinstance(obj, (pd.Series, pd.Index)):\n            return transform_series(obj, force_list=True)\n        elif isinstance(obj, np.ndarray):\n            return transform_array(obj, force_list=True)\n        elif isinstance(obj, collections.deque):\n            return list(map(self.default, obj))\n        elif isinstance(obj, Model):\n            return obj.ref\n        elif isinstance(obj, HasProps):\n            return obj.properties_with_values(include_defaults=False)\n        elif isinstance(obj, Color):\n            return obj.to_css()\n\n        else:\n            return self.transform_python_types(obj)", "output": "The required ``default`` method for ``JSONEncoder`` subclasses.\n\n        Args:\n            obj (obj) :\n\n                The object to encode. Anything not specifically handled in\n                this method is passed on to the default system JSON encoder.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_tojson(eval_ctx, value, indent=None):\n    \"\"\"\n    \"\"\"\n    policies = eval_ctx.environment.policies\n    dumper = policies['json.dumps_function']\n    options = policies['json.dumps_kwargs']\n    if indent is not None:\n        options = dict(options)\n        options['indent'] = indent\n    return htmlsafe_json_dumps(value, dumper=dumper, **options)", "output": "Dumps a structure to JSON so that it's safe to use in ``<script>``\n    tags.  It accepts the same arguments and returns a JSON string.  Note that\n    this is available in templates through the ``|tojson`` filter which will\n    also mark the result as safe.  Due to how this function escapes certain\n    characters this is safe even if used outside of ``<script>`` tags.\n\n    The following characters are escaped in strings:\n\n    -   ``<``\n    -   ``>``\n    -   ``&``\n    -   ``'``\n\n    This makes it safe to embed such strings in any place in HTML with the\n    notable exception of double quoted attributes.  In that case single\n    quote your attributes or HTML escape it in addition.\n\n    The indent parameter can be used to enable pretty printing.  Set it to\n    the number of spaces that the structures should be indented with.\n\n    Note that this filter is for use in HTML contexts only.\n\n    .. versionadded:: 2.9", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_scalar_multiply(net, node, model, builder):\n    \"\"\"\n    \"\"\"\n    import numpy as _np\n    input_name, output_name = _get_input_output_name(net, node)\n    name = node['name']\n    param = _get_attr(node)\n    alpha = _np.array([float(param['scalar'])])\n    builder.add_scale(name = name, input_name = input_name,\n            output_name = output_name, W = alpha, has_bias=False, b=None)", "output": "Convert a scalar multiply layer from mxnet to coreml.\n\n    Parameters\n    ----------\n    net: network\n        A mxnet network object.\n\n    node: layer\n        Node to convert.\n\n    model: model\n        An model for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_figure_as(self, fig, fmt):\n        \"\"\"\"\"\"\n        fext, ffilt = {\n            'image/png': ('.png', 'PNG (*.png)'),\n            'image/jpeg': ('.jpg', 'JPEG (*.jpg;*.jpeg;*.jpe;*.jfif)'),\n            'image/svg+xml': ('.svg', 'SVG (*.svg);;PNG (*.png)')}[fmt]\n\n        figname = get_unique_figname(getcwd_or_home(), 'Figure', fext)\n\n        self.redirect_stdio.emit(False)\n        fname, fext = getsavefilename(\n            parent=self.parent(), caption='Save Figure',\n            basedir=figname, filters=ffilt,\n            selectedfilter='', options=None)\n        self.redirect_stdio.emit(True)\n\n        if fname:\n            save_figure_tofile(fig, fmt, fname)", "output": "Save the figure to a file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_nics(vm_):\n    '''\n    \n    '''\n    nics = []\n    if 'public_lan' in vm_:\n        firewall_rules = []\n        # Set LAN to public if it already exists, otherwise create a new\n        # public LAN.\n        if 'public_firewall_rules' in vm_:\n            firewall_rules = _get_firewall_rules(vm_['public_firewall_rules'])\n        nic = NIC(lan=set_public_lan(int(vm_['public_lan'])),\n                  name='public',\n                  firewall_rules=firewall_rules)\n        if 'public_ips' in vm_:\n            nic.ips = _get_ip_addresses(vm_['public_ips'])\n        nics.append(nic)\n\n    if 'private_lan' in vm_:\n        firewall_rules = []\n        if 'private_firewall_rules' in vm_:\n            firewall_rules = _get_firewall_rules(vm_['private_firewall_rules'])\n        nic = NIC(lan=int(vm_['private_lan']),\n                  name='private',\n                  firewall_rules=firewall_rules)\n        if 'private_ips' in vm_:\n            nic.ips = _get_ip_addresses(vm_['private_ips'])\n        if 'nat' in vm_ and 'private_ips' not in vm_:\n            nic.nat = vm_['nat']\n        nics.append(nic)\n    return nics", "output": "Create network interfaces on appropriate LANs as defined in cloud profile.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop(self) -> None:\n        \"\"\"\"\"\"\n        self._running = False\n        if self._timeout is not None:\n            self.io_loop.remove_timeout(self._timeout)\n            self._timeout = None", "output": "Stops the timer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_and_add_parameters(params):\n    '''\n    \n    '''\n    global _current_parameter\n    if _is_simple_type(params):\n        _current_parameter = SimpleParameter(params)\n        _current_option.add_parameter(_current_parameter)\n    else:\n        # must be a list\n        for i in params:\n            if _is_simple_type(i):\n                _current_parameter = SimpleParameter(i)\n            else:\n                _current_parameter = TypedParameter()\n                _parse_typed_parameter(i)\n            _current_option.add_parameter(_current_parameter)", "output": "Parses the configuration and creates Parameter instances.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_autosign_dir(self, keyid):\n        '''\n        \n        '''\n        autosign_dir = os.path.join(self.opts['pki_dir'], 'minions_autosign')\n\n        # cleanup expired files\n        expire_minutes = self.opts.get('autosign_timeout', 120)\n        if expire_minutes > 0:\n            min_time = time.time() - (60 * int(expire_minutes))\n            for root, dirs, filenames in salt.utils.path.os_walk(autosign_dir):\n                for f in filenames:\n                    stub_file = os.path.join(autosign_dir, f)\n                    mtime = os.path.getmtime(stub_file)\n                    if mtime < min_time:\n                        log.warning('Autosign keyid expired %s', stub_file)\n                        os.remove(stub_file)\n\n        stub_file = os.path.join(autosign_dir, keyid)\n        if not os.path.exists(stub_file):\n            return False\n        os.remove(stub_file)\n        return True", "output": "Check a keyid for membership in a autosign directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enable_api_key(apiKey, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        response = _api_key_patch_replace(conn, apiKey, '/enabled', 'True')\n        return {'apiKey': _convert_datetime_str(response)}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "enable the given apiKey.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_apigateway.enable_api_key api_key", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_jids():\n    '''\n    \n    '''\n    ret = {}\n    for returner_ in __opts__[CONFIG_KEY]:\n        ret.update(_mminion().returners['{0}.get_jids'.format(returner_)]())\n\n    return ret", "output": "Return all job data from all returners", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _callable_func(self, func, axis, *args, **kwargs):\n        \"\"\"\n        \"\"\"\n\n        def callable_apply_builder(df, axis=0):\n            if not axis:\n                df.index = index\n                df.columns = pandas.RangeIndex(len(df.columns))\n            else:\n                df.columns = index\n                df.index = pandas.RangeIndex(len(df.index))\n            result = df.apply(func, axis=axis, *args, **kwargs)\n            return result\n\n        index = self.index if not axis else self.columns\n        func_prepared = self._build_mapreduce_func(callable_apply_builder, axis=axis)\n        result_data = self._map_across_full_axis(axis, func_prepared)\n        return self._post_process_apply(result_data, axis)", "output": "Apply callable functions across given axis.\n\n        Args:\n            func: The functions to apply.\n            axis: Target axis to apply the function along.\n\n        Returns:\n            A new PandasQueryCompiler.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def query_item(name, query_string, order='Rank'):\n    '''\n    \n    '''\n    status, result = _query(\n        action=name,\n        args={'query': query_string,\n              'order': order}\n    )\n    return result", "output": "Query a type of record for one or more items. Requires a valid query string.\n    See https://rally1.rallydev.com/slm/doc/webservice/introduction.jsp for\n    information on query syntax.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion rallydev.query_<item name> <query string> [<order>]\n        salt myminion rallydev.query_task '(Name contains github)'\n        salt myminion rallydev.query_task '(Name contains reactor)' Rank", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def scroll_constrain (self):\n        ''''''\n\n        if self.scroll_row_start <= 0:\n            self.scroll_row_start = 1\n        if self.scroll_row_end > self.rows:\n            self.scroll_row_end = self.rows", "output": "This keeps the scroll region within the screen region.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bash(command=\"bash\"):\n    \"\"\"\"\"\"\n    bashrc = os.path.join(os.path.dirname(__file__), 'bashrc.sh')\n    child = pexpect.spawn(command, ['--rcfile', bashrc], echo=False,\n                          encoding='utf-8')\n\n    # If the user runs 'env', the value of PS1 will be in the output. To avoid\n    # replwrap seeing that as the next prompt, we'll embed the marker characters\n    # for invisible characters in the prompt; these show up when inspecting the\n    # environment variable, but not when bash displays the prompt.\n    ps1 = PEXPECT_PROMPT[:5] + u'\\\\[\\\\]' + PEXPECT_PROMPT[5:]\n    ps2 = PEXPECT_CONTINUATION_PROMPT[:5] + u'\\\\[\\\\]' + PEXPECT_CONTINUATION_PROMPT[5:]\n    prompt_change = u\"PS1='{0}' PS2='{1}' PROMPT_COMMAND=''\".format(ps1, ps2)\n\n    return REPLWrapper(child, u'\\\\$', prompt_change,\n                       extra_init_cmd=\"export PAGER=cat\")", "output": "Start a bash shell and return a :class:`REPLWrapper` object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def contract_multiplier(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            return self.__dict__[\"contract_multiplier\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'contract_multiplier' \".format(self.order_book_id)\n            )", "output": "[float] \u5408\u7ea6\u4e58\u6570\uff0c\u4f8b\u5982\u6caa\u6df1300\u80a1\u6307\u671f\u8d27\u7684\u4e58\u6570\u4e3a300.0\uff08\u671f\u8d27\u4e13\u7528\uff09", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def combineByKey(self, createCombiner, mergeValue, mergeCombiners,\n                     numPartitions=None):\n        \"\"\"\n        \n        \"\"\"\n        if numPartitions is None:\n            numPartitions = self._sc.defaultParallelism\n\n        def func(rdd):\n            return rdd.combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions)\n        return self.transform(func)", "output": "Return a new DStream by applying combineByKey to each RDD.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sysfs_attr(name, value=None, log_lvl=None, log_msg=None):\n    '''\n    \n    '''\n    if isinstance(name, six.string_types):\n        name = [name]\n    res = __salt__['sysfs.attr'](os.path.join(*name), value)\n    if not res and log_lvl is not None and log_msg is not None:\n        log.log(LOG[log_lvl], log_msg)\n    return res", "output": "Simple wrapper with logging around sysfs.attr", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_cname(name, data, **api_opts):\n    '''\n    \n    '''\n    o = get_cname(name=name, **api_opts)\n    if not o:\n        raise Exception('CNAME record not found')\n    return update_object(objref=o['_ref'], data=data, **api_opts)", "output": "Update CNAME. This is a helper call to update_object.\n\n    Find a CNAME ``_ref`` then call update_object with the record data.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call infoblox.update_cname name=example.example.com data=\"{\n                'canonical':'example-ha-0.example.com',\n                'use_ttl':true,\n                'ttl':200,\n                'comment':'Salt managed CNAME'}\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_new_command(command):\n    \"\"\"\n    \n    \"\"\"\n    dest = command.script_parts[1].split(os.sep)\n    if dest[-1] == '':\n        dest = dest[:-1]\n\n    if dest[0] == '':\n        cwd = os.sep\n        dest = dest[1:]\n    elif six.PY2:\n        cwd = os.getcwdu()\n    else:\n        cwd = os.getcwd()\n    for directory in dest:\n        if directory == \".\":\n            continue\n        elif directory == \"..\":\n            cwd = os.path.split(cwd)[0]\n            continue\n        best_matches = get_close_matches(directory, _get_sub_dirs(cwd), cutoff=MAX_ALLOWED_DIFF)\n        if best_matches:\n            cwd = os.path.join(cwd, best_matches[0])\n        else:\n            return cd_mkdir.get_new_command(command)\n    return u'cd \"{0}\"'.format(cwd)", "output": "Attempt to rebuild the path string by spellchecking the directories.\n    If it fails (i.e. no directories are a close enough match), then it\n    defaults to the rules of cd_mkdir.\n    Change sensitivity by changing MAX_ALLOWED_DIFF. Default value is 0.6", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def readlink(path):\n    '''\n    \n    '''\n    if sys.getwindowsversion().major < 6:\n        raise SaltInvocationError('Symlinks are only supported on Windows Vista or later.')\n\n    try:\n        return salt.utils.path.readlink(path)\n    except OSError as exc:\n        if exc.errno == errno.EINVAL:\n            raise CommandExecutionError('{0} is not a symbolic link'.format(path))\n        raise CommandExecutionError(exc.__str__())\n    except Exception as exc:\n        raise CommandExecutionError(exc)", "output": "Return the path that a symlink points to\n\n    This is only supported on Windows Vista or later.\n\n    Inline with Unix behavior, this function will raise an error if the path is\n    not a symlink, however, the error raised will be a SaltInvocationError, not\n    an OSError.\n\n    Args:\n        path (str): The path to the symlink\n\n    Returns:\n        str: The path that the symlink points to\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.readlink /path/to/link", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_data(self, text, colsep=u\"\\t\", rowsep=u\"\\n\",\r\n                     transpose=False, skiprows=0, comments='#'):\r\n        \"\"\"\"\"\"\r\n        data = self._shape_text(text, colsep, rowsep, transpose, skiprows,\r\n                                comments)\r\n        self._model = PreviewTableModel(data)\r\n        self.setModel(self._model)", "output": "Put data into table model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _try_cast(self, result, obj, numeric_only=False):\n        \"\"\"\n        \n\n        \"\"\"\n        if obj.ndim > 1:\n            dtype = obj._values.dtype\n        else:\n            dtype = obj.dtype\n\n        if not is_scalar(result):\n            if is_datetime64tz_dtype(dtype):\n                # GH 23683\n                # Prior results _may_ have been generated in UTC.\n                # Ensure we localize to UTC first before converting\n                # to the target timezone\n                try:\n                    result = obj._values._from_sequence(\n                        result, dtype='datetime64[ns, UTC]'\n                    )\n                    result = result.astype(dtype)\n                except TypeError:\n                    # _try_cast was called at a point where the result\n                    # was already tz-aware\n                    pass\n            elif is_extension_array_dtype(dtype):\n                # The function can return something of any type, so check\n                # if the type is compatible with the calling EA.\n                try:\n                    result = obj._values._from_sequence(result, dtype=dtype)\n                except Exception:\n                    # https://github.com/pandas-dev/pandas/issues/22850\n                    # pandas has no control over what 3rd-party ExtensionArrays\n                    # do in _values_from_sequence. We still want ops to work\n                    # though, so we catch any regular Exception.\n                    pass\n            elif numeric_only and is_numeric_dtype(dtype) or not numeric_only:\n                result = maybe_downcast_to_dtype(result, dtype)\n\n        return result", "output": "Try to cast the result to our obj original type,\n        we may have roundtripped through object in the mean-time.\n\n        If numeric_only is True, then only try to cast numerics\n        and not datetimelikes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_sanitize_files(self):\n        \"\"\"\n        \n\n        \"\"\"\n        if self.parent.config.option.sanitize_with is not None:\n            return [self.parent.config.option.sanitize_with]\n        else:\n            return []", "output": "Return list of all sanitize files provided by the user on the command line.\n\n        N.B.: We only support one sanitize file at the moment, but\n              this is likely to change in the future", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rest_get(url, timeout):\n    ''''''\n    try:\n        response = requests.get(url, timeout=timeout)\n        return response\n    except Exception as e:\n        print('Get exception {0} when sending http get to url {1}'.format(str(e), url))\n        return None", "output": "Call rest get method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generator(self, z):\n        \"\"\" \"\"\"\n        nf = 64\n        l = FullyConnected('fc0', z, nf * 8 * 4 * 4, activation=tf.identity)\n        l = tf.reshape(l, [-1, 4, 4, nf * 8])\n        l = BNReLU(l)\n        with argscope(Conv2DTranspose, activation=BNReLU, kernel_size=4, strides=2):\n            l = Conv2DTranspose('deconv1', l, nf * 4)\n            l = Conv2DTranspose('deconv2', l, nf * 2)\n            l = Conv2DTranspose('deconv3', l, nf)\n            l = Conv2DTranspose('deconv4', l, 3, activation=tf.identity)\n            l = tf.tanh(l, name='gen')\n        return l", "output": "return an image generated from z", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_cli_param(params, key, value):\n    '''\n    \n    '''\n    if value is not None:\n        params.append('--{0}={1}'.format(key, value))", "output": "Adds key and value as a command line parameter to params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe(self, **kwargs):\n        \"\"\"\n        \"\"\"\n        # Use pandas to calculate the correct columns\n        new_columns = (\n            pandas.DataFrame(columns=self.columns)\n            .astype(self.dtypes)\n            .describe(**kwargs)\n            .columns\n        )\n\n        def describe_builder(df, internal_indices=[], **kwargs):\n            return df.iloc[:, internal_indices].describe(**kwargs)\n\n        # Apply describe and update indices, columns, and dtypes\n        func = self._prepare_method(describe_builder, **kwargs)\n        new_data = self._full_axis_reduce_along_select_indices(func, 0, new_columns)\n        new_index = self.compute_index(0, new_data, False)\n        return self.__constructor__(new_data, new_index, new_columns)", "output": "Generates descriptive statistics.\n\n        Returns:\n            DataFrame object containing the descriptive statistics of the DataFrame.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_next_page_response(self):\n        \"\"\"\n        \"\"\"\n        params = self._get_query_params()\n        if self._HTTP_METHOD == \"GET\":\n            return self.api_request(\n                method=self._HTTP_METHOD, path=self.path, query_params=params\n            )\n        elif self._HTTP_METHOD == \"POST\":\n            return self.api_request(\n                method=self._HTTP_METHOD, path=self.path, data=params\n            )\n        else:\n            raise ValueError(\"Unexpected HTTP method\", self._HTTP_METHOD)", "output": "Requests the next page from the path provided.\n\n        Returns:\n            dict: The parsed JSON response of the next page's contents.\n\n        Raises:\n            ValueError: If the HTTP method is not ``GET`` or ``POST``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Unpack(self, msg):\n    \"\"\"\"\"\"\n    descriptor = msg.DESCRIPTOR\n    if not self.Is(descriptor):\n      return False\n    msg.ParseFromString(self.value)\n    return True", "output": "Unpacks the current Any message into specified message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_toplosses(cls, learn, n_imgs=None, **kwargs):\n        \"\"\n        train_ds, train_idxs = cls.get_toplosses_idxs(learn, n_imgs, **kwargs)\n        return train_ds, train_idxs", "output": "Gets indices with top losses.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_api_resources(restApiId, path,\n                         region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    path_parts = path.split('/')\n    created = []\n    current_path = ''\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        for path_part in path_parts:\n            if current_path == '/':\n                current_path = '{0}{1}'.format(current_path, path_part)\n            else:\n                current_path = '{0}/{1}'.format(current_path, path_part)\n            r = describe_api_resource(restApiId, current_path,\n                                      region=region, key=key, keyid=keyid, profile=profile)\n            resource = r.get('resource')\n            if not resource:\n                resource = conn.create_resource(restApiId=restApiId, parentId=created[-1]['id'], pathPart=path_part)\n            created.append(resource)\n\n        if created:\n            return {'created': True, 'restApiId': restApiId, 'resources': created}\n        else:\n            return {'created': False, 'error': 'unexpected error.'}\n    except ClientError as e:\n        return {'created': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Given rest api id, and an absolute resource path, create all the resources and\n    return all resources in the resourcepath, returns False on failure.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_apigateway.create_api_resources myapi_id resource_path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(self, client=None):\n        \"\"\"\n        \"\"\"\n        client = self._require_client(client)\n        client._connection.api_request(method=\"DELETE\", path=self.path)", "output": "API call:  delete the zone via a DELETE request\n\n        See\n        https://cloud.google.com/dns/api/v1/managedZones/delete\n\n        :type client: :class:`google.cloud.dns.client.Client`\n        :param client:\n            (Optional) the client to use.  If not passed, falls back to the\n            ``client`` stored on the current zone.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_color_scheme(self, color_scheme, reset=True):\n        \"\"\"\"\"\"\n        self.set_bracket_matcher_color_scheme(color_scheme)\n        self.style_sheet, dark_color = create_qss_style(color_scheme)\n        self.syntax_style = color_scheme\n        self._style_sheet_changed()\n        self._syntax_style_changed()\n        if reset:\n            self.reset(clear=True)\n        if not dark_color:\n            self.silent_execute(\"%colors linux\")\n        else:\n            self.silent_execute(\"%colors lightbg\")", "output": "Set color scheme of the shell.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def usage(title, message, tutorial_message, tutorial, css_path=CSS_PATH):\n    \"\"\"\"\"\"\n    env = Environment()\n    env.loader = FileSystemLoader(osp.join(CONFDIR_PATH, 'templates'))\n    usage = env.get_template(\"usage.html\")\n    return usage.render(css_path=css_path, title=title, intro_message=message,\n                        tutorial_message=tutorial_message, tutorial=tutorial)", "output": "Print a usage message on the rich text view", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_sequence_with_spacing(self, # pylint: disable=no-self-use\n                                   new_grammar,\n                                   expressions: List[Expression],\n                                   name: str = '') -> Sequence:\n        \"\"\"\n        \n        \"\"\"\n        expressions = [subexpression\n                       for expression in expressions\n                       for subexpression in (expression, new_grammar['ws'])]\n        return Sequence(*expressions, name=name)", "output": "This is a helper method for generating sequences, since we often want a list of expressions\n        with whitespaces between them.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def line_stack(self, x, y, **kw):\n        ''' \n\n        '''\n        if all(isinstance(val, (list, tuple)) for val in (x,y)):\n            raise ValueError(\"Only one of x or y may be a list of stackers\")\n\n        result = []\n\n        if isinstance(y, (list, tuple)):\n            kw['x'] = x\n            for kw in _single_stack(y, \"y\", **kw):\n                result.append(self.line(**kw))\n            return result\n\n        if isinstance(x, (list, tuple)):\n            kw['y'] = y\n            for kw in _single_stack(x, \"x\", **kw):\n                result.append(self.line(**kw))\n            return result\n\n        return [self.line(x, y, **kw)]", "output": "Generate multiple ``Line`` renderers for lines stacked vertically\n        or horizontally.\n\n        Args:\n            x (seq[str]) :\n\n            y (seq[str]) :\n\n        Additionally, the ``name`` of the renderer will be set to\n        the value of each successive stacker (this is useful with the\n        special hover variable ``$name``)\n\n        Any additional keyword arguments are passed to each call to ``hbar``.\n        If a keyword value is a list or tuple, then each call will get one\n        value from the sequence.\n\n        Returns:\n            list[GlyphRenderer]\n\n        Examples:\n\n            Assuming a ``ColumnDataSource`` named ``source`` with columns\n            *2106* and *2017*, then the following call to ``line_stack`` with\n            stackers for the y-coordinates will will create two ``Line``\n            renderers that stack:\n\n            .. code-block:: python\n\n                p.line_stack(['2016', '2017'], x='x', color=['blue', 'red'], source=source)\n\n            This is equivalent to the following two separate calls:\n\n            .. code-block:: python\n\n                p.line(y=stack('2016'),         x='x', color='blue', source=source, name='2016')\n                p.line(y=stack('2016', '2017'), x='x', color='red',  source=source, name='2017')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def class_balanced_sigmoid_cross_entropy(logits, label, name='cross_entropy_loss'):\n    \"\"\"\n    \n    \"\"\"\n    with tf.name_scope('class_balanced_sigmoid_cross_entropy'):\n        y = tf.cast(label, tf.float32)\n\n        count_neg = tf.reduce_sum(1. - y)\n        count_pos = tf.reduce_sum(y)\n        beta = count_neg / (count_neg + count_pos)\n\n        pos_weight = beta / (1 - beta)\n        cost = tf.nn.weighted_cross_entropy_with_logits(logits=logits, targets=y, pos_weight=pos_weight)\n        cost = tf.reduce_mean(cost * (1 - beta))\n        zero = tf.equal(count_pos, 0.0)\n    return tf.where(zero, 0.0, cost, name=name)", "output": "The class-balanced cross entropy loss,\n    as in `Holistically-Nested Edge Detection\n    <http://arxiv.org/abs/1504.06375>`_.\n\n    Args:\n        logits: of shape (b, ...).\n        label: of the same shape. the ground truth in {0,1}.\n    Returns:\n        class-balanced cross entropy loss.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _encode_wiki_sections(sections, vocab):\n  \"\"\"\"\"\"\n  ids = []\n  section_boundaries = []\n  for i, section in enumerate(sections):\n    if i > 0:\n      # Skip including article title\n      ids.extend(vocab.encode(_format_title(_normalize_text(section.title))))\n    ids.extend(vocab.encode(_normalize_text(section.text)))\n    section_boundaries.append(len(ids))\n\n  return ids, section_boundaries", "output": "Encodes sections with vocab. Returns ids and section boundaries.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def policy_definitions_list(hide_builtin=False, **kwargs):\n    '''\n    \n\n    '''\n    result = {}\n    polconn = __utils__['azurearm.get_client']('policy', **kwargs)\n    try:\n        policy_defs = __utils__['azurearm.paged_object_to_list'](polconn.policy_definitions.list())\n\n        for policy in policy_defs:\n            if not (hide_builtin and policy['policy_type'] == 'BuiltIn'):\n                result[policy['name']] = policy\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('resource', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    List all policy definitions for a subscription.\n\n    :param hide_builtin: Boolean which will filter out BuiltIn policy definitions from the result.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_resource.policy_definitions_list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dev_examples(self, data_dir):\n        \"\"\"\"\"\"\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"dev_matched.tsv\")),\n            \"dev_matched\")", "output": "See base class.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def epochs(steps=None, epoch_steps=1):\n  \"\"\"\n  \"\"\"\n  try:\n    iter(epoch_steps)\n  except TypeError:\n    epoch_steps = itertools.repeat(epoch_steps)\n\n  step = 0\n  for epoch, epoch_steps in enumerate(epoch_steps):\n    epoch_steps = min(epoch_steps, steps - step)\n    yield (epoch + 1, epoch_steps)\n    step += epoch_steps\n    if steps and step >= steps:\n      break", "output": "Iterator over epochs until steps is reached. 1-indexed.\n\n  Args:\n    steps: int, total number of steps. Infinite if None.\n    epoch_steps: int, number of steps per epoch. Can also be an iterable<int> to\n      enable variable length epochs.\n\n  Yields:\n    (epoch: int, epoch id, epoch_steps: int, number of steps in this epoch)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flasher(msg, severity=None):\n    \"\"\"\"\"\"\n    try:\n        flash(msg, severity)\n    except RuntimeError:\n        if severity == 'danger':\n            logging.error(msg)\n        else:\n            logging.info(msg)", "output": "Flask's flash if available, logging call if not", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_routing_header(params):\n    \"\"\"\n    \"\"\"\n    if sys.version_info[0] < 3:\n        # Python 2 does not have the \"safe\" parameter for urlencode.\n        return urlencode(params).replace(\"%2F\", \"/\")\n    return urlencode(\n        params,\n        # Per Google API policy (go/api-url-encoding), / is not encoded.\n        safe=\"/\",\n    )", "output": "Returns a routing header string for the given request parameters.\n\n    Args:\n        params (Mapping[str, Any]): A dictionary containing the request\n            parameters used for routing.\n\n    Returns:\n        str: The routing header string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rename(self, names, inplace=False):\n        \"\"\"\n        \n        \"\"\"\n        if (type(names) is not dict):\n            raise TypeError('names must be a dictionary: oldname -> newname')\n\n        if inplace:\n            self.__is_dirty__ = True\n            with cython_context():\n                if self._is_vertex_frame():\n                    graph_proxy = self.__graph__.__proxy__.rename_vertex_fields(names.keys(), names.values())\n                    self.__graph__.__proxy__ = graph_proxy\n                elif self._is_edge_frame():\n                    graph_proxy = self.__graph__.__proxy__.rename_edge_fields(names.keys(), names.values())\n                    self.__graph__.__proxy__ = graph_proxy\n            return self\n        else:\n            return super(GFrame, self).rename(names, inplace=inplace)", "output": "Rename the columns using the 'names' dict.  This changes the names of\n        the columns given as the keys and replaces them with the names given as\n        the values.\n\n        If inplace == False (default) this operation does not modify the\n        current SFrame, returning a new SFrame.\n\n        If inplace == True, this operation modifies the current\n        SFrame, returning self.\n\n        Parameters\n        ----------\n        names : dict[string, string]\n            Dictionary of [old_name, new_name]\n\n        inplace : bool, optional. Defaults to False.\n            Whether the SFrame is modified in place.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_paths(scheme=_get_default_scheme(), vars=None, expand=True):\n    \"\"\"\n    \"\"\"\n    _ensure_cfg_read()\n    if expand:\n        return _expand_vars(scheme, vars)\n    else:\n        return dict(_SCHEMES.items(scheme))", "output": "Return a mapping containing an install scheme.\n\n    ``scheme`` is the install scheme name. If not provided, it will\n    return the default scheme for the current platform.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getName(self):\n        \n        \"\"\"\n        if self.__name:\n            return self.__name\n        elif self.__parent:\n            par = self.__parent()\n            if par:\n                return par.__lookup(self)\n            else:\n                return None\n        elif (len(self) == 1 and\n               len(self.__tokdict) == 1 and\n               next(iter(self.__tokdict.values()))[0][1] in (0,-1)):\n            return next(iter(self.__tokdict.keys()))\n        else:\n            return None", "output": "r\"\"\"\n        Returns the results name for this token expression. Useful when several\n        different expressions might match at a particular location.\n\n        Example::\n\n            integer = Word(nums)\n            ssn_expr = Regex(r\"\\d\\d\\d-\\d\\d-\\d\\d\\d\\d\")\n            house_number_expr = Suppress('#') + Word(nums, alphanums)\n            user_data = (Group(house_number_expr)(\"house_number\")\n                        | Group(ssn_expr)(\"ssn\")\n                        | Group(integer)(\"age\"))\n            user_info = OneOrMore(user_data)\n\n            result = user_info.parseString(\"22 111-22-3333 #221B\")\n            for item in result:\n                print(item.getName(), ':', item[0])\n\n        prints::\n\n            age : 22\n            ssn : 111-22-3333\n            house_number : 221B", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def created(self):\n        \"\"\"\n        \"\"\"\n        statistics = self._properties.get(\"statistics\")\n        if statistics is not None:\n            millis = statistics.get(\"creationTime\")\n            if millis is not None:\n                return _helpers._datetime_from_microseconds(millis * 1000.0)", "output": "Datetime at which the job was created.\n\n        :rtype: ``datetime.datetime``, or ``NoneType``\n        :returns: the creation time (None until set from the server).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def export_coreml(self, filename):\n        \"\"\"\n        \n        \"\"\"\n        from turicreate.toolkits import _coreml_utils\n        display_name = \"boosted trees classifier\"\n        short_description = _coreml_utils._mlmodel_short_description(display_name)\n        context = {\"mode\" : \"classification\",\n                   \"model_type\" : \"boosted_trees\",\n                   \"version\": _turicreate.__version__,\n                   \"class\": self.__class__.__name__,\n                   \"short_description\": short_description,\n                   'user_defined':{\n                    'turicreate_version': _turicreate.__version__\n                   }\n                }\n        self._export_coreml_impl(filename, context)", "output": "Export the model in Core ML format.\n\n        Parameters\n        ----------\n        filename: str\n          A valid filename where the model can be saved.\n\n        Examples\n        --------\n        >>> model.export_coreml(\"MyModel.mlmodel\")", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_parameter_specs(self, name_prefix=\"\"):\n    \"\"\"\"\"\"\n    specs = []\n    for name, categories, _ in self._categorical_params.values():\n      spec = {\n          \"parameterName\": name_prefix + name,\n          \"type\": \"CATEGORICAL\",\n          \"categoricalValues\": categories,\n      }\n      specs.append(spec)\n\n    for name, feasible_points, scale, _ in self._discrete_params.values():\n      spec = {\n          \"parameterName\": name_prefix + name,\n          \"type\": \"DISCRETE\",\n          \"discreteValues\": feasible_points,\n      }\n      if scale:\n        spec[\"scaleType\"] = self.SCALES_STR[scale]\n      specs.append(spec)\n\n    for name, min_val, max_val, scale, _ in self._float_params.values():\n      spec = {\n          \"parameterName\": name_prefix + name,\n          \"type\": \"DOUBLE\",\n          \"minValue\": min_val,\n          \"maxValue\": max_val,\n      }\n      if scale:\n        spec[\"scaleType\"] = self.SCALES_STR[scale]\n      specs.append(spec)\n\n    for name, min_val, max_val, scale, _ in self._int_params.values():\n      spec = {\n          \"parameterName\": name_prefix + name,\n          \"type\": \"INTEGER\",\n          \"minValue\": min_val,\n          \"maxValue\": max_val,\n      }\n      if scale:\n        spec[\"scaleType\"] = self.SCALES_STR[scale]\n      specs.append(spec)\n\n    return specs", "output": "To list of dicts suitable for Cloud ML Engine hyperparameter tuning.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_event(self,\n                  request,\n                  tag='',\n                  matcher=prefix_matcher.__func__,\n                  callback=None,\n                  timeout=None\n                  ):\n        '''\n        \n        '''\n        # if the request finished, no reason to allow event fetching, since we\n        # can't send back to the client\n        if request._finished:\n            future = Future()\n            future.set_exception(TimeoutException())\n            return future\n\n        future = Future()\n        if callback is not None:\n            def handle_future(future):\n                tornado.ioloop.IOLoop.current().add_callback(callback, future)\n            future.add_done_callback(handle_future)\n        # add this tag and future to the callbacks\n        self.tag_map[(tag, matcher)].append(future)\n        self.request_map[request].append((tag, matcher, future))\n\n        if timeout:\n            timeout_future = tornado.ioloop.IOLoop.current().call_later(timeout, self._timeout_future, tag, matcher, future)\n            self.timeout_map[future] = timeout_future\n\n        return future", "output": "Get an event (asynchronous of course) return a future that will get it later", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def open_session(\n        self, window_size=None, max_packet_size=None, timeout=None\n    ):\n        \"\"\"\n        \n        \"\"\"\n        return self.open_channel(\n            \"session\",\n            window_size=window_size,\n            max_packet_size=max_packet_size,\n            timeout=timeout,\n        )", "output": "Request a new channel to the server, of type ``\"session\"``.  This is\n        just an alias for calling `open_channel` with an argument of\n        ``\"session\"``.\n\n        .. note:: Modifying the the window and packet sizes might have adverse\n            effects on the session created. The default values are the same\n            as in the OpenSSH code base and have been battle tested.\n\n        :param int window_size:\n            optional window size for this session.\n        :param int max_packet_size:\n            optional max packet size for this session.\n\n        :return: a new `.Channel`\n\n        :raises:\n            `.SSHException` -- if the request is rejected or the session ends\n            prematurely\n\n        .. versionchanged:: 1.13.4/1.14.3/1.15.3\n            Added the ``timeout`` argument.\n        .. versionchanged:: 1.15\n            Added the ``window_size`` and ``max_packet_size`` arguments.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_sparse_tuple(sequence):\n    \n    \"\"\"\n    indices = np.asarray(list(zip([0]*len(sequence), range(len(sequence)))), dtype=np.int64)\n    shape = np.asarray([1, len(sequence)], dtype=np.int64)\n    return indices, sequence, shape", "output": "r\"\"\"Creates a sparse representention of ``sequence``.\n        Returns a tuple with (indices, values, shape)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expanduser(self):\n        \"\"\" \n        \"\"\"\n        if (not (self._drv or self._root)\n                and self._parts and self._parts[0][:1] == '~'):\n            homedir = self._flavour.gethomedir(self._parts[0][1:])\n            return self._from_parts([homedir] + self._parts[1:])\n\n        return self", "output": "Return a new path with expanded ~ and ~user constructs\n        (as returned by os.path.expanduser)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_position(self):\n        \"\"\"\n        \n        \"\"\"\n        xq_positions = self._get_position()\n        balance = self.get_balance()[0]\n        position_list = []\n        for pos in xq_positions:\n            volume = pos[\"weight\"] * balance[\"asset_balance\"] / 100\n            position_list.append(\n                {\n                    \"cost_price\": volume / 100,\n                    \"current_amount\": 100,\n                    \"enable_amount\": 100,\n                    \"income_balance\": 0,\n                    \"keep_cost_price\": volume / 100,\n                    \"last_price\": volume / 100,\n                    \"market_value\": volume,\n                    \"position_str\": \"random\",\n                    \"stock_code\": pos[\"stock_symbol\"],\n                    \"stock_name\": pos[\"stock_name\"],\n                }\n            )\n        return position_list", "output": "\u83b7\u53d6\u6301\u4ed3\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_file(cls, h5_file, country_code):\n        \"\"\"\n        \n        \"\"\"\n        if h5_file.attrs['version'] != VERSION:\n            raise ValueError(\n                'mismatched version: file is of version %s, expected %s' % (\n                    h5_file.attrs['version'],\n                    VERSION,\n                ),\n            )\n\n        return cls(h5_file[country_code])", "output": "Construct from an h5py.File and a country code.\n\n        Parameters\n        ----------\n        h5_file : h5py.File\n            An HDF5 daily pricing file.\n        country_code : str\n            The ISO 3166 alpha-2 country code for the country to read.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_transition_list (self, list_input_symbols, state, action=None, next_state=None):\n\n        ''' '''\n\n        if next_state is None:\n            next_state = state\n        for input_symbol in list_input_symbols:\n            self.add_transition (input_symbol, state, action, next_state)", "output": "This adds the same transition for a list of input symbols.\n        You can pass a list or a string. Note that it is handy to use\n        string.digits, string.whitespace, string.letters, etc. to add\n        transitions that match character classes.\n\n        The action may be set to None in which case the process() method will\n        ignore the action and only set the next_state. The next_state may be\n        set to None in which case the current state will be unchanged.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_backward_begin(self, last_loss:Rank0Tensor, **kwargs:Any) -> Rank0Tensor:\n        \"\"\n        #To avoid gradient underflow, we scale the gradients\n        ret_loss = last_loss * self.loss_scale\n        return {'last_loss': ret_loss}", "output": "Scale gradients up by `self.loss_scale` to prevent underflow.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _fully_connected(self, x, out_dim):\n        \"\"\"\"\"\"\n        x = tf.reshape(x, [self.hps.batch_size, -1])\n        w = tf.get_variable(\n            \"DW\", [x.get_shape()[1], out_dim],\n            initializer=tf.uniform_unit_scaling_initializer(factor=1.0))\n        b = tf.get_variable(\n            \"biases\", [out_dim], initializer=tf.constant_initializer())\n        return tf.nn.xw_plus_b(x, w, b)", "output": "FullyConnected layer for final output.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def super_lm_moe():\n  \"\"\"\"\"\"\n  hparams = super_lm_base()\n  hparams.layers = (\n      (\"n,att,m,d,a,\" \"n,moe,m,d,a,\") * 4 + \"n,ffn,d\")\n  hparams.moe_num_experts = 32\n  hparams.moe_hidden_sizes = \"1024\"\n  return hparams", "output": "Add mixture of experts with ~1B params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uninstall_ruby(ruby, runas=None):\n    '''\n    \n    '''\n    ruby = re.sub(r'^ruby-', '', ruby)\n    _rbenv_exec(['uninstall', '--force', ruby], runas=runas)\n    return True", "output": "Uninstall a ruby implementation.\n\n    ruby\n        The version of ruby to uninstall. Should match one of the versions\n        listed by :py:func:`rbenv.versions <salt.modules.rbenv.versions>`.\n\n    runas\n        The user under which to run rbenv. If not specified, then rbenv will be\n        run as the user under which Salt is running.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rbenv.uninstall_ruby 2.0.0-p0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_upgrades(refresh=False, root=None, **kwargs):  # pylint: disable=W0613\n    '''\n    \n    '''\n    upgrades = {}\n    cmd = ['pacman', '-S', '-p', '-u', '--print-format', '%n %v']\n\n    if root is not None:\n        cmd.extend(('-r', root))\n\n    if refresh:\n        cmd.append('-y')\n\n    call = __salt__['cmd.run_all'](cmd,\n                                   python_shell=False,\n                                   output_loglevel='trace')\n\n    if call['retcode'] != 0:\n        comment = ''\n        if 'stderr' in call:\n            comment += call['stderr']\n        if 'stdout' in call:\n            comment += call['stdout']\n        if comment:\n            comment = ': ' + comment\n        raise CommandExecutionError('Error listing upgrades' + comment)\n    else:\n        out = call['stdout']\n\n    for line in salt.utils.itertools.split(out, '\\n'):\n        try:\n            pkgname, pkgver = line.split()\n        except ValueError:\n            continue\n        if pkgname.lower() == 'downloading' and '.db' in pkgver.lower():\n            # Antergos (and possibly other Arch derivatives) add lines when pkg\n            # metadata is being downloaded. Because these lines, when split,\n            # contain two columns (i.e. 'downloading community.db...'), we will\n            # skip this line to keep it from being interpreted as an upgrade.\n            continue\n        upgrades[pkgname] = pkgver\n    return upgrades", "output": "List all available package upgrades on this system\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.list_upgrades", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def info(self):\n        \"\"\"\"\"\"\n        proc = Popen(['zsh', '-c', 'echo $ZSH_VERSION'],\n                     stdout=PIPE, stderr=DEVNULL)\n        version = proc.stdout.read().decode('utf-8').strip()\n        return u'ZSH {}'.format(version)", "output": "Returns the name and version of the current shell", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_not_considered_falsey(value, ignore_types=()):\n    '''\n    \n    '''\n    return isinstance(value, bool) or type(value) in ignore_types or value", "output": "Helper function for filter_falsey to determine if something is not to be\n    considered falsey.\n\n    :param any value: The value to consider\n    :param list ignore_types: The types to ignore when considering the value.\n\n    :return bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_positive_multiple_of_10(string):\n    \"\"\"\"\"\"\n    try:\n        value = int(string)\n    except ValueError:\n        msg = '\"%r\" is not a positive multiple of 10 (greater zero).' % string\n        raise argparse.ArgumentTypeError(msg)\n    if value <= 0 or value % 10 != 0:\n        msg = '\"%r\" is not a positive multiple of 10 (greater zero).' % string\n        raise argparse.ArgumentTypeError(msg)\n    return value", "output": "Converts a string into its encoded positive integer (greater zero) or throws an exception.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send_email_sns(sender, subject, message, topic_ARN, image_png):\n    \"\"\"\n    \n    \"\"\"\n    from boto3 import resource as boto3_resource\n\n    sns = boto3_resource('sns')\n    topic = sns.Topic(topic_ARN[0])\n\n    # Subject is max 100 chars\n    if len(subject) > 100:\n        subject = subject[0:48] + '...' + subject[-49:]\n\n    response = topic.publish(Subject=subject, Message=message)\n\n    logger.debug((\"Message sent to SNS.\\nMessageId: {},\\nRequestId: {},\\n\"\n                 \"HTTPSStatusCode: {}\").format(response['MessageId'],\n                                               response['ResponseMetadata']['RequestId'],\n                                               response['ResponseMetadata']['HTTPStatusCode']))", "output": "Sends notification through AWS SNS. Takes Topic ARN from recipients.\n\n    Does not handle access keys.  Use either\n      1/ configuration file\n      2/ EC2 instance profile\n\n    See also https://boto3.readthedocs.io/en/latest/guide/configuration.html.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def count_generated_adv_examples(self):\n    \"\"\"\"\"\"\n    result = {}\n    for v in itervalues(self.data):\n      s_id = v['submission_id']\n      result[s_id] = result.get(s_id, 0) + len(v['images'])\n    return result", "output": "Returns total number of all generated adversarial examples.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_relative_pythonpath(self, value):\r\n        \"\"\"\"\"\"\r\n        self.pythonpath = [osp.abspath(osp.join(self.root_path, path))\r\n                           for path in value]", "output": "Set PYTHONPATH list relative paths", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def consume(self, chars, min=0, max=-1):\n        \"\"\"\n        \n        \"\"\"\n        while self.current in chars and max != 0:\n            min -= 1\n            max -= 1\n            if not self.inc():\n                break\n\n        # failed to consume minimum number of characters\n        if min > 0:\n            self.parse_error(UnexpectedCharError)", "output": "Consume chars until min/max is satisfied is valid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def canonicalize_clusters(clusters: DefaultDict[int, List[Tuple[int, int]]]) -> List[List[Tuple[int, int]]]:\n    \"\"\"\n    \n    \"\"\"\n    merged_clusters: List[Set[Tuple[int, int]]] = []\n    for cluster in clusters.values():\n        cluster_with_overlapping_mention = None\n        for mention in cluster:\n            # Look at clusters we have already processed to\n            # see if they contain a mention in the current\n            # cluster for comparison.\n            for cluster2 in merged_clusters:\n                if mention in cluster2:\n                    # first cluster in merged clusters\n                    # which contains this mention.\n                    cluster_with_overlapping_mention = cluster2\n                    break\n            # Already encountered overlap - no need to keep looking.\n            if cluster_with_overlapping_mention is not None:\n                break\n        if cluster_with_overlapping_mention is not None:\n            # Merge cluster we are currently processing into\n            # the cluster in the processed list.\n            cluster_with_overlapping_mention.update(cluster)\n        else:\n            merged_clusters.append(set(cluster))\n    return [list(c) for c in merged_clusters]", "output": "The CONLL 2012 data includes 2 annotated spans which are identical,\n    but have different ids. This checks all clusters for spans which are\n    identical, and if it finds any, merges the clusters containing the\n    identical spans.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _delete_resource(resource, name=None, resource_id=None, region=None,\n                     key=None, keyid=None, profile=None, **kwargs):\n    '''\n    \n    '''\n\n    if not _exactly_one((name, resource_id)):\n        raise SaltInvocationError('One (but not both) of name or id must be '\n                                  'provided.')\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n        try:\n            delete_resource = getattr(conn, 'delete_' + resource)\n        except AttributeError:\n            raise AttributeError('{0} function does not exist for boto VPC '\n                                 'connection.'.format('delete_' + resource))\n        if name:\n            resource_id = _get_resource_id(resource, name,\n                                           region=region, key=key,\n                                           keyid=keyid, profile=profile)\n            if not resource_id:\n                return {'deleted': False, 'error': {'message':\n                        '{0} {1} does not exist.'.format(resource, name)}}\n\n        if delete_resource(resource_id, **kwargs):\n            _cache_id(name, sub_resource=resource,\n                      resource_id=resource_id,\n                      invalidate=True,\n                      region=region,\n                      key=key, keyid=keyid,\n                      profile=profile)\n            return {'deleted': True}\n        else:\n            if name:\n                e = '{0} {1} was not deleted.'.format(resource, name)\n            else:\n                e = '{0} was not deleted.'.format(resource)\n            return {'deleted': False, 'error': {'message': e}}\n    except BotoServerError as e:\n        return {'deleted': False, 'error': __utils__['boto.get_error'](e)}", "output": "Delete a VPC resource. Returns True if successful, otherwise False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_delete_key_pb(self):\n        \"\"\"\n        \"\"\"\n        new_mutation = _datastore_pb2.Mutation()\n        self._mutations.append(new_mutation)\n        return new_mutation.delete", "output": "Adds a new mutation for a key to be deleted.\n\n        :rtype: :class:`.entity_pb2.Key`\n        :returns: The newly created key protobuf that will be\n                  deleted when sent with a commit.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lstm_hidden_bias(tensor: torch.Tensor) -> None:\n    \"\"\"\n    \n    \"\"\"\n    # gates are (b_hi|b_hf|b_hg|b_ho) of shape (4*hidden_size)\n    tensor.data.zero_()\n    hidden_size = tensor.shape[0] // 4\n    tensor.data[hidden_size:(2 * hidden_size)] = 1.0", "output": "Initialize the biases of the forget gate to 1, and all other gates to 0,\n    following Jozefowicz et al., An Empirical Exploration of Recurrent Network Architectures", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_user_dir_path(self):\n        \"\"\"\"\"\"\n        xdg_config_home = os.environ.get('XDG_CONFIG_HOME', '~/.config')\n        user_dir = Path(xdg_config_home, 'thefuck').expanduser()\n        legacy_user_dir = Path('~', '.thefuck').expanduser()\n\n        # For backward compatibility use legacy '~/.thefuck' if it exists:\n        if legacy_user_dir.is_dir():\n            warn(u'Config path {} is deprecated. Please move to {}'.format(\n                legacy_user_dir, user_dir))\n            return legacy_user_dir\n        else:\n            return user_dir", "output": "Returns Path object representing the user config resource", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _properties_from_dict(d, key_name='key'):\n    '''\n    \n    '''\n    fields = []\n    for key, value in six.iteritems(d):\n        if isinstance(value, dict):\n            fields.append({\n                key_name: key,\n                'refValue': value['ref'],\n            })\n        else:\n            fields.append({\n                key_name: key,\n                'stringValue': value,\n            })\n    return fields", "output": "Transforms dictionary into pipeline object properties.\n\n    The output format conforms to boto's specification.\n\n    Example input:\n        {\n            'a': '1',\n            'b': {\n                'ref': '2'\n            },\n        }\n\n    Example output:\n        [\n            {\n                'key': 'a',\n                'stringValue': '1',\n            },\n            {\n                'key': 'b',\n                'refValue': '2',\n            },\n        ]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_s3_uri(uri):\n    \"\"\"\n    \"\"\"\n    if not isinstance(uri, string_types):\n        return None\n\n    url = urlparse(uri)\n    query = parse_qs(url.query)\n\n    if url.scheme == 's3' and url.netloc and url.path:\n        s3_pointer = {\n            'Bucket': url.netloc,\n            'Key': url.path.lstrip('/')\n        }\n        if 'versionId' in query and len(query['versionId']) == 1:\n            s3_pointer['Version'] = query['versionId'][0]\n        return s3_pointer\n    else:\n        return None", "output": "Parses a S3 Uri into a dictionary of the Bucket, Key, and VersionId\n\n    :return: a BodyS3Location dict or None if not an S3 Uri\n    :rtype: dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_row(self):\r\n        \"\"\"\"\"\"\r\n        row = self.currentIndex().row()\r\n        rows = self.proxy_model.rowCount()\r\n        if row + 1 == rows:\r\n            row = -1\r\n        self.selectRow(row + 1)", "output": "Move to next row from currently selected row.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _assert_valid_categorical_missing_value(value):\n    \"\"\"\n    \n    \"\"\"\n    label_types = LabelArray.SUPPORTED_SCALAR_TYPES\n    if not isinstance(value, label_types):\n        raise TypeError(\n            \"Categorical terms must have missing values of type \"\n            \"{types}.\".format(\n                types=' or '.join([t.__name__ for t in label_types]),\n            )\n        )", "output": "Check that value is a valid categorical missing_value.\n\n    Raises a TypeError if the value is cannot be used as the missing_value for\n    a categorical_dtype Term.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_secret(name, namespace='default', decode=False, **kwargs):\n    '''\n    \n    '''\n    cfg = _setup_conn(**kwargs)\n    try:\n        api_instance = kubernetes.client.CoreV1Api()\n        api_response = api_instance.read_namespaced_secret(name, namespace)\n\n        if api_response.data and (decode or decode == 'True'):\n            for key in api_response.data:\n                value = api_response.data[key]\n                api_response.data[key] = base64.b64decode(value)\n\n        return api_response.to_dict()\n    except (ApiException, HTTPError) as exc:\n        if isinstance(exc, ApiException) and exc.status == 404:\n            return None\n        else:\n            log.exception(\n                'Exception when calling '\n                'CoreV1Api->read_namespaced_secret'\n            )\n            raise CommandExecutionError(exc)\n    finally:\n        _cleanup(**cfg)", "output": "Return the kubernetes secret defined by name and namespace.\n    The secrets can be decoded if specified by the user. Warning: this has\n    security implications.\n\n    CLI Examples::\n\n        salt '*' kubernetes.show_secret confidential default\n        salt '*' kubernetes.show_secret name=confidential namespace=default\n        salt '*' kubernetes.show_secret name=confidential decode=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_tensor_value_info(\n        name,  # type: Text\n        elem_type,  # type: int\n        shape,  # type: Optional[Sequence[Union[Text, int]]]\n        doc_string=\"\",  # type: Text\n        shape_denotation=None,  # type: Optional[List[Text]]\n):  # type: (...) -> ValueInfoProto\n    \"\"\"\"\"\"\n    value_info_proto = ValueInfoProto()\n    value_info_proto.name = name\n    if doc_string:\n        value_info_proto.doc_string = doc_string\n\n    tensor_type_proto = value_info_proto.type.tensor_type\n    tensor_type_proto.elem_type = elem_type\n\n    tensor_shape_proto = tensor_type_proto.shape\n\n    if shape is not None:\n        # You might think this is a no-op (extending a normal Python\n        # list by [] certainly is), but protobuf lists work a little\n        # differently; if a field is never set, it is omitted from the\n        # resulting protobuf; a list that is explicitly set to be\n        # empty will get an (empty) entry in the protobuf. This\n        # difference is visible to our consumers, so make sure we emit\n        # an empty shape!\n        tensor_shape_proto.dim.extend([])\n\n        if shape_denotation:\n            if len(shape_denotation) != len(shape):\n                raise ValueError(\n                    'Invalid shape_denotation. '\n                    'Must be of the same length as shape.')\n\n        for i, d in enumerate(shape):\n            dim = tensor_shape_proto.dim.add()\n            if d is None:\n                pass\n            elif isinstance(d, integer_types):\n                dim.dim_value = d\n            elif isinstance(d, text_type):\n                dim.dim_param = d\n            else:\n                raise ValueError(\n                    'Invalid item in shape: {}. '\n                    'Needs to of integer_types or text_type.'.format(d))\n\n            if shape_denotation:\n                dim.denotation = shape_denotation[i]\n\n    return value_info_proto", "output": "Makes a ValueInfoProto based on the data type and shape.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _maybe_cache(arg, format, cache, convert_listlike):\n    \"\"\"\n    \n    \"\"\"\n    from pandas import Series\n    cache_array = Series()\n    if cache:\n        # Perform a quicker unique check\n        from pandas import Index\n        unique_dates = Index(arg).unique()\n        if len(unique_dates) < len(arg):\n            cache_dates = convert_listlike(unique_dates.to_numpy(),\n                                           True, format)\n            cache_array = Series(cache_dates, index=unique_dates)\n    return cache_array", "output": "Create a cache of unique dates from an array of dates\n\n    Parameters\n    ----------\n    arg : integer, float, string, datetime, list, tuple, 1-d array, Series\n    format : string\n        Strftime format to parse time\n    cache : boolean\n        True attempts to create a cache of converted values\n    convert_listlike : function\n        Conversion function to apply on dates\n\n    Returns\n    -------\n    cache_array : Series\n        Cache of converted, unique dates. Can be empty", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_to_arn(arns, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    results = []\n    for arn in arns:\n        if arn.startswith(\"scaling_policy:\"):\n            _, as_group, scaling_policy_name = arn.split(\":\")\n            policy_arn = __salt__[\"boto_asg.get_scaling_policy_arn\"](\n                as_group, scaling_policy_name, region, key, keyid, profile\n            )\n            if policy_arn:\n                results.append(policy_arn)\n            else:\n                log.error('Could not convert: %s', arn)\n        else:\n            results.append(arn)\n    return results", "output": "Convert a list of strings into actual arns. Converts convenience names such\n    as 'scaling_policy:...'\n\n    CLI Example::\n\n        salt '*' convert_to_arn 'scaling_policy:'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_de_listed(self):\n        \"\"\"\n        \n        \"\"\"\n        env = Environment.get_instance()\n        instrument = env.get_instrument(self._order_book_id)\n        current_date = env.trading_dt\n\n        if instrument.de_listed_date is not None:\n            if instrument.de_listed_date.date() > env.config.base.end_date:\n                return False\n            if current_date >= env.data_proxy.get_previous_trading_date(instrument.de_listed_date):\n                return True\n        return False", "output": "\u5224\u65ad\u5408\u7ea6\u662f\u5426\u8fc7\u671f", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reparentChildren(self, newParent):\n        \"\"\"\n\n        \"\"\"\n        # XXX - should this method be made more general?\n        for child in self.childNodes:\n            newParent.appendChild(child)\n        self.childNodes = []", "output": "Move all the children of the current node to newParent.\n        This is needed so that trees that don't store text as nodes move the\n        text in the correct way\n\n        :arg newParent: the node to move all this node's children to", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def x_build_targets_target( self, node ):\n        '''\n        \n        '''\n        target_node = node\n        name = self.get_child_data(target_node,tag='name',strip=True)\n        path = self.get_child_data(target_node,tag='path',strip=True)\n        jam_target = self.get_child_data(target_node,tag='jam-target',strip=True)\n        #~ Map for jam targets to virtual targets.\n        self.target[jam_target] = {\n            'name' : name,\n            'path' : path\n            }\n        #~ Create the ancestry.\n        dep_node = self.get_child(self.get_child(target_node,tag='dependencies'),tag='dependency')\n        while dep_node:\n            child = self.get_data(dep_node,strip=True)\n            child_jam_target = '<p%s>%s' % (path,child.split('//',1)[1])\n            self.parent[child_jam_target] = jam_target\n            dep_node = self.get_sibling(dep_node.nextSibling,tag='dependency')\n        return None", "output": "Process the target dependency DAG into an ancestry tree so we can look up\n        which top-level library and test targets specific build actions correspond to.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dispatch(intent_request):\n    \"\"\"\n    \n    \"\"\"\n\n    logger.debug('dispatch userId={}, intentName={}'.format(intent_request['userId'], intent_request['currentIntent']['name']))\n\n    intent_name = intent_request['currentIntent']['name']\n\n    # Dispatch to your bot's intent handlers\n    if intent_name == 'OrderFlowers':\n        return order_flowers(intent_request)\n\n    raise Exception('Intent with name ' + intent_name + ' not supported')", "output": "Called when the user specifies an intent for this bot.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def restart(name, timeout=10):\n    '''\n    \n    '''\n    ret = _change_state(name, 'restart', 'running', timeout=timeout)\n    if ret['result']:\n        ret['restarted'] = True\n    return ret", "output": "Restarts a container\n\n    name\n        Container name or ID\n\n    timeout : 10\n        Timeout in seconds after which the container will be killed (if it has\n        not yet gracefully shut down)\n\n\n    **RETURN DATA**\n\n    A dictionary will be returned, containing the following keys:\n\n    - ``status`` - A dictionary showing the prior state of the container as\n      well as the new state\n    - ``result`` - A boolean noting whether or not the action was successful\n    - ``restarted`` - If restart was successful, this key will be present and\n      will be set to ``True``.\n\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt myminion docker.restart mycontainer\n        salt myminion docker.restart mycontainer timeout=20", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_rng(obj=None):\n    \"\"\"\n    \n    \"\"\"\n    seed = (id(obj) + os.getpid() +\n            int(datetime.now().strftime(\"%Y%m%d%H%M%S%f\"))) % 4294967295\n    if _RNG_SEED is not None:\n        seed = _RNG_SEED\n    return np.random.RandomState(seed)", "output": "Get a good RNG seeded with time, pid and the object.\n\n    Args:\n        obj: some object to use to generate random seed.\n    Returns:\n        np.random.RandomState: the RNG.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _error_detail(data, item):\n    '''\n    \n    '''\n    err = item['errorDetail']\n    if 'code' in err:\n        try:\n            msg = ': '.join((\n                item['errorDetail']['code'],\n                item['errorDetail']['message']\n            ))\n        except TypeError:\n            msg = '{0}: {1}'.format(\n                item['errorDetail']['code'],\n                item['errorDetail']['message'],\n            )\n    else:\n        msg = item['errorDetail']['message']\n    data.append(msg)", "output": "Process an API error, updating the data structure", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def grains():\n    '''\n    \n    '''\n    if not DETAILS.get('grains_cache', {}):\n        DETAILS['grains_cache'] = GRAINS_CACHE\n        try:\n            query = {'type': 'op', 'cmd': '<show><system><info></info></system></show>'}\n            DETAILS['grains_cache'] = call(query)['result']['system']\n        except Exception as err:\n            pass\n    return DETAILS['grains_cache']", "output": "Get the grains from the proxied device", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_buckets(self, job_id, timestamp=None, body=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if job_id in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n        return self.transport.perform_request(\n            \"GET\",\n            _make_path(\n                \"_ml\", \"anomaly_detectors\", job_id, \"results\", \"buckets\", timestamp\n            ),\n            params=params,\n            body=body,\n        )", "output": "`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-bucket.html>`_\n\n        :arg job_id: ID of the job to get bucket results from\n        :arg timestamp: The timestamp of the desired single bucket result\n        :arg body: Bucket selection details if not provided in URI\n        :arg anomaly_score: Filter for the most anomalous buckets\n        :arg desc: Set the sort direction\n        :arg end: End time filter for buckets\n        :arg exclude_interim: Exclude interim results\n        :arg expand: Include anomaly records\n        :arg from_: skips a number of buckets\n        :arg size: specifies a max number of buckets to get\n        :arg sort: Sort buckets by a particular field\n        :arg start: Start time filter for buckets", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy(self, extra=None):\n        \"\"\"\n        \n        \"\"\"\n        if extra is None:\n            extra = dict()\n        bestModel = self.bestModel.copy(extra)\n        avgMetrics = self.avgMetrics\n        subModels = self.subModels\n        return CrossValidatorModel(bestModel, avgMetrics, subModels)", "output": "Creates a copy of this instance with a randomly generated uid\n        and some extra params. This copies the underlying bestModel,\n        creates a deep copy of the embedded paramMap, and\n        copies the embedded and extra parameters over.\n        It does not copy the extra Params into the subModels.\n\n        :param extra: Extra parameters to copy to the new instance\n        :return: Copy of this instance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_get_commodity_option_CF_contract_time_to_market():\n    '''\n    \n    '''\n\n    result = QA_fetch_get_option_list('tdx')\n    # pprint.pprint(result)\n    #  category  market code name desc  code\n\n    # df = pd.DataFrame()\n    rows = []\n    result['meaningful_name'] = None\n    for idx in result.index:\n        # pprint.pprint((idx))\n        strCategory = result.loc[idx, \"category\"]\n        strMarket = result.loc[idx, \"market\"]\n        strCode = result.loc[idx, \"code\"]  #\n        strName = result.loc[idx, 'name']  #\n        strDesc = result.loc[idx, 'desc']  #\n\n        # \u5982\u679c\u540c\u65f6\u83b7\u53d6\uff0c \u4e0d\u540c\u7684 \u671f\u8d27\u4ea4\u6613\u6240\u6570\u636e\uff0c pytdx\u4f1a connection close \u8fde\u63a5\u4e2d\u65ad\uff1f\n        # if strName.startswith(\"CU\") or strName.startswith(\"M\") or strName.startswith('SR'):\n        if strName.startswith(\"CF\"):\n            # print(strCategory,' ', strMarket, ' ', strCode, ' ', strName, ' ', strDesc, )\n            row = result.loc[idx]\n            rows.append(row)\n\n    return rows\n\n    pass", "output": "\u94dc\u671f\u6743  CU \u5f00\u5934   \u4e0a\u671f\u8bc1\n    \u8c46\u7c95    M\u5f00\u5934     \u5927\u5546\u6240\n    \u767d\u7cd6    SR\u5f00\u5934    \u90d1\u5546\u6240\n    \u6d4b\u8bd5\u4e2d\u53d1\u73b0\uff0c\u884c\u60c5\u4e0d\u592a\u7a33\u5b9a \uff1f \u662f \u901a\u8fbe\u4fe1 IP \u7684\u95ee\u9898 \uff1f", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exec_cmd(self, cmd):\n        '''\n        \n        '''\n        cmd = self._cmd_str(cmd)\n\n        logmsg = 'Executing command: {0}'.format(cmd)\n        if self.passwd:\n            logmsg = logmsg.replace(self.passwd, ('*' * 6))\n        if 'decode(\"base64\")' in logmsg or 'base64.b64decode(' in logmsg:\n            log.debug('Executed SHIM command. Command logged to TRACE')\n            log.trace(logmsg)\n        else:\n            log.debug(logmsg)\n\n        ret = self._run_cmd(cmd)\n        return ret", "output": "Execute a remote command", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put_logging(Bucket,\n           TargetBucket=None, TargetPrefix=None, TargetGrants=None,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        logstate = {}\n        targets = {'TargetBucket': TargetBucket,\n                   'TargetGrants': TargetGrants,\n                   'TargetPrefix': TargetPrefix}\n        for key, val in six.iteritems(targets):\n            if val is not None:\n                logstate[key] = val\n        if logstate:\n            logstatus = {'LoggingEnabled': logstate}\n        else:\n            logstatus = {}\n        if TargetGrants is not None and isinstance(TargetGrants, six.string_types):\n            TargetGrants = salt.utils.json.loads(TargetGrants)\n        conn.put_bucket_logging(Bucket=Bucket, BucketLoggingStatus=logstatus)\n        return {'updated': True, 'name': Bucket}\n    except ClientError as e:\n        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Given a valid config, update the logging parameters for a bucket.\n\n    Returns {updated: true} if parameters were updated and returns\n    {updated: False} if parameters were not updated.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.put_logging my_bucket log_bucket '[{...}]' prefix", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve_sam_resource_id_refs(self, input, supported_resource_id_refs):\n        \"\"\"\n        \n        \"\"\"\n        return self._traverse(input, supported_resource_id_refs, self._try_resolve_sam_resource_id_refs)", "output": "Some SAM resources have their logical ids mutated from the original id that the customer writes in the\n        template. This method recursively walks the tree and updates these logical ids from the old value\n        to the new value that is generated by SAM.\n\n        Example:\n            {\"Ref\": \"MyLayer\"} -> {\"Ref\": \"MyLayerABC123\"}\n\n        This method does not attempt to validate a reference. If it is invalid or non-resolvable, it skips the\n        occurrence and continues with the rest. It is recommended that you have an external process that detects and\n        surfaces invalid references.\n\n        For first call, it is recommended that `template` is the entire CFN template in order to handle\n        references in Mapping or Output sections.\n\n        :param dict input: CFN template that needs resolution. This method will modify the input\n            directly resolving references. In subsequent recursions, this will be a fragment of the CFN template.\n        :param dict supported_resource_id_refs: Dictionary that maps old logical ids to new ones.\n        :return list errors: List of dictionary containing information about invalid reference. Empty list otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sphinx_build(self, kind):\n        \"\"\"\n        \n        \"\"\"\n        if kind not in ('html', 'latex'):\n            raise ValueError('kind must be html or latex, '\n                             'not {}'.format(kind))\n\n        cmd = ['sphinx-build', '-b', kind]\n        if self.num_jobs:\n            cmd += ['-j', str(self.num_jobs)]\n        if self.warnings_are_errors:\n            cmd += ['-W', '--keep-going']\n        if self.verbosity:\n            cmd.append('-{}'.format('v' * self.verbosity))\n        cmd += ['-d', os.path.join(BUILD_PATH, 'doctrees'),\n                SOURCE_PATH, os.path.join(BUILD_PATH, kind)]\n        return subprocess.call(cmd)", "output": "Call sphinx to build documentation.\n\n        Attribute `num_jobs` from the class is used.\n\n        Parameters\n        ----------\n        kind : {'html', 'latex'}\n\n        Examples\n        --------\n        >>> DocBuilder(num_jobs=4)._sphinx_build('html')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fileno(self):\n        \"\"\"\n        \n        \"\"\"\n        self.lock.acquire()\n        try:\n            if self._pipe is not None:\n                return self._pipe.fileno()\n            # create the pipe and feed in any existing data\n            self._pipe = pipe.make_pipe()\n            p1, p2 = pipe.make_or_pipe(self._pipe)\n            self.in_buffer.set_event(p1)\n            self.in_stderr_buffer.set_event(p2)\n            return self._pipe.fileno()\n        finally:\n            self.lock.release()", "output": "Returns an OS-level file descriptor which can be used for polling, but\n        but not for reading or writing.  This is primarily to allow Python's\n        ``select`` module to work.\n\n        The first time ``fileno`` is called on a channel, a pipe is created to\n        simulate real OS-level file descriptor (FD) behavior.  Because of this,\n        two OS-level FDs are created, which will use up FDs faster than normal.\n        (You won't notice this effect unless you have hundreds of channels\n        open at the same time.)\n\n        :return: an OS-level file descriptor (`int`)\n\n        .. warning::\n            This method causes channel reads to be slightly less efficient.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(X_train, X_test, y_train, y_test, PARAMS):\n    ''''''\n    model.fit(X_train, y_train)\n    predict_y = model.predict(X_test)\n    score = r2_score(y_test, predict_y)\n    LOG.debug('r2 score: %s' % score)\n    nni.report_final_result(score)", "output": "Train model and predict result", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def viewers(self):\n        \"\"\"\n        \"\"\"\n        result = set()\n        for role in self._VIEWER_ROLES:\n            for member in self._bindings.get(role, ()):\n                result.add(member)\n        return frozenset(result)", "output": "Legacy access to viewer role.\n\n        DEPRECATED:  use ``policy[\"roles/viewers\"]`` instead", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_(app, endpoint, id=None, **kwargs):\n    '''\n    \n    '''\n    return _dict(_get(app, endpoint, id=id, auth_required=True if app in AUTH_ENDPOINTS else False, **kwargs))", "output": "Get a single item from NetBox.\n\n    app\n        String of netbox app, e.g., ``dcim``, ``circuits``, ``ipam``\n    endpoint\n        String of app endpoint, e.g., ``sites``, ``regions``, ``devices``\n\n    Returns a single dictionary\n\n    To get an item based on ID.\n\n    .. code-block:: bash\n\n        salt myminion netbox.get dcim devices id=123\n\n    Or using named arguments that correspond with accepted filters on\n    the NetBox endpoint.\n\n    .. code-block:: bash\n\n        salt myminion netbox.get dcim devices name=my-router", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def default_security_rules_list(security_group, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    result = {}\n\n    secgroup = network_security_group_get(\n        security_group=security_group,\n        resource_group=resource_group,\n        **kwargs\n    )\n\n    if 'error' in secgroup:\n        return secgroup\n\n    try:\n        result = secgroup['default_security_rules']\n    except KeyError as exc:\n        log.error('No default security rules found for %s!', security_group)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    List default security rules within a security group.\n\n    :param security_group: The network security group to query.\n\n    :param resource_group: The resource group name assigned to the\n        network security group.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.default_security_rules_list testnsg testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_interface_router(self, router, subnet):\n        '''\n        \n        '''\n        router_id = self._find_router_id(router)\n        subnet_id = self._find_subnet_id(subnet)\n        return self.network_conn.add_interface_router(\n            router=router_id, body={'subnet_id': subnet_id})", "output": "Adds an internal network interface to the specified router", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_func_until_ret_arg(fun, kwargs, fun_call=None,\n                           argument_being_watched=None, required_argument_response=None):\n    '''\n    \n    '''\n    status = None\n    while status != required_argument_response:\n        f_result = fun(kwargs, call=fun_call)\n        r_set = {}\n        for d in f_result:\n            if isinstance(d, list):\n                d0 = d[0]\n                if isinstance(d0, dict):\n                    for k, v in six.iteritems(d0):\n                        r_set[k] = v\n        status = _unwrap_dict(r_set, argument_being_watched)\n        log.debug(\n            'Function: %s, Watched arg: %s, Response: %s',\n            six.text_type(fun).split(' ')[1], argument_being_watched, status\n        )\n        time.sleep(5)\n\n    return True", "output": "Waits until the function retrieves some required argument.\n    NOTE: Tested with ec2 describe_volumes and describe_snapshots only.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uninstall(names):\n    '''\n    \n    '''\n    # Create a Windows Update Agent instance\n    wua = salt.utils.win_update.WindowsUpdateAgent()\n\n    # Search for Updates\n    updates = wua.search(names)\n\n    if updates.count() == 0:\n        raise CommandExecutionError('No updates found')\n\n    return wua.uninstall(updates)", "output": ".. versionadded:: 2017.7.0\n\n    Uninstall updates.\n\n    Args:\n\n        names (str, list):\n            A single update or a list of updates to uninstall. This can be any\n            combination of GUIDs, KB numbers, or names. GUIDs or KBs are\n            preferred.\n\n    Returns:\n\n        dict: A dictionary containing the details about the uninstalled updates\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        # Normal Usage\n        salt '*' win_wua.uninstall KB3121212\n\n        # As a list\n        salt '*' win_wua.uninstall guid=['12345678-abcd-1234-abcd-1234567890ab', 'KB1231231']", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def end(self):\n        \"\"\"\"\"\"\n        for depth in xrange(len(self.names) - 1, -1, -1):\n            self.out_f.write('{0}}}\\n'.format(self.prefix(depth)))", "output": "Generate the closing part", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def histogram(name, data, step=None, buckets=None, description=None):\n  \"\"\"\n  \"\"\"\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=None, description=description)\n  # TODO(https://github.com/tensorflow/tensorboard/issues/2109): remove fallback\n  summary_scope = (\n      getattr(tf.summary.experimental, 'summary_scope', None) or\n      tf.summary.summary_scope)\n  with summary_scope(\n      name, 'histogram_summary', values=[data, buckets, step]) as (tag, _):\n    tensor = _buckets(data, bucket_count=buckets)\n    return tf.summary.write(\n        tag=tag, tensor=tensor, step=step, metadata=summary_metadata)", "output": "Write a histogram summary.\n\n  Arguments:\n    name: A name for this summary. The summary tag used for TensorBoard will\n      be this name prefixed by any active name scopes.\n    data: A `Tensor` of any shape. Must be castable to `float64`.\n    step: Explicit `int64`-castable monotonic step value for this summary. If\n      omitted, this defaults to `tf.summary.experimental.get_step()`, which must\n      not be None.\n    buckets: Optional positive `int`. The output will have this\n      many buckets, except in two edge cases. If there is no data, then\n      there are no buckets. If there is data but all points have the\n      same value, then there is one bucket whose left and right\n      endpoints are the same.\n    description: Optional long-form description for this summary, as a\n      constant `str`. Markdown is supported. Defaults to empty.\n\n  Returns:\n    True on success, or false if no summary was emitted because no default\n    summary writer was available.\n\n  Raises:\n    ValueError: if a default writer exists, but no step was provided and\n      `tf.summary.experimental.get_step()` is None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def classifer_metrics(label, pred):\n    \"\"\"\n    \n    \"\"\"\n    prediction = np.argmax(pred, axis=1)\n    label = label.astype(int)\n\n    pred_is_entity = prediction != not_entity_index\n    label_is_entity = label != not_entity_index\n\n    corr_pred = (prediction == label) == (pred_is_entity == True)\n\n    #how many entities are there?\n    num_entities = np.sum(label_is_entity)\n    entity_preds = np.sum(pred_is_entity)\n\n    #how many times did we correctly predict an entity?\n    correct_entitites = np.sum(corr_pred[pred_is_entity])\n\n    #precision: when we predict entity, how often are we right?\n    precision = correct_entitites/entity_preds\n    if entity_preds == 0:\n        precision = np.nan\n\n    #recall: of the things that were an entity, how many did we catch?\n    recall = correct_entitites / num_entities\n    if num_entities == 0:\n        recall = np.nan\n    f1 = 2 * precision * recall / (precision + recall)\n    return precision, recall, f1", "output": "computes f1, precision and recall on the entity class", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_fmdump_verbose(output):\n    '''\n    \n    '''\n    result = []\n    output = output.split(\"\\n\")\n\n    fault = []\n    verbose_fault = {}\n    for line in output:\n        if line.startswith('TIME'):\n            fault.append(line)\n            if verbose_fault:\n                result.append(verbose_fault)\n                verbose_fault = {}\n        elif len(fault) == 1:\n            fault.append(line)\n            verbose_fault = _parse_fmdump(\"\\n\".join(fault))[0]\n            fault = []\n        elif verbose_fault:\n            if 'details' not in verbose_fault:\n                verbose_fault['details'] = \"\"\n            if line.strip() == '':\n                continue\n            verbose_fault['details'] = '{0}{1}\\n'.format(\n                verbose_fault['details'],\n                line\n            )\n    if verbose_fault:\n        result.append(verbose_fault)\n\n    return result", "output": "Parses fmdump verbose output", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_cr_with_newline(message: str):\n    \"\"\"\n    \n    \"\"\"\n    if '\\r' in message:\n        message = message.replace('\\r', '')\n        if not message or message[-1] != '\\n':\n            message += '\\n'\n    return message", "output": "TQDM and requests use carriage returns to get the training line to update for each batch\n    without adding more lines to the terminal output.  Displaying those in a file won't work\n    correctly, so we'll just make sure that each batch shows up on its one line.\n    :param message: the message to permute\n    :return: the message with carriage returns replaced with newlines", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def arange(start, stop=None, step=1.0, repeat=1, infer_range=False, name=None, dtype=None):\n    \"\"\"\n    \"\"\"\n    if dtype is None:\n        dtype = _numpy.float32\n    return _internal._arange(start=start, stop=stop, step=step, repeat=repeat,\n                             infer_range=infer_range, name=name, dtype=dtype)", "output": "Returns evenly spaced values within a given interval.\n\n    Values are generated within the half-open interval [`start`, `stop`). In other\n    words, the interval includes `start` but excludes `stop`. The function is\n    similar to the built-in Python function `range` and to `numpy.arange`,\n    but returns a `Symbol`.\n\n    Parameters\n    ----------\n    start : number, optional\n        Start of interval. The interval includes this value. The default start value is 0.\n    stop : number\n        End of interval. The interval does not include this value.\n    step : number, optional\n        Spacing between values.\n    repeat : int, optional\n        \"The repeating time of all elements.\n        E.g repeat=3, the element a will be repeated three times --> a, a, a.\n    infer_range : boolean, optional\n        When set to True, infer the stop position from the start, step,\n        repeat, and output tensor size.\n    dtype : str or numpy.dtype, optional\n        The value type of the inner value, default to ``np.float32``.\n\n    Returns\n    -------\n    out : Symbol\n        The created Symbol", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chhome(name, home, **kwargs):\n    '''\n    \n    '''\n    if six.PY2:\n        name = _to_unicode(name)\n        home = _to_unicode(home)\n\n    kwargs = salt.utils.args.clean_kwargs(**kwargs)\n    persist = kwargs.pop('persist', False)\n    if kwargs:\n        salt.utils.args.invalid_kwargs(kwargs)\n    if persist:\n        log.info('Ignoring unsupported \\'persist\\' argument to user.chhome')\n\n    pre_info = info(name)\n\n    if not pre_info:\n        return False\n\n    if home == pre_info['home']:\n        return True\n\n    if not update(name=name, home=home):\n        return False\n\n    post_info = info(name)\n    if post_info['home'] != pre_info['home']:\n        return post_info['home'] == home\n\n    return False", "output": "Change the home directory of the user, pass True for persist to move files\n    to the new home directory if the old home directory exist.\n\n    Args:\n        name (str): The name of the user whose home directory you wish to change\n\n        home (str): The new location of the home directory\n\n    Returns:\n        bool: True if successful, otherwise False\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.chhome foo \\\\\\\\fileserver\\\\home\\\\foo True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def inference(self):\n        \"\"\"\n        \n        \"\"\"\n        handles = [h for h in self._handles if not h.is_training]\n        return TowerTensorHandles(handles)", "output": "Returns:\n            A :class:`TowerTensorHandles`, containing only the inference towers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_single_feature_methods(cls):\n    \"\"\"\n    \"\"\"\n    # Sanity check: This only makes sense if we are building the GAPIC\n    # subclass and have enums already attached.\n    if not hasattr(cls, \"enums\"):\n        return cls\n\n    # Add each single-feature method to the class.\n    for feature in cls.enums.Feature.Type:\n        # Sanity check: Do not make a method for the falsy feature.\n        if feature.name == \"TYPE_UNSPECIFIED\":\n            continue\n\n        # Assign the appropriate metadata to the function.\n        detect = _create_single_feature_method(feature)\n\n        # Assign a qualified name to the function, and perform module\n        # replacement on the docstring.\n        detect.__qualname__ = \"{cls}.{name}\".format(\n            cls=cls.__name__, name=detect.__name__\n        )\n        detect.__doc__ = detect.__doc__.format(module=cls.__module__)\n\n        # Place the function on the class being created.\n        setattr(cls, detect.__name__, detect)\n\n    # Done; return the class.\n    return cls", "output": "Custom decorator intended for :class:`~vision.helpers.VisionHelpers`.\n\n    This metaclass adds a `{feature}` method for every feature\n    defined on the Feature enum.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _trigger(self):\n        \"\"\"\n        \n        \"\"\"\n        if len(self._stat_now):\n            self._stat_now['epoch_num'] = self.epoch_num\n            self._stat_now['global_step'] = self.global_step\n\n            self._stats.append(self._stat_now)\n            self._stat_now = {}\n            self._write_stat()", "output": "Add stats to json and dump to disk.\n        Note that this method is idempotent.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_member_pool(lb, member, pool_name):\n    '''\n    \n    '''\n    if __opts__['load_balancers'].get(lb, None):\n        (username, password) = list(__opts__['load_balancers'][lb].values())\n    else:\n        raise Exception('Unable to find `{0}` load balancer'.format(lb))\n    F5 = F5Mgmt(lb, username, password)\n    return F5.check_member_pool(member, pool_name)", "output": "Check a pool member exists in a specific pool\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-run f5.check_member_pool load_balancer 10.0.0.1 my_pool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_request_arguments(self, request):\n    \"\"\"\n    \"\"\"\n    inference_addresses = request.args.get('inference_address').split(',')\n    model_names = request.args.get('model_name').split(',')\n    model_versions = request.args.get('model_version').split(',')\n    model_signatures = request.args.get('model_signature').split(',')\n    if len(model_names) != len(inference_addresses):\n      raise common_utils.InvalidUserInputError('Every model should have a ' +\n                                                'name and address.')\n    return inference_addresses, model_names, model_versions, model_signatures", "output": "Parses comma separated request arguments\n\n    Args:\n      request: A request that should contain 'inference_address', 'model_name',\n        'model_version', 'model_signature'.\n\n    Returns:\n      A tuple of lists for model parameters", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read_metrics(repo, metrics, branch):\n    \"\"\"\n    \"\"\"\n    res = {}\n    for out, typ, xpath in metrics:\n        assert out.scheme == \"local\"\n        if not typ:\n            typ = os.path.splitext(out.path.lower())[1].replace(\".\", \"\")\n        if out.use_cache:\n            open_fun = open\n            path = repo.cache.local.get(out.checksum)\n        else:\n            open_fun = repo.tree.open\n            path = out.path\n        try:\n\n            with open_fun(path) as fd:\n                metric = _read_metric(\n                    fd,\n                    typ=typ,\n                    xpath=xpath,\n                    rel_path=out.rel_path,\n                    branch=branch,\n                )\n        except IOError as e:\n            if e.errno == errno.ENOENT:\n                logger.warning(\n                    NO_METRICS_FILE_AT_REFERENCE_WARNING.format(\n                        out.rel_path, branch\n                    )\n                )\n                metric = None\n            else:\n                raise\n\n        if not metric:\n            continue\n\n        res[out.rel_path] = metric\n\n    return res", "output": "Read the content of each metric file and format it.\n\n    Args:\n        metrics (list): List of metric touples\n        branch (str): Branch to look up for metrics.\n\n    Returns:\n        A dict mapping keys with metrics path name and content.\n        For example:\n\n        {'metric.csv': (\"value_mse  deviation_mse   data_set\\n\"\n                        \"0.421601   0.173461        train\\n\"\n                        \"0.67528    0.289545        testing\\n\"\n                        \"0.671502   0.297848        validation\\n\")}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def diff_tree(candidate_config=None,\n              candidate_path=None,\n              running_config=None,\n              running_path=None,\n              saltenv='base'):\n    '''\n    \n    '''\n    candidate_tree = tree(config=candidate_config,\n                          path=candidate_path,\n                          saltenv=saltenv)\n    running_tree = tree(config=running_config,\n                        path=running_path,\n                        saltenv=saltenv)\n    return salt.utils.dictdiffer.deep_diff(running_tree, candidate_tree)", "output": "Return the diff, as Python dictionary, between the candidate and the running\n    configuration.\n\n    candidate_config\n        The candidate configuration sent as text. This argument is ignored when\n        ``candidate_path`` is set.\n\n    candidate_path\n        Absolute or remote path from where to load the candidate configuration\n        text. This argument allows any URI supported by\n        :py:func:`cp.get_url <salt.modules.cp.get_url>`), e.g., ``salt://``,\n        ``https://``, ``s3://``, ``ftp:/``, etc.\n\n    running_config\n        The running configuration sent as text. This argument is ignored when\n        ``running_path`` is set.\n\n    running_path\n        Absolute or remote path from where to load the runing configuration\n        text. This argument allows any URI supported by\n        :py:func:`cp.get_url <salt.modules.cp.get_url>`), e.g., ``salt://``,\n        ``https://``, ``s3://``, ``ftp:/``, etc.\n\n    saltenv: ``base``\n        Salt fileserver environment from which to retrieve the file.\n        Ignored if ``candidate_path`` or ``running_path`` is not a\n        ``salt://`` URL.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' iosconfig.diff_tree candidate_path=salt://path/to/candidate.cfg running_path=salt://path/to/running.cfg", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unpackb(packed, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    unpacker = Unpacker(None, **kwargs)\n    unpacker.feed(packed)\n    try:\n        ret = unpacker._unpack()\n    except OutOfData:\n        raise UnpackValueError(\"Data is not enough.\")\n    if unpacker._got_extradata():\n        raise ExtraData(ret, unpacker._get_extradata())\n    return ret", "output": "Unpack an object from `packed`.\n\n    Raises `ExtraData` when `packed` contains extra bytes.\n    See :class:`Unpacker` for options.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def focusOutEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        event.ignore()\r\n        # Inspired from CompletionWidget.focusOutEvent() in file\r\n        # widgets/sourcecode/base.py line 212\r\n        if sys.platform == \"darwin\":\r\n            if event.reason() != Qt.ActiveWindowFocusReason:\r\n                self.close()\r\n        else:\r\n            self.close()", "output": "Reimplement Qt method to close the widget when loosing focus.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def event_return(events):\n    '''\n    \n    '''\n    for event in events:\n        tag = event.get('tag', '')\n        data = event.get('data', '')\n        query = '''INSERT INTO {keyspace}.salt_events (\n                     id, alter_time, data, master_id, tag\n                   ) VALUES (\n                     ?, ?, ?, ?, ?)\n                 '''.format(keyspace=_get_keyspace())\n        statement_arguments = [six.text_type(uuid.uuid1()),\n                               int(time.time() * 1000),\n                               salt.utils.json.dumps(data).replace(\"'\", \"''\"),\n                               __opts__['id'],\n                               tag]\n\n        # cassandra_cql.cql_query may raise a CommandExecutionError\n        try:\n            __salt__['cassandra_cql.cql_query_with_prepare'](query, 'salt_events',\n                                                             statement_arguments,\n                                                             asynchronous=True)\n        except CommandExecutionError:\n            log.critical('Could not store events with Cassandra returner.')\n            raise\n        except Exception as e:\n            log.critical(\n                'Unexpected error while inserting into salt_events: %s', e)\n            raise", "output": "Return event to one of potentially many clustered cassandra nodes\n\n    Requires that configuration be enabled via 'event_return'\n    option in master config.\n\n    Cassandra does not support an auto-increment feature due to the\n    highly inefficient nature of creating a monotonically increasing\n    number across all nodes in a distributed database. Each event\n    will be assigned a uuid by the connecting client.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_doc(self, objtxt):\n        \"\"\"\"\"\"\n        if self._reading:\n            return\n        wait_loop = QEventLoop()\n        self.sig_got_reply.connect(wait_loop.quit)\n        self.silent_exec_method(\"get_ipython().kernel.get_doc('%s')\" % objtxt)\n        wait_loop.exec_()\n\n        # Remove loop connection and loop\n        self.sig_got_reply.disconnect(wait_loop.quit)\n        wait_loop = None\n\n        return self._kernel_reply", "output": "Get object documentation dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _transpose_dict_list(dict_list):\n  \"\"\"\"\"\"\n  # 1. Unstack numpy arrays into list\n  dict_list = utils.map_nested(np_to_list, dict_list, dict_only=True)\n\n  # 2. Extract the sequence length (and ensure the length is constant for all\n  # elements)\n  length = {'value': None}  # dict because `nonlocal` is Python3 only\n  def update_length(elem):\n    if length['value'] is None:\n      length['value'] = len(elem)\n    elif length['value'] != len(elem):\n      raise ValueError(\n          'The length of all elements of one sequence should be the same. '\n          'Got {} != {}'.format(length['value'], len(elem)))\n    return elem\n  utils.map_nested(update_length, dict_list, dict_only=True)\n\n  # 3. Extract each individual elements\n  return [\n      utils.map_nested(lambda elem: elem[i], dict_list, dict_only=True)   # pylint: disable=cell-var-from-loop\n      for i in range(length['value'])\n  ]", "output": "Transpose a nested dict[list] into a list[nested dict].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render_json_tree(tree, indent):\n    \"\"\"\n\n    \"\"\"\n    tree = sorted_tree(tree)\n    branch_keys = set(r.key for r in flatten(tree.values()))\n    nodes = [p for p in tree.keys() if p.key not in branch_keys]\n    key_tree = dict((k.key, v) for k, v in tree.items())\n    get_children = lambda n: key_tree.get(n.key, [])\n\n    def aux(node, parent=None, chain=None):\n        if chain is None:\n            chain = [node.project_name]\n\n        d = node.as_dict()\n        if parent:\n            d['required_version'] = node.version_spec if node.version_spec else 'Any'\n        else:\n            d['required_version'] = d['installed_version']\n\n        d['dependencies'] = [\n            aux(c, parent=node, chain=chain+[c.project_name])\n            for c in get_children(node)\n            if c.project_name not in chain\n        ]\n\n        return d\n\n    return json.dumps([aux(p) for p in nodes], indent=indent)", "output": "Converts the tree into a nested json representation.\n\n    The json repr will be a list of hashes, each hash having the following fields:\n      - package_name\n      - key\n      - required_version\n      - installed_version\n      - dependencies: list of dependencies\n\n    :param dict tree: dependency tree\n    :param int indent: no. of spaces to indent json\n    :returns: json representation of the tree\n    :rtype: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def safe_zip(*args):\n  \"\"\"\n  \"\"\"\n  length = len(args[0])\n  if not all(len(arg) == length for arg in args):\n    raise ValueError(\"Lengths of arguments do not match: \"\n                     + str([len(arg) for arg in args]))\n  return list(zip(*args))", "output": "like zip but with these properties:\n  - returns a list, rather than an iterator. This is the old Python2 zip behavior.\n  - a guarantee that all arguments are the same length.\n  (normal zip silently drops entries to make them the same length)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_s3_url(url):\n    \"\"\"\n    \n    \"\"\"\n    bucket = ''\n    path = ''\n    if url:\n        result = urlparse(url)\n        bucket = result.netloc\n        path = result.path.strip('/')\n    return bucket, path", "output": "Parses S3 URL.\n\n    Returns bucket (domain) and file (full path).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_product(cls, iterables, sortorder=None, names=None):\n        \"\"\"\n        \n        \"\"\"\n        from pandas.core.arrays.categorical import _factorize_from_iterables\n        from pandas.core.reshape.util import cartesian_product\n\n        if not is_list_like(iterables):\n            raise TypeError(\"Input must be a list / sequence of iterables.\")\n        elif is_iterator(iterables):\n            iterables = list(iterables)\n\n        codes, levels = _factorize_from_iterables(iterables)\n        codes = cartesian_product(codes)\n        return MultiIndex(levels, codes, sortorder=sortorder, names=names)", "output": "Make a MultiIndex from the cartesian product of multiple iterables.\n\n        Parameters\n        ----------\n        iterables : list / sequence of iterables\n            Each iterable has unique labels for each level of the index.\n        sortorder : int or None\n            Level of sortedness (must be lexicographically sorted by that\n            level).\n        names : list / sequence of str, optional\n            Names for the levels in the index.\n\n        Returns\n        -------\n        index : MultiIndex\n\n        See Also\n        --------\n        MultiIndex.from_arrays : Convert list of arrays to MultiIndex.\n        MultiIndex.from_tuples : Convert list of tuples to MultiIndex.\n        MultiIndex.from_frame : Make a MultiIndex from a DataFrame.\n\n        Examples\n        --------\n        >>> numbers = [0, 1, 2]\n        >>> colors = ['green', 'purple']\n        >>> pd.MultiIndex.from_product([numbers, colors],\n        ...                            names=['number', 'color'])\n        MultiIndex(levels=[[0, 1, 2], ['green', 'purple']],\n                   codes=[[0, 0, 1, 1, 2, 2], [0, 1, 0, 1, 0, 1]],\n                   names=['number', 'color'])", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def page_erase(addr):\n    \"\"\"\"\"\"\n    if __verbose:\n        print(\"Erasing page: 0x%x...\" % (addr))\n\n    # Send DNLOAD with first byte=0x41 and page address\n    buf = struct.pack(\"<BI\", 0x41, addr)\n    __dev.ctrl_transfer(0x21, __DFU_DNLOAD, 0, __DFU_INTERFACE, buf, __TIMEOUT)\n\n    # Execute last command\n    if get_status() != __DFU_STATE_DFU_DOWNLOAD_BUSY:\n        raise Exception(\"DFU: erase failed\")\n\n    # Check command state\n    if get_status() != __DFU_STATE_DFU_DOWNLOAD_IDLE:\n\n        raise Exception(\"DFU: erase failed\")", "output": "Erases a single page.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_index_to_pipfile(self, index, verify_ssl=True):\n        \"\"\"\"\"\"\n        # Read and append Pipfile.\n        p = self.parsed_pipfile\n        try:\n            self.get_source(url=index)\n        except SourceNotFound:\n            source = {\"url\": index, \"verify_ssl\": verify_ssl}\n        else:\n            return\n        source[\"name\"] = self.src_name_from_url(index)\n        # Add the package to the group.\n        if \"source\" not in p:\n            p[\"source\"] = [source]\n        else:\n            p[\"source\"].append(source)\n        # Write Pipfile.\n        self.write_toml(p)", "output": "Adds a given index to the Pipfile.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def pin(self):\n        \"\"\"\n        \"\"\"\n\n        await self._state.http.pin_message(self.channel.id, self.id)\n        self.pinned = True", "output": "|coro|\n\n        Pins the message.\n\n        You must have the :attr:`~Permissions.manage_messages` permission to do\n        this in a non-private channel context.\n\n        Raises\n        -------\n        Forbidden\n            You do not have permissions to pin the message.\n        NotFound\n            The message or channel was not found or deleted.\n        HTTPException\n            Pinning the message failed, probably due to the channel\n            having more than 50 pinned messages.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_whitelist(host, whitelist):\n    ''' \n\n     '''\n    if ':' not in host:\n        host = host + ':80'\n\n    if host in whitelist:\n        return True\n\n    return any(match_host(host, pattern) for pattern in whitelist)", "output": "Check a given request host against a whitelist.\n\n    Args:\n        host (str) :\n            A host string to compare against a whitelist.\n\n            If the host does not specify a port, then ``\":80\"`` is implicitly\n            assumed.\n\n        whitelist (seq[str]) :\n            A list of host patterns to match against\n\n    Returns:\n        ``True``, if ``host`` matches any pattern in ``whitelist``, otherwise\n        ``False``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _MakeFileDescriptorProto(proto_file_name, full_name, field_items):\n  \"\"\"\"\"\"\n  package, name = full_name.rsplit('.', 1)\n  file_proto = descriptor_pb2.FileDescriptorProto()\n  file_proto.name = os.path.join(package.replace('.', '/'), proto_file_name)\n  file_proto.package = package\n  desc_proto = file_proto.message_type.add()\n  desc_proto.name = name\n  for f_number, (f_name, f_type) in enumerate(field_items, 1):\n    field_proto = desc_proto.field.add()\n    field_proto.name = f_name\n    field_proto.number = f_number\n    field_proto.label = descriptor_pb2.FieldDescriptorProto.LABEL_OPTIONAL\n    field_proto.type = f_type\n  return file_proto", "output": "Populate FileDescriptorProto for MessageFactory's DescriptorPool.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_w3c_caps(caps):\n    \"\"\"\n    \"\"\"\n    caps = copy.deepcopy(caps)\n    profile = caps.get('firefox_profile')\n    always_match = {}\n    if caps.get('proxy') and caps['proxy'].get('proxyType'):\n        caps['proxy']['proxyType'] = caps['proxy']['proxyType'].lower()\n    for k, v in caps.items():\n        if v and k in _OSS_W3C_CONVERSION:\n            always_match[_OSS_W3C_CONVERSION[k]] = v.lower() if k == 'platform' else v\n        if k in _W3C_CAPABILITY_NAMES or ':' in k:\n            always_match[k] = v\n    if profile:\n        moz_opts = always_match.get('moz:firefoxOptions', {})\n        # If it's already present, assume the caller did that intentionally.\n        if 'profile' not in moz_opts:\n            # Don't mutate the original capabilities.\n            new_opts = copy.deepcopy(moz_opts)\n            new_opts['profile'] = profile\n            always_match['moz:firefoxOptions'] = new_opts\n    return {\"firstMatch\": [{}], \"alwaysMatch\": always_match}", "output": "Makes a W3C alwaysMatch capabilities object.\n\n    Filters out capability names that are not in the W3C spec. Spec-compliant\n    drivers will reject requests containing unknown capability names.\n\n    Moves the Firefox profile, if present, from the old location to the new Firefox\n    options object.\n\n    :Args:\n     - caps - A dictionary of capabilities requested by the caller.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self, obj, matches=None, mt=None, lt=None, eq=None):\n        '''\n        \n        '''\n        updated = False\n        objects = list()\n        for _obj in self.get(obj.__class__):\n            if self.__criteria(_obj, matches=matches, mt=mt, lt=lt, eq=eq):\n                objects.append(obj)\n                updated = True\n            else:\n                objects.append(_obj)\n        self.flush(obj._TABLE)\n        self.create_table_from_object(obj)\n        for obj in objects:\n            self.store(obj)\n\n        return updated", "output": "Update object(s) in the database.\n\n        :param obj:\n        :param matches:\n        :param mt:\n        :param lt:\n        :param eq:\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_arrow_type(at):\n    \"\"\" \n    \"\"\"\n    import pyarrow.types as types\n    if types.is_boolean(at):\n        spark_type = BooleanType()\n    elif types.is_int8(at):\n        spark_type = ByteType()\n    elif types.is_int16(at):\n        spark_type = ShortType()\n    elif types.is_int32(at):\n        spark_type = IntegerType()\n    elif types.is_int64(at):\n        spark_type = LongType()\n    elif types.is_float32(at):\n        spark_type = FloatType()\n    elif types.is_float64(at):\n        spark_type = DoubleType()\n    elif types.is_decimal(at):\n        spark_type = DecimalType(precision=at.precision, scale=at.scale)\n    elif types.is_string(at):\n        spark_type = StringType()\n    elif types.is_binary(at):\n        spark_type = BinaryType()\n    elif types.is_date32(at):\n        spark_type = DateType()\n    elif types.is_timestamp(at):\n        spark_type = TimestampType()\n    elif types.is_list(at):\n        if types.is_timestamp(at.value_type):\n            raise TypeError(\"Unsupported type in conversion from Arrow: \" + str(at))\n        spark_type = ArrayType(from_arrow_type(at.value_type))\n    elif types.is_struct(at):\n        if any(types.is_struct(field.type) for field in at):\n            raise TypeError(\"Nested StructType not supported in conversion from Arrow: \" + str(at))\n        return StructType(\n            [StructField(field.name, from_arrow_type(field.type), nullable=field.nullable)\n             for field in at])\n    else:\n        raise TypeError(\"Unsupported type in conversion from Arrow: \" + str(at))\n    return spark_type", "output": "Convert pyarrow type to Spark data type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_master(path=MASTER_CF):\n    '''\n    \n    '''\n    with salt.utils.files.fopen(path, 'r') as fh_:\n        full_conf = salt.utils.stringutils.to_unicode(fh_.read())\n\n    # Condense the file based on line continuations, but keep order, comments\n    # and whitespace\n    conf_list = []\n    conf_dict = {}\n    for line in full_conf.splitlines():\n        if not line.strip() or line.strip().startswith('#'):\n            conf_list.append(line)\n            continue\n        comps = line.strip().split()\n        conf_line = {\n            'service': comps[0],\n            'conn_type': comps[1],\n            'private': comps[2],\n            'unpriv': comps[3],\n            'chroot': comps[4],\n            'wakeup': comps[5],\n            'maxproc': comps[6],\n            'command': ' '.join(comps[7:]),\n        }\n        dict_key = '{0} {1}'.format(comps[0], comps[1])\n        conf_list.append(conf_line)\n        conf_dict[dict_key] = conf_line\n\n    return conf_dict, conf_list", "output": "Parse the master.cf file. This file is essentially a whitespace-delimited\n    columnar file. The columns are: service, type, private (yes), unpriv (yes),\n    chroot (yes), wakeup (never), maxproc (100), command + args.\n\n    This function parses out the columns, leaving empty lines and comments\n    intact. Where the value doesn't detract from the default, a dash (-) will\n    be used.\n\n    Returns a dict of the active config lines, and a list of the entire file,\n    in order. These compliment each other.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_defined(self, objtxt, force_import=False):\r\n        \"\"\"\"\"\"\r\n        return isdefined(objtxt, force_import=force_import,\r\n                         namespace=self.locals)", "output": "Return True if object is defined", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def backup(file_name, jail=None, chroot=None, root=None):\n    '''\n    \n    '''\n    ret = __salt__['cmd.run'](\n        _pkg(jail, chroot, root) + ['backup', '-d', file_name],\n        output_loglevel='trace',\n        python_shell=False\n    )\n    return ret.split('...')[1]", "output": "Export installed packages into yaml+mtree file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.backup /tmp/pkg\n\n    jail\n        Backup packages from the specified jail. Note that this will run the\n        command within the jail, and so the path to the backup file will be\n        relative to the root of the jail\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.backup /tmp/pkg jail=<jail name or id>\n\n    chroot\n        Backup packages from the specified chroot (ignored if ``jail`` is\n        specified). Note that this will run the command within the chroot, and\n        so the path to the backup file will be relative to the root of the\n        chroot.\n\n    root\n        Backup packages from the specified root (ignored if ``jail`` is\n        specified). Note that this will run the command within the root, and\n        so the path to the backup file will be relative to the root of the\n        root.\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.backup /tmp/pkg chroot=/path/to/chroot", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def text(self):\n        \"\"\"\n        \"\"\"\n\n        # Try charset from content-type\n        content = None\n        encoding = self.encoding\n\n        if not self.content:\n            return str('')\n\n        # Fallback to auto-detected encoding.\n        if self.encoding is None:\n            encoding = self.apparent_encoding\n\n        # Decode unicode from given encoding.\n        try:\n            content = str(self.content, encoding, errors='replace')\n        except (LookupError, TypeError):\n            # A LookupError is raised if the encoding was not found which could\n            # indicate a misspelling or similar mistake.\n            #\n            # A TypeError can be raised if encoding is None\n            #\n            # So we try blindly encoding.\n            content = str(self.content, errors='replace')\n\n        return content", "output": "Content of the response, in unicode.\n\n        If Response.encoding is None, encoding will be guessed using\n        ``chardet``.\n\n        The encoding of the response content is determined based solely on HTTP\n        headers, following RFC 2616 to the letter. If you can take advantage of\n        non-HTTP knowledge to make a better guess at the encoding, you should\n        set ``r.encoding`` appropriately before accessing this property.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def SMA(Series, N, M=1):\n    \"\"\"\n    \n    \"\"\"\n    ret = []\n    i = 1\n    length = len(Series)\n    # \u8df3\u8fc7X\u4e2d\u524d\u9762\u51e0\u4e2a nan \u503c\n    while i < length:\n        if np.isnan(Series.iloc[i]):\n            i += 1\n        else:\n            break\n    preY = Series.iloc[i]  # Y'\n    ret.append(preY)\n    while i < length:\n        Y = (M * Series.iloc[i] + (N - M) * preY) / float(N)\n        ret.append(Y)\n        preY = Y\n        i += 1\n    return pd.Series(ret, index=Series.tail(len(ret)).index)", "output": "\u5a01\u5ec9SMA\u7b97\u6cd5\n\n    \u672c\u6b21\u4fee\u6b63\u4e3b\u8981\u662f\u5bf9\u4e8e\u8fd4\u56de\u503c\u7684\u4f18\u5316,\u73b0\u5728\u7684\u8fd4\u56de\u503c\u4f1a\u5e26\u4e0a\u539f\u5148\u8f93\u5165\u7684\u7d22\u5f15index\n    2018/5/3\n    @yutiansut", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _single_replace(self, to_replace, method, inplace, limit):\n    \"\"\"\n    \n    \"\"\"\n    if self.ndim != 1:\n        raise TypeError('cannot replace {0} with method {1} on a {2}'\n                        .format(to_replace, method, type(self).__name__))\n\n    orig_dtype = self.dtype\n    result = self if inplace else self.copy()\n    fill_f = missing.get_fill_func(method)\n\n    mask = missing.mask_missing(result.values, to_replace)\n    values = fill_f(result.values, limit=limit, mask=mask)\n\n    if values.dtype == orig_dtype and inplace:\n        return\n\n    result = pd.Series(values, index=self.index,\n                       dtype=self.dtype).__finalize__(self)\n\n    if inplace:\n        self._update_inplace(result._data)\n        return\n\n    return result", "output": "Replaces values in a Series using the fill method specified when no\n    replacement value is given in the replace method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_batch_begin(self, last_input, last_target, train, **kwargs):\n        \"\"\n        if not train: return\n        lambd = np.random.beta(self.alpha, self.alpha, last_target.size(0))\n        lambd = np.concatenate([lambd[:,None], 1-lambd[:,None]], 1).max(1)\n        lambd = last_input.new(lambd)\n        shuffle = torch.randperm(last_target.size(0)).to(last_input.device)\n        x1, y1 = last_input[shuffle], last_target[shuffle]\n        if self.stack_x:\n            new_input = [last_input, last_input[shuffle], lambd]\n        else: \n            new_input = (last_input * lambd.view(lambd.size(0),1,1,1) + x1 * (1-lambd).view(lambd.size(0),1,1,1))\n        if self.stack_y:\n            new_target = torch.cat([last_target[:,None].float(), y1[:,None].float(), lambd[:,None].float()], 1)\n        else:\n            if len(last_target.shape) == 2:\n                lambd = lambd.unsqueeze(1).float()\n            new_target = last_target.float() * lambd + y1.float() * (1-lambd)\n        return {'last_input': new_input, 'last_target': new_target}", "output": "Applies mixup to `last_input` and `last_target` if `train`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_np_compat():\n    \"\"\"\n    \n    \"\"\"\n    curr = ctypes.c_bool()\n    check_call(_LIB.MXIsNumpyCompatible(ctypes.byref(curr)))\n    return curr.value", "output": "Checks whether the NumPy compatibility is currently turned on.\n    NumPy-compatibility is turned off by default in backend.\n\n    Returns\n    -------\n        A bool value indicating whether the NumPy compatibility is currently on.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_indexer(cls, name, indexer):\n        \"\"\"\"\"\"\n        if getattr(cls, name, None) is None:\n            _indexer = functools.partial(indexer, name)\n            setattr(cls, name, property(_indexer, doc=indexer.__doc__))", "output": "Create an indexer like _name in the class.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_scaling_policy_arn(as_group, scaling_policy_name, region=None,\n                           key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    retries = 30\n    while retries > 0:\n        retries -= 1\n        try:\n            policies = conn.get_all_policies(as_group=as_group)\n            for policy in policies:\n                if policy.name == scaling_policy_name:\n                    return policy.policy_arn\n            log.error('Could not convert: %s', as_group)\n            return None\n        except boto.exception.BotoServerError as e:\n            if e.error_code != 'Throttling':\n                raise\n            log.debug('Throttled by API, will retry in 5 seconds')\n            time.sleep(5)\n\n    log.error('Maximum number of retries exceeded')\n    return None", "output": "Return the arn for a scaling policy in a specific autoscale group or None\n    if not found. Mainly used as a helper method for boto_cloudwatch_alarm, for\n    linking alarms to scaling policies.\n\n    CLI Example::\n\n        salt '*' boto_asg.get_scaling_policy_arn mygroup mypolicy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Update(self, menu=None, tooltip=None,filename=None, data=None, data_base64=None,):\n        '''\n        \n        '''\n        # Menu\n        if menu is not None:\n            self.Menu = menu\n            qmenu = QMenu()\n            qmenu.setTitle(self.Menu[0])\n            AddTrayMenuItem(qmenu, self.Menu[1], self)\n            self.TrayIcon.setContextMenu(qmenu)\n        # Tooltip\n        if tooltip is not None:\n            self.TrayIcon.setToolTip(str(tooltip))\n        # Icon\n        qicon = None\n        if filename is not None:\n            qicon = QIcon(filename)\n        elif data is not None:\n            ba = QtCore.QByteArray.fromRawData(data)\n            pixmap = QtGui.QPixmap()\n            pixmap.loadFromData(ba)\n            qicon = QIcon(pixmap)\n        elif data_base64 is not None:\n            ba = QtCore.QByteArray.fromBase64(data_base64)\n            pixmap = QtGui.QPixmap()\n            pixmap.loadFromData(ba)\n            qicon = QIcon(pixmap)\n        if qicon is not None:\n            self.TrayIcon.setIcon(qicon)", "output": "Updates the menu, tooltip or icon\n        :param menu: menu defintion\n        :param tooltip: string representing tooltip\n        :param filename:  icon filename\n        :param data:  icon raw image\n        :param data_base64: icon base 64 image\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_address(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The create_address function must be called with -f or --function.'\n        )\n\n    if not kwargs or 'name' not in kwargs:\n        log.error(\n            'A name must be specified when creating an address.'\n        )\n        return False\n    if 'region' not in kwargs:\n        log.error(\n            'A region must be specified for the address.'\n        )\n        return False\n\n    name = kwargs['name']\n    ex_region = kwargs['region']\n    ex_address = kwargs.get(\"address\", None)\n    kwargs['region'] = _expand_region(kwargs['region'])\n\n    conn = get_conn()\n\n    __utils__['cloud.fire_event'](\n        'event',\n        'create address',\n        'salt/cloud/address/creating',\n        args=salt.utils.data.simple_types_filter(kwargs),\n        sock_dir=__opts__['sock_dir'],\n        transport=__opts__['transport']\n    )\n\n    addy = conn.ex_create_address(name, ex_region, ex_address)\n\n    __utils__['cloud.fire_event'](\n        'event',\n        'created address',\n        'salt/cloud/address/created',\n        args=salt.utils.data.simple_types_filter(kwargs),\n        sock_dir=__opts__['sock_dir'],\n        transport=__opts__['transport']\n    )\n\n    log.info('Created GCE Address %s', name)\n\n    return _expand_address(addy)", "output": "Create a static address in a region.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f create_address gce name=my-ip region=us-central1 address=IP", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetch(self):\n        '''\n        \n        '''\n        try:\n            with self.gen_lock(lock_type='update'):\n                log.debug('Fetching %s remote \\'%s\\'', self.role, self.id)\n                # Run provider-specific fetch code\n                return self._fetch()\n        except GitLockError as exc:\n            if exc.errno == errno.EEXIST:\n                log.warning(\n                    'Update lock file is present for %s remote \\'%s\\', '\n                    'skipping. If this warning persists, it is possible that '\n                    'the update process was interrupted, but the lock could '\n                    'also have been manually set. Removing %s or running '\n                    '\\'salt-run cache.clear_git_lock %s type=update\\' will '\n                    'allow updates to continue for this remote.',\n                    self.role,\n                    self.id,\n                    self._get_lock_file(lock_type='update'),\n                    self.role,\n                )\n            return False", "output": "Fetch the repo. If the local copy was updated, return True. If the\n        local copy was already up-to-date, return False.\n\n        This function requires that a _fetch() function be implemented in a\n        sub-class.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def export_module_spec(spec, path, checkpoint_path, name_transform_fn):\n  \"\"\"\"\"\"\n  with tf.Graph().as_default():\n    m = Module(spec)\n    assign_map = {\n        name_transform_fn(name): value for name, value in m.variable_map.items()\n    }\n    tf_v1.train.init_from_checkpoint(checkpoint_path, assign_map)\n    init_op = tf_v1.initializers.global_variables()\n    with tf_v1.Session() as session:\n      session.run(init_op)\n      m.export(path, session)", "output": "Helper function to ModuleSpec.export().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_pipe(self, name, component):\n        \"\"\"\n        \"\"\"\n        if name not in self.pipe_names:\n            raise ValueError(Errors.E001.format(name=name, opts=self.pipe_names))\n        self.pipeline[self.pipe_names.index(name)] = (name, component)", "output": "Replace a component in the pipeline.\n\n        name (unicode): Name of the component to replace.\n        component (callable): Pipeline component.\n\n        DOCS: https://spacy.io/api/language#replace_pipe", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def L1(layer=\"input\", constant=0, batch=None):\n  \"\"\"\"\"\"\n  if batch is None:\n    return lambda T: tf.reduce_sum(tf.abs(T(layer) - constant))\n  else:\n    return lambda T: tf.reduce_sum(tf.abs(T(layer)[batch] - constant))", "output": "L1 norm of layer. Generally used as penalty.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ModifiedDecoder(wire_type, decode_value, modify_value):\n  \"\"\"\n  \"\"\"\n\n  # Reusing _SimpleDecoder is slightly slower than copying a bunch of code, but\n  # not enough to make a significant difference.\n\n  def InnerDecode(buffer, pos):\n    (result, new_pos) = decode_value(buffer, pos)\n    return (modify_value(result), new_pos)\n  return _SimpleDecoder(wire_type, InnerDecode)", "output": "Like SimpleDecoder but additionally invokes modify_value on every value\n  before storing it.  Usually modify_value is ZigZagDecode.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_absent(name, family='ipv4', **kwargs):\n    '''\n    \n    '''\n\n    ret = {'name': name,\n           'changes': {},\n           'result': None,\n           'comment': ''}\n\n    set_check = __salt__['ipset.check_set'](name, family)\n    if not set_check:\n        ret['result'] = True\n        ret['comment'] = ('ipset set {0} for {1} is already absent'\n                          .format(name, family))\n        return ret\n    if __opts__['test']:\n        ret['comment'] = 'ipset set {0} for {1} would be removed'.format(\n            name,\n            family)\n        return ret\n    flush_set = __salt__['ipset.flush'](name, family)\n    if flush_set:\n        command = __salt__['ipset.delete_set'](name, family)\n        if command is True:\n            ret['changes'] = {'locale': name}\n            ret['result'] = True\n            ret['comment'] = ('ipset set {0} deleted successfully for family {1}'\n                              .format(name, family))\n        else:\n            ret['result'] = False\n            ret['comment'] = ('Failed to delete set {0} for {2}: {1}'\n                              .format(name, command.strip(), family))\n    else:\n        ret['result'] = False\n        ret['comment'] = 'Failed to flush set {0} for {2}: {1}'.format(\n            name,\n            flush_set.strip(),\n            family\n        )\n    return ret", "output": ".. versionadded:: 2014.7.0\n\n    Verify the set is absent.\n\n    family\n        Networking family, either ipv4 or ipv6", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge_configs(default, overwrite):\n    \"\"\"\n    \"\"\"\n    new_config = copy.deepcopy(default)\n\n    for k, v in overwrite.items():\n        # Make sure to preserve existing items in\n        # nested dicts, for example `abbreviations`\n        if isinstance(v, dict):\n            new_config[k] = merge_configs(default[k], v)\n        else:\n            new_config[k] = v\n\n    return new_config", "output": "Recursively update a dict with the key/value pair of another.\n\n    Dict values that are dictionaries themselves will be updated, whilst\n    preserving existing keys.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _execute_command(self, key, *args):\n        \"\"\"\n        \"\"\"\n        client = self.redis_clients[key.redis_shard_hash() % len(\n            self.redis_clients)]\n        return client.execute_command(*args)", "output": "Execute a Redis command on the appropriate Redis shard based on key.\n\n        Args:\n            key: The object ID or the task ID that the query is about.\n            args: The command to run.\n\n        Returns:\n            The value returned by the Redis command.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compute_area_features(features, max_area_width, max_area_height=1, height=1,\n                          epsilon=1e-6):\n  \"\"\"\n  \"\"\"\n  with tf.name_scope(\"compute_area_features\"):\n    tf.logging.info(\"area_attention compute_area_features: %d x %d\",\n                    max_area_height, max_area_width)\n    area_sum, area_heights, area_widths = _compute_sum_image(\n        features, max_area_width=max_area_width,\n        max_area_height=max_area_height, height=height)\n    area_squared_sum, _, _ = _compute_sum_image(\n        tf.pow(features, 2), max_area_width=max_area_width,\n        max_area_height=max_area_height, height=height)\n    sizes = tf.multiply(area_heights, area_widths)\n    float_area_sizes = tf.to_float(sizes)\n    area_mean = tf.div(area_sum, float_area_sizes)\n    s2_n = tf.div(area_squared_sum, float_area_sizes)\n    area_variance = tf.subtract(s2_n, tf.pow(area_mean, 2))\n    area_std = tf.sqrt(tf.abs(area_variance) + epsilon)\n    return area_mean, area_std, area_sum, area_heights, area_widths", "output": "Computes features for each area.\n\n  Args:\n    features: a Tensor in a shape of [batch_size, height * width, depth].\n    max_area_width: the max width allowed for an area.\n    max_area_height: the max height allowed for an area.\n    height: the height of the image.\n    epsilon: the epsilon added to the variance for computing standard deviation.\n  Returns:\n    area_mean: A Tensor of shape [batch_size, num_areas, depth]\n    area_std: A Tensor of shape [batch_size, num_areas, depth]\n    area_sum: A Tensor of shape [batch_size, num_areas, depth]\n    area_heights: A Tensor of shape [batch_size, num_areas, 1]\n    area_widths: A Tensor of shape [batch_size, num_areas, 1]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _learner_interpret(learn:Learner, ds_type:DatasetType=DatasetType.Valid):\n    \"\"\n    return ClassificationInterpretation.from_learner(learn, ds_type=ds_type)", "output": "Create a `ClassificationInterpretation` object from `learner` on `ds_type` with `tta`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extract_random_video_patch(videos, num_frames=-1):\n  \"\"\"\n  \"\"\"\n  if num_frames == -1:\n    return videos\n  batch_size, num_total_frames, h, w, c = common_layers.shape_list(videos)\n  if num_total_frames < num_frames:\n    raise ValueError(\"Expected num_frames <= %d, got %d\" %\n                     (num_total_frames, num_frames))\n\n  # Randomly choose start_inds for each video.\n  frame_start = tf.random_uniform(\n      shape=(batch_size,), minval=0, maxval=num_total_frames - num_frames + 1,\n      dtype=tf.int32)\n\n  # [start[0], start[0] + 1, ... start[0] + num_frames - 1] + ...\n  # [start[batch_size-1], ... start[batch_size-1] + num_frames - 1]\n  range_inds = tf.expand_dims(tf.range(num_frames), axis=0)\n  frame_inds = range_inds + tf.expand_dims(frame_start, axis=1)\n  frame_inds = tf.reshape(frame_inds, [-1])\n\n  # [0]*num_frames + [1]*num_frames + ... [batch_size-1]*num_frames\n  batch_inds = tf.expand_dims(tf.range(batch_size), axis=1)\n  batch_inds = tf.tile(batch_inds, [1, num_frames])\n  batch_inds = tf.reshape(batch_inds, [-1])\n\n  gather_inds = tf.stack((batch_inds, frame_inds), axis=1)\n  video_patches = tf.gather_nd(videos, gather_inds)\n  return tf.reshape(video_patches, (batch_size, num_frames, h, w, c))", "output": "For every video, extract a random consecutive patch of num_frames.\n\n  Args:\n    videos: 5-D Tensor, (NTHWC)\n    num_frames: Integer, if -1 then the entire video is returned.\n  Returns:\n    video_patch: 5-D Tensor, (NTHWC) with T = num_frames.\n  Raises:\n    ValueError: If num_frames is greater than the number of total frames in\n                the video.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_query_argument(\n        self,\n        name: str,\n        default: Union[None, str, _ArgDefaultMarker] = _ARG_DEFAULT,\n        strip: bool = True,\n    ) -> Optional[str]:\n        \"\"\"\n        \"\"\"\n        return self._get_argument(name, default, self.request.query_arguments, strip)", "output": "Returns the value of the argument with the given name\n        from the request query string.\n\n        If default is not provided, the argument is considered to be\n        required, and we raise a `MissingArgumentError` if it is missing.\n\n        If the argument appears in the url more than once, we return the\n        last value.\n\n        .. versionadded:: 3.2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_and_clean_wikicode(raw_content):\n  \"\"\"\"\"\"\n  wikicode = tfds.core.lazy_imports.mwparserfromhell.parse(raw_content)\n\n  # Filters for references, tables, and file/image links.\n  re_rm_wikilink = re.compile(\n      \"^(?:File|Image|Media):\", flags=re.IGNORECASE | re.UNICODE)\n  def rm_wikilink(obj):\n    return bool(re_rm_wikilink.match(six.text_type(obj.title)))\n  def rm_tag(obj):\n    return six.text_type(obj.tag) in {\"ref\", \"table\"}\n  def rm_template(obj):\n    return obj.name.lower() in {\n        \"reflist\", \"notelist\", \"notelist-ua\", \"notelist-lr\", \"notelist-ur\",\n        \"notelist-lg\"}\n\n  def try_remove_obj(obj, section):\n    try:\n      section.remove(obj)\n    except ValueError:\n      # For unknown reasons, objects are sometimes not found.\n      pass\n\n  section_text = []\n  # Filter individual sections to clean.\n  for section in wikicode.get_sections(\n      flat=True, include_lead=True, include_headings=True):\n    for obj in section.ifilter_wikilinks(matches=rm_wikilink, recursive=True):\n      try_remove_obj(obj, section)\n    for obj in section.ifilter_templates(matches=rm_template, recursive=True):\n      try_remove_obj(obj, section)\n    for obj in section.ifilter_tags(matches=rm_tag, recursive=True):\n      try_remove_obj(obj, section)\n\n    section_text.append(section.strip_code().strip())\n  return \"\\n\\n\".join(section_text)", "output": "Strips formatting and unwanted sections from raw page content.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hold_time(self, datetime=None):\n        \"\"\"\n        \"\"\"\n\n        def weights(x):\n            if sum(x['amount']) != 0:\n                return pd.Timestamp(self.datetime\n                                   ) - pd.to_datetime(x.datetime.max())\n            else:\n                return np.nan\n\n        if datetime is None:\n            return self.history_table.set_index(\n                'datetime',\n                drop=False\n            ).sort_index().groupby('code').apply(weights).dropna()\n        else:\n            return self.history_table.set_index(\n                'datetime',\n                drop=False\n            ).sort_index().loc[:datetime].groupby('code').apply(weights\n                                                               ).dropna()", "output": "\u6301\u4ed3\u65f6\u95f4\n\n        Keyword Arguments:\n            datetime {[type]} -- [description] (default: {None})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def empty(shape, ctx=None, dtype=None, stype=None):\n    \"\"\"\n    \"\"\"\n    if stype is None or stype == 'default':\n        return _empty_ndarray(shape, ctx, dtype)\n    else:\n        return _empty_sparse_ndarray(stype, shape, ctx, dtype)", "output": "Returns a new array of given shape and type, without initializing entries.\n\n    Parameters\n    ----------\n    shape : int or tuple of int\n        The shape of the empty array.\n    ctx : Context, optional\n        An optional device context (default is the current default context).\n    dtype : str or numpy.dtype, optional\n        An optional value type (default is `float32`).\n    stype : str, optional\n        An optional storage type (default is `default`).\n\n    Returns\n    -------\n    NDArray, CSRNDArray or RowSparseNDArray\n        A created array.\n\n    Examples\n    --------\n    >>> mx.nd.empty(1)\n    <NDArray 1 @cpu(0)>\n    >>> mx.nd.empty((1,2), mx.gpu(0))\n    <NDArray 1x2 @gpu(0)>\n    >>> mx.nd.empty((1,2), mx.gpu(0), 'float16')\n    <NDArray 1x2 @gpu(0)>\n    >>> mx.nd.empty((1,2), stype='csr')\n    <CSRNDArray 1x2 @cpu(0)>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(self, items):\n        \"\"\"\"\"\"\n        # Remove the ack ID from lease management, and decrement the\n        # byte counter.\n        for item in items:\n            if self._leased_messages.pop(item.ack_id, None) is not None:\n                self._bytes -= item.byte_size\n            else:\n                _LOGGER.debug(\"Item %s was not managed.\", item.ack_id)\n\n        if self._bytes < 0:\n            _LOGGER.debug(\"Bytes was unexpectedly negative: %d\", self._bytes)\n            self._bytes = 0", "output": "Remove messages from lease management.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _to_java(self):\n        \"\"\"\n        \n        \"\"\"\n\n        sc = SparkContext._active_spark_context\n        # TODO: persst validation metrics as well\n        _java_obj = JavaParams._new_java_obj(\n            \"org.apache.spark.ml.tuning.TrainValidationSplitModel\",\n            self.uid,\n            self.bestModel._to_java(),\n            _py2java(sc, []))\n        estimator, epms, evaluator = super(TrainValidationSplitModel, self)._to_java_impl()\n\n        _java_obj.set(\"evaluator\", evaluator)\n        _java_obj.set(\"estimator\", estimator)\n        _java_obj.set(\"estimatorParamMaps\", epms)\n\n        if self.subModels is not None:\n            java_sub_models = [sub_model._to_java() for sub_model in self.subModels]\n            _java_obj.setSubModels(java_sub_models)\n\n        return _java_obj", "output": "Transfer this instance to a Java TrainValidationSplitModel. Used for ML persistence.\n        :return: Java object equivalent to this instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def envs(self, ignore_cache=False):\n        '''\n        \n        '''\n        if not ignore_cache:\n            cache_match = salt.fileserver.check_env_cache(\n                self.opts,\n                self.env_cache\n            )\n            if cache_match is not None:\n                return cache_match\n        ret = set()\n        for repo in self.remotes:\n            repo_envs = repo.envs()\n            for env_list in six.itervalues(repo.saltenv_revmap):\n                repo_envs.update(env_list)\n            ret.update([x for x in repo_envs if repo.env_is_exposed(x)])\n        return sorted(ret)", "output": "Return a list of refs that can be used as environments", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parser_dispatch(flavor):\n    \"\"\"\n    \"\"\"\n    valid_parsers = list(_valid_parsers.keys())\n    if flavor not in valid_parsers:\n        raise ValueError('{invalid!r} is not a valid flavor, valid flavors '\n                         'are {valid}'\n                         .format(invalid=flavor, valid=valid_parsers))\n\n    if flavor in ('bs4', 'html5lib'):\n        if not _HAS_HTML5LIB:\n            raise ImportError(\"html5lib not found, please install it\")\n        if not _HAS_BS4:\n            raise ImportError(\n                \"BeautifulSoup4 (bs4) not found, please install it\")\n        import bs4\n        if LooseVersion(bs4.__version__) <= LooseVersion('4.2.0'):\n            raise ValueError(\"A minimum version of BeautifulSoup 4.2.1 \"\n                             \"is required\")\n\n    else:\n        if not _HAS_LXML:\n            raise ImportError(\"lxml not found, please install it\")\n    return _valid_parsers[flavor]", "output": "Choose the parser based on the input flavor.\n\n    Parameters\n    ----------\n    flavor : str\n        The type of parser to use. This must be a valid backend.\n\n    Returns\n    -------\n    cls : _HtmlFrameParser subclass\n        The parser class based on the requested input flavor.\n\n    Raises\n    ------\n    ValueError\n        * If `flavor` is not a valid backend.\n    ImportError\n        * If you do not have the requested `flavor`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def play(self, ctx, *, query):\n        \"\"\"\"\"\"\n\n        source = discord.PCMVolumeTransformer(discord.FFmpegPCMAudio(query))\n        ctx.voice_client.play(source, after=lambda e: print('Player error: %s' % e) if e else None)\n\n        await ctx.send('Now playing: {}'.format(query))", "output": "Plays a file from the local filesystem", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_get_option_list(ip=None, port=None):\n    \"\"\"\n\n\n    \"\"\"\n    global extension_market_list\n    extension_market_list = QA_fetch_get_extensionmarket_list(\n    ) if extension_market_list is None else extension_market_list\n\n    return extension_market_list.query('category==12 and market!=1')", "output": "\u671f\u6743\u5217\u8868\n\n    Keyword Arguments:\n        ip {[type]} -- [description] (default: {None})\n        port {[type]} -- [description] (default: {None})\n\n    ## \u671f\u6743 OPTION\n            1        12    \u4e34\u65f6\u671f\u6743(\u4e3b\u8981\u662f50ETF)\n            4        12    \u90d1\u5dde\u5546\u54c1\u671f\u6743         OZ\n            5        12    \u5927\u8fde\u5546\u54c1\u671f\u6743         OD\n            6        12    \u4e0a\u6d77\u5546\u54c1\u671f\u6743         OS\n            7        12     \u4e2d\u91d1\u6240\u671f\u6743         OJ\n            8        12    \u4e0a\u6d77\u80a1\u7968\u671f\u6743         QQ\n            9        12    \u6df1\u5733\u80a1\u7968\u671f\u6743      (\u63a8\u6d4b)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_bb(YY, y=\"deprecated\"):\n    \"\"\"\"\"\"\n    cols,rows = np.nonzero(YY)\n    if len(cols)==0: return np.zeros(4, dtype=np.float32)\n    top_row = np.min(rows)\n    left_col = np.min(cols)\n    bottom_row = np.max(rows)\n    right_col = np.max(cols)\n    return np.array([left_col, top_row, right_col, bottom_row], dtype=np.float32)", "output": "Convert mask YY to a bounding box, assumes 0 as background nonzero object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def length_of_indexer(indexer, target=None):\n    \"\"\"\n    \n    \"\"\"\n    if target is not None and isinstance(indexer, slice):\n        target_len = len(target)\n        start = indexer.start\n        stop = indexer.stop\n        step = indexer.step\n        if start is None:\n            start = 0\n        elif start < 0:\n            start += target_len\n        if stop is None or stop > target_len:\n            stop = target_len\n        elif stop < 0:\n            stop += target_len\n        if step is None:\n            step = 1\n        elif step < 0:\n            step = -step\n        return (stop - start + step - 1) // step\n    elif isinstance(indexer, (ABCSeries, Index, np.ndarray, list)):\n        return len(indexer)\n    elif not is_list_like_indexer(indexer):\n        return 1\n    raise AssertionError(\"cannot find the length of the indexer\")", "output": "return the length of a single non-tuple indexer which could be a slice", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keyPressEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        key = event.key()\r\n        if key in [Qt.Key_Up]:\r\n            self._parent.previous_row()\r\n        elif key in [Qt.Key_Down]:\r\n            self._parent.next_row()\r\n        elif key in [Qt.Key_Enter, Qt.Key_Return]:\r\n            self._parent.show_editor()\r\n        else:\r\n            super(ShortcutFinder, self).keyPressEvent(event)", "output": "Qt Override.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def new_with_params(self, param_groups:Collection[Collection[nn.Parameter]]):\n        \"\"\n        opt_func = getattr(self, 'opt_func', self.opt.__class__)\n        opt = opt_func([{'params': p, 'lr':0} for p in param_groups])\n        opt = self.__class__(opt, wd=self.wd, true_wd=self.true_wd, bn_wd=self.bn_wd)\n        opt.lr,opt.opt_func,opt.mom,opt.beta = self.lr,opt_func,self.mom,self.beta\n        return opt", "output": "Create a new `OptimWrapper` from `self` with another `layer_groups` but the same hyper-parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enable_server(name, backend, socket=DEFAULT_SOCKET_URL):\n    '''\n    \n    '''\n\n    if backend == '*':\n        backends = show_backends(socket=socket).split('\\n')\n    else:\n        backends = [backend]\n\n    results = {}\n    for backend in backends:\n        ha_conn = _get_conn(socket)\n        ha_cmd = haproxy.cmds.enableServer(server=name, backend=backend)\n        ha_conn.sendCmd(ha_cmd)\n        results[backend] = list_servers(backend, socket=socket)\n\n    return results", "output": "Enable Server in haproxy\n\n    name\n        Server to enable\n\n    backend\n        haproxy backend, or all backends if \"*\" is supplied\n\n    socket\n        haproxy stats socket, default ``/var/run/haproxy.sock``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' haproxy.enable_server web1.example.com www", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def loadAnns(self, ids=[]):\n        \"\"\"\n        \n        \"\"\"\n        if type(ids) == list:\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]", "output": "Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_annotation(code):\n    \"\"\"\n    \"\"\"\n    module = ast.parse(code)\n    assert type(module) is ast.Module, 'internal error #1'\n    assert len(module.body) == 1, 'Annotation contains more than one expression'\n    assert type(module.body[0]) is ast.Expr, 'Annotation is not expression'\n    return module.body[0]", "output": "Parse an annotation string.\n    Return an AST Expr node.\n    code: annotation string (excluding '@')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dbsize(host=None, port=None, db=None, password=None):\n    '''\n    \n    '''\n    server = _connect(host, port, db, password)\n    return server.dbsize()", "output": "Return the number of keys in the selected database\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.dbsize", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def data(self, index, role):\r\n        \"\"\"\"\"\"\r\n        if not index.isValid():\r\n            return None\r\n        if role == Qt.FontRole:\r\n            return self._font\r\n        label = ''\r\n        if index.column() == self.model.header_shape[1] - 1:\r\n            label = str(self.model.name(0, index.row()))\r\n        elif index.row() == self.model.header_shape[0] - 1:\r\n            label = str(self.model.name(1, index.column()))\r\n        if role == Qt.DisplayRole and label:\r\n            return label\r\n        elif role == Qt.ForegroundRole:\r\n            return self._foreground\r\n        elif role == Qt.BackgroundRole:\r\n            return self._background\r\n        elif role == Qt.BackgroundRole:\r\n            return self._palette.window()\r\n        return None", "output": "Get the information of the levels.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_row(self):\n        \"\"\"\"\"\"\n        row = self.currentIndex().row()\n        rows = self.source_model.rowCount()\n        if row + 1 == rows:\n            row = -1\n        self.selectRow(row + 1)", "output": "Move to next row from currently selected row.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show(config_file=False):\n    '''\n    \n    '''\n    cmd = 'sysctl'\n    ret = {}\n    out = __salt__['cmd.run_stdout'](cmd, output_loglevel='trace')\n    for line in out.splitlines():\n        if not line or '=' not in line:\n            continue\n        comps = line.split('=', 1)\n        ret[comps[0]] = comps[1]\n    return ret", "output": "Return a list of sysctl parameters for this minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sysctl.show", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_all_launch_configurations(region=None, key=None, keyid=None,\n                                  profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    retries = 30\n    while True:\n        try:\n            return conn.get_all_launch_configurations()\n        except boto.exception.BotoServerError as e:\n            if retries and e.code == 'Throttling':\n                log.debug('Throttled by AWS API, retrying in 5 seconds...')\n                time.sleep(5)\n                retries -= 1\n                continue\n            log.error(e)\n            return []", "output": "Fetch and return all Launch Configuration with details.\n\n    CLI example::\n\n        salt myminion boto_asg.get_all_launch_configurations", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_api_id(self, lambda_name):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            response = self.cf_client.describe_stack_resource(StackName=lambda_name,\n                                                              LogicalResourceId='Api')\n            return response['StackResourceDetail'].get('PhysicalResourceId', None)\n        except: # pragma: no cover\n            try:\n                # Try the old method (project was probably made on an older, non CF version)\n                response = self.apigateway_client.get_rest_apis(limit=500)\n\n                for item in response['items']:\n                    if item['name'] == lambda_name:\n                        return item['id']\n\n                logger.exception('Could not get API ID.')\n                return None\n            except: # pragma: no cover\n                # We don't even have an API deployed. That's okay!\n                return None", "output": "Given a lambda_name, return the API id.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dataset_to_stream(dataset, input_name, num_chunks=0, append_targets=False):\n  \"\"\"\"\"\"\n  for example in tfds.as_numpy(dataset):\n    inp, out = example[0][input_name], example[1]\n    if len(out.shape) > 1 and out.shape[-1] == 1:\n      out = np.squeeze(out, axis=-1)\n    if num_chunks > 0:\n      inp = np.split(inp, num_chunks, axis=1)\n      out = np.split(out, num_chunks, axis=1)\n    if append_targets:\n      inp = (inp, out)\n    yield inp, out", "output": "Takes a tf.Dataset and creates a numpy stream of ready batches.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mark_whole_doc_dirty(self):\n        \"\"\"\n        \n        \"\"\"\n        text_cursor = self._editor.textCursor()\n        text_cursor.select(text_cursor.Document)\n        self._editor.document().markContentsDirty(text_cursor.selectionStart(),\n                                                  text_cursor.selectionEnd())", "output": "Marks the whole document as dirty to force a full refresh. **SLOW**", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_batch_get(get_doc_response, reference_map, client):\n    \"\"\"\n    \"\"\"\n    result_type = get_doc_response.WhichOneof(\"result\")\n    if result_type == \"found\":\n        reference = _get_reference(get_doc_response.found.name, reference_map)\n        data = _helpers.decode_dict(get_doc_response.found.fields, client)\n        snapshot = DocumentSnapshot(\n            reference,\n            data,\n            exists=True,\n            read_time=get_doc_response.read_time,\n            create_time=get_doc_response.found.create_time,\n            update_time=get_doc_response.found.update_time,\n        )\n    elif result_type == \"missing\":\n        snapshot = DocumentSnapshot(\n            None,\n            None,\n            exists=False,\n            read_time=get_doc_response.read_time,\n            create_time=None,\n            update_time=None,\n        )\n    else:\n        raise ValueError(\n            \"`BatchGetDocumentsResponse.result` (a oneof) had a field other \"\n            \"than `found` or `missing` set, or was unset\"\n        )\n    return snapshot", "output": "Parse a `BatchGetDocumentsResponse` protobuf.\n\n    Args:\n        get_doc_response (~google.cloud.proto.firestore.v1beta1.\\\n            firestore_pb2.BatchGetDocumentsResponse): A single response (from\n            a stream) containing the \"get\" response for a document.\n        reference_map (Dict[str, .DocumentReference]): A mapping (produced\n            by :func:`_reference_info`) of fully-qualified document paths to\n            document references.\n        client (~.firestore_v1beta1.client.Client): A client that has\n            a document factory.\n\n    Returns:\n       [.DocumentSnapshot]: The retrieved snapshot.\n\n    Raises:\n        ValueError: If the response has a ``result`` field (a oneof) other\n            than ``found`` or ``missing``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_record_set(self, record_set):\n        \"\"\"\n        \"\"\"\n        if not isinstance(record_set, ResourceRecordSet):\n            raise ValueError(\"Pass a ResourceRecordSet\")\n        self._deletions += (record_set,)", "output": "Append a record set to the 'deletions' for the change set.\n\n        :type record_set:\n            :class:`google.cloud.dns.resource_record_set.ResourceRecordSet`\n        :param record_set: the record set to append.\n\n        :raises: ``ValueError`` if ``record_set`` is not of the required type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flatten_and_batch_shift_indices(indices: torch.Tensor,\n                                    sequence_length: int) -> torch.Tensor:\n    \"\"\"\n    \n    \"\"\"\n    # Shape: (batch_size)\n    offsets = get_range_vector(indices.size(0), get_device_of(indices)) * sequence_length\n    for _ in range(len(indices.size()) - 1):\n        offsets = offsets.unsqueeze(1)\n\n    # Shape: (batch_size, d_1, ..., d_n)\n    offset_indices = indices + offsets\n\n    # Shape: (batch_size * d_1 * ... * d_n)\n    offset_indices = offset_indices.view(-1)\n    return offset_indices", "output": "This is a subroutine for :func:`~batched_index_select`. The given ``indices`` of size\n    ``(batch_size, d_1, ..., d_n)`` indexes into dimension 2 of a target tensor, which has size\n    ``(batch_size, sequence_length, embedding_size)``. This function returns a vector that\n    correctly indexes into the flattened target. The sequence length of the target must be\n    provided to compute the appropriate offsets.\n\n    .. code-block:: python\n\n        indices = torch.ones([2,3], dtype=torch.long)\n        # Sequence length of the target tensor.\n        sequence_length = 10\n        shifted_indices = flatten_and_batch_shift_indices(indices, sequence_length)\n        # Indices into the second element in the batch are correctly shifted\n        # to take into account that the target tensor will be flattened before\n        # the indices are applied.\n        assert shifted_indices == [1, 1, 1, 11, 11, 11]\n\n    Parameters\n    ----------\n    indices : ``torch.LongTensor``, required.\n    sequence_length : ``int``, required.\n        The length of the sequence the indices index into.\n        This must be the second dimension of the tensor.\n\n    Returns\n    -------\n    offset_indices : ``torch.LongTensor``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __init_from_list_np2d(self, mats, params_str, ref_dataset):\n        \"\"\"\"\"\"\n        ncol = mats[0].shape[1]\n        nrow = np.zeros((len(mats),), np.int32)\n        if mats[0].dtype == np.float64:\n            ptr_data = (ctypes.POINTER(ctypes.c_double) * len(mats))()\n        else:\n            ptr_data = (ctypes.POINTER(ctypes.c_float) * len(mats))()\n\n        holders = []\n        type_ptr_data = None\n\n        for i, mat in enumerate(mats):\n            if len(mat.shape) != 2:\n                raise ValueError('Input numpy.ndarray must be 2 dimensional')\n\n            if mat.shape[1] != ncol:\n                raise ValueError('Input arrays must have same number of columns')\n\n            nrow[i] = mat.shape[0]\n\n            if mat.dtype == np.float32 or mat.dtype == np.float64:\n                mats[i] = np.array(mat.reshape(mat.size), dtype=mat.dtype, copy=False)\n            else:\n                # change non-float data to float data, need to copy\n                mats[i] = np.array(mat.reshape(mat.size), dtype=np.float32)\n\n            chunk_ptr_data, chunk_type_ptr_data, holder = c_float_array(mats[i])\n            if type_ptr_data is not None and chunk_type_ptr_data != type_ptr_data:\n                raise ValueError('Input chunks must have same type')\n            ptr_data[i] = chunk_ptr_data\n            type_ptr_data = chunk_type_ptr_data\n            holders.append(holder)\n\n        self.handle = ctypes.c_void_p()\n        _safe_call(_LIB.LGBM_DatasetCreateFromMats(\n            ctypes.c_int(len(mats)),\n            ctypes.cast(ptr_data, ctypes.POINTER(ctypes.POINTER(ctypes.c_double))),\n            ctypes.c_int(type_ptr_data),\n            nrow.ctypes.data_as(ctypes.POINTER(ctypes.c_int32)),\n            ctypes.c_int(ncol),\n            ctypes.c_int(C_API_IS_ROW_MAJOR),\n            c_str(params_str),\n            ref_dataset,\n            ctypes.byref(self.handle)))\n        return self", "output": "Initialize data from a list of 2-D numpy matrices.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keep_alive_timeout_callback(self):\n        \"\"\"\n        \n        \"\"\"\n        time_elapsed = time() - self._last_response_time\n        if time_elapsed < self.keep_alive_timeout:\n            time_left = self.keep_alive_timeout - time_elapsed\n            self._keep_alive_timeout_handler = self.loop.call_later(\n                time_left, self.keep_alive_timeout_callback\n            )\n        else:\n            logger.debug(\"KeepAlive Timeout. Closing connection.\")\n            self.transport.close()\n            self.transport = None", "output": "Check if elapsed time since last response exceeds our configured\n        maximum keep alive timeout value and if so, close the transport\n        pipe and let the response writer handle the error.\n\n        :return: None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def query(self, time_indices):\n    \"\"\"\n    \"\"\"\n    if self._disposed:\n      raise ValueError(\n          'Cannot query: this _WatchStore instance is already disposed')\n    if not isinstance(time_indices, (tuple, list)):\n      time_indices = [time_indices]\n    output = []\n    for time_index in time_indices:\n      if isinstance(self._data[time_index], _TensorValueDiscarded):\n        output.append(None)\n      else:\n        data_item = self._data[time_index]\n        if (hasattr(data_item, 'dtype') and\n            tensor_helper.translate_dtype(data_item.dtype) == 'string'):\n          _, _, data_item = tensor_helper.array_view(data_item)\n          data_item = np.array(\n              tensor_helper.process_buffers_for_display(data_item),\n              dtype=np.object)\n        output.append(data_item)\n\n    return output", "output": "Query the values at given time indices.\n\n    Args:\n      time_indices: 0-based time indices to query, as a `list` of `int`.\n\n    Returns:\n      Values as a list of `numpy.ndarray` (for time indices in memory) or\n      `None` (for time indices discarded).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def session_scope(nullpool):\n    \"\"\"\"\"\"\n    if nullpool:\n        engine = sqlalchemy.create_engine(\n            app.config.get('SQLALCHEMY_DATABASE_URI'), poolclass=NullPool)\n        session_class = sessionmaker()\n        session_class.configure(bind=engine)\n        session = session_class()\n    else:\n        session = db.session()\n        session.commit()  # HACK\n\n    try:\n        yield session\n        session.commit()\n    except Exception as e:\n        session.rollback()\n        logging.exception(e)\n        raise\n    finally:\n        session.close()", "output": "Provide a transactional scope around a series of operations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def minimum(self, node):\n        \"\"\"\n         \n        \"\"\"\n        temp_node = node\n        while temp_node.left:\n            temp_node = temp_node.left\n        return temp_node", "output": "find the minimum node when node regard as a root node   \n        :param node:\n        :return: minimum node", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_users(runas=None):\n    '''\n    \n    '''\n    # Windows runas currently requires a password.\n    # Due to this, don't use a default value for\n    # runas in Windows.\n    if runas is None and not salt.utils.platform.is_windows():\n        runas = salt.utils.user.get_user()\n    res = __salt__['cmd.run_all'](\n        [RABBITMQCTL, 'list_users', '-q'],\n        reset_system_locale=False,\n        runas=runas,\n        python_shell=False)\n\n    # func to get tags from string such as \"[admin, monitoring]\"\n    func = lambda string: [x.strip() for x in string[1:-1].split(',')] if ',' in string else [x for x in\n                                                                                              string[1:-1].split(' ')]\n    return _output_to_dict(res, func)", "output": "Return a list of users based off of rabbitmqctl user_list.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rabbitmq.list_users", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sell(self, security, price=0, amount=0, volume=0, entrust_prop=0):\n        \"\"\"\n        \"\"\"\n        return self._trade(security, price, amount, volume, \"sell\")", "output": "\u5356\u51fa\u80a1\u7968\n        :param security: \u80a1\u7968\u4ee3\u7801\n        :param price: \u5356\u51fa\u4ef7\u683c\n        :param amount: \u5356\u51fa\u80a1\u6570\n        :param volume: \u5356\u51fa\u603b\u91d1\u989d \u7531 volume / price \u53d6\u6574\uff0c \u82e5\u6307\u5b9a price \u5219\u6b64\u53c2\u6570\u65e0\u6548\n        :param entrust_prop:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_search_scores(query, choices, ignore_case=True, template='{}',\n                      valid_only=False, sort=False):\n    \"\"\"\n    \"\"\"\n    # First remove spaces from query\n    query = query.replace(' ', '')\n    pattern = get_search_regex(query, ignore_case)\n    results = []\n\n    for choice in choices:\n        r = re.search(pattern, choice)\n        if query and r:\n            result = get_search_score(query, choice, ignore_case=ignore_case,\n                                      apply_regex=False, template=template)\n        else:\n            if query:\n                result = (choice, choice, NOT_FOUND_SCORE)\n            else:\n                result = (choice, choice, NO_SCORE)\n\n        if valid_only:\n            if result[-1] != NOT_FOUND_SCORE:\n                results.append(result)\n        else:\n            results.append(result)\n\n    if sort:\n        results = sorted(results, key=lambda row: row[-1])\n\n    return results", "output": "Search for query inside choices and return a list of tuples.\n\n    Returns a list of tuples of text with the enriched text (if a template is\n    provided) and a score for the match. Lower scores imply a better match.\n\n    Parameters\n    ----------\n    query : str\n        String with letters to search in each choice (in order of appearance).\n    choices : list of str\n        List of sentences/words in which to search for the 'query' letters.\n    ignore_case : bool, optional\n        Optional value perform a case insensitive search (True by default).\n    template : str, optional\n        Optional template string to surround letters found in choices. This is\n        useful when using a rich text editor ('{}' by default).\n        Examples: '<b>{}</b>', '<code>{}</code>', '<i>{}</i>'\n\n    Returns\n    -------\n    results : list of tuples\n        List of tuples where the first item is the text (enriched if a\n        template was used) and a search score. Lower scores means better match.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_classes(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            self.classes = self.imdbs[0].classes\n            self.num_classes = len(self.classes)\n        except AttributeError:\n            # fine, if no classes is provided\n            pass\n\n        if self.num_classes > 0:\n            for db in self.imdbs:\n                assert self.classes == db.classes, \"Multiple imdb must have same classes\"", "output": "check input imdbs, make sure they have same classes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ensure_dtype(func, argname, arg):\n    \"\"\"\n    \n    \"\"\"\n    try:\n        return dtype(arg)\n    except TypeError:\n        raise TypeError(\n            \"{func}() couldn't convert argument \"\n            \"{argname}={arg!r} to a numpy dtype.\".format(\n                func=_qualified_name(func),\n                argname=argname,\n                arg=arg,\n            ),\n        )", "output": "Argument preprocessor that converts the input into a numpy dtype.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from zipline.utils.preprocess import preprocess\n    >>> @preprocess(dtype=ensure_dtype)\n    ... def foo(dtype):\n    ...     return dtype\n    ...\n    >>> foo(float)\n    dtype('float64')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def eventFilter(self, obj, event):\r\n        \"\"\"\"\"\"\r\n        if obj == self.dataTable and event.type() == QEvent.Resize:\r\n            self._resizeVisibleColumnsToContents()\r\n        return False", "output": "Override eventFilter to catch resize event.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_revnet_big():\n  \"\"\"\"\"\"\n  hparams = transformer_revnet_base()\n\n  # The TransformerRevnet uses significantly less memory than the Transformer.\n  # Increase batch size and model size.\n  hparams.batch_size *= 2\n  hparams.hidden_size *= 2\n  hparams.num_heads *= 2\n  hparams.num_hidden_layers += 1\n  return hparams", "output": "Base hparams for TransformerRevnet.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def header(self, axis, x, level=0):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        ax = self._axis(axis)\r\n        return ax.values[x] if not hasattr(ax, 'levels') \\\r\n            else ax.values[x][level]", "output": "Return the values of the labels for the header of columns or rows.\r\n\r\n        The value corresponds to the header of column or row x in the\r\n        given level.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reduce(self, f):\n        \"\"\"\n        \n        \"\"\"\n        f = fail_on_stopiteration(f)\n\n        def func(iterator):\n            iterator = iter(iterator)\n            try:\n                initial = next(iterator)\n            except StopIteration:\n                return\n            yield reduce(f, iterator, initial)\n\n        vals = self.mapPartitions(func).collect()\n        if vals:\n            return reduce(f, vals)\n        raise ValueError(\"Can not reduce() empty RDD\")", "output": "Reduces the elements of this RDD using the specified commutative and\n        associative binary operator. Currently reduces partitions locally.\n\n        >>> from operator import add\n        >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n        15\n        >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n        10\n        >>> sc.parallelize([]).reduce(add)\n        Traceback (most recent call last):\n            ...\n        ValueError: Can not reduce() empty RDD", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_min_eig_vec_proxy(self, use_tf_eig=False):\n    \"\"\"\n    \"\"\"\n    if use_tf_eig:\n      # If smoothness parameter is too small, essentially no smoothing\n      # Just output the eigen vector corresponding to min\n      return tf.cond(self.smooth_placeholder < 1E-8,\n                     self.tf_min_eig_vec,\n                     self.tf_smooth_eig_vec)\n\n    # Using autograph to automatically handle\n    # the control flow of minimum_eigen_vector\n    min_eigen_tf = autograph.to_graph(utils.minimum_eigen_vector)\n\n    def _vector_prod_fn(x):\n      return self.dual_object.get_psd_product(x)\n\n    estimated_eigen_vector = min_eigen_tf(\n        x=self.eig_init_vec_placeholder,\n        num_steps=self.eig_num_iter_placeholder,\n        learning_rate=self.params['eig_learning_rate'],\n        vector_prod_fn=_vector_prod_fn)\n    return estimated_eigen_vector", "output": "Computes the min eigen value and corresponding vector of matrix M.\n\n    Args:\n      use_tf_eig: Whether to use tf's default full eigen decomposition\n    Returns:\n      eig_vec: Minimum absolute eigen value\n      eig_val: Corresponding eigen vector", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare(\n        self,\n        config_path=None,\n        user=None,\n        password=None,\n        exe_path=None,\n        comm_password=None,\n        **kwargs\n    ):\n        \"\"\"\n        \n        \"\"\"\n        if config_path is not None:\n            account = helpers.file2dict(config_path)\n            user = account[\"user\"]\n            password = account[\"password\"]\n            comm_password = account.get(\"comm_password\")\n            exe_path = account.get(\"exe_path\")\n        self.login(\n            user,\n            password,\n            exe_path or self._config.DEFAULT_EXE_PATH,\n            comm_password,\n            **kwargs\n        )", "output": "\u767b\u9646\u5ba2\u6237\u7aef\n        :param config_path: \u767b\u9646\u914d\u7f6e\u6587\u4ef6\uff0c\u8ddf\u53c2\u6570\u767b\u9646\u65b9\u5f0f\u4e8c\u9009\u4e00\n        :param user: \u8d26\u53f7\n        :param password: \u660e\u6587\u5bc6\u7801\n        :param exe_path: \u5ba2\u6237\u7aef\u8def\u5f84\u7c7b\u4f3c r'C:\\\\htzqzyb2\\\\xiadan.exe', \u9ed8\u8ba4 r'C:\\\\htzqzyb2\\\\xiadan.exe'\n        :param comm_password: \u901a\u8baf\u5bc6\u7801\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_current_thumbnail(self, thumbnail):\n        \"\"\"\"\"\"\n        self.current_thumbnail = thumbnail\n        self.figure_viewer.load_figure(\n                thumbnail.canvas.fig, thumbnail.canvas.fmt)\n        for thumbnail in self._thumbnails:\n            thumbnail.highlight_canvas(thumbnail == self.current_thumbnail)", "output": "Set the currently selected thumbnail.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_tab_tip(self, filename, is_modified=None, is_readonly=None):\r\n        \"\"\"\"\"\"\r\n        text = u\"%s \u2014 %s\"\r\n        text = self.__modified_readonly_title(text,\r\n                                              is_modified, is_readonly)\r\n        if self.tempfile_path is not None\\\r\n           and filename == encoding.to_unicode_from_fs(self.tempfile_path):\r\n            temp_file_str = to_text_string(_(\"Temporary file\"))\r\n            return text % (temp_file_str, self.tempfile_path)\r\n        else:\r\n            return text % (osp.basename(filename), osp.dirname(filename))", "output": "Return tab menu title", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_data_volumes(vm_):\n    '''\n    \n    '''\n    ret = []\n    volumes = vm_['volumes']\n    for key, value in six.iteritems(volumes):\n        # Verify the required 'disk_size' property is present in the cloud\n        # profile config\n        if 'disk_size' not in volumes[key].keys():\n            raise SaltCloudConfigError(\n                'The volume \\'{0}\\' is missing \\'disk_size\\''.format(key)\n            )\n        # Use 'HDD' if no 'disk_type' property is present in cloud profile\n        if 'disk_type' not in volumes[key].keys():\n            volumes[key]['disk_type'] = 'HDD'\n\n        # Construct volume object and assign to a list.\n        volume = Volume(\n            name=key,\n            size=volumes[key]['disk_size'],\n            disk_type=volumes[key]['disk_type'],\n            licence_type='OTHER'\n        )\n\n        # Set volume availability zone if defined in the cloud profile\n        if 'disk_availability_zone' in volumes[key].keys():\n            volume.availability_zone = volumes[key]['disk_availability_zone']\n\n        ret.append(volume)\n\n    return ret", "output": "Construct a list of optional data volumes from the cloud profile", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forward(self, inputs, states=None): # pylint: disable=arguments-differ\n        \"\"\"\"\"\"\n        batch_size = inputs.shape[self._batch_axis]\n        skip_states = states is None\n        if skip_states:\n            states = self.cell.begin_state(batch_size, ctx=inputs.context)\n        if isinstance(states, ndarray.NDArray):\n            states = [states]\n        for state, info in zip(states, self.cell.state_info(batch_size)):\n            if state.shape != info['shape']:\n                raise ValueError(\n                    'Invalid recurrent state shape. Expecting %s, got %s.'%(\n                        str(info['shape']), str(state.shape)))\n        states = sum(zip(*((j for j in i) for i in states)), ())\n        outputs, states = self.cell.unroll(\n            inputs.shape[self._axis], inputs, states,\n            layout=self._layout, merge_outputs=True)\n\n        if skip_states:\n            return outputs\n        return outputs, states", "output": "Defines the forward computation. Arguments can be either\n        :py:class:`NDArray` or :py:class:`Symbol`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sort(self, *keys):\n        \"\"\"\n        \n        \"\"\"\n        s = self._clone()\n        s._sort = []\n        for k in keys:\n            if isinstance(k, string_types) and k.startswith('-'):\n                if k[1:] == '_score':\n                    raise IllegalOperation('Sorting by `-_score` is not allowed.')\n                k = {k[1:]: {\"order\": \"desc\"}}\n            s._sort.append(k)\n        return s", "output": "Add sorting information to the search request. If called without\n        arguments it will remove all sort requirements. Otherwise it will\n        replace them. Acceptable arguments are::\n\n            'some.field'\n            '-some.other.field'\n            {'different.field': {'any': 'dict'}}\n\n        so for example::\n\n            s = Search().sort(\n                'category',\n                '-title',\n                {\"price\" : {\"order\" : \"asc\", \"mode\" : \"avg\"}}\n            )\n\n        will sort by ``category``, ``title`` (in descending order) and\n        ``price`` in ascending order using the ``avg`` mode.\n\n        The API returns a copy of the Search object and can thus be chained.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_cog(self, cog):\n        \"\"\"\n        \"\"\"\n\n        if not isinstance(cog, Cog):\n            raise TypeError('cogs must derive from Cog')\n\n        cog = cog._inject(self)\n        self.__cogs[cog.__cog_name__] = cog", "output": "Adds a \"cog\" to the bot.\n\n        A cog is a class that has its own event listeners and commands.\n\n        Parameters\n        -----------\n        cog: :class:`.Cog`\n            The cog to register to the bot.\n\n        Raises\n        -------\n        TypeError\n            The cog does not inherit from :class:`.Cog`.\n        CommandError\n            An error happened during loading.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_tensor(self, name):\n        \"\"\"\n        \n        \"\"\"\n        name = get_op_tensor_name(name)[1]\n        if len(self.ns_name):\n            name_with_ns = self.ns_name + \"/\" + name\n        else:\n            name_with_ns = name\n\n        try:\n            ret = get_op_or_tensor_by_name(name_with_ns)\n        except KeyError:\n            if name in self._extra_tensor_names:\n                return self._extra_tensor_names[name]\n            raise\n        else:\n            if name in self._extra_tensor_names:\n                mapped_tensor = self._extra_tensor_names[name]\n                logger.info(\n                    \"'{}' may refer to both the Tensor/Placeholder '{}' or the input to the tower '{}'.\".format(\n                        name, ret.name, mapped_tensor.name) +\n                    \" Assuming it is the input '{}'.\".format(mapped_tensor.name))\n                return mapped_tensor\n            return ret", "output": "Get a tensor in this tower. The name can be:\n\n        1. The name of the tensor without any tower prefix.\n\n        2. A name in the input signature, if it is used when building the tower.\n\n        In the second case, this method will return the tensor that's used as the corresponding\n        input to the tower. Note that this tensor may have a different name (e.g. may be an output of a queue).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_monitor(redis_address,\n                  stdout_file=None,\n                  stderr_file=None,\n                  autoscaling_config=None,\n                  redis_password=None):\n    \"\"\"\n    \"\"\"\n    monitor_path = os.path.join(\n        os.path.dirname(os.path.abspath(__file__)), \"monitor.py\")\n    command = [\n        sys.executable, \"-u\", monitor_path,\n        \"--redis-address=\" + str(redis_address)\n    ]\n    if autoscaling_config:\n        command.append(\"--autoscaling-config=\" + str(autoscaling_config))\n    if redis_password:\n        command.append(\"--redis-password=\" + redis_password)\n    process_info = start_ray_process(\n        command,\n        ray_constants.PROCESS_TYPE_MONITOR,\n        stdout_file=stdout_file,\n        stderr_file=stderr_file)\n    return process_info", "output": "Run a process to monitor the other processes.\n\n    Args:\n        redis_address (str): The address that the Redis server is listening on.\n        stdout_file: A file handle opened for writing to redirect stdout to. If\n            no redirection should happen, then this should be None.\n        stderr_file: A file handle opened for writing to redirect stderr to. If\n            no redirection should happen, then this should be None.\n        autoscaling_config: path to autoscaling config file.\n        redis_password (str): The password of the redis server.\n\n    Returns:\n        ProcessInfo for the process that was started.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def netbsd_interfaces():\n    '''\n    \n    '''\n    # NetBSD versions prior to 8.0 can still use linux_interfaces()\n    if LooseVersion(os.uname()[2]) < LooseVersion('8.0'):\n        return linux_interfaces()\n\n    ifconfig_path = salt.utils.path.which('ifconfig')\n    cmd = subprocess.Popen(\n        '{0} -a'.format(ifconfig_path),\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT).communicate()[0]\n    return _netbsd_interfaces_ifconfig(salt.utils.stringutils.to_str(cmd))", "output": "Obtain interface information for NetBSD >= 8 where the ifconfig\n    output diverged from other BSD variants (Netmask is now part of the\n    address)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def append(self, other):\n        \"\"\"\n        \n        \"\"\"\n        if type(other) is not SArray:\n            raise RuntimeError(\"SArray append can only work with SArray\")\n\n        if self.dtype != other.dtype:\n            raise RuntimeError(\"Data types in both SArrays have to be the same\")\n\n        with cython_context():\n            return SArray(_proxy = self.__proxy__.append(other.__proxy__))", "output": "Append an SArray to the current SArray. Creates a new SArray with the\n        rows from both SArrays. Both SArrays must be of the same type.\n\n        Parameters\n        ----------\n        other : SArray\n            Another SArray whose rows are appended to current SArray.\n\n        Returns\n        -------\n        out : SArray\n            A new SArray that contains rows from both SArrays, with rows from\n            the ``other`` SArray coming after all rows from the current SArray.\n\n        See Also\n        --------\n        SFrame.append\n\n        Examples\n        --------\n        >>> sa = turicreate.SArray([1, 2, 3])\n        >>> sa2 = turicreate.SArray([4, 5, 6])\n        >>> sa.append(sa2)\n        dtype: int\n        Rows: 6\n        [1, 2, 3, 4, 5, 6]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _SnakeCaseToCamelCase(path_name):\n  \"\"\"\"\"\"\n  result = []\n  after_underscore = False\n  for c in path_name:\n    if c.isupper():\n      raise Error('Fail to print FieldMask to Json string: Path name '\n                  '{0} must not contain uppercase letters.'.format(path_name))\n    if after_underscore:\n      if c.islower():\n        result.append(c.upper())\n        after_underscore = False\n      else:\n        raise Error('Fail to print FieldMask to Json string: The '\n                    'character after a \"_\" must be a lowercase letter '\n                    'in path name {0}.'.format(path_name))\n    elif c == '_':\n      after_underscore = True\n    else:\n      result += c\n\n  if after_underscore:\n    raise Error('Fail to print FieldMask to Json string: Trailing \"_\" '\n                'in path name {0}.'.format(path_name))\n  return ''.join(result)", "output": "Converts a path name from snake_case to camelCase.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setClockFormat(clockFormat, **kwargs):\n    '''\n    \n\n    '''\n    if clockFormat != '12h' and clockFormat != '24h':\n        return False\n    _gsession = _GSettings(user=kwargs.get('user'),\n                           schema='org.gnome.desktop.interface',\n                           key='clock-format')\n    return _gsession._set(clockFormat)", "output": "Set the clock format, either 12h or 24h format.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' gnome.setClockFormat <12h|24h> user=<username>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_dns_challenge_txt(self, zone_id, domain, txt_challenge):\n        \"\"\"\n        \n        \"\"\"\n        print(\"Deleting DNS challenge..\")\n        resp = self.route53.change_resource_record_sets(\n            HostedZoneId=zone_id,\n            ChangeBatch=self.get_dns_challenge_change_batch('DELETE', domain, txt_challenge)\n        )\n\n        return resp", "output": "Remove DNS challenge TXT.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def maybe_future(x: Any) -> Future:\n    \"\"\"\n    \"\"\"\n    if is_future(x):\n        return x\n    else:\n        fut = _create_future()\n        fut.set_result(x)\n        return fut", "output": "Converts ``x`` into a `.Future`.\n\n    If ``x`` is already a `.Future`, it is simply returned; otherwise\n    it is wrapped in a new `.Future`.  This is suitable for use as\n    ``result = yield gen.maybe_future(f())`` when you don't know whether\n    ``f()`` returns a `.Future` or not.\n\n    .. deprecated:: 4.3\n       This function only handles ``Futures``, not other yieldable objects.\n       Instead of `maybe_future`, check for the non-future result types\n       you expect (often just ``None``), and ``yield`` anything unknown.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forward(self, is_train=False, **kwargs):\n        \"\"\"\n        \"\"\"\n        if len(kwargs) != 0:\n            arg_dict = self.arg_dict\n            for name, array in kwargs.items():\n                if not isinstance(array, (NDArray, np.ndarray)):\n                    raise ValueError('only accept keyword argument of NDArrays and numpy.ndarray')\n                if name not in arg_dict:\n                    raise TypeError('Unknown argument %s' % name)\n                if arg_dict[name].shape != array.shape:\n                    raise ValueError('Shape not match! Argument %s, need: %s, received: %s'\n                                     %(name, str(arg_dict[name].shape), str(array.shape)))\n                arg_dict[name][:] = array\n\n        check_call(_LIB.MXExecutorForward(\n            self.handle,\n            ctypes.c_int(int(is_train))))\n\n        return self.outputs", "output": "Calculate the outputs specified by the bound symbol.\n\n        Parameters\n        ----------\n        is_train: bool, optional\n            Whether this forward is for evaluation purpose. If True,\n            a backward call is expected to follow.\n\n        **kwargs\n            Additional specification of input arguments.\n\n        Examples\n        --------\n        >>> # doing forward by specifying data\n        >>> texec.forward(is_train=True, data=mydata)\n        >>> # doing forward by not specifying things, but copy to the executor before hand\n        >>> mydata.copyto(texec.arg_dict['data'])\n        >>> texec.forward(is_train=True)\n        >>> # doing forward by specifying data and get outputs\n        >>> outputs = texec.forward(is_train=True, data=mydata)\n        >>> print(outputs[0].asnumpy())", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def event(self, event):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        if event.type() == QEvent.KeyPress:\r\n            if (event.key() == Qt.Key_Tab or event.key() == Qt.Key_Space):\r\n                text = self.text()\r\n                cursor = self.cursorPosition()\r\n                # fix to include in \"undo/redo\" history\r\n                if cursor != 0 and text[cursor-1] == ' ':\r\n                    text = text[:cursor-1] + ROW_SEPARATOR + ' ' +\\\r\n                        text[cursor:]\r\n                else:\r\n                    text = text[:cursor] + ' ' + text[cursor:]\r\n                self.setCursorPosition(cursor)\r\n                self.setText(text)\r\n                self.setCursorPosition(cursor + 1)\r\n                return False\r\n        return QWidget.event(self, event)", "output": "Qt override.\r\n\r\n        This is needed to be able to intercept the Tab key press event.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _whctrs(anchor):\n        \"\"\"\n        \n        \"\"\"\n        w = anchor[2] - anchor[0] + 1\n        h = anchor[3] - anchor[1] + 1\n        x_ctr = anchor[0] + 0.5 * (w - 1)\n        y_ctr = anchor[1] + 0.5 * (h - 1)\n        return w, h, x_ctr, y_ctr", "output": "Return width, height, x center, and y center for an anchor (window).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _patch(self, doc, source, patches, setter=None):\n        ''' \n\n        '''\n        old = self._saved_copy()\n\n        for name, patch in patches.items():\n            for ind, value in patch:\n                if isinstance(ind, (int, slice)):\n                    self[name][ind] = value\n                else:\n                    shape = self[name][ind[0]][tuple(ind[1:])].shape\n                    self[name][ind[0]][tuple(ind[1:])] = np.array(value, copy=False).reshape(shape)\n\n        from ...document.events import ColumnsPatchedEvent\n\n        self._notify_owners(old,\n                            hint=ColumnsPatchedEvent(doc, source, patches, setter))", "output": "Internal implementation to handle special-casing patch events\n        on ``ColumnDataSource`` columns.\n\n        Normally any changes to the ``.data`` dict attribute on a\n        ``ColumnDataSource`` triggers a notification, causing all of the data\n        to be synchronized between server and clients.\n\n        The ``.patch`` method on column data sources exists to provide a\n        more efficient way to perform patching (i.e. random access) updates\n        to a data source, without having to perform a full synchronization,\n        which would needlessly re-send all the data.\n\n        To accomplish this, this function bypasses the wrapped methods on\n        ``PropertyValueDict`` and uses the unwrapped versions on the dict\n        superclass directly. It then explicitly makes a notification, adding\n        a special ``ColumnsPatchedEvent`` hint to the message containing\n        only the small patched data that BokehJS needs in order to efficiently\n        synchronize.\n\n        .. warning::\n            This function assumes the integrity of ``patches`` has already\n            been verified.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clean_fsbackend(opts):\n    '''\n    \n    '''\n    # Clear remote fileserver backend caches so they get recreated\n    for backend in ('git', 'hg', 'svn'):\n        if backend in opts['fileserver_backend']:\n            env_cache = os.path.join(\n                opts['cachedir'],\n                '{0}fs'.format(backend),\n                'envs.p'\n            )\n            if os.path.isfile(env_cache):\n                log.debug('Clearing %sfs env cache', backend)\n                try:\n                    os.remove(env_cache)\n                except OSError as exc:\n                    log.critical(\n                        'Unable to clear env cache file %s: %s',\n                        env_cache, exc\n                    )\n\n            file_lists_dir = os.path.join(\n                opts['cachedir'],\n                'file_lists',\n                '{0}fs'.format(backend)\n            )\n            try:\n                file_lists_caches = os.listdir(file_lists_dir)\n            except OSError:\n                continue\n            for file_lists_cache in fnmatch.filter(file_lists_caches, '*.p'):\n                cache_file = os.path.join(file_lists_dir, file_lists_cache)\n                try:\n                    os.remove(cache_file)\n                except OSError as exc:\n                    log.critical(\n                        'Unable to file_lists cache file %s: %s',\n                        cache_file, exc\n                    )", "output": "Clean out the old fileserver backends", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _crawl_attribute(this_data, this_attr):\n    '''\n    \n    '''\n    if isinstance(this_data, list):\n        t_list = []\n        for d in this_data:\n            t_list.append(_crawl_attribute(d, this_attr))\n        return t_list\n    else:\n        if isinstance(this_attr, dict):\n            t_dict = {}\n            for k in this_attr:\n                if hasattr(this_data, k):\n                    t_dict[k] = _crawl_attribute(getattr(this_data, k, None), this_attr[k])\n            return t_dict\n        elif isinstance(this_attr, list):\n            this_dict = {}\n            for l in this_attr:\n                this_dict = dictupdate.update(this_dict, _crawl_attribute(this_data, l))\n            return this_dict\n        else:\n            return {this_attr: _recurse_config_to_dict(getattr(this_data, this_attr, None))}", "output": "helper function to crawl an attribute specified for retrieval", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def downsample_bottleneck(x, output_channels, dim='2d', stride=1, scope='h'):\n  \"\"\"\n  \"\"\"\n  conv = CONFIG[dim]['conv']\n  with tf.variable_scope(scope):\n    x = conv(x, output_channels, 1, strides=stride, padding='SAME',\n             activation=None)\n    return x", "output": "Downsamples 'x' by `stride` using a 1x1 convolution filter.\n\n  Args:\n    x: input tensor of size [N, H, W, C]\n    output_channels: Desired number of output channels.\n    dim: '2d' if 2-dimensional, '3d' if 3-dimensional.\n    stride: What stride to use. Usually 1 or 2.\n    scope: Optional variable scope.\n\n  Returns:\n    A downsampled tensor of size [N, H/2, W/2, output_channels] if stride\n    is 2, else returns a tensor of size [N, H, W, output_channels] if\n    stride is 1.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_label(self):\n        \"\"\"\"\"\"\n        txt = _('Autosave files found. What would you like to do?\\n\\n'\n                'This dialog will be shown again on next startup if any '\n                'autosave files are not restored, moved or deleted.')\n        label = QLabel(txt, self)\n        label.setWordWrap(True)\n        self.layout.addWidget(label)", "output": "Add label with explanation at top of dialog window.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _binary_replace(old, new):\n    '''\n    \n    '''\n    old_isbin = not __utils__['files.is_text'](old)\n    new_isbin = not __utils__['files.is_text'](new)\n    if any((old_isbin, new_isbin)):\n        if all((old_isbin, new_isbin)):\n            return 'Replace binary file'\n        elif old_isbin:\n            return 'Replace binary file with text file'\n        elif new_isbin:\n            return 'Replace text file with binary file'\n    return ''", "output": "This function does NOT do any diffing, it just checks the old and new files\n    to see if either is binary, and provides an appropriate string noting the\n    difference between the two files. If neither file is binary, an empty\n    string is returned.\n\n    This function should only be run AFTER it has been determined that the\n    files differ.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_md5(name, path):\n    '''\n    \n    '''\n    output = run_stdout(name,\n                        'md5sum {0}'.format(pipes.quote(path)),\n                        ignore_retcode=True)\n    try:\n        return output.split()[0]\n    except IndexError:\n        # Destination file does not exist or could not be accessed\n        return None", "output": "Get the MD5 checksum of a file from a container", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def namespace_present(name, **kwargs):\n    '''\n    \n\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': False,\n           'comment': ''}\n\n    namespace = __salt__['kubernetes.show_namespace'](name, **kwargs)\n\n    if namespace is None:\n        if __opts__['test']:\n            ret['result'] = None\n            ret['comment'] = 'The namespace is going to be created'\n            return ret\n\n        res = __salt__['kubernetes.create_namespace'](name, **kwargs)\n        ret['result'] = True\n        ret['changes']['namespace'] = {\n            'old': {},\n            'new': res}\n    else:\n        ret['result'] = True if not __opts__['test'] else None\n        ret['comment'] = 'The namespace already exists'\n\n    return ret", "output": "Ensures that the named namespace is present.\n\n    name\n        The name of the namespace.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_graph(self, run_key, device_name, debug=False):\n    \"\"\"\n    \"\"\"\n    return self.get_graphs(run_key, debug=debug).get(device_name, None)", "output": "Get the runtime GraphDef proto associated with a run key and a device.\n\n    Args:\n      run_key: A Session.run kay.\n      device_name: Name of the device in question.\n      debug: Whether the debugger-decoratedgraph is to be retrieved.\n\n    Returns:\n      A `GraphDef` proto.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _copyValues(self, to, extra=None):\n        \"\"\"\n        \n        \"\"\"\n        paramMap = self._paramMap.copy()\n        if extra is not None:\n            paramMap.update(extra)\n        for param in self.params:\n            # copy default params\n            if param in self._defaultParamMap and to.hasParam(param.name):\n                to._defaultParamMap[to.getParam(param.name)] = self._defaultParamMap[param]\n            # copy explicitly set params\n            if param in paramMap and to.hasParam(param.name):\n                to._set(**{param.name: paramMap[param]})\n        return to", "output": "Copies param values from this instance to another instance for\n        params shared by them.\n\n        :param to: the target instance\n        :param extra: extra params to be copied\n        :return: the target instance with param values copied", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_cursor_position(self, line, index):\n        \"\"\"\"\"\"\n        value = 'Line {}, Col {}'.format(line + 1, index + 1)\n        self.set_value(value)", "output": "Update cursor position.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def subvolume_delete(name=None, names=None, commit=None):\n    '''\n    \n\n    '''\n    if not name and not (names and type(names) is list):\n        raise CommandExecutionError('Provide a value for the name parameter')\n\n    if commit and commit not in ('after', 'each'):\n        raise CommandExecutionError('Value for commit not recognized')\n\n    # Filter the names and take the ones that are still there\n    names = [n for n in itertools.chain([name], names or [])\n             if n and subvolume_exists(n)]\n\n    # If the subvolumes are gone, we are done\n    if not names:\n        return False\n\n    cmd = ['btrfs', 'subvolume', 'delete']\n    if commit == 'after':\n        cmd.append('--commit-after')\n    elif commit == 'each':\n        cmd.append('--commit-each')\n    cmd.extend(names)\n\n    res = __salt__['cmd.run_all'](cmd)\n    salt.utils.fsutils._verify_run(res)\n    return True", "output": "Delete the subvolume(s) from the filesystem\n\n    The user can remove one single subvolume (name) or multiple of\n    then at the same time (names). One of the two parameters needs to\n    specified.\n\n    Please, refer to the documentation to understand the implication\n    on the transactions, and when the subvolume is really deleted.\n\n    Return True if the subvolume is deleted, False is the subvolume\n    was already missing.\n\n    name\n        Name of the subvolume to remove\n\n    names\n        List of names of subvolumes to remove\n\n    commit\n        * 'after': Wait for transaction commit at the end\n        * 'each': Wait for transaction commit after each delete\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' btrfs.subvolume_delete /var/volumes/tmp\n        salt '*' btrfs.subvolume_delete /var/volumes/tmp commit=after", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def saabas(model, data):\n    \"\"\" \n    \"\"\"\n    return lambda X: TreeExplainer(model).shap_values(X, approximate=True)", "output": "Saabas\n    color = red_blue_circle(0)\n    linestyle = dotted", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def most_confused(self, min_val:int=1, slice_size:int=1)->Collection[Tuple[str,str,int]]:\n        \"\"\n        cm = self.confusion_matrix(slice_size=slice_size)\n        np.fill_diagonal(cm, 0)\n        res = [(self.data.classes[i],self.data.classes[j],cm[i,j])\n                for i,j in zip(*np.where(cm>=min_val))]\n        return sorted(res, key=itemgetter(2), reverse=True)", "output": "Sorted descending list of largest non-diagonal entries of confusion matrix, presented as actual, predicted, number of occurrences.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transpose(self, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n\n        # construct the args\n        axes, kwargs = self._construct_axes_from_arguments(args, kwargs,\n                                                           require_all=True)\n        axes_names = tuple(self._get_axis_name(axes[a])\n                           for a in self._AXIS_ORDERS)\n        axes_numbers = tuple(self._get_axis_number(axes[a])\n                             for a in self._AXIS_ORDERS)\n\n        # we must have unique axes\n        if len(axes) != len(set(axes)):\n            raise ValueError('Must specify %s unique axes' % self._AXIS_LEN)\n\n        new_axes = self._construct_axes_dict_from(self, [self._get_axis(x)\n                                                         for x in axes_names])\n        new_values = self.values.transpose(axes_numbers)\n        if kwargs.pop('copy', None) or (len(args) and args[-1]):\n            new_values = new_values.copy()\n\n        nv.validate_transpose_for_generic(self, kwargs)\n        return self._constructor(new_values, **new_axes).__finalize__(self)", "output": "Permute the dimensions of the %(klass)s\n\n        Parameters\n        ----------\n        args : %(args_transpose)s\n        copy : boolean, default False\n            Make a copy of the underlying data. Mixed-dtype data will\n            always result in a copy\n        **kwargs\n            Additional keyword arguments will be passed to the function.\n\n        Returns\n        -------\n        y : same as input\n\n        Examples\n        --------\n        >>> p.transpose(2, 0, 1)\n        >>> p.transpose(2, 0, 1, copy=True)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def postprocess(x, n_bits_x=8):\n  \"\"\"\n  \"\"\"\n  x = tf.where(tf.is_finite(x), x, tf.ones_like(x))\n  x = tf.clip_by_value(x, -0.5, 0.5)\n  x += 0.5\n  x = x * 2**n_bits_x\n  return tf.cast(tf.clip_by_value(x, 0, 255), dtype=tf.uint8)", "output": "Converts x from [-0.5, 0.5], to [0, 255].\n\n  Args:\n    x: 3-D or 4-D Tensor normalized between [-0.5, 0.5]\n    n_bits_x: Number of bits representing each pixel of the output.\n              Defaults to 8, to default to 256 possible values.\n  Returns:\n    x: 3-D or 4-D Tensor representing images or videos.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compress(data, compresslevel=9):\n    '''\n    \n    '''\n    buf = BytesIO()\n    with open_fileobj(buf, 'wb', compresslevel) as ogz:\n        if six.PY3 and not isinstance(data, bytes):\n            data = data.encode(__salt_system_encoding__)\n        ogz.write(data)\n    compressed = buf.getvalue()\n    return compressed", "output": "Returns the data compressed at gzip level compression.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_batch(self, frame):\n        \"\"\"\n        \n        \"\"\"\n        frame_resize = mx.nd.array(cv2.resize(frame, (self.data_shape[0], self.data_shape[1])))\n        #frame_resize = mx.img.imresize(frame, self.data_shape[0], self.data_shape[1], cv2.INTER_LINEAR)\n        # Change dimensions from (w,h,channels) to (channels, w, h)\n        frame_t = mx.nd.transpose(frame_resize, axes=(2,0,1))\n        frame_norm = frame_t - self.mean_pixels_nd\n        # Add dimension for batch, results in (1,channels,w,h)\n        batch_frame = [mx.nd.expand_dims(frame_norm, axis=0)]\n        batch_shape = [DataDesc('data', batch_frame[0].shape)]\n        batch = DataBatch(data=batch_frame, provide_data=batch_shape)\n        return batch", "output": ":param frame: an (w,h,channels) numpy array (image)\n        :return: DataBatch of (1,channels,data_shape,data_shape)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clone(module: torch.nn.Module, num_copies: int) -> torch.nn.ModuleList:\n    \"\"\"\"\"\"\n    return torch.nn.ModuleList([copy.deepcopy(module) for _ in range(num_copies)])", "output": "Produce N identical layers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sem(self, ddof=1):\n        \"\"\"\n        \n        \"\"\"\n\n        return self.std(ddof=ddof) / np.sqrt(self.count())", "output": "Compute standard error of the mean of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex.\n\n        Parameters\n        ----------\n        ddof : integer, default 1\n            degrees of freedom", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image(array, domain=None, width=None, format='png', **kwargs):\n  \"\"\"\n  \"\"\"\n\n  image_data = serialize_array(array, fmt=format, domain=domain)\n  image = IPython.display.Image(data=image_data, format=format, width=width)\n  IPython.display.display(image)", "output": "Display an image.\n\n  Args:\n    array: NumPy array representing the image\n    fmt: Image format e.g. png, jpeg\n    domain: Domain of pixel values, inferred from min & max values if None\n    w: width of output image, scaled using nearest neighbor interpolation.\n      size unchanged if None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _iter_decode_generator(input, decoder):\n    \"\"\"\n\n    \"\"\"\n    decode = decoder.decode\n    input = iter(input)\n    for chunck in input:\n        output = decode(chunck)\n        if output:\n            assert decoder.encoding is not None\n            yield decoder.encoding\n            yield output\n            break\n    else:\n        # Input exhausted without determining the encoding\n        output = decode(b'', final=True)\n        assert decoder.encoding is not None\n        yield decoder.encoding\n        if output:\n            yield output\n        return\n\n    for chunck in input:\n        output = decode(chunck)\n        if output:\n            yield output\n    output = decode(b'', final=True)\n    if output:\n        yield output", "output": "Return a generator that first yields the :obj:`Encoding`,\n    then yields output chukns as Unicode strings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imagetransformer_sep_channels_8l():\n  \"\"\"\"\"\"\n  hparams = imagetransformer_base()\n  hparams.num_heads = 4\n  hparams.attention_key_channels = hparams.attention_value_channels = 0\n  hparams.hidden_size = 256\n  hparams.filter_size = 256\n  hparams.num_hidden_layers = 8\n  hparams.sampling_method = \"random\"\n  return hparams", "output": "separate rgb embeddings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def threads():\n    '''\n    \n    '''\n\n    # Test data\n    thread_yields = [100, 200, 500, 1000]\n    thread_locks = [2, 4, 8, 16]\n\n    # Initializing the test variables\n    test_command = 'sysbench --num-threads=64 --test=threads '\n    test_command += '--thread-yields={0} --thread-locks={1} run '\n    result = None\n    ret_val = {}\n\n    # Test begins!\n    for yields, locks in zip(thread_yields, thread_locks):\n        key = 'Yields: {0} Locks: {1}'.format(yields, locks)\n        run_command = test_command.format(yields, locks)\n        result = __salt__['cmd.run'](run_command)\n        ret_val[key] = _parser(result)\n\n    return ret_val", "output": "This tests the performance of the processor's scheduler\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sysbench.threads", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_enabled(jail=None):\n    '''\n    \n    '''\n    ret = []\n    service = _cmd(jail)\n    prf = _get_jail_path(jail) if jail else ''\n    for svc in __salt__['cmd.run']('{0} -e'.format(service)).splitlines():\n        ret.append(os.path.basename(svc))\n\n    # This is workaround for bin/173454 bug\n    for svc in get_all(jail):\n        if svc in ret:\n            continue\n        if not os.path.exists('{0}/etc/rc.conf.d/{1}'.format(prf, svc)):\n            continue\n        if enabled(svc, jail=jail):\n            ret.append(svc)\n\n    return sorted(ret)", "output": "Return what services are set to run on boot\n\n    .. versionchanged:: 2016.3.4\n\n    Support for jail (representing jid or jail name) keyword argument in kwargs\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.get_enabled", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sink_bigquery(client, to_delete):\n    \"\"\"\"\"\"\n    dataset = _sink_bigquery_setup(client)\n    to_delete.append(dataset)\n    SINK_NAME = \"robots-bigquery-%d\" % (_millis(),)\n    FILTER = \"textPayload:robot\"\n\n    # [START sink_bigquery_create]\n    DESTINATION = \"bigquery.googleapis.com%s\" % (dataset.path,)\n    sink = client.sink(SINK_NAME, filter_=FILTER, destination=DESTINATION)\n    assert not sink.exists()  # API call\n    sink.create()  # API call\n    assert sink.exists()  # API call\n    # [END sink_bigquery_create]\n    to_delete.insert(0, sink)", "output": "Sink log entries to bigquery.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_xml_rpc():\n    '''\n    \n    '''\n    vm_ = get_configured_provider()\n\n    xml_rpc = config.get_cloud_config_value(\n        'xml_rpc', vm_, __opts__, search_global=False\n    )\n\n    user = config.get_cloud_config_value(\n        'user', vm_, __opts__, search_global=False\n    )\n\n    password = config.get_cloud_config_value(\n        'password', vm_, __opts__, search_global=False\n    )\n\n    server = salt.ext.six.moves.xmlrpc_client.ServerProxy(xml_rpc)\n\n    return server, user, password", "output": "Uses the OpenNebula cloud provider configurations to connect to the\n    OpenNebula API.\n\n    Returns the server connection created as well as the user and password\n    values from the cloud provider config file used to make the connection.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def verify(zone):\n    '''\n    \n    '''\n    ret = {'status': True}\n\n    ## verify zone\n    res = __salt__['cmd.run_all']('zoneadm -z {zone} verify'.format(\n        zone=zone,\n    ))\n    ret['status'] = res['retcode'] == 0\n    ret['message'] = res['stdout'] if ret['status'] else res['stderr']\n    ret['message'] = ret['message'].replace('zoneadm: ', '')\n    if ret['message'] == '':\n        del ret['message']\n\n    return ret", "output": "Check to make sure the configuration of the specified\n    zone can safely be installed on the machine.\n\n    zone : string\n        name of the zone\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zoneadm.verify dolores", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def infer_batch(self, dataloader):\n        \"\"\"\n        \n        \"\"\"\n        sum_losses = 0\n        len_losses = 0\n        for input_data, input_label in dataloader:\n            data = gluon.utils.split_and_load(input_data, self.ctx, even_split=False)\n            label = gluon.utils.split_and_load(input_label, self.ctx, even_split=False)\n            sum_losses, len_losses = self.infer(data, label)\n            sum_losses += sum_losses\n            len_losses += len_losses\n\n        return sum_losses, len_losses", "output": "Description : inference for LipNet", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_selected(self, linenum):\n        \"\"\"\"\"\"\n        self.parents = _get_parents(self.funcs, linenum)\n        update_selected_cb(self.parents, self.method_cb)\n\n        self.parents = _get_parents(self.classes, linenum)\n        update_selected_cb(self.parents, self.class_cb)", "output": "Updates the dropdowns to reflect the current class and function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wol(mac, bcast='255.255.255.255', destport=9):\n    '''\n    \n    '''\n    dest = salt.utils.network.mac_str_to_bytes(mac)\n    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n    sock.sendto(b'\\xff' * 6 + dest * 16, (bcast, int(destport)))\n    return True", "output": "Send a \"Magic Packet\" to wake up a Minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run network.wol 08-00-27-13-69-77\n        salt-run network.wol 080027136977 255.255.255.255 7\n        salt-run network.wol 08:00:27:13:69:77 255.255.255.255 7", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encipher_vigenere(plaintext, plain_vocab, key):\n  \"\"\"\n  \"\"\"\n  ciphertext = []\n  # generate Vigenere table\n  layers = [\n      ShiftEncryptionLayer(plain_vocab, i) for i in range(len(plain_vocab))\n  ]\n\n  for i, sentence in enumerate(plaintext):\n    cipher_sentence = []\n    for j, character in enumerate(sentence):\n      key_idx = key[j % len(key)]\n      encrypted_char = layers[key_idx].encrypt_character(character)\n      cipher_sentence.append(encrypted_char)\n    ciphertext.append(cipher_sentence)\n\n  return ciphertext", "output": "Encrypt plain text with given key.\n\n  Args:\n    plaintext (list of list of Strings): a list of plain text to encrypt.\n    plain_vocab (list of Integer): unique vocabularies being used.\n    key (list of Integer): key to encrypt cipher using Vigenere table.\n\n  Returns:\n    ciphertext (list of Strings): encrypted plain text.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def export(*pools, **kwargs):\n    '''\n    \n\n    '''\n    ## Configure pool\n    # NOTE: initialize the defaults\n    flags = []\n    targets = []\n\n    # NOTE: set extra config based on kwargs\n    if kwargs.get('force', False):\n        flags.append('-f')\n\n    # NOTE: append the pool name and specifications\n    targets = list(pools)\n\n    ## Export pools\n    res = __salt__['cmd.run_all'](\n        __utils__['zfs.zpool_command'](\n            command='export',\n            flags=flags,\n            target=targets,\n        ),\n        python_shell=False,\n    )\n\n    return __utils__['zfs.parse_command_result'](res, 'exported')", "output": ".. versionadded:: 2015.5.0\n\n    Export storage pools\n\n    pools : string\n        One or more storage pools to export\n\n    force : boolean\n        Force export of storage pools\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zpool.export myzpool ... [force=True|False]\n        salt '*' zpool.export myzpool2 myzpool2 ... [force=True|False]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_session(session_id):\n    '''\n    \n    '''\n    ret = dict()\n    sessions = list_sessions()\n    session = [item for item in sessions if item['session_id'] == session_id]\n\n    if session:\n        ret = session[0]\n\n    if not ret:\n        _LOG.warning('No session found for id: %s', session_id)\n    return ret", "output": "Get information about a session.\n\n    .. versionadded:: 2016.11.0\n\n    :param session_id: The numeric Id of the session.\n    :return: A dictionary of session information.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rdp.get_session session_id\n\n        salt '*' rdp.get_session 99", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_masquerade(zone=None, permanent=True):\n    '''\n    \n    '''\n    if zone:\n        cmd = '--zone={0} --add-masquerade'.format(zone)\n    else:\n        cmd = '--add-masquerade'\n\n    if permanent:\n        cmd += ' --permanent'\n\n    return __firewall_cmd(cmd)", "output": "Enable masquerade on a zone.\n    If zone is omitted, default zone will be used.\n\n    .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' firewalld.add_masquerade\n\n    To enable masquerade on a specific zone\n\n    .. code-block:: bash\n\n        salt '*' firewalld.add_masquerade dmz", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_deployments(jboss_config):\n    '''\n    \n\n       '''\n    log.debug(\"======================== MODULE FUNCTION: jboss7.list_deployments\")\n    command_result = __salt__['jboss7_cli.run_command'](jboss_config, 'deploy')\n    deployments = []\n    if command_result['stdout']:\n        deployments = re.split('\\\\s*', command_result['stdout'])\n    log.debug('deployments=%s', deployments)\n    return deployments", "output": "List all deployments on the jboss instance\n\n    jboss_config\n        Configuration dictionary with properties specified above.\n\n     CLI Example:\n\n     .. code-block:: bash\n\n         salt '*' jboss7.list_deployments '{\"cli_path\": \"integration.modules.sysmod.SysModuleTest.test_valid_docs\", \"controller\": \"10.11.12.13:9999\", \"cli_user\": \"jbossadm\", \"cli_password\": \"jbossadm\"}'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _paths(name=None):\n    '''\n    \n\n    '''\n    name = 'freezer' if not name else name\n    states_path = _states_path()\n    return (\n        os.path.join(states_path, '{}-pkgs.yml'.format(name)),\n        os.path.join(states_path, '{}-reps.yml'.format(name)),\n    )", "output": "Return the full path for the packages and repository freezer\n    files.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_view(self):\r\n        \"\"\"\"\"\"\r\n        self.install_model()\r\n        self.fsmodel.directoryLoaded.connect(\r\n            lambda: self.resizeColumnToContents(0))\r\n        self.setAnimated(False)\r\n        self.setSortingEnabled(True)\r\n        self.sortByColumn(0, Qt.AscendingOrder)\r\n        self.fsmodel.modelReset.connect(self.reset_icon_provider)\r\n        self.reset_icon_provider()\r\n        # Disable the view of .spyproject. \r\n        self.filter_directories()", "output": "Setup view", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def caa_rec(rdatas):\n    '''\n    \n    '''\n    rschema = OrderedDict((\n        ('flags', lambda flag: ['critical'] if int(flag) > 0 else []),\n        ('tag', RFC.CAA_TAGS),\n        ('value', lambda val: val.strip('\\',\"'))\n    ))\n\n    res = _data2rec_group(rschema, rdatas, 'tag')\n\n    for tag in ('issue', 'issuewild'):\n        tag_res = res.get(tag, False)\n        if not tag_res:\n            continue\n        for idx, val in enumerate(tag_res):\n            if ';' not in val:\n                continue\n            val, params = val.split(';', 1)\n            params = dict(param.split('=') for param in shlex.split(params))\n            tag_res[idx] = {val: params}\n\n    return res", "output": "Validate and parse DNS record data for a CAA record\n    :param rdata: DNS record data\n    :return: dict w/fields", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolveSystem(self, sysID):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlACatalogResolveSystem(self._o, sysID)\n        return ret", "output": "Try to lookup the catalog resource for a system ID", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def quit(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            self.execute(Command.QUIT)\n        finally:\n            self.stop_client()\n            self.command_executor.close()", "output": "Quits the driver and closes every associated window.\n\n        :Usage:\n            ::\n\n                driver.quit()", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def translate(srcCol, matching, replace):\n    \"\"\"\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.translate(_to_java_column(srcCol), matching, replace))", "output": "A function translate any character in the `srcCol` by a character in `matching`.\n    The characters in `replace` is corresponding to the characters in `matching`.\n    The translate will happen when any character in the string matching with the character\n    in the `matching`.\n\n    >>> spark.createDataFrame([('translate',)], ['a']).select(translate('a', \"rnlt\", \"123\") \\\\\n    ...     .alias('r')).collect()\n    [Row(r=u'1a2s3ae')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode_transformer(encoder_output, encoder_decoder_attention_bias, targets,\n                       hparams, name):\n  \"\"\"\"\"\"\n  with tf.variable_scope(name):\n    targets = common_layers.flatten4d3d(targets)\n\n    decoder_input, decoder_self_bias = (\n        transformer.transformer_prepare_decoder(targets, hparams))\n\n    decoder_input = tf.nn.dropout(decoder_input,\n                                  1.0 - hparams.layer_prepostprocess_dropout)\n\n    decoder_output = transformer.transformer_decoder(\n        decoder_input, encoder_output, decoder_self_bias,\n        encoder_decoder_attention_bias, hparams)\n    decoder_output = tf.expand_dims(decoder_output, axis=2)\n    decoder_output_shape = common_layers.shape_list(decoder_output)\n    decoder_output = tf.reshape(\n        decoder_output, [decoder_output_shape[0], -1, 1, hparams.hidden_size])\n    # Expand since t2t expects 4d tensors.\n    return decoder_output", "output": "Original Transformer decoder.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def days_and_sids_for_frames(frames):\n    \"\"\"\n    \n    \"\"\"\n    if not frames:\n        days = np.array([], dtype='datetime64[ns]')\n        sids = np.array([], dtype='int64')\n        return days, sids\n\n    # Ensure the indices and columns all match.\n    check_indexes_all_same(\n        [frame.index for frame in frames],\n        message='Frames have mistmatched days.',\n    )\n    check_indexes_all_same(\n        [frame.columns for frame in frames],\n        message='Frames have mismatched sids.',\n    )\n\n    return frames[0].index.values, frames[0].columns.values", "output": "Returns the date index and sid columns shared by a list of dataframes,\n    ensuring they all match.\n\n    Parameters\n    ----------\n    frames : list[pd.DataFrame]\n        A list of dataframes indexed by day, with a column per sid.\n\n    Returns\n    -------\n    days : np.array[datetime64[ns]]\n        The days in these dataframes.\n    sids : np.array[int64]\n        The sids in these dataframes.\n\n    Raises\n    ------\n    ValueError\n        If the dataframes passed are not all indexed by the same days\n        and sids.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dtdElementDesc(self, name):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlGetDtdElementDesc(self._o, name)\n        if ret is None:raise treeError('xmlGetDtdElementDesc() failed')\n        __tmp = xmlElement(_obj=ret)\n        return __tmp", "output": "Search the DTD for the description of this element", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def current_window_handle(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.w3c:\n            return self.execute(Command.W3C_GET_CURRENT_WINDOW_HANDLE)['value']\n        else:\n            return self.execute(Command.GET_CURRENT_WINDOW_HANDLE)['value']", "output": "Returns the handle of the current window.\n\n        :Usage:\n            ::\n\n                driver.current_window_handle", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exponentialRDD(sc, mean, size, numPartitions=None, seed=None):\n        \"\"\"\n        \n        \"\"\"\n        return callMLlibFunc(\"exponentialRDD\", sc._jsc, float(mean), size, numPartitions, seed)", "output": "Generates an RDD comprised of i.i.d. samples from the Exponential\n        distribution with the input mean.\n\n        :param sc: SparkContext used to create the RDD.\n        :param mean: Mean, or 1 / lambda, for the Exponential distribution.\n        :param size: Size of the RDD.\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`).\n        :param seed: Random seed (default: a random long integer).\n        :return: RDD of float comprised of i.i.d. samples ~ Exp(mean).\n\n        >>> mean = 2.0\n        >>> x = RandomRDDs.exponentialRDD(sc, mean, 1000, seed=2)\n        >>> stats = x.stats()\n        >>> stats.count()\n        1000\n        >>> abs(stats.mean() - mean) < 0.5\n        True\n        >>> from math import sqrt\n        >>> abs(stats.stdev() - sqrt(mean)) < 0.5\n        True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_video(self, path):\n        \"\"\"\n        \n        \"\"\"\n        frames = self.get_video_frames(path)\n        self.handle_type(frames)\n        return self", "output": "Read from videos", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sudo_remove_dirtree(dir_name):\n  \"\"\"\n  \"\"\"\n  try:\n    subprocess.check_output(['sudo', 'rm', '-rf', dir_name])\n  except subprocess.CalledProcessError as e:\n    raise WorkerError('Can''t remove directory {0}'.format(dir_name), e)", "output": "Removes directory tree as a superuser.\n\n  Args:\n    dir_name: name of the directory to remove.\n\n  This function is necessary to cleanup directories created from inside a\n  Docker, since they usually written as a root, thus have to be removed as a\n  root.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xception_internal(inputs, hparams):\n  \"\"\"\"\"\"\n  with tf.variable_scope(\"xception\"):\n    cur = inputs\n\n    if cur.get_shape().as_list()[1] > 200:\n      # Large image, Xception entry flow\n      cur = xception_entry(cur, hparams.hidden_size)\n    else:\n      # Small image, conv\n      cur = common_layers.conv_block(\n          cur,\n          hparams.hidden_size, [((1, 1), (3, 3))],\n          first_relu=False,\n          padding=\"SAME\",\n          force2d=True,\n          name=\"small_image_conv\")\n\n    for i in range(hparams.num_hidden_layers):\n      with tf.variable_scope(\"layer_%d\" % i):\n        cur = residual_block(cur, hparams)\n\n    return xception_exit(cur)", "output": "Xception body.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _post_data(options=None, xml=None):\n    '''\n    \n    '''\n    params = {'token': options['token'].strip(), 'cmd': 'submitcheck', 'XMLDATA': xml}\n\n    res = salt.utils.http.query(\n        url=options['url'],\n        method='POST',\n        params=params,\n        data='',\n        decode=True,\n        status=True,\n        header_dict={},\n        opts=__opts__,\n    )\n\n    if res.get('status', None) == salt.ext.six.moves.http_client.OK:\n        if res.get('dict', None) and isinstance(res['dict'], list):\n            _content = res['dict'][0]\n            if _content.get('status', None):\n                return True\n            else:\n                return False\n        else:\n            log.error('No content returned from Nagios NRDP.')\n            return False\n    else:\n        log.error(\n            'Error returned from Nagios NRDP. Status code: %s.',\n            res.status_code\n        )\n        return False", "output": "Post data to Nagios NRDP", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_fun(fun):\n    '''\n    \n    '''\n    query = '''SELECT minion_id, last_fun FROM {keyspace}.minions\n               WHERE last_fun = ?;'''.format(keyspace=_get_keyspace())\n\n    ret = {}\n\n    # cassandra_cql.cql_query may raise a CommandExecutionError\n    try:\n        data = __salt__['cassandra_cql.cql_query'](query, 'get_fun', [fun])\n        if data:\n            for row in data:\n                minion = row.get('minion_id')\n                last_fun = row.get('last_fun')\n                if minion and last_fun:\n                    ret[minion] = last_fun\n    except CommandExecutionError:\n        log.critical('Could not get the list of minions.')\n        raise\n    except Exception as e:\n        log.critical(\n            'Unexpected error while getting list of minions: %s', e)\n        raise\n\n    return ret", "output": "Return a dict of the last function called for all minions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self):\n        \"\"\"\n        \"\"\"\n        if self.ttl[self.idx] <= 0:\n            self.buffers[self.idx] = self.inqueue.get(timeout=300.0)\n            self.ttl[self.idx] = self.cur_max_ttl\n            if self.cur_max_ttl < self.max_ttl:\n                self.cur_max_ttl += 1\n        buf = self.buffers[self.idx]\n        self.ttl[self.idx] -= 1\n        released = self.ttl[self.idx] <= 0\n        if released:\n            self.buffers[self.idx] = None\n        self.idx = (self.idx + 1) % len(self.buffers)\n        return buf, released", "output": "Get a new batch from the internal ring buffer.\n\n        Returns:\n           buf: Data item saved from inqueue.\n           released: True if the item is now removed from the ring buffer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_last_traded_dt(self, asset, dt):\n        \"\"\"\n        \n        \"\"\"\n        country_code = self._country_code_for_assets([asset.sid])\n        return self._readers[country_code].get_last_traded_dt(asset, dt)", "output": "Get the latest day on or before ``dt`` in which ``asset`` traded.\n\n        If there are no trades on or before ``dt``, returns ``pd.NaT``.\n\n        Parameters\n        ----------\n        asset : zipline.asset.Asset\n            The asset for which to get the last traded day.\n        dt : pd.Timestamp\n            The dt at which to start searching for the last traded day.\n\n        Returns\n        -------\n        last_traded : pd.Timestamp\n            The day of the last trade for the given asset, using the\n            input dt as a vantage point.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_field_mapping(self, using=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        return self._get_connection(using).indices.get_field_mapping(index=self._name, **kwargs)", "output": "Retrieve mapping definition of a specific field.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.get_field_mapping`` unchanged.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def project_hidden(x, projection_tensors, hidden_size, num_blocks):\n  \"\"\"\n  \"\"\"\n  batch_size, latent_dim, _ = common_layers.shape_list(x)\n  x = tf.reshape(x, shape=[1, -1, hidden_size])\n  x_tiled = tf.reshape(\n      tf.tile(x, multiples=[num_blocks, 1, 1]),\n      shape=[num_blocks, -1, hidden_size])\n  x_projected = tf.matmul(x_tiled, projection_tensors)\n  x_projected = tf.transpose(x_projected, perm=[1, 0, 2])\n  x_4d = tf.reshape(x_projected, [batch_size, latent_dim, num_blocks, -1])\n  return x_4d", "output": "Project encoder hidden state under num_blocks using projection tensors.\n\n  Args:\n    x: Encoder hidden state of shape [batch_size, latent_dim,  hidden_size].\n    projection_tensors: Projection tensors used to project the hidden state.\n    hidden_size: Dimension of the latent space.\n    num_blocks: Number of blocks in DVQ.\n\n  Returns:\n    x_projected: Projected states of shape [batch_size, latent_dim, num_blocks,\n      hidden_size / num_blocks].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_body_arguments(self, name: str, strip: bool = True) -> List[str]:\n        \"\"\"\n        \"\"\"\n        return self._get_arguments(name, self.request.body_arguments, strip)", "output": "Returns a list of the body arguments with the given name.\n\n        If the argument is not present, returns an empty list.\n\n        .. versionadded:: 3.2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def change_mpl_backend(self, command):\n        \"\"\"\n        \n        \"\"\"\n        if command.startswith('%matplotlib') and \\\n          len(command.splitlines()) == 1:\n            if not 'inline' in command:\n                self.silent_execute(command)", "output": "If the user is trying to change Matplotlib backends with\n        %matplotlib, send the same command again to the kernel to\n        correctly change it.\n\n        Fixes issue 4002", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mask(name, runtime=False, root=None):\n    '''\n    \n    '''\n    _check_for_unit_changes(name)\n\n    cmd = 'mask --runtime' if runtime else 'mask'\n    out = __salt__['cmd.run_all'](\n        _systemctl_cmd(cmd, name, systemd_scope=True, root=root),\n        python_shell=False,\n        redirect_stderr=True)\n\n    if out['retcode'] != 0:\n        raise CommandExecutionError(\n            'Failed to mask service \\'%s\\'' % name,\n            info=out['stdout']\n        )\n\n    return True", "output": ".. versionadded:: 2015.5.0\n    .. versionchanged:: 2015.8.12,2016.3.3,2016.11.0\n        On minions running systemd>=205, `systemd-run(1)`_ is now used to\n        isolate commands run by this function from the ``salt-minion`` daemon's\n        control group. This is done to avoid a race condition in cases where\n        the ``salt-minion`` service is restarted while a service is being\n        modified. If desired, usage of `systemd-run(1)`_ can be suppressed by\n        setting a :mod:`config option <salt.modules.config.get>` called\n        ``systemd.scope``, with a value of ``False`` (no quotes).\n\n    .. _`systemd-run(1)`: https://www.freedesktop.org/software/systemd/man/systemd-run.html\n\n    Mask the specified service with systemd\n\n    runtime : False\n        Set to ``True`` to mask this service only until the next reboot\n\n        .. versionadded:: 2015.8.5\n\n    root\n        Enable/disable/mask unit files in the specified root directory\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.mask foo\n        salt '*' service.mask foo runtime=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_date():\n    '''\n    \n    '''\n    ret = salt.utils.mac_utils.execute_return_result('systemsetup -getdate')\n    return salt.utils.mac_utils.parse_return(ret)", "output": "Displays the current date\n\n    :return: the system date\n    :rtype: str\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' timezone.get_date", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _serve_audio_metadata(self, request):\n    \"\"\"\n    \"\"\"\n    tag = request.args.get('tag')\n    run = request.args.get('run')\n    sample = int(request.args.get('sample', 0))\n\n    events = self._multiplexer.Tensors(run, tag)\n    response = self._audio_response_for_run(events, run, tag, sample)\n    return http_util.Respond(request, response, 'application/json')", "output": "Given a tag and list of runs, serve a list of metadata for audio.\n\n    Note that the actual audio data are not sent; instead, we respond\n    with URLs to the audio. The frontend should treat these URLs as\n    opaque and should not try to parse information about them or\n    generate them itself, as the format may change.\n\n    Args:\n      request: A werkzeug.wrappers.Request object.\n\n    Returns:\n      A werkzeug.Response application.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_column_specs(events, next_value_columns, previous_value_columns):\n    \"\"\"\n    \n    \"\"\"\n    required = required_event_fields(next_value_columns,\n                                     previous_value_columns)\n    received = set(events.columns)\n    missing = required - received\n    if missing:\n        raise ValueError(\n            \"EventsLoader missing required columns {missing}.\\n\"\n            \"Got Columns: {received}\\n\"\n            \"Expected Columns: {required}\".format(\n                missing=sorted(missing),\n                received=sorted(received),\n                required=sorted(required),\n            )\n        )", "output": "Verify that the columns of ``events`` can be used by an EventsLoader to\n    serve the BoundColumns described by ``next_value_columns`` and\n    ``previous_value_columns``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def visit_Assign(self, node, **kwargs):\n        \"\"\"\"\"\"\n        self.visit(node.node, **kwargs)\n        self.visit(node.target, **kwargs)", "output": "Visit assignments in the correct order.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_task_id(source):\n    \"\"\"\n    \"\"\"\n    if type(source) is ray.actor.ActorHandle:\n        return source._ray_actor_id\n    else:\n        if type(source) is ray.TaskID:\n            return source\n        else:\n            return ray._raylet.compute_task_id(source)", "output": "Return the task id associated to the generic source of the signal.\n\n    Args:\n        source: source of the signal, it can be either an object id returned\n            by a task, a task id, or an actor handle.\n\n    Returns:\n        - If source is an object id, return id of task which creted object.\n        - If source is an actor handle, return id of actor's task creator.\n        - If source is a task id, return same task id.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dir_list(self, tgt_env):\n        '''\n        \n        '''\n        ret = set()\n        tree = self.get_tree(tgt_env)\n        if not tree:\n            return ret\n        if self.root(tgt_env):\n            try:\n                tree = tree / self.root(tgt_env)\n            except KeyError:\n                return ret\n            relpath = lambda path: os.path.relpath(path, self.root(tgt_env))\n        else:\n            relpath = lambda path: path\n        add_mountpoint = lambda path: salt.utils.path.join(\n            self.mountpoint(tgt_env), path, use_posixpath=True)\n        for blob in tree.traverse():\n            if isinstance(blob, git.Tree):\n                ret.add(add_mountpoint(relpath(blob.path)))\n        if self.mountpoint(tgt_env):\n            ret.add(self.mountpoint(tgt_env))\n        return ret", "output": "Get list of directories for the target environment using GitPython", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def df_metrics_to_num(self, df, query_object):\n        \"\"\"\"\"\"\n        metrics = [metric for metric in query_object.metrics]\n        for col, dtype in df.dtypes.items():\n            if dtype.type == np.object_ and col in metrics:\n                df[col] = pd.to_numeric(df[col], errors='coerce')", "output": "Converting metrics to numeric when pandas.read_sql cannot", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_breakpoint(self, filename, lineno):\r\n        \"\"\"\"\"\"\r\n        clear_breakpoint(filename, lineno)\r\n        self.breakpoints_saved.emit()\r\n        editorstack = self.get_current_editorstack()\r\n        if editorstack is not None:\r\n            index = self.is_file_opened(filename)\r\n            if index is not None:\r\n                editorstack.data[index].editor.debugger.toogle_breakpoint(\r\n                        lineno)", "output": "Remove a single breakpoint", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tgt_vocab(self):\n        \"\"\"\n        \"\"\"\n        if self._tgt_vocab is None:\n            tgt_vocab_file_name, tgt_vocab_hash = \\\n                self._data_file[self._pair_key]['vocab' + '_' + self._tgt_lang]\n            [tgt_vocab_path] = self._fetch_data_path([(tgt_vocab_file_name, tgt_vocab_hash)])\n            with io.open(tgt_vocab_path, 'r', encoding='utf-8') as in_file:\n                self._tgt_vocab = Vocab.from_json(in_file.read())\n        return self._tgt_vocab", "output": "Target Vocabulary of the Dataset.\n\n        Returns\n        -------\n        tgt_vocab : Vocab\n            Target vocabulary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_default(self, section, option, default_value):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        section = self._check_section_option(section, option)\r\n        for sec, options in self.defaults:\r\n            if sec == section:\r\n                options[ option ] = default_value", "output": "Set Default value for a given (section, option)\r\n        -> called when a new (section, option) is set and no default exists", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_svrg_gradients(self):\n        \"\"\"\n        \"\"\"\n        param_names = self._exec_group.param_names\n        for ctx in range(self._ctx_len):\n            for index, name in enumerate(param_names):\n                g_curr_batch_reg = self._exec_group.grad_arrays[index][ctx]\n                g_curr_batch_special = self._mod_aux._exec_group.grad_arrays[index][ctx]\n                g_special_weight_all_batch = self._param_dict[ctx][name]\n                g_svrg = self._svrg_grads_update_rule(g_curr_batch_reg, g_curr_batch_special,\n                                                      g_special_weight_all_batch)\n                self._exec_group.grad_arrays[index][ctx] = g_svrg", "output": "Calculates gradients based on the SVRG update rule.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_jsonable_history(self):\n    \"\"\"\n    \"\"\"\n    return {value_category_key: tracker.get_description()\n            for (value_category_key, tracker) in self._trackers.items()}", "output": "Creates a JSON-able representation of this object.\n\n    Returns:\n      A dictionary mapping key to EventTrackerDescription (which can be used to\n      create event trackers).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def communicate(sock, command, settings=[]):\r\n    \"\"\"\"\"\"\r\n    try:\r\n        COMMUNICATE_LOCK.acquire()\r\n        write_packet(sock, command)\r\n        for option in settings:\r\n            write_packet(sock, option)\r\n        return read_packet(sock)\r\n    finally:\r\n        COMMUNICATE_LOCK.release()", "output": "Communicate with monitor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def daemon(args):\n    \"\"\"\n    \"\"\"\n    if os.environ.get(DVC_DAEMON):\n        logger.debug(\"skipping launching a new daemon.\")\n        return\n\n    cmd = [sys.executable]\n    if not is_binary():\n        cmd += [\"-m\", \"dvc\"]\n    cmd += [\"daemon\", \"-q\"] + args\n\n    env = fix_env()\n    file_path = os.path.abspath(inspect.stack()[0][1])\n    env[cast_bytes_py2(\"PYTHONPATH\")] = cast_bytes_py2(\n        os.path.dirname(os.path.dirname(file_path))\n    )\n    env[cast_bytes_py2(DVC_DAEMON)] = cast_bytes_py2(\"1\")\n\n    _spawn(cmd, env)", "output": "Launch a `dvc daemon` command in a detached process.\n\n    Args:\n        args (list): list of arguments to append to `dvc daemon` command.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mnist_generator(tmp_dir, training, how_many, start_from=0):\n  \"\"\"\n  \"\"\"\n  _get_mnist(tmp_dir)\n  d = _MNIST_TRAIN_DATA_FILENAME if training else _MNIST_TEST_DATA_FILENAME\n  l = _MNIST_TRAIN_LABELS_FILENAME if training else _MNIST_TEST_LABELS_FILENAME\n  return mnist_common_generator(tmp_dir, training, how_many, d, l, start_from)", "output": "Image generator for MNIST.\n\n  Args:\n    tmp_dir: path to temporary storage directory.\n    training: a Boolean; if true, we use the train set, otherwise the test set.\n    how_many: how many images and labels to generate.\n    start_from: from which image to start.\n\n  Returns:\n    An instance of image_generator that produces MNIST images.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merged_with(self, provider, requirement, parent):\n        \"\"\"\n        \"\"\"\n        infos = list(self.information)\n        infos.append(RequirementInformation(requirement, parent))\n        candidates = [\n            c for c in self.candidates\n            if provider.is_satisfied_by(requirement, c)\n        ]\n        if not candidates:\n            raise RequirementsConflicted(self)\n        return type(self)(candidates, infos)", "output": "Build a new instance from this and a new requirement.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_by_index(self, index):\n        \"\"\"\n           \"\"\"\n        match = str(index)\n        for opt in self.options:\n            if opt.get_attribute(\"index\") == match:\n                self._setSelected(opt)\n                return\n        raise NoSuchElementException(\"Could not locate element with index %d\" % index)", "output": "Select the option at the given index. This is done by examing the \"index\" attribute of an\n           element, and not merely by counting.\n\n           :Args:\n            - index - The option at this index will be selected\n\n           throws NoSuchElementException If there is no option with specified index in SELECT", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def retrieve_password_from_keyring(credential_id, username):\n    '''\n    \n    '''\n    try:\n        import keyring  # pylint: disable=import-error\n        return keyring.get_password(credential_id, username)\n    except ImportError:\n        log.error('USE_KEYRING configured as a password, but no keyring module is installed')\n        return False", "output": "Retrieve particular user's password for a specified credential set from system keyring.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_ssh_config_file(opts):\n    '''\n    \n    '''\n    ssh_config_file = opts.get('ssh_config_file')\n    if not os.path.isfile(ssh_config_file):\n        raise IOError('Cannot find SSH config file')\n    if not os.access(ssh_config_file, os.R_OK):\n        raise IOError('Cannot access SSH config file: {}'.format(ssh_config_file))\n    return ssh_config_file", "output": ":return: Path to the .ssh/config file - usually <home>/.ssh/config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_module_is_text_embedding(module_spec):\n  \"\"\"\n  \"\"\"\n  issues = []\n\n  # Find issues with signature inputs.\n  input_info_dict = module_spec.get_input_info_dict()\n  if len(input_info_dict) != 1:\n    issues.append(\"Module default signature must require only one input\")\n  else:\n    input_info, = input_info_dict.values()\n    input_shape = input_info.get_shape()\n    if not (input_info.dtype == tf.string and input_shape.ndims == 1 and\n            input_shape.as_list() == [None]):\n      issues.append(\n          \"Module default signature must have only one input \"\n          \"tf.Tensor(shape=(?,), dtype=string)\"\n      )\n\n  # Find issues with signature outputs.\n  output_info_dict = module_spec.get_output_info_dict()\n  if \"default\" not in output_info_dict:\n    issues.append(\"Module default signature must have a 'default' output.\")\n  else:\n    output_info = output_info_dict[\"default\"]\n    output_shape = output_info.get_shape()\n    if not (output_info.dtype == tf.float32 and output_shape.ndims == 2 and\n            not output_shape.as_list()[0] and output_shape.as_list()[1]):\n      issues.append(\n          \"Module default signature must have a 'default' output of \"\n          \"tf.Tensor(shape=(?,K), dtype=float32).\"\n      )\n\n  if issues:\n    raise ValueError(\"Module is not a text-embedding: %r\" % issues)", "output": "Raises ValueError if `module_spec` is not a text-embedding module.\n\n  Args:\n    module_spec: A `ModuleSpec` to test.\n\n  Raises:\n    ValueError: if `module_spec` default signature is not compatible with\n    Tensor(string, shape=(?,)) -> Tensor(float32, shape=(?,K)).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_reconciled_name_object(self, other):\n        \"\"\"\n        \n        \"\"\"\n        name = get_op_result_name(self, other)\n        if self.name != name:\n            return self._shallow_copy(name=name)\n        return self", "output": "If the result of a set operation will be self,\n        return self, unless the name changes, in which\n        case make a shallow copy of self.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_command(corrected_commands):\n    \"\"\"\n\n    \"\"\"\n    try:\n        selector = CommandSelector(corrected_commands)\n    except NoRuleMatched:\n        logs.failed('No fucks given' if get_alias() == 'fuck'\n                    else 'Nothing found')\n        return\n\n    if not settings.require_confirmation:\n        logs.show_corrected_command(selector.value)\n        return selector.value\n\n    logs.confirm_text(selector.value)\n\n    for action in read_actions():\n        if action == const.ACTION_SELECT:\n            sys.stderr.write('\\n')\n            return selector.value\n        elif action == const.ACTION_ABORT:\n            logs.failed('\\nAborted')\n            return\n        elif action == const.ACTION_PREVIOUS:\n            selector.previous()\n            logs.confirm_text(selector.value)\n        elif action == const.ACTION_NEXT:\n            selector.next()\n            logs.confirm_text(selector.value)", "output": "Returns:\n\n     - the first command when confirmation disabled;\n     - None when ctrl+c pressed;\n     - selected command.\n\n    :type corrected_commands: Iterable[thefuck.types.CorrectedCommand]\n    :rtype: thefuck.types.CorrectedCommand | None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_file(self, edit, filters=None):\r\n        \"\"\"\"\"\"\r\n        basedir = osp.dirname(to_text_string(edit.text()))\r\n        if not osp.isdir(basedir):\r\n            basedir = getcwd_or_home()\r\n        if filters is None:\r\n            filters = _(\"All files (*)\")\r\n        title = _(\"Select file\")\r\n        filename, _selfilter = getopenfilename(self, title, basedir, filters)\r\n        if filename:\r\n            edit.setText(filename)", "output": "Select File", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_canvas(self):\n        \"\"\"\"\"\"\n        self.fig = None\n        self.fmt = None\n        self._qpix_buffer = []\n        self.repaint()", "output": "Clear the figure that was painted on the widget.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _expand_node(node):\n    '''\n    \n    '''\n    ret = {}\n    ret.update(node.__dict__)\n    try:\n        del ret['extra']['boot_disk']\n    except Exception:  # pylint: disable=W0703\n        pass\n    zone = ret['extra']['zone']\n    ret['extra']['zone'] = {}\n    ret['extra']['zone'].update(zone.__dict__)\n\n    # Remove unserializable GCENodeDriver objects\n    if 'driver' in ret:\n        del ret['driver']\n    if 'driver' in ret['extra']['zone']:\n        del ret['extra']['zone']['driver']\n\n    return ret", "output": "Convert the libcloud Node object into something more serializable.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def redirect(self, url: str, permanent: bool = False, status: int = None) -> None:\n        \"\"\"\n        \"\"\"\n        if self._headers_written:\n            raise Exception(\"Cannot redirect after headers have been written\")\n        if status is None:\n            status = 301 if permanent else 302\n        else:\n            assert isinstance(status, int) and 300 <= status <= 399\n        self.set_status(status)\n        self.set_header(\"Location\", utf8(url))\n        self.finish()", "output": "Sends a redirect to the given (optionally relative) URL.\n\n        If the ``status`` argument is specified, that value is used as the\n        HTTP status code; otherwise either 301 (permanent) or 302\n        (temporary) is chosen based on the ``permanent`` argument.\n        The default is 302 (temporary).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _token_to_subtoken_ids(self, token):\n    \"\"\"\n    \"\"\"\n    cache_location = hash(token) % self._cache_size\n    cache_key, cache_value = self._cache[cache_location]\n    if cache_key == token:\n      return cache_value\n    ret = self._escaped_token_to_subtoken_ids(\n        _escape_token(token, self._alphabet))\n    self._cache[cache_location] = (token, ret)\n    return ret", "output": "Converts token to a list of subtoken ids.\n\n    Args:\n      token: a string.\n    Returns:\n      a list of integers in the range [0, vocab_size)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def grant_access_to_shared_folders_to(name, users=None):\n    '''\n    \n    '''\n    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''}\n    current_state = __salt__['vbox_guest.list_shared_folders_users']()\n    if users is None:\n        users = [name]\n    if current_state == users:\n        ret['result'] = True\n        ret['comment'] = 'System already in the correct state'\n        return ret\n    if __opts__['test']:\n        ret['comment'] = ('List of users who have access to auto-mounted '\n                          'shared folders will be changed')\n        ret['changes'] = {\n            'old': current_state,\n            'new': users,\n        }\n        ret['result'] = None\n        return ret\n\n    new_state = __salt__['vbox_guest.grant_access_to_shared_folders_to'](\n        name=name, users=users)\n\n    ret['comment'] = ('List of users who have access to auto-mounted shared '\n                      'folders was changed')\n    ret['changes'] = {\n        'old': current_state,\n        'new': new_state,\n    }\n    ret['result'] = True\n    return ret", "output": "Grant access to auto-mounted shared folders to the users.\n\n    User is specified by it's name. To grant access for several users use\n    argument `users`.\n\n    name\n        Name of the user to grant access to auto-mounted shared folders to.\n    users\n        List of names of users to grant access to auto-mounted shared folders to.\n        If specified, `name` will not be taken into account.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_credentials_arn(self):\n        \"\"\"\n        \n\n        \"\"\"\n        role = self.iam.Role(self.role_name)\n        self.credentials_arn = role.arn\n        return role, self.credentials_arn", "output": "Given our role name, get and set the credentials_arn.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getattr(self, obj, attribute):\n        \"\"\"\n        \"\"\"\n        try:\n            return getattr(obj, attribute)\n        except AttributeError:\n            pass\n        try:\n            return obj[attribute]\n        except (TypeError, LookupError, AttributeError):\n            return self.undefined(obj=obj, name=attribute)", "output": "Get an item or attribute of an object but prefer the attribute.\n        Unlike :meth:`getitem` the attribute *must* be a bytestring.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def re_run_last_cell(self):\r\n        \"\"\"\"\"\"\r\n        text, line = (self.get_current_editor()\r\n                      .get_last_cell_as_executable_code())\r\n        self._run_cell_text(text, line)", "output": "Run the previous cell again.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def request_offline_members(self, *guilds):\n        \n        \"\"\"\n        if any(not g.large or g.unavailable for g in guilds):\n            raise InvalidArgument('An unavailable or non-large guild was passed.')\n\n        await self._connection.request_offline_members(guilds)", "output": "r\"\"\"|coro|\n\n        Requests previously offline members from the guild to be filled up\n        into the :attr:`.Guild.members` cache. This function is usually not\n        called. It should only be used if you have the ``fetch_offline_members``\n        parameter set to ``False``.\n\n        When the client logs on and connects to the websocket, Discord does\n        not provide the library with offline members if the number of members\n        in the guild is larger than 250. You can check if a guild is large\n        if :attr:`.Guild.large` is ``True``.\n\n        Parameters\n        -----------\n        \\*guilds: :class:`Guild`\n            An argument list of guilds to request offline members for.\n\n        Raises\n        -------\n        InvalidArgument\n            If any guild is unavailable or not large in the collection.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_to_compatible_tensor(value, target, error_prefix):\n  \"\"\"\n  \"\"\"\n  try:\n    tensor = tf_v1.convert_to_tensor_or_indexed_slices(value, target.dtype)\n  except TypeError as e:\n    raise TypeError(\"%s: %s\" % (error_prefix, e))\n  if _is_sparse(tensor) != _is_sparse(target):\n    if _is_sparse(tensor):\n      raise TypeError(\"%s: Is sparse. Expected dense.\" % error_prefix)\n    else:\n      raise TypeError(\"%s: Is dense. Expected sparse.\" % error_prefix)\n  if not tensor.get_shape().is_compatible_with(target.get_shape()):\n    raise TypeError(\"%s: Shape %r is incompatible with %r\" %\n                    (error_prefix, tensor.get_shape(), target.get_shape()))\n  return tensor", "output": "Converts `value` into a tensor that can be feed into `tensor_info`.\n\n  Args:\n    value: A value to convert into Tensor or SparseTensor.\n    target: An object returned by `parse_tensor_info_map`.\n    error_prefix: A string to prefix on raised TypeErrors.\n\n  Raises:\n    TypeError: If it fails to convert.\n\n  Returns:\n    A Tensor or SparseTensor compatible with tensor_info.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _handle_errors(errors):\n  \"\"\"\"\"\"\n  if not errors:\n    return\n  log_all = True  # pylint: disable=unused-variable\n  err_msg = \"T2T: skipped importing {num_missing} data_generators modules.\"\n  print(err_msg.format(num_missing=len(errors)))\n  for module, err in errors:\n    err_str = str(err)\n    if not _is_import_err_msg(err_str, module):\n      print(\"From module %s\" % module)\n      raise err\n    if log_all:\n      print(\"Did not import module: %s; Cause: %s\" % (module, err_str))", "output": "Log out and possibly reraise errors during import.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fire(self, tag, msg):\n        '''\n        \n        '''\n        if __opts__.get('__role') == 'master':\n            fire_master = salt.utils.event.get_master_event(\n                __opts__,\n                __opts__['sock_dir']).fire_master\n        else:\n            fire_master = None\n\n        if fire_master:\n            fire_master(msg, tag)\n        else:\n            __salt__['event.send'](tag, msg)", "output": "This replaces a function in main called 'fire'\n\n        It fires an event into the salt bus.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_common_actions(self):\r\n        \"\"\"\"\"\"\r\n        # Filters\r\n        filters_action = create_action(self, _(\"Edit filename filters...\"),\r\n                                       None, ima.icon('filter'),\r\n                                       triggered=self.edit_filter)\r\n        # Show all files\r\n        all_action = create_action(self, _(\"Show all files\"),\r\n                                   toggled=self.toggle_all)\r\n        all_action.setChecked(self.show_all)\r\n        self.toggle_all(self.show_all)\r\n\r\n        # Show all files\r\n        single_click_to_open = create_action(\r\n            self,\r\n            _(\"Single click to open\"),\r\n            toggled=self.set_single_click_to_open,\r\n        )\r\n        single_click_to_open.setChecked(self.single_click_to_open)\r\n        return [filters_action, all_action, single_click_to_open]", "output": "Setup context menu common actions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(self, table, keyset):\n        \"\"\"\n        \"\"\"\n        delete = Mutation.Delete(table=table, key_set=keyset._to_pb())\n        self._mutations.append(Mutation(delete=delete))", "output": "Delete one or more table rows.\n\n        :type table: str\n        :param table: Name of the table to be modified.\n\n        :type keyset: :class:`~google.cloud.spanner_v1.keyset.Keyset`\n        :param keyset: Keys/ranges identifying rows to delete.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def del_token(token):\n    '''\n    \n    '''\n    token_path = os.path.join(__opts__['token_dir'], token)\n    if os.path.exists(token_path):\n        return os.remove(token_path) is None\n    return False", "output": "Delete an eauth token by name\n\n    CLI Example:\n\n    .. code-block:: shell\n\n        salt-run auth.del_token 6556760736e4077daa601baec2b67c24", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def SetSchema(self, reader):\n        \"\"\" \"\"\"\n        if reader is None: reader__o = None\n        else: reader__o = reader._o\n        ret = libxml2mod.xmlTextReaderSetSchema(reader__o, self._o)\n        return ret", "output": "Use XSD Schema to validate the document as it is processed.\n          Activation is only possible before the first Read(). if\n          @schema is None, then Schema validation is desactivated. @\n          The @schema should not be freed until the reader is\n           deallocated or its use has been deactivated.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_util_time_stamp(time_):\n    \"\"\"\n    \n    \"\"\"\n    if len(str(time_)) == 10:\n        # yyyy-mm-dd\u683c\u5f0f\n        return time.mktime(time.strptime(time_, '%Y-%m-%d'))\n    elif len(str(time_)) == 16:\n        # yyyy-mm-dd hh:mm\u683c\u5f0f\n        return time.mktime(time.strptime(time_, '%Y-%m-%d %H:%M'))\n    else:\n        timestr = str(time_)[0:19]\n        return time.mktime(time.strptime(timestr, '%Y-%m-%d %H:%M:%S'))", "output": "\u5b57\u7b26\u4e32 '2018-01-01 00:00:00'  \u8f6c\u53d8\u6210 float \u7c7b\u578b\u65f6\u95f4 \u7c7b\u4f3c time.time() \u8fd4\u56de\u7684\u7c7b\u578b\n    :param time_: \u5b57\u7b26\u4e32str -- \u6570\u636e\u683c\u5f0f \u6700\u597d\u662f%Y-%m-%d %H:%M:%S \u4e2d\u95f4\u8981\u6709\u7a7a\u683c\n    :return: \u7c7b\u578bfloat", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(self, filename, set_current=True, add_where='end'):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        filename = osp.abspath(to_text_string(filename))\r\n        self.starting_long_process.emit(_(\"Loading %s...\") % filename)\r\n        text, enc = encoding.read(filename)\r\n        finfo = self.create_new_editor(filename, enc, text, set_current,\r\n                                       add_where=add_where)\r\n        index = self.data.index(finfo)\r\n        self._refresh_outlineexplorer(index, update=True)\r\n        self.ending_long_process.emit(\"\")\r\n        if self.isVisible() and self.checkeolchars_enabled \\\r\n           and sourcecode.has_mixed_eol_chars(text):\r\n            name = osp.basename(filename)\r\n            self.msgbox = QMessageBox(\r\n                    QMessageBox.Warning,\r\n                    self.title,\r\n                    _(\"<b>%s</b> contains mixed end-of-line \"\r\n                      \"characters.<br>Spyder will fix this \"\r\n                      \"automatically.\") % name,\r\n                    QMessageBox.Ok,\r\n                    self)\r\n            self.msgbox.exec_()\r\n            self.set_os_eol_chars(index)\r\n        self.is_analysis_done = False\r\n        return finfo", "output": "Load filename, create an editor instance and return it\r\n        *Warning* This is loading file, creating editor but not executing\r\n        the source code analysis -- the analysis must be done by the editor\r\n        plugin (in case multiple editorstack instances are handled)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dir_list(saltenv='base', backend=None):\n    '''\n    \n    '''\n    fileserver = salt.fileserver.Fileserver(__opts__)\n    load = {'saltenv': saltenv, 'fsbackend': backend}\n    return fileserver.dir_list(load=load)", "output": "Return a list of directories in the given environment\n\n    saltenv : base\n        The salt fileserver environment to be listed\n\n    backend\n        Narrow fileserver backends to a subset of the enabled ones. If all\n        passed backends start with a minus sign (``-``), then these backends\n        will be excluded from the enabled backends. However, if there is a mix\n        of backends with and without a minus sign (ex:\n        ``backend=-roots,git``) then the ones starting with a minus sign will\n        be disregarded.\n\n        .. versionadded:: 2015.5.0\n\n    .. note:\n        Keep in mind that executing this function spawns a new process,\n        separate from the master. This means that if the fileserver\n        configuration has been changed in some way since the master has been\n        restarted (e.g. if :conf_master:`fileserver_backend`,\n        :conf_master:`gitfs_remotes`, :conf_master:`hgfs_remotes`, etc. have\n        been updated), then the results of this runner will not accurately\n        reflect what dirs are available to minions.\n\n        When in doubt, use :py:func:`cp.list_master_dirs\n        <salt.modules.cp.list_master_dirs>` to see what dirs the minion can see,\n        and always remember to restart the salt-master daemon when updating\n        the fileserver configuration.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run fileserver.dir_list\n        salt-run fileserver.dir_list saltenv=prod\n        salt-run fileserver.dir_list saltenv=dev backend=git\n        salt-run fileserver.dir_list base hg,roots\n        salt-run fileserver.dir_list -git", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def salt_token_tool():\n    '''\n    \n    '''\n    x_auth = cherrypy.request.headers.get('X-Auth-Token', None)\n\n    # X-Auth-Token header trumps session cookie\n    if x_auth:\n        cherrypy.request.cookie['session_id'] = x_auth", "output": "If the custom authentication header is supplied, put it in the cookie dict\n    so the rest of the session-based auth works as intended", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set(lamp_id, state, method=\"state\"):\n    '''\n    \n    '''\n    try:\n        res = _query(lamp_id, state, action=method, method='PUT')\n    except Exception as err:\n        raise CommandExecutionError(err)\n\n    res = len(res) > 1 and res[-1] or res[0]\n    if res.get('success'):\n        res = {'result': True}\n    elif res.get('error'):\n        res = {'result': False,\n               'description': res['error']['description'],\n               'type': res['error']['type']}\n\n    return res", "output": "Set state to the device by ID.\n\n    :param lamp_id:\n    :param state:\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def persist(self, storageLevel=StorageLevel.MEMORY_AND_DISK):\n        \"\"\"\n        \"\"\"\n        self.is_cached = True\n        javaStorageLevel = self._sc._getJavaStorageLevel(storageLevel)\n        self._jdf.persist(javaStorageLevel)\n        return self", "output": "Sets the storage level to persist the contents of the :class:`DataFrame` across\n        operations after the first time it is computed. This can only be used to assign\n        a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n        If no storage level is specified defaults to (C{MEMORY_AND_DISK}).\n\n        .. note:: The default storage level has changed to C{MEMORY_AND_DISK} to match Scala in 2.0.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def checks_list():\n    '''\n    \n    '''\n    application_url = _get_application_url()\n    log.debug('[uptime] get checks')\n    jcontent = requests.get('{0}/api/checks'.format(application_url)).json()\n    return [x['url'] for x in jcontent]", "output": "List URL checked by uptime\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' uptime.checks_list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_indexing_dispatch_code(key):\n    \"\"\"\"\"\"\n    if isinstance(key, (NDArray, np.ndarray)):\n        return _NDARRAY_ADVANCED_INDEXING\n    elif isinstance(key, list):\n        # TODO(junwu): Add support for nested lists besides integer list\n        for i in key:\n            if not isinstance(i, integer_types):\n                raise TypeError('Indexing NDArray only supports a list of integers as index'\n                                ' when key is of list type, received element=%s of type=%s'\n                                % (str(i), str(type(i))))\n        return _NDARRAY_ADVANCED_INDEXING\n    elif isinstance(key, (integer_types, py_slice)):\n        return _NDARRAY_BASIC_INDEXING\n    elif isinstance(key, tuple):\n        for idx in key:\n            if isinstance(idx, (NDArray, np.ndarray, list, tuple)):\n                return _NDARRAY_ADVANCED_INDEXING\n            elif not isinstance(idx, (py_slice, integer_types)):\n                raise ValueError(\"NDArray does not support slicing with key %s of type %s.\"\n                                 % (str(idx), str(type(idx))))\n        return _NDARRAY_BASIC_INDEXING\n    else:\n        return _NDARRAY_UNSUPPORTED_INDEXING", "output": "Returns a dispatch code for calling basic or advanced indexing functions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _gen_param_code(name, doc, defaultValueStr):\n    \"\"\"\n    \n    \"\"\"\n    # TODO: How to correctly inherit instance attributes?\n    template = '''\n    def set$Name(self, value):\n        \"\"\"\n        Sets the value of :py:attr:`$name`.\n        \"\"\"\n        return self._set($name=value)\n\n    def get$Name(self):\n        \"\"\"\n        Gets the value of $name or its default value.\n        \"\"\"\n        return self.getOrDefault(self.$name)'''\n\n    Name = name[0].upper() + name[1:]\n    return template \\\n        .replace(\"$name\", name) \\\n        .replace(\"$Name\", Name) \\\n        .replace(\"$doc\", doc) \\\n        .replace(\"$defaultValueStr\", str(defaultValueStr))", "output": "Generates Python code for a shared param class.\n\n    :param name: param name\n    :param doc: param doc\n    :param defaultValueStr: string representation of the default value\n    :return: code string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_summaries(self, tagged_data, experiment_name, run_name):\n    \"\"\"\n    \"\"\"\n    logger.debug('Writing summaries for %s tags', len(tagged_data))\n    # Connection used as context manager for auto commit/rollback on exit.\n    # We still need an explicit BEGIN, because it doesn't do one on enter,\n    # it waits until the first DML command - which is totally broken.\n    # See: https://stackoverflow.com/a/44448465/1179226\n    with self._db:\n      self._db.execute('BEGIN TRANSACTION')\n      run_id = self._maybe_init_run(experiment_name, run_name)\n      tag_to_metadata = {\n          tag: tagdata.metadata for tag, tagdata in six.iteritems(tagged_data)\n      }\n      tag_to_id = self._maybe_init_tags(run_id, tag_to_metadata)\n      tensor_values = []\n      for tag, tagdata in six.iteritems(tagged_data):\n        tag_id = tag_to_id[tag]\n        for step, wall_time, tensor_proto in tagdata.values:\n          dtype = tensor_proto.dtype\n          shape = ','.join(str(d.size) for d in tensor_proto.tensor_shape.dim)\n          # Use tensor_proto.tensor_content if it's set, to skip relatively\n          # expensive extraction into intermediate ndarray.\n          data = self._make_blob(\n              tensor_proto.tensor_content or\n              tensor_util.make_ndarray(tensor_proto).tobytes())\n          tensor_values.append((tag_id, step, wall_time, dtype, shape, data))\n      self._db.executemany(\n          \"\"\"\n          INSERT OR REPLACE INTO Tensors (\n            series, step, computed_time, dtype, shape, data\n          ) VALUES (?, ?, ?, ?, ?, ?)\n          \"\"\",\n          tensor_values)", "output": "Transactionally writes the given tagged summary data to the DB.\n\n    Args:\n      tagged_data: map from tag to TagData instances.\n      experiment_name: name of experiment.\n      run_name: name of run.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stSpectralCentroidAndSpread(X, fs):\n    \"\"\"\"\"\"\n    ind = (numpy.arange(1, len(X) + 1)) * (fs/(2.0 * len(X)))\n\n    Xt = X.copy()\n    Xt = Xt / Xt.max()\n    NUM = numpy.sum(ind * Xt)\n    DEN = numpy.sum(Xt) + eps\n\n    # Centroid:\n    C = (NUM / DEN)\n\n    # Spread:\n    S = numpy.sqrt(numpy.sum(((ind - C) ** 2) * Xt) / DEN)\n\n    # Normalize:\n    C = C / (fs / 2.0)\n    S = S / (fs / 2.0)\n\n    return (C, S)", "output": "Computes spectral centroid of frame (given abs(FFT))", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write(self, buf):\n        \"\"\"\n        \"\"\"\n        assert self.writable\n        self._check_pid(allow_reset=False)\n        check_call(_LIB.MXRecordIOWriterWriteRecord(self.handle,\n                                                    ctypes.c_char_p(buf),\n                                                    ctypes.c_size_t(len(buf))))", "output": "Inserts a string buffer as a record.\n\n        Examples\n        ---------\n        >>> record = mx.recordio.MXRecordIO('tmp.rec', 'w')\n        >>> for i in range(5):\n        ...    record.write('record_%d'%i)\n        >>> record.close()\n\n        Parameters\n        ----------\n        buf : string (python2), bytes (python3)\n            Buffer to write.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ssh_wrapper(opts, functions=None, context=None):\n    '''\n    \n    '''\n    return LazyLoader(\n        _module_dirs(\n            opts,\n            'wrapper',\n            base_path=os.path.join(SALT_BASE_PATH, os.path.join('client', 'ssh')),\n        ),\n        opts,\n        tag='wrapper',\n        pack={\n            '__salt__': functions,\n            '__grains__': opts.get('grains', {}),\n            '__pillar__': opts.get('pillar', {}),\n            '__context__': context,\n            },\n    )", "output": "Returns the custom logging handler modules", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fpn_model(features):\n    \"\"\"\n    \n    \"\"\"\n    assert len(features) == 4, features\n    num_channel = cfg.FPN.NUM_CHANNEL\n\n    use_gn = cfg.FPN.NORM == 'GN'\n\n    def upsample2x(name, x):\n        return FixedUnPooling(\n            name, x, 2, unpool_mat=np.ones((2, 2), dtype='float32'),\n            data_format='channels_first')\n\n        # tf.image.resize is, again, not aligned.\n        # with tf.name_scope(name):\n        #     shape2d = tf.shape(x)[2:]\n        #     x = tf.transpose(x, [0, 2, 3, 1])\n        #     x = tf.image.resize_nearest_neighbor(x, shape2d * 2, align_corners=True)\n        #     x = tf.transpose(x, [0, 3, 1, 2])\n        #     return x\n\n    with argscope(Conv2D, data_format='channels_first',\n                  activation=tf.identity, use_bias=True,\n                  kernel_initializer=tf.variance_scaling_initializer(scale=1.)):\n        lat_2345 = [Conv2D('lateral_1x1_c{}'.format(i + 2), c, num_channel, 1)\n                    for i, c in enumerate(features)]\n        if use_gn:\n            lat_2345 = [GroupNorm('gn_c{}'.format(i + 2), c) for i, c in enumerate(lat_2345)]\n        lat_sum_5432 = []\n        for idx, lat in enumerate(lat_2345[::-1]):\n            if idx == 0:\n                lat_sum_5432.append(lat)\n            else:\n                lat = lat + upsample2x('upsample_lat{}'.format(6 - idx), lat_sum_5432[-1])\n                lat_sum_5432.append(lat)\n        p2345 = [Conv2D('posthoc_3x3_p{}'.format(i + 2), c, num_channel, 3)\n                 for i, c in enumerate(lat_sum_5432[::-1])]\n        if use_gn:\n            p2345 = [GroupNorm('gn_p{}'.format(i + 2), c) for i, c in enumerate(p2345)]\n        p6 = MaxPooling('maxpool_p6', p2345[-1], pool_size=1, strides=2, data_format='channels_first', padding='VALID')\n        return p2345 + [p6]", "output": "Args:\n        features ([tf.Tensor]): ResNet features c2-c5\n\n    Returns:\n        [tf.Tensor]: FPN features p2-p6", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _variable_inputs(self, op):\n        \"\"\" \n        \"\"\"\n        if op.name not in self._vinputs:\n            self._vinputs[op.name] = np.array([t.op in self.between_ops or t in self.model_inputs for t in op.inputs])\n        return self._vinputs[op.name]", "output": "Return which inputs of this operation are variable (i.e. depend on the model inputs).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_nodes_full(conn=None, call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The list_nodes_full function must be called with -f or '\n            '--function.'\n        )\n\n    if not conn:\n        conn = get_conn()   # pylint: disable=E0602\n\n    ret = {}\n    datacenter_id = get_datacenter_id()\n    nodes = conn.list_servers(datacenter_id=datacenter_id, depth=3)\n\n    for item in nodes['items']:\n        node = {'id': item['id']}\n        node.update(item['properties'])\n        node['state'] = node.pop('vmState')\n        node['public_ips'] = []\n        node['private_ips'] = []\n        if item['entities']['nics']['items'] > 0:\n            for nic in item['entities']['nics']['items']:\n                if nic['properties']['ips']:\n                    pass\n                ip_address = nic['properties']['ips'][0]\n                if salt.utils.cloud.is_public_ip(ip_address):\n                    node['public_ips'].append(ip_address)\n                else:\n                    node['private_ips'].append(ip_address)\n\n        ret[node['name']] = node\n\n    __utils__['cloud.cache_node_list'](\n        ret,\n        __active_provider_name__.split(':')[0],\n        __opts__\n    )\n\n    return ret", "output": "Return a list of the VMs that are on the provider, with all fields", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_connection(self):\n        \"\"\"\n        \n        \"\"\"\n        self.resurrect()\n        connections = self.connections[:]\n\n        # no live nodes, resurrect one by force and return it\n        if not connections:\n            return self.resurrect(True)\n\n        # only call selector if we have a selection\n        if len(connections) > 1:\n            return self.selector.select(connections)\n\n        # only one connection, no need for a selector\n        return connections[0]", "output": "Return a connection from the pool using the `ConnectionSelector`\n        instance.\n\n        It tries to resurrect eligible connections, forces a resurrection when\n        no connections are availible and passes the list of live connections to\n        the selector instance to choose from.\n\n        Returns a connection instance and it's current fail count.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_frame_basic_stochastic():\n  \"\"\"\"\"\"\n  hparams = basic_deterministic_params.next_frame_basic_deterministic()\n  hparams.stochastic_model = True\n  hparams.add_hparam(\"latent_channels\", 1)\n  hparams.add_hparam(\"latent_std_min\", -5.0)\n  hparams.add_hparam(\"num_iterations_1st_stage\", 15000)\n  hparams.add_hparam(\"num_iterations_2nd_stage\", 15000)\n  hparams.add_hparam(\"latent_loss_multiplier\", 1e-3)\n  hparams.add_hparam(\"latent_loss_multiplier_dynamic\", False)\n  hparams.add_hparam(\"latent_loss_multiplier_alpha\", 1e-5)\n  hparams.add_hparam(\"latent_loss_multiplier_epsilon\", 1.0)\n  hparams.add_hparam(\"latent_loss_multiplier_schedule\", \"constant\")\n  hparams.add_hparam(\"latent_num_frames\", 0)  # 0 means use all frames.\n  hparams.add_hparam(\"anneal_end\", 50000)\n  hparams.add_hparam(\"information_capacity\", 0.0)\n  return hparams", "output": "Basic 2-frame conv model with stochastic tower.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _proc_gnulong(self, tarfile):\n        \"\"\"\n        \"\"\"\n        buf = tarfile.fileobj.read(self._block(self.size))\n\n        # Fetch the next header and process it.\n        try:\n            next = self.fromtarfile(tarfile)\n        except HeaderError:\n            raise SubsequentHeaderError(\"missing or bad subsequent header\")\n\n        # Patch the TarInfo object from the next header with\n        # the longname information.\n        next.offset = self.offset\n        if self.type == GNUTYPE_LONGNAME:\n            next.name = nts(buf, tarfile.encoding, tarfile.errors)\n        elif self.type == GNUTYPE_LONGLINK:\n            next.linkname = nts(buf, tarfile.encoding, tarfile.errors)\n\n        return next", "output": "Process the blocks that hold a GNU longname\n           or longlink member.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_context_menu(self):\r\n        \"\"\"\"\"\"\r\n        self.menu = QMenu(self)\r\n        self.cut_action = create_action(self, _(\"Cut\"),\r\n                                        shortcut=keybinding('Cut'),\r\n                                        icon=ima.icon('editcut'),\r\n                                        triggered=self.cut)\r\n        self.copy_action = create_action(self, _(\"Copy\"),\r\n                                         shortcut=keybinding('Copy'),\r\n                                         icon=ima.icon('editcopy'),\r\n                                         triggered=self.copy)\r\n        paste_action = create_action(self, _(\"Paste\"),\r\n                                     shortcut=keybinding('Paste'),\r\n                                     icon=ima.icon('editpaste'),\r\n                                     triggered=self.paste)\r\n        save_action = create_action(self, _(\"Save history log...\"),\r\n                                    icon=ima.icon('filesave'),\r\n                                    tip=_(\"Save current history log (i.e. all \"\r\n                                          \"inputs and outputs) in a text file\"),\r\n                                    triggered=self.save_historylog)\r\n        self.delete_action = create_action(self, _(\"Delete\"),\r\n                                    shortcut=keybinding('Delete'),\r\n                                    icon=ima.icon('editdelete'),\r\n                                    triggered=self.delete)\r\n        selectall_action = create_action(self, _(\"Select All\"),\r\n                                    shortcut=keybinding('SelectAll'),\r\n                                    icon=ima.icon('selectall'),\r\n                                    triggered=self.selectAll)\r\n        add_actions(self.menu, (self.cut_action, self.copy_action,\r\n                                paste_action, self.delete_action, None,\r\n                                selectall_action, None, save_action) )", "output": "Setup shell context menu", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def allocate_buffers(self):\n        \"\"\n        if self.ite_len is None: len(self)\n        self.idx   = LanguageModelPreLoader.CircularIndex(len(self.dataset.x.items), not self.backwards)\n        self.batch = np.zeros((self.bs, self.bptt+1), dtype=np.int64)\n        self.batch_x, self.batch_y = self.batch[:,0:self.bptt], self.batch[:,1:self.bptt+1]\n        #ro: index of the text we're at inside our datasets for the various batches\n        self.ro    = np.zeros(self.bs, dtype=np.int64)\n        #ri: index of the token we're at inside our current text for the various batches\n        self.ri    = np.zeros(self.bs, dtype=np.int)", "output": "Create the ragged array that will be filled when we ask for items.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_pricing_adjustments(self, columns, dts, assets):\n        \"\"\"\n        \n        \"\"\"\n        out = [None] * len(columns)\n        for i, column in enumerate(columns):\n            adjs = {}\n            for asset in assets:\n                adjs.update(self._get_adjustments_in_range(\n                    asset, dts, column))\n            out[i] = adjs\n        return out", "output": "Returns\n        -------\n        adjustments : list[dict[int -> Adjustment]]\n            A list, where each element corresponds to the `columns`, of\n            mappings from index to adjustment objects to apply at that index.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_single_handler_applications(paths, argvs=None):\n    ''' \n\n    '''\n    applications = {}\n    argvs = {} or argvs\n\n    for path in paths:\n        application = build_single_handler_application(path, argvs.get(path, []))\n\n        route = application.handlers[0].url_path()\n\n        if not route:\n            if '/' in applications:\n                raise RuntimeError(\"Don't know the URL path to use for %s\" % (path))\n            route = '/'\n        applications[route] = application\n\n    return applications", "output": "Return a dictionary mapping routes to Bokeh applications built using\n    single handlers, for specified files or directories.\n\n    This function iterates over ``paths`` and ``argvs`` and calls\n    :func:`~bokeh.command.util.build_single_handler_application` on each\n    to generate the mapping.\n\n    Args:\n        path (seq[str]) : paths to files or directories for creating Bokeh\n            applications.\n\n        argvs (dict[str, list[str]], optional) : mapping of paths to command\n            line arguments to pass to the handler for each path\n\n    Returns:\n        dict[str, Application]\n\n    Raises:\n        RuntimeError", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_hold_with_account(self):\n        \"\"\"\n        \"\"\"\n\n        return self.init_hold.reset_index().assign(\n            account_cookie=self.account_cookie\n        ).set_index(['code',\n                     'account_cookie'])", "output": "\u5e26account_cookie\u7684\u521d\u59cb\u5316\u6301\u4ed3\n\n        Returns:\n            [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def ack(self):\n        \"\"\"\n        \"\"\"\n\n        state = self._state\n        if state.is_bot:\n            raise ClientException('Must not be a bot account to ack messages.')\n        return await state.http.ack_message(self.channel.id, self.id)", "output": "|coro|\n\n        Marks this message as read.\n\n        The user must not be a bot user.\n\n        Raises\n        -------\n        HTTPException\n            Acking failed.\n        ClientException\n            You must not be a bot user.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def additions_installed(name, reboot=False, upgrade_os=False):\n    '''\n    \n    '''\n    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''}\n    current_state = __salt__['vbox_guest.additions_version']()\n    if current_state:\n        ret['result'] = True\n        ret['comment'] = 'System already in the correct state'\n        return ret\n    if __opts__['test']:\n        ret['comment'] = ('The state of VirtualBox Guest Additions will be '\n                          'changed.')\n        ret['changes'] = {\n            'old': current_state,\n            'new': True,\n        }\n        ret['result'] = None\n        return ret\n\n    new_state = __salt__['vbox_guest.additions_install'](\n        reboot=reboot, upgrade_os=upgrade_os)\n\n    ret['comment'] = 'The state of VirtualBox Guest Additions was changed!'\n    ret['changes'] = {\n        'old': current_state,\n        'new': new_state,\n    }\n    ret['result'] = bool(new_state)\n    return ret", "output": "Ensure that the VirtualBox Guest Additions are installed. Uses the CD,\n    connected by VirtualBox.\n\n    name\n        The name has no functional value and is only used as a tracking\n        reference.\n    reboot : False\n        Restart OS to complete installation.\n    upgrade_os : False\n        Upgrade OS (to ensure the latests version of kernel and developer tools\n        installed).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def display_graph(g, format='svg', include_asset_exists=False):\n    \"\"\"\n    \n    \"\"\"\n    try:\n        import IPython.display as display\n    except ImportError:\n        raise NoIPython(\"IPython is not installed.  Can't display graph.\")\n\n    if format == 'svg':\n        display_cls = display.SVG\n    elif format in (\"jpeg\", \"png\"):\n        display_cls = partial(display.Image, format=format, embed=True)\n\n    out = BytesIO()\n    _render(g, out, format, include_asset_exists=include_asset_exists)\n    return display_cls(data=out.getvalue())", "output": "Display a TermGraph interactively from within IPython.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _residual(self, x, in_filter, out_filter, stride,\n                activate_before_residual=False):\n    \"\"\"\"\"\"\n    if activate_before_residual:\n      with tf.variable_scope('shared_activation'):\n        x = self._layer_norm('init_bn', x)\n        x = self._relu(x, self.hps.relu_leakiness)\n        orig_x = x\n    else:\n      with tf.variable_scope('residual_only_activation'):\n        orig_x = x\n        x = self._layer_norm('init_bn', x)\n        x = self._relu(x, self.hps.relu_leakiness)\n\n    with tf.variable_scope('sub1'):\n      x = self._conv('conv1', x, 3, in_filter, out_filter, stride)\n\n    with tf.variable_scope('sub2'):\n      x = self._layer_norm('bn2', x)\n      x = self._relu(x, self.hps.relu_leakiness)\n      x = self._conv('conv2', x, 3, out_filter, out_filter, [1, 1, 1, 1])\n\n    with tf.variable_scope('sub_add'):\n      if in_filter != out_filter:\n        orig_x = tf.nn.avg_pool(orig_x, stride, stride, 'VALID')\n        orig_x = tf.pad(\n            orig_x, [[0, 0], [0, 0], [0, 0],\n                     [(out_filter - in_filter) // 2,\n                      (out_filter - in_filter) // 2]])\n      x += orig_x\n\n    return x", "output": "Residual unit with 2 sub layers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _print_download_progress_msg(self, msg, flush=False):\n    \"\"\"\n    \"\"\"\n    if self._interactive_mode():\n      # Print progress message to console overwriting previous progress\n      # message.\n      self._max_prog_str = max(self._max_prog_str, len(msg))\n      sys.stdout.write(\"\\r%-{}s\".format(self._max_prog_str) % msg)\n      sys.stdout.flush()\n      if flush:\n        print(\"\\n\")\n    else:\n      # Interactive progress tracking is disabled. Print progress to the\n      # standard TF log.\n      logging.info(msg)", "output": "Prints a message about download progress either to the console or TF log.\n\n    Args:\n      msg: Message to print.\n      flush: Indicates whether to flush the output (only used in interactive\n             mode).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode_multipart_formdata(fields, boundary=None):\n    \"\"\"\n    \n    \"\"\"\n    body = BytesIO()\n    if boundary is None:\n        boundary = choose_boundary()\n\n    for field in iter_field_objects(fields):\n        body.write(b('--%s\\r\\n' % (boundary)))\n\n        writer(body).write(field.render_headers())\n        data = field.data\n\n        if isinstance(data, int):\n            data = str(data)  # Backwards compatibility\n\n        if isinstance(data, six.text_type):\n            writer(body).write(data)\n        else:\n            body.write(data)\n\n        body.write(b'\\r\\n')\n\n    body.write(b('--%s--\\r\\n' % (boundary)))\n\n    content_type = str('multipart/form-data; boundary=%s' % boundary)\n\n    return body.getvalue(), content_type", "output": "Encode a dictionary of ``fields`` using the multipart/form-data MIME format.\n\n    :param fields:\n        Dictionary of fields or list of (key, :class:`~urllib3.fields.RequestField`).\n\n    :param boundary:\n        If not specified, then a random boundary will be generated using\n        :func:`urllib3.filepost.choose_boundary`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def file_detector(self, detector):\n        \"\"\"\n        \n        \"\"\"\n        if detector is None:\n            raise WebDriverException(\"You may not set a file detector that is null\")\n        if not isinstance(detector, FileDetector):\n            raise WebDriverException(\"Detector has to be instance of FileDetector\")\n        self._file_detector = detector", "output": "Set the file detector to be used when sending keyboard input.\n        By default, this is set to a file detector that does nothing.\n\n        see FileDetector\n        see LocalFileDetector\n        see UselessFileDetector\n\n        :Args:\n         - detector: The detector to use. Must not be None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _multi_take_opportunity(self, tup):\n        \"\"\"\n        \n        \"\"\"\n        if not all(is_list_like_indexer(x) for x in tup):\n            return False\n\n        # just too complicated\n        if any(com.is_bool_indexer(x) for x in tup):\n            return False\n\n        return True", "output": "Check whether there is the possibility to use ``_multi_take``.\n        Currently the limit is that all axes being indexed must be indexed with\n        list-likes.\n\n        Parameters\n        ----------\n        tup : tuple\n            Tuple of indexers, one per axis\n\n        Returns\n        -------\n        boolean: Whether the current indexing can be passed through _multi_take", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _tar_and_copy(src_dir, target_dir):\n  \"\"\"\"\"\"\n  src_dir = src_dir.rstrip(\"/\")\n  target_dir = target_dir.rstrip(\"/\")\n  tmp_dir = tempfile.gettempdir().rstrip(\"/\")\n  src_base = os.path.basename(src_dir)\n  shell_run(\n      \"tar --exclude=.git -zcf {tmp_dir}/{src_base}.tar.gz -C {src_dir} .\",\n      src_dir=src_dir,\n      src_base=src_base,\n      tmp_dir=tmp_dir)\n  final_destination = \"%s/%s.tar.gz\" % (target_dir, src_base)\n  shell_run(\n      (\"gsutil cp {tmp_dir}/{src_base}.tar.gz \"\n       \"{final_destination}\"),\n      tmp_dir=tmp_dir,\n      src_base=src_base,\n      final_destination=final_destination)\n  return final_destination", "output": "Tar and gzip src_dir and copy to GCS target_dir.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_alarm(deployment_id, metric_name, data, api_key=None, profile=\"telemetry\"):\n    '''\n    \n\n    '''\n\n    auth = _auth(api_key, profile)\n    request_uri = _get_telemetry_base(profile) + \"/alerts\"\n\n    key = \"telemetry.{0}.alerts\".format(deployment_id)\n\n    # set the notification channels if not already set\n    post_body = {\n        \"deployment\": deployment_id,\n        \"filter\": data.get('filter'),\n        \"notificationChannel\": get_notification_channel_id(data.get('escalate_to')).split(),\n        \"condition\": {\n            \"metric\": metric_name,\n            \"max\": data.get('max'),\n            \"min\": data.get('min')\n        }\n    }\n\n    try:\n        response = requests.post(request_uri, data=salt.utils.json.dumps(post_body), headers=auth)\n    except requests.exceptions.RequestException as e:\n        # TODO: May be we should retry?\n        log.error(six.text_type(e))\n\n    if response.status_code >= 200 and response.status_code < 300:\n        # update cache\n        log.info('Created alarm on metric: %s in deployment: %s', metric_name, deployment_id)\n        log.debug('Updating cache for metric %s in deployment %s: %s',\n                  metric_name, deployment_id, response.json())\n        _update_cache(deployment_id, metric_name, response.json())\n    else:\n        log.error(\n            'Failed to create alarm on metric: %s in '\n            'deployment %s: payload: %s',\n            metric_name, deployment_id, salt.utils.json.dumps(post_body)\n        )\n\n    return response.status_code >= 200 and response.status_code < 300, response.json()", "output": "create an telemetry alarms.\n\n    data is a dict of alert configuration data.\n\n    Returns (bool success, str message) tuple.\n\n    CLI Example:\n\n        salt myminion telemetry.create_alarm rs-ds033197 {} profile=telemetry", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate(self, **kwargs: Any) -> bytes:\n        \"\"\"\"\"\"\n        namespace = {\n            \"escape\": escape.xhtml_escape,\n            \"xhtml_escape\": escape.xhtml_escape,\n            \"url_escape\": escape.url_escape,\n            \"json_encode\": escape.json_encode,\n            \"squeeze\": escape.squeeze,\n            \"linkify\": escape.linkify,\n            \"datetime\": datetime,\n            \"_tt_utf8\": escape.utf8,  # for internal use\n            \"_tt_string_types\": (unicode_type, bytes),\n            # __name__ and __loader__ allow the traceback mechanism to find\n            # the generated source code.\n            \"__name__\": self.name.replace(\".\", \"_\"),\n            \"__loader__\": ObjectDict(get_source=lambda name: self.code),\n        }\n        namespace.update(self.namespace)\n        namespace.update(kwargs)\n        exec_in(self.compiled, namespace)\n        execute = typing.cast(Callable[[], bytes], namespace[\"_tt_execute\"])\n        # Clear the traceback module's cache of source data now that\n        # we've generated a new template (mainly for this module's\n        # unittests, where different tests reuse the same name).\n        linecache.clearcache()\n        return execute()", "output": "Generate this template with the given arguments.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def value_loss(value_net_apply,\n               value_net_params,\n               observations,\n               rewards,\n               reward_mask,\n               gamma=0.99):\n  \"\"\"\n  \"\"\"\n\n  B, T = rewards.shape  # pylint: disable=invalid-name\n  assert (B, T + 1) == observations.shape[:2]\n\n  # NOTE: observations is (B, T+1) + OBS, value_prediction is (B, T+1, 1)\n  value_prediction = value_net_apply(observations, value_net_params)\n  assert (B, T + 1, 1) == value_prediction.shape\n\n  return value_loss_given_predictions(value_prediction, rewards, reward_mask,\n                                      gamma)", "output": "Computes the value loss.\n\n  Args:\n    value_net_apply: value net apply function with signature (params, ndarray of\n      shape (B, T+1) + OBS) -> ndarray(B, T+1, 1)\n    value_net_params: params of value_net_apply.\n    observations: np.ndarray of shape (B, T+1) + OBS\n    rewards: np.ndarray of shape (B, T) of rewards.\n    reward_mask: np.ndarray of shape (B, T), the mask over rewards.\n    gamma: float, discount factor.\n\n  Returns:\n    The average L2 value loss, averaged over instances where reward_mask is 1.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _delete(self, pk):\n        \"\"\"\n            \n        \"\"\"\n        item = self.datamodel.get(pk, self._base_filters)\n        if not item:\n            abort(404)\n        try:\n            self.pre_delete(item)\n        except Exception as e:\n            flash(str(e), 'danger')\n        else:\n            view_menu = security_manager.find_view_menu(item.get_perm())\n            pvs = security_manager.get_session.query(\n                security_manager.permissionview_model).filter_by(\n                view_menu=view_menu).all()\n\n            schema_view_menu = None\n            if hasattr(item, 'schema_perm'):\n                schema_view_menu = security_manager.find_view_menu(item.schema_perm)\n\n                pvs.extend(security_manager.get_session.query(\n                    security_manager.permissionview_model).filter_by(\n                    view_menu=schema_view_menu).all())\n\n            if self.datamodel.delete(item):\n                self.post_delete(item)\n\n                for pv in pvs:\n                    security_manager.get_session.delete(pv)\n\n                if view_menu:\n                    security_manager.get_session.delete(view_menu)\n\n                if schema_view_menu:\n                    security_manager.get_session.delete(schema_view_menu)\n\n                security_manager.get_session.commit()\n\n            flash(*self.datamodel.message)\n            self.update_redirect()", "output": "Delete function logic, override to implement diferent logic\n            deletes the record with primary_key = pk\n\n            :param pk:\n                record primary key to delete", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flat_map(self, flatmap_fn):\n        \"\"\"\n        \"\"\"\n        op = Operator(\n            _generate_uuid(),\n            OpType.FlatMap,\n            \"FlatMap\",\n            flatmap_fn,\n            num_instances=self.env.config.parallelism)\n        return self.__register(op)", "output": "Applies a flatmap operator to the stream.\n\n        Attributes:\n             flatmap_fn (function): The user-defined logic of the flatmap\n             (e.g. split()).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def needs_i8_conversion(arr_or_dtype):\n    \"\"\"\n    \n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    return (is_datetime_or_timedelta_dtype(arr_or_dtype) or\n            is_datetime64tz_dtype(arr_or_dtype) or\n            is_period_dtype(arr_or_dtype))", "output": "Check whether the array or dtype should be converted to int64.\n\n    An array-like or dtype \"needs\" such a conversion if the array-like\n    or dtype is of a datetime-like dtype\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array or dtype should be converted to int64.\n\n    Examples\n    --------\n    >>> needs_i8_conversion(str)\n    False\n    >>> needs_i8_conversion(np.int64)\n    False\n    >>> needs_i8_conversion(np.datetime64)\n    True\n    >>> needs_i8_conversion(np.array(['a', 'b']))\n    False\n    >>> needs_i8_conversion(pd.Series([1, 2]))\n    False\n    >>> needs_i8_conversion(pd.Series([], dtype=\"timedelta64[ns]\"))\n    True\n    >>> needs_i8_conversion(pd.DatetimeIndex([1, 2, 3], tz=\"US/Eastern\"))\n    True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vgg11(num_classes=1000, pretrained='imagenet'):\n    \"\"\"\n    \"\"\"\n    model = models.vgg11(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['vgg11'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_vggs(model)\n    return model", "output": "VGG 11-layer model (configuration \"A\")", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def path(self):\n        \"\"\"\n        \"\"\"\n        if not self.name:\n            raise ValueError(\"Cannot determine path without a blob name.\")\n\n        return self.path_helper(self.bucket.path, self.name)", "output": "Getter property for the URL path to this Blob.\n\n        :rtype: str\n        :returns: The URL path to this Blob.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def datasets_create_new(self, dataset_new_request, **kwargs):  # noqa: E501\n        \"\"\"\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.datasets_create_new_with_http_info(dataset_new_request, **kwargs)  # noqa: E501\n        else:\n            (data) = self.datasets_create_new_with_http_info(dataset_new_request, **kwargs)  # noqa: E501\n            return data", "output": "Create a new dataset  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.datasets_create_new(dataset_new_request, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param DatasetNewRequest dataset_new_request: Information for creating a new dataset (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _replace_coerce(self, to_replace, value, inplace=True, regex=False,\n                        convert=False, mask=None):\n        \"\"\"\n        \n        \"\"\"\n        if mask.any():\n            block = super()._replace_coerce(\n                to_replace=to_replace, value=value, inplace=inplace,\n                regex=regex, convert=convert, mask=mask)\n            if convert:\n                block = [b.convert(by_item=True, numeric=False, copy=True)\n                         for b in block]\n            return block\n        return self", "output": "Replace value corresponding to the given boolean array with another\n        value.\n\n        Parameters\n        ----------\n        to_replace : object or pattern\n            Scalar to replace or regular expression to match.\n        value : object\n            Replacement object.\n        inplace : bool, default False\n            Perform inplace modification.\n        regex : bool, default False\n            If true, perform regular expression substitution.\n        convert : bool, default True\n            If true, try to coerce any object types to better types.\n        mask : array-like of bool, optional\n            True indicate corresponding element is ignored.\n\n        Returns\n        -------\n        A new block if there is anything to replace or the original block.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def residual_conv(x, repeat, k, hparams, name, reuse=None):\n  \"\"\"\"\"\"\n  with tf.variable_scope(name, reuse=reuse):\n    dilations_and_kernels = [((1, 1), k) for _ in range(3)]\n    for i in range(repeat):\n      with tf.variable_scope(\"repeat_%d\" % i):\n        y = common_layers.conv_block(\n            common_layers.layer_norm(x, hparams.hidden_size, name=\"lnorm\"),\n            hparams.hidden_size,\n            dilations_and_kernels,\n            padding=\"SAME\",\n            name=\"residual_conv\")\n        y = tf.nn.dropout(y, 1.0 - hparams.dropout)\n        x += y\n    return x", "output": "A stack of convolution blocks with residual connections.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def analyze(self, filename):\r\n        \"\"\"\"\"\"\r\n        if self.dockwidget and not self.ismaximized:\r\n            self.dockwidget.setVisible(True)\r\n            self.dockwidget.setFocus()\r\n            self.dockwidget.raise_()\r\n        self.pylint.analyze(filename)", "output": "Reimplement analyze method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def patch(self, client=None):\n        \"\"\"\n        \"\"\"\n        # Special case: For buckets, it is possible that labels are being\n        # removed; this requires special handling.\n        if self._label_removals:\n            self._changes.add(\"labels\")\n            self._properties.setdefault(\"labels\", {})\n            for removed_label in self._label_removals:\n                self._properties[\"labels\"][removed_label] = None\n\n        # Call the superclass method.\n        return super(Bucket, self).patch(client=client)", "output": "Sends all changed properties in a PATCH request.\n\n        Updates the ``_properties`` with the response from the backend.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use.  If not passed, falls back to the\n                       ``client`` stored on the current object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def route_filters_list_all(**kwargs):\n    '''\n    \n\n    '''\n    result = {}\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        filters = __utils__['azurearm.paged_object_to_list'](netconn.route_filters.list())\n\n        for route_filter in filters:\n            result[route_filter['name']] = route_filter\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    List all route filters within a subscription.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.route_filters_list_all", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_image(img_path, image_dims=None, mean=None):\n    \"\"\"\n    \n    \"\"\"\n\n    import urllib\n\n    filename = img_path.split(\"/\")[-1]\n    if img_path.startswith('http'):\n        urllib.urlretrieve(img_path, filename)\n        img = cv2.imread(filename)\n    else:\n        img = cv2.imread(img_path)\n\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    if image_dims is not None:\n        img = cv2.resize(img, image_dims)  # resize to image_dims to fit model\n    img = np.rollaxis(img, 2) # change to (c, h, w) order\n    img = img[np.newaxis, :]  # extend to (n, c, h, w)\n    if mean is not None:\n        mean = np.array(mean)\n        if mean.shape == (3,):\n            mean = mean[np.newaxis, :, np.newaxis, np.newaxis]  # extend to (n, c, 1, 1)\n        img = img.astype(np.float32) - mean # subtract mean\n\n    return img", "output": "Reads an image from file path or URL, optionally resizing to given image dimensions and\n    subtracting mean.\n    :param img_path: path to file, or url to download\n    :param image_dims: image dimensions to resize to, or None\n    :param mean: mean file to subtract, or None\n    :return: loaded image, in RGB format", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_images(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_images function must be called with '\n            '-f or --function, or with the --list-images option'\n        )\n\n    ret = {}\n    conn = get_conn()\n    response = conn.getCreateObjectOptions()\n    for image in response['operatingSystems']:\n        ret[image['itemPrice']['item']['description']] = {\n            'name': image['itemPrice']['item']['description'],\n            'template': image['template']['operatingSystemReferenceCode'],\n        }\n    return ret", "output": "Return a dict of all available VM images on the cloud provider.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extract_cookies_to_jar(jar, request, response):\n    \"\"\"\n    \"\"\"\n    if not (hasattr(response, '_original_response') and\n            response._original_response):\n        return\n    # the _original_response field is the wrapped httplib.HTTPResponse object,\n    req = MockRequest(request)\n    # pull out the HTTPMessage with the headers and put it in the mock:\n    res = MockResponse(response._original_response.msg)\n    jar.extract_cookies(res, req)", "output": "Extract the cookies from the response into a CookieJar.\n\n    :param jar: cookielib.CookieJar (not necessarily a RequestsCookieJar)\n    :param request: our own requests.Request object\n    :param response: urllib3.HTTPResponse object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_record_field(table, sys_id, field, value):\n    '''\n    \n    '''\n    client = _get_client()\n    client.table = table\n    response = client.update({field: value}, sys_id)\n    return response", "output": "Update the value of a record's field in a servicenow table\n\n    :param table: The table name, e.g. sys_user\n    :type  table: ``str``\n\n    :param sys_id: The unique ID of the record\n    :type  sys_id: ``str``\n\n    :param field: The new value\n    :type  field: ``str``\n\n    :param value: The new value\n    :type  value: ``str``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion servicenow.update_record_field sys_user 2348234 first_name jimmy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def num_gpus():\n    \"\"\"\n\n    \"\"\"\n    count = ctypes.c_int()\n    check_call(_LIB.MXGetGPUCount(ctypes.byref(count)))\n    return count.value", "output": "Query CUDA for the number of GPUs present.\n\n    Raises\n    ------\n    Will raise an exception on any CUDA error.\n\n    Returns\n    -------\n    count : int\n        The number of GPUs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def write_bytes(self, writer: AbstractStreamWriter,\n                          conn: 'Connection') -> None:\n        \"\"\"\"\"\"\n        # 100 response\n        if self._continue is not None:\n            await writer.drain()\n            await self._continue\n\n        protocol = conn.protocol\n        assert protocol is not None\n        try:\n            if isinstance(self.body, payload.Payload):\n                await self.body.write(writer)\n            else:\n                if isinstance(self.body, (bytes, bytearray)):\n                    self.body = (self.body,)  # type: ignore\n\n                for chunk in self.body:\n                    await writer.write(chunk)  # type: ignore\n\n            await writer.write_eof()\n        except OSError as exc:\n            new_exc = ClientOSError(\n                exc.errno,\n                'Can not write request body for %s' % self.url)\n            new_exc.__context__ = exc\n            new_exc.__cause__ = exc\n            protocol.set_exception(new_exc)\n        except asyncio.CancelledError as exc:\n            if not conn.closed:\n                protocol.set_exception(exc)\n        except Exception as exc:\n            protocol.set_exception(exc)\n        finally:\n            self._writer = None", "output": "Support coroutines that yields bytes objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def guess_filename(obj):\n    \"\"\"\"\"\"\n    name = getattr(obj, 'name', None)\n    if (name and isinstance(name, basestring) and name[0] != '<' and\n            name[-1] != '>'):\n        return os.path.basename(name)", "output": "Tries to guess the filename of the given object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __get_cfg_pkgs_dpkg(self):\n        '''\n        \n        '''\n        # Get list of all available packages\n        data = dict()\n\n        for pkg_name in salt.utils.stringutils.to_str(self._syscall('dpkg-query', None, None,\n                                                        '-Wf', \"${binary:Package}\\\\n\")[0]).split(os.linesep):\n            pkg_name = pkg_name.strip()\n            if not pkg_name:\n                continue\n            data[pkg_name] = list()\n            for pkg_cfg_item in salt.utils.stringutils.to_str(self._syscall('dpkg-query', None, None, '-Wf', \"${Conffiles}\\\\n\",\n                                                                pkg_name)[0]).split(os.linesep):\n                pkg_cfg_item = pkg_cfg_item.strip()\n                if not pkg_cfg_item:\n                    continue\n                pkg_cfg_file, pkg_cfg_sum = pkg_cfg_item.strip().split(\" \", 1)\n                data[pkg_name].append(pkg_cfg_file)\n\n            # Dpkg meta data is unreliable. Check every package\n            # and remove which actually does not have config files.\n            if not data[pkg_name]:\n                data.pop(pkg_name)\n\n        return data", "output": "Get packages with configuration files on Dpkg systems.\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def auto_select_categorical_features(X, threshold=10):\n    \"\"\"\n    \"\"\"\n    feature_mask = []\n\n    for column in range(X.shape[1]):\n        if sparse.issparse(X):\n            indptr_start = X.indptr[column]\n            indptr_end = X.indptr[column + 1]\n            unique = np.unique(X.data[indptr_start:indptr_end])\n        else:\n            unique = np.unique(X[:, column])\n\n        feature_mask.append(len(unique) <= threshold)\n\n    return feature_mask", "output": "Make a feature mask of categorical features in X.\n\n    Features with less than 10 unique values are considered categorical.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape=(n_samples, n_features)\n        Dense array or sparse matrix.\n\n    threshold : int\n        Maximum number of unique values per feature to consider the feature\n        to be categorical.\n\n    Returns\n    -------\n    feature_mask : array of booleans of size {n_features, }", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_(package, question, type, value, *extra):\n    '''\n    \n    '''\n\n    if extra:\n        value = ' '.join((value,) + tuple(extra))\n\n    fd_, fname = salt.utils.files.mkstemp(prefix=\"salt-\", close_fd=False)\n\n    line = \"{0} {1} {2} {3}\".format(package, question, type, value)\n    os.write(fd_, salt.utils.stringutils.to_bytes(line))\n    os.close(fd_)\n\n    _set_file(fname)\n\n    os.unlink(fname)\n\n    return True", "output": "Set answers to debconf questions for a package.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' debconf.set <package> <question> <type> <value> [<value> ...]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bninception(num_classes=1000, pretrained='imagenet'):\n    \n    \"\"\"\n    model = BNInception(num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['bninception'][pretrained]\n        assert num_classes == settings['num_classes'], \\\n            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n        model.load_state_dict(model_zoo.load_url(settings['url']))\n        model.input_space = settings['input_space']\n        model.input_size = settings['input_size']\n        model.input_range = settings['input_range']\n        model.mean = settings['mean']\n        model.std = settings['std']\n    return model", "output": "r\"\"\"BNInception model architecture from <https://arxiv.org/pdf/1502.03167.pdf>`_ paper.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_proxy_connection_details():\n    '''\n    \n    '''\n    proxytype = get_proxy_type()\n    if proxytype == 'esxi':\n        details = __salt__['esxi.get_details']()\n    elif proxytype == 'esxcluster':\n        details = __salt__['esxcluster.get_details']()\n    elif proxytype == 'esxdatacenter':\n        details = __salt__['esxdatacenter.get_details']()\n    elif proxytype == 'vcenter':\n        details = __salt__['vcenter.get_details']()\n    elif proxytype == 'esxvm':\n        details = __salt__['esxvm.get_details']()\n    else:\n        raise CommandExecutionError('\\'{0}\\' proxy is not supported'\n                                    ''.format(proxytype))\n    return \\\n            details.get('vcenter') if 'vcenter' in details \\\n            else details.get('host'), \\\n            details.get('username'), \\\n            details.get('password'), details.get('protocol'), \\\n            details.get('port'), details.get('mechanism'), \\\n            details.get('principal'), details.get('domain')", "output": "Returns the connection details of the following proxies: esxi", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_disk_size(vm_, swap, linode_id):\n    \n    '''\n    disk_size = get_linode(kwargs={'linode_id': linode_id})['TOTALHD']\n    return config.get_cloud_config_value(\n        'disk_size', vm_, __opts__, default=disk_size - swap\n    )", "output": "r'''\n    Returns the size of of the root disk in MB.\n\n    vm\\_\n        The VM to get the disk size for.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def append(self, value):\n    \"\"\"\"\"\"\n    self._values.append(self._type_checker.CheckValue(value))\n    if not self._message_listener.dirty:\n      self._message_listener.Modified()", "output": "Appends an item to the list. Similar to list.append().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _multi_blockify(tuples, dtype=None):\n    \"\"\"  \"\"\"\n\n    # group by dtype\n    grouper = itertools.groupby(tuples, lambda x: x[2].dtype)\n\n    new_blocks = []\n    for dtype, tup_block in grouper:\n\n        values, placement = _stack_arrays(list(tup_block), dtype)\n\n        block = make_block(values, placement=placement)\n        new_blocks.append(block)\n\n    return new_blocks", "output": "return an array of blocks that potentially have different dtypes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shift(\n            self,\n            periods: int = 1,\n            fill_value: object = None,\n    ) -> ABCExtensionArray:\n        \"\"\"\n        \n        \"\"\"\n        # Note: this implementation assumes that `self.dtype.na_value` can be\n        # stored in an instance of your ExtensionArray with `self.dtype`.\n        if not len(self) or periods == 0:\n            return self.copy()\n\n        if isna(fill_value):\n            fill_value = self.dtype.na_value\n\n        empty = self._from_sequence(\n            [fill_value] * min(abs(periods), len(self)),\n            dtype=self.dtype\n        )\n        if periods > 0:\n            a = empty\n            b = self[:-periods]\n        else:\n            a = self[abs(periods):]\n            b = empty\n        return self._concat_same_type([a, b])", "output": "Shift values by desired number.\n\n        Newly introduced missing values are filled with\n        ``self.dtype.na_value``.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        periods : int, default 1\n            The number of periods to shift. Negative values are allowed\n            for shifting backwards.\n\n        fill_value : object, optional\n            The scalar value to use for newly introduced missing values.\n            The default is ``self.dtype.na_value``\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        shifted : ExtensionArray\n\n        Notes\n        -----\n        If ``self`` is empty or ``periods`` is 0, a copy of ``self`` is\n        returned.\n\n        If ``periods > len(self)``, then an array of size\n        len(self) is returned, with all values filled with\n        ``self.dtype.na_value``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_rest_apis(self, project_name):\n        \"\"\"\n        \n        \"\"\"\n        all_apis = self.apigateway_client.get_rest_apis(\n            limit=500\n        )\n\n        for api in all_apis['items']:\n            if api['name'] != project_name:\n                continue\n            yield api", "output": "Generator that allows to iterate per every available apis.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_relative_path(root_path, experiment_config, key):\n    ''''''\n    if experiment_config.get(key) and not os.path.isabs(experiment_config.get(key)):\n        absolute_path = os.path.join(root_path, experiment_config.get(key))\n        print_normal('expand %s: %s to %s ' % (key, experiment_config[key], absolute_path))\n        experiment_config[key] = absolute_path", "output": "Change relative path to absolute path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_properties(self, feature):\n        \"\"\"'\"\"\"\n        if not isinstance(feature, b2.build.feature.Feature):\n            feature = b2.build.feature.get(feature)\n        assert isinstance(feature, b2.build.feature.Feature)\n\n        result = []\n        for p in self.all_:\n            if p.feature == feature:\n                result.append(p)\n        return result", "output": "Returns all contained properties associated with 'feature", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_to_directory(self, dataset_info_dir):\n    \"\"\"\"\"\"\n    # Save the metadata from the features (vocabulary, labels,...)\n    if self.features:\n      self.features.save_metadata(dataset_info_dir)\n\n    if self.redistribution_info.license:\n      with tf.io.gfile.GFile(self._license_filename(dataset_info_dir),\n                             \"w\") as f:\n        f.write(self.redistribution_info.license)\n\n    with tf.io.gfile.GFile(self._dataset_info_filename(dataset_info_dir),\n                           \"w\") as f:\n      f.write(self.as_json)", "output": "Write `DatasetInfo` as JSON to `dataset_info_dir`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def token_cli(self, eauth, load):\n        '''\n        \n        '''\n        load['cmd'] = 'mk_token'\n        load['eauth'] = eauth\n        tdata = self._send_token_request(load)\n        if 'token' not in tdata:\n            return tdata\n        try:\n            with salt.utils.files.set_umask(0o177):\n                with salt.utils.files.fopen(self.opts['token_file'], 'w+') as fp_:\n                    fp_.write(tdata['token'])\n        except (IOError, OSError):\n            pass\n        return tdata", "output": "Create the token from the CLI and request the correct data to\n        authenticate via the passed authentication mechanism", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _split_generators(self, dl_manager):\n    \"\"\"\"\"\"\n    image_tar_file = os.path.join(dl_manager.manual_dir,\n                                  self.builder_config.file_name)\n    if not tf.io.gfile.exists(image_tar_file):\n      # The current celebahq generation code depends on a concrete version of\n      # pillow library and cannot be easily ported into tfds.\n      msg = \"You must download the dataset files manually and place them in: \"\n      msg += dl_manager.manual_dir\n      msg += \" as .tar files. See testing/test_data/fake_examples/celeb_a_hq \"\n      raise AssertionError(msg)\n    return [\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TRAIN,\n            num_shards=50,\n            gen_kwargs={\"archive\": dl_manager.iter_archive(image_tar_file)},\n        )\n    ]", "output": "Returns SplitGenerators.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def versions(runas=None):\n    '''\n    \n    '''\n    ret = _pyenv_exec('versions', '--bare', runas=runas)\n    return [] if ret is False else ret.splitlines()", "output": "List the installed versions of python.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pyenv.versions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _strip_dirty(xmltree):\n    '''\n    \n    '''\n    dirty = xmltree.attrib.pop('dirtyId', None)\n    if dirty:\n        xmltree.attrib.pop('admin', None)\n        xmltree.attrib.pop('time', None)\n\n    for child in xmltree:\n        child = _strip_dirty(child)\n\n    return xmltree", "output": "Removes dirtyID tags from the candidate config result. Palo Alto devices will make the candidate configuration with\n    a dirty ID after a change. This can cause unexpected results when parsing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _register_babi_problems():\n  \"\"\"\n  \"\"\"\n  for (subset, subset_suffix) in [(\"en\", \"_1k\"), (\"en-10k\", \"_10k\")]:\n    for problem_name, babi_task_id in six.iteritems(_problems_to_register()):\n      problem_class = type(\"BabiQaConcat\" + problem_name + subset_suffix,\n                           (BabiQaConcat,), {\n                               \"babi_task_id\": babi_task_id,\n                               \"babi_subset\": subset\n                           })\n      registry.register_problem(problem_class)\n      REGISTERED_PROBLEMS.append(problem_class.name)", "output": "It dynamically instantiates a class for each babi subsets-tasks.\n\n   @registry.register_problem\n   class BabiQaConcatAllTasks_10k(EditSequenceRegexProblem):\n     @property\n     def babi_task_id(self):\n       return \"qa0\"\n     @property\n     def babi_subset(self):\n      return \"en-10k\"\n\n  It does not put the classes into the global namespace, so to access the class\n  we rely on the registry or this module\"s REGISTERED_PROBLEMS list.\n  It will be available as\n\n     registry.problem(\"babi_qa_concat_all_tasks_10k\")\n\n  i.e., change camel case to snake case. Numbers are considered lower case\n  characters for these purposes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def elemDump(self, f, cur):\n        \"\"\" \"\"\"\n        if cur is None: cur__o = None\n        else: cur__o = cur._o\n        libxml2mod.xmlElemDump(f, self._o, cur__o)", "output": "Dump an XML/HTML node, recursive behaviour, children are\n           printed too.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _slice_split_info_to_instruction_dicts(self, list_sliced_split_info):\n    \"\"\"\"\"\"\n    instruction_dicts = []\n    for sliced_split_info in list_sliced_split_info:\n      mask = splits_lib.slice_to_percent_mask(sliced_split_info.slice_value)\n\n      # Compute filenames from the given split\n      filepaths = list(sorted(self._build_split_filenames(\n          split_info_list=[sliced_split_info.split_info],\n      )))\n\n      # Compute the offsets\n      if sliced_split_info.split_info.num_examples:\n        shard_id2num_examples = splits_lib.get_shard_id2num_examples(\n            sliced_split_info.split_info.num_shards,\n            sliced_split_info.split_info.num_examples,\n        )\n        mask_offsets = splits_lib.compute_mask_offsets(shard_id2num_examples)\n      else:\n        logging.warning(\n            \"Statistics not present in the dataset. TFDS is not able to load \"\n            \"the total number of examples, so using the subsplit API may not \"\n            \"provide precise subsplits.\"\n        )\n        mask_offsets = [0] * len(filepaths)\n\n      for filepath, mask_offset in zip(filepaths, mask_offsets):\n        instruction_dicts.append({\n            \"filepath\": filepath,\n            \"mask\": mask,\n            \"mask_offset\": mask_offset,\n        })\n    return instruction_dicts", "output": "Return the list of files and reading mask of the files to read.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_file (self, file, file_location, project):\n        \"\"\" \n        \"\"\"\n        if __debug__:\n            from .targets import ProjectTarget\n            assert isinstance(file, basestring)\n            assert isinstance(file_location, basestring)\n            assert isinstance(project, ProjectTarget)\n        # Check if we've created a target corresponding to this file.\n        path = os.path.join(os.getcwd(), file_location, file)\n        path = os.path.normpath(path)\n\n        if path in self.files_:\n            return self.files_ [path]\n\n        file_type = b2.build.type.type (file)\n\n        result = FileTarget (file, file_type, project,\n                             None, file_location)\n        self.files_ [path] = result\n\n        return result", "output": "Creates a virtual target with appropriate name and type from 'file'.\n            If a target with that name in that project was already created, returns that already\n            created target.\n            TODO: more correct way would be to compute path to the file, based on name and source location\n            for the project, and use that path to determine if the target was already created.\n            TODO: passing project with all virtual targets starts to be annoying.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_valid_url(url):\n    \"\"\"\"\"\"\n    from .misc import to_text\n\n    if not url:\n        return url\n    pieces = urllib_parse.urlparse(to_text(url))\n    return all([pieces.scheme, pieces.netloc])", "output": "Checks if a given string is an url", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge_environment_settings(self, url, proxies, stream, verify, cert):\n        \"\"\"\n        \n        \"\"\"\n        # Gather clues from the surrounding environment.\n        if self.trust_env:\n            # Set environment's proxies.\n            no_proxy = proxies.get('no_proxy') if proxies is not None else None\n            env_proxies = get_environ_proxies(url, no_proxy=no_proxy)\n            for (k, v) in env_proxies.items():\n                proxies.setdefault(k, v)\n\n            # Look for requests environment configuration and be compatible\n            # with cURL.\n            if verify is True or verify is None:\n                verify = (os.environ.get('REQUESTS_CA_BUNDLE') or\n                          os.environ.get('CURL_CA_BUNDLE'))\n\n        # Merge all the kwargs.\n        proxies = merge_setting(proxies, self.proxies)\n        stream = merge_setting(stream, self.stream)\n        verify = merge_setting(verify, self.verify)\n        cert = merge_setting(cert, self.cert)\n\n        return {'verify': verify, 'proxies': proxies, 'stream': stream,\n                'cert': cert}", "output": "Check the environment and merge it with some settings.\n\n        :rtype: dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def img2img_transformer2d_base():\n  \"\"\"\"\"\"\n  hparams = image_transformer2d_base()\n  # learning related flags\n  hparams.layer_preprocess_sequence = \"n\"\n  hparams.layer_postprocess_sequence = \"da\"\n  # This version seems to benefit from a higher learning rate.\n  hparams.learning_rate = 0.2\n  hparams.layer_prepostprocess_dropout = 0.1\n  hparams.learning_rate_warmup_steps = 12000\n  hparams.filter_size = 2048\n  hparams.num_encoder_layers = 4\n  hparams.num_decoder_layers = 8\n  hparams.bottom[\"inputs\"] = modalities.image_channel_embeddings_bottom\n  hparams.dec_attention_type = cia.AttentionType.LOCAL_2D\n  hparams.block_raster_scan = True\n  return hparams", "output": "Base params for img2img 2d attention.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def newNs(self, href, prefix):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlNewNs(self._o, href, prefix)\n        if ret is None:raise treeError('xmlNewNs() failed')\n        __tmp = xmlNs(_obj=ret)\n        return __tmp", "output": "Creation of a new Namespace. This function will refuse to\n          create a namespace with a similar prefix than an existing\n          one present on this node. We use href==None in the case of\n           an element creation where the namespace was not defined.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _reference_info(references):\n    \"\"\"\n    \"\"\"\n    document_paths = []\n    reference_map = {}\n    for reference in references:\n        doc_path = reference._document_path\n        document_paths.append(doc_path)\n        reference_map[doc_path] = reference\n\n    return document_paths, reference_map", "output": "Get information about document references.\n\n    Helper for :meth:`~.firestore_v1beta1.client.Client.get_all`.\n\n    Args:\n        references (List[.DocumentReference, ...]): Iterable of document\n            references.\n\n    Returns:\n        Tuple[List[str, ...], Dict[str, .DocumentReference]]: A two-tuple of\n\n        * fully-qualified documents paths for each reference in ``references``\n        * a mapping from the paths to the original reference. (If multiple\n          ``references`` contains multiple references to the same document,\n          that key will be overwritten in the result.)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_pandas(self, is_transposed=False):\n        \"\"\"\n        \"\"\"\n        # In the case this is transposed, it is easier to just temporarily\n        # transpose back then transpose after the conversion. The performance\n        # is the same as if we individually transposed the blocks and\n        # concatenated them, but the code is much smaller.\n        if is_transposed:\n            return self.transpose().to_pandas(False).T\n        else:\n            retrieved_objects = [\n                [obj.to_pandas() for obj in part] for part in self.partitions\n            ]\n            if all(\n                isinstance(part, pandas.Series)\n                for row in retrieved_objects\n                for part in row\n            ):\n                axis = 0\n            elif all(\n                isinstance(part, pandas.DataFrame)\n                for row in retrieved_objects\n                for part in row\n            ):\n                axis = 1\n            else:\n                ErrorMessage.catch_bugs_and_request_email(True)\n            df_rows = [\n                pandas.concat([part for part in row], axis=axis)\n                for row in retrieved_objects\n                if not all(part.empty for part in row)\n            ]\n            if len(df_rows) == 0:\n                return pandas.DataFrame()\n            else:\n                return pandas.concat(df_rows)", "output": "Convert this object into a Pandas DataFrame from the partitions.\n\n        Args:\n            is_transposed: A flag for telling this object that the external\n                representation is transposed, but not the internal.\n\n        Returns:\n            A Pandas DataFrame", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_weights(wgts:Weights, stoi_wgts:Dict[str,int], itos_new:Collection[str]) -> Weights:\n    \"\"\n    dec_bias, enc_wgts = wgts.get('1.decoder.bias', None), wgts['0.encoder.weight']\n    wgts_m = enc_wgts.mean(0)\n    if dec_bias is not None: bias_m = dec_bias.mean(0)\n    new_w = enc_wgts.new_zeros((len(itos_new),enc_wgts.size(1))).zero_()\n    if dec_bias is not None: new_b = dec_bias.new_zeros((len(itos_new),)).zero_()\n    for i,w in enumerate(itos_new):\n        r = stoi_wgts[w] if w in stoi_wgts else -1\n        new_w[i] = enc_wgts[r] if r>=0 else wgts_m\n        if dec_bias is not None: new_b[i] = dec_bias[r] if r>=0 else bias_m\n    wgts['0.encoder.weight'] = new_w\n    if '0.encoder_dp.emb.weight' in wgts: wgts['0.encoder_dp.emb.weight'] = new_w.clone()\n    wgts['1.decoder.weight'] = new_w.clone()\n    if dec_bias is not None: wgts['1.decoder.bias'] = new_b\n    return wgts", "output": "Convert the model `wgts` to go with a new vocabulary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_quota_volume(name, path, size, enable_quota=False):\n    '''\n    \n    '''\n    cmd = 'volume quota {0}'.format(name)\n    if path:\n        cmd += ' limit-usage {0}'.format(path)\n    if size:\n        cmd += ' {0}'.format(size)\n\n    if enable_quota:\n        if not enable_quota_volume(name):\n            pass\n    if not _gluster(cmd):\n        return False\n    return True", "output": "Set quota to glusterfs volume.\n\n    name\n        Name of the gluster volume\n\n    path\n        Folder path for restriction in volume (\"/\")\n\n    size\n        Hard-limit size of the volume (MB/GB)\n\n    enable_quota\n        Enable quota before set up restriction\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' glusterfs.set_quota_volume <volume> <path> <size> enable_quota=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _format_strings(self):\n        \"\"\"  \"\"\"\n\n        values = self.values\n\n        if not isinstance(values, DatetimeIndex):\n            values = DatetimeIndex(values)\n\n        if self.formatter is not None and callable(self.formatter):\n            return [self.formatter(x) for x in values]\n\n        fmt_values = format_array_from_datetime(\n            values.asi8.ravel(),\n            format=_get_format_datetime64_from_values(values,\n                                                      self.date_format),\n            na_rep=self.nat_rep).reshape(values.shape)\n        return fmt_values.tolist()", "output": "we by definition have DO NOT have a TZ", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_metric_value(session_or_group, metric_name):\n  \"\"\"\n  \"\"\"\n  # Note: We can speed this up by converting the metric_values field\n  # to a dictionary on initialization, to avoid a linear search here. We'll\n  # need to wrap the SessionGroup and Session protos in a python object for\n  # that.\n  for metric_value in session_or_group.metric_values:\n    if (metric_value.name.tag == metric_name.tag and\n        metric_value.name.group == metric_name.group):\n      return metric_value", "output": "Returns the metric_value for a given metric in a session or session group.\n\n  Args:\n    session_or_group: A Session protobuffer or SessionGroup protobuffer.\n    metric_name: A MetricName protobuffer. The metric to search for.\n  Returns:\n    A MetricValue protobuffer representing the value of the given metric or\n    None if no such metric was found in session_or_group.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def css_files(self):\n        ''' \n\n        '''\n        bokehjsdir = self.bokehjsdir()\n        js_files = []\n        for root, dirnames, files in os.walk(bokehjsdir):\n            for fname in files:\n                if fname.endswith(\".css\"):\n                    js_files.append(join(root, fname))\n        return js_files", "output": "The CSS files in the BokehJS directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chgrp(path, group):\n    '''\n    \n    '''\n    func_name = '{0}.chgrp'.format(__virtualname__)\n    if __opts__.get('fun', '') == func_name:\n        log.info('The function %s should not be used on Windows systems; see '\n                 'function docs for details.', func_name)\n    log.debug('win_file.py %s Doing nothing for %s', func_name, path)\n\n    return None", "output": "Change the group of a file\n\n    Under Windows, this will do nothing.\n\n    While a file in Windows does have a 'primary group', this rarely used\n    attribute generally has no bearing on permissions unless intentionally\n    configured and is only used to support Unix compatibility features (e.g.\n    Services For Unix, NFS services).\n\n    Salt, therefore, remaps this function to do nothing while still being\n    compatible with Unix behavior. When managing Windows systems,\n    this function is superfluous and will generate an info level log entry if\n    used directly.\n\n    If you do actually want to set the 'primary group' of a file, use ``file\n    .chpgrp``.\n\n    To set group permissions use ``file.set_perms``\n\n    Args:\n        path (str): The path to the file or directory\n        group (str): The group (unused)\n\n    Returns:\n        None\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.chpgrp c:\\\\temp\\\\test.txt administrators", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_from_json(self, obj, json, models=None, setter=None):\n        ''' \n\n        '''\n        if isinstance(json, dict):\n            # we want to try to keep the \"format\" of the data spec as string, dict, or number,\n            # assuming the serialized dict is compatible with that.\n            old = getattr(obj, self.name)\n            if old is not None:\n                try:\n                    self.property._type.validate(old, False)\n                    if 'value' in json:\n                        json = json['value']\n                except ValueError:\n                    if isinstance(old, string_types) and 'field' in json:\n                        json = json['field']\n                # leave it as a dict if 'old' was a dict\n\n        super(DataSpecPropertyDescriptor, self).set_from_json(obj, json, models, setter)", "output": "Sets the value of this property from a JSON value.\n\n        This method first\n\n        Args:\n            obj (HasProps) :\n\n            json (JSON-dict) :\n\n            models(seq[Model], optional) :\n\n            setter (ClientSession or ServerSession or None, optional) :\n                This is used to prevent \"boomerang\" updates to Bokeh apps.\n                (default: None)\n\n                In the context of a Bokeh server application, incoming updates\n                to properties will be annotated with the session that is\n                doing the updating. This value is propagated through any\n                subsequent change notifications that the update triggers.\n                The session can compare the event setter to itself, and\n                suppress any updates that originate from itself.\n\n        Returns:\n            None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fashion_mnist_generator(tmp_dir, training, how_many, start_from=0):\n  \"\"\"\n  \"\"\"\n  _get_fashion_mnist(tmp_dir)\n  d = _FASHION_MNIST_LOCAL_FILE_PREFIX + (\n      _MNIST_TRAIN_DATA_FILENAME if training else _MNIST_TEST_DATA_FILENAME)\n  l = _FASHION_MNIST_LOCAL_FILE_PREFIX + (\n      _MNIST_TRAIN_LABELS_FILENAME if training else _MNIST_TEST_LABELS_FILENAME)\n  return mnist_common_generator(tmp_dir, training, how_many, d, l, start_from)", "output": "Image generator for FashionMNIST.\n\n  Args:\n    tmp_dir: path to temporary storage directory.\n    training: a Boolean; if true, we use the train set, otherwise the test set.\n    how_many: how many images and labels to generate.\n    start_from: from which image to start.\n\n  Returns:\n    An instance of image_generator that produces MNIST images.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def service_create(name, service_type, description=None, profile=None,\n                   **connection_args):\n    '''\n    \n    '''\n    kstone = auth(profile, **connection_args)\n    service = kstone.services.create(name, service_type, description=description)\n    return service_get(service.id, profile=profile, **connection_args)", "output": "Add service to Keystone service catalog\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.service_create nova compute \\\n'OpenStack Compute Service'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _collectAsArrow(self):\n        \"\"\"\n        \n        \"\"\"\n        with SCCallSiteSync(self._sc) as css:\n            sock_info = self._jdf.collectAsArrowToPython()\n\n        # Collect list of un-ordered batches where last element is a list of correct order indices\n        results = list(_load_from_socket(sock_info, ArrowCollectSerializer()))\n        batches = results[:-1]\n        batch_order = results[-1]\n\n        # Re-order the batch list using the correct order\n        return [batches[i] for i in batch_order]", "output": "Returns all records as a list of ArrowRecordBatches, pyarrow must be installed\n        and available on driver and worker Python environments.\n\n        .. note:: Experimental.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fix_header_comment(filename, timestamp):\n    \"\"\"\"\"\"\n    # Fix input file.\n    name = os.path.basename( filename )\n    for line in fileinput.input( filename, inplace=1, mode=\"rU\" ):\n        # If header-comment already contains anything for '$Id$', remove it.\n        line = re.sub(r'\\$Id:[^$]+\\$', r'$Id$', line.rstrip())\n        # Replace '$Id$' by a string containing the file's name (and a timestamp)!\n        line = re.sub(re.escape(r'$Id$'), r'$Id: ' + name + r' ' + timestamp.isoformat() + r' $', line.rstrip())\n        print(line)", "output": "Fixes the header-comment of the given file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_batches(self, batch_size, shuffle=True):\n        \"\"\"\n        \"\"\"\n        batches = []\n        for bkt_idx, bucket in enumerate(self._buckets):\n            bucket_size = bucket.shape[1]\n            n_tokens = bucket_size * self._bucket_lengths[bkt_idx]\n            n_splits = min(max(n_tokens // batch_size, 1), bucket_size)\n            range_func = np.random.permutation if shuffle else np.arange\n            for bkt_batch in np.array_split(range_func(bucket_size), n_splits):\n                batches.append((bkt_idx, bkt_batch))\n\n        if shuffle:\n            np.random.shuffle(batches)\n\n        for bkt_idx, bkt_batch in batches:\n            word_inputs = self._buckets[bkt_idx][:, bkt_batch, 0]  # word_id x sent_id\n            tag_inputs = self._buckets[bkt_idx][:, bkt_batch, 1]\n            arc_targets = self._buckets[bkt_idx][:, bkt_batch, 2]\n            rel_targets = self._buckets[bkt_idx][:, bkt_batch, 3]\n            yield word_inputs, tag_inputs, arc_targets, rel_targets", "output": "Get batch iterator\n\n        Parameters\n        ----------\n        batch_size : int\n            size of one batch\n        shuffle : bool\n            whether to shuffle batches. Don't set to True when evaluating on dev or test set.\n        Returns\n        -------\n        tuple\n            word_inputs, tag_inputs, arc_targets, rel_targets", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_hive(args, check_return_code=True):\n    \"\"\"\n    \n    \"\"\"\n    cmd = load_hive_cmd() + args\n    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = p.communicate()\n    if check_return_code and p.returncode != 0:\n        raise HiveCommandError(\"Hive command: {0} failed with error code: {1}\".format(\" \".join(cmd), p.returncode),\n                               stdout, stderr)\n    return stdout.decode('utf-8')", "output": "Runs the `hive` from the command line, passing in the given args, and\n    returning stdout.\n\n    With the apache release of Hive, so of the table existence checks\n    (which are done using DESCRIBE do not exit with a return code of 0\n    so we need an option to ignore the return code and just return stdout for parsing", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replaced_stream(stream_name):\n    \"\"\"\n    \n    \"\"\"\n    orig_stream = getattr(sys, stream_name)\n    new_stream = six.StringIO()\n    try:\n        setattr(sys, stream_name, new_stream)\n        yield getattr(sys, stream_name)\n    finally:\n        setattr(sys, stream_name, orig_stream)", "output": "Context manager to temporarily swap out *stream_name* with a stream wrapper.\n\n    :param str stream_name: The name of a sys stream to wrap\n    :returns: A ``StreamWrapper`` replacement, temporarily\n\n    >>> orig_stdout = sys.stdout\n    >>> with replaced_stream(\"stdout\") as stdout:\n    ...     sys.stdout.write(\"hello\")\n    ...     assert stdout.getvalue() == \"hello\"\n\n    >>> sys.stdout.write(\"hello\")\n    'hello'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dump_profiles(self, path):\n        \"\"\"  \"\"\"\n        for id, profiler, _ in self.profilers:\n            profiler.dump(id, path)\n        self.profilers = []", "output": "Dump the profile stats into directory `path`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def runner(self, load):\n        '''\n        \n        '''\n        # All runner opts pass through eauth\n        auth_type, err_name, key = self._prep_auth_info(load)\n\n        # Authenticate\n        auth_check = self.loadauth.check_authentication(load, auth_type)\n        error = auth_check.get('error')\n\n        if error:\n            # Authentication error occurred: do not continue.\n            return {'error': error}\n\n        # Authorize\n        runner_check = self.ckminions.runner_check(\n            auth_check.get('auth_list', []),\n            load['fun'],\n            load['kwarg']\n        )\n        username = auth_check.get('username')\n        if not runner_check:\n            return {'error': {'name': err_name,\n                              'message': 'Authentication failure of type \"{0}\" occurred '\n                                         'for user {1}.'.format(auth_type, username)}}\n        elif isinstance(runner_check, dict) and 'error' in runner_check:\n            # A dictionary with an error name/message was handled by ckminions.runner_check\n            return runner_check\n\n        # Authorized. Do the job!\n        try:\n            fun = load.pop('fun')\n            runner_client = salt.runner.RunnerClient(self.opts)\n            return runner_client.asynchronous(fun,\n                                              load.get('kwarg', {}),\n                                              username)\n        except Exception as exc:\n            log.exception('Exception occurred while introspecting %s')\n            return {'error': {'name': exc.__class__.__name__,\n                              'args': exc.args,\n                              'message': six.text_type(exc)}}", "output": "Send a master control function back to the runner system", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _module_parents(module_name):\n        '''\n        \n        '''\n        spl = module_name.split('.')\n        for i in range(len(spl), 0, -1):\n            yield '.'.join(spl[0:i])\n        if module_name:\n            yield ''", "output": ">>> list(Register._module_parents('a.b'))\n        ['a.b', 'a', '']", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def connect_put_namespaced_pod_proxy_with_path(self, name, namespace, path, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_put_namespaced_pod_proxy_with_path_with_http_info(name, namespace, path, **kwargs)\n        else:\n            (data) = self.connect_put_namespaced_pod_proxy_with_path_with_http_info(name, namespace, path, **kwargs)\n            return data", "output": "connect PUT requests to proxy of Pod\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_put_namespaced_pod_proxy_with_path(name, namespace, path, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodProxyOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str path: path to the resource (required)\n        :param str path2: Path is the URL path to use for the current proxy request to pod.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_tables(self):\n        \"\"\"\n        \"\"\"\n        table_list_pb = self._client.table_admin_client.list_tables(self.name)\n\n        result = []\n        for table_pb in table_list_pb:\n            table_prefix = self.name + \"/tables/\"\n            if not table_pb.name.startswith(table_prefix):\n                raise ValueError(\n                    \"Table name {} not of expected format\".format(table_pb.name)\n                )\n            table_id = table_pb.name[len(table_prefix) :]\n            result.append(self.table(table_id))\n\n        return result", "output": "List the tables in this instance.\n\n        For example:\n\n        .. literalinclude:: snippets.py\n            :start-after: [START bigtable_list_tables]\n            :end-before: [END bigtable_list_tables]\n\n        :rtype: list of :class:`Table <google.cloud.bigtable.table.Table>`\n        :returns: The list of tables owned by the instance.\n        :raises: :class:`ValueError <exceptions.ValueError>` if one of the\n                 returned tables has a name that is not of the expected format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_index_list(collections=DATABASE.index_list):\n    ''\n    return pd.DataFrame([item for item in collections.find()]).drop('_id', axis=1, inplace=False).set_index('code', drop=False)", "output": "\u83b7\u53d6\u6307\u6570\u5217\u8868", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_trial_info(self, expr_dir):\n        \"\"\"\n        \"\"\"\n        meta = self._build_trial_meta(expr_dir)\n\n        self.logger.debug(\"Create trial for %s\" % meta)\n\n        trial_record = TrialRecord.from_json(meta)\n        trial_record.save()", "output": "Create information for given trial.\n\n        Meta file will be loaded if exists, and the trial information\n        will be saved in db backend.\n\n        Args:\n            expr_dir (str): Directory path of the experiment.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_floatingip(self, floatingip_id, port=None):\n        '''\n        \n        '''\n        if port is None:\n            body = {'floatingip': {}}\n        else:\n            port_id = self._find_port_id(port)\n            body = {'floatingip': {'port_id': port_id}}\n        return self.network_conn.update_floatingip(\n            floatingip=floatingip_id, body=body)", "output": "Updates a floatingip, disassociates the floating ip if\n        port is set to `None`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_path(cls, archive_path: str, predictor_name: str = None) -> 'Predictor':\n        \"\"\"\n        \n        \"\"\"\n        return Predictor.from_archive(load_archive(archive_path), predictor_name)", "output": "Instantiate a :class:`Predictor` from an archive path.\n\n        If you need more detailed configuration options, such as running the predictor on the GPU,\n        please use `from_archive`.\n\n        Parameters\n        ----------\n        archive_path The path to the archive.\n\n        Returns\n        -------\n        A Predictor instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _os_release_info(self):\n        \"\"\"\n        \n        \"\"\"\n        if os.path.isfile(self.os_release_file):\n            with open(self.os_release_file) as release_file:\n                return self._parse_os_release_content(release_file)\n        return {}", "output": "Get the information items from the specified os-release file.\n\n        Returns:\n            A dictionary containing all information items.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _schema_from_json_file_object(self, file_obj):\n        \"\"\"\n        \"\"\"\n        json_data = json.load(file_obj)\n        return [SchemaField.from_api_repr(field) for field in json_data]", "output": "Helper function for schema_from_json that takes a\n       file object that describes a table schema.\n\n       Returns:\n            List of schema field objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_tags(filesystemid,\n                tags,\n                keyid=None,\n                key=None,\n                profile=None,\n                region=None,\n                **kwargs):\n    '''\n    \n    '''\n\n    client = _get_conn(key=key, keyid=keyid, profile=profile, region=region)\n\n    client.delete_tags(FileSystemId=filesystemid, Tags=tags)", "output": "Deletes the specified tags from a file system.\n\n    filesystemid\n        (string) - ID of the file system for whose tags will be removed.\n\n    tags\n        (list[string]) - The tag keys to delete to the file system\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' boto_efs.delete_tags", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def relaxNGParse(self):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlRelaxNGParse(self._o)\n        if ret is None:raise parserError('xmlRelaxNGParse() failed')\n        __tmp = relaxNgSchema(_obj=ret)\n        return __tmp", "output": "parse a schema definition resource and build an internal\n           XML Shema struture which can be used to validate instances.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def slice_hidden(x, hidden_size, num_blocks):\n  \"\"\"\n  \"\"\"\n  batch_size, latent_dim, _ = common_layers.shape_list(x)\n  block_dim = hidden_size // num_blocks\n  x_sliced = tf.reshape(x,\n                        shape=[batch_size, latent_dim, num_blocks, block_dim])\n  return x_sliced", "output": "Slice encoder hidden state under num_blocks.\n\n  Args:\n    x: Encoder hidden state of shape [batch_size, latent_dim, hidden_size].\n    hidden_size: Dimension of the latent space.\n    num_blocks: Number of blocks in DVQ.\n\n  Returns:\n    Sliced states of shape [batch_size, latent_dim, num_blocks, block_dim].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def company_path(cls, project, company):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/companies/{company}\", project=project, company=company\n        )", "output": "Return a fully-qualified company string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install_requirements_command(requirements_path):\n    ''''''\n    cmds = 'cd ' + requirements_path + ' && {0} -m pip install --user -r requirements.txt'\n    #TODO refactor python logic\n    if sys.platform == \"win32\":\n        cmds = cmds.format('python')\n    else:\n        cmds = cmds.format('python3')\n    call(cmds, shell=True)", "output": "install requirements.txt", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_valid_datatype(datatype_instance):\n    \"\"\"\n    \n    \"\"\"\n\n    # Remap so we can still use the python types for the simple cases\n    global _simple_type_remap\n    if datatype_instance in _simple_type_remap:\n        return True\n\n    # Now set the protobuf from this interface.\n    if isinstance(datatype_instance, (Int64, Double, String, Array)):\n        return True\n\n    elif isinstance(datatype_instance, Dictionary):\n        kt = datatype_instance.key_type\n\n        if isinstance(kt, (Int64, String)):\n            return True\n\n    return False", "output": "Returns true if datatype_instance is a valid datatype object and false otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cast_boolean(value):\n    \"\"\"\n    \n    \"\"\"\n    _BOOLEANS = {'1': True, 'yes': True, 'true': True, 'on': True,\n                 '0': False, 'no': False, 'false': False, 'off': False, '': False}\n    value = str(value)\n    if value.lower() not in _BOOLEANS:\n        raise ValueError('Not a boolean: %s' % value)\n\n    return _BOOLEANS[value.lower()]", "output": "Helper to convert config values to boolean as ConfigParser do.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_dvs_config_dict(dvs_name, dvs_config):\n    '''\n    \n    '''\n    log.trace('Building the dict of the DVS \\'%s\\' config', dvs_name)\n    conf_dict = {'name': dvs_name,\n                 'contact_email': dvs_config.contact.contact,\n                 'contact_name': dvs_config.contact.name,\n                 'description': dvs_config.description,\n                 'lacp_api_version': dvs_config.lacpApiVersion,\n                 'network_resource_control_version':\n                 dvs_config.networkResourceControlVersion,\n                 'network_resource_management_enabled':\n                 dvs_config.networkResourceManagementEnabled,\n                 'max_mtu': dvs_config.maxMtu}\n    if isinstance(dvs_config.uplinkPortPolicy,\n                  vim.DVSNameArrayUplinkPortPolicy):\n        conf_dict.update(\n            {'uplink_names': dvs_config.uplinkPortPolicy.uplinkPortName})\n    return conf_dict", "output": "Returns the dict representation of the DVS config\n\n    dvs_name\n        The name of the DVS\n\n    dvs_config\n        The DVS config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def configure_namespacebrowser(self):\n        \"\"\"\"\"\"\n        # Update namespace view\n        self.sig_namespace_view.connect(lambda data:\n            self.namespacebrowser.process_remote_view(data))\n\n        # Update properties of variables\n        self.sig_var_properties.connect(lambda data:\n            self.namespacebrowser.set_var_properties(data))", "output": "Configure associated namespace browser widget", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def suspend(name, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The suspend action must be called with '\n            '-a or --action.'\n        )\n\n    vm_properties = [\n        \"name\",\n        \"summary.runtime.powerState\"\n    ]\n\n    vm_list = salt.utils.vmware.get_mors_with_properties(_get_si(), vim.VirtualMachine, vm_properties)\n\n    for vm in vm_list:\n        if vm[\"name\"] == name:\n            if vm[\"summary.runtime.powerState\"] == \"poweredOff\":\n                ret = 'cannot suspend in powered off state'\n                log.info('VM %s %s', name, ret)\n                return ret\n            elif vm[\"summary.runtime.powerState\"] == \"suspended\":\n                ret = 'already suspended'\n                log.info('VM %s %s', name, ret)\n                return ret\n            try:\n                log.info('Suspending VM %s', name)\n                task = vm[\"object\"].Suspend()\n                salt.utils.vmware.wait_for_task(task, name, 'suspend')\n            except Exception as exc:\n                log.error(\n                    'Error while suspending VM %s: %s',\n                    name, exc,\n                    # Show the traceback if the debug logging level is enabled\n                    exc_info_on_loglevel=logging.DEBUG\n                )\n                return 'failed to suspend'\n\n    return 'suspended'", "output": "To suspend a VM using its name\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a suspend vmname", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def realized_pnl(self):\n        \"\"\"\n        \n        \"\"\"\n        return sum(position.realized_pnl for position in six.itervalues(self._positions))", "output": "[float] \u5e73\u4ed3\u76c8\u4e8f", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(connect_spec, dn):\n    '''\n    '''\n    l = connect(connect_spec)\n    log.info('deleting entry: dn: %s', repr(dn))\n    try:\n        l.c.delete_s(dn)\n    except ldap.LDAPError as e:\n        _convert_exception(e)\n    return True", "output": "Delete an entry from an LDAP database.\n\n    :param connect_spec:\n        See the documentation for the ``connect_spec`` parameter for\n        :py:func:`connect`.\n\n    :param dn:\n        Distinguished name of the entry.\n\n    :returns:\n        ``True`` if successful, raises an exception otherwise.\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt '*' ldap3.delete \"{\n            'url': 'ldaps://ldap.example.com/',\n            'bind': {\n                'method': 'simple',\n                'password': 'secret'}\n        }\" dn='cn=admin,dc=example,dc=com'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe_api_deployment(restApiId, deploymentId, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        deployment = conn.get_deployment(restApiId=restApiId, deploymentId=deploymentId)\n        return {'deployment': _convert_datetime_str(deployment)}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "Get API deployment for a given restApiId and deploymentId.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_apigateway.describe_api_deployent restApiId deploymentId", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _aux_type(self, i):\n        \"\"\"\n        \"\"\"\n        aux_type = ctypes.c_int()\n        check_call(_LIB.MXNDArrayGetAuxType(self.handle, i, ctypes.byref(aux_type)))\n        return _DTYPE_MX_TO_NP[aux_type.value]", "output": "Data-type of the array's ith aux data.\n\n        Returns\n        -------\n        numpy.dtype\n            This BaseSparseNDArray's aux data type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode_base64_dict(data):\n    ''' \n\n    '''\n    b64 = base64.b64decode(data['__ndarray__'])\n    array = np.copy(np.frombuffer(b64, dtype=data['dtype']))\n    if len(data['shape']) > 1:\n        array = array.reshape(data['shape'])\n    return array", "output": "Decode a base64 encoded array into a NumPy array.\n\n    Args:\n        data (dict) : encoded array data to decode\n\n    Data should have the format encoded by :func:`encode_base64_dict`.\n\n    Returns:\n        np.ndarray", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_actions(name, location='\\\\'):\n    \n    '''\n    # Create the task service object\n    with salt.utils.winapi.Com():\n        task_service = win32com.client.Dispatch(\"Schedule.Service\")\n    task_service.Connect()\n\n    # Get the folder to list folders from\n    task_folder = task_service.GetFolder(location)\n    task_definition = task_folder.GetTask(name).Definition\n    actions = task_definition.Actions\n\n    ret = []\n    for action in actions:\n        ret.append(action.Id)\n\n    return ret", "output": "r'''\n    List all actions that pertain to a task in the specified location.\n\n    :param str name: The name of the task for which list actions.\n\n    :param str location: A string value representing the location of the task\n        from which to list actions. Default is '\\\\' which is the root for the\n        task scheduler (C:\\Windows\\System32\\tasks).\n\n    :return: Returns a list of actions.\n    :rtype: list\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'minion-id' task.list_actions <task_name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _exec(**kwargs):\n    '''\n    \n    '''\n    if 'ignore_retcode' not in kwargs:\n        kwargs['ignore_retcode'] = True\n    if 'output_loglevel' not in kwargs:\n        kwargs['output_loglevel'] = 'quiet'\n    return salt.modules.cmdmod.run_all(**kwargs)", "output": "Simple internal wrapper for cmdmod.run", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _change_state(interface, new_state):\n    '''\n    \n    '''\n    if __grains__['lsb_distrib_id'] == 'nilrt':\n        return _change_state_legacy(interface, new_state)\n    service = _interface_to_service(interface)\n    if not service:\n        raise salt.exceptions.CommandExecutionError('Invalid interface name: {0}'.format(interface))\n    connected = _connected(service)\n    if (not connected and new_state == 'up') or (connected and new_state == 'down'):\n        service = pyconnman.ConnService(os.path.join(SERVICE_PATH, service))\n        try:\n            state = service.connect() if new_state == 'up' else service.disconnect()\n            return state is None\n        except Exception:\n            raise salt.exceptions.CommandExecutionError('Couldn\\'t {0} service: {1}\\n'\n                                                        .format('enable' if new_state == 'up' else 'disable', service))\n    return True", "output": "Enable or disable an interface\n\n    Change adapter mode to TCP/IP. If previous adapter mode was EtherCAT, the target will need reboot.\n\n    :param interface: interface label\n    :param new_state: up or down\n    :return: True if the service was enabled, otherwise an exception will be thrown.\n    :rtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forward(self,\n                layer_input: torch.Tensor,\n                layer_output: torch.Tensor,\n                layer_index: int = None,\n                total_layers: int = None) -> torch.Tensor:\n        # pylint: disable=arguments-differ\n        \"\"\"\n        \n        \"\"\"\n        if layer_index is not None and total_layers is not None:\n            dropout_prob = 1.0 * self.undecayed_dropout_prob * layer_index / total_layers\n        else:\n            dropout_prob = 1.0 * self.undecayed_dropout_prob\n        if self.training:\n            if torch.rand(1) < dropout_prob:\n                return layer_input\n            else:\n                return layer_output + layer_input\n        else:\n            return (1 - dropout_prob) * layer_output + layer_input", "output": "Apply dropout to this layer, for this whole mini-batch.\n        dropout_prob = layer_index / total_layers * undecayed_dropout_prob if layer_idx and\n        total_layers is specified, else it will use the undecayed_dropout_prob directly.\n\n        Parameters\n        ----------\n        layer_input ``torch.FloatTensor`` required\n            The input tensor of this layer.\n        layer_output ``torch.FloatTensor`` required\n            The output tensor of this layer, with the same shape as the layer_input.\n        layer_index ``int``\n            The layer index, starting from 1. This is used to calcuate the dropout prob\n            together with the `total_layers` parameter.\n        total_layers ``int``\n            The total number of layers.\n\n        Returns\n        -------\n        output: ``torch.FloatTensor``\n            A tensor with the same shape as `layer_input` and `layer_output`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hosts(self):\n        \"\"\"\n\n        \"\"\"\n        network = int(self.network_address)\n        broadcast = int(self.broadcast_address)\n        for x in long_range(1, broadcast - network + 1):\n            yield self._address_class(network + x)", "output": "Generate Iterator over usable hosts in a network.\n\n          This is like __iter__ except it doesn't return the\n          Subnet-Router anycast address.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_word(self, word, freq=None, tag=None):\n        \"\"\"\n        \n        \"\"\"\n        self.check_initialized()\n        word = strdecode(word)\n        freq = int(freq) if freq is not None else self.suggest_freq(word, False)\n        self.FREQ[word] = freq\n        self.total += freq\n        if tag:\n            self.user_word_tag_tab[word] = tag\n        for ch in xrange(len(word)):\n            wfrag = word[:ch + 1]\n            if wfrag not in self.FREQ:\n                self.FREQ[wfrag] = 0\n        if freq == 0:\n            finalseg.add_force_split(word)", "output": "Add a word to dictionary.\n\n        freq and tag can be omitted, freq defaults to be a calculated value\n        that ensures the word can be cut out.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toBlockMatrix(self, rowsPerBlock=1024, colsPerBlock=1024):\n        \"\"\"\n        \n        \"\"\"\n        java_block_matrix = self._java_matrix_wrapper.call(\"toBlockMatrix\",\n                                                           rowsPerBlock,\n                                                           colsPerBlock)\n        return BlockMatrix(java_block_matrix, rowsPerBlock, colsPerBlock)", "output": "Convert this matrix to a BlockMatrix.\n\n        :param rowsPerBlock: Number of rows that make up each block.\n                             The blocks forming the final rows are not\n                             required to have the given number of rows.\n        :param colsPerBlock: Number of columns that make up each block.\n                             The blocks forming the final columns are not\n                             required to have the given number of columns.\n\n        >>> rows = sc.parallelize([IndexedRow(0, [1, 2, 3]),\n        ...                        IndexedRow(6, [4, 5, 6])])\n        >>> mat = IndexedRowMatrix(rows).toBlockMatrix()\n\n        >>> # This IndexedRowMatrix will have 7 effective rows, due to\n        >>> # the highest row index being 6, and the ensuing\n        >>> # BlockMatrix will have 7 rows as well.\n        >>> print(mat.numRows())\n        7\n\n        >>> print(mat.numCols())\n        3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_container_root(name):\n    '''\n    \n    '''\n    path = _root(name)\n    if os.path.exists(path):\n        __context__['retcode'] = salt.defaults.exitcodes.SALT_BUILD_FAIL\n        raise CommandExecutionError(\n            'Container {0} already exists'.format(name)\n        )\n    else:\n        try:\n            os.makedirs(path)\n            return path\n        except OSError as exc:\n            raise CommandExecutionError(\n                'Unable to make container root directory {0}: {1}'\n                .format(name, exc)\n            )", "output": "Make the container root directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_svc(rcd, service_status):\n    '''\n    \n    '''\n    ena = None\n    lines = __salt__['cmd.run']('{0} rcvar'.format(rcd)).splitlines()\n    for rcvar in lines:\n        if rcvar.startswith('$') and '={0}'.format(service_status) in rcvar:\n            ena = 'yes'\n        elif rcvar.startswith('#'):\n            svc = rcvar.split(' ', 1)[1]\n        else:\n            continue\n\n    if ena and svc:\n        return svc\n    return None", "output": "Returns a unique service status", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rowCount(self, index=None):\r\n        \"\"\"\"\"\"\r\n        if self.axis == 0:\r\n            return max(1, self._shape[0])\r\n        else:\r\n            if self.total_rows <= self.rows_loaded:\r\n                return self.total_rows\r\n            else:\r\n                return self.rows_loaded", "output": "Get number of rows in the header.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def split_none(self):\n        \"\"\n        val = self[[]]\n        val.ignore_empty = True\n        return self._split(self.path, self, val)", "output": "Don't split the data and create an empty validation set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _propagate(self, package):  # type: (str) -> None\n        \"\"\"\n        \n        \"\"\"\n        changed = set()\n        changed.add(package)\n\n        while changed:\n            package = changed.pop()\n\n            # Iterate in reverse because conflict resolution tends to produce more\n            # general incompatibilities as time goes on. If we look at those first,\n            # we can derive stronger assignments sooner and more eagerly find\n            # conflicts.\n            for incompatibility in reversed(self._incompatibilities[package]):\n                result = self._propagate_incompatibility(incompatibility)\n\n                if result is _conflict:\n                    # If the incompatibility is satisfied by the solution, we use\n                    # _resolve_conflict() to determine the root cause of the conflict as a\n                    # new incompatibility.\n                    #\n                    # It also backjumps to a point in the solution\n                    # where that incompatibility will allow us to derive new assignments\n                    # that avoid the conflict.\n                    root_cause = self._resolve_conflict(incompatibility)\n\n                    # Back jumping erases all the assignments we did at the previous\n                    # decision level, so we clear [changed] and refill it with the\n                    # newly-propagated assignment.\n                    changed.clear()\n                    changed.add(str(self._propagate_incompatibility(root_cause)))\n                    break\n                elif result is not None:\n                    changed.add(result)", "output": "Performs unit propagation on incompatibilities transitively\n        related to package to derive new assignments for _solution.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_model(self, model, retry=DEFAULT_RETRY, not_found_ok=False):\n        \"\"\"\n        \"\"\"\n        if isinstance(model, str):\n            model = ModelReference.from_string(model, default_project=self.project)\n\n        if not isinstance(model, (Model, ModelReference)):\n            raise TypeError(\"model must be a Model or a ModelReference\")\n\n        try:\n            self._call_api(retry, method=\"DELETE\", path=model.path)\n        except google.api_core.exceptions.NotFound:\n            if not not_found_ok:\n                raise", "output": "[Beta] Delete a model\n\n        See\n        https://cloud.google.com/bigquery/docs/reference/rest/v2/models/delete\n\n        Args:\n            model (Union[ \\\n                :class:`~google.cloud.bigquery.model.Model`, \\\n                :class:`~google.cloud.bigquery.model.ModelReference`, \\\n                str, \\\n            ]):\n                A reference to the model to delete. If a string is passed in,\n                this method attempts to create a model reference from a\n                string using\n                :func:`google.cloud.bigquery.model.ModelReference.from_string`.\n            retry (:class:`google.api_core.retry.Retry`):\n                (Optional) How to retry the RPC.\n            not_found_ok (bool):\n                Defaults to ``False``. If ``True``, ignore \"not found\" errors\n                when deleting the model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_log_entry(entry_pb):\n    \"\"\"\n    \"\"\"\n    try:\n        return MessageToDict(entry_pb)\n    except TypeError:\n        if entry_pb.HasField(\"proto_payload\"):\n            proto_payload = entry_pb.proto_payload\n            entry_pb.ClearField(\"proto_payload\")\n            entry_mapping = MessageToDict(entry_pb)\n            entry_mapping[\"protoPayload\"] = proto_payload\n            return entry_mapping\n        else:\n            raise", "output": "Special helper to parse ``LogEntry`` protobuf into a dictionary.\n\n    The ``proto_payload`` field in ``LogEntry`` is of type ``Any``. This\n    can be problematic if the type URL in the payload isn't in the\n    ``google.protobuf`` registry. To help with parsing unregistered types,\n    this function will remove ``proto_payload`` before parsing.\n\n    :type entry_pb: :class:`.log_entry_pb2.LogEntry`\n    :param entry_pb: Log entry protobuf.\n\n    :rtype: dict\n    :returns: The parsed log entry. The ``protoPayload`` key may contain\n              the raw ``Any`` protobuf from ``entry_pb.proto_payload`` if\n              it could not be parsed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_params(self, arg_params, aux_params, allow_missing=False, force_init=True,\n                   allow_extra=False):\n        \"\"\"\n        \"\"\"\n        if not allow_missing:\n            self.init_params(initializer=None, arg_params=arg_params, aux_params=aux_params,\n                             allow_missing=allow_missing, force_init=force_init,\n                             allow_extra=allow_extra)\n            return\n\n        if self.params_initialized and not force_init:\n            warnings.warn(\"Parameters already initialized and force_init=False. \"\n                          \"set_params call ignored.\", stacklevel=2)\n            return\n\n        self._exec_group.set_params(arg_params, aux_params, allow_extra=allow_extra)\n\n        # because we didn't update self._arg_params, they are dirty now.\n        self._params_dirty = True\n        self.params_initialized = True", "output": "Assigns parameter and aux state values.\n\n        Parameters\n        ----------\n        arg_params : dict\n            Dictionary of name to `NDArray`.\n        aux_params : dict\n            Dictionary of name to `NDArray`.\n        allow_missing : bool\n            If ``True``, params could contain missing values, and the initializer will be\n            called to fill those missing params.\n        force_init : bool\n            If ``True``, will force re-initialize even if already initialized.\n        allow_extra : boolean, optional\n            Whether allow extra parameters that are not needed by symbol.\n            If this is True, no error will be thrown when arg_params or aux_params\n            contain extra parameters that is not needed by the executor.\n        Examples\n        --------\n        >>> # An example of setting module parameters.\n        >>> sym, arg_params, aux_params = mx.model.load_checkpoint(model_prefix, n_epoch_load)\n        >>> mod.set_params(arg_params=arg_params, aux_params=aux_params)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clone(self):\n        \"\"\n        return self.__class__(FlowField(self.size, self.flow.flow.clone()), scale=False, y_first=False)", "output": "Mimic the behavior of torch.clone for `ImagePoints` objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate(self,\n                 asset,\n                 amount,\n                 portfolio,\n                 algo_datetime,\n                 algo_current_data):\n        \"\"\"\n        \n        \"\"\"\n        if self.restrictions.is_restricted(asset, algo_datetime):\n            self.handle_violation(asset, amount, algo_datetime)", "output": "Fail if the asset is in the restricted_list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset(self):\n        \"\"\n        for layer in self.layers:     layer.reset()\n        if self.bidirectional:\n            for layer in self.layers_bwd: layer.reset()", "output": "If your convolutional window is greater than 1 and you save previous xs, you must reset at the beginning of each new sequence.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sum(self, axis=0, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        nv.validate_sum(args, kwargs)\n        valid_vals = self._valid_sp_values\n        sp_sum = valid_vals.sum()\n        if self._null_fill_value:\n            return sp_sum\n        else:\n            nsparse = self.sp_index.ngaps\n            return sp_sum + self.fill_value * nsparse", "output": "Sum of non-NA/null values\n\n        Returns\n        -------\n        sum : float", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _run_kaggle_command(command_args, competition_name):\n  \"\"\"\"\"\"\n  try:\n    output = sp.check_output(command_args)\n    return tf.compat.as_text(output)\n  except sp.CalledProcessError as err:\n    output = err.output\n    _log_command_output(output, error=True)\n    if output.startswith(b\"404\"):\n      logging.error(_NOT_FOUND_ERR_MSG, competition_name)\n      raise\n    logging.error(_ERR_MSG, competition_name)\n    raise", "output": "Run kaggle command with subprocess.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_similarity_task_tokens(args):\n    \"\"\"\"\"\"\n    tokens = set()\n    for _, _, dataset in iterate_similarity_datasets(args):\n        tokens.update(\n            itertools.chain.from_iterable((d[0], d[1]) for d in dataset))\n    return tokens", "output": "Returns a set of all tokens occurring the evaluation datasets.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def memory_usage(self, deep=False):\n        \"\"\"\n        \n        \"\"\"\n        if hasattr(self.array, 'memory_usage'):\n            return self.array.memory_usage(deep=deep)\n\n        v = self.array.nbytes\n        if deep and is_object_dtype(self) and not PYPY:\n            v += lib.memory_usage_of_objects(self.array)\n        return v", "output": "Memory usage of the values\n\n        Parameters\n        ----------\n        deep : bool\n            Introspect the data deeply, interrogate\n            `object` dtypes for system-level memory consumption\n\n        Returns\n        -------\n        bytes used\n\n        See Also\n        --------\n        numpy.ndarray.nbytes\n\n        Notes\n        -----\n        Memory usage does not include memory consumed by elements that\n        are not components of the array if deep=False or if used on PyPy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_scheduled_actions(conn, as_name, scheduled_actions):\n    '''\n    \n    '''\n    if scheduled_actions:\n        for name, action in six.iteritems(scheduled_actions):\n            if 'start_time' in action and isinstance(action['start_time'], six.string_types):\n                action['start_time'] = datetime.datetime.strptime(\n                    action['start_time'], DATE_FORMAT\n                )\n            if 'end_time' in action and isinstance(action['end_time'], six.string_types):\n                action['end_time'] = datetime.datetime.strptime(\n                    action['end_time'], DATE_FORMAT\n                )\n            conn.create_scheduled_group_action(as_name, name,\n                desired_capacity=action.get('desired_capacity'),\n                min_size=action.get('min_size'),\n                max_size=action.get('max_size'),\n                start_time=action.get('start_time'),\n                end_time=action.get('end_time'),\n                recurrence=action.get('recurrence')\n            )", "output": "Helper function to create scheduled actions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_suffix (self, specified_name, file_type, prop_set):\n        \"\"\" \n        \"\"\"\n        assert isinstance(specified_name, basestring)\n        assert isinstance(file_type, basestring)\n        assert isinstance(prop_set, property_set.PropertySet)\n        suffix = b2.build.type.generated_target_suffix (file_type, prop_set)\n\n        if suffix:\n            return specified_name + '.' + suffix\n\n        else:\n            return specified_name", "output": "Appends the suffix appropriate to 'type/property_set' combination\n            to the specified name and returns the result.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_trial(self, trial):\n        \"\"\"\n        \"\"\"\n        trial.set_verbose(self._verbose)\n        self._trials.append(trial)\n        with warn_if_slow(\"scheduler.on_trial_add\"):\n            self._scheduler_alg.on_trial_add(self, trial)\n        self.trial_executor.try_checkpoint_metadata(trial)", "output": "Adds a new trial to this TrialRunner.\n\n        Trials may be added at any time.\n\n        Args:\n            trial (Trial): Trial to queue.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_population(self, population_size, graph_max_layer, graph_min_layer):\n        \"\"\"\n        \n        \"\"\"\n        population = []\n        graph = Graph(max_layer_num=graph_max_layer, min_layer_num=graph_min_layer,\n                      inputs=[Layer(LayerType.input.value, output=[4, 5], size='x'), Layer(LayerType.input.value, output=[4, 5], size='y')],\n                      output=[Layer(LayerType.output.value, inputs=[4], size='x'), Layer(LayerType.output.value, inputs=[5], size='y')],\n                      hide=[Layer(LayerType.attention.value, inputs=[0, 1], output=[2]),\n                            Layer(LayerType.attention.value, inputs=[1, 0], output=[3])])\n        for _ in range(population_size):\n            graph_tmp = copy.deepcopy(graph)\n            graph_tmp.mutation()\n            population.append(Individual(indiv_id=self.generate_new_id(), graph_cfg=graph_tmp, result=None))\n        return population", "output": "initialize populations for evolution tuner", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _partition_tasks(worker):\n    \"\"\"\n    \n    \"\"\"\n    task_history = worker._add_task_history\n    pending_tasks = {task for(task, status, ext) in task_history if status == 'PENDING'}\n    set_tasks = {}\n    set_tasks[\"completed\"] = {task for (task, status, ext) in task_history if status == 'DONE' and task in pending_tasks}\n    set_tasks[\"already_done\"] = {task for (task, status, ext) in task_history\n                                 if status == 'DONE' and task not in pending_tasks and task not in set_tasks[\"completed\"]}\n    set_tasks[\"ever_failed\"] = {task for (task, status, ext) in task_history if status == 'FAILED'}\n    set_tasks[\"failed\"] = set_tasks[\"ever_failed\"] - set_tasks[\"completed\"]\n    set_tasks[\"scheduling_error\"] = {task for(task, status, ext) in task_history if status == 'UNKNOWN'}\n    set_tasks[\"still_pending_ext\"] = {task for (task, status, ext) in task_history\n                                      if status == 'PENDING' and task not in set_tasks[\"ever_failed\"] and task not in set_tasks[\"completed\"] and not ext}\n    set_tasks[\"still_pending_not_ext\"] = {task for (task, status, ext) in task_history\n                                          if status == 'PENDING' and task not in set_tasks[\"ever_failed\"] and task not in set_tasks[\"completed\"] and ext}\n    set_tasks[\"run_by_other_worker\"] = set()\n    set_tasks[\"upstream_failure\"] = set()\n    set_tasks[\"upstream_missing_dependency\"] = set()\n    set_tasks[\"upstream_run_by_other_worker\"] = set()\n    set_tasks[\"upstream_scheduling_error\"] = set()\n    set_tasks[\"not_run\"] = set()\n    return set_tasks", "output": "Takes a worker and sorts out tasks based on their status.\n    Still_pending_not_ext is only used to get upstream_failure, upstream_missing_dependency and run_by_other_worker", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_future_list(collections=DATABASE.future_list):\n    ''\n    return pd.DataFrame([item for item in collections.find()]).drop('_id', axis=1, inplace=False).set_index('code', drop=False)", "output": "\u83b7\u53d6\u671f\u8d27\u5217\u8868", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ls(path, load_path=None):  # pylint: disable=C0103\n    '''\n    \n    '''\n    def _match(path):\n        ''' Internal match function '''\n        try:\n            matches = aug.match(salt.utils.stringutils.to_str(path))\n        except RuntimeError:\n            return {}\n\n        ret = {}\n        for _ma in matches:\n            ret[_ma] = aug.get(_ma)\n        return ret\n\n    load_path = _check_load_paths(load_path)\n\n    aug = _Augeas(loadpath=load_path)\n\n    path = path.rstrip('/') + '/'\n    match_path = path + '*'\n\n    matches = _match(match_path)\n    ret = {}\n\n    for key, value in six.iteritems(matches):\n        name = _lstrip_word(key, path)\n        if _match(key + '/*'):\n            ret[name + '/'] = value  # has sub nodes, e.g. directory\n        else:\n            ret[name] = value\n    return ret", "output": "List the direct children of a node\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' augeas.ls /files/etc/passwd\n\n    path\n        The path to list\n\n    .. versionadded:: 2016.3.0\n\n    load_path\n        A colon-spearated list of directories that modules should be searched\n        in. This is in addition to the standard load path and the directories\n        in AUGEAS_LENS_LIB.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forward(self, x, *args):\n        \"\"\"\"\"\"\n        if isinstance(x, NDArray):\n            with x.context as ctx:\n                if self._active:\n                    return self._call_cached_op(x, *args)\n\n                try:\n                    params = {i: j.data(ctx) for i, j in self._reg_params.items()}\n                except DeferredInitializationError:\n                    self._deferred_infer_shape(x, *args)\n                    for _, i in self.params.items():\n                        i._finish_deferred_init()\n                    params = {i: j.data(ctx) for i, j in self._reg_params.items()}\n\n                return self.hybrid_forward(ndarray, x, *args, **params)\n\n        assert isinstance(x, Symbol), \\\n            \"HybridBlock requires the first argument to forward be either \" \\\n            \"Symbol or NDArray, but got %s\"%type(x)\n        params = {i: j.var() for i, j in self._reg_params.items()}\n        with self.name_scope():\n            return self.hybrid_forward(symbol, x, *args, **params)", "output": "Defines the forward computation. Arguments can be either\n        :py:class:`NDArray` or :py:class:`Symbol`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def has_body(self) -> bool:\n        \"\"\"\"\"\"\n        warnings.warn(\n            \"Deprecated, use .can_read_body #2005\",\n            DeprecationWarning, stacklevel=2)\n        return not self._payload.at_eof()", "output": "Return True if request's HTTP BODY can be read, False otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_filter(filetypes, ext):\n    \"\"\"\"\"\"\n    if not ext:\n        return ALL_FILTER\n    for title, ftypes in filetypes:\n        if ext in ftypes:\n            return _create_filter(title, ftypes)\n    else:\n        return ''", "output": "Return filter associated to file extension", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def schedule_enable(enable):\n    '''\n    \n    '''\n    status = salt.utils.mac_utils.validate_enabled(enable)\n\n    cmd = ['softwareupdate',\n           '--schedule',\n           salt.utils.mac_utils.validate_enabled(status)]\n    salt.utils.mac_utils.execute_return_success(cmd)\n\n    return salt.utils.mac_utils.validate_enabled(schedule_enabled()) == status", "output": "Enable/disable automatic update scheduling.\n\n    :param enable: True/On/Yes/1 to turn on automatic updates. False/No/Off/0\n        to turn off automatic updates. If this value is empty, the current\n        status will be returned.\n\n    :type: bool str\n\n    :return: True if scheduling is enabled, False if disabled\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n       salt '*' softwareupdate.schedule_enable on|off", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def actualize_source_type (self, sources, prop_set):\n        \"\"\" \n        \"\"\"\n        assert is_iterable_typed(sources, VirtualTarget)\n        assert isinstance(prop_set, property_set.PropertySet)\n        result = []\n        for i in sources:\n            scanner = None\n\n# FIXME: what's this?\n#            if isinstance (i, str):\n#                i = self.manager_.get_object (i)\n\n            if i.type ():\n                scanner = b2.build.type.get_scanner (i.type (), prop_set)\n\n            r = i.actualize (scanner)\n            result.append (r)\n\n        return result", "output": "Helper for 'actualize_sources'.\n            For each passed source, actualizes it with the appropriate scanner.\n            Returns the actualized virtual targets.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def function_info(self, functionKey):\r\n        \"\"\"\"\"\"\r\n        node_type = 'function'\r\n        filename, line_number, function_name = functionKey\r\n        if function_name == '<module>':\r\n            modulePath, moduleName = osp.split(filename)\r\n            node_type = 'module'\r\n            if moduleName == '__init__.py':\r\n                modulePath, moduleName = osp.split(modulePath)\r\n            function_name = '<' + moduleName + '>'\r\n        if not filename or filename == '~':\r\n            file_and_line = '(built-in)'\r\n            node_type = 'builtin'\r\n        else:\r\n            if function_name == '__init__':\r\n                node_type = 'constructor'                \r\n            file_and_line = '%s : %d' % (filename, line_number)\r\n        return filename, line_number, function_name, file_and_line, node_type", "output": "Returns processed information about the function's name and file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_session(self, session, start_info, groups_by_name):\n    \"\"\"\n    \"\"\"\n    # If the group_name is empty, this session's group contains only\n    # this session. Use the session name for the group name since session\n    # names are unique.\n    group_name = start_info.group_name or session.name\n    if group_name in groups_by_name:\n      groups_by_name[group_name].sessions.extend([session])\n    else:\n      # Create the group and add the session as the first one.\n      group = api_pb2.SessionGroup(\n          name=group_name,\n          sessions=[session],\n          monitor_url=start_info.monitor_url)\n      # Copy hparams from the first session (all sessions should have the same\n      # hyperparameter values) into result.\n      # There doesn't seem to be a way to initialize a protobuffer map in the\n      # constructor.\n      for (key, value) in six.iteritems(start_info.hparams):\n        group.hparams[key].CopyFrom(value)\n      groups_by_name[group_name] = group", "output": "Adds a new Session protobuffer to the 'groups_by_name' dictionary.\n\n    Called by _build_session_groups when we encounter a new session. Creates\n    the Session protobuffer and adds it to the relevant group in the\n    'groups_by_name' dict. Creates the session group if this is the first time\n    we encounter it.\n\n    Args:\n      session: api_pb2.Session. The session to add.\n      start_info: The SessionStartInfo protobuffer associated with the session.\n      groups_by_name: A str to SessionGroup protobuffer dict. Representing the\n        session groups and sessions found so far.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_csv(col, options={}):\n    \"\"\"\n    \n    \"\"\"\n\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.to_csv(_to_java_column(col), options)\n    return Column(jc)", "output": "Converts a column containing a :class:`StructType` into a CSV string.\n    Throws an exception, in the case of an unsupported type.\n\n    :param col: name of column containing a struct.\n    :param options: options to control converting. accepts the same options as the CSV datasource.\n\n    >>> from pyspark.sql import Row\n    >>> data = [(1, Row(name='Alice', age=2))]\n    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n    >>> df.select(to_csv(df.value).alias(\"csv\")).collect()\n    [Row(csv=u'2,Alice')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _disbatch_runner(self, chunk):\n        '''\n        \n        '''\n        full_return = chunk.pop('full_return', False)\n        pub_data = self.saltclients['runner'](chunk)\n        tag = pub_data['tag'] + '/ret'\n        try:\n            event = yield self.application.event_listener.get_event(self, tag=tag)\n\n            # only return the return data\n            ret = event if full_return else event['data']['return']\n            raise tornado.gen.Return(ret)\n        except TimeoutException:\n            raise tornado.gen.Return('Timeout waiting for runner to execute')", "output": "Disbatch runner client commands", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nms(dets, thresh):\n    \"\"\"\n    \n    \"\"\"\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep", "output": "greedily select boxes with high confidence and overlap with current maximum <= thresh\n    rule out overlap >= thresh\n    :param dets: [[x1, y1, x2, y2 score]]\n    :param thresh: retain overlap < thresh\n    :return: indexes to keep", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def alive(opts):\n    '''\n    \n    '''\n    if salt.utils.napalm.not_always_alive(opts):\n        return True  # don't force reconnection for not-always alive proxies\n        # or regular minion\n    is_alive_ret = call('is_alive', **{})\n    if not is_alive_ret.get('result', False):\n        log.debug(\n            '[%s] Unable to execute `is_alive`: %s',\n            opts.get('id'), is_alive_ret.get('comment')\n        )\n        # if `is_alive` is not implemented by the underneath driver,\n        # will consider the connection to be still alive\n        # we don't want overly request connection reestablishment\n        # NOTE: revisit this if IOS is still not stable\n        #       and return False to force reconnection\n        return True\n    flag = is_alive_ret.get('out', {}).get('is_alive', False)\n    log.debug('Is %s still alive? %s', opts.get('id'), 'Yes.' if flag else 'No.')\n    return flag", "output": "Return the connection status with the remote device.\n\n    .. versionadded:: 2017.7.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve_invite(invite):\n    \"\"\"\n    \n    \"\"\"\n    from .invite import Invite  # circular import\n    if isinstance(invite, Invite) or isinstance(invite, Object):\n        return invite.id\n    else:\n        rx = r'(?:https?\\:\\/\\/)?discord(?:\\.gg|app\\.com\\/invite)\\/(.+)'\n        m = re.match(rx, invite)\n        if m:\n            return m.group(1)\n    return invite", "output": "Resolves an invite from a :class:`Invite`, URL or ID\n\n    Parameters\n    -----------\n    invite: Union[:class:`Invite`, :class:`Object`, :class:`str`]\n        The invite.\n\n    Returns\n    --------\n    :class:`str`\n        The invite code.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_mu_tensor(self):\n    \"\"\"\n    \"\"\"\n    root = self._get_cubic_root()\n    dr = self._h_max / self._h_min\n    mu = tf.maximum(\n        root**2, ((tf.sqrt(dr) - 1) / (tf.sqrt(dr) + 1))**2)\n    return mu", "output": "Get the min mu which minimize the surrogate.\n\n    Returns:\n      The mu_t.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def databunch(self, path:PathOrStr=None, bs:int=64, val_bs:int=None, num_workers:int=defaults.cpus,\n                  dl_tfms:Optional[Collection[Callable]]=None, device:torch.device=None, collate_fn:Callable=data_collate,\n                  no_check:bool=False, **kwargs)->'DataBunch':\n        \"\"\n        path = Path(ifnone(path, self.path))\n        data = self.x._bunch.create(self.train, self.valid, test_ds=self.test, path=path, bs=bs, val_bs=val_bs,\n                                    num_workers=num_workers, device=device, collate_fn=collate_fn, no_check=no_check, **kwargs)\n        if getattr(self, 'normalize', False):#In case a normalization was serialized\n            norm = self.normalize\n            data.normalize((norm['mean'], norm['std']), do_x=norm['do_x'], do_y=norm['do_y'])\n        data.label_list = self\n        return data", "output": "Create an `DataBunch` from self, `path` will override `self.path`, `kwargs` are passed to `DataBunch.create`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def file_hash(load, fnd):\n    '''\n    \n    '''\n    if 'env' in load:\n        # \"env\" is not supported; Use \"saltenv\".\n        load.pop('env')\n\n    ret = {}\n\n    if 'saltenv' not in load:\n        return ret\n\n    if 'path' not in fnd or 'bucket' not in fnd or not fnd['path']:\n        return ret\n\n    cached_file_path = _get_cached_file_name(\n            fnd['bucket'],\n            load['saltenv'],\n            fnd['path'])\n\n    if os.path.isfile(cached_file_path):\n        ret['hsum'] = salt.utils.hashutils.get_hash(cached_file_path)\n        ret['hash_type'] = 'md5'\n\n    return ret", "output": "Return an MD5 file hash", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def owner(*paths, **kwargs):\n    '''\n    \n    '''\n    if not paths:\n        return ''\n    ret = {}\n    cmd_prefix = ['rpm', '-qf', '--queryformat', '%{name}']\n    for path in paths:\n        ret[path] = __salt__['cmd.run_stdout'](\n            cmd_prefix + [path],\n            output_loglevel='trace',\n            python_shell=False\n        )\n        if 'not owned' in ret[path].lower():\n            ret[path] = ''\n    if len(ret) == 1:\n        return next(six.itervalues(ret))\n    return ret", "output": ".. versionadded:: 2014.7.0\n\n    Return the name of the package that owns the file. Multiple file paths can\n    be passed. Like :mod:`pkg.version <salt.modules.yumpkg.version>`, if a\n    single path is passed, a string will be returned, and if multiple paths are\n    passed, a dictionary of file/package name pairs will be returned.\n\n    If the file is not owned by a package, or is not present on the minion,\n    then an empty string will be returned for that path.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' pkg.owner /usr/bin/apachectl\n        salt '*' pkg.owner /usr/bin/apachectl /etc/httpd/conf/httpd.conf", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _bin_update_items(self, items, replace_at_most_one,\n                          replacements, leftovers):\n        \"\"\"\n        \n        \"\"\"\n        for key, value in items:\n            # If there are existing items with key <key> that have yet to be\n            # marked for replacement, mark that item's value to be replaced by\n            # <value> by appending it to <replacements>.\n            if key in self and key not in replacements:\n                replacements[key] = [value]\n            elif (key in self and not replace_at_most_one and\n                  len(replacements[key]) < len(self.values(key))):\n                replacements[key].append(value)\n            else:\n                if replace_at_most_one:\n                    replacements[key] = [value]\n                else:\n                    leftovers.append((key, value))", "output": "<replacements and <leftovers> are modified directly, ala pass by\n        reference.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def detail(device='/dev/md0'):\n    '''\n    \n    '''\n    ret = {}\n    ret['members'] = {}\n\n    # Lets make sure the device exists before running mdadm\n    if not os.path.exists(device):\n        msg = \"Device {0} doesn't exist!\"\n        raise CommandExecutionError(msg.format(device))\n\n    cmd = ['mdadm', '--detail', device]\n    for line in __salt__['cmd.run_stdout'](cmd, python_shell=False).splitlines():\n        if line.startswith(device):\n            continue\n        if ' ' not in line:\n            continue\n        if ':' not in line:\n            if '/dev/' in line:\n                comps = line.split()\n                state = comps[4:-1]\n                ret['members'][comps[0]] = {\n                    'device': comps[-1],\n                    'major': comps[1],\n                    'minor': comps[2],\n                    'number': comps[0],\n                    'raiddevice': comps[3],\n                    'state': ' '.join(state),\n                }\n            continue\n        comps = line.split(' : ')\n        comps[0] = comps[0].lower()\n        comps[0] = comps[0].strip()\n        comps[0] = comps[0].replace(' ', '_')\n        ret[comps[0]] = comps[1].strip()\n    return ret", "output": "Show detail for a specified RAID device\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' raid.detail '/dev/md0'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_wheel(wheel_directory, config_settings=None, metadata_directory=None):\n    \"\"\"\"\"\"\n    poetry = Poetry.create(\".\")\n\n    return unicode(\n        WheelBuilder.make_in(\n            poetry, SystemEnv(Path(sys.prefix)), NullIO(), Path(wheel_directory)\n        )\n    )", "output": "Builds a wheel, places it in wheel_directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def print_format_output(dataframe):\n    \"\"\"\n    \"\"\"\n    print_df = pd.DataFrame()\n    dropped_cols = []\n    empty_cols = []\n    # column display priority is based on the info_keys passed in\n    for i, col in enumerate(dataframe):\n        if dataframe[col].isnull().all():\n            # Don't add col to print_df if is fully empty\n            empty_cols += [col]\n            continue\n\n        print_df[col] = dataframe[col]\n        test_table = tabulate(print_df, headers=\"keys\", tablefmt=\"psql\")\n        if str(test_table).index(\"\\n\") > TERM_WIDTH:\n            # Drop all columns beyond terminal width\n            print_df.drop(col, axis=1, inplace=True)\n            dropped_cols += list(dataframe.columns)[i:]\n            break\n\n    table = tabulate(\n        print_df, headers=\"keys\", tablefmt=\"psql\", showindex=\"never\")\n\n    print(table)\n    if dropped_cols:\n        print(\"Dropped columns:\", dropped_cols)\n        print(\"Please increase your terminal size to view remaining columns.\")\n    if empty_cols:\n        print(\"Empty columns:\", empty_cols)\n\n    return table, dropped_cols, empty_cols", "output": "Prints output of given dataframe to fit into terminal.\n\n    Returns:\n        table (pd.DataFrame): Final outputted dataframe.\n        dropped_cols (list): Columns dropped due to terminal size.\n        empty_cols (list): Empty columns (dropped on default).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathNextAttribute(self, ctxt):\n        \"\"\" \"\"\"\n        if ctxt is None: ctxt__o = None\n        else: ctxt__o = ctxt._o\n        ret = libxml2mod.xmlXPathNextAttribute(ctxt__o, self._o)\n        if ret is None:raise xpathError('xmlXPathNextAttribute() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Traversal function for the \"attribute\" direction TODO:\n           support DTD inherited default attributes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_psd_product(self, vector, dtype=None):\n    \"\"\"\n    \"\"\"\n    # For convenience, think of x as [\\alpha, \\beta]\n    if dtype is None:\n      dtype = self.nn_dtype\n    vector = tf.cast(vector, self.nn_dtype)\n    alpha = tf.reshape(vector[0], shape=[1, 1])\n    beta = vector[1:]\n    # Computing the product of matrix_h with beta part of vector\n    # At first layer, h is simply diagonal\n    h_beta = self.get_h_product(beta)\n\n    # Constructing final result using vector_g\n    result = tf.concat(\n        [\n            alpha * self.nu + tf.reduce_sum(tf.multiply(beta, self.vector_g)),\n            tf.multiply(alpha, self.vector_g) + h_beta\n        ],\n        axis=0)\n    return tf.cast(result, dtype)", "output": "Function that provides matrix product interface with PSD matrix.\n\n    Args:\n      vector: the vector to be multiplied with matrix M\n\n    Returns:\n      result_product: Matrix product of M and vector", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sliced_gan():\n  \"\"\"\"\"\"\n  hparams = common_hparams.basic_params1()\n  hparams.optimizer = \"adam\"\n  hparams.learning_rate_constant = 0.0002\n  hparams.learning_rate_warmup_steps = 500\n  hparams.learning_rate_schedule = \"constant * linear_warmup\"\n  hparams.label_smoothing = 0.0\n  hparams.batch_size = 128\n  hparams.hidden_size = 128\n  hparams.initializer = \"uniform_unit_scaling\"\n  hparams.initializer_gain = 1.0\n  hparams.weight_decay = 1e-6\n  hparams.kernel_height = 4\n  hparams.kernel_width = 4\n  hparams.bottleneck_bits = 128\n  hparams.add_hparam(\"discriminator_batchnorm\", True)\n  hparams.add_hparam(\"num_sliced_vecs\", 4096)\n  return hparams", "output": "Basic parameters for a vanilla_gan.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def restore_checkpoint(self) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n        \"\"\"\n        \n        \"\"\"\n        latest_checkpoint = self.find_latest_checkpoint()\n\n        if latest_checkpoint is None:\n            # No checkpoint to restore, start at 0\n            return {}, {}\n\n        model_path, training_state_path = latest_checkpoint\n\n        # Load the parameters onto CPU, then transfer to GPU.\n        # This avoids potential OOM on GPU for large models that\n        # load parameters onto GPU then make a new GPU copy into the parameter\n        # buffer. The GPU transfer happens implicitly in load_state_dict.\n        model_state = torch.load(model_path, map_location=nn_util.device_mapping(-1))\n        training_state = torch.load(training_state_path, map_location=nn_util.device_mapping(-1))\n        return model_state, training_state", "output": "Restores a model from a serialization_dir to the last saved checkpoint.\n        This includes a training state (typically consisting of an epoch count and optimizer state),\n        which is serialized separately from  model parameters. This function should only be used to\n        continue training - if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        `` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))``\n\n        If ``self._serialization_dir`` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return empty dicts.\n\n        Returns\n        -------\n        states: Tuple[Dict[str, Any], Dict[str, Any]]\n            The model state and the training state.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(path, saltenv=None):\n    '''\n    \n    '''\n    if salt.utils.platform.is_windows():\n        path = salt.utils.path.sanitize_win_path(path)\n    path = salt.utils.data.decode(path)\n\n    query = 'saltenv={0}'.format(saltenv) if saltenv else ''\n    url = salt.utils.data.decode(urlunparse(('file', '', path, '', query, '')))\n    return 'salt://{0}'.format(url[len('file:///'):])", "output": "join `path` and `saltenv` into a 'salt://' URL.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reduce(self, func):\n        \"\"\"\n        \n        \"\"\"\n        return self.map(lambda x: (None, x)).reduceByKey(func, 1).map(lambda x: x[1])", "output": "Return a new DStream in which each RDD has a single element\n        generated by reducing each RDD of this DStream.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_auth_gssapi_keyex(\n        self, username, gss_authenticated=AUTH_FAILED, cc_file=None\n    ):\n        \"\"\"\n        \n        \"\"\"\n        if gss_authenticated == AUTH_SUCCESSFUL:\n            return AUTH_SUCCESSFUL\n        return AUTH_FAILED", "output": "Authenticate the given user to the server if he is a valid krb5\n        principal and GSS-API Key Exchange was performed.\n        If GSS-API Key Exchange was not performed, this authentication method\n        won't be available.\n\n        :param str username: The username of the authenticating client\n        :param int gss_authenticated: The result of the krb5 authentication\n        :param str cc_filename: The krb5 client credentials cache filename\n        :return: ``AUTH_FAILED`` if the user is not authenticated otherwise\n                 ``AUTH_SUCCESSFUL``\n        :rtype: int\n        :note: Kerberos credential delegation is not supported.\n        :see: `.ssh_gss` `.kex_gss`\n        :note: : We are just checking in L{AuthHandler} that the given user is\n                 a valid krb5 principal!\n                 We don't check if the krb5 principal is allowed to log in on\n                 the server, because there is no way to do that in python. So\n                 if you develop your own SSH server with paramiko for a cetain\n                 plattform like Linux, you should call C{krb5_kuserok()} in\n                 your local kerberos library to make sure that the\n                 krb5_principal has an account on the server and is allowed\n                 to log in as a user.\n        :see: http://www.unix.com/man-page/all/3/krb5_kuserok/", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rpc(name, dest=None, **kwargs):\n    '''\n    \n    '''\n    ret = {'name': name, 'changes': {}, 'result': True, 'comment': ''}\n    ret['changes'] = __salt__['junos.rpc'](name, dest, **kwargs)\n    return ret", "output": "Executes the given rpc. The returned data can be stored in a file\n    by specifying the destination path with dest as an argument\n\n    .. code-block:: yaml\n\n        get-interface-information:\n            junos:\n              - rpc\n              - dest: /home/user/rpc.log\n              - interface_name: lo0\n\n\n    Parameters:\n      Required\n        * cmd:\n          The rpc to be executed. (default = None)\n      Optional\n        * dest:\n          Destination file where the rpc output is stored. (default = None)\n          Note that the file will be stored on the proxy minion. To push the\n          files to the master use the salt's following execution module: \\\n            :py:func:`cp.push <salt.modules.cp.push>`\n        * format:\n          The format in which the rpc reply must be stored in file specified in the dest\n          (used only when dest is specified) (default = xml)\n        * kwargs: keyworded arguments taken by rpc call like-\n            * timeout:\n              Set NETCONF RPC timeout. Can be used for commands which\n              take a while to execute. (default= 30 seconds)\n            * filter:\n              Only to be used with 'get-config' rpc to get specific configuration.\n            * terse:\n              Amount of information you want.\n            * interface_name:\n              Name of the interface whose information you want.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def text(self, x, y, text):\n        \"\"\"\n        \"\"\"\n        for i, char in enumerate(text):\n            self.point(x + i, y, char)", "output": "Print a text on ASCII canvas.\n\n        Args:\n            x (int): x coordinate where the text should start.\n            y (int): y coordinate where the text should start.\n            text (str): string that should be printed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def query_positions(self, accounts):\n        \"\"\"\n        \"\"\"\n        try:\n            data = self.call(\"positions\", {'client': accounts})\n            if data is not None:\n                cash_part = data.get('subAccounts', {}).get('\u4eba\u6c11\u5e01', False)\n                if cash_part:\n                    cash_available = cash_part.get('\u53ef\u7528\u91d1\u989d', cash_part.get('\u53ef\u7528'))\n\n                position_part = data.get('dataTable', False)\n                if position_part:\n                    res = data.get('dataTable', False)\n                    if res:\n                        hold_headers = res['columns']\n                        hold_headers = [\n                            cn_en_compare[item] for item in hold_headers\n                        ]\n                        hold_available = pd.DataFrame(\n                            res['rows'],\n                            columns=hold_headers\n                        )\n                if len(hold_available) == 1 and hold_available.amount[0] in [\n                        None,\n                        '',\n                        0\n                ]:\n                    hold_available = pd.DataFrame(\n                        data=None,\n                        columns=hold_headers\n                    )\n                return {\n                    'cash_available':\n                    cash_available,\n                    'hold_available':\n                    hold_available.assign(\n                        amount=hold_available.amount.apply(float)\n                    ).loc[:,\n                          ['code',\n                           'amount']].set_index('code').amount\n                }\n            else:\n                print(data)\n                return False, 'None ACCOUNT'\n        except:\n            return False", "output": "\u67e5\u8be2\u73b0\u91d1\u548c\u6301\u4ed3\n\n        Arguments:\n            accounts {[type]} -- [description]\n\n        Returns:\n            dict-- {'cash_available':xxx,'hold_available':xxx}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_binding_info(host_header='', ip_address='*', port=80):\n    '''\n    \n    '''\n    return ':'.join([ip_address, six.text_type(port),\n                    host_header.replace(' ', '')])", "output": "Combine the host header, IP address, and TCP port into bindingInformation\n    format. Binding Information specifies information to communicate with a\n    site. It includes the IP address, the port number, and an optional host\n    header (usually a host name) to communicate with the site.\n\n    Args:\n        host_header (str): Usually a hostname\n        ip_address (str): The IP address\n        port (int): The port\n\n    Returns:\n        str: A properly formatted bindingInformation string (IP:port:hostheader)\n            eg: 192.168.0.12:80:www.contoso.com", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_jpeg_decoding(module_spec):\n  \"\"\"\n  \"\"\"\n  input_height, input_width = hub.get_expected_image_size(module_spec)\n  input_depth = hub.get_num_image_channels(module_spec)\n  jpeg_data = tf.placeholder(tf.string, name='DecodeJPGInput')\n  decoded_image = tf.image.decode_jpeg(jpeg_data, channels=input_depth)\n  # Convert from full range of uint8 to range [0,1] of float32.\n  decoded_image_as_float = tf.image.convert_image_dtype(decoded_image,\n                                                        tf.float32)\n  decoded_image_4d = tf.expand_dims(decoded_image_as_float, 0)\n  resize_shape = tf.stack([input_height, input_width])\n  resize_shape_as_int = tf.cast(resize_shape, dtype=tf.int32)\n  resized_image = tf.image.resize_bilinear(decoded_image_4d,\n                                           resize_shape_as_int)\n  return jpeg_data, resized_image", "output": "Adds operations that perform JPEG decoding and resizing to the graph..\n\n  Args:\n    module_spec: The hub.ModuleSpec for the image module being used.\n\n  Returns:\n    Tensors for the node to feed JPEG data into, and the output of the\n      preprocessing steps.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_specifier(ireq):\n    \"\"\"\n    \n    \"\"\"\n    # TODO: Ideally, this is carried over to the pip library itself\n    specs = ireq.specifier._specs if ireq.req is not None else []\n    specs = sorted(specs, key=lambda x: x._spec[1])\n    return \",\".join(str(s) for s in specs) or \"<any>\"", "output": "Generic formatter for pretty printing the specifier part of\n    InstallRequirements to the terminal.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clip_eta(eta, ord, eps):\n  \"\"\"\n  \n  \"\"\"\n\n  # Clipping perturbation eta to self.ord norm ball\n  if ord not in [np.inf, 1, 2]:\n    raise ValueError('ord must be np.inf, 1, or 2.')\n  reduc_ind = list(xrange(1, len(eta.get_shape())))\n  avoid_zero_div = 1e-12\n  if ord == np.inf:\n    eta = clip_by_value(eta, -eps, eps)\n  else:\n    if ord == 1:\n      raise NotImplementedError(\"The expression below is not the correct way\"\n                                \" to project onto the L1 norm ball.\")\n      norm = tf.maximum(avoid_zero_div,\n                        reduce_sum(tf.abs(eta),\n                                   reduc_ind, keepdims=True))\n    elif ord == 2:\n      # avoid_zero_div must go inside sqrt to avoid a divide by zero\n      # in the gradient through this operation\n      norm = tf.sqrt(tf.maximum(avoid_zero_div,\n                                reduce_sum(tf.square(eta),\n                                           reduc_ind,\n                                           keepdims=True)))\n    # We must *clip* to within the norm ball, not *normalize* onto the\n    # surface of the ball\n    factor = tf.minimum(1., div(eps, norm))\n    eta = eta * factor\n  return eta", "output": "Helper function to clip the perturbation to epsilon norm ball.\n  :param eta: A tensor with the current perturbation.\n  :param ord: Order of the norm (mimics Numpy).\n              Possible values: np.inf, 1 or 2.\n  :param eps: Epsilon, bound of the perturbation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def split_predicate(ex: Extraction) -> Extraction:\n    \"\"\"\n    \n    \"\"\"\n    rel_toks = ex.toks[char_to_word_index(ex.rel.span[0], ex.sent) \\\n                       : char_to_word_index(ex.rel.span[1], ex.sent) + 1]\n    if not rel_toks:\n        return ex\n\n    verb_inds = [tok_ind for (tok_ind, tok)\n                 in enumerate(rel_toks)\n                 if tok.tag_.startswith('VB')]\n\n    last_verb_ind = verb_inds[-1] if verb_inds \\\n                    else (len(rel_toks) - 1)\n\n    rel_parts = [element_from_span([rel_toks[last_verb_ind]],\n                                   'V')]\n\n    before_verb = rel_toks[ : last_verb_ind]\n    after_verb = rel_toks[last_verb_ind + 1 : ]\n\n    if before_verb:\n        rel_parts.append(element_from_span(before_verb, \"BV\"))\n\n    if after_verb:\n        rel_parts.append(element_from_span(after_verb, \"AV\"))\n\n    return Extraction(ex.sent, ex.toks, ex.arg1, rel_parts, ex.args2, ex.confidence)", "output": "Ensure single word predicate\n    by adding \"before-predicate\" and \"after-predicate\"\n    arguments.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filename_to_url(filename: str, cache_dir: str = None) -> Tuple[str, str]:\n    \"\"\"\n    \n    \"\"\"\n    if cache_dir is None:\n        cache_dir = CACHE_DIRECTORY\n\n    cache_path = os.path.join(cache_dir, filename)\n    if not os.path.exists(cache_path):\n        raise FileNotFoundError(\"file {} not found\".format(cache_path))\n\n    meta_path = cache_path + '.json'\n    if not os.path.exists(meta_path):\n        raise FileNotFoundError(\"file {} not found\".format(meta_path))\n\n    with open(meta_path) as meta_file:\n        metadata = json.load(meta_file)\n    url = metadata['url']\n    etag = metadata['etag']\n\n    return url, etag", "output": "Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``FileNotFoundError`` if `filename` or its stored metadata do not exist.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decref_dependencies(self, term, refcounts):\n        \"\"\"\n        \n        \"\"\"\n        garbage = set()\n        # Edges are tuple of (from, to).\n        for parent, _ in self.graph.in_edges([term]):\n            refcounts[parent] -= 1\n            # No one else depends on this term. Remove it from the\n            # workspace to conserve memory.\n            if refcounts[parent] == 0:\n                garbage.add(parent)\n        return garbage", "output": "Decrement in-edges for ``term`` after computation.\n\n        Parameters\n        ----------\n        term : zipline.pipeline.Term\n            The term whose parents should be decref'ed.\n        refcounts : dict[Term -> int]\n            Dictionary of refcounts.\n\n        Return\n        ------\n        garbage : set[Term]\n            Terms whose refcounts hit zero after decrefing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def seek_write(path, data, offset):\n    '''\n    \n    '''\n    path = os.path.expanduser(path)\n    seek_fh = os.open(path, os.O_WRONLY)\n    try:\n        os.lseek(seek_fh, int(offset), 0)\n        ret = os.write(seek_fh, data)\n        os.fsync(seek_fh)\n    finally:\n        os.close(seek_fh)\n    return ret", "output": ".. versionadded:: 2014.1.0\n\n    Seek to a position on a file and write to it\n\n    path\n        path to file\n\n    data\n        data to write to file\n\n    offset\n        position in file to start writing\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.seek_write /path/to/file 'some data' 4096", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_train_begin(self, pbar:PBar, metrics_names:Collection[str], **kwargs:Any)->None:\n        \"\"\n        self.pbar = pbar\n        self.names = ['epoch', 'train_loss'] if self.no_val else ['epoch', 'train_loss', 'valid_loss']\n        self.metrics_names = metrics_names\n        self.names += self.metrics_names\n        if hasattr(self, '_added_met_names'): self.names += self._added_met_names\n        if self.add_time: self.names.append('time')\n        if not self.silent: self.pbar.write(self.names, table=True)\n        self.losses,self.val_losses,self.lrs,self.moms,self.metrics,self.nb_batches = [],[],[],[],[],[]", "output": "Initialize recording status at beginning of training.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def str_startswith(arr, pat, na=np.nan):\n    \"\"\"\n    \n    \"\"\"\n    f = lambda x: x.startswith(pat)\n    return _na_map(f, arr, na, dtype=bool)", "output": "Test if the start of each string element matches a pattern.\n\n    Equivalent to :meth:`str.startswith`.\n\n    Parameters\n    ----------\n    pat : str\n        Character sequence. Regular expressions are not accepted.\n    na : object, default NaN\n        Object shown if element tested is not a string.\n\n    Returns\n    -------\n    Series or Index of bool\n        A Series of booleans indicating whether the given pattern matches\n        the start of each string element.\n\n    See Also\n    --------\n    str.startswith : Python standard library string method.\n    Series.str.endswith : Same as startswith, but tests the end of string.\n    Series.str.contains : Tests if string element contains a pattern.\n\n    Examples\n    --------\n    >>> s = pd.Series(['bat', 'Bear', 'cat', np.nan])\n    >>> s\n    0     bat\n    1    Bear\n    2     cat\n    3     NaN\n    dtype: object\n\n    >>> s.str.startswith('b')\n    0     True\n    1    False\n    2    False\n    3      NaN\n    dtype: object\n\n    Specifying `na` to be `False` instead of `NaN`.\n\n    >>> s.str.startswith('b', na=False)\n    0     True\n    1    False\n    2    False\n    3    False\n    dtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enabled(name, **kwargs):\n    '''\n    \n    '''\n    if _service_is_upstart(name):\n        return _upstart_is_enabled(name)\n    else:\n        if _service_is_sysv(name):\n            return _sysv_is_enabled(name)\n    return None", "output": "Check to see if the named service is enabled to start on boot\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.enabled <service name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Prep(self, size, additionalBytes):\n        \"\"\"\n        \n        \"\"\"\n\n        # Track the biggest thing we've ever aligned to.\n        if size > self.minalign:\n            self.minalign = size\n\n        # Find the amount of alignment needed such that `size` is properly\n        # aligned after `additionalBytes`:\n        alignSize = (~(len(self.Bytes) - self.Head() + additionalBytes)) + 1\n        alignSize &= (size - 1)\n\n        # Reallocate the buffer if needed:\n        while self.Head() < alignSize+size+additionalBytes:\n            oldBufSize = len(self.Bytes)\n            self.growByteBuffer()\n            updated_head = self.head + len(self.Bytes) - oldBufSize\n            self.head = UOffsetTFlags.py_type(updated_head)\n        self.Pad(alignSize)", "output": "Prep prepares to write an element of `size` after `additional_bytes`\n        have been written, e.g. if you write a string, you need to align\n        such the int length field is aligned to SizeInt32, and the string\n        data follows it directly.\n        If all you need to do is align, `additionalBytes` will be 0.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _construct_stage(self, deployment, swagger):\n        \"\"\"\n        \"\"\"\n\n        # If StageName is some intrinsic function, then don't prefix the Stage's logical ID\n        # This will NOT create duplicates because we allow only ONE stage per API resource\n        stage_name_prefix = self.stage_name if isinstance(self.stage_name, string_types) else \"\"\n\n        stage = ApiGatewayStage(self.logical_id + stage_name_prefix + 'Stage',\n                                attributes=self.passthrough_resource_attributes)\n        stage.RestApiId = ref(self.logical_id)\n        stage.update_deployment_ref(deployment.logical_id)\n        stage.StageName = self.stage_name\n        stage.CacheClusterEnabled = self.cache_cluster_enabled\n        stage.CacheClusterSize = self.cache_cluster_size\n        stage.Variables = self.variables\n        stage.MethodSettings = self.method_settings\n        stage.AccessLogSetting = self.access_log_setting\n        stage.CanarySetting = self.canary_setting\n        stage.TracingEnabled = self.tracing_enabled\n\n        if swagger is not None:\n            deployment.make_auto_deployable(stage, swagger)\n\n        return stage", "output": "Constructs and returns the ApiGateway Stage.\n\n        :param model.apigateway.ApiGatewayDeployment deployment: the Deployment for this Stage\n        :returns: the Stage to which this SAM Api corresponds\n        :rtype: model.apigateway.ApiGatewayStage", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def token_generator(tree_path, source_token_vocab, target_token_vocab,\n                    eos=None):\n  \"\"\"\n  \"\"\"\n  eos_list = [] if eos is None else [eos]\n  with tf.gfile.GFile(tree_path, mode=\"r\") as tree_file:\n    tree_line = tree_file.readline()\n    while tree_line:\n      source, target = words_and_tags_from_wsj_tree(tree_line)\n      source_ints = source_token_vocab.encode(source.strip()) + eos_list\n      target_ints = target_token_vocab.encode(target.strip()) + eos_list\n      yield {\"inputs\": source_ints, \"targets\": target_ints}\n      tree_line = tree_file.readline()", "output": "Generator for parsing as a sequence-to-sequence task that uses tokens.\n\n  This generator assumes the files at source_path and target_path have\n  the same number of lines and yields dictionaries of \"inputs\" and \"targets\"\n  where inputs and targets are token ids from source and target lines\n  converted to integers using the token_map.\n\n  Args:\n    tree_path: path to the file with WSJ format trees, one per line.\n    source_token_vocab: GenericVocabulary object for source vocabulary.\n    target_token_vocab: GenericVocabulary object for target vocabulary.\n    eos: integer to append at the end of each sequence (default: None).\n\n  Yields:\n    A dictionary {\"inputs\": source-line, \"targets\": target-line} where\n    the lines are integer lists converted from tokens in the file lines.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ParseDict(js_dict, message, ignore_unknown_fields=False):\n  \"\"\"\n  \"\"\"\n  parser = _Parser(ignore_unknown_fields)\n  parser.ConvertMessage(js_dict, message)\n  return message", "output": "Parses a JSON dictionary representation into a message.\n\n  Args:\n    js_dict: Dict representation of a JSON message.\n    message: A protocol buffer message to merge into.\n    ignore_unknown_fields: If True, do not raise errors for unknown fields.\n\n  Returns:\n    The same message passed as argument.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _format_strings(self):\n        \"\"\"  \"\"\"\n\n        values = self.values.astype(object)\n        is_dates_only = _is_dates_only(values)\n        formatter = (self.formatter or\n                     _get_format_datetime64(is_dates_only,\n                                            date_format=self.date_format))\n        fmt_values = [formatter(x) for x in values]\n\n        return fmt_values", "output": "we by definition have a TZ", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def positive_integer(value):\n    \"\"\"\n    \"\"\"\n    try:\n        value = int(value)\n    except Exception:\n        raise argparse.ArgumentTypeError('Invalid int value: \\'{}\\''.format(value))\n    if value < 0:\n        raise argparse.ArgumentTypeError('Invalid positive int value: \\'{}\\''.format(value))\n    return value", "output": "Ensure that the provided value is a positive integer.\n\n    Parameters\n    ----------\n    value: int\n        The number to evaluate\n\n    Returns\n    -------\n    value: int\n        Returns a positive integer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_last_traded_dt(self, asset, dt):\n        \"\"\"\n        \n        \"\"\"\n        rf = self._roll_finders[asset.roll_style]\n        sid = (rf.get_contract_center(asset.root_symbol,\n                                      dt,\n                                      asset.offset))\n        if sid is None:\n            return pd.NaT\n        contract = rf.asset_finder.retrieve_asset(sid)\n        return self._bar_reader.get_last_traded_dt(contract, dt)", "output": "Get the latest minute on or before ``dt`` in which ``asset`` traded.\n\n        If there are no trades on or before ``dt``, returns ``pd.NaT``.\n\n        Parameters\n        ----------\n        asset : zipline.asset.Asset\n            The asset for which to get the last traded minute.\n        dt : pd.Timestamp\n            The minute at which to start searching for the last traded minute.\n\n        Returns\n        -------\n        last_traded : pd.Timestamp\n            The dt of the last trade for the given asset, using the input\n            dt as a vantage point.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def infer_bulk_complete_from_fs(datetimes, datetime_to_task, datetime_to_re):\n    \"\"\"\n    \n    \"\"\"\n    filesystems_and_globs_by_location = _get_filesystems_and_globs(datetime_to_task, datetime_to_re)\n    paths_by_datetime = [[o.path for o in flatten_output(datetime_to_task(d))] for d in datetimes]\n    listing = set()\n    for (f, g), p in zip(filesystems_and_globs_by_location, zip(*paths_by_datetime)):  # transposed, so here we're iterating over logical outputs, not datetimes\n        listing |= _list_existing(f, g, p)\n\n    # quickly learn everything that's missing\n    missing_datetimes = []\n    for d, p in zip(datetimes, paths_by_datetime):\n        if not set(p) <= listing:\n            missing_datetimes.append(d)\n\n    return missing_datetimes", "output": "Efficiently determines missing datetimes by filesystem listing.\n\n    The current implementation works for the common case of a task writing\n    output to a ``FileSystemTarget`` whose path is built using strftime with\n    format like '...%Y...%m...%d...%H...', without custom ``complete()`` or\n    ``exists()``.\n\n    (Eventually Luigi could have ranges of completion as first-class citizens.\n    Then this listing business could be factored away/be provided for\n    explicitly in target API or some kind of a history server.)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_query_arguments(self, name: str, strip: bool = True) -> List[str]:\n        \"\"\"\n        \"\"\"\n        return self._get_arguments(name, self.request.query_arguments, strip)", "output": "Returns a list of the query arguments with the given name.\n\n        If the argument is not present, returns an empty list.\n\n        .. versionadded:: 3.2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def print_file(self):\r\n        \"\"\"\"\"\"\r\n        editor = self.get_current_editor()\r\n        filename = self.get_current_filename()\r\n        printer = Printer(mode=QPrinter.HighResolution,\r\n                          header_font=self.get_plugin_font('printer_header'))\r\n        printDialog = QPrintDialog(printer, editor)\r\n        if editor.has_selected_text():\r\n            printDialog.setOption(QAbstractPrintDialog.PrintSelection, True)\r\n        self.redirect_stdio.emit(False)\r\n        answer = printDialog.exec_()\r\n        self.redirect_stdio.emit(True)\r\n        if answer == QDialog.Accepted:\r\n            self.starting_long_process(_(\"Printing...\"))\r\n            printer.setDocName(filename)\r\n            editor.print_(printer)\r\n            self.ending_long_process()", "output": "Print current file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_freq_tuples(my_list, print_total_threshold):\n    \"\"\"  \"\"\"\n    d = {}\n    for token in my_list:\n        d.setdefault(token, 0)\n        d[token] += 1\n    return sorted(d.items(), key=operator.itemgetter(1), reverse=True)[:print_total_threshold]", "output": "Turn a list of errors into frequency-sorted tuples thresholded by a certain total number", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_list(self, src_list, dest_list, inplace=False, regex=False):\n        \"\"\"  \"\"\"\n\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n\n        # figure out our mask a-priori to avoid repeated replacements\n        values = self.as_array()\n\n        def comp(s, regex=False):\n            \"\"\"\n            Generate a bool array by perform an equality check, or perform\n            an element-wise regular expression matching\n            \"\"\"\n            if isna(s):\n                return isna(values)\n            if hasattr(s, 'asm8'):\n                return _compare_or_regex_search(maybe_convert_objects(values),\n                                                getattr(s, 'asm8'), regex)\n            return _compare_or_regex_search(values, s, regex)\n\n        masks = [comp(s, regex) for i, s in enumerate(src_list)]\n\n        result_blocks = []\n        src_len = len(src_list) - 1\n        for blk in self.blocks:\n\n            # its possible to get multiple result blocks here\n            # replace ALWAYS will return a list\n            rb = [blk if inplace else blk.copy()]\n            for i, (s, d) in enumerate(zip(src_list, dest_list)):\n                new_rb = []\n                for b in rb:\n                    m = masks[i][b.mgr_locs.indexer]\n                    convert = i == src_len\n                    result = b._replace_coerce(mask=m, to_replace=s, value=d,\n                                               inplace=inplace,\n                                               convert=convert, regex=regex)\n                    if m.any():\n                        new_rb = _extend_blocks(result, new_rb)\n                    else:\n                        new_rb.append(b)\n                rb = new_rb\n            result_blocks.extend(rb)\n\n        bm = self.__class__(result_blocks, self.axes)\n        bm._consolidate_inplace()\n        return bm", "output": "do a list replace", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def highlight_block(self, text):\r\n        \"\"\"\"\"\"\r\n        text = to_text_string(text)\r\n        if text.startswith((\"c\", \"C\")):\r\n            self.setFormat(0, len(text), self.formats[\"comment\"])\r\n            self.highlight_spaces(text)\r\n        else:\r\n            FortranSH.highlight_block(self, text)\r\n            self.setFormat(0, 5, self.formats[\"comment\"])\r\n            self.setFormat(73, max([73, len(text)]),\r\n                           self.formats[\"comment\"])", "output": "Implement highlight specific for Fortran77.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def todo_results_changed(self):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        editorstack = self.get_current_editorstack()\r\n        results = editorstack.get_todo_results()\r\n        index = editorstack.get_stack_index()\r\n        if index != -1:\r\n            filename = editorstack.data[index].filename\r\n            for other_editorstack in self.editorstacks:\r\n                if other_editorstack is not editorstack:\r\n                    other_editorstack.set_todo_results(filename, results)\r\n        self.update_todo_actions()", "output": "Synchronize todo results between editorstacks\r\n        Refresh todo list navigation buttons", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def density(self):\n        \"\"\"\n        \n        \"\"\"\n        tot_nonsparse = sum(ser.sp_index.npoints\n                            for _, ser in self.items())\n        tot = len(self.index) * len(self.columns)\n        return tot_nonsparse / float(tot)", "output": "Ratio of non-sparse points to total (dense) data points\n        represented in the frame", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mxnet_prefer_gpu():\n    \"\"\"\n    \"\"\"\n    gpu = int(os.environ.get('MXNET_GPU', default=0))\n    if gpu in mx.test_utils.list_gpus():\n        return mx.gpu(gpu)\n    return mx.cpu()", "output": "If gpu available return gpu, else cpu\n\n    Returns\n    -------\n    context : Context\n        The preferable GPU context.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_loffset(self, result):\n        \"\"\"\n        \n        \"\"\"\n\n        needs_offset = (\n            isinstance(self.loffset, (DateOffset, timedelta,\n                                      np.timedelta64)) and\n            isinstance(result.index, DatetimeIndex) and\n            len(result.index) > 0\n        )\n\n        if needs_offset:\n            result.index = result.index + self.loffset\n\n        self.loffset = None\n        return result", "output": "If loffset is set, offset the result index.\n\n        This is NOT an idempotent routine, it will be applied\n        exactly once to the result.\n\n        Parameters\n        ----------\n        result : Series or DataFrame\n            the result of resample", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_plane_axes(self, axis):\n        \"\"\"\n        \n        \"\"\"\n        return [self._get_axis(axi)\n                for axi in self._get_plane_axes_index(axis)]", "output": "Get my plane axes indexes: these are already\n        (as compared with higher level planes),\n        as we are returning a DataFrame axes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dropout_no_scaling(x, keep_prob):\n  \"\"\"\n  \"\"\"\n  if keep_prob == 1.0:\n    return x\n  mask = tf.less(tf.random_uniform(tf.shape(x)), keep_prob)\n  return x * cast_like(mask, x)", "output": "Like tf.nn.dropout, but does not scale up.  Works on integers also.\n\n  Args:\n    x: a Tensor\n    keep_prob: a floating point number\n\n  Returns:\n    Tensor of the same shape as x.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def subvolume_sync(path, subvolids=None, sleep=None):\n    '''\n    \n\n    '''\n    if subvolids and type(subvolids) is not list:\n        raise CommandExecutionError('Subvolids parameter must be a list')\n\n    cmd = ['btrfs', 'subvolume', 'sync']\n    if sleep:\n        cmd.extend(['-s', sleep])\n\n    cmd.append(path)\n    if subvolids:\n        cmd.extend(subvolids)\n\n    res = __salt__['cmd.run_all'](cmd)\n    salt.utils.fsutils._verify_run(res)\n    return True", "output": "Wait until given subvolume are completely removed from the\n    filesystem after deletion.\n\n    path\n        Mount point for the filesystem\n\n    subvolids\n        List of IDs of subvolumes to wait for\n\n    sleep\n        Sleep N seconds betwenn checks (default: 1)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' btrfs.subvolume_sync /var/volumes/tmp\n        salt '*' btrfs.subvolume_sync /var/volumes/tmp subvolids='[257]'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def inherit_doc(cls):\n    \"\"\"\n    \n    \"\"\"\n    for name, func in vars(cls).items():\n        # only inherit docstring for public functions\n        if name.startswith(\"_\"):\n            continue\n        if not func.__doc__:\n            for parent in cls.__bases__:\n                parent_func = getattr(parent, name, None)\n                if parent_func and getattr(parent_func, \"__doc__\", None):\n                    func.__doc__ = parent_func.__doc__\n                    break\n    return cls", "output": "A decorator that makes a class inherit documentation from its parents.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _soup_strings(soup):\n  \"\"\"\"\"\"\n  paragraph_tags = set([\n      \"caption\", \"details\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\", \"p\", \"td\",\n      \"div\", \"span\"\n  ])\n\n  skip_children = None\n  for descendant in soup.descendants:\n    # If we've treated a tag as a contiguous paragraph, don't re-emit the\n    # children (see below).\n    if skip_children is not None:\n      try:\n        in_skip = descendant in skip_children  # pylint: disable=unsupported-membership-test\n      except RecursionError:  # pylint: disable=undefined-variable\n        # Possible for this check to hit a nasty infinite recursion because of\n        # BeautifulSoup __eq__ checks.\n        in_skip = True\n      if in_skip:\n        continue\n      else:\n        skip_children = None\n\n    # Treat some tags as contiguous paragraphs, regardless of other tags nested\n    # inside (like <a> or <b>).\n    if isinstance(descendant, bs4.Tag):\n      if descendant.name in paragraph_tags:\n        if descendant.find_all(paragraph_tags):\n          # If there are nested paragraph tags, don't treat it as a single\n          # contiguous tag.\n          continue\n        skip_children = list(descendant.descendants)\n        text = \" \".join(descendant.get_text(\" \", strip=True).split())\n        if text:\n          yield text\n        continue\n\n    if (isinstance(descendant, bs4.Comment) or\n        not isinstance(descendant, bs4.NavigableString)):\n      continue\n\n    text = \" \".join(descendant.strip().split())\n    if text:\n      yield text", "output": "Return text strings in soup.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _getgrnam(name, root=None):\n    '''\n    \n    '''\n    root = root or '/'\n    passwd = os.path.join(root, 'etc/group')\n    with salt.utils.files.fopen(passwd) as fp_:\n        for line in fp_:\n            line = salt.utils.stringutils.to_unicode(line)\n            comps = line.strip().split(':')\n            if len(comps) < 4:\n                log.debug('Ignoring group line: %s', line)\n                continue\n            if comps[0] == name:\n                # Generate a getpwnam compatible output\n                comps[2] = int(comps[2])\n                comps[3] = comps[3].split(',') if comps[3] else []\n                return grp.struct_group(comps)\n    raise KeyError('getgrnam(): name not found: {}'.format(name))", "output": "Alternative implementation for getgrnam, that use only /etc/group", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reload(self, client=None):\n        \"\"\"\n        \"\"\"\n        client = self._require_client(client)\n        query_params = self._query_params\n        # Pass only '?projection=noAcl' here because 'acl' and related\n        # are handled via custom endpoints.\n        query_params[\"projection\"] = \"noAcl\"\n        api_response = client._connection.api_request(\n            method=\"GET\",\n            path=self.path,\n            query_params=query_params,\n            headers=self._encryption_headers(),\n            _target_object=self,\n        )\n        self._set_properties(api_response)", "output": "Reload properties from Cloud Storage.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use.  If not passed, falls back to the\n                       ``client`` stored on the current object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_cron_file(user, path):\n    '''\n    \n    '''\n    if _check_instance_uid_match(user) or __grains__.get('os_family') in ('Solaris', 'AIX'):\n        return __salt__['cmd.retcode'](_get_cron_cmdstr(path),\n                                       runas=user,\n                                       python_shell=False) == 0\n    else:\n        return __salt__['cmd.retcode'](_get_cron_cmdstr(path, user),\n                                       python_shell=False) == 0", "output": "Writes the contents of a file to a user's crontab\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cron.write_cron_file root /tmp/new_cron\n\n    .. versionchanged:: 2015.8.9\n\n    .. note::\n\n        Some OS' do not support specifying user via the `crontab` command i.e. (Solaris, AIX)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(path, size=None):\n        \"\"\"\n        \n        \"\"\"\n        gen = tf.python_io.tf_record_iterator(path)\n        ds = DataFromGenerator(gen)\n        ds = MapData(ds, loads)\n        if size is not None:\n            ds = FixedSizeData(ds, size)\n        return ds", "output": "Args:\n            size (int): total number of records. If not provided, the returned dataflow will have no `__len__()`.\n                It's needed because this metadata is not stored in the TFRecord file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_string_tensor_event(event):\n  \"\"\"\"\"\"\n  string_arr = tensor_util.make_ndarray(event.tensor_proto)\n  html = text_array_to_html(string_arr)\n  return {\n      'wall_time': event.wall_time,\n      'step': event.step,\n      'text': html,\n  }", "output": "Convert a TensorEvent into a JSON-compatible response.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def local_attention_1d(x,\n                       hparams,\n                       attention_type=\"local_unmasked\",\n                       q_padding=\"VALID\",\n                       kv_padding=\"VALID\"):\n  \"\"\"\"\"\"\n  # self-attention\n  x, x_shape, is_4d = maybe_reshape_4d_to_3d(x)\n  with tf.variable_scope(\"local_1d_self_att\"):\n    y = common_attention.multihead_attention(\n        x,\n        None,\n        None,\n        hparams.attention_key_channels or hparams.hidden_size,\n        hparams.attention_value_channels or hparams.hidden_size,\n        hparams.hidden_size,\n        hparams.num_heads,\n        hparams.attention_dropout,\n        attention_type=attention_type,\n        shared_rel=hparams.shared_rel,\n        block_width=hparams.block_width,\n        block_length=hparams.block_length,\n        q_padding=q_padding,\n        kv_padding=kv_padding,\n        q_filter_width=hparams.q_filter_width,\n        kv_filter_width=hparams.kv_filter_width,\n        make_image_summary=False,\n        name=\"self_attention\")\n    if is_4d:\n      y = tf.reshape(y, x_shape)\n    return y", "output": "Local 1d self attention.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def weak_lru_cache(maxsize=100):\n    \"\"\"\n\n    \"\"\"\n    class desc(lazyval):\n        def __get__(self, instance, owner):\n            if instance is None:\n                return self\n            try:\n                return self._cache[instance]\n            except KeyError:\n                inst = ref(instance)\n\n                @_weak_lru_cache(maxsize)\n                @wraps(self._get)\n                def wrapper(*args, **kwargs):\n                    return self._get(inst(), *args, **kwargs)\n\n                self._cache[instance] = wrapper\n                return wrapper\n\n        @_weak_lru_cache(maxsize)\n        def __call__(self, *args, **kwargs):\n            return self._get(*args, **kwargs)\n\n    return desc", "output": "Weak least-recently-used cache decorator.\n\n    If *maxsize* is set to None, the LRU features are disabled and the cache\n    can grow without bound.\n\n    Arguments to the cached function must be hashable. Any that are weak-\n    referenceable will be stored by weak reference.  Once any of the args have\n    been garbage collected, the entry will be removed from the cache.\n\n    View the cache statistics named tuple (hits, misses, maxsize, currsize)\n    with f.cache_info().  Clear the cache and statistics with f.cache_clear().\n\n    See:  http://en.wikipedia.org/wiki/Cache_algorithms#Least_Recently_Used", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_value_to_set(self):\n        \"\"\"\n        \n        \"\"\"\n        ret = self._get_value_to_set()\n        if ret is not None and ret != self._last_value:\n            if self.epoch_num != self._last_epoch_set:  # Print this message at most once every epoch\n                if self._last_value is None:\n                    logger.info(\"[HyperParamSetter] At global_step={}, {} is set to {:.6f}\".format(\n                        self.global_step, self.param.readable_name, ret))\n                else:\n                    logger.info(\"[HyperParamSetter] At global_step={}, {} changes from {:.6f} to {:.6f}\".format(\n                        self.global_step, self.param.readable_name, self._last_value, ret))\n            self._last_epoch_set = self.epoch_num\n            self._last_value = ret\n        return ret", "output": "Returns:\n            The value to assign to the variable.\n\n        Note:\n            Subclasses will implement the abstract method\n            :meth:`_get_value_to_set`, which should return a new value to\n            set, or return None to do nothing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def asfreq(self, freq=None, how='E'):\n        \"\"\"\n        \n        \"\"\"\n        how = libperiod._validate_end_alias(how)\n\n        freq = Period._maybe_convert_freq(freq)\n\n        base1, mult1 = libfrequencies.get_freq_code(self.freq)\n        base2, mult2 = libfrequencies.get_freq_code(freq)\n\n        asi8 = self.asi8\n        # mult1 can't be negative or 0\n        end = how == 'E'\n        if end:\n            ordinal = asi8 + mult1 - 1\n        else:\n            ordinal = asi8\n\n        new_data = period_asfreq_arr(ordinal, base1, base2, end)\n\n        if self._hasnans:\n            new_data[self._isnan] = iNaT\n\n        return type(self)(new_data, freq=freq)", "output": "Convert the Period Array/Index to the specified frequency `freq`.\n\n        Parameters\n        ----------\n        freq : str\n            a frequency\n        how : str {'E', 'S'}\n            'E', 'END', or 'FINISH' for end,\n            'S', 'START', or 'BEGIN' for start.\n            Whether the elements should be aligned to the end\n            or start within pa period. January 31st ('END') vs.\n            January 1st ('START') for example.\n\n        Returns\n        -------\n        new : Period Array/Index with the new frequency\n\n        Examples\n        --------\n        >>> pidx = pd.period_range('2010-01-01', '2015-01-01', freq='A')\n        >>> pidx\n        PeriodIndex(['2010', '2011', '2012', '2013', '2014', '2015'],\n        dtype='period[A-DEC]', freq='A-DEC')\n\n        >>> pidx.asfreq('M')\n        PeriodIndex(['2010-12', '2011-12', '2012-12', '2013-12', '2014-12',\n        '2015-12'], dtype='period[M]', freq='M')\n\n        >>> pidx.asfreq('M', how='S')\n        PeriodIndex(['2010-01', '2011-01', '2012-01', '2013-01', '2014-01',\n        '2015-01'], dtype='period[M]', freq='M')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _SkipFieldMessage(tokenizer):\n  \"\"\"\n  \"\"\"\n\n  if tokenizer.TryConsume('<'):\n    delimiter = '>'\n  else:\n    tokenizer.Consume('{')\n    delimiter = '}'\n\n  while not tokenizer.LookingAt('>') and not tokenizer.LookingAt('}'):\n    _SkipField(tokenizer)\n\n  tokenizer.Consume(delimiter)", "output": "Skips over a field message.\n\n  Args:\n    tokenizer: A tokenizer to parse the field name and values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pow(base, exp):\n    \"\"\"\n    \"\"\"\n    if isinstance(base, Symbol) and isinstance(exp, Symbol):\n        return _internal._Power(base, exp)\n    if isinstance(base, Symbol) and isinstance(exp, Number):\n        return _internal._PowerScalar(base, scalar=exp)\n    if isinstance(base, Number) and isinstance(exp, Symbol):\n        return _internal._RPowerScalar(exp, scalar=base)\n    if isinstance(base, Number) and isinstance(exp, Number):\n        return base**exp\n    else:\n        raise TypeError('types (%s, %s) not supported' % (str(type(base)), str(type(exp))))", "output": "Returns element-wise result of base element raised to powers from exp element.\n\n    Both inputs can be Symbol or scalar number.\n    Broadcasting is not supported. Use `broadcast_pow` instead.\n\n    `sym.pow` is being deprecated, please use `sym.power` instead.\n\n    Parameters\n    ---------\n    base : Symbol or scalar\n        The base symbol\n    exp : Symbol or scalar\n        The exponent symbol\n\n    Returns\n    -------\n    Symbol or scalar\n        The bases in x raised to the exponents in y.\n\n    Examples\n    --------\n    >>> mx.sym.pow(2, 3)\n    8\n    >>> x = mx.sym.Variable('x')\n    >>> y = mx.sym.Variable('y')\n    >>> z = mx.sym.pow(x, 2)\n    >>> z.eval(x=mx.nd.array([1,2]))[0].asnumpy()\n    array([ 1.,  4.], dtype=float32)\n    >>> z = mx.sym.pow(3, y)\n    >>> z.eval(y=mx.nd.array([2,3]))[0].asnumpy()\n    array([  9.,  27.], dtype=float32)\n    >>> z = mx.sym.pow(x, y)\n    >>> z.eval(x=mx.nd.array([3,4]), y=mx.nd.array([2,3]))[0].asnumpy()\n    array([  9.,  64.], dtype=float32)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate(self, x, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    # Parse and save attack-specific parameters\n    assert self.parse_params(**kwargs)\n\n    if self.ord != np.inf:\n      raise NotImplementedError(self.ord)\n    eta = tf.random_uniform(tf.shape(x), -self.eps, self.eps,\n                            dtype=self.tf_dtype)\n    adv_x = x + eta\n    if self.clip_min is not None or self.clip_max is not None:\n      assert self.clip_min is not None and self.clip_max is not None\n      adv_x = tf.clip_by_value(adv_x, self.clip_min, self.clip_max)\n\n    return adv_x", "output": "Generate symbolic graph for adversarial examples and return.\n\n    :param x: The model's symbolic inputs.\n    :param kwargs: See `parse_params`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_padding_lengths(self) -> Dict[str, Dict[str, int]]:\n        \"\"\"\n        \n        \"\"\"\n        padding_lengths: Dict[str, Dict[str, int]] = defaultdict(dict)\n        all_instance_lengths: List[Dict[str, Dict[str, int]]] = [instance.get_padding_lengths()\n                                                                 for instance in self.instances]\n        if not all_instance_lengths:\n            return {**padding_lengths}\n        all_field_lengths: Dict[str, List[Dict[str, int]]] = defaultdict(list)\n        for instance_lengths in all_instance_lengths:\n            for field_name, instance_field_lengths in instance_lengths.items():\n                all_field_lengths[field_name].append(instance_field_lengths)\n        for field_name, field_lengths in all_field_lengths.items():\n            for padding_key in field_lengths[0].keys():\n                max_value = max(x[padding_key] if padding_key in x else 0 for x in field_lengths)\n                padding_lengths[field_name][padding_key] = max_value\n        return {**padding_lengths}", "output": "Gets the maximum padding lengths from all ``Instances`` in this batch.  Each ``Instance``\n        has multiple ``Fields``, and each ``Field`` could have multiple things that need padding.\n        We look at all fields in all instances, and find the max values for each (field_name,\n        padding_key) pair, returning them in a dictionary.\n\n        This can then be used to convert this batch into arrays of consistent length, or to set\n        model parameters, etc.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gen_shell(opts, **kwargs):\n    '''\n    \n    '''\n    if kwargs['winrm']:\n        try:\n            import saltwinshell\n            shell = saltwinshell.Shell(opts, **kwargs)\n        except ImportError:\n            log.error('The saltwinshell library is not available')\n            sys.exit(salt.defaults.exitcodes.EX_GENERIC)\n    else:\n        shell = Shell(opts, **kwargs)\n    return shell", "output": "Return the correct shell interface for the target system", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_scala_docs(app):\n    \"\"\"\"\"\"\n    scala_path = app.builder.srcdir + '/../scala-package'\n    scala_doc_sources = 'find . -type f -name \"*.scala\" | egrep \\\"\\.\\/core|\\.\\/infer\\\" | egrep -v \\\"\\/javaapi\\\"  | egrep -v \\\"Suite\\\"'\n    scala_doc_classpath = ':'.join([\n        '`find native -name \"*.jar\" | grep \"target/lib/\" | tr \"\\\\n\" \":\" `',\n        '`find macros -name \"*.jar\" | tr \"\\\\n\" \":\" `',\n        '`find core -name \"*.jar\" | tr \"\\\\n\" \":\" `',\n        '`find infer -name \"*.jar\" | tr \"\\\\n\" \":\" `'\n    ])\n    # There are unresolvable errors on mxnet 1.2.x. We are ignoring those errors while aborting the ci on newer versions\n    scala_ignore_errors = '; exit 0' if any(v in _BUILD_VER for v in ['1.2.', '1.3.']) else ''\n    _run_cmd('cd {}; scaladoc `{}` -classpath {} -feature -deprecation {}'\n             .format(scala_path, scala_doc_sources, scala_doc_classpath, scala_ignore_errors))\n    dest_path = app.builder.outdir + '/api/scala/docs'\n    _run_cmd('rm -rf ' + dest_path)\n    _run_cmd('mkdir -p ' + dest_path)\n    # 'index' and 'package.html' do not exist in later versions of scala; delete these after upgrading scala>2.12.x\n    scaladocs = ['index', 'index.html', 'org', 'lib', 'index.js', 'package.html']\n    for doc_file in scaladocs:\n        _run_cmd('cd ' + scala_path + ' && mv -f ' + doc_file + ' ' + dest_path + '; exit 0')", "output": "build scala doc and then move the outdir", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_dbus_locale():\n    '''\n    \n    '''\n    bus = dbus.SystemBus()\n    localed = bus.get_object('org.freedesktop.locale1',\n                             '/org/freedesktop/locale1')\n    properties = dbus.Interface(localed, 'org.freedesktop.DBus.Properties')\n    system_locale = properties.Get('org.freedesktop.locale1', 'Locale')\n\n    ret = {}\n    for env_var in system_locale:\n        env_var = six.text_type(env_var)\n        match = re.match(r'^([A-Z_]+)=(.*)$', env_var)\n        if match:\n            ret[match.group(1)] = match.group(2).replace('\"', '')\n        else:\n            log.error('Odd locale parameter \"%s\" detected in dbus locale '\n                      'output. This should not happen. You should '\n                      'probably investigate what caused this.', env_var)\n\n    return ret", "output": "Get the 'System Locale' parameters from dbus", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _getText(nodelist):\n    '''\n    \n    '''\n    rc = []\n    for node in nodelist:\n        if node.nodeType == node.TEXT_NODE:\n            rc.append(node.data)\n    return ''.join(rc)", "output": "Simple function to return value from XML", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compute_etag(self) -> Optional[str]:\n        \"\"\"\n        \"\"\"\n        hasher = hashlib.sha1()\n        for part in self._write_buffer:\n            hasher.update(part)\n        return '\"%s\"' % hasher.hexdigest()", "output": "Computes the etag header to be used for this request.\n\n        By default uses a hash of the content written so far.\n\n        May be overridden to provide custom etag implementations,\n        or may return None to disable tornado's default etag support.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_report_metric_data(self, data):\n        \"\"\"\n        \n        \"\"\"\n        if data['type'] == 'FINAL':\n            self._handle_final_metric_data(data)\n        elif data['type'] == 'PERIODICAL':\n            if self.assessor is not None:\n                self._handle_intermediate_metric_data(data)\n            else:\n                pass\n        else:\n            raise ValueError('Data type not supported: {}'.format(data['type']))", "output": "data: a dict received from nni_manager, which contains:\n              - 'parameter_id': id of the trial\n              - 'value': metric value reported by nni.report_final_result()\n              - 'type': report type, support {'FINAL', 'PERIODICAL'}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def template_delete(call=None, kwargs=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The template_delete function must be called with -f or --function.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    name = kwargs.get('name', None)\n    template_id = kwargs.get('template_id', None)\n\n    if template_id:\n        if name:\n            log.warning(\n                'Both the \\'template_id\\' and \\'name\\' arguments were provided. '\n                '\\'template_id\\' will take precedence.'\n            )\n    elif name:\n        template_id = get_template_id(kwargs={'name': name})\n    else:\n        raise SaltCloudSystemExit(\n            'The template_delete function requires either a \\'name\\' or a \\'template_id\\' '\n            'to be provided.'\n        )\n\n    server, user, password = _get_xml_rpc()\n    auth = ':'.join([user, password])\n    response = server.one.template.delete(auth, int(template_id))\n\n    data = {\n        'action': 'template.delete',\n        'deleted': response[0],\n        'template_id': response[1],\n        'error_code': response[2],\n    }\n\n    return data", "output": "Deletes the given template from OpenNebula. Either a name or a template_id must\n    be supplied.\n\n    .. versionadded:: 2016.3.0\n\n    name\n        The name of the template to delete. Can be used instead of ``template_id``.\n\n    template_id\n        The ID of the template to delete. Can be used instead of ``name``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f template_delete opennebula name=my-template\n        salt-cloud --function template_delete opennebula template_id=5", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def text_changed(self):\r\n        \"\"\"\"\"\"\r\n        # Save text as bytes, if it was initially bytes\r\n        if self.is_binary:\r\n            self.text = to_binary_string(self.edit.toPlainText(), 'utf8')\r\n        else:\r\n            self.text = to_text_string(self.edit.toPlainText())\r\n        if self.btn_save_and_close:\r\n            self.btn_save_and_close.setEnabled(True)\r\n            self.btn_save_and_close.setAutoDefault(True)\r\n            self.btn_save_and_close.setDefault(True)", "output": "Text has changed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def param_show(param=None):\n    '''\n    \n    '''\n    ret = _run_varnishadm('param.show', [param])\n    if ret['retcode']:\n        return False\n    else:\n        result = {}\n        for line in ret['stdout'].split('\\n'):\n            m = re.search(r'^(\\w+)\\s+(.*)$', line)\n            result[m.group(1)] = m.group(2)\n            if param:\n                # When we ask to varnishadm for a specific param, it gives full\n                # info on what that parameter is, so we just process the first\n                # line and we get out of the loop\n                break\n        return result", "output": "Show params of varnish cache\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' varnish.param_show param", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_whitespace_split_regex(text):\n    '''\n    \n\n    '''\n    def __build_parts(text):\n        lexer = shlex.shlex(text)\n        lexer.whitespace_split = True\n        lexer.commenters = ''\n        if r\"'\\\"\" in text:\n            lexer.quotes = ''\n        elif '\\'' in text:\n            lexer.quotes = '\"'\n        elif '\"' in text:\n            lexer.quotes = '\\''\n        return list(lexer)\n\n    regex = r''\n    for line in text.splitlines():\n        parts = [re.escape(s) for s in __build_parts(line)]\n        regex += r'(?:[\\s]+)?{0}(?:[\\s]+)?'.format(r'(?:[\\s]+)?'.join(parts))\n    return r'(?m)^{0}$'.format(regex)", "output": "Create a regular expression at runtime which should match ignoring the\n    addition or deletion of white space or line breaks, unless between commas\n\n    Example:\n\n    .. code-block:: python\n\n        >>> import re\n        >>> import salt.utils.stringutils\n        >>> regex = salt.utils.stringutils.build_whitespace_split_regex(\n        ...     \"\"\"if [ -z \"$debian_chroot\" ] && [ -r /etc/debian_chroot ]; then\"\"\"\n        ... )\n\n        >>> regex\n        '(?:[\\\\s]+)?if(?:[\\\\s]+)?\\\\[(?:[\\\\s]+)?\\\\-z(?:[\\\\s]+)?\\\\\"\\\\$debian'\n        '\\\\_chroot\\\\\"(?:[\\\\s]+)?\\\\](?:[\\\\s]+)?\\\\&\\\\&(?:[\\\\s]+)?\\\\[(?:[\\\\s]+)?'\n        '\\\\-r(?:[\\\\s]+)?\\\\/etc\\\\/debian\\\\_chroot(?:[\\\\s]+)?\\\\]\\\\;(?:[\\\\s]+)?'\n        'then(?:[\\\\s]+)?'\n        >>> re.search(\n        ...     regex,\n        ...     \"\"\"if [ -z \"$debian_chroot\" ] && [ -r /etc/debian_chroot ]; then\"\"\"\n        ... )\n\n        <_sre.SRE_Match object at 0xb70639c0>\n        >>>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dump(self, out_dir=''):\n        \"\"\"\"\"\"\n        with self.no_unpicklable_properties():\n            self.job_file = os.path.join(out_dir, 'job-instance.pickle')\n            if self.__module__ == '__main__':\n                d = pickle.dumps(self)\n                module_name = os.path.basename(sys.argv[0]).rsplit('.', 1)[0]\n                d = d.replace('(c__main__', \"(c\" + module_name)\n                open(self.job_file, \"w\").write(d)\n            else:\n                pickle.dump(self, open(self.job_file, \"w\"))", "output": "Dump instance to file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def loadLabeledPoints(sc, path, minPartitions=None):\n        \"\"\"\n        \n        \"\"\"\n        minPartitions = minPartitions or min(sc.defaultParallelism, 2)\n        return callMLlibFunc(\"loadLabeledPoints\", sc, path, minPartitions)", "output": "Load labeled points saved using RDD.saveAsTextFile.\n\n        :param sc: Spark context\n        :param path: file or directory path in any Hadoop-supported file\n                     system URI\n        :param minPartitions: min number of partitions\n        @return: labeled data stored as an RDD of LabeledPoint\n\n        >>> from tempfile import NamedTemporaryFile\n        >>> from pyspark.mllib.util import MLUtils\n        >>> from pyspark.mllib.regression import LabeledPoint\n        >>> examples = [LabeledPoint(1.1, Vectors.sparse(3, [(0, -1.23), (2, 4.56e-7)])),\n        ...             LabeledPoint(0.0, Vectors.dense([1.01, 2.02, 3.03]))]\n        >>> tempFile = NamedTemporaryFile(delete=True)\n        >>> tempFile.close()\n        >>> sc.parallelize(examples, 1).saveAsTextFile(tempFile.name)\n        >>> MLUtils.loadLabeledPoints(sc, tempFile.name).collect()\n        [LabeledPoint(1.1, (3,[0,2],[-1.23,4.56e-07])), LabeledPoint(0.0, [1.01,2.02,3.03])]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_week_day(base_date, weekday):\n    \"\"\"\n    \n    \"\"\"\n    day_of_week = base_date.weekday()\n    end_of_this_week = base_date + timedelta(days=6 - day_of_week)\n    day = end_of_this_week + timedelta(days=1)\n    while day.weekday() != weekday:\n        day = day + timedelta(days=1)\n    return day", "output": "Finds next weekday", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_output_for_errors(data, command, **kwargs):\n    '''\n    \n    '''\n    if re.search('% Invalid', data):\n        raise CommandExecutionError({\n            'rejected_input': command,\n            'message': 'CLI excution error',\n            'code': '400',\n            'cli_error': data.lstrip(),\n        })\n    if kwargs.get('error_pattern') is not None:\n        for re_line in kwargs.get('error_pattern'):\n            if re.search(re_line, data):\n                raise CommandExecutionError({\n                    'rejected_input': command,\n                    'message': 'CLI excution error',\n                    'code': '400',\n                    'cli_error': data.lstrip(),\n                })", "output": "Helper method to parse command output for error information", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_wikitext103_l16k_memory_v0():\n  \"\"\"\"\"\"\n  hparams = transformer_wikitext103_l4k_memory_v0()\n\n  hparams.max_length = 16384\n  hparams.split_targets_chunk_length = 64\n  hparams.split_targets_max_chunks = int(\n      hparams.max_length / hparams.split_targets_chunk_length)\n\n  # The hparams specify batch size *before* chunking, but we want to have a\n  # consistent 4K batch size *after* chunking to fully utilize the hardware.\n  target_tokens_per_batch = 4096\n  hparams.batch_size = int(target_tokens_per_batch * (\n      hparams.max_length / hparams.split_targets_chunk_length))\n\n  hparams.max_relative_position = 2 * hparams.split_targets_chunk_length\n\n  return hparams", "output": "HParams for training languagemodel_wikitext103_l16k with memory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parametrized_bottleneck(x, hparams):\n  \"\"\"\"\"\"\n  if hparams.bottleneck_kind == \"tanh_discrete\":\n    d, _ = tanh_discrete_bottleneck(\n        x, hparams.bottleneck_bits, hparams.bottleneck_noise * 0.5,\n        hparams.discretize_warmup_steps, hparams.mode)\n    return d, 0.0\n  if hparams.bottleneck_kind == \"isemhash\":\n    return isemhash_bottleneck(\n        x, hparams.bottleneck_bits, hparams.bottleneck_noise * 0.5,\n        hparams.discretize_warmup_steps, hparams.mode,\n        hparams.isemhash_noise_dev, hparams.isemhash_mix_prob)\n  if hparams.bottleneck_kind == \"vq\":\n    return vq_discrete_bottleneck(x, hparams.bottleneck_bits, hparams.vq_beta,\n                                  hparams.vq_decay, hparams.vq_epsilon)\n  if hparams.bottleneck_kind == \"em\":\n    return vq_discrete_bottleneck(\n        x,\n        hparams.bottleneck_bits,\n        hparams.vq_beta,\n        hparams.vq_decay,\n        hparams.vq_epsilon,\n        soft_em=True,\n        num_samples=hparams.vq_num_samples)\n  if hparams.bottleneck_kind == \"gumbel_softmax\":\n    return gumbel_softmax_discrete_bottleneck(\n        x,\n        hparams.bottleneck_bits,\n        hparams.vq_beta,\n        hparams.vq_decay,\n        hparams.vq_epsilon,\n        hparams.temperature_warmup_steps,\n        hard=False,\n        summary=True)\n\n  raise ValueError(\n      \"Unsupported hparams.bottleneck_kind %s\" % hparams.bottleneck_kind)", "output": "Meta-function calling all the above bottlenecks with hparams.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def network_interface_list_effective_network_security_groups(name, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        nic = netconn.network_interfaces.list_effective_network_security_groups(\n            network_interface_name=name,\n            resource_group_name=resource_group\n        )\n        nic.wait()\n        groups = nic.result()\n        groups = groups.as_dict()\n        result = groups['value']\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Get all network security groups applied to a specific network interface.\n\n    :param name: The name of the network interface to query.\n\n    :param resource_group: The resource group name assigned to the\n        network interface.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.network_interface_list_effective_network_security_groups test-iface0 testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def request_snapshot(self, mode, priority=19, **kwargs):\n        '''\n        \n        '''\n        if mode not in self.MODE:\n            raise InspectorSnapshotException(\"Unknown mode: '{0}'\".format(mode))\n\n        if is_alive(self.pidfile):\n            raise CommandExecutionError('Inspection already in progress.')\n\n        self._prepare_full_scan(**kwargs)\n\n        os.system(\"nice -{0} python {1} {2} {3} {4} & > /dev/null\".format(\n            priority, __file__, os.path.dirname(self.pidfile), os.path.dirname(self.dbfile), mode))", "output": "Take a snapshot of the system.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_classes(self, items):\n        \"\"\n        classes = super().generate_classes([o[1] for o in items])\n        classes = ['background'] + list(classes)\n        return classes", "output": "Generate classes from unique `items` and add `background`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_env():\n    ''' \n    '''\n    if getattr(sys, 'frozen', False) and hasattr(sys, '_MEIPASS'):\n        # PyInstaller uses _MEIPASS and only works with jinja2.FileSystemLoader\n        templates_path = join(sys._MEIPASS, 'bokeh', 'core', '_templates')\n    else:\n        # Non-frozen Python and cx_Freeze can use __file__ directly\n        templates_path = join(dirname(__file__), '_templates')\n\n    return Environment(loader=FileSystemLoader(templates_path))", "output": "Get the correct Jinja2 Environment, also for frozen scripts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def upgrade(bin_env=None,\n            user=None,\n            cwd=None,\n            use_vt=False):\n    '''\n    \n    '''\n    ret = {'changes': {},\n           'result': True,\n           'comment': '',\n           }\n    cmd = _get_pip_bin(bin_env)\n    cmd.extend(['install', '-U'])\n\n    old = list_(bin_env=bin_env, user=user, cwd=cwd)\n\n    cmd_kwargs = dict(cwd=cwd, use_vt=use_vt)\n    if bin_env and os.path.isdir(bin_env):\n        cmd_kwargs['env'] = {'VIRTUAL_ENV': bin_env}\n    errors = False\n    for pkg in list_upgrades(bin_env=bin_env, user=user, cwd=cwd):\n        if pkg == 'salt':\n            if salt.utils.platform.is_windows():\n                continue\n        result = __salt__['cmd.run_all'](cmd + [pkg], **cmd_kwargs)\n        if result['retcode'] != 0:\n            errors = True\n        if 'stderr' in result:\n            ret['comment'] += result['stderr']\n    if errors:\n        ret['result'] = False\n\n    _clear_context(bin_env)\n    new = list_(bin_env=bin_env, user=user, cwd=cwd)\n\n    ret['changes'] = salt.utils.data.compare_dicts(old, new)\n\n    return ret", "output": ".. versionadded:: 2015.5.0\n\n    Upgrades outdated pip packages.\n\n    .. note::\n        On Windows you can't update salt from pip using salt, so salt will be\n        skipped\n\n    Returns a dict containing the changes.\n\n        {'<package>':  {'old': '<old-version>',\n                        'new': '<new-version>'}}\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pip.upgrade", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _load_data(self,\n                   equities,\n                   futures,\n                   exchanges,\n                   root_symbols,\n                   equity_supplementary_mappings):\n        \"\"\"\n        \n        \"\"\"\n        # Set named identifier columns as indices, if provided.\n        _normalize_index_columns_in_place(\n            equities=equities,\n            equity_supplementary_mappings=equity_supplementary_mappings,\n            futures=futures,\n            exchanges=exchanges,\n            root_symbols=root_symbols,\n        )\n\n        futures_output = self._normalize_futures(futures)\n\n        equity_supplementary_mappings_output = (\n            self._normalize_equity_supplementary_mappings(\n                equity_supplementary_mappings,\n            )\n        )\n\n        exchanges_output = _generate_output_dataframe(\n            data_subset=exchanges,\n            defaults=_exchanges_defaults,\n        )\n\n        equities_output, equities_mappings = self._normalize_equities(\n            equities,\n            exchanges_output,\n        )\n\n        root_symbols_output = _generate_output_dataframe(\n            data_subset=root_symbols,\n            defaults=_root_symbols_defaults,\n        )\n\n        return AssetData(\n            equities=equities_output,\n            equities_mappings=equities_mappings,\n            futures=futures_output,\n            exchanges=exchanges_output,\n            root_symbols=root_symbols_output,\n            equity_supplementary_mappings=equity_supplementary_mappings_output,\n        )", "output": "Returns a standard set of pandas.DataFrames:\n        equities, futures, exchanges, root_symbols", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _put_to_history(self, command_script):\n        \"\"\"\"\"\"\n        history_file_name = self._get_history_file_name()\n        if os.path.isfile(history_file_name):\n            with open(history_file_name, 'a') as history:\n                entry = self._get_history_line(command_script)\n                if six.PY2:\n                    history.write(entry.encode('utf-8'))\n                else:\n                    history.write(entry)", "output": "Puts command script to shell history.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _fetch(self):\n        '''\n        \n        '''\n        origin = self.repo.remotes[0]\n        try:\n            fetch_results = origin.fetch()\n        except AssertionError:\n            fetch_results = origin.fetch()\n\n        new_objs = False\n        for fetchinfo in fetch_results:\n            if fetchinfo.old_commit is not None:\n                log.debug(\n                    '%s has updated \\'%s\\' for remote \\'%s\\' '\n                    'from %s to %s',\n                    self.role,\n                    fetchinfo.name,\n                    self.id,\n                    fetchinfo.old_commit.hexsha[:7],\n                    fetchinfo.commit.hexsha[:7]\n                )\n                new_objs = True\n            elif fetchinfo.flags in (fetchinfo.NEW_TAG,\n                                     fetchinfo.NEW_HEAD):\n                log.debug(\n                    '%s has fetched new %s \\'%s\\' for remote \\'%s\\'',\n                    self.role,\n                    'tag' if fetchinfo.flags == fetchinfo.NEW_TAG else 'head',\n                    fetchinfo.name,\n                    self.id\n                )\n                new_objs = True\n\n        cleaned = self.clean_stale_refs()\n        return True if (new_objs or cleaned) else None", "output": "Fetch the repo. If the local copy was updated, return True. If the\n        local copy was already up-to-date, return False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode(self, inputs, states=None, valid_length=None):\n        \"\"\"\n        \"\"\"\n        return self.encoder(self.src_embed(inputs), states, valid_length)", "output": "Encode the input sequence.\n\n        Parameters\n        ----------\n        inputs : NDArray\n        states : list of NDArrays or None, default None\n        valid_length : NDArray or None, default None\n\n        Returns\n        -------\n        outputs : list\n            Outputs of the encoder.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def runner(name, **kwargs):\n    '''\n    \n    '''\n    try:\n        jid = __orchestration_jid__\n    except NameError:\n        log.debug(\n            'Unable to fire args event due to missing __orchestration_jid__'\n        )\n        jid = None\n\n    if __opts__.get('test', False):\n        ret = {\n            'name': name,\n            'result': None,\n            'changes': {},\n            'comment': \"Runner function '{0}' would be executed.\".format(name)\n        }\n        return ret\n\n    out = __salt__['saltutil.runner'](name,\n                                      __orchestration_jid__=jid,\n                                      __env__=__env__,\n                                      full_return=True,\n                                      **kwargs)\n\n    if kwargs.get('asynchronous'):\n        out['return'] = out.copy()\n        out['success'] = 'jid' in out and 'tag' in out\n\n    runner_return = out.get('return')\n    if isinstance(runner_return, dict) and 'Error' in runner_return:\n        out['success'] = False\n\n    success = out.get('success', True)\n    ret = {'name': name,\n           'changes': {'return': runner_return},\n           'result': success}\n    ret['comment'] = \"Runner function '{0}' {1}.\".format(\n        name,\n        'executed' if success else 'failed',\n    )\n\n    ret['__orchestration__'] = True\n    if 'jid' in out:\n        ret['__jid__'] = out['jid']\n\n    return ret", "output": "Execute a runner module on the master\n\n    .. versionadded:: 2014.7.0\n\n    name\n        The name of the function to run\n\n    kwargs\n        Any keyword arguments to pass to the runner function\n\n    asynchronous\n        Run the salt command but don't wait for a reply.\n\n        .. versionadded:: neon\n\n\n    .. code-block:: yaml\n\n         run-manage-up:\n          salt.runner:\n            - name: manage.up", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clip_by_global_norm_per_ctx(self, max_norm=1.0, param_names=None):\n        \"\"\"\n        \"\"\"\n        assert self.binded and self.params_initialized and self.optimizer_initialized\n        num_ctx = len(self._exec_group.grad_arrays[0])\n        grad_array_per_ctx = [[] for i in range(num_ctx)]\n        assert(param_names is not None)\n        for param_name in param_names:\n            param_idx = self._exec_group.param_names.index(param_name)\n            grad_val = self._exec_group.grad_arrays[param_idx]\n            assert(len(grad_val) == num_ctx)\n            for i in range(num_ctx):\n                grad_array_per_ctx[i].append(grad_val[i])\n        norm_vals = []\n        for i in range(num_ctx):\n            mx.gluon.utils.clip_global_norm(grad_array_per_ctx[i], max_norm)", "output": "Clips gradient norm.\n\n        The norm is computed over all gradients together, as if they were\n         concatenated into a single vector. Gradients are modified in-place.\n\n        The method is first used in\n         `[ICML2013] On the difficulty of training recurrent neural networks`\n\n        Note that the gradients are concatenated per context in this implementation.\n\n        Examples\n        --------\n        An example of using clip_grad_norm to clip the gradient before updating the parameters::\n            >>> #Get the gradient via back-propagation\n            >>> net.forward_backward(data_batch=data_batch)\n            >>> norm_val = net.clip_by_global_norm(max_norm=2.0, param_names='w0')\n            >>> net.update()", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fire_event(key, msg, tag, sock_dir, args=None, transport='zeromq'):\n    '''\n    \n    '''\n    event = salt.utils.event.get_event(\n        'master',\n        sock_dir,\n        transport,\n        listen=False)\n\n    try:\n        event.fire_event(msg, tag)\n    except ValueError:\n        # We're using at least a 0.17.x version of salt\n        if isinstance(args, dict):\n            args[key] = msg\n        else:\n            args = {key: msg}\n        event.fire_event(args, tag)\n\n    # https://github.com/zeromq/pyzmq/issues/173#issuecomment-4037083\n    # Assertion failed: get_load () == 0 (poller_base.cpp:32)\n    time.sleep(0.025)", "output": "Fire deploy action", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def competitions_submissions_upload(self, file, guid, content_length, last_modified_date_utc, **kwargs):  # noqa: E501\n        \"\"\"\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.competitions_submissions_upload_with_http_info(file, guid, content_length, last_modified_date_utc, **kwargs)  # noqa: E501\n        else:\n            (data) = self.competitions_submissions_upload_with_http_info(file, guid, content_length, last_modified_date_utc, **kwargs)  # noqa: E501\n            return data", "output": "Upload competition submission file  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.competitions_submissions_upload(file, guid, content_length, last_modified_date_utc, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param file file: Competition submission file (required)\n        :param str guid: Location where submission should be uploaded (required)\n        :param int content_length: Content length of file in bytes (required)\n        :param int last_modified_date_utc: Last modified date of file in milliseconds since epoch in UTC (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_windows_dll_path():\n    \"\"\"\n    \n    \"\"\"\n\n    lib_path = os.path.dirname(os.path.abspath(_pylambda_worker.__file__))\n    lib_path = os.path.abspath(os.path.join(lib_path, os.pardir))\n\n    def errcheck_bool(result, func, args):\n        if not result:\n            last_error = ctypes.get_last_error()\n            if last_error != 0:\n                raise ctypes.WinError(last_error)\n            else:\n                raise OSError\n        return args\n\n    # Also need to set the dll loading directory to the main\n    # folder so windows attempts to load all DLLs from this\n    # directory.\n    import ctypes.wintypes as wintypes\n\n    try:\n        kernel32 = ctypes.WinDLL('kernel32', use_last_error=True)\n        kernel32.SetDllDirectoryW.errcheck = errcheck_bool\n        kernel32.SetDllDirectoryW.argtypes = (wintypes.LPCWSTR,)\n        kernel32.SetDllDirectoryW(lib_path)\n    except Exception as e:\n        logging.getLogger(__name__).warning(\n            \"Error setting DLL load orders: %s (things should still work).\" % str(e))", "output": "Sets the dll load path so that things are resolved correctly.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_process_mapping():\n    \"\"\"\n    \"\"\"\n    output = subprocess.check_output([\n        'ps', '-ww', '-o', 'pid=', '-o', 'ppid=', '-o', 'args=',\n    ])\n    if not isinstance(output, str):\n        output = output.decode(sys.stdout.encoding)\n    processes = {}\n    for line in output.split('\\n'):\n        try:\n            pid, ppid, args = line.strip().split(None, 2)\n        except ValueError:\n            continue\n        processes[pid] = Process(\n            args=tuple(shlex.split(args)), pid=pid, ppid=ppid,\n        )\n    return processes", "output": "Try to look up the process tree via the output of `ps`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_runtime_class(self, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_runtime_class_with_http_info(body, **kwargs)\n        else:\n            (data) = self.create_runtime_class_with_http_info(body, **kwargs)\n            return data", "output": "create a RuntimeClass\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_runtime_class(body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param V1beta1RuntimeClass body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1beta1RuntimeClass\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hexists(key, field, host=None, port=None, db=None, password=None):\n    '''\n    \n    '''\n    server = _connect(host, port, db, password)\n    return server.hexists(key, field)", "output": "Determine if a hash fields exists.\n\n    .. versionadded:: 2017.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.hexists foo_hash bar_field", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_cam(imggrad, conv_out):\n    \"\"\"\"\"\"\n    weights = np.mean(imggrad, axis=(1, 2))\n    cam = np.ones(conv_out.shape[1:], dtype=np.float32)\n    for i, w in enumerate(weights):\n        cam += w * conv_out[i, :, :]\n    cam = cv2.resize(cam, (imggrad.shape[1], imggrad.shape[2]))\n    cam = np.maximum(cam, 0)\n    cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam)) \n    cam = np.uint8(cam * 255)\n    return cam", "output": "Compute CAM. Refer section 3 of https://arxiv.org/abs/1610.02391 for details", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shell_sort(arr):\n    ''' \n    '''\n    n = len(arr)\n    # Initialize size of the gap\n    gap = n//2\n    \n    while gap > 0:\n        y_index = gap\n        while y_index < len(arr):\n            y = arr[y_index]\n            x_index = y_index - gap\n            while x_index >= 0 and y < arr[x_index]:\n                arr[x_index + gap] = arr[x_index]\n                x_index = x_index - gap\n            arr[x_index + gap] = y\n            y_index = y_index + 1\n        gap = gap//2\n        \n    return arr", "output": "Shell Sort\n        Complexity: O(n^2)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_attached_message(self, key, message_type, tags=None, required=False):\n    \"\"\"\n    \"\"\"\n    attached_bytes = self._get_attached_bytes(key, tags)\n    if attached_bytes is None:\n      if required:\n        raise KeyError(\"No attached message for key '%s' in graph version %s \"\n                       \"of Hub Module\" % (key, sorted(tags or [])))\n      else:\n        return None\n    message = message_type()\n    message.ParseFromString(attached_bytes)\n    return message", "output": "Returns the message attached to the module under the given key, or None.\n\n    Module publishers can attach protocol messages to modules at creation time\n    to provide module consumers with additional information, e.g., on module\n    usage or provenance (see see hub.attach_message()). A typical use would be\n    to store a small set of named values with modules of a certain type so\n    that a support library for consumers of such modules can be parametric\n    in those values.\n\n    This method can also be called on a Module instantiated from a ModuleSpec,\n    then `tags` are set to those used in module instatiation.\n\n    Args:\n      key: A string with the key of an attached message.\n      message_type: A concrete protocol message class (*not* object) used\n        to parse the attached message from its serialized representation.\n        The message type for a particular key must be advertised with the key.\n      tags: Optional set of strings, specifying the graph variant from which\n        to read the attached message.\n      required: An optional boolean. Setting it true changes the effect of\n        an unknown `key` from returning None to raising a KeyError with text\n        about attached messages.\n\n    Returns:\n      An instance of `message_type` with the message contents attached to the\n      module, or `None` if `key` is unknown and `required` is False.\n\n    Raises:\n      KeyError: if `key` is unknown and `required` is True.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def walk_commands(self):\n        \"\"\"\"\"\"\n        from .core import GroupMixin\n        for command in self.__cog_commands__:\n            if command.parent is None:\n                yield command\n                if isinstance(command, GroupMixin):\n                    yield from command.walk_commands()", "output": "An iterator that recursively walks through this cog's commands and subcommands.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def equities_sids_for_country_code(self, country_code):\n        \"\"\"\n        \"\"\"\n        sids = self._compute_asset_lifetimes([country_code]).sid\n        return tuple(sids.tolist())", "output": "Return all of the sids for a given country.\n\n        Parameters\n        ----------\n        country_code : str\n            An ISO 3166 alpha-2 country code.\n\n        Returns\n        -------\n        tuple[int]\n            The sids whose exchanges are in this country.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def emit_save_figure(self):\n        \"\"\"\n        \n        \"\"\"\n        self.sig_save_figure.emit(self.canvas.fig, self.canvas.fmt)", "output": "Emit a signal when the toolbutton to save the figure is clicked.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_meta_fields():\n    '''\n    \n    '''\n    ret = {}\n    status, result = _query(action='meta', command='fields')\n    root = ET.fromstring(result)\n    fields = root.getchildren()\n    for field in fields:\n        field_id = None\n        field_ret = {'name': field.text}\n        for item in field.items():\n            field_ret[item[0]] = item[1]\n            if item[0] == 'id':\n                field_id = item[1]\n        ret[field_id] = field_ret\n    return ret", "output": "Show all meta data fields for this company.\n\n    CLI Example:\n\n        salt myminion bamboohr.list_meta_fields", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def route(\n        self,\n        uri,\n        methods=frozenset({\"GET\"}),\n        host=None,\n        strict_slashes=None,\n        stream=False,\n        version=None,\n        name=None,\n    ):\n        \"\"\"\n        \"\"\"\n\n        # Fix case where the user did not prefix the URL with a /\n        # and will probably get confused as to why it's not working\n        if not uri.startswith(\"/\"):\n            uri = \"/\" + uri\n\n        if stream:\n            self.is_request_stream = True\n\n        if strict_slashes is None:\n            strict_slashes = self.strict_slashes\n\n        def response(handler):\n            args = list(signature(handler).parameters.keys())\n\n            if not args:\n                raise ValueError(\n                    \"Required parameter `request` missing \"\n                    \"in the {0}() route?\".format(handler.__name__)\n                )\n\n            if stream:\n                handler.is_stream = stream\n\n            self.router.add(\n                uri=uri,\n                methods=methods,\n                handler=handler,\n                host=host,\n                strict_slashes=strict_slashes,\n                version=version,\n                name=name,\n            )\n            return handler\n\n        return response", "output": "Decorate a function to be registered as a route\n\n        :param uri: path of the URL\n        :param methods: list or tuple of methods allowed\n        :param host:\n        :param strict_slashes:\n        :param stream:\n        :param version:\n        :param name: user defined route name for url_for\n        :return: decorated function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_epoch_begin(self, **kwargs):\n        \"\"\n        self.metrics = {name:0. for name in self.names}\n        self.nums = 0", "output": "Initialize the metrics for this epoch.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put(self, rank, val):\n        \"\"\"\n        \n        \"\"\"\n        idx = bisect.bisect(self.ranks, rank)\n        self.ranks.insert(idx, rank)\n        self.data.insert(idx, val)", "output": "Args:\n            rank(int): rank of th element. All elements must have different ranks.\n            val: an object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save(df, path, data_paths):\n        \"\"\"\n        \n        \"\"\"\n        size = _reset_df_and_get_size(df)\n        buffer = defaultdict(list)\n\n        with get_tqdm(total=size) as pbar:\n            for dp in df:\n                assert len(dp) == len(data_paths), \"Datapoint has {} components!\".format(len(dp))\n                for k, el in zip(data_paths, dp):\n                    buffer[k].append(el)\n                pbar.update()\n\n        with h5py.File(path, 'w') as hf, get_tqdm(total=len(data_paths)) as pbar:\n            for data_path in data_paths:\n                hf.create_dataset(data_path, data=buffer[data_path])\n                pbar.update()", "output": "Args:\n            df (DataFlow): the DataFlow to serialize.\n            path (str): output hdf5 file.\n            data_paths (list[str]): list of h5 paths. It should have the same\n                length as each datapoint, and each path should correspond to one\n                component of the datapoint.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode(data, mime_type='', charset='utf-8', base64=True):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(data, six.text_type):\n        data = data.encode(charset)\n    else:\n        charset = None\n    if base64:\n        data = utils.text(b64encode(data))\n    else:\n        data = utils.text(quote(data))\n\n    result = ['data:', ]\n    if mime_type:\n        result.append(mime_type)\n    if charset:\n        result.append(';charset=')\n        result.append(charset)\n    if base64:\n        result.append(';base64')\n    result.append(',')\n    result.append(data)\n\n    return ''.join(result)", "output": "Encode data to DataURL", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_info_dir():\n  \"\"\"\n  \"\"\"\n  path = os.path.join(tempfile.gettempdir(), \".tensorboard-info\")\n  try:\n    os.makedirs(path)\n  except OSError as e:\n    if e.errno == errno.EEXIST and os.path.isdir(path):\n      pass\n    else:\n      raise\n  else:\n    os.chmod(path, 0o777)\n  return path", "output": "Get path to directory in which to store info files.\n\n  The directory returned by this function is \"owned\" by this module. If\n  the contents of the directory are modified other than via the public\n  functions of this module, subsequent behavior is undefined.\n\n  The directory will be created if it does not exist.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def normalize(self, stats:Collection[Tensor]=None, do_x:bool=True, do_y:bool=False)->None:\n        \"\"\n        if getattr(self,'norm',False): raise Exception('Can not call normalize twice')\n        if stats is None: self.stats = self.batch_stats()\n        else:             self.stats = stats\n        self.norm,self.denorm = normalize_funcs(*self.stats, do_x=do_x, do_y=do_y)\n        self.add_tfm(self.norm)\n        return self", "output": "Add normalize transform using `stats` (defaults to `DataBunch.batch_stats`)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ssl_wrap_socket(\n    socket: socket.socket,\n    ssl_options: Union[Dict[str, Any], ssl.SSLContext],\n    server_hostname: str = None,\n    **kwargs: Any\n) -> ssl.SSLSocket:\n    \"\"\"\n    \"\"\"\n    context = ssl_options_to_context(ssl_options)\n    if ssl.HAS_SNI:\n        # In python 3.4, wrap_socket only accepts the server_hostname\n        # argument if HAS_SNI is true.\n        # TODO: add a unittest (python added server-side SNI support in 3.4)\n        # In the meantime it can be manually tested with\n        # python3 -m tornado.httpclient https://sni.velox.ch\n        return context.wrap_socket(socket, server_hostname=server_hostname, **kwargs)\n    else:\n        return context.wrap_socket(socket, **kwargs)", "output": "Returns an ``ssl.SSLSocket`` wrapping the given socket.\n\n    ``ssl_options`` may be either an `ssl.SSLContext` object or a\n    dictionary (as accepted by `ssl_options_to_context`).  Additional\n    keyword arguments are passed to ``wrap_socket`` (either the\n    `~ssl.SSLContext` method or the `ssl` module function as\n    appropriate).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def consume(self, chars, min=0, max=-1):\n        \"\"\"\n        \n        \"\"\"\n        return self._src.consume(chars=chars, min=min, max=max)", "output": "Consume chars until min/max is satisfied is valid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def refresh(self, new_path=None, force_current=False):\r\n        \"\"\"\"\"\"\r\n        if new_path is None:\r\n            new_path = getcwd_or_home()\r\n        if force_current:\r\n            index = self.set_current_folder(new_path)\r\n            self.expand(index)\r\n            self.setCurrentIndex(index)\r\n        self.set_previous_enabled.emit(\r\n                             self.histindex is not None and self.histindex > 0)\r\n        self.set_next_enabled.emit(self.histindex is not None and \\\r\n                                   self.histindex < len(self.history)-1)\r\n        # Disable the view of .spyproject. \r\n        self.filter_directories()", "output": "Refresh widget\r\n        force=False: won't refresh widget if path has not changed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _lookup_field(self, path):\n        \"\"\" \n        \"\"\"\n        if path.startswith('^'):\n            path = path[1:]\n            context = self.document if path.startswith('^') \\\n                else self.root_document\n        else:\n            context = self.document\n\n        parts = path.split('.')\n        for part in parts:\n            if part not in context:\n                return None, None\n            context = context.get(part)\n\n        return parts[-1], context", "output": "Searches for a field as defined by path. This method is used by the\n            ``dependency`` evaluation logic.\n\n        :param path: Path elements are separated by a ``.``. A leading ``^``\n                     indicates that the path relates to the document root,\n                     otherwise it relates to the currently evaluated document,\n                     which is possibly a subdocument.\n                     The sequence ``^^`` at the start will be interpreted as a\n                     literal ``^``.\n        :type path: :class:`str`\n        :returns: Either the found field name and its value or :obj:`None` for\n                  both.\n        :rtype: A two-value :class:`tuple`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kill_command(pid):\n    ''''''\n    if sys.platform == 'win32':\n        process = psutil.Process(pid=pid)\n        process.send_signal(signal.CTRL_BREAK_EVENT)\n    else:\n        cmds = ['kill', str(pid)]\n        call(cmds)", "output": "kill command", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _load_lib():\n    \"\"\"\"\"\"\n    lib_paths = find_lib_path()\n    if not lib_paths:\n        return None\n    try:\n        pathBackup = os.environ['PATH'].split(os.pathsep)\n    except KeyError:\n        pathBackup = []\n    lib_success = False\n    os_error_list = []\n    for lib_path in lib_paths:\n        try:\n            # needed when the lib is linked with non-system-available dependencies\n            os.environ['PATH'] = os.pathsep.join(pathBackup + [os.path.dirname(lib_path)])\n            lib = ctypes.cdll.LoadLibrary(lib_path)\n            lib_success = True\n        except OSError as e:\n            os_error_list.append(str(e))\n            continue\n        finally:\n            os.environ['PATH'] = os.pathsep.join(pathBackup)\n    if not lib_success:\n        libname = os.path.basename(lib_paths[0])\n        raise XGBoostError(\n            'XGBoost Library ({}) could not be loaded.\\n'.format(libname) +\n            'Likely causes:\\n' +\n            '  * OpenMP runtime is not installed ' +\n            '(vcomp140.dll or libgomp-1.dll for Windows, ' +\n            'libgomp.so for UNIX-like OSes)\\n' +\n            '  * You are running 32-bit Python on a 64-bit OS\\n' +\n            'Error message(s): {}\\n'.format(os_error_list))\n    lib.XGBGetLastError.restype = ctypes.c_char_p\n    lib.callback = _get_log_callback_func()\n    if lib.XGBRegisterLogCallback(lib.callback) != 0:\n        raise XGBoostError(lib.XGBGetLastError())\n    return lib", "output": "Load xgboost Library.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _fetch_layer_uri(self, layer):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            layer_version_response = self.lambda_client.get_layer_version(LayerName=layer.layer_arn,\n                                                                          VersionNumber=layer.version)\n        except NoCredentialsError:\n            raise CredentialsRequired(\"Layers require credentials to download the layers locally.\")\n        except ClientError as e:\n            error_code = e.response.get('Error').get('Code')\n            error_exc = {\n                'AccessDeniedException': CredentialsRequired(\n                    \"Credentials provided are missing lambda:Getlayerversion policy that is needed to download the \"\n                    \"layer or you do not have permission to download the layer\"),\n                'ResourceNotFoundException': ResourceNotFound(\"{} was not found.\".format(layer.arn))\n            }\n\n            if error_code in error_exc:\n                raise error_exc[error_code]\n\n            # If it was not 'AccessDeniedException' or 'ResourceNotFoundException' re-raise\n            raise e\n\n        return layer_version_response.get(\"Content\").get(\"Location\")", "output": "Fetch the Layer Uri based on the LayerVersion Arn\n\n        Parameters\n        ----------\n        layer samcli.commands.local.lib.provider.LayerVersion\n            LayerVersion to fetch\n\n        Returns\n        -------\n        str\n            The Uri to download the LayerVersion Content from\n\n        Raises\n        ------\n        samcli.commands.local.cli_common.user_exceptions.NoCredentialsError\n            When the Credentials given are not sufficient to call AWS Lambda", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __execute_cmd(command):\n    '''\n    \n    '''\n    cmd = __salt__['cmd.run_all']('racadm {0}'.format(command))\n\n    if cmd['retcode'] != 0:\n        log.warning('racadm return an exit code \\'%s\\'.', cmd['retcode'])\n        return False\n\n    return True", "output": "Execute rac commands", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_pkg_install_time(pkg, arch=None):\n    '''\n    \n    '''\n    iso_time = iso_time_t = None\n    loc_root = '/var/lib/dpkg/info'\n    if pkg is not None:\n        locations = []\n        if arch is not None and arch != 'all':\n            locations.append(os.path.join(loc_root, '{0}:{1}.list'.format(pkg, arch)))\n\n        locations.append(os.path.join(loc_root, '{0}.list'.format(pkg)))\n        for location in locations:\n            try:\n                iso_time_t = int(os.path.getmtime(location))\n                iso_time = datetime.datetime.utcfromtimestamp(iso_time_t).isoformat() + 'Z'\n                break\n            except OSError:\n                pass\n\n        if iso_time is None:\n            log.debug('Unable to get package installation time for package \"%s\".', pkg)\n\n    return iso_time, iso_time_t", "output": "Return package install time, based on the /var/lib/dpkg/info/<package>.list\n\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_list(path_in):\n    \"\"\"\n    \"\"\"\n    with open(path_in) as fin:\n        while True:\n            line = fin.readline()\n            if not line:\n                break\n            line = [i.strip() for i in line.strip().split('\\t')]\n            line_len = len(line)\n            # check the data format of .lst file\n            if line_len < 3:\n                print('lst should have at least has three parts, but only has %s parts for %s' % (line_len, line))\n                continue\n            try:\n                item = [int(line[0])] + [line[-1]] + [float(i) for i in line[1:-1]]\n            except Exception as e:\n                print('Parsing lst met error for %s, detail: %s' % (line, e))\n                continue\n            yield item", "output": "Reads the .lst file and generates corresponding iterator.\n    Parameters\n    ----------\n    path_in: string\n    Returns\n    -------\n    item iterator that contains information in .lst file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exception(self, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n\n        def decorator(handler):\n            exception = FutureException(handler, args, kwargs)\n            self.exceptions.append(exception)\n            return handler\n\n        return decorator", "output": "This method enables the process of creating a global exception\n        handler for the current blueprint under question.\n\n        :param args: List of Python exceptions to be caught by the handler\n        :param kwargs: Additional optional arguments to be passed to the\n            exception handler\n\n        :return a decorated method to handle global exceptions for any\n            route registered under this blueprint.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def info():\n    '''\n    \n    '''\n    cmd = r'cscript C:\\Windows\\System32\\slmgr.vbs /dli'\n    out = __salt__['cmd.run'](cmd)\n\n    match = re.search(r'Name: (.*)\\r\\nDescription: (.*)\\r\\nPartial Product Key: (.*)\\r\\nLicense Status: (.*)', out,\n                      re.MULTILINE)\n\n    if match is not None:\n        groups = match.groups()\n        return {\n            'name': groups[0],\n            'description': groups[1],\n            'partial_key': groups[2],\n            'licensed': 'Licensed' in groups[3]\n        }\n\n    return None", "output": "Return information about the license, if the license is not\n    correctly activated this will return None.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' license.info", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def MergeMessage(\n      self, source, destination,\n      replace_message, replace_repeated):\n    \"\"\"\"\"\"\n    _MergeMessage(\n        self._root, source, destination, replace_message, replace_repeated)", "output": "Merge all fields specified by this tree from source to destination.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _send_cmd(self, cmd: str):\n        \"\"\"\"\"\"\n        self._sock.sendall(cmd.encode(encoding='latin-1', errors='strict'))", "output": "Encode IQFeed API messages.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_network_backing(network_name, switch_type, parent_ref):\n    '''\n    \n    '''\n    log.trace('Configuring virtual machine network backing network_name=%s '\n              'switch_type=%s parent=%s',\n              network_name, switch_type,\n              salt.utils.vmware.get_managed_object_name(parent_ref))\n    backing = {}\n    if network_name:\n        if switch_type == 'standard':\n            networks = salt.utils.vmware.get_networks(\n                parent_ref,\n                network_names=[network_name])\n            if not networks:\n                raise salt.exceptions.VMwareObjectRetrievalError(\n                    'The network \\'{0}\\' could not be '\n                    'retrieved.'.format(network_name))\n            network_ref = networks[0]\n            backing = vim.vm.device.VirtualEthernetCard.NetworkBackingInfo()\n            backing.deviceName = network_name\n            backing.network = network_ref\n        elif switch_type == 'distributed':\n            networks = salt.utils.vmware.get_dvportgroups(\n                parent_ref,\n                portgroup_names=[network_name])\n            if not networks:\n                raise salt.exceptions.VMwareObjectRetrievalError(\n                    'The port group \\'{0}\\' could not be '\n                    'retrieved.'.format(network_name))\n            network_ref = networks[0]\n            dvs_port_connection = vim.dvs.PortConnection(\n                portgroupKey=network_ref.key,\n                switchUuid=network_ref.config.distributedVirtualSwitch.uuid)\n            backing = \\\n                vim.vm.device.VirtualEthernetCard.DistributedVirtualPortBackingInfo()\n            backing.port = dvs_port_connection\n    return backing", "output": "Returns a vim.vm.device.VirtualDevice.BackingInfo object specifying a\n    virtual ethernet card backing information\n\n    network_name\n        string, network name\n\n    switch_type\n        string, type of switch\n\n    parent_ref\n        Parent reference to search for network", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_recv(self, callback):\n        '''\n        \n        '''\n        if callback is None:\n            return self.message_client.on_recv(callback)\n\n        @tornado.gen.coroutine\n        def wrap_callback(body):\n            if not isinstance(body, dict):\n                # TODO: For some reason we need to decode here for things\n                #       to work. Fix this.\n                body = salt.utils.msgpack.loads(body)\n                if six.PY3:\n                    body = salt.transport.frame.decode_embedded_strs(body)\n            ret = yield self._decode_payload(body)\n            callback(ret)\n        return self.message_client.on_recv(wrap_callback)", "output": "Register an on_recv callback", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_txn_selector(self):\n        \"\"\"\"\"\"\n        if self._transaction_id is not None:\n            return TransactionSelector(id=self._transaction_id)\n\n        if self._read_timestamp:\n            key = \"read_timestamp\"\n            value = _datetime_to_pb_timestamp(self._read_timestamp)\n        elif self._min_read_timestamp:\n            key = \"min_read_timestamp\"\n            value = _datetime_to_pb_timestamp(self._min_read_timestamp)\n        elif self._max_staleness:\n            key = \"max_staleness\"\n            value = _timedelta_to_duration_pb(self._max_staleness)\n        elif self._exact_staleness:\n            key = \"exact_staleness\"\n            value = _timedelta_to_duration_pb(self._exact_staleness)\n        else:\n            key = \"strong\"\n            value = True\n\n        options = TransactionOptions(\n            read_only=TransactionOptions.ReadOnly(**{key: value})\n        )\n\n        if self._multi_use:\n            return TransactionSelector(begin=options)\n        else:\n            return TransactionSelector(single_use=options)", "output": "Helper for :meth:`read`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_current_searchpath(self):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        idx = self.currentIndex()\r\n        if idx == CWD:\r\n            return self.path\r\n        elif idx == PROJECT:\r\n            return self.project_path\r\n        elif idx == FILE_PATH:\r\n            return self.file_path\r\n        else:\r\n            return self.external_path", "output": "Returns the path corresponding to the currently selected item\r\n        in the combobox.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __load_temp_file(self):\r\n        \"\"\"\"\"\"\r\n        if not osp.isfile(self.TEMPFILE_PATH):\r\n            # Creating temporary file\r\n            default = ['# -*- coding: utf-8 -*-',\r\n                       '\"\"\"', _(\"Spyder Editor\"), '',\r\n                       _(\"This is a temporary script file.\"),\r\n                       '\"\"\"', '', '']\r\n            text = os.linesep.join([encoding.to_unicode(qstr)\r\n                                    for qstr in default])\r\n            try:\r\n                encoding.write(to_text_string(text), self.TEMPFILE_PATH,\r\n                               'utf-8')\r\n            except EnvironmentError:\r\n                self.new()\r\n                return\r\n\r\n        self.load(self.TEMPFILE_PATH)", "output": "Load temporary file from a text file in user home directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_local_module_dir(cache_dir, module_name):\n  \"\"\"\"\"\"\n  tf_v1.gfile.MakeDirs(cache_dir)\n  return os.path.join(cache_dir, module_name)", "output": "Creates and returns the name of directory where to cache a module.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rename(name, new_name):\n    '''\n    \n    '''\n    current_info = info(name)\n    if not current_info:\n        raise CommandExecutionError('User \\'{0}\\' does not exist'.format(name))\n    new_info = info(new_name)\n    if new_info:\n        raise CommandExecutionError(\n            'User \\'{0}\\' already exists'.format(new_name)\n        )\n    cmd = ['pw', 'usermod', '-l', new_name, '-n', name]\n    __salt__['cmd.run'](cmd)\n    post_info = info(new_name)\n    if post_info['name'] != current_info['name']:\n        return post_info['name'] == new_name\n    return False", "output": "Change the username for a named user\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.rename name new_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def actualize_sources (self, sources, prop_set):\n        \"\"\" \n        \"\"\"\n        assert is_iterable_typed(sources, VirtualTarget)\n        assert isinstance(prop_set, property_set.PropertySet)\n        dependencies = self.properties_.get ('<dependency>')\n\n        self.dependency_only_sources_ += self.actualize_source_type (dependencies, prop_set)\n        self.actual_sources_ += self.actualize_source_type (sources, prop_set)\n\n        # This is used to help bjam find dependencies in generated headers\n        # in other main targets.\n        # Say:\n        #\n        #   make a.h : ....... ;\n        #   exe hello : hello.cpp : <implicit-dependency>a.h ;\n        #\n        # However, for bjam to find the dependency the generated target must\n        # be actualized (i.e. have the jam target). In the above case,\n        # if we're building just hello (\"bjam hello\"), 'a.h' won't be\n        # actualized unless we do it here.\n        implicit = self.properties_.get(\"<implicit-dependency>\")\n\n        for i in implicit:\n            i.actualize()", "output": "Creates actual jam targets for sources. Initializes two member\n            variables:\n            'self.actual_sources_' -- sources which are passed to updating action\n            'self.dependency_only_sources_' -- sources which are made dependencies, but\n            are not used otherwise.\n\n            New values will be *appended* to the variables. They may be non-empty,\n            if caller wants it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def supports_build_in_container(config):\n    \"\"\"\n    \n    \"\"\"\n\n    def _key(c):\n        return str(c.language) + str(c.dependency_manager) + str(c.application_framework)\n\n    # This information could have beeen bundled inside the Workflow Config object. But we this way because\n    # ultimately the workflow's implementation dictates whether it can run within a container or not.\n    # A \"workflow config\" is like a primary key to identify the workflow. So we use the config as a key in the\n    # map to identify which workflows can support building within a container.\n\n    unsupported = {\n        _key(DOTNET_CLIPACKAGE_CONFIG): \"We do not support building .NET Core Lambda functions within a container. \"\n                                        \"Try building without the container. Most .NET Core functions will build \"\n                                        \"successfully.\",\n    }\n\n    thiskey = _key(config)\n    if thiskey in unsupported:\n        return False, unsupported[thiskey]\n\n    return True, None", "output": "Given a workflow config, this method provides a boolean on whether the workflow can run within a container or not.\n\n    Parameters\n    ----------\n    config namedtuple(Capability)\n        Config specifying the particular build workflow\n\n    Returns\n    -------\n    tuple(bool, str)\n        True, if this workflow can be built inside a container. False, along with a reason message if it cannot be.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_data_for_problem(problem):\n  \"\"\"\"\"\"\n  training_gen, dev_gen, test_gen = _SUPPORTED_PROBLEM_GENERATORS[problem]\n\n  num_train_shards = FLAGS.num_shards or 10\n  tf.logging.info(\"Generating training data for %s.\", problem)\n  train_output_files = generator_utils.train_data_filenames(\n      problem + generator_utils.UNSHUFFLED_SUFFIX, FLAGS.data_dir,\n      num_train_shards)\n  generator_utils.generate_files(training_gen(), train_output_files,\n                                 FLAGS.max_cases)\n  num_dev_shards = int(num_train_shards * 0.1)\n  tf.logging.info(\"Generating development data for %s.\", problem)\n  dev_output_files = generator_utils.dev_data_filenames(\n      problem + generator_utils.UNSHUFFLED_SUFFIX, FLAGS.data_dir,\n      num_dev_shards)\n  generator_utils.generate_files(dev_gen(), dev_output_files)\n  num_test_shards = int(num_train_shards * 0.1)\n  test_output_files = []\n  test_gen_data = test_gen()\n  if test_gen_data is not None:\n    tf.logging.info(\"Generating test data for %s.\", problem)\n    test_output_files = generator_utils.test_data_filenames(\n        problem + generator_utils.UNSHUFFLED_SUFFIX, FLAGS.data_dir,\n        num_test_shards)\n    generator_utils.generate_files(test_gen_data, test_output_files)\n  all_output_files = train_output_files + dev_output_files + test_output_files\n  generator_utils.shuffle_dataset(all_output_files)", "output": "Generate data for a problem in _SUPPORTED_PROBLEM_GENERATORS.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def touch(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.marker_table_bound is None:\n            self.create_marker_table()\n\n        table = self.marker_table_bound\n        id_exists = self.exists()\n        with self.engine.begin() as conn:\n            if not id_exists:\n                ins = table.insert().values(update_id=self.update_id, target_table=self.target_table,\n                                            inserted=datetime.datetime.now())\n            else:\n                ins = table.update().where(sqlalchemy.and_(table.c.update_id == self.update_id,\n                                                           table.c.target_table == self.target_table)).\\\n                    values(update_id=self.update_id, target_table=self.target_table,\n                           inserted=datetime.datetime.now())\n            conn.execute(ins)\n        assert self.exists()", "output": "Mark this update as complete.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_labels(node, apiserver_url):\n    ''''''\n    # Prepare URL\n    url = \"{0}/api/v1/nodes/{1}\".format(apiserver_url, node)\n    # Make request\n    ret = http.query(url)\n    # Check requests status\n    if 'body' in ret:\n        ret = salt.utils.json.loads(ret.get('body'))\n    elif ret.get('status', 0) == 404:\n        return \"Node {0} doesn't exist\".format(node)\n    else:\n        return ret\n    # Get and return labels\n    return ret.get('metadata', {}).get('labels', {})", "output": "Get all labels from a kube node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pipeline_absent(name):\n    '''\n    \n    '''\n\n    ret = {'name': name, 'changes': {}, 'result': True, 'comment': ''}\n\n    try:\n        pipeline = __salt__['elasticsearch.pipeline_get'](id=name)\n        if pipeline and name in pipeline:\n            if __opts__['test']:\n                ret['comment'] = 'Pipeline {0} will be removed'.format(name)\n                ret['changes']['old'] = pipeline[name]\n                ret['result'] = None\n            else:\n                ret['result'] = __salt__['elasticsearch.pipeline_delete'](id=name)\n                if ret['result']:\n                    ret['comment'] = 'Successfully removed pipeline {0}'.format(name)\n                    ret['changes']['old'] = pipeline[name]\n                else:\n                    ret['comment'] = 'Failed to remove pipeline {0} for unknown reasons'.format(name)\n        else:\n            ret['comment'] = 'Pipeline {0} is already absent'.format(name)\n    except Exception as err:\n        ret['result'] = False\n        ret['comment'] = six.text_type(err)\n\n    return ret", "output": "Ensure that the named pipeline is absent\n\n    name\n        Name of the pipeline to remove", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def maybe_download(url, work_directory):\n    \"\"\"\"\"\"\n    filename = url.split(\"/\")[-1]\n    filepath = os.path.join(work_directory, filename)\n    if not os.path.exists(filepath):\n        logger.info(\"Downloading to {}...\".format(filepath))\n        download(url, work_directory)\n    return filepath", "output": "Download the data from Marlin's website, unless it's already here.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def evaluate_train_and_eval(step, inputs, predict_fun, eval_steps, rng,\n                            train_sw=None, eval_sw=None, history=None):\n  \"\"\"\"\"\"\n  step_log(step, \"Evaluation\")\n  train_metrics, eval_metrics = [\n      evaluate(  # pylint: disable=g-complex-comprehension\n          itertools.islice(input_stream(), eval_steps),\n          predict_fun,\n          _METRICS,\n          rng)\n      for input_stream in\n      [inputs.train_eval_stream, inputs.eval_stream]]\n  if train_sw:\n    log_metrics(train_metrics, train_sw, \"train\", step, history=history)\n  if eval_sw:\n    log_metrics(eval_metrics, eval_sw, \"eval\", step, history=history)\n  step_log(step, \"Finished evaluation\")\n  return train_metrics, eval_metrics", "output": "Evalaute on train and eval data, and log metrics.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        '''\n        \n        '''\n        new_mins = list(salt.utils.minions.CkMinions(self.opts).connected_ids())\n        cc = cache_cli(self.opts)\n        cc.get_cached()\n        cc.put_cache([new_mins])\n        log.debug('ConCache CacheWorker update finished')", "output": "Gather currently connected minions and update the cache", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def WideResnet(num_blocks=3, hidden_size=64, num_output_classes=10,\n               mode='train'):\n  \"\"\"\n  \"\"\"\n  del mode\n  return layers.Serial(\n      layers.Conv(hidden_size, (3, 3), padding='SAME'),\n      WideResnetGroup(num_blocks, hidden_size),\n      WideResnetGroup(num_blocks, hidden_size * 2, (2, 2)),\n      WideResnetGroup(num_blocks, hidden_size * 4, (2, 2)), layers.BatchNorm(),\n      layers.Relu(), layers.AvgPool(pool_size=(8, 8)), layers.Flatten(),\n      layers.Dense(num_output_classes), layers.LogSoftmax())", "output": "WideResnet from https://arxiv.org/pdf/1605.07146.pdf.\n\n  Args:\n    num_blocks: int, number of blocks in a group.\n    hidden_size: the size of the first hidden layer (multiplied later).\n    num_output_classes: int, number of classes to distinguish.\n    mode: is it training or eval.\n\n  Returns:\n    The WideResnet model with given layer and output sizes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_path(cls, path):\n        \"\"\"\n        \"\"\"\n        stat_res = os.stat(path)\n        return cls.from_int(stat.S_IMODE(stat_res.st_mode))", "output": "Make a new :class:`FilePerms` object based on the permissions\n        assigned to the file or directory at *path*.\n\n        Args:\n            path (str): Filesystem path of the target file.\n\n        >>> from os.path import expanduser\n        >>> 'r' in FilePerms.from_path(expanduser('~')).user  # probably\n        True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def debugDumpNodeList(self, output, depth):\n        \"\"\" \"\"\"\n        libxml2mod.xmlDebugDumpNodeList(output, self._o, depth)", "output": "Dumps debug information for the list of element node, it is\n           recursive", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _do_scale(image, size):\n  \"\"\"\"\"\"\n  shape = tf.cast(tf.shape(image), tf.float32)\n  w_greater = tf.greater(shape[0], shape[1])\n  shape = tf.cond(w_greater,\n                  lambda: tf.cast([shape[0] / shape[1] * size, size], tf.int32),\n                  lambda: tf.cast([size, shape[1] / shape[0] * size], tf.int32))\n\n  return tf.image.resize_bicubic([image], shape)[0]", "output": "Rescale the image by scaling the smaller spatial dimension to `size`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cluster_list(verbose=False):\n    '''\n    \n    '''\n    cmd = [salt.utils.path.which('pg_lsclusters'), '--no-header']\n    ret = __salt__['cmd.run_all'](' '.join([pipes.quote(c) for c in cmd]))\n    if ret.get('retcode', 0) != 0:\n        log.error('Error listing clusters')\n    cluster_dict = _parse_pg_lscluster(ret['stdout'])\n    if verbose:\n        return cluster_dict\n    return cluster_dict.keys()", "output": "Return a list of cluster of Postgres server (tuples of version and name).\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' postgres.cluster_list\n\n        salt '*' postgres.cluster_list verbose=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_http_json(operation, api_request, result_type, **kwargs):\n    \"\"\"\n    \"\"\"\n    operation_proto = json_format.ParseDict(operation, operations_pb2.Operation())\n    refresh = functools.partial(_refresh_http, api_request, operation_proto.name)\n    cancel = functools.partial(_cancel_http, api_request, operation_proto.name)\n    return Operation(operation_proto, refresh, cancel, result_type, **kwargs)", "output": "Create an operation future using a HTTP/JSON client.\n\n    This interacts with the long-running operations `service`_ (specific\n    to a given API) via `HTTP/JSON`_.\n\n    .. _HTTP/JSON: https://cloud.google.com/speech/reference/rest/\\\n            v1beta1/operations#Operation\n\n    Args:\n        operation (dict): Operation as a dictionary.\n        api_request (Callable): A callable used to make an API request. This\n            should generally be\n            :meth:`google.cloud._http.Connection.api_request`.\n        result_type (:func:`type`): The protobuf result type.\n        kwargs: Keyword args passed into the :class:`Operation` constructor.\n\n    Returns:\n        ~.api_core.operation.Operation: The operation future to track the given\n            operation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def virtual_machines_list(resource_group, **kwargs):\n    '''\n    \n\n    '''\n    result = {}\n    compconn = __utils__['azurearm.get_client']('compute', **kwargs)\n    try:\n        vms = __utils__['azurearm.paged_object_to_list'](\n            compconn.virtual_machines.list(\n                resource_group_name=resource_group\n            )\n        )\n        for vm in vms:  # pylint: disable=invalid-name\n            result[vm['name']] = vm\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('compute', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    List all virtual machines within a resource group.\n\n    :param resource_group: The resource group name to list virtual\n        machines within.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_compute.virtual_machines_list testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def global_injector_decorator(inject_globals):\n    '''\n    \n    '''\n    def inner_decorator(f):\n        @functools.wraps(f)\n        def wrapper(*args, **kwargs):\n            with salt.utils.context.func_globals_inject(f, **inject_globals):\n                return f(*args, **kwargs)\n        return wrapper\n    return inner_decorator", "output": "Decorator used by the LazyLoader to inject globals into a function at\n    execute time.\n\n    globals\n        Dictionary with global variables to inject", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_disabled():\n    '''\n    \n    '''\n    ret = set()\n    for name in _iter_service_names():\n        if _service_is_upstart(name):\n            if _upstart_is_disabled(name):\n                ret.add(name)\n        else:\n            if _service_is_sysv(name):\n                if _sysv_is_disabled(name):\n                    ret.add(name)\n    return sorted(ret)", "output": "Return the disabled services\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.get_disabled", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clean(bundle, before, after, keep_last):\n    \"\"\"\n    \"\"\"\n    bundles_module.clean(\n        bundle,\n        before,\n        after,\n        keep_last,\n    )", "output": "Clean up data downloaded with the ingest command.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(self, data, name=None):\n        ''' \n\n        '''\n        if name is None:\n            n = len(self.data)\n            while \"Series %d\"%n in self.data:\n                n += 1\n            name = \"Series %d\"%n\n        self.data[name] = data\n        return name", "output": "Appends a new column of data to the data source.\n\n        Args:\n            data (seq) : new data to add\n            name (str, optional) : column name to use.\n                If not supplied, generate a name of the form \"Series ####\"\n\n        Returns:\n            str:  the column name used", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __build_python_module_cache(self):\n        \"\"\"\n        \"\"\"\n        cache = {}\n        for importer, mname, ispkg in pkgutil.walk_packages(b2.__path__, prefix='b2.'):\n            basename = mname.split('.')[-1]\n            # since the jam code is only going to have \"import toolset ;\"\n            # it doesn't matter if there are separately named \"b2.build.toolset\" and\n            # \"b2.contrib.toolset\" as it is impossible to know which the user is\n            # referring to.\n            if basename in cache:\n                self.manager.errors()('duplicate module name \"{0}\" '\n                                      'found in boost-build path'.format(basename))\n            cache[basename] = mname\n        self.__python_module_cache = cache", "output": "Recursively walks through the b2/src subdirectories and\n        creates an index of base module name to package name. The\n        index is stored within self.__python_module_cache and allows\n        for an O(1) module lookup.\n\n        For example, given the base module name `toolset`,\n        self.__python_module_cache['toolset'] will return\n        'b2.build.toolset'\n\n        pkgutil.walk_packages() will find any python package\n        provided a directory contains an __init__.py. This has the\n        added benefit of allowing libraries to be installed and\n        automatically avaiable within the contrib directory.\n\n        *Note*: pkgutil.walk_packages() will import any subpackage\n        in order to access its __path__variable. Meaning:\n        any initialization code will be run if the package hasn't\n        already been imported.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def summarize_video_metrics(hook_args):\n  \"\"\"\"\"\"\n  problem_name = hook_args.problem.name\n  current_problem = hook_args.problem\n  hparams = hook_args.hparams\n  output_dirs = hook_args.output_dirs\n  predictions = hook_args.predictions\n  frame_shape = [\n      current_problem.frame_height, current_problem.frame_width,\n      current_problem.num_channels\n  ]\n  metrics_graph = tf.Graph()\n  with metrics_graph.as_default():\n    if predictions:\n      metrics_results, _ = video_metrics.compute_video_metrics_from_predictions(\n          predictions, decode_hparams=hook_args.decode_hparams)\n    else:\n      metrics_results, _ = video_metrics.compute_video_metrics_from_png_files(\n          output_dirs, problem_name, hparams.video_num_target_frames,\n          frame_shape)\n\n  summary_values = []\n  for name, array in six.iteritems(metrics_results):\n    for ind, val in enumerate(array):\n      tag = \"metric_{}/{}\".format(name, ind)\n      summary_values.append(tf.Summary.Value(tag=tag, simple_value=val))\n  return summary_values", "output": "Computes video metrics summaries using the decoder output.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def usermacro_delete(macroids, **kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'usermacro.delete'\n            if isinstance(macroids, list):\n                params = macroids\n            else:\n                params = [macroids]\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result']['hostmacroids']\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": "Delete host usermacros.\n\n    :param macroids: macroids of the host usermacros\n\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    return: IDs of the deleted host usermacro.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zabbix.usermacro_delete 21", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_image_tensors(self)->([Tensor], [Tensor], [Tensor]):\n        \"\"\n        orig_images, gen_images, real_images = [], [], []\n        for image_set in self.image_sets:\n            orig_images.append(image_set.orig.px)\n            gen_images.append(image_set.gen.px)\n            real_images.append(image_set.real.px) \n        return orig_images, gen_images, real_images", "output": "Gets list of image tensors from lists of Image objects, as a tuple of original, generated and real(target) images.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copyChar(len, out, val):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlCopyChar(len, out, val)\n    return ret", "output": "append the char value in the array", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def evaluate(self, brain_info):\n        \"\"\"\n        \n        \"\"\"\n        feed_dict = {self.model.batch_size: len(brain_info.vector_observations),\n                     self.model.sequence_length: 1}\n        epsilon = None\n        if self.use_recurrent:\n            if not self.use_continuous_act:\n                feed_dict[self.model.prev_action] = brain_info.previous_vector_actions.reshape(\n                    [-1, len(self.model.act_size)])\n            if brain_info.memories.shape[1] == 0:\n                brain_info.memories = self.make_empty_memory(len(brain_info.agents))\n            feed_dict[self.model.memory_in] = brain_info.memories\n        if self.use_continuous_act:\n            epsilon = np.random.normal(\n                size=(len(brain_info.vector_observations), self.model.act_size[0]))\n            feed_dict[self.model.epsilon] = epsilon\n        feed_dict = self._fill_eval_dict(feed_dict, brain_info)\n        run_out = self._execute_model(feed_dict, self.inference_dict)\n        if self.use_continuous_act:\n            run_out['random_normal_epsilon'] = epsilon\n        return run_out", "output": "Evaluates policy for the agent experiences provided.\n        :param brain_info: BrainInfo object containing inputs.\n        :return: Outputs from network as defined by self.inference_dict.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lr_find2(self, start_lr=1e-5, end_lr=10, num_it = 100, wds=None, linear=False, stop_dv=True, **kwargs):\n        \"\"\"\n        \"\"\"\n        self.save('tmp')\n        layer_opt = self.get_layer_opt(start_lr, wds)\n        self.sched = LR_Finder2(layer_opt, num_it, end_lr, linear=linear, metrics=self.metrics, stop_dv=stop_dv)\n        self.fit_gen(self.model, self.data, layer_opt, num_it//len(self.data.trn_dl) + 1, all_val=True, **kwargs)\n        self.load('tmp')", "output": "A variant of lr_find() that helps find the best learning rate. It doesn't do\n        an epoch but a fixed num of iterations (which may be more or less than an epoch\n        depending on your data).\n        At each step, it computes the validation loss and the metrics on the next\n        batch of the validation data, so it's slower than lr_find().\n\n        Args:\n            start_lr (float/numpy array) : Passing in a numpy array allows you\n                to specify learning rates for a learner's layer_groups\n            end_lr (float) : The maximum learning rate to try.\n            num_it : the number of iterations you want it to run\n            wds (iterable/float)\n            stop_dv : stops (or not) when the losses starts to explode.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def groupByKey(self, numPartitions=None, partitionFunc=portable_hash):\n        \"\"\"\n        \n        \"\"\"\n        def createCombiner(x):\n            return [x]\n\n        def mergeValue(xs, x):\n            xs.append(x)\n            return xs\n\n        def mergeCombiners(a, b):\n            a.extend(b)\n            return a\n\n        memory = self._memory_limit()\n        serializer = self._jrdd_deserializer\n        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)\n\n        def combine(iterator):\n            merger = ExternalMerger(agg, memory * 0.9, serializer)\n            merger.mergeValues(iterator)\n            return merger.items()\n\n        locally_combined = self.mapPartitions(combine, preservesPartitioning=True)\n        shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)\n\n        def groupByKey(it):\n            merger = ExternalGroupBy(agg, memory, serializer)\n            merger.mergeCombiners(it)\n            return merger.items()\n\n        return shuffled.mapPartitions(groupByKey, True).mapValues(ResultIterable)", "output": "Group the values for each key in the RDD into a single sequence.\n        Hash-partitions the resulting RDD with numPartitions partitions.\n\n        .. note:: If you are grouping in order to perform an aggregation (such as a\n            sum or average) over each key, using reduceByKey or aggregateByKey will\n            provide much better performance.\n\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n        >>> sorted(rdd.groupByKey().mapValues(len).collect())\n        [('a', 2), ('b', 1)]\n        >>> sorted(rdd.groupByKey().mapValues(list).collect())\n        [('a', [1, 1]), ('b', [1])]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_gru(builder, layer, input_names, output_names, keras_layer):\n    \"\"\"\n    \"\"\"\n\n    hidden_size = keras_layer.output_dim\n    input_size = keras_layer.input_shape[-1]\n\n    output_all = keras_layer.return_sequences\n    reverse_input = keras_layer.go_backwards\n\n    if keras_layer.consume_less not in ['cpu', 'gpu']:\n        raise ValueError('Cannot convert Keras layer with consume_less = %s' % keras_layer.consume_less)\n\n    # Keras: Z R O\n    # CoreML: Z R O\n    W_h, W_x, b = ([], [], [])\n    if keras_layer.consume_less == 'cpu':\n        W_x.append(keras_layer.get_weights()[0].T)\n        W_x.append(keras_layer.get_weights()[3].T)\n        W_x.append(keras_layer.get_weights()[6].T)\n\n        W_h.append(keras_layer.get_weights()[1].T)\n        W_h.append(keras_layer.get_weights()[4].T)\n        W_h.append(keras_layer.get_weights()[7].T)\n\n        b.append(keras_layer.get_weights()[2])\n        b.append(keras_layer.get_weights()[5])\n        b.append(keras_layer.get_weights()[8])\n    else:\n        print('consume less not implemented')\n\n    # Set actication type\n    inner_activation_str = _get_recurrent_activation_name_from_keras(keras_layer.inner_activation)\n    activation_str = _get_recurrent_activation_name_from_keras(keras_layer.activation)\n\n    # Add to the network\n    builder.add_gru(\n       name = layer,\n       W_h = W_h, W_x = W_x, b = b,\n       input_size = input_size,\n       hidden_size = hidden_size,\n       input_names = input_names,\n       output_names = output_names,\n       activation = activation_str,\n       inner_activation = inner_activation_str,\n       output_all=output_all,\n       reverse_input = reverse_input)", "output": "Convert a GRU layer from keras to coreml.\n\n    Parameters\n    ----------\n    keras_layer: layer\n        A keras layer object.\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_docword(filename, vocab_filename):\n    \"\"\"\n    \n    \"\"\"\n    vocab = _turicreate.SFrame.read_csv(vocab_filename, header=None)['X1']\n    vocab = list(vocab)\n\n    sf = _turicreate.SFrame.read_csv(filename, header=False)\n    sf = sf[3:]\n    sf['X2'] = sf['X1'].apply(lambda x: [int(z) for z in x.split(' ')])\n    del sf['X1']\n    sf = sf.unpack('X2', column_name_prefix='', column_types=[int,int,int])\n    docs = sf.unstack(['1', '2'], 'bow').sort('0')['bow']\n    docs = docs.apply(lambda x: {vocab[k-1]:v for (k, v) in six.iteritems(x)})\n\n    return docs", "output": "Parse a file that's in \"docword\" format. This consists of a 3-line header\n    comprised of the document count, the vocabulary count, and the number of\n    tokens, i.e. unique (doc_id, word_id) pairs. After the header, each line\n    contains a space-separated triple of (doc_id, word_id, frequency), where\n    frequency is the number of times word_id occurred in document doc_id.\n\n    This format assumes that documents and words are identified by a positive\n    integer (whose lowest value is 1).\n    Thus, the first word in the vocabulary file has word_id=1.\n\n    2\n    272\n    5\n    1 5 1\n    1 105 3\n    1 272 5\n    2 1 3\n    ...\n\n    Parameters\n    ----------\n    filename : str\n        The name of the file to parse.\n\n    vocab_filename : str\n        A list of words that are used for this data set.\n\n    Returns\n    -------\n    out : SArray\n        Each element represents a document in bag-of-words format.\n\n    Examples\n    --------\n    >>> textfile = 'https://static.turi.com/datasets/text/docword.nips.txt')\n    >>> vocab = 'https://static.turi.com/datasets/text/vocab.nips.txt')\n    >>> docs = turicreate.text_analytics.parse_docword(textfile, vocab)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def try_to_replace(self, provider, other, problems):\n        \"\"\"\n        \n        \"\"\"\n        rlist = self.reqts[other]\n        unmatched = set()\n        for s in rlist:\n            matcher = self.get_matcher(s)\n            if not matcher.match(provider.version):\n                unmatched.add(s)\n        if unmatched:\n            # can't replace other with provider\n            problems.add(('cantreplace', provider, other,\n                          frozenset(unmatched)))\n            result = False\n        else:\n            # can replace other with provider\n            self.remove_distribution(other)\n            del self.reqts[other]\n            for s in rlist:\n                self.reqts.setdefault(provider, set()).add(s)\n            self.add_distribution(provider)\n            result = True\n        return result", "output": "Attempt to replace one provider with another. This is typically used\n        when resolving dependencies from multiple sources, e.g. A requires\n        (B >= 1.0) while C requires (B >= 1.1).\n\n        For successful replacement, ``provider`` must meet all the requirements\n        which ``other`` fulfills.\n\n        :param provider: The provider we are trying to replace with.\n        :param other: The provider we're trying to replace.\n        :param problems: If False is returned, this will contain what\n                         problems prevented replacement. This is currently\n                         a tuple of the literal string 'cantreplace',\n                         ``provider``, ``other``  and the set of requirements\n                         that ``provider`` couldn't fulfill.\n        :return: True if we can replace ``other`` with ``provider``, else\n                 False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_selected_filenames(self):\r\n        \"\"\"\"\"\"\r\n        if self.selectionMode() == self.ExtendedSelection:\r\n            if self.selectionModel() is None:\r\n                return []\r\n            return [self.get_filename(idx) for idx in \r\n                    self.selectionModel().selectedRows()]\r\n        else:\r\n            return [self.get_filename(self.currentIndex())]", "output": "Return selected filenames", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_split_zone(zone, _conn, private_zone):\n    '''\n    \n    '''\n    for _zone in _conn.get_zones():\n        if _zone.name == zone:\n            _private_zone = True if _zone.config['PrivateZone'].lower() == 'true' else False\n            if _private_zone == private_zone:\n                return _zone\n    return False", "output": "With boto route53, zones can only be matched by name\n    or iterated over in a list.  Since the name will be the\n    same for public and private zones in a split DNS situation,\n    iterate over the list and match the zone name and public/private\n    status.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def debugDumpOneNode(self, output, depth):\n        \"\"\" \"\"\"\n        libxml2mod.xmlDebugDumpOneNode(output, self._o, depth)", "output": "Dumps debug information for the element node, it is not\n           recursive", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def iou(boxes1, boxes2):\n  \"\"\"\n  \"\"\"\n  intersect = intersection(boxes1, boxes2)\n  area1 = area(boxes1)\n  area2 = area(boxes2)\n  union = np.expand_dims(area1, axis=1) + np.expand_dims(\n      area2, axis=0) - intersect\n  return intersect / union", "output": "Computes pairwise intersection-over-union between box collections.\n\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes.\n    boxes2: a numpy array with shape [M, 4] holding M boxes.\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise iou scores.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process(self):\r\n        \"\"\"\"\"\"\r\n        var_name = self.name_edt.text()\r\n        try:\r\n            self.var_name = str(var_name)\r\n        except UnicodeEncodeError:\r\n            self.var_name = to_text_string(var_name)\r\n        if self.text_widget.get_as_data():\r\n            self.clip_data = self._get_table_data()\r\n        elif self.text_widget.get_as_code():\r\n            self.clip_data = try_to_eval(\r\n                to_text_string(self._get_plain_text()))\r\n        else:\r\n            self.clip_data = to_text_string(self._get_plain_text())\r\n        self.accept()", "output": "Process the data from clipboard", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_data_table(self):\r\n        \"\"\"\"\"\"\r\n        self.dataTable = DataFrameView(self, self.dataModel,\r\n                                       self.table_header.horizontalHeader(),\r\n                                       self.hscroll, self.vscroll)\r\n        self.dataTable.verticalHeader().hide()\r\n        self.dataTable.horizontalHeader().hide()\r\n        self.dataTable.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)\r\n        self.dataTable.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOff)\r\n        self.dataTable.setHorizontalScrollMode(QTableView.ScrollPerPixel)\r\n        self.dataTable.setVerticalScrollMode(QTableView.ScrollPerPixel)\r\n        self.dataTable.setFrameStyle(QFrame.Plain)\r\n        self.dataTable.setItemDelegate(QItemDelegate())\r\n        self.layout.addWidget(self.dataTable, 1, 1)\r\n        self.setFocusProxy(self.dataTable)\r\n        self.dataTable.sig_sort_by_column.connect(self._sort_update)\r\n        self.dataTable.sig_fetch_more_columns.connect(self._fetch_more_columns)\r\n        self.dataTable.sig_fetch_more_rows.connect(self._fetch_more_rows)", "output": "Create the QTableView that will hold the data model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_callbacks(self, callbacks, monitors):\n        \"\"\"\n        \n        \"\"\"\n        assert isinstance(callbacks, list), callbacks\n        assert isinstance(monitors, list), monitors\n        describe_trainable_vars()   # TODO weird\n\n        self.register_callback(MaintainStepCounter())\n        for cb in callbacks:\n            self.register_callback(cb)\n        for cb in self._callbacks:\n            assert not isinstance(cb, MonitorBase), \"Monitor cannot be pre-registered for now!\"\n        registered_monitors = []\n        for m in monitors:\n            if self.register_callback(m):\n                registered_monitors.append(m)\n        self.monitors = Monitors(registered_monitors)\n        self.register_callback(self.monitors)   # monitors is also a callback\n\n        # some final operations that might modify the graph\n        logger.info(\"Setup callbacks graph ...\")\n        self._callbacks = Callbacks(self._callbacks)\n        self._callbacks.setup_graph(weakref.proxy(self))", "output": "Setup callbacks and monitors. Must be called after the main graph is built.\n\n        Args:\n            callbacks ([Callback]):\n            monitors ([MonitorBase]):", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathNewContext(self):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlXPathNewContext(self._o)\n        if ret is None:raise xpathError('xmlXPathNewContext() failed')\n        __tmp = xpathContext(_obj=ret)\n        return __tmp", "output": "Create a new xmlXPathContext", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(key, host=None, port=None, db=None, password=None):\n    '''\n    \n    '''\n    server = _connect(host, port, db, password)\n    return server.exists(key)", "output": "Return true if the key exists in redis\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.exists foo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def config_name_from_full_name(full_name):\n    \"\"\"\n    \"\"\"\n    projects, _, configs, result = full_name.split(\"/\")\n    if projects != \"projects\" or configs != \"configs\":\n        raise ValueError(\n            \"Unexpected format of resource\",\n            full_name,\n            'Expected \"projects/{proj}/configs/{cfg}\"',\n        )\n    return result", "output": "Extract the config name from a full resource name.\n\n      >>> config_name_from_full_name('projects/my-proj/configs/my-config')\n      \"my-config\"\n\n    :type full_name: str\n    :param full_name:\n        The full resource name of a config. The full resource name looks like\n        ``projects/project-name/configs/config-name`` and is returned as the\n        ``name`` field of a config resource.  See\n        https://cloud.google.com/deployment-manager/runtime-configurator/reference/rest/v1beta1/projects.configs\n\n    :rtype: str\n    :returns: The config's short name, given its full resource name.\n    :raises: :class:`ValueError` if ``full_name`` is not the expected format", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def export(cwd,\n           remote,\n           target=None,\n           user=None,\n           username=None,\n           password=None,\n           revision='HEAD',\n           *opts):\n    '''\n    \n    '''\n    opts += (remote,)\n    if target:\n        opts += (target,)\n    revision_args = '-r'\n    opts += (revision_args, six.text_type(revision),)\n    return _run_svn('export', cwd, user, username, password, opts)", "output": "Create an unversioned copy of a tree.\n\n    cwd\n        The path to the Subversion repository\n\n    remote : None\n        URL and path to file or directory checkout\n\n    target : None\n        The name to give the file or directory working copy\n        Default: svn uses the remote basename\n\n    user : None\n        Run svn as a user other than what the minion runs as\n\n    username : None\n        Connect to the Subversion server as another user\n\n    password : None\n        Connect to the Subversion server with this password\n\n        .. versionadded:: 0.17.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' svn.export /path/to/repo svn://remote/repo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def follow_stats(self, index, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if index in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, \"_ccr\", \"stats\"), params=params\n        )", "output": "`<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-follow-stats.html>`_\n\n        :arg index: A comma-separated list of index patterns; use `_all` to\n            perform the operation on all indices", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _numeric_param_check_range(variable_name, variable_value, range_bottom, range_top):\n    \"\"\"\n    \n    \"\"\"\n    err_msg = \"%s must be between %i and %i\"\n\n    if variable_value < range_bottom or variable_value > range_top:\n        raise ToolkitError(err_msg % (variable_name, range_bottom, range_top))", "output": "Checks if numeric parameter is within given range", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ToJsonString(self):\n    \"\"\"\n    \"\"\"\n    nanos = self.nanos % _NANOS_PER_SECOND\n    total_sec = self.seconds + (self.nanos - nanos) // _NANOS_PER_SECOND\n    seconds = total_sec % _SECONDS_PER_DAY\n    days = (total_sec - seconds) // _SECONDS_PER_DAY\n    dt = datetime(1970, 1, 1) + timedelta(days, seconds)\n\n    result = dt.isoformat()\n    if (nanos % 1e9) == 0:\n      # If there are 0 fractional digits, the fractional\n      # point '.' should be omitted when serializing.\n      return result + 'Z'\n    if (nanos % 1e6) == 0:\n      # Serialize 3 fractional digits.\n      return result + '.%03dZ' % (nanos / 1e6)\n    if (nanos % 1e3) == 0:\n      # Serialize 6 fractional digits.\n      return result + '.%06dZ' % (nanos / 1e3)\n    # Serialize 9 fractional digits.\n    return result + '.%09dZ' % nanos", "output": "Converts Timestamp to RFC 3339 date string format.\n\n    Returns:\n      A string converted from timestamp. The string is always Z-normalized\n      and uses 3, 6 or 9 fractional digits as required to represent the\n      exact time. Example of the return format: '1972-01-01T10:00:20.021Z'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_alert_config(deployment_id, metric_name=None, api_key=None, profile=\"telemetry\"):\n    '''\n    \n    '''\n\n    auth = _auth(profile=profile)\n    alert = False\n\n    key = \"telemetry.{0}.alerts\".format(deployment_id)\n\n    if key not in __context__:\n        try:\n            get_url = _get_telemetry_base(profile) + \"/alerts?deployment={0}\".format(deployment_id)\n            response = requests.get(get_url, headers=auth)\n        except requests.exceptions.RequestException as e:\n            log.error(six.text_type(e))\n            return False\n\n        http_result = {}\n        if response.status_code == 200:\n            for alert in response.json():\n                http_result[alert.get('condition', {}).get('metric')] = alert\n                __context__[key] = http_result\n\n    if not __context__.get(key):\n        return []\n\n    alerts = __context__[key].values()\n\n    if metric_name:\n        return __context__[key].get(metric_name)\n\n    return [alert['_id'] for alert in alerts if '_id' in alert]", "output": "Get all alert definitions associated with a given deployment or if metric_name\n    is specified, obtain the specific alert config\n\n    Returns dictionary or list of dictionaries.\n\n    CLI Example:\n\n        salt myminion telemetry.get_alert_config rs-ds033197 currentConnections profile=telemetry\n        salt myminion telemetry.get_alert_config rs-ds033197 profile=telemetry", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_load(jid):\n    '''\n    \n    '''\n    cb_ = _get_connection()\n\n    try:\n        jid_doc = cb_.get(six.text_type(jid))\n    except couchbase.exceptions.NotFoundError:\n        return {}\n\n    ret = {}\n    try:\n        ret = jid_doc.value['load']\n        ret['Minions'] = jid_doc.value['minions']\n    except KeyError as e:\n        log.error(e)\n\n    return ret", "output": "Return the load data that marks a specified jid", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetch_and_execute_function_to_run(self, key):\n        \"\"\"\"\"\"\n        (driver_id, serialized_function,\n         run_on_other_drivers) = self.redis_client.hmget(\n             key, [\"driver_id\", \"function\", \"run_on_other_drivers\"])\n\n        if (utils.decode(run_on_other_drivers) == \"False\"\n                and self.worker.mode == ray.SCRIPT_MODE\n                and driver_id != self.worker.task_driver_id.binary()):\n            return\n\n        try:\n            # Deserialize the function.\n            function = pickle.loads(serialized_function)\n            # Run the function.\n            function({\"worker\": self.worker})\n        except Exception:\n            # If an exception was thrown when the function was run, we record\n            # the traceback and notify the scheduler of the failure.\n            traceback_str = traceback.format_exc()\n            # Log the error message.\n            utils.push_error_to_driver(\n                self.worker,\n                ray_constants.FUNCTION_TO_RUN_PUSH_ERROR,\n                traceback_str,\n                driver_id=ray.DriverID(driver_id))", "output": "Run on arbitrary function on the worker.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def split_auth_from_netloc(netloc):\n    \"\"\"\n    \n    \"\"\"\n    if '@' not in netloc:\n        return netloc, (None, None)\n\n    # Split from the right because that's how urllib.parse.urlsplit()\n    # behaves if more than one @ is present (which can be checked using\n    # the password attribute of urlsplit()'s return value).\n    auth, netloc = netloc.rsplit('@', 1)\n    if ':' in auth:\n        # Split from the left because that's how urllib.parse.urlsplit()\n        # behaves if more than one : is present (which again can be checked\n        # using the password attribute of the return value)\n        user_pass = auth.split(':', 1)\n    else:\n        user_pass = auth, None\n\n    user_pass = tuple(\n        None if x is None else urllib_unquote(x) for x in user_pass\n    )\n\n    return netloc, user_pass", "output": "Parse out and remove the auth information from a netloc.\n\n    Returns: (netloc, (username, password)).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_tags(DomainName=None, ARN=None,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        if ARN is None:\n            if DomainName is None:\n                raise SaltInvocationError('One (but not both) of ARN or '\n                         'domain must be specified.')\n            domaindata = status(DomainName=DomainName,\n                            region=region, key=key, keyid=keyid,\n                            profile=profile)\n            if not domaindata or 'domain' not in domaindata:\n                log.warning('Domain tags not updated')\n                return {'tagged': False}\n            ARN = domaindata.get('domain', {}).get('ARN')\n        elif DomainName is not None:\n            raise SaltInvocationError('One (but not both) of ARN or '\n                         'domain must be specified.')\n        ret = conn.list_tags(ARN=ARN)\n        log.warning(ret)\n        tlist = ret.get('TagList', [])\n        tagdict = {}\n        for tag in tlist:\n            tagdict[tag.get('Key')] = tag.get('Value')\n        return {'tags': tagdict}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "List tags of a trail\n\n    Returns:\n        tags:\n          - {...}\n          - {...}\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_cloudtrail.list_tags my_trail", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def matches(self, new, old):\n        ''' \n\n        '''\n        if isinstance(new, np.ndarray) or isinstance(old, np.ndarray):\n            return np.array_equal(new, old)\n\n        if pd:\n            if isinstance(new, pd.Series) or isinstance(old, pd.Series):\n                return np.array_equal(new, old)\n\n            if isinstance(new, pd.Index) or isinstance(old, pd.Index):\n                return np.array_equal(new, old)\n\n        try:\n\n            # this handles the special but common case where there is a dict with array\n            # or series as values (e.g. the .data property of a ColumnDataSource)\n            if isinstance(new, dict) and isinstance(old, dict):\n                if set(new.keys()) != set(old.keys()):\n                    return False\n                return all(self.matches(new[k], old[k]) for k in new)\n\n            return new == old\n\n        # if the comparison fails for some reason, just punt and return no-match\n        except ValueError:\n            return False", "output": "Whether two parameters match values.\n\n        If either ``new`` or ``old`` is a NumPy array or Pandas Series or Index,\n        then the result of ``np.array_equal`` will determine if the values match.\n\n        Otherwise, the result of standard Python equality will be returned.\n\n        Returns:\n            True, if new and old match, False otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filepattern(self, data_dir, mode, shard=None):\n    \"\"\"\n    \"\"\"\n    path = os.path.join(data_dir, self.dataset_filename())\n    shard_str = \"-%05d\" % shard if shard is not None else \"\"\n    if mode == DatasetSplit.TRAIN:\n      suffix = \"train\"\n    elif mode in [DatasetSplit.EVAL, tf.estimator.ModeKeys.PREDICT]:\n      suffix = \"dev\"\n    else:\n      assert mode == DatasetSplit.TEST\n      suffix = \"test\"\n\n    return \"%s-%s%s*\" % (path, suffix, shard_str)", "output": "Get filepattern for data files for mode.\n\n    Matches mode to a suffix.\n    * DatasetSplit.TRAIN: train\n    * DatasetSplit.EVAL: dev\n    * DatasetSplit.TEST: test\n    * tf.estimator.ModeKeys.PREDICT: dev\n\n    Args:\n      data_dir: str, data directory.\n      mode: DatasetSplit\n      shard: int, if provided, will only read data from the specified shard.\n\n    Returns:\n      filepattern str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _kernel_shape(self, input_shape):\n    \"\"\"\"\"\"\n    kernel_size_iter = iter(self._kernel_size)\n    return [self._filters if c == 'O' else\n            input_shape[self._lhs_spec.index('C')] if c == 'I' else\n            next(kernel_size_iter) for c in self._rhs_spec]", "output": "Helper to calculate the kernel shape.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deploy(jboss_config, source_file):\n    '''\n    \n       '''\n    log.debug(\"======================== MODULE FUNCTION: jboss7.deploy, source_file=%s\", source_file)\n    command = 'deploy {source_file} --force '.format(source_file=source_file)\n    return __salt__['jboss7_cli.run_command'](jboss_config, command, fail_on_error=False)", "output": "Deploy the application on the jboss instance from the local file system where minion is running.\n\n    jboss_config\n        Configuration dictionary with properties specified above.\n    source_file\n        Source file to deploy from\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' jboss7.deploy '{\"cli_path\": \"integration.modules.sysmod.SysModuleTest.test_valid_docs\", \"controller\": \"10.11.12.13:9999\", \"cli_user\": \"jbossadm\", \"cli_password\": \"jbossadm\"}' /opt/deploy_files/my_deploy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_str(s):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(s, bytes):\n        s = s.decode('utf-8')\n    elif not isinstance(s, str):\n        s = str(s)\n    return s", "output": "Convert bytes and non-string into Python 3 str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def status(name, location='\\\\'):\n    \n    '''\n    # Check for existing folder\n    if name not in list_tasks(location):\n        return '{0} not found in {1}'.format(name, location)\n\n    # connect to the task scheduler\n    with salt.utils.winapi.Com():\n        task_service = win32com.client.Dispatch(\"Schedule.Service\")\n    task_service.Connect()\n\n    # get the folder where the task is defined\n    task_folder = task_service.GetFolder(location)\n    task = task_folder.GetTask(name)\n\n    return states[task.State]", "output": "r'''\n    Determine the status of a task. Is it Running, Queued, Ready, etc.\n\n    :param str name: The name of the task for which to return the status\n\n    :param str location: A string value representing the location of the task.\n        Default is '\\\\' which is the root for the task scheduler\n        (C:\\Windows\\System32\\tasks).\n\n    :return: The current status of the task. Will be one of the following:\n\n    - Unknown\n    - Disabled\n    - Queued\n    - Ready\n    - Running\n\n    :rtype: string\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'minion-id' task.list_status <task_name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def execute(self):\n        \"\"\"\n        \n        \"\"\"\n        es = connections.get_connection(self._using)\n\n        self._response = self._response_class(\n            self,\n            es.update_by_query(\n                index=self._index,\n                body=self.to_dict(),\n                **self._params\n            )\n        )\n        return self._response", "output": "Execute the search and return an instance of ``Response`` wrapping all\n        the data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_external_data(tensor, base_path):  # type: (TensorProto, Text) -> None\n    \"\"\"\n    \n    \"\"\"\n    info = ExternalDataInfo(tensor)\n    external_data_file_path = os.path.join(base_path, info.location)\n\n    # Retrieve the tensor's data from raw_data or load external file\n    if not tensor.HasField(\"raw_data\"):\n        raise ValueError(\"raw_data field doesn't exist.\")\n\n    # Create file if it doesn't exist\n    if not os.path.isfile(external_data_file_path):\n        open(external_data_file_path, 'ab').close()\n\n    # Open file for reading and writing at random locations ('r+b')\n    with open(external_data_file_path, 'r+b') as data_file:\n        data_file.seek(0, 2)\n        if info.offset is not None:\n            # Pad file to required offset if needed\n            file_size = data_file.tell()\n            if info.offset > file_size:\n                data_file.write(b\"\\0\" * (info.offset - file_size))\n\n            data_file.seek(info.offset)\n        offset = data_file.tell()\n        data_file.write(tensor.raw_data)\n        set_external_data(tensor, info.location, offset, data_file.tell() - offset)", "output": "Write tensor data to an external file according to information in the `external_data` field.\n\n    @params\n    tensor: Tensor object to be serialized\n    base_path: System path of a folder where tensor data is to be stored", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_corrected_commands(command):\n    \"\"\"\n\n    \"\"\"\n    corrected_commands = (\n        corrected for rule in get_rules()\n        if rule.is_match(command)\n        for corrected in rule.get_corrected_commands(command))\n    return organize_commands(corrected_commands)", "output": "Returns generator with sorted and unique corrected commands.\n\n    :type command: thefuck.types.Command\n    :rtype: Iterable[thefuck.types.CorrectedCommand]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def open_file(self, fname, external=False):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        fname = to_text_string(fname)\r\n        ext = osp.splitext(fname)[1]\r\n        if encoding.is_text_file(fname):\r\n            self.editor.load(fname)\r\n        elif self.variableexplorer is not None and ext in IMPORT_EXT:\r\n            self.variableexplorer.import_data(fname)\r\n        elif not external:\r\n            fname = file_uri(fname)\r\n            programs.start_file(fname)", "output": "Open filename with the appropriate application\r\n        Redirect to the right widget (txt -> editor, spydata -> workspace, ...)\r\n        or open file outside Spyder (if extension is not supported)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_all(self):\n        '''\n        \n        '''\n        for status, keys in six.iteritems(self.list_keys()):\n            for key in keys:\n                try:\n                    os.remove(os.path.join(self.opts['pki_dir'], status, key))\n                    eload = {'result': True,\n                             'act': 'delete',\n                             'id': key}\n                    self.event.fire_event(eload,\n                                          salt.utils.event.tagify(prefix='key'))\n                except (OSError, IOError):\n                    pass\n        self.check_minion_cache()\n        if self.opts.get('rotate_aes_key'):\n            salt.crypt.dropfile(self.opts['cachedir'], self.opts['user'])\n        return self.list_keys()", "output": "Delete all keys", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _lockcmd(subcmd, pkgname=None, **kwargs):\n    '''\n    \n    '''\n\n    jail = kwargs.pop('jail', None)\n    chroot = kwargs.pop('chroot', None)\n    root = kwargs.pop('root', None)\n\n    locked_pkgs = []\n\n    cmd = _pkg(jail, chroot, root)\n    cmd.append(subcmd)\n    cmd.append('-y')\n    cmd.append('--quiet')\n    cmd.append('--show-locked')\n\n    if pkgname:\n        cmd.append(pkgname)\n\n    out = __salt__['cmd.run_all'](cmd, output_loglevel='trace', python_shell=False)\n\n    if out['retcode'] != 0:\n        raise CommandExecutionError(\n            'Problem encountered {0}ing packages'.format(subcmd),\n            info={'result': out}\n        )\n\n    for line in salt.utils.itertools.split(out['stdout'], '\\n'):\n        if not line:\n            continue\n        try:\n            pkgname = line.rsplit('-', 1)[0]\n        except ValueError:\n            continue\n        locked_pkgs.append(pkgname)\n\n    log.debug('Locked packages: %s', ', '.join(locked_pkgs))\n    return locked_pkgs", "output": "Helper function for lock and unlock commands, because their syntax is identical.\n\n    Run the lock/unlock command, and return a list of locked packages", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _depth_limited_walk(top, max_depth=None):\n    '''\n    \n    '''\n    for root, dirs, files in salt.utils.path.os_walk(top):\n        if max_depth is not None:\n            rel_depth = root.count(os.path.sep) - top.count(os.path.sep)\n            if rel_depth >= max_depth:\n                del dirs[:]\n        yield (six.text_type(root), list(dirs), list(files))", "output": "Walk the directory tree under root up till reaching max_depth.\n    With max_depth=None (default), do not limit depth.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(self, fnames=None):\r\n        \"\"\"\"\"\"\r\n        if fnames is None:\r\n            fnames = self.get_selected_filenames()\r\n        multiple = len(fnames) > 1\r\n        yes_to_all = None\r\n        for fname in fnames:\r\n            spyproject_path = osp.join(fname,'.spyproject')\r\n            if osp.isdir(fname) and osp.exists(spyproject_path):\r\n                QMessageBox.information(self, _('File Explorer'),\r\n                                        _(\"The current directory contains a \"\r\n                                        \"project.<br><br>\"\r\n                                        \"If you want to delete\"\r\n                                        \" the project, please go to \"\r\n                                        \"<b>Projects</b> &raquo; <b>Delete \"\r\n                                        \"Project</b>\"))\r\n            else:    \r\n                yes_to_all = self.delete_file(fname, multiple, yes_to_all)\r\n                if yes_to_all is not None and not yes_to_all:\r\n                    # Canceled\r\n                    break", "output": "Delete files", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def AVEDEV(Series, N):\n    \"\"\"\n    \n    \"\"\"\n    return Series.rolling(N).apply(lambda x: (np.abs(x - x.mean())).mean(), raw=True)", "output": "\u5e73\u5747\u7edd\u5bf9\u504f\u5dee mean absolute deviation\n    \u4fee\u6b63: 2018-05-25 \n\n    \u4e4b\u524d\u7528mad\u7684\u8ba1\u7b97\u6a21\u5f0f\u4f9d\u7136\u8fd4\u56de\u7684\u662f\u5355\u503c", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def longest_non_repeat_v2(string):\n    \"\"\"\n    \n    \"\"\"\n    if string is None:\n        return 0\n    start, max_len = 0, 0\n    used_char = {}\n    for index, char in enumerate(string):\n        if char in used_char and start <= used_char[char]:\n            start = used_char[char] + 1\n        else:\n            max_len = max(max_len, index - start + 1)\n        used_char[char] = index\n    return max_len", "output": "Find the length of the longest substring\n    without repeating characters.\n    Uses alternative algorithm.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def source(self, fields=None, **kwargs):\n        \"\"\"\n        \n\n        \"\"\"\n        s = self._clone()\n\n        if fields and kwargs:\n            raise ValueError(\"You cannot specify fields and kwargs at the same time.\")\n\n        if fields is not None:\n            s._source = fields\n            return s\n\n        if kwargs and not isinstance(s._source, dict):\n            s._source = {}\n\n        for key, value in kwargs.items():\n            if value is None:\n                try:\n                    del s._source[key]\n                except KeyError:\n                    pass\n            else:\n                s._source[key] = value\n\n        return s", "output": "Selectively control how the _source field is returned.\n\n        :arg fields: wildcard string, array of wildcards, or dictionary of includes and excludes\n\n        If ``fields`` is None, the entire document will be returned for\n        each hit.  If fields is a dictionary with keys of 'include' and/or\n        'exclude' the fields will be either included or excluded appropriately.\n\n        Calling this multiple times with the same named parameter will override the\n        previous values with the new ones.\n\n        Example::\n\n            s = Search()\n            s = s.source(include=['obj1.*'], exclude=[\"*.description\"])\n\n            s = Search()\n            s = s.source(include=['obj1.*']).source(exclude=[\"*.description\"])", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_json(cls, filename):\n        \"\"\"\n        \n\n        \"\"\"\n        proxy = UnitySArrayProxy()\n        proxy.load_from_json_record_files(_make_internal_url(filename))\n        return cls(_proxy = proxy)", "output": "Construct an SArray from a json file or glob of json files.\n        The json file must contain a list of dictionaries. The returned\n        SArray type will be of dict type\n\n        Parameters\n        ----------\n        filename : str\n          The filename or glob to load into an SArray.\n\n        Examples\n        --------\n        Construct an SArray from a local JSON file named 'data.json':\n\n        >>> turicreate.SArray.read_json('/data/data.json')\n\n        Construct an SArray from all JSON files /data/data*.json\n\n        >>> turicreate.SArray.read_json('/data/data*.json')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uniform(low:Number, high:Number=None, size:Optional[List[int]]=None)->FloatOrTensor:\n    \"\"\n    if high is None: high=low\n    return random.uniform(low,high) if size is None else torch.FloatTensor(*listify(size)).uniform_(low,high)", "output": "Draw 1 or shape=`size` random floats from uniform dist: min=`low`, max=`high`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mouseMoveEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        text = self.get_line_at(event.pos())\r\n        if get_error_match(text):\r\n            if not self.__cursor_changed:\r\n                QApplication.setOverrideCursor(QCursor(Qt.PointingHandCursor))\r\n                self.__cursor_changed = True\r\n            event.accept()\r\n            return\r\n        if self.__cursor_changed:\r\n            QApplication.restoreOverrideCursor()\r\n            self.__cursor_changed = False\r\n        self.QT_CLASS.mouseMoveEvent(self, event)", "output": "Show Pointing Hand Cursor on error messages", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_tile(node, **kwargs):\n    \"\"\"\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    reps_list = convert_string_to_list(attrs[\"reps\"])\n\n    initializer = kwargs[\"initializer\"]\n    reps_shape_np = np.array(reps_list, dtype='int64')\n    data_type = onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[reps_shape_np.dtype]\n    dims = np.shape(reps_shape_np)\n\n    output_shape_name = \"reps_attr_tensor\" + str(kwargs[\"idx\"])\n    tensor_node = onnx.helper.make_tensor_value_info(output_shape_name, data_type, dims)\n\n    initializer.append(\n        onnx.helper.make_tensor(\n            name=output_shape_name,\n            data_type=data_type,\n            dims=dims,\n            vals=reps_list,\n            raw=False,\n        )\n    )\n\n    input_nodes.append(output_shape_name)\n    tile_node = onnx.helper.make_node(\n        \"Tile\",\n        input_nodes,\n        [name],\n        name=name\n    )\n\n    return [tensor_node, tile_node]", "output": "Map MXNet's Tile operator attributes to onnx's Tile\n    operator and return the created node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sync_all(name, **kwargs):\n    '''\n    \n    '''\n    ret = {'name': name, 'changes': {}, 'result': True, 'comment': ''}\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = \"saltutil.sync_all would have been run\"\n        return ret\n\n    try:\n        sync_status = __salt__['saltutil.sync_all'](**kwargs)\n        for key, value in sync_status.items():\n            if value:\n                ret['changes'][key] = value\n                ret['comment'] = \"Sync performed\"\n    except Exception as e:\n        log.error(\"Failed to run saltutil.sync_all: %s\", e)\n        ret['result'] = False\n        ret['comment'] = \"Failed to run sync_all: {0}\".format(e)\n        return ret\n\n    if not ret['changes']:\n        ret['comment'] = \"No updates to sync\"\n\n    return ret", "output": "Performs the same task as saltutil.sync_all module\n    See :mod:`saltutil module for full list of options <salt.modules.saltutil>`\n\n    .. code-block:: yaml\n\n        sync_everything:\n          saltutil.sync_all:\n            - refresh: True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_ner(tag):\n    \"\"\"\n    \n    \"\"\"\n    tag_match = re.match(\"([A-Z_]+)-([A-Z_]+)\", tag)\n    if tag_match:\n        return True\n    elif tag == \"O\":\n        return True\n    else:\n        return False", "output": "Check the 10th column of the first token to determine if the file contains\n    NER tags", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_conv_weight(layer):\n    '''\n    '''\n    n_filters = layer.filters\n    filter_shape = (layer.kernel_size,) * get_n_dim(layer)\n    weight = np.zeros((n_filters, n_filters) + filter_shape)\n\n    center = tuple(map(lambda x: int((x - 1) / 2), filter_shape))\n    for i in range(n_filters):\n        filter_weight = np.zeros((n_filters,) + filter_shape)\n        index = (i,) + center\n        filter_weight[index] = 1\n        weight[i, ...] = filter_weight\n    bias = np.zeros(n_filters)\n\n    layer.set_weights(\n        (add_noise(weight, np.array([0, 1])), add_noise(bias, np.array([0, 1])))\n    )", "output": "initilize conv layer weight.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ints_to_td64ns(data, unit=\"ns\"):\n    \"\"\"\n    \n    \"\"\"\n    copy_made = False\n    unit = unit if unit is not None else \"ns\"\n\n    if data.dtype != np.int64:\n        # converting to int64 makes a copy, so we can avoid\n        # re-copying later\n        data = data.astype(np.int64)\n        copy_made = True\n\n    if unit != \"ns\":\n        dtype_str = \"timedelta64[{unit}]\".format(unit=unit)\n        data = data.view(dtype_str)\n\n        # TODO: watch out for overflows when converting from lower-resolution\n        data = data.astype(\"timedelta64[ns]\")\n        # the astype conversion makes a copy, so we can avoid re-copying later\n        copy_made = True\n\n    else:\n        data = data.view(\"timedelta64[ns]\")\n\n    return data, copy_made", "output": "Convert an ndarray with integer-dtype to timedelta64[ns] dtype, treating\n    the integers as multiples of the given timedelta unit.\n\n    Parameters\n    ----------\n    data : numpy.ndarray with integer-dtype\n    unit : str, default \"ns\"\n        The timedelta unit to treat integers as multiples of.\n\n    Returns\n    -------\n    numpy.ndarray : timedelta64[ns] array converted from data\n    bool : whether a copy was made", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_jid(jid):\n    '''\n    \n    '''\n    query = '''SELECT minion_id, full_ret FROM {keyspace}.salt_returns\n               WHERE jid = ?;'''.format(keyspace=_get_keyspace())\n\n    ret = {}\n\n    # cassandra_cql.cql_query may raise a CommandExecutionError\n    try:\n        data = __salt__['cassandra_cql.cql_query_with_prepare'](query, 'get_jid', [jid])\n        if data:\n            for row in data:\n                minion = row.get('minion_id')\n                full_ret = row.get('full_ret')\n                if minion and full_ret:\n                    ret[minion] = salt.utils.json.loads(full_ret)\n    except CommandExecutionError:\n        log.critical('Could not select job specific information.')\n        raise\n    except Exception as e:\n        log.critical(\n            'Unexpected error while getting job specific information: %s', e)\n        raise\n\n    return ret", "output": "Return the information returned when the specified job id was executed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode_value(value, client):\n    \"\"\"\n    \"\"\"\n    value_type = value.WhichOneof(\"value_type\")\n\n    if value_type == \"null_value\":\n        return None\n    elif value_type == \"boolean_value\":\n        return value.boolean_value\n    elif value_type == \"integer_value\":\n        return value.integer_value\n    elif value_type == \"double_value\":\n        return value.double_value\n    elif value_type == \"timestamp_value\":\n        return DatetimeWithNanoseconds.from_timestamp_pb(value.timestamp_value)\n    elif value_type == \"string_value\":\n        return value.string_value\n    elif value_type == \"bytes_value\":\n        return value.bytes_value\n    elif value_type == \"reference_value\":\n        return reference_value_to_document(value.reference_value, client)\n    elif value_type == \"geo_point_value\":\n        return GeoPoint(value.geo_point_value.latitude, value.geo_point_value.longitude)\n    elif value_type == \"array_value\":\n        return [decode_value(element, client) for element in value.array_value.values]\n    elif value_type == \"map_value\":\n        return decode_dict(value.map_value.fields, client)\n    else:\n        raise ValueError(\"Unknown ``value_type``\", value_type)", "output": "Converts a Firestore protobuf ``Value`` to a native Python value.\n\n    Args:\n        value (google.cloud.firestore_v1beta1.types.Value): A\n            Firestore protobuf to be decoded / parsed / converted.\n        client (~.firestore_v1beta1.client.Client): A client that has\n            a document factory.\n\n    Returns:\n        Union[NoneType, bool, int, float, datetime.datetime, \\\n            str, bytes, dict, ~google.cloud.Firestore.GeoPoint]: A native\n        Python value converted from the ``value``.\n\n    Raises:\n        NotImplementedError: If the ``value_type`` is ``reference_value``.\n        ValueError: If the ``value_type`` is unknown.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, resource):\n        \"\"\"\n        \"\"\"\n        action = resource[\"action\"]\n        instance = cls(action[\"storageClass\"], _factory=True)\n        instance.update(resource)\n        return instance", "output": "Factory:  construct instance from resource.\n\n        :type resource: dict\n        :param resource: mapping as returned from API call.\n\n        :rtype: :class:`LifecycleRuleDelete`\n        :returns: Instance created from resource.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_lr_mult(self, args_lr_mult):\n        \"\"\"\n        \"\"\"\n        self.lr_mult = {}\n        if self.sym_info:\n            attr, arg_names = self.sym_info\n            for name in arg_names:\n                if name in attr and '__lr_mult__' in attr[name]:\n                    self.lr_mult[name] = float(attr[name]['__lr_mult__'])\n        self.lr_mult.update(args_lr_mult)", "output": "Sets an individual learning rate multiplier for each parameter.\n\n        If you specify a learning rate multiplier for a parameter, then\n        the learning rate for the parameter will be set as the product of\n        the global learning rate `self.lr` and its multiplier.\n\n        .. note:: The default learning rate multiplier of a `Variable`\n            can be set with `lr_mult` argument in the constructor.\n\n        Parameters\n        ----------\n        args_lr_mult : dict of str/int to float\n            For each of its key-value entries, the learning rate multipler for the\n            parameter specified in the key will be set as the given value.\n\n            You can specify the parameter with either its name or its index.\n            If you use the name, you should pass `sym` in the constructor,\n            and the name you specified in the key of `args_lr_mult` should match\n            the name of the parameter in `sym`. If you use the index, it should\n            correspond to the index of the parameter used in the `update` method.\n\n            Specifying a parameter by its index is only supported for backward\n            compatibility, and we recommend to use the name instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _normalize_batch(b:Tuple[Tensor,Tensor], mean:FloatTensor, std:FloatTensor, do_x:bool=True, do_y:bool=False)->Tuple[Tensor,Tensor]:\n    \"\"\n    x,y = b\n    mean,std = mean.to(x.device),std.to(x.device)\n    if do_x: x = normalize(x,mean,std)\n    if do_y and len(y.shape) == 4: y = normalize(y,mean,std)\n    return x,y", "output": "`b` = `x`,`y` - normalize `x` array of imgs and `do_y` optionally `y`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetch(self, log_group_name, start=None, end=None, filter_pattern=None):\n        \"\"\"\n        \n        \"\"\"\n\n        kwargs = {\n            \"logGroupName\": log_group_name,\n            \"interleaved\": True\n        }\n\n        if start:\n            kwargs[\"startTime\"] = to_timestamp(start)\n\n        if end:\n            kwargs[\"endTime\"] = to_timestamp(end)\n\n        if filter_pattern:\n            kwargs[\"filterPattern\"] = filter_pattern\n\n        while True:\n            LOG.debug(\"Fetching logs from CloudWatch with parameters %s\", kwargs)\n            result = self.cw_client.filter_log_events(**kwargs)\n\n            # Several events will be returned. Yield one at a time\n            for event in result.get('events', []):\n                yield LogEvent(log_group_name, event)\n\n            # Keep iterating until there are no more logs left to query.\n            next_token = result.get(\"nextToken\", None)\n            kwargs[\"nextToken\"] = next_token\n            if not next_token:\n                break", "output": "Fetch logs from all streams under the given CloudWatch Log Group and yields in the output. Optionally, caller\n        can filter the logs using a pattern or a start/end time.\n\n        Parameters\n        ----------\n        log_group_name : string\n            Name of CloudWatch Logs Group to query.\n\n        start : datetime.datetime\n            Optional start time for logs.\n\n        end : datetime.datetime\n            Optional end time for logs.\n\n        filter_pattern : str\n            Expression to filter the logs by. This is passed directly to CloudWatch, so any expression supported by\n            CloudWatch Logs API is supported here.\n\n        Yields\n        ------\n\n        samcli.lib.logs.event.LogEvent\n            Object containing the information from each log event returned by CloudWatch Logs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def print_header(text):\n  \"\"\"\"\"\"\n  print()\n  print('#'*(len(text)+4))\n  print('# ' + text + ' #')\n  print('#'*(len(text)+4))\n  print()", "output": "Prints header with given text and frame composed of '#' characters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xmoe2_v1():\n  \"\"\"\n  \"\"\"\n  hparams = xmoe2_dense(0)\n  moe.set_default_moe_hparams(hparams)\n  hparams.decoder_layers = (\n      [\"local_att\", \"local_att\", \"drd\",\n       \"att\", \"drd\", \"local_att\", \"local_att\", \"hmoe\"] * 4)[:-1]\n  hparams.d_ff = 2048\n  hparams.d_kv = 128\n  hparams.moe_hidden_size = 32768\n  hparams.mesh_shape = \"b0:4;b1:8\"\n  hparams.layout = \"outer_batch:b0;inner_batch:b1,expert_x:b1,expert_y:b0\"\n  hparams.outer_batch_size = 4\n  hparams.moe_num_experts = [8, 4]\n  hparams.num_heads = 4\n  return hparams", "output": "Model incorporating mixture-of-experts and local-attention.\n\n  ~6B parameters\n\n  32 experts in 3 hierarchichal moe layers.\n\n  Returns:\n    a hparams", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def source_file_name(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            fname = inspect.getsourcefile(self.code_obj)\n        except TypeError:\n            # In some cases the object is something complex like a cython\n            # object that can't be easily introspected. An it's better to\n            # return the source code file of the object as None, than crash\n            pass\n        else:\n            if fname:\n                fname = os.path.relpath(fname, BASE_PATH)\n                return fname", "output": "File name where the object is implemented (e.g. pandas/core/frame.py).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_template(self, name=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_template\", name), params=params\n        )", "output": "Retrieve an index template by its name.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html>`_\n\n        :arg name: The name of the template\n        :arg flat_settings: Return settings in flat format (default: false)\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg include_type_name: Specify whether requests and responses should include a\n            type name (default: depends on Elasticsearch version).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(overlay):\n    '''\n    \n    '''\n    ret = list()\n    old_overlays = list_local()\n    cmd = 'layman --quietness=0 --add {0}'.format(overlay)\n    add_attempt = __salt__['cmd.run_all'](cmd, python_shell=False, stdin='y')\n    if add_attempt['retcode'] != 0:\n        raise salt.exceptions.CommandExecutionError(add_attempt['stdout'])\n    new_overlays = list_local()\n\n    # If we did not have any overlays before and we successfully added\n    # a new one. We need to ensure the make.conf is sourcing layman's\n    # make.conf so emerge can see the overlays\n    if not old_overlays and new_overlays:\n        srcline = 'source /var/lib/layman/make.conf'\n        makeconf = _get_makeconf()\n        if not __salt__['file.contains'](makeconf, 'layman'):\n            __salt__['file.append'](makeconf, srcline)\n\n    ret = [overlay for overlay in new_overlays if overlay not in old_overlays]\n    return ret", "output": "Add the given overlay from the cached remote list to your locally\n    installed overlays. Specify 'ALL' to add all overlays from the\n    remote list.\n\n    Return a list of the new overlay(s) added:\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' layman.add <overlay name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tfrecord_iterator_for_problem(problem, data_dir,\n                                  dataset_split=tf.estimator.ModeKeys.TRAIN):\n  \"\"\"\"\"\"\n  filenames = tf.gfile.Glob(problem.filepattern(data_dir, mode=dataset_split))\n  example_spec = problem.example_reading_spec()[0]\n  return tfrecord_iterator(filenames, example_spec=example_spec)", "output": "Iterate over the records on disk for the Problem.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_util_calc_time(func, *args, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    _time = datetime.datetime.now()\n    func(*args, **kwargs)\n    print(datetime.datetime.now() - _time)", "output": "'\u8017\u65f6\u957f\u5ea6\u7684\u88c5\u9970\u5668'\n    :param func:\n    :param args:\n    :param kwargs:\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _microseconds_from_datetime(value):\n    \"\"\"\n    \"\"\"\n    if not value.tzinfo:\n        value = value.replace(tzinfo=UTC)\n    # Regardless of what timezone is on the value, convert it to UTC.\n    value = value.astimezone(UTC)\n    # Convert the datetime to a microsecond timestamp.\n    return int(calendar.timegm(value.timetuple()) * 1e6) + value.microsecond", "output": "Convert non-none datetime to microseconds.\n\n    :type value: :class:`datetime.datetime`\n    :param value: The timestamp to convert.\n\n    :rtype: int\n    :returns: The timestamp, in microseconds.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_update_from_file(mode='create', uuid=None, path=None):\n    '''\n    \n    '''\n    ret = {}\n    if not os.path.isfile(path) or path is None:\n        ret['Error'] = 'File ({0}) does not exists!'.format(path)\n        return ret\n    # vmadm validate create|update [-f <filename>]\n    cmd = 'vmadm validate {mode} {brand} -f {path}'.format(\n        mode=mode,\n        brand=get(uuid)['brand'] if uuid is not None else '',\n        path=path\n    )\n    res = __salt__['cmd.run_all'](cmd)\n    retcode = res['retcode']\n    if retcode != 0:\n        ret['Error'] = _exit_status(retcode)\n        if 'stderr' in res:\n            if res['stderr'][0] == '{':\n                ret['Error'] = salt.utils.json.loads(res['stderr'])\n            else:\n                ret['Error'] = res['stderr']\n        return ret\n    # vmadm create|update [-f <filename>]\n    cmd = 'vmadm {mode} {uuid} -f {path}'.format(\n        mode=mode,\n        uuid=uuid if uuid is not None else '',\n        path=path\n    )\n    res = __salt__['cmd.run_all'](cmd)\n    retcode = res['retcode']\n    if retcode != 0:\n        ret['Error'] = _exit_status(retcode)\n        if 'stderr' in res:\n            if res['stderr'][0] == '{':\n                ret['Error'] = salt.utils.json.loads(res['stderr'])\n            else:\n                ret['Error'] = res['stderr']\n        return ret\n    else:\n        if res['stderr'].startswith('Successfully created VM'):\n            return res['stderr'][24:]\n    return True", "output": "Create vm from file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def notification_channel_path(cls, project, notification_channel):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/notificationChannels/{notification_channel}\",\n            project=project,\n            notification_channel=notification_channel,\n        )", "output": "Return a fully-qualified notification_channel string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_tall_pretrain_lm_tpu_adafactor_large():\n  \"\"\"\"\"\"\n  hparams = transformer_tall_pretrain_lm_tpu_adafactor()\n  hparams.hidden_size = 1024\n  hparams.num_heads = 16\n  hparams.filter_size = 32768  # max fitting in 16G memory is 49152, batch 2\n  hparams.batch_size = 4\n  hparams.multiproblem_mixing_schedule = \"constant\"\n  # Task order: lm/en-de/en-fr/en-ro/de-en/fr-en/ro-en/cnndm/mnli/squad.\n  hparams.multiproblem_per_task_threshold = \"320,80,160,1,80,160,2,20,10,5\"\n  return hparams", "output": "Hparams for transformer on LM pretraining on TPU, large model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def serialize_array(array, domain=(0, 1), fmt='png', quality=70):\n  \"\"\"\n  \"\"\"\n  normalized = _normalize_array(array, domain=domain)\n  return _serialize_normalized_array(normalized, fmt=fmt, quality=quality)", "output": "Given an arbitrary rank-3 NumPy array,\n  returns the byte representation of the encoded image.\n\n  Args:\n    array: NumPy array of dtype uint8 and range 0 to 255\n    domain: expected range of values in array, see `_normalize_array()`\n    fmt: string describing desired file format, defaults to 'png'\n    quality: specifies compression quality from 0 to 100 for lossy formats\n\n  Returns:\n    image data as BytesIO buffer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _execute_and_seal_error(method, arg, method_name):\n    \"\"\"\n    \"\"\"\n    try:\n        return method(arg)\n    except Exception:\n        return ray.worker.RayTaskError(method_name, traceback.format_exc())", "output": "Execute method with arg and return the result.\n\n    If the method fails, return a RayTaskError so it can be sealed in the\n    resultOID and retried by user.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_common_actions(self):\r\n        \"\"\"\"\"\"\r\n        self.collapse_all_action = create_action(self,\r\n                                     text=_('Collapse all'),\r\n                                     icon=ima.icon('collapse'),\r\n                                     triggered=self.collapseAll)\r\n        self.expand_all_action = create_action(self,\r\n                                     text=_('Expand all'),\r\n                                     icon=ima.icon('expand'),\r\n                                     triggered=self.expandAll)\r\n        self.restore_action = create_action(self,\r\n                                     text=_('Restore'),\r\n                                     tip=_('Restore original tree layout'),\r\n                                     icon=ima.icon('restore'),\r\n                                     triggered=self.restore)\r\n        self.collapse_selection_action = create_action(self,\r\n                                     text=_('Collapse selection'),\r\n                                     icon=ima.icon('collapse_selection'),\r\n                                     triggered=self.collapse_selection)\r\n        self.expand_selection_action = create_action(self,\r\n                                     text=_('Expand selection'),\r\n                                     icon=ima.icon('expand_selection'),\r\n                                     triggered=self.expand_selection)\r\n        return [self.collapse_all_action, self.expand_all_action,\r\n                self.restore_action, None,\r\n                self.collapse_selection_action, self.expand_selection_action]", "output": "Setup context menu common actions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def commitAndCloseEditor(self):\r\n        \"\"\"\"\"\"\r\n        editor = self.sender()\r\n        # Avoid a segfault with PyQt5. Variable value won't be changed\r\n        # but at least Spyder won't crash. It seems generated by a bug in sip.\r\n        try:\r\n            self.commitData.emit(editor)\r\n        except AttributeError:\r\n            pass\r\n        self.closeEditor.emit(editor, QAbstractItemDelegate.NoHint)", "output": "Commit and close editor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_against_chunks(self, chunks):\n        # type: (Iterator[bytes]) -> None\n        \"\"\"\n\n        \"\"\"\n        gots = {}\n        for hash_name in iterkeys(self._allowed):\n            try:\n                gots[hash_name] = hashlib.new(hash_name)\n            except (ValueError, TypeError):\n                raise InstallationError('Unknown hash name: %s' % hash_name)\n\n        for chunk in chunks:\n            for hash in itervalues(gots):\n                hash.update(chunk)\n\n        for hash_name, got in iteritems(gots):\n            if got.hexdigest() in self._allowed[hash_name]:\n                return\n        self._raise(gots)", "output": "Check good hashes against ones built from iterable of chunks of\n        data.\n\n        Raise HashMismatch if none match.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_continuous_query(database, name, **client_args):\n    '''\n    \n    '''\n    client = _client(**client_args)\n\n    try:\n        for db, cqs in client.query('SHOW CONTINUOUS QUERIES').items():\n            if db[0] == database:\n                return next((cq for cq in cqs if cq.get('name') == name))\n    except StopIteration:\n        return {}\n    return {}", "output": "Get an existing continuous query.\n\n    database\n        Name of the database for which the continuous query was\n        defined.\n\n    name\n        Name of the continuous query to get.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb.get_continuous_query mydb cq_month", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kernels_push_cli(self, folder):\n        \"\"\" \n        \"\"\"\n        folder = folder or os.getcwd()\n        result = self.kernels_push(folder)\n\n        if result is None:\n            print('Kernel push error: see previous output')\n        elif not result.error:\n            if result.invalidTags:\n                print(\n                    'The following are not valid tags and could not be added '\n                    'to the kernel: ' + str(result.invalidTags))\n            if result.invalidDatasetSources:\n                print(\n                    'The following are not valid dataset sources and could not '\n                    'be added to the kernel: ' +\n                    str(result.invalidDatasetSources))\n            if result.invalidCompetitionSources:\n                print(\n                    'The following are not valid competition sources and could '\n                    'not be added to the kernel: ' +\n                    str(result.invalidCompetitionSources))\n            if result.invalidKernelSources:\n                print(\n                    'The following are not valid kernel sources and could not '\n                    'be added to the kernel: ' +\n                    str(result.invalidKernelSources))\n\n            if result.versionNumber:\n                print('Kernel version %s successfully pushed.  Please check '\n                      'progress at %s' % (result.versionNumber, result.url))\n            else:\n                # Shouldn't happen but didn't test exhaustively\n                print('Kernel version successfully pushed.  Please check '\n                      'progress at %s' % result.url)\n        else:\n            print('Kernel push error: ' + result.error)", "output": "client wrapper for kernels_push, with same arguments.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_pb(self):\n        \"\"\"\n        \"\"\"\n        intersection = table_v2_pb2.GcRule.Intersection(\n            rules=[rule.to_pb() for rule in self.rules]\n        )\n        return table_v2_pb2.GcRule(intersection=intersection)", "output": "Converts the intersection into a single GC rule as a protobuf.\n\n        :rtype: :class:`.table_v2_pb2.GcRule`\n        :returns: The converted current object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def export_datasource_schema(back_references):\n    \"\"\"\"\"\"\n    data = dict_import_export.export_schema_to_dict(\n        back_references=back_references)\n    yaml.safe_dump(data, stdout, default_flow_style=False)", "output": "Export datasource YAML schema to stdout", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_json(self, json, models=None):\n        ''' \n        '''\n        if json is None:\n            return None\n        elif not isinstance(json, dict):\n            raise DeserializationError(\"%s expected a dict or None, got %s\" % (self, json))\n        new_data = {}\n        for key, value in json.items():\n            key = self.keys_type.from_json(key, models)\n            if isinstance(value, dict) and '__ndarray__' in value:\n                new_data[key] = decode_base64_dict(value)\n            elif isinstance(value, list) and any(isinstance(el, dict) and '__ndarray__' in el for el in value):\n                new_list = []\n                for el in value:\n                    if isinstance(el, dict) and '__ndarray__' in el:\n                        el = decode_base64_dict(el)\n                    elif isinstance(el, list):\n                        el = self.values_type.from_json(el)\n                    new_list.append(el)\n                new_data[key] = new_list\n            else:\n                new_data[key] = self.values_type.from_json(value, models)\n        return new_data", "output": "Decodes column source data encoded as lists or base64 strings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setval(key, val, dict_=None, delim=DEFAULT_TARGET_DELIM):\n    '''\n    \n    '''\n    if not dict_:\n        dict_ = {}\n    prev_hier = dict_\n    dict_hier = key.split(delim)\n    for each in dict_hier[:-1]:\n        if each not in prev_hier:\n            prev_hier[each] = {}\n        prev_hier = prev_hier[each]\n    prev_hier[dict_hier[-1]] = copy.deepcopy(val)\n    return dict_", "output": "Set a value under the dictionary hierarchy identified\n    under the key. The target 'foo/bar/baz' returns the\n    dictionary hierarchy {'foo': {'bar': {'baz': {}}}}.\n\n    .. note::\n\n        Currently this doesn't work with integers, i.e.\n        cannot build lists dynamically.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' formula.setval foo:baz:bar True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_def(self, text):\r\n        \"\"\"\"\"\"\r\n        self.__init__()\r\n\r\n        if not is_start_of_function(text):\r\n            return\r\n\r\n        self.func_indent = get_indent(text)\r\n\r\n        text = text.strip()\r\n        text = text.replace('\\r\\n', '')\r\n        text = text.replace('\\n', '')\r\n\r\n        return_type_re = re.search(r'->[ ]*([a-zA-Z0-9_,()\\[\\] ]*):$', text)\r\n        if return_type_re:\r\n            self.return_type_annotated = return_type_re.group(1)\r\n            text_end = text.rfind(return_type_re.group(0))\r\n        else:\r\n            self.return_type_annotated = None\r\n            text_end = len(text)\r\n\r\n        pos_args_start = text.find('(') + 1\r\n        pos_args_end = text.rfind(')', pos_args_start, text_end)\r\n\r\n        self.args_text = text[pos_args_start:pos_args_end]\r\n\r\n        args_list = self.split_args_text_to_list(self.args_text)\r\n        if args_list is not None:\r\n            self.has_info = True\r\n            self.split_arg_to_name_type_value(args_list)", "output": "Parse the function definition text.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_info(shape_list, num_classes):\n  \"\"\"\"\"\"\n  feature_info = collections.namedtuple(\"FeatureInfo\", [\"shape\", \"num_classes\"])\n  cur_shape = list(shape_list[0])\n  # We need to merge the provided shapes, put None where they disagree.\n  for shape in shape_list:\n    if len(shape) != len(cur_shape):\n      raise ValueError(\"Shapes need to have the same number of dimensions.\")\n    for i in range(len(shape)):\n      if cur_shape[i] is not None:\n        if shape[i] != cur_shape[i]:\n          cur_shape[i] = None\n  return feature_info(cur_shape, num_classes)", "output": "Create an info-like tuple for feature given some shapes and vocab size.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(self):\n        \"\"\"\n        \"\"\"\n        if self._leaser is None:\n            return 0\n\n        return max(\n            [\n                self._leaser.message_count / self._flow_control.max_messages,\n                self._leaser.bytes / self._flow_control.max_bytes,\n            ]\n        )", "output": "Return the current load.\n\n        The load is represented as a float, where 1.0 represents having\n        hit one of the flow control limits, and values between 0.0 and 1.0\n        represent how close we are to them. (0.5 means we have exactly half\n        of what the flow control setting allows, for example.)\n\n        There are (currently) two flow control settings; this property\n        computes how close the manager is to each of them, and returns\n        whichever value is higher. (It does not matter that we have lots of\n        running room on setting A if setting B is over.)\n\n        Returns:\n            float: The load value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def output_names(self):\n        \"\"\"\"\"\"\n        if self.binded:\n            return self._curr_module.output_names\n        else:\n            symbol, _, _ = self._call_sym_gen(self._default_bucket_key)\n            return symbol.list_outputs()", "output": "A list of names for the outputs of this module.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def discard_local_changes(cwd,\n                          path='.',\n                          user=None,\n                          password=None,\n                          ignore_retcode=False,\n                          output_encoding=None):\n    '''\n    \n    '''\n    cwd = _expand_path(cwd, user)\n    command = ['git', 'checkout', '--', path]\n    # Checkout message goes to stderr\n    return _git_run(command,\n                    cwd=cwd,\n                    user=user,\n                    password=password,\n                    ignore_retcode=ignore_retcode,\n                    redirect_stderr=True,\n                    output_encoding=output_encoding)['stdout']", "output": ".. versionadded:: 2019.2.0\n\n    Runs a ``git checkout -- <path>`` from the directory specified by ``cwd``.\n\n    cwd\n        The path to the git checkout\n\n    path\n        path relative to cwd (defaults to ``.``)\n\n    user\n        User under which to run the git command. By default, the command is run\n        by the user under which the minion is running.\n\n    password\n        Windows only. Required when specifying ``user``. This parameter will be\n        ignored on non-Windows platforms.\n\n    ignore_retcode : False\n        If ``True``, do not log an error to the minion log if the git command\n        returns a nonzero exit status.\n\n    output_encoding\n        Use this option to specify which encoding to use to decode the output\n        from any git commands which are run. This should not be needed in most\n        cases.\n\n        .. note::\n            This should only be needed if the files in the repository were\n            created with filenames using an encoding other than UTF-8 to handle\n            Unicode characters.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion git.discard_local_changes /path/to/repo\n        salt myminion git.discard_local_changes /path/to/repo path=foo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def preprocess_for_eval(image, image_size=224, normalize=True):\n  \"\"\"\n  \"\"\"\n  if normalize: image = tf.to_float(image) / 255.0\n  image = _do_scale(image, image_size + 32)\n  if normalize: image = _normalize(image)\n  image = _center_crop(image, image_size)\n  image = tf.reshape(image, [image_size, image_size, 3])\n  return image", "output": "Preprocesses the given image for evaluation.\n\n  Args:\n    image: `Tensor` representing an image of arbitrary size.\n    image_size: int, how large the output image should be.\n    normalize: bool, if True the image is normalized.\n\n  Returns:\n    A preprocessed image `Tensor`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_files(package, conn=None):\n    '''\n    \n    '''\n    close = False\n    if conn is None:\n        close = True\n        conn = init()\n\n    data = conn.execute('SELECT package FROM packages WHERE package=?', (package, ))\n    if not data.fetchone():\n        if close:\n            conn.close()\n        return None\n\n    ret = []\n    data = conn.execute('SELECT path, sum FROM files WHERE package=?', (package, ))\n    for file_ in data.fetchall():\n        ret.append(file_)\n    if close:\n        conn.close()\n\n    return ret", "output": "List files for an installed package", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cancel_order(order):\n    \"\"\"\n    \n    \"\"\"\n    env = Environment.get_instance()\n    if env.can_cancel_order(order):\n        env.broker.cancel_order(order)\n    return order", "output": "\u64a4\u5355\n\n    :param order: \u9700\u8981\u64a4\u9500\u7684order\u5bf9\u8c61\n    :type order: :class:`~Order` object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def newDocText(self, content):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlNewDocText(self._o, content)\n        if ret is None:raise treeError('xmlNewDocText() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Creation of a new text node within a document.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set(ctx, key, value):\n    ''''''\n    file = ctx.obj['FILE']\n    quote = ctx.obj['QUOTE']\n    success, key, value = set_key(file, key, value, quote)\n    if success:\n        click.echo('%s=%s' % (key, value))\n    else:\n        exit(1)", "output": "Store the given key/value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_device_by_label(devices, label):\n    '''\n    \n    '''\n    device_labels = [d for d in devices if d.deviceInfo.label == label]\n    if device_labels:\n        return device_labels[0]\n    else:\n        raise salt.exceptions.VMwareObjectNotFoundError(\n            'Virtual machine device with '\n            'label {0} does not exist'.format(label))", "output": "Returns the device with the given label, raises error if the device is\n    not found.\n\n    devices\n        list of vim.vm.device.VirtualDevice objects\n\n    key\n        Unique key of device", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read_stderr(self):\r\n        \"\"\"\"\"\"\r\n        # We need to read stderr_file as bytes to be able to\r\n        # detect its encoding with chardet\r\n        f = open(self.stderr_file, 'rb')\r\n\r\n        try:\r\n            stderr_text = f.read()\r\n\r\n            # This is needed to avoid showing an empty error message\r\n            # when the kernel takes too much time to start.\r\n            # See issue 8581\r\n            if not stderr_text:\r\n                return ''\r\n\r\n            # This is needed since the stderr file could be encoded\r\n            # in something different to utf-8.\r\n            # See issue 4191\r\n            encoding = get_coding(stderr_text)\r\n            stderr_text = to_text_string(stderr_text, encoding)\r\n            return stderr_text\r\n        finally:\r\n            f.close()", "output": "Read the stderr file of the kernel.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _coo_to_sparse_series(A, dense_index=False):\n    \"\"\"\n    \n    \"\"\"\n    s = Series(A.data, MultiIndex.from_arrays((A.row, A.col)))\n    s = s.sort_index()\n    s = s.to_sparse()  # TODO: specify kind?\n    if dense_index:\n        # is there a better constructor method to use here?\n        i = range(A.shape[0])\n        j = range(A.shape[1])\n        ind = MultiIndex.from_product([i, j])\n        s = s.reindex(ind)\n    return s", "output": "Convert a scipy.sparse.coo_matrix to a SparseSeries.\n    Use the defaults given in the SparseSeries constructor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def base_to_int(s, base):\n    \"\"\"\n        \n    \"\"\"\n    \n    digit = {}\n    for i,c in enumerate(string.digits + string.ascii_uppercase):\n        digit[c] = i\n    multiplier = 1\n    res = 0\n    for c in s[::-1]:\n        res += digit[c] * multiplier\n        multiplier *= base\n    return res", "output": "Note : You can use int() built-in function instread of this.\n        :type s: str\n        :type base: int\n        :rtype: int", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def refine (self, requirements):\n        \"\"\" \n        \"\"\"\n        assert isinstance(requirements, PropertySet)\n        if requirements not in self.refined_:\n            r = property.refine(self.all_, requirements.all_)\n\n            self.refined_[requirements] = create(r)\n\n        return self.refined_[requirements]", "output": "Refines this set's properties using the requirements passed as an argument.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show(self, ax:plt.Axes=None, figsize:tuple=(3,3), title:Optional[str]=None, hide_axis:bool=True,\n              cmap:str=None, y:Any=None, **kwargs):\n        \"\"\n        cmap = ifnone(cmap, defaults.cmap)\n        ax = show_image(self, ax=ax, hide_axis=hide_axis, cmap=cmap, figsize=figsize)\n        if y is not None: y.show(ax=ax, **kwargs)\n        if title is not None: ax.set_title(title)", "output": "Show image on `ax` with `title`, using `cmap` if single-channel, overlaid with optional `y`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Tags(self):\n    \"\"\"\n    \"\"\"\n    return {\n        IMAGES: self.images.Keys(),\n        AUDIO: self.audios.Keys(),\n        HISTOGRAMS: self.histograms.Keys(),\n        SCALARS: self.scalars.Keys(),\n        COMPRESSED_HISTOGRAMS: self.compressed_histograms.Keys(),\n        TENSORS: self.tensors.Keys(),\n        # Use a heuristic: if the metagraph is available, but\n        # graph is not, then we assume the metagraph contains the graph.\n        GRAPH: self._graph is not None,\n        META_GRAPH: self._meta_graph is not None,\n        RUN_METADATA: list(self._tagged_metadata.keys())\n    }", "output": "Return all tags found in the value stream.\n\n    Returns:\n      A `{tagType: ['list', 'of', 'tags']}` dictionary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def config_shortcut(action, context, name, parent):\n    \"\"\"\n    \n    \"\"\"\n    keystr = get_shortcut(context, name)\n    qsc = QShortcut(QKeySequence(keystr), parent, action)\n    qsc.setContext(Qt.WidgetWithChildrenShortcut)\n    sc = Shortcut(data=(qsc, context, name))\n    return sc", "output": "Create a Shortcut namedtuple for a widget\n    \n    The data contained in this tuple will be registered in\n    our shortcuts preferences page", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def policy_backward(eph, epx, epdlogp, model):\n    \"\"\"\"\"\"\n    dW2 = np.dot(eph.T, epdlogp).ravel()\n    dh = np.outer(epdlogp, model[\"W2\"])\n    # Backprop relu.\n    dh[eph <= 0] = 0\n    dW1 = np.dot(dh.T, epx)\n    return {\"W1\": dW1, \"W2\": dW2}", "output": "backward pass. (eph is array of intermediate hidden states)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_native_types(self, slicer=None, na_rep=None, date_format=None,\n                        quoting=None, **kwargs):\n        \"\"\"  \"\"\"\n\n        values = self.values\n        i8values = self.values.view('i8')\n\n        if slicer is not None:\n            values = values[..., slicer]\n            i8values = i8values[..., slicer]\n\n        from pandas.io.formats.format import _get_format_datetime64_from_values\n        fmt = _get_format_datetime64_from_values(values, date_format)\n\n        result = tslib.format_array_from_datetime(\n            i8values.ravel(), tz=getattr(self.values, 'tz', None),\n            format=fmt, na_rep=na_rep).reshape(i8values.shape)\n        return np.atleast_2d(result)", "output": "convert to our native types format, slicing if desired", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def plot_val_with_title(self, idxs, y):\n        \"\"\" \n        \"\"\"\n        # if there are any samples to be displayed\n        if len(idxs) > 0:\n            imgs = np.stack([self.ds[x][0] for x in idxs])\n            title_probs = [self.probs[x,y] for x in idxs]\n\n            return plots(self.ds.denorm(imgs), rows=1, titles=title_probs)\n        # if idxs is empty return false\n        else:\n            return False;", "output": "Displays the images and their probabilities of belonging to a certain class\n\n            Arguments:\n                idxs (numpy.ndarray): indexes of the image samples from the dataset\n                y (int): the selected class\n\n            Returns:\n                Plots the images in n rows [rows = n]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resource_exists(resource, name=None, resource_id=None, tags=None,\n                    region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        return {'exists': bool(_find_resources(resource, name=name,\n                                               resource_id=resource_id,\n                                               tags=tags, region=region,\n                                               key=key, keyid=keyid,\n                                               profile=profile))}\n    except BotoServerError as e:\n        return {'error': __utils__['boto.get_error'](e)}", "output": "Given a resource type and name, return {exists: true} if it exists,\n    {exists: false} if it does not exist, or {error: {message: error text}\n    on error.\n\n    .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_vpc.resource_exists internet_gateway myigw", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_tuple_end(self, extra_end_rules=None):\n        \"\"\"\"\"\"\n        if self.stream.current.type in ('variable_end', 'block_end', 'rparen'):\n            return True\n        elif extra_end_rules is not None:\n            return self.stream.current.test_any(extra_end_rules)\n        return False", "output": "Are we at the end of a tuple?", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def preprocess():\n    \"\"\n\n    text = open(\"./_sources/reference.rst\", \"r\").read()\n    os.remove(\"./_sources/reference.rst\")\n\n    if not os.path.exists(\"./_sources/reference\"):\n        os.makedirs(\"./_sources/reference\")\n\n    def pairwise(iterable):\n        \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n\n        iteration = iter(iterable)\n        return izip(iteration, iteration)\n\n    sections = map(str.strip, re.split(r\"<!--\\s*(.+)\\s*-->\", text))\n    for section, content in pairwise(sections[1:]):\n        if section.endswith(\".proto\"):\n            section_name = section[:-len(\".proto\")]\n            file_name = \"./_sources/reference/{0}.rst\".format(section_name)\n            with open(file_name, \"w\") as f:\n                f.truncate()\n                f.write(content)\n                f.close()", "output": "splits _sources/reference.rst into separate files", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mouseMoveEvent(self, event):\n        \"\"\"\n        \"\"\"\n        line_number = self.editor.get_linenumber_from_mouse_event(event)\n        block = self.editor.document().findBlockByNumber(line_number-1)\n        data = block.userData()\n\n        # this disables pyflakes messages if there is an active drag/selection\n        # operation\n        check = self._released == -1\n        if data and data.code_analysis and check:\n            self.editor.show_code_analysis_results(line_number,\n                                                   data)\n        else:\n            self.editor.hide_tooltip()\n\n        if event.buttons() == Qt.LeftButton:\n            self._released = line_number\n            self.editor.select_lines(self._pressed, self._released)", "output": "Override Qt method.\n\n        Show code analisis, if left button pressed select lines.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _edge_list_to_sframe(ls, src_column_name, dst_column_name):\n    \"\"\"\n    \n    \"\"\"\n    sf = SFrame()\n\n    if type(ls) == list:\n        cols = reduce(set.union, (set(v.attr.keys()) for v in ls))\n        sf[src_column_name] = [e.src_vid for e in ls]\n        sf[dst_column_name] = [e.dst_vid for e in ls]\n        for c in cols:\n            sf[c] = [e.attr.get(c) for e in ls]\n\n    elif type(ls) == Edge:\n        sf[src_column_name] = [ls.src_vid]\n        sf[dst_column_name] = [ls.dst_vid]\n\n    else:\n        raise TypeError('Edges type {} is Not supported.'.format(type(ls)))\n\n    return sf", "output": "Convert a list of edges into an SFrame.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_args():\n    \"\"\"\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Text Classification with FastText',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    # Computation options\n    group = parser.add_argument_group('Computation arguments')\n    group.add_argument('--input', type=str, help='Input file location')\n    group.add_argument(\n        '--validation', type=str, help='Validation file Location ')\n    group.add_argument(\n        '--output', type=str, help='Location to save trained model')\n    group.add_argument(\n        '--ngrams', type=int, default=1, help='NGrams used for training')\n    group.add_argument(\n        '--batch_size', type=int, default=16, help='Batch size for training.')\n    group.add_argument('--epochs', type=int, default=10, help='Epoch limit')\n    group.add_argument(\n        '--gpu',\n        type=int,\n        help=('Number (index) of GPU to run on, e.g. 0. '\n              'If not specified, uses CPU.'))\n    group.add_argument(\n        '--no-hybridize',\n        action='store_true',\n        help='Disable hybridization of gluon HybridBlocks.')\n\n    # Model\n    group = parser.add_argument_group('Model arguments')\n    group.add_argument(\n        '--emsize', type=int, default=100, help='Size of embedding vectors.')\n\n    # Optimization options\n    group = parser.add_argument_group('Optimization arguments')\n    group.add_argument('--optimizer', type=str, default='adam')\n    group.add_argument('--lr', type=float, default=0.05)\n\n    args = parser.parse_args()\n    return args", "output": "Parse command line arguments.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe_configs(self, resources, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n\n        f, futmap = AdminClient._make_futures(resources, ConfigResource,\n                                              AdminClient._make_resource_result)\n\n        super(AdminClient, self).describe_configs(resources, f, **kwargs)\n\n        return futmap", "output": "Get configuration for the specified resources.\n\n        The future result() value is a dict(<configname, ConfigEntry>).\n\n        :warning: Multiple resources and resource types may be requested,\n                  but at most one resource of type RESOURCE_BROKER is allowed\n                  per call since these resource requests must be sent to the\n                  broker specified in the resource.\n\n        :param list(ConfigResource) resources: Resources to get configuration for.\n        :param float request_timeout: Set the overall request timeout in seconds,\n                  including broker lookup, request transmission, operation time\n                  on broker, and response. Default: `socket.timeout.ms*1000.0`\n        :param bool validate_only: Tell broker to only validate the request,\n                  without creating the partitions. Default: False\n\n        :returns: a dict of futures for each resource, keyed by the ConfigResource.\n        :rtype: dict(<ConfigResource, future>)\n\n        :raises KafkaException: Operation failed locally or on broker.\n        :raises TypeException: Invalid input.\n        :raises ValueException: Invalid input.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def c2f(r, i, ctype_name):\n    \"\"\"\n    \n    \"\"\"\n\n    ftype = c2f_dict[ctype_name]\n    return np.typeDict[ctype_name](ftype(r) + 1j * ftype(i))", "output": "Convert strings to complex number instance with specified numpy type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_profiles(self):\n        \"\"\"  \"\"\"\n        for i, (id, profiler, showed) in enumerate(self.profilers):\n            if not showed and profiler:\n                profiler.show(id)\n                # mark it as showed\n                self.profilers[i][2] = True", "output": "Print the profile stats to stdout", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_retcode(ret, highstate=None):\n    '''\n    \n    '''\n\n    # Set default retcode to 0\n    __context__['retcode'] = 0\n\n    if isinstance(ret, list):\n        __context__['retcode'] = 1\n        return\n    if not salt.utils.state.check_result(ret, highstate=highstate):\n\n        __context__['retcode'] = 2", "output": "Set the return code based on the data back from the state system", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def source_path(cls, organization, source):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"organizations/{organization}/sources/{source}\",\n            organization=organization,\n            source=source,\n        )", "output": "Return a fully-qualified source string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cursor_position_changed(self):\r\n        \"\"\"\"\"\"\r\n        if self.bracepos is not None:\r\n            self.__highlight(self.bracepos, cancel=True)\r\n            self.bracepos = None\r\n        cursor = self.textCursor()\r\n        if cursor.position() == 0:\r\n            return\r\n        cursor.movePosition(QTextCursor.PreviousCharacter,\r\n                            QTextCursor.KeepAnchor)\r\n        text = to_text_string(cursor.selectedText())\r\n        pos1 = cursor.position()\r\n        if text in (')', ']', '}'):\r\n            pos2 = self.find_brace_match(pos1, text, forward=False)\r\n        elif text in ('(', '[', '{'):\r\n            pos2 = self.find_brace_match(pos1, text, forward=True)\r\n        else:\r\n            return\r\n        if pos2 is not None:\r\n            self.bracepos = (pos1, pos2)\r\n            self.__highlight(self.bracepos, color=self.matched_p_color)\r\n        else:\r\n            self.bracepos = (pos1,)\r\n            self.__highlight(self.bracepos, color=self.unmatched_p_color)", "output": "Brace matching", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ec2_credentials_create(user_id=None, name=None,\n                           tenant_id=None, tenant=None,\n                           profile=None, **connection_args):\n    '''\n    \n    '''\n    kstone = auth(profile, **connection_args)\n\n    if name:\n        user_id = user_get(name=name, profile=profile,\n                           **connection_args)[name]['id']\n    if not user_id:\n        return {'Error': 'Could not resolve User ID'}\n\n    if tenant:\n        tenant_id = tenant_get(name=tenant, profile=profile,\n                               **connection_args)[tenant]['id']\n    if not tenant_id:\n        return {'Error': 'Could not resolve Tenant ID'}\n\n    newec2 = kstone.ec2.create(user_id, tenant_id)\n    return {'access': newec2.access,\n            'secret': newec2.secret,\n            'tenant_id': newec2.tenant_id,\n            'user_id': newec2.user_id}", "output": "Create EC2-compatible credentials for user per tenant\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.ec2_credentials_create name=admin tenant=admin\n\n        salt '*' keystone.ec2_credentials_create \\\n        user_id=c965f79c4f864eaaa9c3b41904e67082 \\\n        tenant_id=722787eb540849158668370dc627ec5f", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_args_and_kwargs(self, cmdline):\n        '''\n        \n        '''\n        # Parse args and kwargs\n        args = []\n        kwargs = {}\n\n        if len(cmdline) > 1:\n            for item in cmdline[1:]:\n                if '=' in item:\n                    (key, value) = item.split('=', 1)\n                    kwargs[key] = value\n                else:\n                    args.append(item)\n        return (args, kwargs)", "output": "cmdline: list\n\n        returns tuple of: args (list), kwargs (dict)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_search(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.search_images(**kwargs)", "output": "Search for images\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' glanceng.image_search name=image1\n        salt '*' glanceng.image_search", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def version(*names, **kwargs):\n    '''\n    \n    '''\n    with_origin = kwargs.pop('with_origin', False)\n    ret = __salt__['pkg_resource.version'](*names, **kwargs)\n    if not salt.utils.data.is_true(with_origin):\n        return ret\n    # Put the return value back into a dict since we're adding a subdict\n    if len(names) == 1:\n        ret = {names[0]: ret}\n    origins = __context__.get('pkg.origin', {})\n    return dict([\n        (x, {'origin': origins.get(x, ''), 'version': y})\n        for x, y in six.iteritems(ret)\n    ])", "output": "Returns a string representing the package version or an empty string if not\n    installed. If more than one package name is specified, a dict of\n    name/version pairs is returned.\n\n    .. note::\n\n        This function can accessed using ``pkg.info`` in addition to\n        ``pkg.version``, to more closely match the CLI usage of ``pkg(8)``.\n\n    jail\n        Get package version information for the specified jail\n\n    chroot\n        Get package version information for the specified chroot (ignored if\n        ``jail`` is specified)\n\n    root\n        Get package version information for the specified root (ignored if\n        ``jail`` is specified)\n\n    with_origin : False\n        Return a nested dictionary containing both the origin name and version\n        for each specified package.\n\n        .. versionadded:: 2014.1.0\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.version <package name>\n        salt '*' pkg.version <package name> jail=<jail name or id>\n        salt '*' pkg.version <package1> <package2> <package3> ...", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode_example(self, tfexample_dict):\n    \"\"\"\"\"\"\n    tensor_dict = {}\n    # Iterate over the Tensor dict keys\n    for feature_key, feature in six.iteritems(self._feature_dict):\n      decoded_feature = decode_single_feature_from_dict(\n          feature_k=feature_key,\n          feature=feature,\n          tfexample_dict=tfexample_dict,\n      )\n      tensor_dict[feature_key] = decoded_feature\n    return tensor_dict", "output": "See base class for details.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_connection():\n    '''\n    \n    '''\n    global COUCHBASE_CONN\n    if COUCHBASE_CONN is None:\n        opts = _get_options()\n        if opts['password']:\n            COUCHBASE_CONN = couchbase.Couchbase.connect(host=opts['host'],\n                                                         port=opts['port'],\n                                                         bucket=opts['bucket'],\n                                                         password=opts['password'])\n        else:\n            COUCHBASE_CONN = couchbase.Couchbase.connect(host=opts['host'],\n                                                         port=opts['port'],\n                                                         bucket=opts['bucket'])\n\n    return COUCHBASE_CONN", "output": "Global function to access the couchbase connection (and make it if its closed)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rollforward(self, dt):\n        \"\"\"\n        \n        \"\"\"\n        dt = as_timestamp(dt)\n        if not self.onOffset(dt):\n            dt = dt + self.__class__(1, normalize=self.normalize, **self.kwds)\n        return dt", "output": "Roll provided date forward to next offset only if not on offset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_friend(self):\n        \"\"\"\n        \"\"\"\n        r = self.relationship\n        if r is None:\n            return False\n        return r.type is RelationshipType.friend", "output": ":class:`bool`: Checks if the user is your friend.\n\n        .. note::\n\n            This only applies to non-bot accounts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read_temp(data):\n    '''\n    \n    '''\n    tout = StringIO()\n    tout.write(data)\n    tout.seek(0)\n    output = tout.readlines()\n    tout.close()\n\n    return output", "output": "Return what would be written to disk", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_domain_list(list_name):\n    '''\n    \n\n    '''\n    payload = {\"jsonrpc\": \"2.0\",\n               \"id\": \"ID0\",\n               \"method\": \"get_policy_domain_names\",\n               \"params\": [list_name, 0, 256]}\n\n    response = __proxy__['bluecoat_sslv.call'](payload, False)\n\n    return _convert_to_list(response, 'item_name')", "output": "Retrieves a specific policy domain name list.\n\n    list_name(str): The name of the specific policy domain name list to retrieve.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bluecoat_sslv.get_domain_list MyDomainNameList", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def awaitTermination(self, timeout=None):\n        \"\"\"\n        \"\"\"\n        if timeout is not None:\n            if not isinstance(timeout, (int, float)) or timeout < 0:\n                raise ValueError(\"timeout must be a positive integer or float. Got %s\" % timeout)\n            return self._jsq.awaitTermination(int(timeout * 1000))\n        else:\n            return self._jsq.awaitTermination()", "output": "Waits for the termination of `this` query, either by :func:`query.stop()` or by an\n        exception. If the query has terminated with an exception, then the exception will be thrown.\n        If `timeout` is set, it returns whether the query has terminated or not within the\n        `timeout` seconds.\n\n        If the query has terminated, then all subsequent calls to this method will either return\n        immediately (if the query was terminated by :func:`stop()`), or throw the exception\n        immediately (if the query has terminated with exception).\n\n        throws :class:`StreamingQueryException`, if `this` query has terminated with an exception", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset_namespace(self):\r\n        \"\"\"\"\"\"\r\n        self.shellwidget.reset_namespace(warning=self.reset_warning,\r\n                                         message=True)", "output": "Resets the namespace by removing all names defined by the user", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_cloud_environment():\n    '''\n    \n    '''\n    cloud_environment = config.get_cloud_config_value(\n                            'cloud_environment',\n                            get_configured_provider(), __opts__, search_global=False\n                        )\n    try:\n        cloud_env_module = importlib.import_module('msrestazure.azure_cloud')\n        cloud_env = getattr(cloud_env_module, cloud_environment or 'AZURE_PUBLIC_CLOUD')\n    except (AttributeError, ImportError):\n        raise SaltCloudSystemExit(\n            'The azure {0} cloud environment is not available.'.format(cloud_environment)\n        )\n\n    return cloud_env", "output": "Get the cloud environment object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transactions(self, dt=None):\n        \"\"\"\n        \"\"\"\n        if dt is None:\n            # flatten the by-day transactions\n            return [\n                txn\n                for by_day in itervalues(self._processed_transactions)\n                for txn in by_day\n            ]\n\n        return self._processed_transactions.get(dt, [])", "output": "Retrieve the dict-form of all of the transactions in a given bar or\n        for the whole simulation.\n\n        Parameters\n        ----------\n        dt : pd.Timestamp or None, optional\n            The particular datetime to look up transactions for. If not passed,\n            or None is explicitly passed, all of the transactions will be\n            returned.\n\n        Returns\n        -------\n        transactions : list[dict]\n            The transaction information.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def focusInEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        self.focus_changed.emit()\r\n        self.focus_in.emit()\r\n        self.highlight_current_cell()\r\n        QPlainTextEdit.focusInEvent(self, event)", "output": "Reimplemented to handle focus", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def highlight_current_line(self):\r\n        \"\"\"\"\"\"\r\n        selection = TextDecoration(self.textCursor())\r\n        selection.format.setProperty(QTextFormat.FullWidthSelection,\r\n                                     to_qvariant(True))\r\n        selection.format.setBackground(self.currentline_color)\r\n        selection.cursor.clearSelection()\r\n        self.set_extra_selections('current_line', [selection])\r\n        self.update_extra_selections()", "output": "Highlight current line", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _homogenize_dict(self, frames, intersect=True, dtype=None):\n        \"\"\"\n        \n        \"\"\"\n\n        result = dict()\n        # caller differs dict/ODict, preserved type\n        if isinstance(frames, OrderedDict):\n            result = OrderedDict()\n\n        adj_frames = OrderedDict()\n        for k, v in frames.items():\n            if isinstance(v, dict):\n                adj_frames[k] = self._constructor_sliced(v)\n            else:\n                adj_frames[k] = v\n\n        axes = self._AXIS_ORDERS[1:]\n        axes_dict = {a: ax for a, ax in zip(axes, self._extract_axes(\n                     self, adj_frames, axes, intersect=intersect))}\n\n        reindex_dict = {self._AXIS_SLICEMAP[a]: axes_dict[a] for a in axes}\n        reindex_dict['copy'] = False\n        for key, frame in adj_frames.items():\n            if frame is not None:\n                result[key] = frame.reindex(**reindex_dict)\n            else:\n                result[key] = None\n\n        axes_dict['data'] = result\n        axes_dict['dtype'] = dtype\n        return axes_dict", "output": "Conform set of _constructor_sliced-like objects to either\n        an intersection of indices / columns or a union.\n\n        Parameters\n        ----------\n        frames : dict\n        intersect : boolean, default True\n\n        Returns\n        -------\n        dict of aligned results & indices", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def global_callback(self):\r\n        \"\"\"\"\"\"\r\n        widget = QApplication.focusWidget()\r\n        action = self.sender()\r\n        callback = from_qvariant(action.data(), to_text_string)\r\n        from spyder.plugins.editor.widgets.editor import TextEditBaseWidget\r\n        from spyder.plugins.ipythonconsole.widgets import ControlWidget\r\n\r\n        if isinstance(widget, (TextEditBaseWidget, ControlWidget)):\r\n            getattr(widget, callback)()\r\n        else:\r\n            return", "output": "Global callback", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_port_fwd(zone, src, dest, proto='tcp', dstaddr='', permanent=True):\n    '''\n    \n    '''\n    cmd = '--zone={0} --add-forward-port=port={1}:proto={2}:toport={3}:toaddr={4}'.format(\n        zone,\n        src,\n        proto,\n        dest,\n        dstaddr\n    )\n\n    if permanent:\n        cmd += ' --permanent'\n\n    return __firewall_cmd(cmd)", "output": "Add port forwarding.\n\n    .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' firewalld.add_port_fwd public 80 443 tcp", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vb_get_box():\n    '''\n    \n    '''\n    vb_get_manager()\n\n    try:\n        # This works in older versions of the SDK, but does not seem to work anymore.\n        vbox = _virtualboxManager.vbox\n    except AttributeError:\n        vbox = _virtualboxManager.getVirtualBox()\n\n    return vbox", "output": "Needed for certain operations in the SDK e.g creating sessions\n    @return:\n    @rtype: IVirtualBox", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def startDrag(self, dropActions):\r\n        \"\"\"\"\"\"\r\n        data = QMimeData()\r\n        data.setUrls([QUrl(fname) for fname in self.get_selected_filenames()])\r\n        drag = QDrag(self)\r\n        drag.setMimeData(data)\r\n        drag.exec_()", "output": "Reimplement Qt Method - handle drag event", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _list_queues():\n    '''\n    \n    '''\n    queue_dir = __opts__['sqlite_queue_dir']\n    files = os.path.join(queue_dir, '*.db')\n    paths = glob.glob(files)\n    queues = [os.path.splitext(os.path.basename(item))[0] for item in paths]\n\n    return queues", "output": "Return a list of sqlite databases in the queue_dir", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chfullname(name, fullname):\n    '''\n    \n    '''\n    fullname = salt.utils.data.decode(fullname)\n    pre_info = info(name)\n    if not pre_info:\n        raise CommandExecutionError('User \\'{0}\\' does not exist'.format(name))\n    pre_info['fullname'] = salt.utils.data.decode(pre_info['fullname'])\n    if fullname == pre_info['fullname']:\n        return True\n    _dscl(\n        ['/Users/{0}'.format(name), 'RealName', fullname],\n        # use a 'create' command, because a 'change' command would fail if\n        # current fullname is an empty string. The 'create' will just overwrite\n        # this field.\n        ctype='create'\n    )\n    # dscl buffers changes, sleep 1 second before checking if new value\n    # matches desired value\n    time.sleep(1)\n\n    current = salt.utils.data.decode(info(name).get('fullname'))\n    return current == fullname", "output": "Change the user's Full Name\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.chfullname foo 'Foo Bar'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_text(self, text, changed=True,\r\n                  forward=True, case=False, words=False,\r\n                  regexp=False):\r\n        \"\"\"\"\"\"\r\n        if not WEBENGINE:\r\n            findflag = QWebEnginePage.FindWrapsAroundDocument\r\n        else:\r\n            findflag = 0\r\n\r\n        if not forward:\r\n            findflag = findflag | QWebEnginePage.FindBackward\r\n        if case:\r\n            findflag = findflag | QWebEnginePage.FindCaseSensitively\r\n\r\n        return self.findText(text, QWebEnginePage.FindFlags(findflag))", "output": "Find text", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def temp_fail_retry(error, fun, *args):\r\n    \"\"\"\"\"\"\r\n    while 1:\r\n        try:\r\n            return fun(*args)\r\n        except error as e:\r\n            eintr = errno.WSAEINTR if os.name == 'nt' else errno.EINTR\r\n            if e.args[0] == eintr:\r\n                continue\r\n            raise", "output": "Retry to execute function, ignoring EINTR error (interruptions)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render_and_create_dir(dirname, context, output_dir, environment,\n                          overwrite_if_exists=False):\n    \"\"\"\"\"\"\n    name_tmpl = environment.from_string(dirname)\n    rendered_dirname = name_tmpl.render(**context)\n\n    dir_to_create = os.path.normpath(\n        os.path.join(output_dir, rendered_dirname)\n    )\n\n    logger.debug('Rendered dir {} must exist in output_dir {}'.format(\n        dir_to_create,\n        output_dir\n    ))\n\n    output_dir_exists = os.path.exists(dir_to_create)\n\n    if output_dir_exists:\n        if overwrite_if_exists:\n            logger.debug(\n                'Output directory {} already exists,'\n                'overwriting it'.format(dir_to_create)\n            )\n        else:\n            msg = 'Error: \"{}\" directory already exists'.format(dir_to_create)\n            raise OutputDirExistsException(msg)\n    else:\n        make_sure_path_exists(dir_to_create)\n\n    return dir_to_create, not output_dir_exists", "output": "Render name of a directory, create the directory, return its path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enabled(name, **kwargs):\n    '''\n    \n    '''\n    jail = kwargs.get('jail', '')\n    if not available(name, jail):\n        log.error('Service %s not found', name)\n        return False\n\n    cmd = '{0} {1} rcvar'.format(_cmd(jail), name)\n\n    for line in __salt__['cmd.run_stdout'](cmd, python_shell=False).splitlines():\n        if '_enable=\"' not in line:\n            continue\n        _, state, _ = line.split('\"', 2)\n        return state.lower() in ('yes', 'true', 'on', '1')\n\n    # probably will never reached\n    return False", "output": "Return True if the named service is enabled, false otherwise\n\n    name\n        Service name\n\n    .. versionchanged:: 2016.3.4\n\n    Support for jail (representing jid or jail name) keyword argument in kwargs\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.enabled <service name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_qt():\r\n    \"\"\"\"\"\"\r\n    qt_infos = dict(pyqt5=(\"PyQt5\", \"5.6\"))\r\n    try:\r\n        import qtpy\r\n        package_name, required_ver = qt_infos[qtpy.API]\r\n        actual_ver = qtpy.PYQT_VERSION\r\n        if LooseVersion(actual_ver) < LooseVersion(required_ver):\r\n            show_warning(\"Please check Spyder installation requirements:\\n\"\r\n                         \"%s %s+ is required (found v%s).\"\r\n                         % (package_name, required_ver, actual_ver))\r\n    except ImportError:\r\n        show_warning(\"Failed to import qtpy.\\n\"\r\n                     \"Please check Spyder installation requirements:\\n\\n\"\r\n                     \"qtpy 1.2.0+ and\\n\"\r\n                     \"%s %s+\\n\\n\"\r\n                     \"are required to run Spyder.\"\r\n                     % (qt_infos['pyqt5']))", "output": "Check Qt binding requirements", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unpack_file_url(\n    link,  # type: Link\n    location,  # type: str\n    download_dir=None,  # type: Optional[str]\n    hashes=None  # type: Optional[Hashes]\n):\n    # type: (...) -> None\n    \"\"\"\n    \"\"\"\n    link_path = url_to_path(link.url_without_fragment)\n\n    # If it's a url to a local directory\n    if is_dir_url(link):\n        if os.path.isdir(location):\n            rmtree(location)\n        shutil.copytree(link_path, location, symlinks=True)\n        if download_dir:\n            logger.info('Link is a directory, ignoring download_dir')\n        return\n\n    # If --require-hashes is off, `hashes` is either empty, the\n    # link's embedded hash, or MissingHashes; it is required to\n    # match. If --require-hashes is on, we are satisfied by any\n    # hash in `hashes` matching: a URL-based or an option-based\n    # one; no internet-sourced hash will be in `hashes`.\n    if hashes:\n        hashes.check_against_path(link_path)\n\n    # If a download dir is specified, is the file already there and valid?\n    already_downloaded_path = None\n    if download_dir:\n        already_downloaded_path = _check_download_dir(link,\n                                                      download_dir,\n                                                      hashes)\n\n    if already_downloaded_path:\n        from_path = already_downloaded_path\n    else:\n        from_path = link_path\n\n    content_type = mimetypes.guess_type(from_path)[0]\n\n    # unpack the archive to the build dir location. even when only downloading\n    # archives, they have to be unpacked to parse dependencies\n    unpack_file(from_path, location, content_type, link)\n\n    # a download dir is specified and not already downloaded\n    if download_dir and not already_downloaded_path:\n        _copy_file(from_path, download_dir, link)", "output": "Unpack link into location.\n\n    If download_dir is provided and link points to a file, make a copy\n    of the link file inside download_dir.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def average_sharded_losses(sharded_losses):\n  \"\"\"\n  \"\"\"\n  losses = {}\n  for loss_name in sorted(sharded_losses[0]):\n    all_shards = [shard_losses[loss_name] for shard_losses in sharded_losses]\n    if isinstance(all_shards[0], tuple):\n      sharded_num, sharded_den = zip(*all_shards)\n      mean_loss = (\n          tf.add_n(sharded_num) / tf.maximum(\n              tf.cast(1.0, sharded_den[0].dtype), tf.add_n(sharded_den)))\n    else:\n      mean_loss = tf.reduce_mean(all_shards)\n\n    losses[loss_name] = mean_loss\n  return losses", "output": "Average losses across datashards.\n\n  Args:\n    sharded_losses: list<dict<str loss_name, Tensor loss>>. The loss\n      can be a single Tensor or a 2-tuple (numerator and denominator).\n\n  Returns:\n    losses: dict<str loss_name, Tensor avg_loss>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fixed_crop(src, x0, y0, w, h, size=None, interp=2):\n    \"\"\"\n    \"\"\"\n    out = nd.slice(src, begin=(y0, x0, 0), end=(y0 + h, x0 + w, int(src.shape[2])))\n    if size is not None and (w, h) != size:\n        sizes = (h, w, size[1], size[0])\n        out = imresize(out, *size, interp=_get_interp_method(interp, sizes))\n    return out", "output": "Crop src at fixed location, and (optionally) resize it to size.\n\n    Parameters\n    ----------\n    src : NDArray\n        Input image\n    x0 : int\n        Left boundary of the cropping area\n    y0 : int\n        Top boundary of the cropping area\n    w : int\n        Width of the cropping area\n    h : int\n        Height of the cropping area\n    size : tuple of (w, h)\n        Optional, resize to new size after cropping\n    interp : int, optional, default=2\n        Interpolation method. See resize_short for details.\n\n    Returns\n    -------\n    NDArray\n        An `NDArray` containing the cropped image.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_server(self):\n    \"\"\"\"\"\"\n    app = application.standard_tensorboard_wsgi(self.flags,\n                                                self.plugin_loaders,\n                                                self.assets_zip_provider)\n    return self.server_class(app, self.flags)", "output": "Constructs the TensorBoard WSGI app and instantiates the server.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attention_lm_base():\n  \"\"\"\"\"\"\n  hparams = common_hparams.basic_params1()\n  hparams.hidden_size = 1024\n  hparams.batch_size = 8192\n  hparams.max_length = 256\n  hparams.dropout = 0.0\n  hparams.clip_grad_norm = 0.  # i.e. no gradient clipping\n  hparams.optimizer_adam_epsilon = 1e-9\n  hparams.learning_rate_decay_scheme = \"noam\"\n  hparams.learning_rate = 0.1\n  hparams.learning_rate_warmup_steps = 2000\n  hparams.initializer_gain = 1.0\n  hparams.num_hidden_layers = 6\n  hparams.initializer = \"uniform_unit_scaling\"\n  hparams.weight_decay = 0.0\n  hparams.optimizer_adam_beta1 = 0.9\n  hparams.optimizer_adam_beta2 = 0.98\n  hparams.label_smoothing = 0.0\n  hparams.shared_embedding_and_softmax_weights = False\n\n  hparams.add_hparam(\"filter_size\", 4096)  # Add new ones like this.\n  # attention-related flags\n  hparams.add_hparam(\"num_heads\", 8)\n  hparams.add_hparam(\"attention_key_channels\", 0)\n  hparams.add_hparam(\"attention_value_channels\", 0)\n  # All hyperparameters ending in \"dropout\" are automatically set to 0.0\n  # when not in training mode.\n  hparams.add_hparam(\"attention_dropout\", 0.0)\n  hparams.add_hparam(\"relu_dropout\", 0.0)\n  hparams.add_hparam(\"pos\", \"timing\")  # timing, none\n  hparams.add_hparam(\"encoder_full_attention\", False)\n  return hparams", "output": "Set of hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def order_by(self, field_path, direction=ASCENDING):\n        \"\"\"\n        \"\"\"\n        field_path_module.split_field_path(field_path)  # raises\n\n        order_pb = self._make_order(field_path, direction)\n\n        new_orders = self._orders + (order_pb,)\n        return self.__class__(\n            self._parent,\n            projection=self._projection,\n            field_filters=self._field_filters,\n            orders=new_orders,\n            limit=self._limit,\n            offset=self._offset,\n            start_at=self._start_at,\n            end_at=self._end_at,\n        )", "output": "Modify the query to add an order clause on a specific field.\n\n        See :meth:`~.firestore_v1beta1.client.Client.field_path` for\n        more information on **field paths**.\n\n        Successive :meth:`~.firestore_v1beta1.query.Query.order_by` calls\n        will further refine the ordering of results returned by the query\n        (i.e. the new \"order by\" fields will be added to existing ones).\n\n        Args:\n            field_path (str): A field path (``.``-delimited list of\n                field names) on which to order the query results.\n            direction (Optional[str]): The direction to order by. Must be one\n                of :attr:`ASCENDING` or :attr:`DESCENDING`, defaults to\n                :attr:`ASCENDING`.\n\n        Returns:\n            ~.firestore_v1beta1.query.Query: An ordered query. Acts as a\n            copy of the current query, modified with the newly added\n            \"order by\" constraint.\n\n        Raises:\n            ValueError: If ``field_path`` is invalid.\n            ValueError: If ``direction`` is not one of :attr:`ASCENDING` or\n                :attr:`DESCENDING`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chdir(self, directory=None, browsing_history=False):\r\n        \"\"\"\"\"\"\r\n        if directory is not None:\r\n            directory = osp.abspath(to_text_string(directory))\r\n        if browsing_history:\r\n            directory = self.history[self.histindex]\r\n        elif directory in self.history:\r\n            self.histindex = self.history.index(directory)\r\n        else:\r\n            if self.histindex is None:\r\n                self.history = []\r\n            else:\r\n                self.history = self.history[:self.histindex+1]\r\n            if len(self.history) == 0 or \\\r\n               (self.history and self.history[-1] != directory):\r\n                self.history.append(directory)\r\n            self.histindex = len(self.history)-1\r\n        directory = to_text_string(directory)\r\n        try:\r\n            PermissionError\r\n            FileNotFoundError\r\n        except NameError:\r\n            PermissionError = OSError\r\n            if os.name == 'nt':\r\n                FileNotFoundError = WindowsError\r\n            else:\r\n                FileNotFoundError = IOError\r\n        try:\r\n            os.chdir(directory)\r\n            self.sig_open_dir.emit(directory)\r\n            self.refresh(new_path=directory, force_current=True)\r\n        except PermissionError:\r\n            QMessageBox.critical(self.parent_widget, \"Error\",\r\n                                 _(\"You don't have the right permissions to \"\r\n                                   \"open this directory\"))\r\n        except FileNotFoundError:\r\n            # Handle renaming directories on the fly. See issue #5183\r\n            self.history.pop(self.histindex)", "output": "Set directory as working directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def aggregate(self, search):\n        \"\"\"\n        \n        \"\"\"\n        for f, facet in iteritems(self.facets):\n            agg = facet.get_aggregation()\n            agg_filter = MatchAll()\n            for field, filter in iteritems(self._filters):\n                if f == field:\n                    continue\n                agg_filter &= filter\n            search.aggs.bucket(\n                '_filter_' + f,\n                'filter',\n                filter=agg_filter\n            ).bucket(f, agg)", "output": "Add aggregations representing the facets selected, including potential\n        filters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_archive_format(name, function, extra_args=None, description=''):\n    \"\"\"\n    \"\"\"\n    if extra_args is None:\n        extra_args = []\n    if not isinstance(function, collections.Callable):\n        raise TypeError('The %s object is not callable' % function)\n    if not isinstance(extra_args, (tuple, list)):\n        raise TypeError('extra_args needs to be a sequence')\n    for element in extra_args:\n        if not isinstance(element, (tuple, list)) or len(element) !=2:\n            raise TypeError('extra_args elements are : (arg_name, value)')\n\n    _ARCHIVE_FORMATS[name] = (function, extra_args, description)", "output": "Registers an archive format.\n\n    name is the name of the format. function is the callable that will be\n    used to create archives. If provided, extra_args is a sequence of\n    (name, value) tuples that will be passed as arguments to the callable.\n    description can be provided to describe the format, and will be returned\n    by the get_archive_formats() function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def policy_assignment_delete(name, scope, **kwargs):\n    '''\n    \n\n    '''\n    result = False\n    polconn = __utils__['azurearm.get_client']('policy', **kwargs)\n    try:\n        # pylint: disable=unused-variable\n        policy = polconn.policy_assignments.delete(\n            policy_assignment_name=name,\n            scope=scope\n        )\n        result = True\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('resource', str(exc), **kwargs)\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Delete a policy assignment.\n\n    :param name: The name of the policy assignment to delete.\n\n    :param scope: The scope of the policy assignment.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_resource.policy_assignment_delete testassign \\\n        /subscriptions/bc75htn-a0fhsi-349b-56gh-4fghti-f84852", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def output(data, **kwargs):  # pylint: disable=unused-argument\n    '''\n    \n    '''\n    try:\n        if 'output_indent' not in __opts__:\n            return salt.utils.json.dumps(data, default=repr, indent=4)\n\n        indent = __opts__.get('output_indent')\n        sort_keys = False\n\n        if indent is None:\n            indent = None\n\n        elif indent == 'pretty':\n            indent = 4\n            sort_keys = True\n\n        elif isinstance(indent, int):\n            if indent >= 0:\n                indent = indent\n            else:\n                indent = None\n\n        return salt.utils.json.dumps(data, default=repr, indent=indent, sort_keys=sort_keys)\n\n    except UnicodeDecodeError as exc:\n        log.error('Unable to serialize output to json')\n        return salt.utils.json.dumps(\n            {'error': 'Unable to serialize output to json',\n             'message': six.text_type(exc)}\n        )\n\n    except TypeError:\n        log.debug('An error occurred while outputting JSON', exc_info=True)\n    # Return valid JSON for unserializable objects\n    return salt.utils.json.dumps({})", "output": "Print the output data in JSON", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def argument_types(self) -> List[Type]:\n        \"\"\"\n        \n        \"\"\"\n        arguments = [self.first]\n        remaining_type = self.second\n        while isinstance(remaining_type, ComplexType):\n            arguments.append(remaining_type.first)\n            remaining_type = remaining_type.second\n        return arguments", "output": "Gives the types of all arguments to this function.  For functions returning a basic type,\n        we grab all ``.first`` types until ``.second`` is no longer a ``ComplexType``.  That logic\n        is implemented here in the base class.  If you have a higher-order function that returns a\n        function itself, you need to override this method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shap_values(self, X):\n        \"\"\" \n        \"\"\"\n\n        # convert dataframes\n        if str(type(X)).endswith(\"pandas.core.series.Series'>\"):\n            X = X.values\n        elif str(type(X)).endswith(\"'pandas.core.frame.DataFrame'>\"):\n            X = X.values\n\n        #assert str(type(X)).endswith(\"'numpy.ndarray'>\"), \"Unknown instance type: \" + str(type(X))\n        assert len(X.shape) == 1 or len(X.shape) == 2, \"Instance must have 1 or 2 dimensions!\"\n\n        if self.feature_dependence == \"correlation\":\n            phi = np.matmul(np.matmul(X[:,self.valid_inds], self.avg_proj.T), self.x_transform.T) - self.mean_transformed\n            phi = np.matmul(phi, self.avg_proj)\n\n            full_phi = np.zeros(((phi.shape[0], self.M)))\n            full_phi[:,self.valid_inds] = phi\n\n            return full_phi\n\n        elif self.feature_dependence == \"independent\":\n            if len(self.coef.shape) == 1:\n                return np.array(X - self.mean) * self.coef\n            else:\n                return [np.array(X - self.mean) * self.coef[i] for i in range(self.coef.shape[0])]", "output": "Estimate the SHAP values for a set of samples.\n\n        Parameters\n        ----------\n        X : numpy.array or pandas.DataFrame\n            A matrix of samples (# samples x # features) on which to explain the model's output.\n\n        Returns\n        -------\n        For models with a single output this returns a matrix of SHAP values\n        (# samples x # features). Each row sums to the difference between the model output for that\n        sample and the expected value of the model output (which is stored as expected_value\n        attribute of the explainer).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_moving_sequence(image, pad_lefts, total_padding):\n  \"\"\"\n  \"\"\"\n\n  with tf.name_scope(\"moving_sequence\"):\n    def get_padded_image(args):\n      pad_left, = args\n      pad_right = total_padding - pad_left\n      padding = tf.stack([pad_left, pad_right], axis=-1)\n      z = tf.zeros((1, 2), dtype=pad_left.dtype)\n      padding = tf.concat([padding, z], axis=0)\n      return tf.pad(image, padding)\n\n    padded_images = tf.map_fn(\n        get_padded_image, [pad_lefts], dtype=tf.uint8, infer_shape=False,\n        back_prop=False)\n\n  return padded_images", "output": "Create a moving image sequence from the given image a left padding values.\n\n  Args:\n    image: [in_h, in_w, n_channels] uint8 array\n    pad_lefts: [sequence_length, 2] int32 array of left padding values\n    total_padding: tensor of padding values, (pad_h, pad_w)\n\n  Returns:\n    [sequence_length, out_h, out_w, n_channels] uint8 image sequence, where\n      out_h = in_h + pad_h, out_w = in_w + out_w", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _record_from_json(value, field):\n    \"\"\"\"\"\"\n    if _not_null(value, field):\n        record = {}\n        record_iter = zip(field.fields, value[\"f\"])\n        for subfield, cell in record_iter:\n            converter = _CELLDATA_FROM_JSON[subfield.field_type]\n            if subfield.mode == \"REPEATED\":\n                value = [converter(item[\"v\"], subfield) for item in cell[\"v\"]]\n            else:\n                value = converter(cell[\"v\"], subfield)\n            record[subfield.name] = value\n        return record", "output": "Coerce 'value' to a mapping, if set or not nullable.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def var(self, ddof=1, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        nv.validate_groupby_func('var', args, kwargs)\n        if ddof == 1:\n            try:\n                return self._cython_agg_general('var', **kwargs)\n            except Exception:\n                f = lambda x: x.var(ddof=ddof, **kwargs)\n                with _group_selection_context(self):\n                    return self._python_agg_general(f)\n        else:\n            f = lambda x: x.var(ddof=ddof, **kwargs)\n            with _group_selection_context(self):\n                return self._python_agg_general(f)", "output": "Compute variance of groups, excluding missing values.\n\n        For multiple groupings, the result index will be a MultiIndex.\n\n        Parameters\n        ----------\n        ddof : integer, default 1\n            degrees of freedom", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_json(cls, json_info):\n        \"\"\"\"\"\"\n        if json_info is None:\n            return None\n        return JobRecord(\n            job_id=json_info[\"job_id\"],\n            name=json_info[\"job_name\"],\n            user=json_info[\"user\"],\n            type=json_info[\"type\"],\n            start_time=json_info[\"start_time\"])", "output": "Build a Job instance from a json string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def varea_stack(self, stackers, **kw):\n        ''' \n\n        '''\n        result = []\n        for kw in _double_stack(stackers, \"y1\", \"y2\", **kw):\n            result.append(self.varea(**kw))\n        return result", "output": "Generate multiple ``VArea`` renderers for levels stacked bottom\n        to top.\n\n        Args:\n            stackers (seq[str]) : a list of data source field names to stack\n                successively for ``y1`` and ``y1`` varea coordinates.\n\n                Additionally, the ``name`` of the renderer will be set to\n                the value of each successive stacker (this is useful with the\n                special hover variable ``$name``)\n\n        Any additional keyword arguments are passed to each call to ``varea``.\n        If a keyword value is a list or tuple, then each call will get one\n        value from the sequence.\n\n        Returns:\n            list[GlyphRenderer]\n\n        Examples:\n\n            Assuming a ``ColumnDataSource`` named ``source`` with columns\n            *2016* and *2017*, then the following call to ``varea_stack`` will\n            will create two ``VArea`` renderers that stack:\n\n            .. code-block:: python\n\n                p.varea_stack(['2016', '2017'], x='x', color=['blue', 'red'], source=source)\n\n            This is equivalent to the following two separate calls:\n\n            .. code-block:: python\n\n                p.varea(y1=stack(),       y2=stack('2016'),         x='x', color='blue', source=source, name='2016')\n                p.varea(y1=stack('2016'), y2=stack('2016', '2017'), x='x', color='red',  source=source, name='2017')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_all_evals(models, treebanks, out_file, check_parse, print_freq_tasks):\n    \"\"\"\"  \"\"\"\n    print_header = True\n\n    for tb_lang, treebank_list in treebanks.items():\n        print()\n        print(\"Language\", tb_lang)\n        for text_path in treebank_list:\n            print(\" Evaluating on\", text_path)\n\n            gold_path = text_path.parent / (text_path.stem + '.conllu')\n            print(\"  Gold data from \", gold_path)\n\n            # nested try blocks to ensure the code can continue with the next iteration after a failure\n            try:\n                with gold_path.open(mode='r', encoding='utf-8') as gold_file:\n                    gold_ud = conll17_ud_eval.load_conllu(gold_file)\n\n                for nlp, nlp_loading_time, nlp_name in models[tb_lang]:\n                    try:\n                        print(\"   Benchmarking\", nlp_name)\n                        tmp_output_path = text_path.parent / str('tmp_' + nlp_name + '.conllu')\n                        run_single_eval(nlp, nlp_loading_time, nlp_name, text_path, gold_ud, tmp_output_path, out_file,\n                                        print_header, check_parse, print_freq_tasks)\n                        print_header = False\n                    except Exception as e:\n                        print(\"    Ran into trouble: \", str(e))\n            except Exception as e:\n                print(\"   Ran into trouble: \", str(e))", "output": "Run an evaluation for each language with its specified models and treebanks", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_syspath(self, syspath):\r\n        \"\"\"\"\"\"\r\n        if syspath is not None:\r\n            editor = CollectionsEditor(self)\r\n            editor.setup(syspath, title=\"sys.path contents\", readonly=True,\r\n                         width=600, icon=ima.icon('syspath'))\r\n            self.dialog_manager.show(editor)\r\n        else:\r\n            return", "output": "Show sys.path contents.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(self, option=None):\n        \"\"\"\n        \"\"\"\n        write_pb = _helpers.pb_for_delete(self._document_path, option)\n        commit_response = self._client._firestore_api.commit(\n            self._client._database_string,\n            [write_pb],\n            transaction=None,\n            metadata=self._client._rpc_metadata,\n        )\n\n        return commit_response.commit_time", "output": "Delete the current document in the Firestore database.\n\n        Args:\n            option (Optional[~.firestore_v1beta1.client.WriteOption]): A\n               write option to make assertions / preconditions on the server\n               state of the document before applying changes.\n\n        Returns:\n            google.protobuf.timestamp_pb2.Timestamp: The time that the delete\n            request was received by the server. If the document did not exist\n            when the delete was sent (i.e. nothing was deleted), this method\n            will still succeed and will still return the time that the\n            request was received by the server.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resource_groups_list(**kwargs):\n    '''\n    \n\n    '''\n    result = {}\n    resconn = __utils__['azurearm.get_client']('resource', **kwargs)\n    try:\n        groups = __utils__['azurearm.paged_object_to_list'](resconn.resource_groups.list())\n\n        for group in groups:\n            result[group['name']] = group\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('resource', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    List all resource groups within a subscription.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_resource.resource_groups_list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def loop_until_closed(self, suppress_warning=False):\n        ''' \n\n        '''\n\n        suppress_warning # shut up flake\n\n        from bokeh.util.deprecation import deprecated\n        deprecated(\"ClientSession.loop_until_closed is deprecated, and will be removed in an eventual 2.0 release. \"\n                   \"Run Bokeh applications directly on a Bokeh server instead. See:\\n\\n\"\n                   \"    https//docs.bokeh.org/en/latest/docs/user_guide/server.html\\n\")\n\n        self._connection.loop_until_closed()", "output": "Execute a blocking loop that runs and executes event callbacks\n        until the connection is closed (e.g. by hitting Ctrl-C).\n\n        While this method can be used to run Bokeh application code \"outside\"\n        the Bokeh server, this practice is HIGHLY DISCOURAGED for any real\n        use case. This function is intented to facilitate testing ONLY.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def base_url(self) -> _URL:\n        \"\"\"\"\"\"\n\n        # Support for <base> tag.\n        base = self.find('base', first=True)\n        if base:\n            result = base.attrs.get('href', '').strip()\n            if result:\n                return result\n\n        # Parse the url to separate out the path\n        parsed = urlparse(self.url)._asdict()\n\n        # Remove any part of the path after the last '/'\n        parsed['path'] = '/'.join(parsed['path'].split('/')[:-1]) + '/'\n\n        # Reconstruct the url with the modified path\n        parsed = (v for v in parsed.values())\n        url = urlunparse(parsed)\n\n        return url", "output": "The base URL for the page. Supports the ``<base>`` tag\n        (`learn more <https://www.w3schools.com/tags/tag_base.asp>`_).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _object_reducer(o, names=('id', 'name', 'path', 'httpMethod',\n                              'statusCode', 'Created', 'Deleted',\n                              'Updated', 'Flushed', 'Associated', 'Disassociated')):\n    '''\n    \n    '''\n    result = {}\n    if isinstance(o, dict):\n        for k, v in six.iteritems(o):\n            if isinstance(v, dict):\n                reduced = v if k == 'variables' else _object_reducer(v, names)\n                if reduced or _name_matches(k, names):\n                    result[k] = reduced\n            elif isinstance(v, list):\n                newlist = []\n                for val in v:\n                    reduced = _object_reducer(val, names)\n                    if reduced or _name_matches(k, names):\n                        newlist.append(reduced)\n                if newlist:\n                    result[k] = newlist\n            else:\n                if _name_matches(k, names):\n                    result[k] = v\n    return result", "output": "Helper function to reduce the amount of information that will be kept in the change log\n    for API GW related return values", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_last_visible_toolbars(self):\r\n        \"\"\"\"\"\"\r\n        toolbars_names = CONF.get('main', 'last_visible_toolbars', default=[])\r\n\r\n        if toolbars_names:\r\n            dic = {}\r\n            for toolbar in self.toolbarslist:\r\n                dic[toolbar.objectName()] = toolbar\r\n\r\n            toolbars = []\r\n            for name in toolbars_names:\r\n                if name in dic:\r\n                    toolbars.append(dic[name])\r\n            self.visible_toolbars = toolbars\r\n        else:\r\n            self.get_visible_toolbars()\r\n        self._update_show_toolbars_action()", "output": "Loads the last visible toolbars from the .ini file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enable_logging(self, bucket_name, object_prefix=\"\"):\n        \"\"\"\n        \"\"\"\n        info = {\"logBucket\": bucket_name, \"logObjectPrefix\": object_prefix}\n        self._patch_property(\"logging\", info)", "output": "Enable access logging for this bucket.\n\n        See https://cloud.google.com/storage/docs/access-logs\n\n        :type bucket_name: str\n        :param bucket_name: name of bucket in which to store access logs\n\n        :type object_prefix: str\n        :param object_prefix: prefix for access log filenames", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_index(self):\n        \"\"\"\n        \n        \"\"\"\n        es = self._init_connection()\n        if es.indices.exists(index=self.index):\n            es.indices.delete(index=self.index)", "output": "Delete the index, if it exists.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def time_pad(x, filter_size, dilations):\n  \"\"\"\n  \"\"\"\n  x_shape = common_layers.shape_list(x)\n  if filter_size == [1, 1, 1]:\n    return x\n  _, h, w = filter_size\n  eff_h = h + (h - 1)*(dilations[2] - 1)\n  eff_w = w + (w - 1)*(dilations[3] - 1)\n  a = (eff_h - 1) // 2  # vertical padding size\n  b = (eff_w - 1) // 2  # horizontal padding size\n  c = filter_size[0] - 1\n\n  # pad across edges.\n  padding = [[0, 0], [c, 0], [a, a], [b, b], [0, 0]]\n\n  # concat a binary feature across channels to indicate a padding.\n  # 1 indicates that the feature is a padding.\n  x_bias = tf.zeros(x_shape[:-1] + [1])\n  x_bias = tf.pad(x_bias, padding, constant_values=1)\n  x_pad = tf.pad(x, padding)\n  x_pad = tf.concat((x_bias, x_pad), axis=-1)\n  return x_pad", "output": "Pad left across time and pad valid across the spatial components.\n\n  Also concats a binary feature that indicates if a feature is padded or not.\n\n  Args:\n    x: 5-D Tensor, (NTHWC)\n    filter_size: list of ints\n    dilations: list of ints, dilations - 1 specifies the number of holes\n               between two filter elements.\n  Returns:\n    x_pad: 5-D Tensor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fprop(self, x, dropout=False, dropout_dict=None, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    include_prob = self.include_prob\n    if dropout_dict is not None:\n      assert dropout\n      if self.name in dropout_dict:\n        include_prob = dropout_dict[self.name]\n    if dropout:\n      return tf.nn.dropout(x, include_prob)\n    return x", "output": "Forward propagation as either no-op or dropping random units.\n    :param x: The input to the layer\n    :param dropout: bool specifying whether to drop units\n    :param dropout_dict: dict\n        This dictionary is usually not needed.\n        In rare cases, generally for research purposes, this dictionary\n        makes it possible to run forward propagation with a different\n        dropout include probability.\n        This dictionary should be passed as a named argument to the MLP\n        class, which will then pass it to *all* layers' fprop methods.\n        Other layers will just receive this as an ignored kwargs entry.\n        Each dropout layer looks up its own name in this dictionary\n        to read out its include probability.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cli(self, eauth):\n        '''\n        \n        '''\n        ret = {}\n        if not eauth:\n            print('External authentication system has not been specified')\n            return ret\n        fstr = '{0}.auth'.format(eauth)\n        if fstr not in self.auth:\n            print(('The specified external authentication system \"{0}\" is '\n                   'not available').format(eauth))\n            print(\"Available eauth types: {0}\".format(\", \".join(self.auth.file_mapping.keys())))\n            return ret\n\n        args = salt.utils.args.arg_lookup(self.auth[fstr])\n        for arg in args['args']:\n            if arg in self.opts:\n                ret[arg] = self.opts[arg]\n            elif arg.startswith('pass'):\n                ret[arg] = getpass.getpass('{0}: '.format(arg))\n            else:\n                ret[arg] = input('{0}: '.format(arg))\n        for kwarg, default in list(args['kwargs'].items()):\n            if kwarg in self.opts:\n                ret['kwarg'] = self.opts[kwarg]\n            else:\n                ret[kwarg] = input('{0} [{1}]: '.format(kwarg, default))\n\n        # Use current user if empty\n        if 'username' in ret and not ret['username']:\n            ret['username'] = salt.utils.user.get_user()\n\n        return ret", "output": "Execute the CLI options to fill in the extra data needed for the\n        defined eauth system", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_context(context_file='cookiecutter.json', default_context=None,\n                     extra_context=None):\n    \"\"\"\n    \"\"\"\n    context = OrderedDict([])\n\n    try:\n        with open(context_file) as file_handle:\n            obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n    except ValueError as e:\n        # JSON decoding error.  Let's throw a new exception that is more\n        # friendly for the developer or user.\n        full_fpath = os.path.abspath(context_file)\n        json_exc_message = str(e)\n        our_exc_message = (\n            'JSON decoding error while loading \"{0}\".  Decoding'\n            ' error details: \"{1}\"'.format(full_fpath, json_exc_message))\n        raise ContextDecodingException(our_exc_message)\n\n    # Add the Python object to the context dictionary\n    file_name = os.path.split(context_file)[1]\n    file_stem = file_name.split('.')[0]\n    context[file_stem] = obj\n\n    # Overwrite context variable defaults with the default context from the\n    # user's global config, if available\n    if default_context:\n        apply_overwrites_to_context(obj, default_context)\n    if extra_context:\n        apply_overwrites_to_context(obj, extra_context)\n\n    logger.debug('Context generated is {}'.format(context))\n    return context", "output": "Generate the context for a Cookiecutter project template.\n\n    Loads the JSON file as a Python object, with key being the JSON filename.\n\n    :param context_file: JSON file containing key/value pairs for populating\n        the cookiecutter's variables.\n    :param default_context: Dictionary containing config to take into account.\n    :param extra_context: Dictionary containing configuration overrides", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uniform(low, high, random_state):\n    '''\n    \n    '''\n    assert high > low, 'Upper bound must be larger than lower bound'\n    return random_state.uniform(low, high)", "output": "low: an float that represent an lower bound\n    high: an float that represent an upper bound\n    random_state: an object of numpy.random.RandomState", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fib_iter(n):\n    \"\"\"\n    \"\"\"\n\n    # precondition\n    assert n >= 0, 'n must be positive integer'\n\n    fib_1 = 0\n    fib_2 = 1\n    sum = 0\n    if n <= 1:\n        return n\n    for _ in range(n-1):\n        sum = fib_1 + fib_2\n        fib_1 = fib_2\n        fib_2 = sum\n    return sum", "output": "[summary]\n    Works iterative approximate O(n)\n\n    Arguments:\n        n {[int]} -- [description]\n    \n    Returns:\n        [int] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wheels(opts, whitelist=None, context=None):\n    '''\n    \n    '''\n    if context is None:\n        context = {}\n    return LazyLoader(\n        _module_dirs(opts, 'wheel'),\n        opts,\n        tag='wheel',\n        whitelist=whitelist,\n        pack={'__context__': context},\n    )", "output": "Returns the wheels modules", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def destroy(name, stop=False, path=None):\n    '''\n    \n    '''\n    _ensure_exists(name, path=path)\n    if not stop and state(name, path=path) != 'stopped':\n        raise CommandExecutionError(\n            'Container \\'{0}\\' is not stopped'.format(name)\n        )\n    return _change_state('lxc-destroy', name, None, path=path)", "output": "Destroy the named container.\n\n    .. warning::\n\n        Destroys all data associated with the container.\n\n    path\n        path to the container parent directory (default: /var/lib/lxc)\n\n        .. versionadded:: 2015.8.0\n\n    stop : False\n        If ``True``, the container will be destroyed even if it is\n        running/frozen.\n\n        .. versionchanged:: 2015.5.0\n            Default value changed to ``False``. This more closely matches the\n            behavior of ``lxc-destroy(1)``, and also makes it less likely that\n            an accidental command will destroy a running container that was\n            being used for important things.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' lxc.destroy foo\n        salt '*' lxc.destroy foo stop=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def scan(self, search_path=None):\n        \"\"\"\n        \"\"\"\n        if search_path is None:\n            search_path = sys.path\n\n        for item in search_path:\n            for dist in find_distributions(item):\n                self.add(dist)", "output": "Scan `search_path` for distributions usable in this environment\n\n        Any distributions found are added to the environment.\n        `search_path` should be a sequence of ``sys.path`` items.  If not\n        supplied, ``sys.path`` is used.  Only distributions conforming to\n        the platform/python version defined at initialization are added.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(self):\n        \"\"\"\n        \"\"\"\n        try:\n            self._client.instance_admin_client.get_instance(name=self.name)\n            return True\n        # NOTE: There could be other exceptions that are returned to the user.\n        except NotFound:\n            return False", "output": "Check whether the instance already exists.\n\n        For example:\n\n        .. literalinclude:: snippets.py\n            :start-after: [START bigtable_check_instance_exists]\n            :end-before: [END bigtable_check_instance_exists]\n\n        :rtype: bool\n        :returns: True if the table exists, else False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_rnn_checkpoint(cells, prefix, epoch, symbol, arg_params, aux_params):\n    \"\"\"\n    \"\"\"\n    if isinstance(cells, BaseRNNCell):\n        cells = [cells]\n    for cell in cells:\n        arg_params = cell.unpack_weights(arg_params)\n    save_checkpoint(prefix, epoch, symbol, arg_params, aux_params)", "output": "Save checkpoint for model using RNN cells.\n    Unpacks weight before saving.\n\n    Parameters\n    ----------\n    cells : mxnet.rnn.RNNCell or list of RNNCells\n        The RNN cells used by this symbol.\n    prefix : str\n        Prefix of model name.\n    epoch : int\n        The epoch number of the model.\n    symbol : Symbol\n        The input symbol\n    arg_params : dict of str to NDArray\n        Model parameter, dict of name to NDArray of net's weights.\n    aux_params : dict of str to NDArray\n        Model parameter, dict of name to NDArray of net's auxiliary states.\n\n    Notes\n    -----\n    - ``prefix-symbol.json`` will be saved for symbol.\n    - ``prefix-epoch.params`` will be saved for parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _new_Index(cls, d):\n    \"\"\"\n    \n    \"\"\"\n    # required for backward compat, because PI can't be instantiated with\n    # ordinals through __new__ GH #13277\n    if issubclass(cls, ABCPeriodIndex):\n        from pandas.core.indexes.period import _new_PeriodIndex\n        return _new_PeriodIndex(cls, **d)\n    return cls.__new__(cls, **d)", "output": "This is called upon unpickling, rather than the default which doesn't\n    have arguments and breaks __new__.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def execute_lines(self, lines):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        for line in lines.splitlines():\r\n            stripped_line = line.strip()\r\n            if stripped_line.startswith('#'):\r\n                continue\r\n            self.write(line+os.linesep, flush=True)\r\n            self.execute_command(line+\"\\n\")\r\n            self.flush()", "output": "Execute a set of lines as multiple command\r\n        lines: multiple lines of text to be executed as single commands", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_states(batch_size, num_lstm_layer, num_hidden):\n    \"\"\"\n    \n    \"\"\"\n    init_c = [('l%d_init_c' % l, (batch_size, num_hidden)) for l in range(num_lstm_layer)]\n    init_h = [('l%d_init_h' % l, (batch_size, num_hidden)) for l in range(num_lstm_layer)]\n    return init_c + init_h", "output": "Returns name and shape of init states of LSTM network\n\n    Parameters\n    ----------\n    batch_size: list of tuple of str and tuple of int and int\n    num_lstm_layer: int\n    num_hidden: int\n\n    Returns\n    -------\n    list of tuple of str and tuple of int and int", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_kvstore(kvstore, num_device, arg_params):\n    \"\"\"\n    \"\"\"\n    update_on_kvstore = bool(int(os.getenv('MXNET_UPDATE_ON_KVSTORE', \"1\")))\n    if kvstore is None:\n        kv = None\n    elif isinstance(kvstore, kvs.KVStore):\n        kv = kvstore\n    elif isinstance(kvstore, str):\n        # create kvstore using the string type\n        if num_device == 1 and 'dist' not in kvstore:\n            # no need to use kv for single device and single machine\n            kv = None\n        else:\n            kv = kvs.create(kvstore)\n            if kvstore == 'local':\n            # automatically select a proper local\n                max_size = max(np.prod(param.shape) for param in\n                               arg_params.values())\n                if max_size > 1024 * 1024 * 16:\n                    update_on_kvstore = False\n    else:\n        raise TypeError('kvstore must be KVStore, str or None')\n\n    if kv is None:\n        update_on_kvstore = False\n\n    return (kv, update_on_kvstore)", "output": "Create kvstore\n    This function select and create a proper kvstore if given the kvstore type.\n\n    Parameters\n    ----------\n    kvstore : KVStore or str\n        The kvstore.\n    num_device : int\n        The number of devices\n    arg_params : dict of str to `NDArray`.\n        Model parameter, dict of name to `NDArray` of net's weights.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_local_users(self, disabled=None):\n        '''\n        \n        '''\n        users = dict()\n        path = '/etc/passwd'\n        with salt.utils.files.fopen(path, 'r') as fp_:\n            for line in fp_:\n                line = line.strip()\n                if ':' not in line:\n                    continue\n                name, password, uid, gid, gecos, directory, shell = line.split(':')\n                active = not (password == '*' or password.startswith('!'))\n                if (disabled is False and active) or (disabled is True and not active) or disabled is None:\n                    users[name] = {\n                        'uid': uid,\n                        'git': gid,\n                        'info': gecos,\n                        'home': directory,\n                        'shell': shell,\n                        'disabled': not active\n                    }\n\n        return users", "output": "Return all known local accounts to the system.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_triggers(name, location='\\\\'):\n    \n    '''\n    # Create the task service object\n    with salt.utils.winapi.Com():\n        task_service = win32com.client.Dispatch(\"Schedule.Service\")\n    task_service.Connect()\n\n    # Get the folder to list folders from\n    task_folder = task_service.GetFolder(location)\n    task_definition = task_folder.GetTask(name).Definition\n    triggers = task_definition.Triggers\n\n    ret = []\n    for trigger in triggers:\n        ret.append(trigger.Id)\n\n    return ret", "output": "r'''\n    List all triggers that pertain to a task in the specified location.\n\n    :param str name: The name of the task for which list triggers.\n\n    :param str location: A string value representing the location of the task\n        from which to list triggers. Default is '\\\\' which is the root for the\n        task scheduler (C:\\Windows\\System32\\tasks).\n\n    :return: Returns a list of triggers.\n    :rtype: list\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'minion-id' task.list_triggers <task_name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sell_open(id_or_ins, amount, price=None, style=None):\n    \"\"\"\n    \n    \"\"\"\n    return order(id_or_ins, amount, SIDE.SELL, POSITION_EFFECT.OPEN, cal_style(price, style))", "output": "\u5356\u51fa\u5f00\u4ed3\n\n    :param id_or_ins: \u4e0b\u5355\u6807\u7684\u7269\n    :type id_or_ins: :class:`~Instrument` object | `str` | List[:class:`~Instrument`] | List[`str`]\n\n    :param int amount: \u4e0b\u5355\u624b\u6570\n\n    :param float price: \u4e0b\u5355\u4ef7\u683c\uff0c\u9ed8\u8ba4\u4e3aNone\uff0c\u8868\u793a :class:`~MarketOrder`, \u6b64\u53c2\u6570\u4e3b\u8981\u7528\u4e8e\u7b80\u5316 `style` \u53c2\u6570\u3002\n\n    :param style: \u4e0b\u5355\u7c7b\u578b, \u9ed8\u8ba4\u662f\u5e02\u4ef7\u5355\u3002\u76ee\u524d\u652f\u6301\u7684\u8ba2\u5355\u7c7b\u578b\u6709 :class:`~LimitOrder` \u548c :class:`~MarketOrder`\n    :type style: `OrderStyle` object\n\n    :return: :class:`~Order` object | None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self, fnames=None):\r\n        \"\"\"\"\"\"\r\n        if fnames is None:\r\n            fnames = self.get_selected_filenames()\r\n        for fname in fnames:\r\n            self.sig_run.emit(fname)", "output": "Run Python scripts", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _load_keras_model(model_network_path, model_weight_path, custom_objects=None):\n    \"\"\"\n    \"\"\"\n    from keras.models import model_from_json\n    import json\n\n    # Load the model network\n    json_file = open(model_network_path, 'r')\n    loaded_model_json = json_file.read()\n    json_file.close()\n\n    if not custom_objects:\n        custom_objects = {}\n\n    # Load the model weights\n    loaded_model = model_from_json(loaded_model_json, custom_objects=custom_objects)\n    loaded_model.load_weights(model_weight_path)\n\n    return loaded_model", "output": "Load a keras model from disk\n\n    Parameters\n    ----------\n    model_network_path: str\n        Path where the model network path is (json file)\n\n    model_weight_path: str\n        Path where the model network weights are (hd5 file)\n\n    custom_objects:\n        A dictionary of layers or other custom classes\n        or functions used by the model\n\n    Returns\n    -------\n    model: A keras model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _protobuf_value_to_string(value):\n  \"\"\"\n  \"\"\"\n  value_in_json = json_format.MessageToJson(value)\n  if value.HasField(\"string_value\"):\n    # Remove the quotations.\n    return value_in_json[1:-1]\n  return value_in_json", "output": "Returns a string representation of given google.protobuf.Value message.\n\n  Args:\n    value: google.protobuf.Value message. Assumed to be of type 'number',\n      'string' or 'bool'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forward(self, data_batch, is_train=None):\n        \"\"\"\n        \"\"\"\n        assert self.binded and self.params_initialized\n        self.switch_bucket(data_batch.bucket_key, data_batch.provide_data,\n                           data_batch.provide_label)\n        self._curr_module.forward(data_batch, is_train=is_train)", "output": "Forward computation.\n\n        Parameters\n        ----------\n        data_batch : DataBatch\n        is_train : bool\n            Defaults to ``None``, in which case `is_train` is take as ``self.for_training``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_bigram_pair_string(self, text):\n        \"\"\"\n        \n        \"\"\"\n        bigram_pairs = []\n\n        if len(text) <= 2:\n            text_without_punctuation = text.translate(self.punctuation_table)\n            if len(text_without_punctuation) >= 1:\n                text = text_without_punctuation\n\n        document = self.nlp(text)\n\n        if len(text) <= 2:\n            bigram_pairs = [\n                token.lemma_.lower() for token in document\n            ]\n        else:\n            tokens = [\n                token for token in document if token.is_alpha and not token.is_stop\n            ]\n\n            if len(tokens) < 2:\n                tokens = [\n                    token for token in document if token.is_alpha\n                ]\n\n            for index in range(1, len(tokens)):\n                bigram_pairs.append('{}:{}'.format(\n                    tokens[index - 1].pos_,\n                    tokens[index].lemma_.lower()\n                ))\n\n        if not bigram_pairs:\n            bigram_pairs = [\n                token.lemma_.lower() for token in document\n            ]\n\n        return ' '.join(bigram_pairs)", "output": "Return a string of text containing part-of-speech, lemma pairs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def googlenet_resize(im, targ, min_area_frac, min_aspect_ratio, max_aspect_ratio, flip_hw_p, interpolation=cv2.INTER_AREA):\n    \"\"\" \n    \"\"\"\n    h,w,*_ = im.shape\n    area = h*w\n    for _ in range(10):\n        targetArea = random.uniform(min_area_frac, 1.0) * area\n        aspectR = random.uniform(min_aspect_ratio, max_aspect_ratio)\n        ww = int(np.sqrt(targetArea * aspectR) + 0.5)\n        hh = int(np.sqrt(targetArea / aspectR) + 0.5)\n        if flip_hw_p:\n            ww, hh = hh, ww\n        if hh <= h and ww <= w:\n            x1 = 0 if w == ww else random.randint(0, w - ww)\n            y1 = 0 if h == hh else random.randint(0, h - hh)\n            out = im[y1:y1 + hh, x1:x1 + ww]\n            out = cv2.resize(out, (targ, targ), interpolation=interpolation)\n            return out\n    out = scale_min(im, targ, interpolation=interpolation)\n    out = center_crop(out)\n    return out", "output": "Randomly crop an image with an aspect ratio and returns a squared resized image of size targ\n    \n    References:\n    1. https://arxiv.org/pdf/1409.4842.pdf\n    2. https://arxiv.org/pdf/1802.07888.pdf", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_json(self, indent=None, separators=None, sort_keys=False):\n    \"\"\"\n    \"\"\"\n    def remove_callables(x):\n      \"\"\"Omit callable elements from input with arbitrary nesting.\"\"\"\n      if isinstance(x, dict):\n        return {k: remove_callables(v) for k, v in six.iteritems(x)\n                if not callable(v)}\n      elif isinstance(x, list):\n        return [remove_callables(i) for i in x if not callable(i)]\n      return x\n    return json.dumps(\n        remove_callables(self.values()),\n        indent=indent,\n        separators=separators,\n        sort_keys=sort_keys)", "output": "Serializes the hyperparameters into JSON.\n\n    Args:\n      indent: If a non-negative integer, JSON array elements and object members\n        will be pretty-printed with that indent level. An indent level of 0, or\n        negative, will only insert newlines. `None` (the default) selects the\n        most compact representation.\n      separators: Optional `(item_separator, key_separator)` tuple. Default is\n        `(', ', ': ')`.\n      sort_keys: If `True`, the output dictionaries will be sorted by key.\n\n    Returns:\n      A JSON string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_hsv(cls, h, s, v):\n        \"\"\"\"\"\"\n        rgb = colorsys.hsv_to_rgb(h, s, v)\n        return cls.from_rgb(*(int(x * 255) for x in rgb))", "output": "Constructs a :class:`Colour` from an HSV tuple.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convertVectorColumnsToML(dataset, *cols):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(dataset, DataFrame):\n            raise TypeError(\"Input dataset must be a DataFrame but got {}.\".format(type(dataset)))\n        return callMLlibFunc(\"convertVectorColumnsToML\", dataset, list(cols))", "output": "Converts vector columns in an input DataFrame from the\n        :py:class:`pyspark.mllib.linalg.Vector` type to the new\n        :py:class:`pyspark.ml.linalg.Vector` type under the `spark.ml`\n        package.\n\n        :param dataset:\n          input dataset\n        :param cols:\n          a list of vector columns to be converted.\n          New vector columns will be ignored. If unspecified, all old\n          vector columns will be converted excepted nested ones.\n        :return:\n          the input dataset with old vector columns converted to the\n          new vector type\n\n        >>> import pyspark\n        >>> from pyspark.mllib.linalg import Vectors\n        >>> from pyspark.mllib.util import MLUtils\n        >>> df = spark.createDataFrame(\n        ...     [(0, Vectors.sparse(2, [1], [1.0]), Vectors.dense(2.0, 3.0))],\n        ...     [\"id\", \"x\", \"y\"])\n        >>> r1 = MLUtils.convertVectorColumnsToML(df).first()\n        >>> isinstance(r1.x, pyspark.ml.linalg.SparseVector)\n        True\n        >>> isinstance(r1.y, pyspark.ml.linalg.DenseVector)\n        True\n        >>> r2 = MLUtils.convertVectorColumnsToML(df, \"x\").first()\n        >>> isinstance(r2.x, pyspark.ml.linalg.SparseVector)\n        True\n        >>> isinstance(r2.y, pyspark.mllib.linalg.DenseVector)\n        True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def relpath(self, current_file, rel_path):\n        \"\"\"\n        \n        \"\"\"\n        script_dir = os.path.dirname(os.path.abspath(current_file))\n        rel_path = os.path.abspath(os.path.join(script_dir, rel_path))\n        return rel_path", "output": "Compute path given current file and relative path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def begin_transaction(self, project_id, transaction_options=None):\n        \"\"\"\n        \"\"\"\n        request_pb = _datastore_pb2.BeginTransactionRequest()\n        return _rpc(\n            self.client._http,\n            project_id,\n            \"beginTransaction\",\n            self.client._base_url,\n            request_pb,\n            _datastore_pb2.BeginTransactionResponse,\n        )", "output": "Perform a ``beginTransaction`` request.\n\n        :type project_id: str\n        :param project_id: The project to connect to. This is\n                           usually your project name in the cloud console.\n\n        :type transaction_options: ~.datastore_v1.types.TransactionOptions\n        :param transaction_options: (Optional) Options for a new transaction.\n\n        :rtype: :class:`.datastore_pb2.BeginTransactionResponse`\n        :returns: The returned protobuf response object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vq_discrete_bottleneck(x, hparams):\n  \"\"\"\"\"\"\n  tf.logging.info(\"Using EMA with beta = {}\".format(hparams.beta))\n  bottleneck_size = 2**hparams.bottleneck_bits\n  x_shape = common_layers.shape_list(x)\n  x = tf.reshape(x, [-1, hparams.hidden_size])\n  x_means_hot, e_loss = vq_nearest_neighbor(\n      x, hparams)\n  means, ema_means, ema_count = (hparams.means, hparams.ema_means,\n                                 hparams.ema_count)\n\n  # Update the ema variables\n  updated_ema_count = moving_averages.assign_moving_average(\n      ema_count,\n      tf.reduce_sum(x_means_hot, axis=0),\n      hparams.decay,\n      zero_debias=False)\n\n  dw = tf.matmul(x_means_hot, x, transpose_a=True)\n  updated_ema_means = moving_averages.assign_moving_average(\n      ema_means, dw, hparams.decay, zero_debias=False)\n  n = tf.reduce_sum(updated_ema_count, axis=-1, keepdims=True)\n  updated_ema_count = (\n      (updated_ema_count + hparams.epsilon) /\n      (n + bottleneck_size * hparams.epsilon) * n)\n  # pylint: disable=g-no-augmented-assignment\n  updated_ema_means = updated_ema_means / tf.expand_dims(\n      updated_ema_count, axis=-1)\n  # pylint: enable=g-no-augmented-assignment\n  with tf.control_dependencies([e_loss]):\n    update_means = tf.assign(means, updated_ema_means)\n    with tf.control_dependencies([update_means]):\n      loss = hparams.beta * e_loss\n\n  discrete = tf.reshape(x_means_hot, x_shape[:-1] + [bottleneck_size])\n  return discrete, loss", "output": "Simple vector quantized discrete bottleneck.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def logger_format(self, value):\n        \"\"\"\n        \n        \"\"\"\n        self.__logger_format = value\n        self.logger_formatter = logging.Formatter(self.__logger_format)", "output": "Sets the logger_format.\n\n        The logger_formatter will be updated when sets logger_format.\n\n        :param value: The format string.\n        :type: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_nb(fname):\n    \"\"\n    with open(fname,'r') as f: return nbformat.reads(f.read(), as_version=4)", "output": "Read a notebook in `fname` and return its corresponding json", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def method_returns_something(self):\n        '''\n        \n        '''\n\n        def get_returns_not_on_nested_functions(node):\n            returns = [node] if isinstance(node, ast.Return) else []\n            for child in ast.iter_child_nodes(node):\n                # Ignore nested functions and its subtrees.\n                if not isinstance(child, ast.FunctionDef):\n                    child_returns = get_returns_not_on_nested_functions(child)\n                    returns.extend(child_returns)\n            return returns\n\n        tree = ast.parse(self.method_source).body\n        if tree:\n            returns = get_returns_not_on_nested_functions(tree[0])\n            return_values = [r.value for r in returns]\n            # Replace NameConstant nodes valued None for None.\n            for i, v in enumerate(return_values):\n                if isinstance(v, ast.NameConstant) and v.value is None:\n                    return_values[i] = None\n            return any(return_values)\n        else:\n            return False", "output": "Check if the docstrings method can return something.\n\n        Bare returns, returns valued None and returns from nested functions are\n        disconsidered.\n\n        Returns\n        -------\n        bool\n            Whether the docstrings method can return something.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fillna(self, value, limit=None, inplace=False, downcast=None):\n        \"\"\" \n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n\n        if not self._can_hold_na:\n            if inplace:\n                return self\n            else:\n                return self.copy()\n\n        mask = isna(self.values)\n        if limit is not None:\n            if not is_integer(limit):\n                raise ValueError('Limit must be an integer')\n            if limit < 1:\n                raise ValueError('Limit must be greater than 0')\n            if self.ndim > 2:\n                raise NotImplementedError(\"number of dimensions for 'fillna' \"\n                                          \"is currently limited to 2\")\n            mask[mask.cumsum(self.ndim - 1) > limit] = False\n\n        # fillna, but if we cannot coerce, then try again as an ObjectBlock\n        try:\n            values, _ = self._try_coerce_args(self.values, value)\n            blocks = self.putmask(mask, value, inplace=inplace)\n            blocks = [b.make_block(values=self._try_coerce_result(b.values))\n                      for b in blocks]\n            return self._maybe_downcast(blocks, downcast)\n        except (TypeError, ValueError):\n\n            # we can't process the value, but nothing to do\n            if not mask.any():\n                return self if inplace else self.copy()\n\n            # operate column-by-column\n            def f(m, v, i):\n                block = self.coerce_to_target_dtype(value)\n\n                # slice out our block\n                if i is not None:\n                    block = block.getitem_block(slice(i, i + 1))\n                return block.fillna(value,\n                                    limit=limit,\n                                    inplace=inplace,\n                                    downcast=None)\n\n            return self.split_and_operate(mask, f, inplace)", "output": "fillna on the block with the value. If we fail, then convert to\n        ObjectBlock and try again", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def profile(self):\n        \"\"\"\n        \"\"\"\n\n        state = self._state\n        data = await state.http.get_user_profile(self.id)\n\n        def transform(d):\n            return state._get_guild(int(d['id']))\n\n        since = data.get('premium_since')\n        mutual_guilds = list(filter(None, map(transform, data.get('mutual_guilds', []))))\n        return Profile(flags=data['user'].get('flags', 0),\n                       premium_since=parse_time(since),\n                       mutual_guilds=mutual_guilds,\n                       user=self,\n                       connected_accounts=data['connected_accounts'])", "output": "|coro|\n\n        Gets the user's profile.\n\n        .. note::\n\n            This only applies to non-bot accounts.\n\n        Raises\n        -------\n        Forbidden\n            Not allowed to fetch profiles.\n        HTTPException\n            Fetching the profile failed.\n\n        Returns\n        --------\n        :class:`Profile`\n            The profile of the user.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy(self):\r\n        \"\"\"\"\"\"\r\n        cliptxt = self._sel_to_text( self.selectedIndexes() )\r\n        clipboard = QApplication.clipboard()\r\n        clipboard.setText(cliptxt)", "output": "Copy text to clipboard", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _to_java_object_rdd(rdd):\n    \"\"\" \n    \"\"\"\n    rdd = rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))\n    return rdd.ctx._jvm.org.apache.spark.ml.python.MLSerDe.pythonToJava(rdd._jrdd, True)", "output": "Return an JavaRDD of Object by unpickling\n\n    It will convert each Python object into Java object by Pyrolite, whenever the\n    RDD is serialized in batch or not.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_get_realtime(code, market):\n    \"\"\"\n    \n    \"\"\"\n    res = None\n    if market == MARKET_TYPE.STOCK_CN:\n        res = QATdx.QA_fetch_get_stock_realtime(code)\n    elif market == MARKET_TYPE.FUTURE_CN:\n        res = QATdx.QA_fetch_get_future_realtime(code)\n\n    return res", "output": "\u7edf\u4e00\u7684\u83b7\u53d6\u671f\u8d27/\u80a1\u7968\u5b9e\u65f6\u884c\u60c5\u7684\u63a5\u53e3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dl_dirname(url):\n  \"\"\"\"\"\"\n  checksum = hashlib.sha256(tf.compat.as_bytes(url)).hexdigest()\n  return get_dl_fname(url, checksum)", "output": "Returns name of temp dir for given url.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare_post_parameters(self, post_params=None, files=None):\n        \"\"\"\n        \"\"\"\n        params = []\n\n        if post_params:\n            params = post_params\n\n        if files:\n            for k, v in six.iteritems(files):\n                if not v:\n                    continue\n                file_names = v if type(v) is list else [v]\n                for n in file_names:\n                    with open(n, 'rb') as f:\n                        filename = os.path.basename(f.name)\n                        filedata = f.read()\n                        mimetype = (mimetypes.guess_type(filename)[0] or\n                                    'application/octet-stream')\n                        params.append(\n                            tuple([k, tuple([filename, filedata, mimetype])]))\n\n        return params", "output": "Builds form parameters.\n\n        :param post_params: Normal form parameters.\n        :param files: File parameters.\n        :return: Form parameters with files.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sendline(command, method='cli_show_ascii', **kwargs):\n    '''\n    \n    '''\n    try:\n        if CONNECTION == 'ssh':\n            result = _sendline_ssh(command, **kwargs)\n        elif CONNECTION == 'nxapi':\n            result = _nxapi_request(command, method, **kwargs)\n    except (TerminalException, NxosCliError) as e:\n        log.error(e)\n        raise\n    return result", "output": "Send arbitrary show or config commands to the NX-OS device.\n\n    command\n        The command to be sent.\n\n    method:\n        ``cli_show_ascii``: Return raw test or unstructured output.\n        ``cli_show``: Return structured output.\n        ``cli_conf``: Send configuration commands to the device.\n        Defaults to ``cli_show_ascii``.\n\n        NOTES for SSH proxy minon:\n          ``method`` is ignored for SSH proxy minion.\n          Only show commands are supported and data is returned unstructured.\n          This function is preserved for backwards compatibilty.\n\n    .. code-block: bash\n\n        salt '*' nxos.cmd sendline 'show run | include \"^username admin password\"'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_data_csv(fname, frames, preproc):\n   \"\"\"\"\"\"\n   fdata = open(fname, \"w\")\n   dr = Parallel()(delayed(get_data)(lst,preproc) for lst in frames)\n   data,result = zip(*dr)\n   for entry in data:\n      fdata.write(','.join(entry)+'\\r\\n')\n   print(\"All finished, %d slices in total\" % len(data))\n   fdata.close()\n   result = np.ravel(result)\n   return result", "output": "Write data to csv file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dec(data, **kwargs):\n    '''\n    \n    '''\n    if 'keyfile' in kwargs:\n        salt.utils.versions.warn_until(\n            'Neon',\n            'The \\'keyfile\\' argument has been deprecated and will be removed in Salt '\n            '{version}. Please use \\'sk_file\\' argument instead.'\n        )\n        kwargs['sk_file'] = kwargs['keyfile']\n\n        # set boxtype to `secretbox` to maintain backward compatibility\n        kwargs['box_type'] = 'secretbox'\n\n    if 'key' in kwargs:\n        salt.utils.versions.warn_until(\n            'Neon',\n            'The \\'key\\' argument has been deprecated and will be removed in Salt '\n            '{version}. Please use \\'sk\\' argument instead.'\n        )\n        kwargs['sk'] = kwargs['key']\n\n        # set boxtype to `secretbox` to maintain backward compatibility\n        kwargs['box_type'] = 'secretbox'\n\n    box_type = _get_config(**kwargs)['box_type']\n    if box_type == 'secretbox':\n        return secretbox_decrypt(data, **kwargs)\n    return sealedbox_decrypt(data, **kwargs)", "output": "Alias to `{box_type}_decrypt`\n\n    box_type: secretbox, sealedbox(default)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(self):\n        '''\n        \n        '''\n        super(ProxyMinion, self).start()\n        try:\n            if check_user(self.config['user']):\n                self.action_log_info('The Proxy Minion is starting up')\n                self.verify_hash_type()\n                self.minion.tune_in()\n                if self.minion.restart:\n                    raise SaltClientError('Proxy Minion could not connect to Master')\n        except (KeyboardInterrupt, SaltSystemExit) as exc:\n            self.action_log_info('Proxy Minion Stopping')\n            if isinstance(exc, KeyboardInterrupt):\n                log.warning('Exiting on Ctrl-c')\n                self.shutdown()\n            else:\n                log.error(exc)\n                self.shutdown(exc.code)", "output": "Start the actual proxy minion.\n\n        If sub-classed, don't **ever** forget to run:\n\n            super(YourSubClass, self).start()\n\n        NOTE: Run any required code before calling `super()`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_weight_histograms(self, iteration:int)->None:\n        \"\"\n        generator, critic = self.learn.gan_trainer.generator, self.learn.gan_trainer.critic\n        self.hist_writer.write(model=generator, iteration=iteration, tbwriter=self.tbwriter, name='generator')\n        self.hist_writer.write(model=critic,    iteration=iteration, tbwriter=self.tbwriter, name='critic')", "output": "Writes model weight histograms to Tensorboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save(self):\n        \"\"\"\n        \"\"\"\n        self.client.update(\n            {\n                'portfolio_cookie': self.portfolio_cookie,\n                'user_cookie': self.user_cookie\n            },\n            {'$set': self.message},\n            upsert=True\n        )", "output": "\u5b58\u50a8\u8fc7\u7a0b", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dqn_sym_nips(action_num, data=None, name='dqn'):\n    \"\"\"\n    \"\"\"\n    if data is None:\n        net = mx.symbol.Variable('data')\n    else:\n        net = data\n    net = mx.symbol.Convolution(data=net, name='conv1', kernel=(8, 8), stride=(4, 4), num_filter=16)\n    net = mx.symbol.Activation(data=net, name='relu1', act_type=\"relu\")\n    net = mx.symbol.Convolution(data=net, name='conv2', kernel=(4, 4), stride=(2, 2), num_filter=32)\n    net = mx.symbol.Activation(data=net, name='relu2', act_type=\"relu\")\n    net = mx.symbol.Flatten(data=net)\n    net = mx.symbol.FullyConnected(data=net, name='fc3', num_hidden=256)\n    net = mx.symbol.Activation(data=net, name='relu3', act_type=\"relu\")\n    net = mx.symbol.FullyConnected(data=net, name='fc4', num_hidden=action_num)\n    net = mx.symbol.Custom(data=net, name=name, op_type='DQNOutput')\n    return net", "output": "Structure of the Deep Q Network in the NIPS 2013 workshop paper:\n    Playing Atari with Deep Reinforcement Learning (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)\n\n    Parameters\n    ----------\n    action_num : int\n    data : mxnet.sym.Symbol, optional\n    name : str, optional", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_data(self, data_dir, tmp_dir=None, task_id=-1):\n    \"\"\"\"\"\"\n    if not self._rollouts_by_epoch_and_split[self.current_epoch]:\n      # Data not loaded from disk.\n      self._split_current_epoch()\n\n    rollouts_by_split = self._rollouts_by_epoch_and_split[self.current_epoch]\n    splits_and_paths = self.splits_and_paths(data_dir)\n\n    for (split, paths) in splits_and_paths:\n      rollouts = rollouts_by_split[split]\n      num_frames = self._calc_num_frames(rollouts)\n      shard_size = num_frames // len(paths)\n\n      frame_gen = self._generate_frames(rollouts)\n      for (path_index, path) in enumerate(paths):\n        limit = shard_size\n        # Put the remainder in the last shard to preserve the ordering.\n        if path_index == len(paths) - 1:\n          limit = None\n        generator_utils.generate_files(\n            itertools.islice(frame_gen, limit), [path],\n            cycle_every_n=float(\"inf\")\n        )", "output": "Saves the current epoch rollouts to disk, split into train/dev sets.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(zone):\n    '''\n    \n    '''\n    ret = {'status': True}\n\n    # delete zone\n    res = __salt__['cmd.run_all']('zonecfg -z {zone} delete -F'.format(\n        zone=zone,\n    ))\n    ret['status'] = res['retcode'] == 0\n    ret['message'] = res['stdout'] if ret['status'] else res['stderr']\n    if ret['message'] == '':\n        del ret['message']\n    else:\n        ret['message'] = _clean_message(ret['message'])\n\n    return ret", "output": "Delete the specified configuration from memory and stable storage.\n\n    zone : string\n        name of zone\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zonecfg.delete epyon", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save(self, path):\n    \"\"\"\n    \"\"\"\n    json.dump(dict(loss=self.__class__.__name__,\n                   params=self.hparams),\n              open(os.path.join(path, 'loss.json'), 'wb'))", "output": "Save loss in json format", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_api_model(restApiId, modelName, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.delete_model(restApiId=restApiId, modelName=modelName)\n        return {'deleted': True}\n    except ClientError as e:\n        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Delete a model identified by name in a given API\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_apigateway.delete_api_model restApiId modelName", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_de_listed(self):\n        \"\"\"\n        \n        \"\"\"\n        instrument = Environment.get_instance().get_instrument(self._order_book_id)\n        current_date = Environment.get_instance().trading_dt\n        if instrument.de_listed_date is not None and current_date >= instrument.de_listed_date:\n            return True\n        return False", "output": "\u5224\u65ad\u5408\u7ea6\u662f\u5426\u8fc7\u671f", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _print_config_text(tree, indentation=0):\n    '''\n    \n    '''\n    config = ''\n    for key, value in six.iteritems(tree):\n        config += '{indent}{line}\\n'.format(indent=' '*indentation, line=key)\n        if value:\n            config += _print_config_text(value, indentation=indentation+1)\n    return config", "output": "Return the config as text from a config tree.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def allowed(subset=None, show_ip=False, show_ipv4=None):\n    '''\n    \n    '''\n    show_ip = _show_ip_migration(show_ip, show_ipv4)\n    return list_state(subset=subset, show_ip=show_ip)", "output": ".. versionadded:: 2015.8.0\n    .. versionchanged:: 2019.2.0\n        The 'show_ipv4' argument has been renamed to 'show_ip' as it now\n        includes IPv6 addresses for IPv6-connected minions.\n\n    Print a list of all minions that are up according to Salt's presence\n    detection (no commands will be sent to minions)\n\n    subset : None\n        Pass in a CIDR range to filter minions by IP address.\n\n    show_ip : False\n        Also show the IP address each minion is connecting from.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run manage.allowed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keep_tc_pos(func):\n    \"\"\"\n    \n    \"\"\"\n    @functools.wraps(func)\n    def wrapper(editor, *args, **kwds):\n        \"\"\" Decorator \"\"\"\n        sb = editor.verticalScrollBar()\n        spos = sb.sliderPosition()\n        pos = editor.textCursor().position()\n        retval = func(editor, *args, **kwds)\n        text_cursor = editor.textCursor()\n        text_cursor.setPosition(pos)\n        editor.setTextCursor(text_cursor)\n        sb.setSliderPosition(spos)\n        return retval\n    return wrapper", "output": "Cache text cursor position and restore it when the wrapped\n    function exits.\n\n    This decorator can only be used on modes or panels.\n\n    :param func: wrapped function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_sam_function_resource(name, resource_properties, layers):\n        \"\"\"\n        \n        \"\"\"\n\n        codeuri = SamFunctionProvider._extract_sam_function_codeuri(name, resource_properties, \"CodeUri\")\n\n        LOG.debug(\"Found Serverless function with name='%s' and CodeUri='%s'\", name, codeuri)\n\n        return Function(\n            name=name,\n            runtime=resource_properties.get(\"Runtime\"),\n            memory=resource_properties.get(\"MemorySize\"),\n            timeout=resource_properties.get(\"Timeout\"),\n            handler=resource_properties.get(\"Handler\"),\n            codeuri=codeuri,\n            environment=resource_properties.get(\"Environment\"),\n            rolearn=resource_properties.get(\"Role\"),\n            layers=layers\n        )", "output": "Converts a AWS::Serverless::Function resource to a Function configuration usable by the provider.\n\n        :param string name: LogicalID of the resource NOTE: This is *not* the function name because not all functions\n            declare a name\n        :param dict resource_properties: Properties of this resource\n        :return samcli.commands.local.lib.provider.Function: Function configuration", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attention_lm_moe_memory_efficient():\n  \"\"\"\"\"\"\n  hparams = attention_lm_moe_large()\n  hparams.diet_experts = True\n  hparams.layer_preprocess_sequence = \"n\"\n  hparams.layer_postprocess_sequence = \"da\"\n  hparams.layer_prepostprocess_dropout = 0.0\n  hparams.memory_efficient_ffn = True\n  hparams.attention_type = AttentionType.MEMORY_EFFICIENT\n  hparams.num_heads = 8\n  hparams.factored_logits = True\n  return hparams", "output": "Memory-efficient version.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def accept(self):\n        \"\"\"\n        \n        \"\"\"\n        AutosaveErrorDialog.show_errors = not self.dismiss_box.isChecked()\n        return QDialog.accept(self)", "output": "Update `show_errors` and hide dialog box.\n\n        Overrides method of `QDialogBox`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def masked_pick_probability(x, y, temp, cos_distance):\n    \"\"\"\n    \"\"\"\n    return SNNLCrossEntropy.pick_probability(x, temp, cos_distance) * \\\n        SNNLCrossEntropy.same_label_mask(y, y)", "output": "The pairwise sampling probabilities for the elements of x for neighbor\n    points which share labels.\n    :param x: a matrix\n    :param y: a list of labels for each element of x\n    :param temp: Temperature\n    :cos_distance: Boolean for using cosine or Euclidean distance\n\n    :returns: A tensor for the pairwise sampling probabilities.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n    \"\"\"\"\"\"\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()", "output": "Truncates a pair of sequences to a maximum sequence length.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getStorageLevel(self):\n        \"\"\"\n        \n        \"\"\"\n        java_storage_level = self._jrdd.getStorageLevel()\n        storage_level = StorageLevel(java_storage_level.useDisk(),\n                                     java_storage_level.useMemory(),\n                                     java_storage_level.useOffHeap(),\n                                     java_storage_level.deserialized(),\n                                     java_storage_level.replication())\n        return storage_level", "output": "Get the RDD's current storage level.\n\n        >>> rdd1 = sc.parallelize([1,2])\n        >>> rdd1.getStorageLevel()\n        StorageLevel(False, False, False, False, 1)\n        >>> print(rdd1.getStorageLevel())\n        Serialized 1x Replicated", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def path_url(self):\n        \"\"\"\"\"\"\n\n        url = []\n\n        p = urlsplit(self.url)\n\n        path = p.path\n        if not path:\n            path = '/'\n\n        url.append(path)\n\n        query = p.query\n        if query:\n            url.append('?')\n            url.append(query)\n\n        return ''.join(url)", "output": "Build the path URL to use.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_SU_save_stock_min(engine, client=DATABASE):\n    \"\"\"\n    \"\"\"\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_stock_min(client=client)", "output": "save stock_min\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _contains_policies(self, resource_properties):\n        \"\"\"\n        \n        \"\"\"\n        return resource_properties is not None \\\n            and isinstance(resource_properties, dict) \\\n            and self.POLICIES_PROPERTY_NAME in resource_properties", "output": "Is there policies data in this resource?\n\n        :param dict resource_properties: Properties of the resource\n        :return: True if we can process this resource. False, otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _linux_brdel(br):\n    '''\n    \n    '''\n    brctl = _tool_path('brctl')\n    return __salt__['cmd.run']('{0} delbr {1}'.format(brctl, br),\n                               python_shell=False)", "output": "Internal, deletes the bridge", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def match(path, value='', load_path=None):\n    '''\n    \n    '''\n    load_path = _check_load_paths(load_path)\n\n    aug = _Augeas(loadpath=load_path)\n    ret = {}\n\n    try:\n        matches = aug.match(path)\n    except RuntimeError:\n        return ret\n\n    for _match in matches:\n        if value and aug.get(_match) == value:\n            ret[_match] = value\n        elif not value:\n            ret[_match] = aug.get(_match)\n    return ret", "output": "Get matches for path expression\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' augeas.match /files/etc/services/service-name ssh\n\n    path\n        The path to match\n\n    value\n        The value to match on\n\n    .. versionadded:: 2016.3.0\n\n    load_path\n        A colon-spearated list of directories that modules should be searched\n        in. This is in addition to the standard load path and the directories\n        in AUGEAS_LENS_LIB.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send_patch_document(self, event):\n        \"\"\"  \"\"\"\n        msg = self.protocol.create('PATCH-DOC', [event])\n        return self._socket.send_message(msg)", "output": "Sends a PATCH-DOC message, returning a Future that's completed when it's written out.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def import_obj(cls, i_datasource, import_time=None):\n        \"\"\"\n        \"\"\"\n        def lookup_datasource(d):\n            return db.session.query(DruidDatasource).filter(\n                DruidDatasource.datasource_name == d.datasource_name,\n                DruidCluster.cluster_name == d.cluster_name,\n            ).first()\n\n        def lookup_cluster(d):\n            return db.session.query(DruidCluster).filter_by(\n                cluster_name=d.cluster_name).one()\n        return import_datasource.import_datasource(\n            db.session, i_datasource, lookup_cluster, lookup_datasource,\n            import_time)", "output": "Imports the datasource from the object to the database.\n\n         Metrics and columns and datasource will be overridden if exists.\n         This function can be used to import/export dashboards between multiple\n         superset instances. Audit metadata isn't copies over.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chloginclass(name, loginclass, root=None):\n    '''\n    \n    '''\n    if __grains__['kernel'] != 'OpenBSD':\n        return False\n\n    if loginclass == get_loginclass(name):\n        return True\n\n    cmd = ['usermod', '-L', loginclass, name]\n\n    if root is not None and __grains__['kernel'] != 'AIX':\n        cmd.extend(('-R', root))\n\n    __salt__['cmd.run'](cmd, python_shell=False)\n    return get_loginclass(name) == loginclass", "output": "Change the default login class of the user\n\n    name\n        User to modify\n\n    loginclass\n        Login class for the new account\n\n    root\n        Directory to chroot into\n\n    .. note::\n        This function only applies to OpenBSD systems.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.chloginclass foo staff", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def snapshot_present(name, recursive=False, properties=None):\n    '''\n    \n\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    ## log configuration\n    log.debug('zfs.snapshot_present::%s::config::recursive = %s',\n              name, recursive)\n    log.debug('zfs.snapshot_present::%s::config::properties = %s',\n              name, properties)\n\n    ## ensure properties are zfs values\n    if properties:\n        properties = __utils__['zfs.from_auto_dict'](properties)\n\n    ## check we have valid snapshot name\n    if not __utils__['zfs.is_snapshot'](name):\n        ret['result'] = False\n        ret['comment'] = 'invalid snapshot name: {0}'.format(name)\n        return ret\n\n    ## ensure snapshot exits\n    if not __salt__['zfs.exists'](name, **{'type': 'snapshot'}):\n        ## NOTE: create the snapshot\n        if not __opts__['test']:\n            mod_res = __salt__['zfs.snapshot'](name, **{'recursive': recursive, 'properties': properties})\n        else:\n            mod_res = OrderedDict([('snapshotted', True)])\n\n        ret['result'] = mod_res['snapshotted']\n        if ret['result']:\n            ret['changes'][name] = 'snapshotted'\n            if properties:\n                ret['changes'][name] = properties\n            ret['comment'] = 'snapshot {0} was created'.format(name)\n        else:\n            ret['comment'] = 'failed to create snapshot {0}'.format(name)\n            if 'error' in mod_res:\n                ret['comment'] = mod_res['error']\n    else:\n        ## NOTE: snapshot already exists\n        ret['comment'] = 'snapshot is present'\n\n    return ret", "output": "ensure snapshot exists and has properties set\n\n    name : string\n        name of snapshot\n    recursive : boolean\n        recursively create snapshots of all descendent datasets\n    properties : dict\n        additional zfs properties (-o)\n\n    .. note:\n        Properties are only set at creation time", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_config(self, lesson=None):\n        \"\"\"\n        \n        \"\"\"\n        if not self.data:\n            return {}\n        if lesson is None:\n            lesson = self.lesson_num\n        lesson = max(0, min(lesson, self.max_lesson_num))\n        config = {}\n        parameters = self.data['parameters']\n        for key in parameters:\n            config[key] = parameters[key][lesson]\n        return config", "output": "Returns reset parameters which correspond to the lesson.\n        :param lesson: The lesson you want to get the config of. If None, the\n               current lesson is returned.\n        :return: The configuration of the reset parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _mixed_precision_is_enabled(hparams):\n  \"\"\"\"\"\"\n  activation_dtype = hparams.activation_dtype\n  weight_dtype = hparams.weight_dtype\n  return activation_dtype == tf.float16 and weight_dtype == tf.float32", "output": "Should be the same as in common_attention, avoiding import.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def insert(self, i, tab_index):\r\n        \"\"\"\"\"\"\r\n        _id = id(self.editor.tabs.widget(tab_index))\r\n        self.history.insert(i, _id)", "output": "Insert the widget (at tab index) in the position i (index).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_namespaced_replica_set_status(self, name, namespace, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_replica_set_status_with_http_info(name, namespace, body, **kwargs)\n        else:\n            (data) = self.replace_namespaced_replica_set_status_with_http_info(name, namespace, body, **kwargs)\n            return data", "output": "replace status of the specified ReplicaSet\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_replica_set_status(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ReplicaSet (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1ReplicaSet body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1ReplicaSet\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def value_present(name, xpath, value, **kwargs):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    if 'test' not in kwargs:\n        kwargs['test'] = __opts__.get('test', False)\n\n    current_value = __salt__['xml.get_value'](name, xpath)\n    if not current_value:\n        ret['result'] = False\n        ret['comment'] = 'xpath query {0} not found in {1}'.format(xpath, name)\n        return ret\n\n    if current_value != value:\n        if kwargs['test']:\n            ret['result'] = None\n            ret['comment'] = '{0} will be updated'.format(name)\n            ret['changes'] = {name: {'old': current_value, 'new': value}}\n        else:\n            results = __salt__['xml.set_value'](name, xpath, value)\n            ret['result'] = results\n            ret['comment'] = '{0} updated'.format(name)\n            ret['changes'] = {name: {'old': current_value, 'new': value}}\n    else:\n        ret['comment'] = '{0} is already present'.format(value)\n\n    return ret", "output": ".. versionadded:: Neon\n\n    Manages a given XML file\n\n    name : string\n        The location of the XML file to manage, as an absolute path.\n\n    xpath : string\n        xpath location to manage\n\n    value : string\n        value to ensure present\n\n    .. code-block:: yaml\n\n        ensure_value_true:\n          xml.value_present:\n            - name: /tmp/test.xml\n            - xpath: .//playwright[@id='1']\n            - value: William Shakespeare", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _delete_element(name, element_type, data, server=None):\n    '''\n    \n    '''\n    _api_delete('{0}/{1}'.format(element_type, quote(name, safe='')), data, server)\n    return name", "output": "Delete an element", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ones_matrix_band_part(rows, cols, num_lower, num_upper, out_shape=None):\n  \"\"\"\n  \"\"\"\n  if all([isinstance(el, int) for el in [rows, cols, num_lower, num_upper]]):\n    # Needed info is constant, so we construct in numpy\n    if num_lower < 0:\n      num_lower = rows - 1\n    if num_upper < 0:\n      num_upper = cols - 1\n    lower_mask = np.tri(cols, rows, num_lower).T\n    upper_mask = np.tri(rows, cols, num_upper)\n    band = np.ones((rows, cols)) * lower_mask * upper_mask\n    if out_shape:\n      band = band.reshape(out_shape)\n    band = tf.constant(band, tf.float32)\n  else:\n    band = tf.matrix_band_part(\n        tf.ones([rows, cols]), tf.cast(num_lower, tf.int64),\n        tf.cast(num_upper, tf.int64))\n    if out_shape:\n      band = tf.reshape(band, out_shape)\n\n  return band", "output": "Matrix band part of ones.\n\n  Args:\n    rows: int determining number of rows in output\n    cols: int\n    num_lower: int, maximum distance backward. Negative values indicate\n      unlimited.\n    num_upper: int, maximum distance forward. Negative values indicate\n      unlimited.\n    out_shape: shape to reshape output by.\n\n  Returns:\n    Tensor of size rows * cols reshaped into shape out_shape.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vm_snapshot_create(vm_name, kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The vm_snapshot_create action must be called with -a or --action.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    snapshot_name = kwargs.get('snapshot_name', None)\n    if snapshot_name is None:\n        raise SaltCloudSystemExit(\n            'The vm_snapshot_create function requires a \\'snapshot_name\\' to be provided.'\n        )\n\n    server, user, password = _get_xml_rpc()\n    auth = ':'.join([user, password])\n    vm_id = int(get_vm_id(kwargs={'name': vm_name}))\n    response = server.one.vm.snapshotcreate(auth, vm_id, snapshot_name)\n\n    data = {\n        'action': 'vm.snapshotcreate',\n        'snapshot_created': response[0],\n        'snapshot_id': response[1],\n        'error_code': response[2],\n    }\n\n    return data", "output": "Creates a new virtual machine snapshot from the provided VM.\n\n    .. versionadded:: 2016.3.0\n\n    vm_name\n        The name of the VM from which to create the snapshot.\n\n    snapshot_name\n        The name of the snapshot to be created.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a vm_snapshot_create my-vm snapshot_name=my-new-snapshot", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def until_not(self, method, message=''):\n        \"\"\"\n        \"\"\"\n        end_time = time.time() + self._timeout\n        while True:\n            try:\n                value = method(self._driver)\n                if not value:\n                    return value\n            except self._ignored_exceptions:\n                return True\n            time.sleep(self._poll)\n            if time.time() > end_time:\n                break\n        raise TimeoutException(message)", "output": "Calls the method provided with the driver as an argument until the \\\n        return value evaluates to ``False``.\n\n        :param method: callable(WebDriver)\n        :param message: optional message for :exc:`TimeoutException`\n        :returns: the result of the last call to `method`, or\n                  ``True`` if `method` has raised one of the ignored exceptions\n        :raises: :exc:`selenium.common.exceptions.TimeoutException` if timeout occurs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def iterate_analogy_datasets(args):\n    \"\"\"\n\n    \"\"\"\n    for dataset_name in args.analogy_datasets:\n        parameters = nlp.data.list_datasets(dataset_name)\n        for key_values in itertools.product(*parameters.values()):\n            kwargs = dict(zip(parameters.keys(), key_values))\n            yield dataset_name, kwargs, nlp.data.create(dataset_name, **kwargs)", "output": "Generator over all analogy evaluation datasets.\n\n    Iterates over dataset names, keyword arguments for their creation and the\n    created dataset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def frosted_glass_blur(x, severity=1):\n  \"\"\"\n  \"\"\"\n  # sigma, max_delta, iterations\n  c = [(0.7, 1, 2), (0.9, 2, 1), (1, 2, 3), (1.1, 3, 2), (1.5, 4,\n                                                          2)][severity - 1]\n  x = np.uint8(\n      tfds.core.lazy_imports.skimage.filters.gaussian(\n          np.array(x) / 255., sigma=c[0], multichannel=True) * 255)\n\n  # locally shuffle pixels\n  for _ in range(c[2]):\n    for h in range(x.shape[0] - c[1], c[1], -1):\n      for w in range(x.shape[1] - c[1], c[1], -1):\n        dx, dy = np.random.randint(-c[1], c[1], size=(2,))\n        h_prime, w_prime = h + dy, w + dx\n        # swap\n        x[h, w], x[h_prime, w_prime] = x[h_prime, w_prime], x[h, w]\n  x_clip = np.clip(\n      tfds.core.lazy_imports.skimage.filters.gaussian(\n          x / 255., sigma=c[0], multichannel=True), 0, 1)\n  x_clip *= 255\n  return around_and_astype(x_clip)", "output": "Frosted glass blurring to images.\n\n  Apply frosted glass blurring to images by shuffling pixels locally.\n\n  Args:\n    x: numpy array, uncorrupted image, assumed to have uint8 pixel in [0,255].\n    severity: integer, severity of corruption.\n\n  Returns:\n    numpy array, image with uint8 pixels in [0,255]. Applied frosted glass blur.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_sid(self, sid, df, invalid_data_behavior='warn'):\n        \"\"\"\n        \n        \"\"\"\n        cols = {\n            'open': df.open.values,\n            'high': df.high.values,\n            'low': df.low.values,\n            'close': df.close.values,\n            'volume': df.volume.values,\n        }\n        dts = df.index.values\n        # Call internal method, since DataFrame has already ensured matching\n        # index and value lengths.\n        self._write_cols(sid, dts, cols, invalid_data_behavior)", "output": "Write the OHLCV data for the given sid.\n        If there is no bcolz ctable yet created for the sid, create it.\n        If the length of the bcolz ctable is not exactly to the date before\n        the first day provided, fill the ctable with 0s up to that date.\n\n        Parameters\n        ----------\n        sid : int\n            The asset identifer for the data being written.\n        df : pd.DataFrame\n            DataFrame of market data with the following characteristics.\n            columns : ('open', 'high', 'low', 'close', 'volume')\n                open : float64\n                high : float64\n                low  : float64\n                close : float64\n                volume : float64|int64\n            index : DatetimeIndex of market minutes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def host_delete(hostids, **kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'host.delete'\n            if not isinstance(hostids, list):\n                params = [hostids]\n            else:\n                params = hostids\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result']['hostids']\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": "Delete hosts.\n\n    .. versionadded:: 2016.3.0\n\n    :param hostids: Hosts (hostids) to delete.\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    :return: IDs of the deleted hosts.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.host_delete 10106", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge(intervals):\n        \"\"\"  \"\"\"\n        out = []\n        for i in sorted(intervals, key=lambda i: i.start):\n            if out and i.start <= out[-1].end:\n                out[-1].end = max(out[-1].end, i.end)\n            else:\n                out += i,\n        return out", "output": "Merge two intervals into one.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def batch_snapshot(self, read_timestamp=None, exact_staleness=None):\n        \"\"\"\n        \"\"\"\n        return BatchSnapshot(\n            self, read_timestamp=read_timestamp, exact_staleness=exact_staleness\n        )", "output": "Return an object which wraps a batch read / query.\n\n        :type read_timestamp: :class:`datetime.datetime`\n        :param read_timestamp: Execute all reads at the given timestamp.\n\n        :type exact_staleness: :class:`datetime.timedelta`\n        :param exact_staleness: Execute all reads at a timestamp that is\n                                ``exact_staleness`` old.\n\n        :rtype: :class:`~google.cloud.spanner_v1.database.BatchSnapshot`\n        :returns: new wrapper", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_config_file(self, config_data=None, quiet=False):\n        \"\"\"\n        \"\"\"\n        if config_data is None:\n            config_data = {}\n\n        if os.path.exists(self.config):\n\n            try:\n                if os.name != 'nt':\n                    permissions = os.stat(self.config).st_mode\n                    if (permissions & 4) or (permissions & 32):\n                        print(\n                            'Warning: Your Kaggle API key is readable by other '\n                            'users on this system! To fix this, you can run ' +\n                            '\\'chmod 600 {}\\''.format(self.config))\n\n                with open(self.config) as f:\n                    config_data = json.load(f)\n            except:\n                pass\n\n        else:\n\n            # Warn the user that configuration will be reliant on environment\n            if not quiet:\n                print('No Kaggle API config file found, will use environment.')\n\n        return config_data", "output": "read_config_file is the first effort to get a username\n           and key to authenticate to the Kaggle API. Since we can get the\n           username and password from the environment, it's not required.\n\n           Parameters\n           ==========\n           config_data: the Configuration object to save a username and\n                        password, if defined\n           quiet: suppress verbose print of output (default is False)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy(source):\n    \"\"\n\n    def do_copy(target):\n        if source.__doc__:\n            target.__doc__ = source.__doc__\n        return target\n\n    return do_copy", "output": "Copy a docstring from another source function (if present)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_indicator_SKDJ(DataFrame, N=9, M=3):\n    \"\"\"\n    \n\n    \"\"\"\n    CLOSE = DataFrame['close']\n    LOWV = LLV(DataFrame['low'], N)\n    HIGHV = HHV(DataFrame['high'], N)\n    RSV = EMA((CLOSE - LOWV) / (HIGHV - LOWV) * 100, M)\n    K = EMA(RSV, M)\n    D = MA(K, M)\n    DICT = {'RSV': RSV, 'SKDJ_K': K, 'SKDJ_D': D}\n\n    return pd.DataFrame(DICT)", "output": "1.\u6307\u6807>80 \u65f6\uff0c\u56de\u6863\u673a\u7387\u5927\uff1b\u6307\u6807<20 \u65f6\uff0c\u53cd\u5f39\u673a\u7387\u5927\uff1b\n    2.K\u572820\u5de6\u53f3\u5411\u4e0a\u4ea4\u53c9D\u65f6\uff0c\u89c6\u4e3a\u4e70\u8fdb\u4fe1\u53f7\u53c2\u8003\uff1b \n    3.K\u572880\u5de6\u53f3\u5411\u4e0b\u4ea4\u53c9D\u65f6\uff0c\u89c6\u4e3a\u5356\u51fa\u4fe1\u53f7\u53c2\u8003\uff1b\n    4.SKDJ\u6ce2\u52a8\u4e8e50\u5de6\u53f3\u7684\u4efb\u4f55\u8baf\u53f7\uff0c\u5176\u4f5c\u7528\u4e0d\u5927\u3002", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attention_lm_translation():\n  \"\"\"\"\"\"\n  hparams = attention_lm_base()\n  hparams.layer_preprocess_sequence = \"n\"\n  hparams.layer_postprocess_sequence = \"da\"\n  hparams.learning_rate = 0.4\n  hparams.prepend_mode = \"prepend_inputs_masked_attention\"\n  hparams.max_length = 512\n  hparams.label_smoothing = 0.1\n  hparams.shared_embedding_and_softmax_weights = True\n  return hparams", "output": "Version to use for seq2seq.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_backend():\n    \"\"\"\"\"\"\n    ep = os.environ['PEP517_BUILD_BACKEND']\n    mod_path, _, obj_path = ep.partition(':')\n    try:\n        obj = import_module(mod_path)\n    except ImportError:\n        raise BackendUnavailable\n    if obj_path:\n        for path_part in obj_path.split('.'):\n            obj = getattr(obj, path_part)\n    return obj", "output": "Find and load the build backend", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_concat(net, node, module, builder):\n    \"\"\"\n    \"\"\"\n    # Get input and output names\n    input_names, output_name = _get_input_output_name(net, node, 'all')\n    name = node['name']\n    mode = 'CONCAT'\n    builder.add_elementwise(name = name, input_names = input_names,\n            output_name = output_name, mode = mode)", "output": "Convert concat layer from mxnet to coreml.\n\n    Parameters\n    ----------\n    network: net\n        A mxnet network object.\n\n    layer: node\n        Node to convert.\n\n    module: module\n        An module for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_data_iter_plan(self):\n        \"\"\n        # truncate each bucket into multiple of batch-size\n        bucket_n_batches = []\n        for i in range(len(self.data)):\n            bucket_n_batches.append(np.floor((self.data[i]) / self.batch_size))\n            self.data[i] = self.data[i][:int(bucket_n_batches[i]*self.batch_size)]\n\n        bucket_plan = np.hstack([np.zeros(n, int)+i for i, n in enumerate(bucket_n_batches)])\n        np.random.shuffle(bucket_plan)\n\n        bucket_idx_all = [np.random.permutation(len(x)) for x in self.data]\n\n        self.bucket_plan = bucket_plan\n        self.bucket_idx_all = bucket_idx_all\n        self.bucket_curr_idx = [0 for x in self.data]\n\n        self.data_buffer = []\n        self.label_buffer = []\n        for i_bucket in range(len(self.data)):\n            if not self.model_parallel:\n                data = np.zeros((self.batch_size, self.buckets[i_bucket]))\n                label = np.zeros((self.batch_size, self.buckets[i_bucket]))\n                self.data_buffer.append(data)\n                self.label_buffer.append(label)\n            else:\n                data = np.zeros((self.buckets[i_bucket], self.batch_size))\n                self.data_buffer.append(data)\n\n        if self.model_parallel:\n            # Transpose data if model parallel\n            for i in range(len(self.data)):\n                bucket_data = self.data[i]\n                self.data[i] = np.transpose(bucket_data)", "output": "make a random data iteration plan", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_path_match(req_path: str, cookie_path: str) -> bool:\n        \"\"\"\"\"\"\n        if not req_path.startswith(\"/\"):\n            req_path = \"/\"\n\n        if req_path == cookie_path:\n            return True\n\n        if not req_path.startswith(cookie_path):\n            return False\n\n        if cookie_path.endswith(\"/\"):\n            return True\n\n        non_matching = req_path[len(cookie_path):]\n\n        return non_matching.startswith(\"/\")", "output": "Implements path matching adhering to RFC 6265.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def output(data, **kwargs):  # pylint: disable=unused-argument\n    '''\n    \n    '''\n\n    params = {}\n    if 'output_indent' not in __opts__:\n        # default indentation\n        params.update(default_flow_style=False)\n    elif __opts__['output_indent'] >= 0:\n        # custom indent\n        params.update(default_flow_style=False,\n                      indent=__opts__['output_indent'])\n    else:  # no indentation\n        params.update(default_flow_style=True,\n                      indent=0)\n    try:\n        return salt.utils.yaml.safe_dump(data, **params)\n    except Exception as exc:\n        import pprint\n        log.exception(\n            'Exception %s encountered when trying to serialize %s',\n            exc, pprint.pformat(data)\n        )", "output": "Print out YAML using the block mode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_data():\n    ''''''\n    boston = load_boston()\n    X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=99, test_size=0.25)\n    #normalize data\n    ss_X = StandardScaler()\n    ss_y = StandardScaler()\n\n    X_train = ss_X.fit_transform(X_train)\n    X_test = ss_X.transform(X_test)\n    y_train = ss_y.fit_transform(y_train[:, None])[:,0]\n    y_test = ss_y.transform(y_test[:, None])[:,0]\n\n    return X_train, X_test, y_train, y_test", "output": "Load dataset, use boston dataset", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def num_trees(n):\n    \"\"\"\n    \n    \"\"\"\n    dp = [0] * (n+1)\n    dp[0] = 1\n    dp[1] = 1\n    for i in range(2, n+1):\n        for j in range(i+1):\n            dp[i] += dp[i-j] * dp[j-1]\n    return dp[-1]", "output": ":type n: int\n    :rtype: int", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, resource):\n        \"\"\"\n        \"\"\"\n        array_type = resource[\"parameterType\"][\"arrayType\"][\"type\"]\n        if array_type == \"STRUCT\":\n            return cls._from_api_repr_struct(resource)\n        return cls._from_api_repr_scalar(resource)", "output": "Factory: construct parameter from JSON resource.\n\n        :type resource: dict\n        :param resource: JSON mapping of parameter\n\n        :rtype: :class:`~google.cloud.bigquery.query.ArrayQueryParameter`\n        :returns: instance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_host_ipv4(name=None, mac=None, allow_array=False, **api_opts):\n    '''\n    \n    '''\n    data = get_host(name=name, mac=mac, **api_opts)\n    if data and 'ipv4addrs' in data:\n        l = []\n        for a in data['ipv4addrs']:\n            if 'ipv4addr' in a:\n                l.append(a['ipv4addr'])\n        if allow_array:\n            return l\n        if l:\n            return l[0]\n    return None", "output": "Get ipv4 address from host record.\n\n    Use `allow_array` to return possible multiple values.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-call infoblox.get_host_ipv4 host=localhost.domain.com\n        salt-call infoblox.get_host_ipv4 mac=00:50:56:84:6e:ae", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_metadata(self, key):\n        \"\"\"  \"\"\"\n        if getattr(getattr(self.group, 'meta', None), key, None) is not None:\n            return self.parent.select(self._get_metadata_path(key))\n        return None", "output": "return the meta data array for this key", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def split_first(s, delims):\n    \"\"\"\n    \n    \"\"\"\n    min_idx = None\n    min_delim = None\n    for d in delims:\n        idx = s.find(d)\n        if idx < 0:\n            continue\n\n        if min_idx is None or idx < min_idx:\n            min_idx = idx\n            min_delim = d\n\n    if min_idx is None or min_idx < 0:\n        return s, '', None\n\n    return s[:min_idx], s[min_idx + 1:], min_delim", "output": "Given a string and an iterable of delimiters, split on the first found\n    delimiter. Return two split parts and the matched delimiter.\n\n    If not found, then the first part is the full input string.\n\n    Example::\n\n        >>> split_first('foo/bar?baz', '?/=')\n        ('foo', 'bar?baz', '/')\n        >>> split_first('foo/bar?baz', '123')\n        ('foo/bar?baz', '', None)\n\n    Scales linearly with number of delims. Not ideal for large number of delims.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dates(self, start_date, end_date, return_name=False):\n        \"\"\"\n        \n        \"\"\"\n        start_date = Timestamp(start_date)\n        end_date = Timestamp(end_date)\n\n        filter_start_date = start_date\n        filter_end_date = end_date\n\n        if self.year is not None:\n            dt = Timestamp(datetime(self.year, self.month, self.day))\n            if return_name:\n                return Series(self.name, index=[dt])\n            else:\n                return [dt]\n\n        dates = self._reference_dates(start_date, end_date)\n        holiday_dates = self._apply_rule(dates)\n        if self.days_of_week is not None:\n            holiday_dates = holiday_dates[np.in1d(holiday_dates.dayofweek,\n                                                  self.days_of_week)]\n\n        if self.start_date is not None:\n            filter_start_date = max(self.start_date.tz_localize(\n                filter_start_date.tz), filter_start_date)\n        if self.end_date is not None:\n            filter_end_date = min(self.end_date.tz_localize(\n                filter_end_date.tz), filter_end_date)\n        holiday_dates = holiday_dates[(holiday_dates >= filter_start_date) &\n                                      (holiday_dates <= filter_end_date)]\n        if return_name:\n            return Series(self.name, index=holiday_dates)\n        return holiday_dates", "output": "Calculate holidays observed between start date and end date\n\n        Parameters\n        ----------\n        start_date : starting date, datetime-like, optional\n        end_date : ending date, datetime-like, optional\n        return_name : bool, optional, default=False\n            If True, return a series that has dates and holiday names.\n            False will only return dates.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_fs(self):\n        '''\n        \n        '''\n\n        data = dict()\n        for dev, dev_data in salt.utils.fsutils._blkid().items():\n            dev = self._get_disk_size(dev)\n            device = dev.pop('device')\n            dev['type'] = dev_data['type']\n            data[device] = dev\n\n        return data", "output": "Get available file systems and their types.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self, prms):\n        \"\"\"\n        \n        \"\"\"\n        with self.sess.as_default():\n            fetches = []\n            feeds = {}\n            for name, value in six.iteritems(prms):\n                assert name in self.name_map\n                var = self.name_map[name]\n                fetches.append(var.initializer)\n                # This is the implementation of `var.load`\n                feeds[var.initializer.inputs[1]] = SessionUpdate.relaxed_value_for_var(value, var)\n            self.sess.run(fetches, feed_dict=feeds)", "output": "Args:\n            prms(dict): dict of {variable name: value}\n                Any name in prms must be in the graph and in vars_to_update.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bigtable_readers(self):\n        \"\"\"\n        \"\"\"\n        result = set()\n        for member in self._bindings.get(BIGTABLE_READER_ROLE, ()):\n            result.add(member)\n        return frozenset(result)", "output": "Access to bigtable.reader role memebers\n\n        For example:\n\n        .. literalinclude:: snippets.py\n            :start-after: [START bigtable_readers_policy]\n            :end-before: [END bigtable_readers_policy]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(self, auto_confirm=False, verbose=False):\n        \"\"\"\"\"\"\n\n        if not self.paths:\n            logger.info(\n                \"Can't uninstall '%s'. No files were found to uninstall.\",\n                self.dist.project_name,\n            )\n            return\n\n        dist_name_version = (\n            self.dist.project_name + \"-\" + self.dist.version\n        )\n        logger.info('Uninstalling %s:', dist_name_version)\n\n        with indent_log():\n            if auto_confirm or self._allowed_to_proceed(verbose):\n                moved = self._moved_paths\n\n                for_rename = compress_for_rename(self.paths)\n\n                for path in sorted(compact(for_rename)):\n                    moved.stash(path)\n                    logger.debug('Removing file or directory %s', path)\n\n                for pth in self.pth.values():\n                    pth.remove()\n\n                logger.info('Successfully uninstalled %s', dist_name_version)", "output": "Remove paths in ``self.paths`` with confirmation (unless\n        ``auto_confirm`` is True).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _show_loading_page(self):\r\n        \"\"\"\"\"\"\r\n        self.shellwidget.hide()\r\n        self.infowidget.show()\r\n        self.info_page = self.loading_page\r\n        self.set_info_page()", "output": "Show animation while the kernel is loading.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_allof(self, definitions, field, value):\n        \"\"\"  \"\"\"\n        valids, _errors = \\\n            self.__validate_logical('allof', definitions, field, value)\n        if valids < len(definitions):\n            self._error(field, errors.ALLOF, _errors,\n                        valids, len(definitions))", "output": "{'type': 'list', 'logical': 'allof'}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_turicreate_object_type(url):\n    '''\n    \n    '''\n    from .._connect import main as _glconnect\n    ret = _glconnect.get_unity().get_turicreate_object_type(_make_internal_url(url))\n\n    # to be consistent, we use sgraph instead of graph here\n    if ret == 'graph':\n        ret = 'sgraph'\n    return ret", "output": "Given url where a Turi Create object is persisted, return the Turi\n    Create object type: 'model', 'graph', 'sframe', or 'sarray'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_function(name, doc=\"\"):\n    \"\"\"\"\"\"\n    def _(col):\n        sc = SparkContext._active_spark_context\n        jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)\n        return Column(jc)\n    _.__name__ = name\n    _.__doc__ = doc\n    return _", "output": "Create a PySpark function by its name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_serv(ret=None):\n    '''\n    \n    '''\n    _options = _get_options(ret)\n    host = _options.get('host')\n    port = _options.get('port')\n    database = _options.get('db')\n    user = _options.get('user')\n    password = _options.get('password')\n    version = _get_version(host, port, user, password)\n\n    if version and \"v0.8\" in version:\n        return influxdb.influxdb08.InfluxDBClient(host=host,\n                            port=port,\n                            username=user,\n                            password=password,\n                            database=database\n        )\n    else:\n        return influxdb.InfluxDBClient(host=host,\n                            port=port,\n                            username=user,\n                            password=password,\n                            database=database\n        )", "output": "Return an influxdb client object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _engine_builder(con):\n    \"\"\"\n    \n    \"\"\"\n    global _SQLALCHEMY_INSTALLED\n    if isinstance(con, str):\n        try:\n            import sqlalchemy\n        except ImportError:\n            _SQLALCHEMY_INSTALLED = False\n        else:\n            con = sqlalchemy.create_engine(con)\n            return con\n\n    return con", "output": "Returns a SQLAlchemy engine from a URI (if con is a string)\n    else it just return con without modifying it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_from_generator(cls,\n                           generator,\n                           target_size,\n                           max_subtoken_length=None,\n                           reserved_tokens=None):\n    \"\"\"\n    \"\"\"\n    token_counts = collections.defaultdict(int)\n    for item in generator:\n      for tok in tokenizer.encode(native_to_unicode(item)):\n        token_counts[tok] += 1\n    encoder = cls.build_to_target_size(\n        target_size, token_counts, 1, 1e3,\n        max_subtoken_length=max_subtoken_length,\n        reserved_tokens=reserved_tokens)\n    return encoder", "output": "Builds a SubwordTextEncoder from the generated text.\n\n    Args:\n      generator: yields text.\n      target_size: int, approximate vocabulary size to create.\n      max_subtoken_length: Maximum length of a subtoken. If this is not set,\n        then the runtime and memory use of creating the vocab is quadratic in\n        the length of the longest token. If this is set, then it is instead\n        O(max_subtoken_length * length of longest token).\n      reserved_tokens: List of reserved tokens. The global variable\n        `RESERVED_TOKENS` must be a prefix of `reserved_tokens`. If this\n        argument is `None`, it will use `RESERVED_TOKENS`.\n\n    Returns:\n      SubwordTextEncoder with `vocab_size` approximately `target_size`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_wider_graph(graph):\n    ''' \n    '''\n    weighted_layer_ids = graph.wide_layer_ids()\n    weighted_layer_ids = list(\n        filter(lambda x: graph.layer_list[x].output.shape[-1], weighted_layer_ids)\n    )\n    wider_layers = sample(weighted_layer_ids, 1)\n\n    for layer_id in wider_layers:\n        layer = graph.layer_list[layer_id]\n        if is_layer(layer, \"Conv\"):\n            n_add = layer.filters\n        else:\n            n_add = layer.units\n\n        graph.to_wider_model(layer_id, n_add)\n    return graph", "output": "wider graph", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def addContentLen(self, content, len):\n        \"\"\" \"\"\"\n        libxml2mod.xmlNodeAddContentLen(self._o, content, len)", "output": "Append the extra substring to the node content. NOTE: In\n          contrast to xmlNodeSetContentLen(), @content is supposed to\n          be raw text, so unescaped XML special chars are allowed,\n           entity references are not supported.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ip_ifaces():\n    '''\n    \n    '''\n    tmp = {}\n    ret = {}\n    if_ = None\n    at_ = None\n    out = __salt__['cmd.run']('ip a')\n    for line in out.splitlines():\n        if not line.startswith(' '):\n            comps = line.split(':')\n            if_ = comps[1].strip()\n            opts_comps = comps[2].strip().split()\n            flags = opts_comps.pop(0).lstrip('<').rstrip('>').split(',')\n            opts_iter = iter(opts_comps)\n            ret[if_] = {\n                'flags': flags,\n                'options': dict(list(zip(opts_iter, opts_iter)))\n            }\n        else:\n            if line.strip().startswith('link'):\n                comps = iter(line.strip().split())\n                ret[if_]['link_layer'] = dict(list(zip(comps, comps)))\n            elif line.strip().startswith('inet'):\n                comps = line.strip().split()\n                at_ = comps[0]\n                if len(comps) % 2 != 0:\n                    last = comps.pop()\n                    comps[-1] += ' {0}'.format(last)\n                ifi = iter(comps)\n                ret[if_][at_] = dict(list(zip(ifi, ifi)))\n            else:\n                comps = line.strip().split()\n                ifi = iter(comps)\n                ret[if_][at_].update(dict(list(zip(ifi, ifi))))\n    return ret", "output": "Parse output from 'ip a'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(self, context):\n    \"\"\"\n    \"\"\"\n    try:\n      # pylint: disable=g-import-not-at-top,unused-import\n      import tensorflow\n      # Available in TensorFlow 1.14 or later, so do import check\n      # pylint: disable=g-import-not-at-top,unused-import\n      from tensorflow.python.eager import profiler_client\n    except ImportError:\n      return\n    # pylint: disable=g-import-not-at-top\n    from tensorboard.plugins.profile.profile_plugin import ProfilePlugin\n    return ProfilePlugin(context)", "output": "Returns the plugin, if possible.\n\n    Args:\n      context: The TBContext flags.\n\n    Returns:\n      A ProfilePlugin instance or None if it couldn't be loaded.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_pretraining_model(nlp, tok2vec):\n    \"\"\"\n    \"\"\"\n    output_size = nlp.vocab.vectors.data.shape[1]\n    output_layer = chain(\n        LN(Maxout(300, pieces=3)), Affine(output_size, drop_factor=0.0)\n    )\n    # This is annoying, but the parser etc have the flatten step after\n    # the tok2vec. To load the weights in cleanly, we need to match\n    # the shape of the models' components exactly. So what we cann\n    # \"tok2vec\" has to be the same set of processes as what the components do.\n    tok2vec = chain(tok2vec, flatten)\n    model = chain(tok2vec, output_layer)\n    model = masked_language_model(nlp.vocab, model)\n    model.tok2vec = tok2vec\n    model.output_layer = output_layer\n    model.begin_training([nlp.make_doc(\"Give it a doc to infer shapes\")])\n    return model", "output": "Define a network for the pretraining. We simply add an output layer onto\n    the tok2vec input model. The tok2vec input model needs to be a model that\n    takes a batch of Doc objects (as a list), and returns a list of arrays.\n    Each array in the output needs to have one row per token in the doc.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_csi_node(self, name, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_csi_node_with_http_info(name, body, **kwargs)\n        else:\n            (data) = self.replace_csi_node_with_http_info(name, body, **kwargs)\n            return data", "output": "replace the specified CSINode\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_csi_node(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the CSINode (required)\n        :param V1beta1CSINode body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1beta1CSINode\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def record_evaluation(eval_result):\n    \"\"\"\n    \"\"\"\n    if not isinstance(eval_result, dict):\n        raise TypeError('Eval_result should be a dictionary')\n    eval_result.clear()\n\n    def _init(env):\n        for data_name, _, _, _ in env.evaluation_result_list:\n            eval_result.setdefault(data_name, collections.defaultdict(list))\n\n    def _callback(env):\n        if not eval_result:\n            _init(env)\n        for data_name, eval_name, result, _ in env.evaluation_result_list:\n            eval_result[data_name][eval_name].append(result)\n    _callback.order = 20\n    return _callback", "output": "Create a callback that records the evaluation history into ``eval_result``.\n\n    Parameters\n    ----------\n    eval_result : dict\n       A dictionary to store the evaluation results.\n\n    Returns\n    -------\n    callback : function\n        The callback that records the evaluation history into the passed dictionary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def disassociate_eip_address(public_ip=None, association_id=None, region=None,\n                             key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    try:\n        return conn.disassociate_address(public_ip, association_id)\n    except boto.exception.BotoServerError as e:\n        log.error(e)\n        return False", "output": "Disassociate an Elastic IP address from a currently running instance. This\n    requires exactly one of either 'association_id' or 'public_ip', depending\n    on whether you\u2019re dealing with a VPC or EC2 Classic address.\n\n    public_ip\n        (string) \u2013 Public IP address, for EC2 Classic allocations.\n    association_id\n        (string) \u2013 Association ID for a VPC-bound EIP.\n\n    returns\n        (bool)   - True on success, False on failure.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_ec2.disassociate_eip_address association_id=eipassoc-e3ba2d16\n\n    .. versionadded:: 2016.3.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_requests(status=None):\n    '''\n    \n    '''\n    if status:\n        url = '{0}/request?status={1}'.format(_base_url(), status)\n    else:\n        url = '{0}/request'.format(_base_url())\n\n    reqs = _paginate(url,\n                     \"requests\",\n                     method='GET',\n                     decode=True,\n                     decode_type='json',\n                     raise_error=False,\n                     header_dict={\n                         'X-DC-DEVKEY': _api_key(),\n                         'Content-Type': 'application/json',\n                     }\n    )\n\n    ret = {'requests': reqs}\n    return ret", "output": "List certificate requests made to CertCentral. You can filter by\n    status: ``pending``, ``approved``, ``rejected``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run digicert.list_requests pending", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_outputs(self, merge_multi_context=True):\n        \"\"\"\n        \"\"\"\n        assert self.binded and self.params_initialized\n        return self._modules[-1].get_outputs(merge_multi_context=merge_multi_context)", "output": "Gets outputs from a previous forward computation.\n\n        Parameters\n        ----------\n        merge_multi_context : bool\n            Default is ``True``. In the case when data-parallelism is used, the outputs\n            will be collected from multiple devices. A ``True`` value indicate that we\n            should merge the collected results so that they look like from a single\n            executor.\n\n        Returns\n        -------\n        list of NDArray or list of list of NDArray\n            If `merge_multi_context` is ``True``, it is like ``[out1,\n            out2]``. Otherwise, it is like ``[[out1_dev1, out1_dev2], [out2_dev1,\n            out2_dev2]]``. All the output elements are numpy arrays.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def status(name, runas=None):\n    '''\n    \n    '''\n\n    contains_globbing = bool(re.search(r'\\*|\\?|\\[.+\\]', name))\n    if contains_globbing:\n        services = fnmatch.filter(get_all(), name)\n    else:\n        services = [name]\n    results = {}\n    for service in services:\n        service_info = _service_by_name(service)\n\n        lookup_name = service_info['plist']['Label'] if service_info else service\n        launchctl_data = _get_launchctl_data(lookup_name, runas=runas)\n\n        if launchctl_data:\n            if BEFORE_YOSEMITE:\n                if six.PY3:\n                    results[service] = 'PID' in plistlib.loads(launchctl_data)\n                else:\n                    results[service] = 'PID' in dict(plistlib.readPlistFromString(launchctl_data))\n            else:\n                pattern = '\"PID\" = [0-9]+;'\n                results[service] = True if re.search(pattern, launchctl_data) else False\n        else:\n            results[service] = False\n    if contains_globbing:\n        return results\n    return results[name]", "output": "Return the status for a service via systemd.\n    If the name contains globbing, a dict mapping service name to True/False\n    values is returned.\n\n    .. versionchanged:: 2018.3.0\n        The service name can now be a glob (e.g. ``salt*``)\n\n    Args:\n        name (str): The name of the service to check\n        runas (str): User to run launchctl commands\n\n    Returns:\n        bool: True if running, False otherwise\n        dict: Maps service name to True if running, False otherwise\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.status <service name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_dnamasq(filename):\n    '''\n    \n    '''\n    fileopts = {}\n\n    if not os.path.isfile(filename):\n        raise CommandExecutionError(\n            'Error: No such file \\'{0}\\''.format(filename)\n        )\n\n    with salt.utils.files.fopen(filename, 'r') as fp_:\n        for line in fp_:\n            line = salt.utils.stringutils.to_unicode(line)\n            if not line.strip():\n                continue\n            if line.startswith('#'):\n                continue\n            if '=' in line:\n                comps = line.split('=')\n                if comps[0] in fileopts:\n                    if isinstance(fileopts[comps[0]], six.string_types):\n                        temp = fileopts[comps[0]]\n                        fileopts[comps[0]] = [temp]\n                    fileopts[comps[0]].append(comps[1].strip())\n                else:\n                    fileopts[comps[0]] = comps[1].strip()\n            else:\n                if 'unparsed' not in fileopts:\n                    fileopts['unparsed'] = []\n                fileopts['unparsed'].append(line)\n    return fileopts", "output": "Generic function for parsing dnsmasq files including includes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def group_by_count(iterable: List[Any], count: int, default_value: Any) -> List[List[Any]]:\n    \"\"\"\n    \n    \"\"\"\n    return [list(l) for l in zip_longest(*[iter(iterable)] * count, fillvalue=default_value)]", "output": "Takes a list and groups it into sublists of size ``count``, using ``default_value`` to pad the\n    list at the end if the list is not divisable by ``count``.\n\n    For example:\n    >>> group_by_count([1, 2, 3, 4, 5, 6, 7], 3, 0)\n    [[1, 2, 3], [4, 5, 6], [7, 0, 0]]\n\n    This is a short method, but it's complicated and hard to remember as a one-liner, so we just\n    make a function out of it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_expected_type(model, expected_type):\n    \"\"\"\n    \"\"\"\n    if (model.__class__.__name__ != expected_type.__name__):\n        raise TypeError(\"Expected model of type '%s' (got %s)\" % \\\n                (expected_type.__name__, model.__class__.__name__))", "output": "Check if a model is of the right type. Raise error if not.\n\n    Parameters\n    ----------\n    model: model\n        Any scikit-learn model\n\n    expected_type: Type\n        Expected type of the scikit-learn.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def as_ordered(self, inplace=False):\n        \"\"\"\n        \n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        return self.set_ordered(True, inplace=inplace)", "output": "Set the Categorical to be ordered.\n\n        Parameters\n        ----------\n        inplace : bool, default False\n           Whether or not to set the ordered attribute in-place or return\n           a copy of this categorical with ordered set to True.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main(self, ignored_argv=('',)):\n    \"\"\"\n    \"\"\"\n    self._install_signal_handler(signal.SIGTERM, \"SIGTERM\")\n    if self.flags.inspect:\n      logger.info('Not bringing up TensorBoard, but inspecting event files.')\n      event_file = os.path.expanduser(self.flags.event_file)\n      efi.inspect(self.flags.logdir, event_file, self.flags.tag)\n      return 0\n    if self.flags.version_tb:\n      print(version.VERSION)\n      return 0\n    try:\n      server = self._make_server()\n      sys.stderr.write('TensorBoard %s at %s (Press CTRL+C to quit)\\n' %\n                       (version.VERSION, server.get_url()))\n      sys.stderr.flush()\n      self._register_info(server)\n      server.serve_forever()\n      return 0\n    except TensorBoardServerException as e:\n      logger.error(e.msg)\n      sys.stderr.write('ERROR: %s\\n' % e.msg)\n      sys.stderr.flush()\n      return -1", "output": "Blocking main function for TensorBoard.\n\n    This method is called by `tensorboard.main.run_main`, which is the\n    standard entrypoint for the tensorboard command line program. The\n    configure() method must be called first.\n\n    Args:\n      ignored_argv: Do not pass. Required for Abseil compatibility.\n\n    Returns:\n      Process exit code, i.e. 0 if successful or non-zero on failure. In\n      practice, an exception will most likely be raised instead of\n      returning non-zero.\n\n    :rtype: int", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_index_from_filename(self, filename):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        filenames = [d.filename for d in self.data]\r\n        return filenames.index(filename)", "output": "Return the position index of a file in the tab bar of the editorstack\r\n        from its name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_hparams_from_hparams(target_hparams, source_hparams, prefix):\n  \"\"\"\"\"\"\n  for (param_name, param_value) in six.iteritems(source_hparams.values()):\n    if param_name.startswith(prefix):\n      target_hparams.set_hparam(param_name[len(prefix):], param_value)", "output": "Copy a subset of hparams to target_hparams.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_best_indexes(logits, n_best_size):\n    \"\"\"\"\"\"\n    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n\n    best_indexes = []\n    for i in range(len(index_and_score)):\n        if i >= n_best_size:\n            break\n        best_indexes.append(index_and_score[i][0])\n    return best_indexes", "output": "Get the n-best logits from a list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def example_reading_spec(self):\n    \"\"\"\"\"\"\n    video_fields, video_decoders = (\n        video_utils.VideoProblem.example_reading_spec(self))\n    env_fields, env_decoders = env_problem.EnvProblem.example_reading_spec(self)\n\n    # Remove raw observations field since we want to capture them as videos.\n    env_fields.pop(env_problem.OBSERVATION_FIELD)\n    env_decoders.pop(env_problem.OBSERVATION_FIELD)\n\n    # Add frame number spec and decoder.\n    env_fields[_FRAME_NUMBER_FIELD] = tf.FixedLenFeature((1,), tf.int64)\n    env_decoders[\n        _FRAME_NUMBER_FIELD] = tf.contrib.slim.tfexample_decoder.Tensor(\n            _FRAME_NUMBER_FIELD)\n\n    # Add video fields and decoders\n    env_fields.update(video_fields)\n    env_decoders.update(video_decoders)\n    return env_fields, env_decoders", "output": "Return a mix of env and video data fields and decoders.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def crop(im, r, c, sz): \n    '''\n     \n    '''\n    return im[r:r+sz, c:c+sz]", "output": "crop image into a square of size sz,", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_objs_combined_axis(objs, intersect=False, axis=0, sort=True):\n    \"\"\"\n    \n    \"\"\"\n    obs_idxes = [obj._get_axis(axis) for obj in objs\n                 if hasattr(obj, '_get_axis')]\n    if obs_idxes:\n        return _get_combined_index(obs_idxes, intersect=intersect, sort=sort)", "output": "Extract combined index: return intersection or union (depending on the\n    value of \"intersect\") of indexes on given axis, or None if all objects\n    lack indexes (e.g. they are numpy arrays).\n\n    Parameters\n    ----------\n    objs : list of objects\n        Each object will only be considered if it has a _get_axis\n        attribute.\n    intersect : bool, default False\n        If True, calculate the intersection between indexes. Otherwise,\n        calculate the union.\n    axis : {0 or 'index', 1 or 'outer'}, default 0\n        The axis to extract indexes from.\n    sort : bool, default True\n        Whether the result index should come out sorted or not.\n\n    Returns\n    -------\n    Index", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_shortcut(self, qaction_or_qshortcut, context, name,\n                          add_sc_to_tip=False):\n        \"\"\"\n        \n        \"\"\"\n        self.main.register_shortcut(qaction_or_qshortcut, context,\n                                    name, add_sc_to_tip)", "output": "Register QAction or QShortcut to Spyder main application.\n\n        if add_sc_to_tip is True, the shortcut is added to the\n        action's tooltip", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kill(self, sig):\n        '''\n        '''\n        if sys.platform == 'win32':\n            if sig in [signal.SIGINT, signal.CTRL_C_EVENT]:\n                sig = signal.CTRL_C_EVENT\n            elif sig in [signal.SIGBREAK, signal.CTRL_BREAK_EVENT]:\n                sig = signal.CTRL_BREAK_EVENT\n            else:\n                sig = signal.SIGTERM\n\n        os.kill(self.proc.pid, sig)", "output": "Sends a Unix signal to the subprocess.\n\n        Use constants from the :mod:`signal` module to specify which signal.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _construct_auto_distance(feature_names, column_names, column_types, sample):\n    \"\"\"\n    \n    \"\"\"\n\n    ## Make a dictionary from the column_names and column_types\n    col_type_dict = {k: v for k, v in zip(column_names, column_types)}\n\n    ## Loop through feature names, appending a distance component if the\n    #  feature's type is *not* numeric. If the type *is* numeric, append it to\n    #  the numeric_cols list, then at the end make a numeric columns distance\n    #  component.\n    composite_distance_params = []\n    numeric_cols = []\n\n    for c in feature_names:\n        if col_type_dict[c] == str:\n            composite_distance_params.append([[c], _turicreate.distances.levenshtein, 1])\n        elif col_type_dict[c] == dict:\n            composite_distance_params.append([[c], _turicreate.distances.jaccard, 1])\n        elif col_type_dict[c] == array.array:\n            composite_distance_params.append([[c], _turicreate.distances.euclidean, 1])\n        elif col_type_dict[c] == list:\n            only_str_lists = _validate_lists(sample[c], allowed_types=[str])\n            if not only_str_lists:\n                raise TypeError(\"Only lists of all str objects are currently supported\")\n            composite_distance_params.append([[c], _turicreate.distances.jaccard, 1])\n        elif col_type_dict[c] in [int, float, array.array, list]:\n            numeric_cols.append(c)\n        else:\n            raise TypeError(\"Unable to automatically determine a distance \"+\\\n                \"for column {}\".format(c))\n\n    # Make the standalone numeric column distance component\n    if len(numeric_cols) > 0:\n        composite_distance_params.append([numeric_cols, _turicreate.distances.euclidean, 1])\n\n    return composite_distance_params", "output": "Construct composite distance parameters based on selected features and their\n    types.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ixs(self, i, axis=0):\n        \"\"\"\n        \n        \"\"\"\n        label = self.index[i]\n        if isinstance(label, Index):\n            return self.take(i, axis=axis)\n        else:\n            return self._get_val_at(i)", "output": "Return the i-th value or values in the SparseSeries by location\n\n        Parameters\n        ----------\n        i : int, slice, or sequence of integers\n\n        Returns\n        -------\n        value : scalar (int) or Series (slice, sequence)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode(data, encoding=None, errors='strict', keep=False,\n           preserve_dict_class=False, preserve_tuples=False):\n    '''\n    \n    '''\n    if isinstance(data, Mapping):\n        return encode_dict(data, encoding, errors, keep,\n                           preserve_dict_class, preserve_tuples)\n    elif isinstance(data, list):\n        return encode_list(data, encoding, errors, keep,\n                           preserve_dict_class, preserve_tuples)\n    elif isinstance(data, tuple):\n        return encode_tuple(data, encoding, errors, keep, preserve_dict_class) \\\n            if preserve_tuples \\\n            else encode_list(data, encoding, errors, keep,\n                             preserve_dict_class, preserve_tuples)\n    else:\n        try:\n            return salt.utils.stringutils.to_bytes(data, encoding, errors)\n        except TypeError:\n            # to_bytes raises a TypeError when input is not a\n            # string/bytestring/bytearray. This is expected and simply\n            # means we are going to leave the value as-is.\n            pass\n        except UnicodeEncodeError:\n            if not keep:\n                raise\n        return data", "output": "Generic function which will encode whichever type is passed, if necessary\n\n    If `strict` is True, and `keep` is False, and we fail to encode, a\n    UnicodeEncodeError will be raised. Passing `keep` as True allows for the\n    original value to silently be returned in cases where encoding fails. This\n    can be useful for cases where the data passed to this function is likely to\n    contain binary blobs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def edit_item(self):\r\n        \"\"\"\"\"\"\r\n        index = self.currentIndex()\r\n        if not index.isValid():\r\n            return\r\n        # TODO: Remove hard coded \"Value\" column number (3 here)\r\n        self.edit(index.child(index.row(), 3))", "output": "Edit item", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send(signal):\n    \"\"\"\n    \"\"\"\n    if hasattr(ray.worker.global_worker, \"actor_creation_task_id\"):\n        source_key = ray.worker.global_worker.actor_id.hex()\n    else:\n        # No actors; this function must have been called from a task\n        source_key = ray.worker.global_worker.current_task_id.hex()\n\n    encoded_signal = ray.utils.binary_to_hex(cloudpickle.dumps(signal))\n    ray.worker.global_worker.redis_client.execute_command(\n        \"XADD \" + source_key + \" * signal \" + encoded_signal)", "output": "Send signal.\n\n    The signal has a unique identifier that is computed from (1) the id\n    of the actor or task sending this signal (i.e., the actor or task calling\n    this function), and (2) an index that is incremented every time this\n    source sends a signal. This index starts from 1.\n\n    Args:\n        signal: Signal to be sent.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_volume(size, name, profile, location_id=None, **libcloud_kwargs):\n    '''\n    \n    '''\n    conn = _get_driver(profile=profile)\n    libcloud_kwargs = salt.utils.args.clean_kwargs(**libcloud_kwargs)\n    if location_id is not None:\n        location = _get_by_id(conn.list_locations(), location_id)\n    else:\n        location = None\n    # TODO : Support creating from volume snapshot\n\n    volume = conn.create_volume(size, name, location, snapshot=None, **libcloud_kwargs)\n    return _simple_volume(volume)", "output": "Create a storage volume\n\n    :param size: Size of volume in gigabytes (required)\n    :type size: ``int``\n\n    :param name: Name of the volume to be created\n    :type name: ``str``\n\n    :param location_id: Which data center to create a volume in. If\n                            empty, undefined behavior will be selected.\n                            (optional)\n    :type location_id: ``str``\n\n    :param profile: The profile key\n    :type  profile: ``str``\n\n    :param libcloud_kwargs: Extra arguments for the driver's list_volumes method\n    :type  libcloud_kwargs: ``dict``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion libcloud_compute.create_volume 1000 vol1 profile1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(self, key):\n        \"\"\"\n        \n        \"\"\"\n        if key not in self.map:\n            self.map[key] = len(self.items)\n            self.items.append(key)\n        return self.map[key]", "output": "Add `key` as an item to this OrderedSet, then return its index.\n\n        If `key` is already in the OrderedSet, return the index it already\n        had.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_model_by_id(self, model_id):\n        \"\"\"\n        \"\"\"\n\n        with open(os.path.join(self.path, str(model_id) + \".json\")) as fin:\n            json_str = fin.read().replace(\"\\n\", \"\")\n\n        load_model = json_to_graph(json_str)\n        return load_model", "output": "Get the model by model_id\n\n        Parameters\n        ----------\n        model_id : int\n            model index\n        \n        Returns\n        -------\n        load_model : Graph\n            the model graph representation", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cov(self, other=None, pairwise=None, bias=False, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        if other is None:\n            other = self._selected_obj\n            # only default unset\n            pairwise = True if pairwise is None else pairwise\n        other = self._shallow_copy(other)\n\n        def _get_cov(X, Y):\n            X = self._shallow_copy(X)\n            Y = self._shallow_copy(Y)\n            cov = libwindow.ewmcov(X._prep_values(), Y._prep_values(),\n                                   self.com, int(self.adjust),\n                                   int(self.ignore_na), int(self.min_periods),\n                                   int(bias))\n            return X._wrap_result(cov)\n\n        return _flex_binary_moment(self._selected_obj, other._selected_obj,\n                                   _get_cov, pairwise=bool(pairwise))", "output": "Exponential weighted sample covariance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_gid(name):\n    \"\"\"\"\"\"\n    if getgrnam is None or name is None:\n        return None\n    try:\n        result = getgrnam(name)\n    except KeyError:\n        result = None\n    if result is not None:\n        return result[2]\n    return None", "output": "Returns a gid, given a group name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def text(name, data, step=None, description=None):\n  \"\"\"\n  \"\"\"\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=None, description=description)\n  # TODO(https://github.com/tensorflow/tensorboard/issues/2109): remove fallback\n  summary_scope = (\n      getattr(tf.summary.experimental, 'summary_scope', None) or\n      tf.summary.summary_scope)\n  with summary_scope(\n      name, 'text_summary', values=[data, step]) as (tag, _):\n    tf.debugging.assert_type(data, tf.string)\n    return tf.summary.write(\n        tag=tag, tensor=data, step=step, metadata=summary_metadata)", "output": "Write a text summary.\n\n  Arguments:\n    name: A name for this summary. The summary tag used for TensorBoard will\n      be this name prefixed by any active name scopes.\n    data: A UTF-8 string tensor value.\n    step: Explicit `int64`-castable monotonic step value for this summary. If\n      omitted, this defaults to `tf.summary.experimental.get_step()`, which must\n      not be None.\n    description: Optional long-form description for this summary, as a\n      constant `str`. Markdown is supported. Defaults to empty.\n\n  Returns:\n    True on success, or false if no summary was emitted because no default\n    summary writer was available.\n\n  Raises:\n    ValueError: if a default writer exists, but no step was provided and\n      `tf.summary.experimental.get_step()` is None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_all_caps(x:Collection[str]) -> Collection[str]:\n    \"\"\n    res = []\n    for t in x:\n        if t.isupper() and len(t) > 1: res.append(TK_UP); res.append(t.lower())\n        else: res.append(t)\n    return res", "output": "Replace tokens in ALL CAPS in `x` by their lower version and add `TK_UP` before.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_months(self, date, months):\n        \"\"\"\n        \n        \"\"\"\n        year = date.year + (date.month + months - 1) // 12\n        month = (date.month + months - 1) % 12 + 1\n        return datetime.date(year=year, month=month, day=1)", "output": "Add ``months`` months to ``date``.\n\n        Unfortunately we can't use timedeltas to add months because timedelta counts in days\n        and there's no foolproof way to add N months in days without counting the number of\n        days per month.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def login(self, user, password, exe_path, comm_password=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            self._app = pywinauto.Application().connect(\n                path=self._run_exe_path(exe_path), timeout=1\n            )\n        # pylint: disable=broad-except\n        except Exception:\n            self._app = pywinauto.Application().start(exe_path)\n\n            # wait login window ready\n            while True:\n                try:\n                    self._app.top_window().Edit1.wait(\"ready\")\n                    break\n                except RuntimeError:\n                    pass\n\n            self._app.top_window().Edit1.type_keys(user)\n            self._app.top_window().Edit2.type_keys(password)\n            edit3 = self._app.top_window().window(control_id=0x3eb)\n            while True:\n                try:\n                    code = self._handle_verify_code()\n                    edit3.type_keys(code)\n                    time.sleep(1)\n                    self._app.top_window()[\"\u786e\u5b9a(Y)\"].click()\n                    # detect login is success or not\n                    try:\n                        self._app.top_window().wait_not(\"exists\", 5)\n                        break\n\n                    # pylint: disable=broad-except\n                    except Exception:\n                        self._app.top_window()[\"\u786e\u5b9a\"].click()\n\n                # pylint: disable=broad-except\n                except Exception:\n                    pass\n\n            self._app = pywinauto.Application().connect(\n                path=self._run_exe_path(exe_path), timeout=10\n            )\n        self._main = self._app.window(title=\"\u7f51\u4e0a\u80a1\u7968\u4ea4\u6613\u7cfb\u7edf5.0\")", "output": "\u767b\u9646\u5ba2\u6237\u7aef\n\n        :param user: \u8d26\u53f7\n        :param password: \u660e\u6587\u5bc6\u7801\n        :param exe_path: \u5ba2\u6237\u7aef\u8def\u5f84\u7c7b\u4f3c 'C:\\\\\u4e2d\u56fd\u94f6\u6cb3\u8bc1\u5238\u53cc\u5b50\u661f3.2\\\\Binarystar.exe',\n            \u9ed8\u8ba4 'C:\\\\\u4e2d\u56fd\u94f6\u6cb3\u8bc1\u5238\u53cc\u5b50\u661f3.2\\\\Binarystar.exe'\n        :param comm_password: \u901a\u8baf\u5bc6\u7801, \u534e\u6cf0\u9700\u8981\uff0c\u53ef\u4e0d\u8bbe\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n    \"\"\"\"\"\"\n    cmd = ['gsutil', 'ls', os.path.join(self.source_dir, '**')]\n    try:\n      files_list = subprocess.check_output(cmd).split('\\n')\n    except subprocess.CalledProcessError:\n      logging.error('Can''t read source directory')\n    all_submissions = [\n        s for s in files_list\n        if s.endswith('.zip') or s.endswith('.tar') or s.endswith('.tar.gz')\n    ]\n    for submission_path in all_submissions:\n      self.validate_and_copy_one_submission(submission_path)\n    self.stats.log_stats()\n    self.save_id_to_path_mapping()\n    if self.containers_file:\n      with open(self.containers_file, 'w') as f:\n        f.write('\\n'.join(sorted(self.list_of_containers)))", "output": "Runs validation of all submissions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_folder(cls, path:PathOrStr, extensions:Collection[str]=None, recurse:bool=True,\n                    include:Optional[Collection[str]]=None, processor:PreProcessors=None, **kwargs)->'ItemList':\n        \"\"\"\"\"\"\n        path = Path(path)\n        return cls(get_files(path, extensions, recurse=recurse, include=include), path=path, processor=processor, **kwargs)", "output": "Create an `ItemList` in `path` from the filenames that have a suffix in `extensions`.\n        `recurse` determines if we search subfolders.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _try_recover(self, trial, error_msg):\n        \"\"\"\n        \"\"\"\n        try:\n            self.trial_executor.stop_trial(\n                trial,\n                error=error_msg is not None,\n                error_msg=error_msg,\n                stop_logger=False)\n            trial.result_logger.flush()\n            if self.trial_executor.has_resources(trial.resources):\n                logger.info(\"Attempting to recover\"\n                            \" trial state from last checkpoint.\")\n                self.trial_executor.start_trial(trial)\n                if trial.status == Trial.ERROR:\n                    raise RuntimeError(\"Trial did not start correctly.\")\n            else:\n                logger.debug(\"Notifying Scheduler and requeueing trial.\")\n                self._requeue_trial(trial)\n        except Exception:\n            logger.exception(\"Error recovering trial from checkpoint, abort.\")\n            self._scheduler_alg.on_trial_error(self, trial)\n            self._search_alg.on_trial_complete(trial.trial_id, error=True)", "output": "Tries to recover trial.\n\n        Notifies SearchAlgorithm and Scheduler if failure to recover.\n\n        Args:\n            trial (Trial): Trial to recover.\n            error_msg (str): Error message from prior to invoking this method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def provide_label(self):\n        \"\"\"\"\"\"\n        return [(k, tuple([self.batch_size] + list(v.shape[1:]))) for k, v in self.label]", "output": "The name and shape of label provided by this iterator", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_id(self, id, module):\n        \"\"\"\"\"\"\n        assert isinstance(id, basestring)\n        assert isinstance(module, basestring)\n        self.id2module[id] = module", "output": "Associate the given id with the given project module.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gen_ascii_docs(src='fastai'):\n    \"\"\"\n    \"\"\"\n    os.chdir(Path(__file__).absolute().parent)\n    with working_directory('..'):\n        path = Path(src)\n        if path.is_dir():\n            file_paths = list(path.glob('**/*.py'))\n        else:\n            file_paths = [path]\n\n    pat = re.compile('^(?!__init__).*.py\\Z')\n    for file_path in file_paths:\n        if pat.match(file_path.name):\n            file_path.parent.mkdir(parents=True, exist_ok=True)\n            with working_directory('..'):\n                tmpl_str = parse_module(file_path)\n\n            (file_path.parent/(file_path.name.rsplit('.',1)[0] + '.adoc.tmpl')).write_text(tmpl_str)\n            (file_path.parent/(file_path.name.rsplit('.',1)[0] + '.adoc')).write_text(re.sub(r\"{{(.*?)}}\", parse_tmpl, tmpl_str, flags=re.DOTALL))\n    if path.is_dir():\n        subprocess.call(['asciidoctor', str(path) + '/**/*.adoc'])\n    else:\n        subprocess.call(['asciidoctor', str(path).rsplit('.',1)[0] + '.adoc'])", "output": "Generate documentation for fastai library in HTML (asciidoctor required)\n    :param str src: The absolute/relative path of source file/dir", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_weights(self, m):\n        \"\"\" \n        \"\"\"\n        classname = m.__class__.__name__\n        if classname.find('Linear') != -1:\n            if hasattr(m, 'weight') and m.weight is not None:\n                self.init_weight(m.weight)\n            if hasattr(m, 'bias') and m.bias is not None:\n                self.init_bias(m.bias)\n        elif classname.find('AdaptiveEmbedding') != -1:\n            if hasattr(m, 'emb_projs'):\n                for i in range(len(m.emb_projs)):\n                    if m.emb_projs[i] is not None:\n                        nn.init.normal_(m.emb_projs[i], 0.0, self.config.proj_init_std)\n        elif classname.find('Embedding') != -1:\n            if hasattr(m, 'weight'):\n                self.init_weight(m.weight)\n        elif classname.find('ProjectedAdaptiveLogSoftmax') != -1:\n            if hasattr(m, 'cluster_weight') and m.cluster_weight is not None:\n                self.init_weight(m.cluster_weight)\n            if hasattr(m, 'cluster_bias') and m.cluster_bias is not None:\n                self.init_bias(m.cluster_bias)\n            if hasattr(m, 'out_projs'):\n                for i in range(len(m.out_projs)):\n                    if m.out_projs[i] is not None:\n                        nn.init.normal_(m.out_projs[i], 0.0, self.config.proj_init_std)\n        elif classname.find('LayerNorm') != -1:\n            if hasattr(m, 'weight'):\n                nn.init.normal_(m.weight, 1.0, self.config.init_std)\n            if hasattr(m, 'bias') and m.bias is not None:\n                self.init_bias(m.bias)\n        elif classname.find('TransformerLM') != -1:\n            if hasattr(m, 'r_emb'):\n                self.init_weight(m.r_emb)\n            if hasattr(m, 'r_w_bias'):\n                self.init_weight(m.r_w_bias)\n            if hasattr(m, 'r_r_bias'):\n                self.init_weight(m.r_r_bias)\n            if hasattr(m, 'r_bias'):\n                self.init_bias(m.r_bias)", "output": "Initialize the weights.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _maybe_mask_result(self, result, mask, other, op_name):\n        \"\"\"\n        \n        \"\"\"\n\n        # may need to fill infs\n        # and mask wraparound\n        if is_float_dtype(result):\n            mask |= (result == np.inf) | (result == -np.inf)\n\n        # if we have a float operand we are by-definition\n        # a float result\n        # or our op is a divide\n        if ((is_float_dtype(other) or is_float(other)) or\n                (op_name in ['rtruediv', 'truediv', 'rdiv', 'div'])):\n            result[mask] = np.nan\n            return result\n\n        return type(self)(result, mask, copy=False)", "output": "Parameters\n        ----------\n        result : array-like\n        mask : array-like bool\n        other : scalar or array-like\n        op_name : str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def top(**kwargs):\n    '''\n    \n    '''\n    if 'id' not in kwargs['opts']:\n        return {}\n    cmd = '{0} {1}'.format(\n            __opts__['master_tops']['ext_nodes'],\n            kwargs['opts']['id']\n            )\n    ndata = salt.utils.yaml.safe_load(\n        subprocess.Popen(\n            cmd,\n            shell=True,\n            stdout=subprocess.PIPE).communicate()[0]\n    )\n    if not ndata:\n        log.info('master_tops ext_nodes call did not return any data')\n    ret = {}\n    if 'environment' in ndata:\n        env = ndata['environment']\n    else:\n        env = 'base'\n\n    if 'classes' in ndata:\n        if isinstance(ndata['classes'], dict):\n            ret[env] = list(ndata['classes'])\n        elif isinstance(ndata['classes'], list):\n            ret[env] = ndata['classes']\n        else:\n            return ret\n    else:\n        log.info('master_tops ext_nodes call did not have a dictionary with a \"classes\" key.')\n\n    return ret", "output": "Run the command configured", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vserver_sslcert_add(v_name, sc_name, **connection_args):\n    '''\n    \n    '''\n    ret = True\n    if vserver_sslcert_exists(v_name, sc_name, **connection_args):\n        return False\n    nitro = _connect(**connection_args)\n    if nitro is None:\n        return False\n    sslcert = NSSSLVServerSSLCertKeyBinding()\n    sslcert.set_vservername(v_name)\n    sslcert.set_certkeyname(sc_name)\n    try:\n        NSSSLVServerSSLCertKeyBinding.add(nitro, sslcert)\n    except NSNitroError as error:\n        log.debug('netscaler module error - NSSSLVServerSSLCertKeyBinding.add() failed: %s', error)\n        ret = False\n    _disconnect(nitro)\n    return ret", "output": "Binds a SSL certificate to a vserver\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' netscaler.vserver_sslcert_add 'vserverName' 'sslCertificateName'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def l1_error(true, pred):\n  \"\"\"\"\"\"\n  return tf.reduce_sum(tf.abs(true - pred)) / tf.to_float(tf.size(pred))", "output": "L1 distance between tensors true and pred.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_toplosses_idxs(cls, learn, n_imgs, **kwargs):\n        \"\"\n        dl = learn.data.fix_dl\n        if not n_imgs: n_imgs = len(dl.dataset)\n        _,_,top_losses = learn.get_preds(ds_type=DatasetType.Fix, with_loss=True)\n        idxs = torch.topk(top_losses, n_imgs)[1]\n        return cls.padded_ds(dl.dataset, **kwargs), idxs", "output": "Sorts `ds_type` dataset by top losses and returns dataset and sorted indices.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_user_tags(name, tags, runas=None):\n    '''\n    '''\n    if runas is None and not salt.utils.platform.is_windows():\n        runas = salt.utils.user.get_user()\n\n    if not isinstance(tags, (list, tuple)):\n        tags = [tags]\n\n    res = __salt__['cmd.run_all'](\n        [RABBITMQCTL, 'set_user_tags', name] + list(tags),\n        reset_system_locale=False,\n        runas=runas,\n        python_shell=False)\n    msg = \"Tag(s) set\"\n    return _format_response(res, msg)", "output": "Add user tags via rabbitmqctl set_user_tags\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rabbitmq.set_user_tags myadmin administrator", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def import_file(name, loc):\n    \"\"\"\n    \"\"\"\n    loc = path2str(loc)\n    if is_python_pre_3_5:\n        import imp\n\n        return imp.load_source(name, loc)\n    else:\n        import importlib.util\n\n        spec = importlib.util.spec_from_file_location(name, str(loc))\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n        return module", "output": "Import module from a file. Used to load models from a directory.\n\n    name (unicode): Name of module to load.\n    loc (unicode / Path): Path to the file.\n    RETURNS: The loaded module.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def restart_kernel(self):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        sw = self.shellwidget\r\n\r\n        if not running_under_pytest() and self.ask_before_restart:\r\n            message = _('Are you sure you want to restart the kernel?')\r\n            buttons = QMessageBox.Yes | QMessageBox.No\r\n            result = QMessageBox.question(self, _('Restart kernel?'),\r\n                                          message, buttons)\r\n        else:\r\n            result = None\r\n\r\n        if (result == QMessageBox.Yes or\r\n                running_under_pytest() or\r\n                not self.ask_before_restart):\r\n            if sw.kernel_manager:\r\n                if self.infowidget.isVisible():\r\n                    self.infowidget.hide()\r\n                    sw.show()\r\n                try:\r\n                    sw.kernel_manager.restart_kernel(\r\n                        stderr=self.stderr_handle)\r\n                except RuntimeError as e:\r\n                    sw._append_plain_text(\r\n                        _('Error restarting kernel: %s\\n') % e,\r\n                        before_prompt=True\r\n                    )\r\n                else:\r\n                    # For issue 6235.  IPython was changing the setting of\r\n                    # %colors on windows by assuming it was using a dark\r\n                    # background.  This corrects it based on the scheme.\r\n                    self.set_color_scheme(sw.syntax_style)\r\n                    sw._append_html(_(\"<br>Restarting kernel...\\n<hr><br>\"),\r\n                                    before_prompt=False)\r\n            else:\r\n                sw._append_plain_text(\r\n                    _('Cannot restart a kernel not started by Spyder\\n'),\r\n                    before_prompt=True\r\n                )", "output": "Restart the associated kernel.\r\n\r\n        Took this code from the qtconsole project\r\n        Licensed under the BSD license", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kernels_status(self, kernel):\n        \"\"\" \n        \"\"\"\n        if kernel is None:\n            raise ValueError('A kernel must be specified')\n        if '/' in kernel:\n            self.validate_kernel_string(kernel)\n            kernel_url_list = kernel.split('/')\n            owner_slug = kernel_url_list[0]\n            kernel_slug = kernel_url_list[1]\n        else:\n            owner_slug = self.get_config_value(self.CONFIG_NAME_USER)\n            kernel_slug = kernel\n        response = self.process_response(\n            self.kernel_status_with_http_info(owner_slug, kernel_slug))\n        return response", "output": "call to the api to get the status of a kernel.\n             Parameters\n            ==========\n            kernel: the kernel to get the status for", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def n_hot(ids, c):\n    '''\n    \n    '''\n    res = np.zeros((c,), dtype=np.float32)\n    res[ids] = 1\n    return res", "output": "one hot encoding by index. Returns array of length c, where all entries are 0, except for the indecies in ids", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _fmt_metric(value, show_stdv=True):\n    \"\"\"\"\"\"\n    if len(value) == 2:\n        return '%s:%g' % (value[0], value[1])\n    if len(value) == 3:\n        if show_stdv:\n            return '%s:%g+%g' % (value[0], value[1], value[2])\n        return '%s:%g' % (value[0], value[1])\n    raise ValueError(\"wrong metric value\")", "output": "format metric string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _recenter(self):\n        \"\"\"\n        \n        \"\"\"\n        for split_idx in range(len(self._splits)):\n            split = self._splits[split_idx]\n            len_idx = self._split2len_idx[split]\n            if split == self._splits[-1]:\n                continue\n            right_split = self._splits[split_idx + 1]\n\n            # Try shifting the centroid to the left\n            if len_idx > 0 and self._lengths[len_idx - 1] not in self._split_cntr:\n                new_split = self._lengths[len_idx - 1]\n                left_delta = self._len_cntr[split] * (right_split - new_split) - self._split_cntr[split] * (\n                        split - new_split)\n                if left_delta < 0:\n                    self._splits[split_idx] = new_split\n                    self._split2len_idx[new_split] = len_idx - 1\n                    del self._split2len_idx[split]\n                    self._split_cntr[split] -= self._len_cntr[split]\n                    self._split_cntr[right_split] += self._len_cntr[split]\n                    self._split_cntr[new_split] = self._split_cntr[split]\n                    del self._split_cntr[split]\n\n            # Try shifting the centroid to the right\n            elif len_idx < len(self._lengths) - 2 and self._lengths[len_idx + 1] not in self._split_cntr:\n                new_split = self._lengths[len_idx + 1]\n                right_delta = self._split_cntr[split] * (new_split - split) - self._len_cntr[split] * (\n                        new_split - split)\n                if right_delta <= 0:\n                    self._splits[split_idx] = new_split\n                    self._split2len_idx[new_split] = len_idx + 1\n                    del self._split2len_idx[split]\n                    self._split_cntr[split] += self._len_cntr[split]\n                    self._split_cntr[right_split] -= self._len_cntr[split]\n                    self._split_cntr[new_split] = self._split_cntr[split]\n                    del self._split_cntr[split]", "output": "one iteration of k-means", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def export_dashboards(print_stdout, dashboard_file):\n    \"\"\"\"\"\"\n    data = dashboard_import_export.export_dashboards(db.session)\n    if print_stdout or not dashboard_file:\n        print(data)\n    if dashboard_file:\n        logging.info('Exporting dashboards to %s', dashboard_file)\n        with open(dashboard_file, 'w') as data_stream:\n            data_stream.write(data)", "output": "Export dashboards to JSON", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_classifier(self, prefix):\n        \"\"\"  \"\"\"\n        with self.name_scope():\n            classifier = nn.Dense(2, prefix=prefix)\n        return classifier", "output": "Construct a decoder for the next sentence prediction task", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zip_dir(directory):\n    \"\"\"\"\"\"\n    result = io.BytesIO()\n    dlen = len(directory)\n    with ZipFile(result, \"w\") as zf:\n        for root, dirs, files in os.walk(directory):\n            for name in files:\n                full = os.path.join(root, name)\n                rel = root[dlen:]\n                dest = os.path.join(rel, name)\n                zf.write(full, dest)\n    return result", "output": "zip a directory tree into a BytesIO object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def equal_levels(self, other):\n        \"\"\"\n        \n\n        \"\"\"\n        if self.nlevels != other.nlevels:\n            return False\n\n        for i in range(self.nlevels):\n            if not self.levels[i].equals(other.levels[i]):\n                return False\n        return True", "output": "Return True if the levels of both MultiIndex objects are the same", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(text):\n        \"\"\"\n        \"\"\"\n        try:\n            return int(text)\n        except ValueError:\n            try:\n                amount = float(text)\n                assert not isnan(amount) and not isinf(amount)\n                return amount\n            except (ValueError, AssertionError):\n                return None", "output": "Try to parse into a number.\n\n        Return:\n            the number (int or float) if successful; otherwise None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def detect_batch(self, batch):\n        \"\"\"\n        \n        \"\"\"\n        self.mod.forward(batch, is_train=False)\n        detections = self.mod.get_outputs()[0]\n        positive_detections = Detector.filter_positive_detections(detections)\n        return positive_detections", "output": "Return detections for batch\n        :param batch:\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _encode_json(obj):\n    '''\n    \n    '''\n    def _dump_obj(obj):\n        if isinstance(obj, dict):\n            return obj\n        d = dict()\n        for k in dir(obj):\n            if not k.startswith('_'):\n                d[k] = getattr(obj, k)\n        return d\n    return json.dumps(obj, default=_dump_obj)", "output": "Encode object as json str.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_docs(self, arg=None):\n        '''\n        \n        '''\n        if arg:\n            if '*' in arg:\n                target_mod = arg\n                _use_fnmatch = True\n            else:\n                target_mod = arg + '.' if not arg.endswith('.') else arg\n                _use_fnmatch = False\n            if _use_fnmatch:\n                docs = [(fun, self.functions[fun].__doc__)\n                        for fun in fnmatch.filter(self.functions, target_mod)]\n            else:\n                docs = [(fun, self.functions[fun].__doc__)\n                        for fun in sorted(self.functions)\n                        if fun == arg or fun.startswith(target_mod)]\n        else:\n            docs = [(fun, self.functions[fun].__doc__)\n                    for fun in sorted(self.functions)]\n        docs = dict(docs)\n        return salt.utils.doc.strip_rst(docs)", "output": "Return a dictionary of functions and the inline documentation for each", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def saveFileEnc(self, filename, encoding):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlSaveFileEnc(filename, self._o, encoding)\n        return ret", "output": "Dump an XML document, converting it to the given encoding", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _clean_workers(self):\n        \"\"\"\"\"\"\n        while self._bag_collector:\n            self._bag_collector.popleft()\n        self._timer_worker_delete.stop()", "output": "Delete periodically workers in workers bag.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _normalize_index_columns_in_place(equities,\n                                      equity_supplementary_mappings,\n                                      futures,\n                                      exchanges,\n                                      root_symbols):\n    \"\"\"\n    \n    \"\"\"\n    for frame, column_name in ((equities, 'sid'),\n                               (equity_supplementary_mappings, 'sid'),\n                               (futures, 'sid'),\n                               (exchanges, 'exchange'),\n                               (root_symbols, 'root_symbol')):\n        if frame is not None and column_name in frame:\n            frame.set_index(column_name, inplace=True)", "output": "Update dataframes in place to set indentifier columns as indices.\n\n    For each input frame, if the frame has a column with the same name as its\n    associated index column, set that column as the index.\n\n    Otherwise, assume the index already contains identifiers.\n\n    If frames are passed as None, they're ignored.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_dict(self):\n        \"\"\"\n        \"\"\"\n        self.validate_properties()\n\n        resource_dict = self._generate_resource_dict()\n\n        return {self.logical_id: resource_dict}", "output": "Validates that the required properties for this Resource have been provided, then returns a dict\n        corresponding to the given Resource object. This dict will take the format of a single entry in the Resources\n        section of a CloudFormation template, and will take the following format. ::\n\n            {\n                \"<logical id>\": {\n                    \"Type\": \"<resource type>\",\n                    \"DependsOn\": \"<value specified by user>\",\n                    \"Properties\": {\n                        <set of properties>\n                    }\n                }\n            }\n\n        The resulting dict can then be serialized to JSON or YAML and included as part of a CloudFormation template.\n\n        :returns: a dict corresponding to this Resource's entry in a CloudFormation template\n        :rtype: dict\n        :raises TypeError: if a required property is missing from this Resource", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enable_inheritance(path, objectType, clear=False):\n    '''\n    \n    '''\n    dc = daclConstants()\n    objectType = dc.getObjectTypeBit(objectType)\n    path = dc.processPath(path, objectType)\n\n    return _set_dacl_inheritance(path, objectType, True, None, clear)", "output": "enable/disable inheritance on an object\n\n    Args:\n        path: The path to the object\n        objectType: The type of object (FILE, DIRECTORY, REGISTRY)\n        clear: True will remove non-Inherited ACEs from the ACL\n\n    Returns (dict): A dictionary containing the results\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'minion-id' win_dacl.enable_inheritance c:\\temp directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_before_transform_template(self, template_dict):\n        \"\"\"\n        \n        \"\"\"\n\n        try:\n            global_section = Globals(template_dict)\n        except InvalidGlobalsSectionException as ex:\n            raise InvalidDocumentException([ex])\n\n        # For each resource in template, try and merge with Globals if necessary\n        template = SamTemplate(template_dict)\n        for logicalId, resource in template.iterate():\n            resource.properties = global_section.merge(resource.type, resource.properties)\n            template.set(logicalId, resource)\n\n        # Remove the Globals section from template if necessary\n        Globals.del_section(template_dict)", "output": "Hook method that runs before a template gets transformed. In this method, we parse and process Globals section\n        from the template (if present).\n\n        :param dict template_dict: SAM template as a dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_terminal(self):\r\n        \"\"\"\"\"\"\r\n        self.clear()\r\n        self.new_prompt(self.interpreter.p2 if self.interpreter.more else self.interpreter.p1)", "output": "Reimplement ShellBaseWidget method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_default(sld, tld):\n    '''\n    \n    '''\n    opts = salt.utils.namecheap.get_opts('namecheap.domains.dns.setDefault')\n    opts['SLD'] = sld\n    opts['TLD'] = tld\n    response_xml = salt.utils.namecheap.post_request(opts)\n    if response_xml is None:\n        return False\n\n    dnsresult = response_xml.getElementsByTagName('DomainDNSSetDefaultResult')[0]\n    return salt.utils.namecheap.string_to_value(dnsresult.getAttribute('Updated'))", "output": "Sets domain to use namecheap default DNS servers. Required for free\n    services like Host record management, URL forwarding, email forwarding,\n    dynamic DNS and other value added services.\n\n    sld\n        SLD of the domain name\n\n    tld\n        TLD of the domain name\n\n    Returns ``True`` if the domain was successfully pointed at the default DNS\n    servers.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' namecheap_domains_dns.set_default sld tld", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def create_category(self, name, *, overwrites=None, reason=None):\n        \"\"\"\n        \"\"\"\n        data = await self._create_channel(name, overwrites, ChannelType.category, reason=reason)\n        channel = CategoryChannel(state=self._state, guild=self, data=data)\n\n        # temporarily add to the cache\n        self._channels[channel.id] = channel\n        return channel", "output": "|coro|\n\n        Same as :meth:`create_text_channel` except makes a :class:`CategoryChannel` instead.\n\n        .. note::\n\n            The ``category`` parameter is not supported in this function since categories\n            cannot have categories.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait_for_ssh(ip):\n  \"\"\"\"\"\"\n  for _ in range(12):\n    with safe_socket() as s:\n      try:\n        s.connect((ip, 22))\n        return True\n      except socket.timeout:\n        pass\n    time.sleep(10)\n  return False", "output": "Wait for SSH to be available at given IP address.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_reward_encoder():\n        \"\"\"\"\"\"\n        last_reward = tf.Variable(0, name=\"last_reward\", trainable=False, dtype=tf.float32)\n        new_reward = tf.placeholder(shape=[], dtype=tf.float32, name='new_reward')\n        update_reward = tf.assign(last_reward, new_reward)\n        return last_reward, new_reward, update_reward", "output": "Creates TF ops to track and increment recent average cumulative reward.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tensors_blocked_by_false(ops):\n    \"\"\" \n    \"\"\"\n    blocked = []\n    def recurse(op):\n        if op.type == \"Switch\":\n            blocked.append(op.outputs[1]) # the true path is blocked since we assume the ops we trace are False\n        else:\n            for out in op.outputs:\n                for c in out.consumers():\n                    recurse(c)\n    for op in ops:\n        recurse(op)\n\n    return blocked", "output": "Follows a set of ops assuming their value is False and find blocked Switch paths.\n\n    This is used to prune away parts of the model graph that are only used during the training\n    phase (like dropout, batch norm, etc.).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _random_pad_proposal(self, label, height, width):\n        \"\"\"\"\"\"\n        from math import sqrt\n        if not self.enabled or height <= 0 or width <= 0:\n            return ()\n        min_area = self.area_range[0] * height * width\n        max_area = self.area_range[1] * height * width\n        for _ in range(self.max_attempts):\n            ratio = random.uniform(*self.aspect_ratio_range)\n            if ratio <= 0:\n                continue\n            h = int(round(sqrt(min_area / ratio)))\n            max_h = int(round(sqrt(max_area / ratio)))\n            if round(h * ratio) < width:\n                h = int((width + 0.499999) / ratio)\n            if h < height:\n                h = height\n            if h > max_h:\n                h = max_h\n            if h < max_h:\n                h = random.randint(h, max_h)\n            w = int(round(h * ratio))\n            if (h - height) < 2 or (w - width) < 2:\n                continue  # marginal padding is not helpful\n\n            y = random.randint(0, max(0, h - height))\n            x = random.randint(0, max(0, w - width))\n            new_label = self._update_labels(label, (x, y, w, h), height, width)\n            return (x, y, w, h, new_label)\n        return ()", "output": "Generate random padding region", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def secure(self):\n        '''\n        \n        '''\n        log.debug('ConCache securing sockets')\n        if os.path.exists(self.cache_sock):\n            os.chmod(self.cache_sock, 0o600)\n        if os.path.exists(self.update_sock):\n            os.chmod(self.update_sock, 0o600)\n        if os.path.exists(self.upd_t_sock):\n            os.chmod(self.upd_t_sock, 0o600)", "output": "secure the sockets for root-only access", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def namePop(ctxt):\n    \"\"\" \"\"\"\n    if ctxt is None: ctxt__o = None\n    else: ctxt__o = ctxt._o\n    ret = libxml2mod.namePop(ctxt__o)\n    return ret", "output": "Pops the top element name from the name stack", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _machinectl(cmd,\n                output_loglevel='debug',\n                ignore_retcode=False,\n                use_vt=False):\n    '''\n    \n    '''\n    prefix = 'machinectl --no-legend --no-pager'\n    return __salt__['cmd.run_all']('{0} {1}'.format(prefix, cmd),\n                                   output_loglevel=output_loglevel,\n                                   ignore_retcode=ignore_retcode,\n                                   use_vt=use_vt)", "output": "Helper function to run machinectl", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lock_file(filename, interval=.5, timeout=15):\n    '''\n    \n    '''\n    log.trace('Attempting to obtain lock for %s', filename)\n    lock = filename + '.lock'\n    start = time.time()\n    while True:\n        if os.path.exists(lock):\n            if time.time() - start >= timeout:\n                log.warning('Unable to obtain lock for %s', filename)\n                return False\n            time.sleep(interval)\n        else:\n            break\n\n    with salt.utils.files.fopen(lock, 'a'):\n        pass", "output": "Lock a file; if it is already locked, then wait for it to become available\n    before locking it.\n\n    Note that these locks are only recognized by Salt Cloud, and not other\n    programs or platforms.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self, old_cmd):\n        \"\"\"\n\n        \"\"\"\n        if self.side_effect:\n            self.side_effect(old_cmd, self.script)\n        if settings.alter_history:\n            shell.put_to_history(self.script)\n        # This depends on correct setting of PYTHONIOENCODING by the alias:\n        logs.debug(u'PYTHONIOENCODING: {}'.format(\n            os.environ.get('PYTHONIOENCODING', '!!not-set!!')))\n\n        print(self._get_script())", "output": "Runs command from rule for passed command.\n\n        :type old_cmd: Command", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def key_from_req(req):\n    \"\"\"\"\"\"\n    if hasattr(req, \"key\"):\n        # from pkg_resources, such as installed dists for pip-sync\n        key = req.key\n    else:\n        # from packaging, such as install requirements from requirements.txt\n        key = req.name\n\n    key = key.replace(\"_\", \"-\").lower()\n    return key", "output": "Get an all-lowercase version of the requirement's name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_editor_doc(self, doc, force_refresh=False):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        if (self.locked and not force_refresh):\r\n            return\r\n        self.switch_to_editor_source()\r\n        self._last_editor_doc = doc\r\n        self.object_edit.setText(doc['obj_text'])\r\n\r\n        if self.rich_help:\r\n            self.render_sphinx_doc(doc)\r\n        else:\r\n            self.set_plain_text(doc, is_code=False)\r\n\r\n        if self.dockwidget is not None:\r\n            self.dockwidget.blockSignals(True)\r\n        self.__eventually_raise_help(doc['docstring'], force=force_refresh)\r\n        if self.dockwidget is not None:\r\n            self.dockwidget.blockSignals(False)", "output": "Use the help plugin to show docstring dictionary computed\r\n        with introspection plugin from the Editor plugin", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, attr):\n        \"\"\"\n        \n        \"\"\"\n        if self._attr:\n            ret = self._attr.copy()\n            if attr:\n                ret.update(attr)\n            return ret\n        else:\n            return attr if attr else {}", "output": "Get the attribute dict given the attribute set by the symbol.\n\n        Parameters\n        ----------\n        attr : dict of string to string\n            The attribute passed in by user during symbol creation.\n\n        Returns\n        -------\n        attr : dict of string to string\n            Updated attributes to add other scope related attributes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def browser_attach_timeout(self, value):\n        \"\"\"\n        \n\n        \"\"\"\n        if not isinstance(value, int):\n            raise ValueError('Browser Attach Timeout must be an integer.')\n        self._options[self.BROWSER_ATTACH_TIMEOUT] = value", "output": "Sets the options Browser Attach Timeout\n\n        :Args:\n         - value: Timeout in milliseconds", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop(name, kill=False, path=None, use_vt=None):\n    '''\n    \n    '''\n    _ensure_exists(name, path=path)\n    orig_state = state(name, path=path)\n    if orig_state == 'frozen' and not kill:\n        # Gracefully stopping a frozen container is slower than unfreezing and\n        # then stopping it (at least in my testing), so if we're not\n        # force-stopping the container, unfreeze it first.\n        unfreeze(name, path=path)\n    cmd = 'lxc-stop'\n    if kill:\n        cmd += ' -k'\n    ret = _change_state(cmd, name, 'stopped', use_vt=use_vt, path=path)\n    ret['state']['old'] = orig_state\n    return ret", "output": "Stop the named container\n\n    path\n        path to the container parent directory\n        default: /var/lib/lxc (system)\n\n        .. versionadded:: 2015.8.0\n\n    kill: False\n        Do not wait for the container to stop, kill all tasks in the container.\n        Older LXC versions will stop containers like this irrespective of this\n        argument.\n\n        .. versionchanged:: 2015.5.0\n            Default value changed to ``False``\n\n    use_vt\n        run the command through VT\n\n        .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion lxc.stop name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def servicegroup_exists(sg_name, sg_type=None, **connection_args):\n    '''\n    \n    '''\n    sg = _servicegroup_get(sg_name, **connection_args)\n    if sg is None:\n        return False\n    if sg_type is not None and sg_type.upper() != sg.get_servicetype():\n        return False\n    return True", "output": "Checks if a service group exists\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' netscaler.servicegroup_exists 'serviceGroupName'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clip_action(action, space):\n    \"\"\"\n    \"\"\"\n\n    if isinstance(space, gym.spaces.Box):\n        return np.clip(action, space.low, space.high)\n    elif isinstance(space, gym.spaces.Tuple):\n        if type(action) not in (tuple, list):\n            raise ValueError(\"Expected tuple space for actions {}: {}\".format(\n                action, space))\n        out = []\n        for a, s in zip(action, space.spaces):\n            out.append(clip_action(a, s))\n        return out\n    else:\n        return action", "output": "Called to clip actions to the specified range of this policy.\n\n    Arguments:\n        action: Single action.\n        space: Action space the actions should be present in.\n\n    Returns:\n        Clipped batch of actions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_build_status(req_id, nodename):\n    '''\n    \n    '''\n    counter = 0\n    req_id = six.text_type(req_id)\n    while counter < 10:\n        queue = clc.v1.Blueprint.GetStatus(request_id=(req_id))\n        if queue[\"PercentComplete\"] == 100:\n            server_name = queue[\"Servers\"][0]\n            creds = get_creds()\n            clc.v2.SetCredentials(creds[\"user\"], creds[\"password\"])\n            ip_addresses = clc.v2.Server(server_name).ip_addresses\n            internal_ip_address = ip_addresses[0][\"internal\"]\n            return internal_ip_address\n        else:\n            counter = counter + 1\n            log.info('Creating Cloud VM %s Time out in %s minutes',\n                     nodename, six.text_type(10 - counter))\n            time.sleep(60)", "output": "get the build status from CLC to make sure we dont return to early", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_path_to_sys_path(self):\r\n        \"\"\"\"\"\"\r\n        for path in reversed(self.get_spyder_pythonpath()):\r\n            sys.path.insert(1, path)", "output": "Add Spyder path to sys.path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_vm_size(vm_):\n    \n    '''\n    vm_size = config.get_cloud_config_value('size', vm_, __opts__)\n    ram = avail_sizes()[vm_size]['RAM']\n\n    if vm_size.startswith('Linode'):\n        vm_size = vm_size.replace('Linode ', '')\n\n    if ram == int(vm_size):\n        return ram\n    else:\n        raise SaltCloudNotFound(\n            'The specified size, {0}, could not be found.'.format(vm_size)\n        )", "output": "r'''\n    Returns the VM's size.\n\n    vm\\_\n        The VM to get the size for.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_config(config_file='/etc/dnsmasq.conf'):\n    '''\n    \n    '''\n    dnsopts = _parse_dnamasq(config_file)\n    if 'conf-dir' in dnsopts:\n        for filename in os.listdir(dnsopts['conf-dir']):\n            if filename.startswith('.'):\n                continue\n            if filename.endswith('~'):\n                continue\n            if filename.endswith('#') and filename.endswith('#'):\n                continue\n            dnsopts.update(_parse_dnamasq('{0}/{1}'.format(dnsopts['conf-dir'],\n                                                        filename)))\n    return dnsopts", "output": "Dumps all options from the config file.\n\n    config_file\n        The location of the config file from which to obtain contents.\n        Defaults to ``/etc/dnsmasq.conf``.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' dnsmasq.get_config\n        salt '*' dnsmasq.get_config config_file=/etc/dnsmasq.conf", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _setup_packages(self, sc):\n        \"\"\"\n        \n\n        \"\"\"\n        packages = self.py_packages\n        if not packages:\n            return\n        for package in packages:\n            mod = importlib.import_module(package)\n            try:\n                mod_path = mod.__path__[0]\n            except AttributeError:\n                mod_path = mod.__file__\n            tar_path = os.path.join(self.run_path, package + '.tar.gz')\n            tar = tarfile.open(tar_path, \"w:gz\")\n            tar.add(mod_path, os.path.basename(mod_path))\n            tar.close()\n            sc.addPyFile(tar_path)", "output": "This method compresses and uploads packages to the cluster", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_pb(self):\n        \"\"\"\n        \"\"\"\n        column_range_kwargs = {\"family_name\": self.column_family_id}\n        if self.start_column is not None:\n            if self.inclusive_start:\n                key = \"start_qualifier_closed\"\n            else:\n                key = \"start_qualifier_open\"\n            column_range_kwargs[key] = _to_bytes(self.start_column)\n        if self.end_column is not None:\n            if self.inclusive_end:\n                key = \"end_qualifier_closed\"\n            else:\n                key = \"end_qualifier_open\"\n            column_range_kwargs[key] = _to_bytes(self.end_column)\n\n        column_range = data_v2_pb2.ColumnRange(**column_range_kwargs)\n        return data_v2_pb2.RowFilter(column_range_filter=column_range)", "output": "Converts the row filter to a protobuf.\n\n        First converts to a :class:`.data_v2_pb2.ColumnRange` and then uses it\n        in the ``column_range_filter`` field.\n\n        :rtype: :class:`.data_v2_pb2.RowFilter`\n        :returns: The converted current object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def output(data, **kwargs):  # pylint: disable=unused-argument\n    '''\n    \n    '''\n    out = ''\n    for id_ in data['data']:\n        out += '{0}\\n'.format(id_)\n        for vm_ in data['data'][id_]['vm_info']:\n            out += '  {0}\\n'.format(vm_)\n            vm_data = data[id_]['vm_info'][vm_]\n            if 'cpu' in vm_data:\n                out += '    CPU: {0}\\n'.format(vm_data['cpu'])\n            if 'mem' in vm_data:\n                out += '    Memory: {0}\\n'.format(vm_data['mem'])\n            if 'state' in vm_data:\n                out += '    State: {0}\\n'.format(vm_data['state'])\n            if 'graphics' in vm_data:\n                if vm_data['graphics'].get('type', '') == 'vnc':\n                    out += '    Graphics: vnc - {0}:{1}\\n'.format(\n                            id_,\n                            vm_data['graphics']['port'])\n            if 'disks' in vm_data:\n                for disk, d_data in six.iteritems(vm_data['disks']):\n                    out += '    Disk - {0}:\\n'.format(disk)\n                    out += '      Size: {0}\\n'.format(d_data['disk size'])\n                    out += '      File: {0}\\n'.format(d_data['file'])\n                    out += '      File Format: {0}\\n'.format(d_data['file format'])\n            if 'nics' in vm_data:\n                for mac in vm_data['nics']:\n                    out += '    Nic - {0}:\\n'.format(mac)\n                    out += '      Source: {0}\\n'.format(\n                                vm_data['nics'][mac]['source'][next(six.iterkeys(vm_data['nics'][mac]['source']))])\n                    out += '      Type: {0}\\n'.format(vm_data['nics'][mac]['type'])\n    return out", "output": "Display output for the salt-run virt.query function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def request_uri(self):\n        \"\"\"\"\"\"\n        uri = self.path or '/'\n\n        if self.query is not None:\n            uri += '?' + self.query\n\n        return uri", "output": "Absolute path including the query string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resizeEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        if not self.isMaximized() and not self.isFullScreen():\r\n            self.window_size = self.size()\r\n        QMainWindow.resizeEvent(self, event)", "output": "Reimplement Qt method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def markers_pass(self, req, extras=None):\n        \"\"\"\n        \n        \"\"\"\n        extra_evals = (\n            req.marker.evaluate({'extra': extra})\n            for extra in self.get(req, ()) + (extras or (None,))\n        )\n        return not req.marker or any(extra_evals)", "output": "Evaluate markers for req against each extra that\n        demanded it.\n\n        Return False if the req has a marker and fails\n        evaluation. Otherwise, return True.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __get_metadata(vm_):\n    '''\n    \n    '''\n    md = config.get_cloud_config_value(\n        'metadata', vm_, __opts__,\n        default='{}', search_global=False)\n    # Consider warning the user that the metadata in the cloud profile\n    # could not be interpreted, bad formatting?\n    try:\n        metadata = literal_eval(md)\n    except Exception:  # pylint: disable=W0703\n        metadata = None\n    if not metadata or not isinstance(metadata, dict):\n        metadata = {'items': [{\n            'key': 'salt-cloud-profile',\n            'value': vm_['profile']\n        }]}\n    else:\n        metadata['salt-cloud-profile'] = vm_['profile']\n        items = []\n        for k, v in six.iteritems(metadata):\n            items.append({'key': k, 'value': v})\n        metadata = {'items': items}\n    return metadata", "output": "Get configured metadata and add 'salt-cloud-profile'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_identifiers(source_code):\r\n    ''''''\r\n    tokens = set(re.split(r\"[^0-9a-zA-Z_.]\", source_code))\r\n    valid = re.compile(r'[a-zA-Z_]')\r\n    return [token for token in tokens if re.match(valid, token)]", "output": "Split source code into python identifier-like tokens", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_ipython_extension(ipython):\n    \"\"\"\"\"\"\n    from google.cloud.bigquery.magics import _cell_magic\n\n    ipython.register_magic_function(\n        _cell_magic, magic_kind=\"cell\", magic_name=\"bigquery\"\n    )", "output": "Called by IPython when this module is loaded as an IPython extension.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reduce(self, cross_series_reducer, *group_by_fields):\n        \"\"\"\n        \"\"\"\n        new_query = copy.deepcopy(self)\n        new_query._cross_series_reducer = cross_series_reducer\n        new_query._group_by_fields = group_by_fields\n        return new_query", "output": "Copy the query and add cross-series reduction.\n\n        Cross-series reduction combines time series by aggregating their\n        data points.\n\n        For example, you could request an aggregated time series for each\n        combination of project and zone as follows::\n\n            from google.cloud.monitoring import enums\n            query = query.reduce(enums.Aggregation.Reducer.REDUCE_MEAN,\n                                 'resource.project_id', 'resource.zone')\n\n        :type cross_series_reducer: str or\n            :class:`~google.cloud.monitoring_v3.gapic.enums.Aggregation.Reducer`\n        :param cross_series_reducer:\n            The approach to be used to combine time series. For example:\n            :data:`Reducer.REDUCE_MEAN`. See\n            :class:`~google.cloud.monitoring_v3.gapic.enums.Aggregation.Reducer`\n            and the descriptions of the `supported reducers`_.\n\n        :type group_by_fields: strs\n        :param group_by_fields:\n            Fields to be preserved by the reduction. For example, specifying\n            just ``\"resource.zone\"`` will result in one time series per zone.\n            The default is to aggregate all of the time series into just one.\n\n        :rtype: :class:`Query`\n        :returns: The new query object.\n\n        .. _supported reducers:\n            https://cloud.google.com/monitoring/api/ref_v3/rest/v3/\\\n            projects.timeSeries/list#Reducer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def append_use_flags(atom, uses=None, overwrite=False):\n    '''\n    \n    '''\n    if not uses:\n        uses = portage.dep.dep_getusedeps(atom)\n    if not uses:\n        return\n    atom = atom[:atom.rfind('[')]\n    append_to_package_conf('use', atom=atom, flags=uses, overwrite=overwrite)", "output": "Append a list of use flags for a given package or DEPEND atom\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' portage_config.append_use_flags \"app-admin/salt[ldap, -libvirt]\"\n        salt '*' portage_config.append_use_flags \">=app-admin/salt-0.14.1\" \"['ldap', '-libvirt']\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resource_create(resource_id, resource_type, resource_options=None, cibfile=None):\n    '''\n    \n    '''\n    return item_create(item='resource',\n                       item_id=resource_id,\n                       item_type=resource_type,\n                       extra_args=resource_options,\n                       cibfile=cibfile)", "output": "Create a resource via pcs command\n\n    resource_id\n        name for the resource\n    resource_type\n        resource type (f.e. ocf:heartbeat:IPaddr2 or VirtualIP)\n    resource_options\n        additional options for creating the resource\n    cibfile\n        use cibfile instead of the live CIB for manipulation\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pcs.resource_create resource_id='galera' resource_type='ocf:heartbeat:galera' resource_options=\"['wsrep_cluster_address=gcomm://node1.example.org,node2.example.org,node3.example.org', '--master']\" cibfile='/tmp/cib_for_galera.cib'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_directory_writable(dirname):\n  \"\"\"\n  \"\"\"\n  retval = shell_call(['docker', 'run', '-v',\n                       '{0}:/output_dir'.format(dirname),\n                       'busybox:1.27.2',\n                       'chmod', '-R', 'a+rwx', '/output_dir'])\n  if not retval:\n    logging.error('Failed to change permissions on directory: %s', dirname)\n  return retval", "output": "Makes directory readable and writable by everybody.\n\n  Args:\n    dirname: name of the directory\n\n  Returns:\n    True if operation was successfull\n\n  If you run something inside Docker container and it writes files, then\n  these files will be written as root user with restricted permissions.\n  So to be able to read/modify these files outside of Docker you have to change\n  permissions to be world readable and writable.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def contains_cursor(self, cursor):\n        \"\"\"\n        \n        \"\"\"\n        start = self.cursor.selectionStart()\n        end = self.cursor.selectionEnd()\n        if cursor.atBlockEnd():\n            end -= 1\n        return start <= cursor.position() <= end", "output": "Checks if the textCursor is in the decoration.\n\n        :param cursor: The text cursor to test\n        :type cursor: QtGui.QTextCursor\n\n        :returns: True if the cursor is over the selection", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def new_prompt(self, prompt):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        if self.get_cursor_line_column()[1] != 0:\r\n            self.write('\\n')\r\n        self.write(prompt, prompt=True)\r\n        # now we update our cursor giving end of prompt\r\n        self.current_prompt_pos = self.get_position('cursor')\r\n        self.ensureCursorVisible()\r\n        self.new_input_line = False", "output": "Print a new prompt and save its (line, index) position", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def text_channels(self):\n        \"\"\"\n        \"\"\"\n        r = [ch for ch in self._channels.values() if isinstance(ch, TextChannel)]\n        r.sort(key=lambda c: (c.position, c.id))\n        return r", "output": "List[:class:`TextChannel`]: A list of text channels that belongs to this guild.\n\n        This is sorted by the position and are in UI order from top to bottom.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_tarball(tar_paths):\n    \"\"\"\n    \n    \"\"\"\n    tarballfile = TemporaryFile()\n\n    with tarfile.open(fileobj=tarballfile, mode='w') as archive:\n        for path_on_system, path_in_tarball in tar_paths.items():\n            archive.add(path_on_system, arcname=path_in_tarball)\n\n    # Flush are seek to the beginning of the file\n    tarballfile.flush()\n    tarballfile.seek(0)\n\n    try:\n        yield tarballfile\n    finally:\n        tarballfile.close()", "output": "Context Manger that creates the tarball of the Docker Context to use for building the image\n\n    Parameters\n    ----------\n    tar_paths dict(str, str)\n        Key representing a full path to the file or directory and the Value representing the path within the tarball\n\n    Yields\n    ------\n        The tarball file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_help(self, ctx):\n        \"\"\"\n        \"\"\"\n        formatter = ctx.make_formatter()\n        self.format_help(ctx, formatter)\n        return formatter.getvalue().rstrip('\\n')", "output": "Formats the help into a string and returns it.  This creates a\n        formatter and will call into the following formatting methods:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_mxnet_ctc_loss(pred, seq_len, label):\n    \"\"\"  \"\"\"\n    pred_ctc = mx.sym.Reshape(data=pred, shape=(-4, seq_len, -1, 0))\n\n    loss = mx.sym.contrib.ctc_loss(data=pred_ctc, label=label)\n    ctc_loss = mx.sym.MakeLoss(loss)\n\n    softmax_class = mx.symbol.SoftmaxActivation(data=pred)\n    softmax_loss = mx.sym.MakeLoss(softmax_class)\n    softmax_loss = mx.sym.BlockGrad(softmax_loss)\n    return mx.sym.Group([softmax_loss, ctc_loss])", "output": "Adds Symbol.WapCTC on top of pred symbol and returns the resulting symbol", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_list(database=None, user=None, password=None, host=None, port=None):\n    '''\n    \n    '''\n    client = _client(user=user, password=password, host=host, port=port)\n\n    if not database:\n        return client.get_list_cluster_admins()\n\n    client.switch_database(database)\n    return client.get_list_users()", "output": "List cluster admins or database users.\n\n    If a database is specified: it will return database users list.\n    If a database is not specified: it will return cluster admins list.\n\n    database\n        The database to list the users from\n\n    user\n        The user to connect as\n\n    password\n        The password of the user\n\n    host\n        The host to connect to\n\n    port\n        The port to connect to\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb08.user_list\n        salt '*' influxdb08.user_list <database>\n        salt '*' influxdb08.user_list <database> <user> <password> <host> <port>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _discretize(a):\n  \"\"\"\"\"\"\n  arr = np.asarray(a)\n  index = np.argsort(arr)\n  inverse_index = np.zeros(arr.size, dtype=np.intp)\n  inverse_index[index] = np.arange(arr.size, dtype=np.intp)\n  arr = arr[index]\n  obs = np.r_[True, arr[1:] != arr[:-1]]\n  return obs.cumsum()[inverse_index] - 1", "output": "Discretizes array values to class labels.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unlock_file(filename):\n    '''\n    \n    '''\n    log.trace('Removing lock for %s', filename)\n    lock = filename + '.lock'\n    try:\n        os.remove(lock)\n    except OSError as exc:\n        log.trace('Unable to remove lock for %s: %s', filename, exc)", "output": "Unlock a locked file\n\n    Note that these locks are only recognized by Salt Cloud, and not other\n    programs or platforms.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def union(self, other, sort=None):\n        \"\"\"\n        \n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other, result_names = self._convert_can_do_setop(other)\n\n        if len(other) == 0 or self.equals(other):\n            return self\n\n        # TODO: Index.union returns other when `len(self)` is 0.\n\n        uniq_tuples = lib.fast_unique_multiple([self._ndarray_values,\n                                                other._ndarray_values],\n                                               sort=sort)\n\n        return MultiIndex.from_arrays(lzip(*uniq_tuples), sortorder=0,\n                                      names=result_names)", "output": "Form the union of two MultiIndex objects\n\n        Parameters\n        ----------\n        other : MultiIndex or array / Index of tuples\n        sort : False or None, default None\n            Whether to sort the resulting Index.\n\n            * None : Sort the result, except when\n\n              1. `self` and `other` are equal.\n              2. `self` has length 0.\n              3. Some values in `self` or `other` cannot be compared.\n                 A RuntimeWarning is issued in this case.\n\n            * False : do not sort the result.\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default value from ``True`` to ``None``\n               (without change in behaviour).\n\n        Returns\n        -------\n        Index\n\n        >>> index.union(index2)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def locate(pattern, database='', limit=0, **kwargs):\n    '''\n    \n    '''\n    options = ''\n    toggles = {\n        'basename': 'b',\n        'count': 'c',\n        'existing': 'e',\n        'follow': 'L',\n        'ignore': 'i',\n        'nofollow': 'P',\n        'wholename': 'w',\n        }\n    for option in kwargs:\n        if bool(kwargs[option]) is True and option in toggles:\n            options += toggles[option]\n    if options:\n        options = '-{0}'.format(options)\n    if database:\n        options += ' -d {0}'.format(database)\n    if limit > 0:\n        options += ' -l {0}'.format(limit)\n    if 'regex' in kwargs and bool(kwargs['regex']) is True:\n        options += ' --regex'\n    cmd = 'locate {0} {1}'.format(options, pattern)\n    out = __salt__['cmd.run'](cmd, python_shell=False).splitlines()\n    return out", "output": "Performs a file lookup. Valid options (and their defaults) are::\n\n        basename=False\n        count=False\n        existing=False\n        follow=True\n        ignore=False\n        nofollow=False\n        wholename=True\n        regex=False\n        database=<locate's default database>\n        limit=<integer, not set by default>\n\n    See the manpage for ``locate(1)`` for further explanation of these options.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' locate.locate", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_tz(tz):\n    \"\"\"  \"\"\"\n    zone = timezones.get_timezone(tz)\n    if zone is None:\n        zone = tz.utcoffset().total_seconds()\n    return zone", "output": "for a tz-aware type, return an encoded zone", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_object_versions(Bucket, Delimiter=None, EncodingType=None, Prefix=None,\n                 region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        Versions = []\n        DeleteMarkers = []\n        args = {'Bucket': Bucket}\n        args.update({'Delimiter': Delimiter}) if Delimiter else None\n        args.update({'EncodingType': EncodingType}) if Delimiter else None\n        args.update({'Prefix': Prefix}) if Prefix else None\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        IsTruncated = True\n        while IsTruncated:\n            ret = conn.list_object_versions(**args)\n            IsTruncated = ret.get('IsTruncated', False)\n            if IsTruncated in ('True', 'true', True):\n                args['KeyMarker'] = ret['NextKeyMarker']\n                args['VersionIdMarker'] = ret['NextVersionIdMarker']\n            Versions += ret.get('Versions', [])\n            DeleteMarkers += ret.get('DeleteMarkers', [])\n        return {'Versions': Versions, 'DeleteMarkers': DeleteMarkers}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "List objects in a given S3 bucket.\n\n    Returns a list of objects.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.list_object_versions mybucket", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_dot(node, **kwargs):\n    \"\"\"\"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n    input_node_a = input_nodes[0]\n    input_node_b = input_nodes[1]\n\n    trans_a_node = None\n    trans_b_node = None\n\n    trans_a = get_boolean_attribute_value(attrs, \"transpose_a\")\n    trans_b = get_boolean_attribute_value(attrs, \"transpose_b\")\n\n    op_name = \"transpose\" + str(kwargs[\"idx\"])\n\n    if trans_a:\n        trans_a_node = create_helper_trans_node(op_name, input_nodes[0], 'a')\n        input_node_a = op_name+\"_a\"\n    if trans_b:\n        trans_b_node = create_helper_trans_node(op_name, input_nodes[1], 'b')\n        input_node_b = op_name+\"_b\"\n\n    matmul_node = onnx.helper.make_node(\n        'MatMul',\n        inputs=[input_node_a, input_node_b],\n        outputs=[name],\n        name=name\n    )\n\n    if not trans_a and not trans_b:\n        return [matmul_node]\n    elif trans_a and not trans_b:\n        return [trans_a_node, matmul_node]\n    elif trans_b and not trans_a:\n        return [trans_b_node, matmul_node]\n    else:\n        return [trans_a_node, trans_b_node, matmul_node]", "output": "Map MXNet's dot operator attributes to onnx's\n    MatMul and Transpose operators based on the values set for\n    transpose_a, transpose_b attributes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def metric_crud(client, to_delete):\n    \"\"\"\"\"\"\n    METRIC_NAME = \"robots-%d\" % (_millis(),)\n    DESCRIPTION = \"Robots all up in your server\"\n    FILTER = \"logName:apache-access AND textPayload:robot\"\n    UPDATED_FILTER = \"textPayload:robot\"\n    UPDATED_DESCRIPTION = \"Danger, Will Robinson!\"\n\n    # [START client_list_metrics]\n    for metric in client.list_metrics():  # API call(s)\n        do_something_with(metric)\n    # [END client_list_metrics]\n\n    # [START metric_create]\n    metric = client.metric(METRIC_NAME, filter_=FILTER, description=DESCRIPTION)\n    assert not metric.exists()  # API call\n    metric.create()  # API call\n    assert metric.exists()  # API call\n    # [END metric_create]\n    to_delete.append(metric)\n\n    # [START metric_reload]\n    existing_metric = client.metric(METRIC_NAME)\n    existing_metric.reload()  # API call\n    # [END metric_reload]\n    assert existing_metric.filter_ == FILTER\n    assert existing_metric.description == DESCRIPTION\n\n    # [START metric_update]\n    existing_metric.filter_ = UPDATED_FILTER\n    existing_metric.description = UPDATED_DESCRIPTION\n    existing_metric.update()  # API call\n    # [END metric_update]\n    existing_metric.reload()\n    assert existing_metric.filter_ == UPDATED_FILTER\n    assert existing_metric.description == UPDATED_DESCRIPTION\n\n    def _metric_delete():\n        # [START metric_delete]\n        metric.delete()\n        # [END metric_delete]\n\n    _backoff_not_found(_metric_delete)\n    to_delete.remove(metric)", "output": "Metric CRUD.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _relative_position_to_absolute_position_unmasked(x):\n  \"\"\"\n  \"\"\"\n  x_shape = common_layers.shape_list(x)\n  batch = x_shape[0]\n  heads = x_shape[1]\n  length = x_shape[2]\n  # Concat columns of pad to shift from relative to absolute indexing.\n  col_pad = tf.zeros((batch, heads, length, 1))\n  x = tf.concat([x, col_pad], axis=3)\n\n  # Concat extra elements so to add up to shape (len+1, 2*len-1).\n  flat_x = tf.reshape(x, [batch, heads, length * 2 * length])\n  flat_pad = tf.zeros((batch, heads, length-1))\n  flat_x_padded = tf.concat([flat_x, flat_pad], axis=2)\n\n  # Reshape and slice out the padded elements.\n  final_x = tf.reshape(flat_x_padded, [batch, heads, length+1, 2*length-1])\n  final_x = final_x[:, :, :, length-1:]\n  final_x = final_x[:, :, :length, :]\n  return final_x", "output": "Converts tensor from relative to aboslute indexing for local attention.\n\n  Args:\n    x: a Tensor of shape [batch (or batch*num_blocks), heads,\n                          length, 2 * length - 1]\n\n  Returns:\n    A Tensor of shape [batch (or batch*num_blocks), heads, length, length-1]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_snapshot(name, snap_name, runas=None, all=False):\n    '''\n    \n    '''\n    # strict means raise an error if multiple snapshot IDs found for the name given\n    strict = not all\n\n    # Validate VM and snapshot names\n    name = salt.utils.data.decode(name)\n    snap_ids = _validate_snap_name(name, snap_name, strict=strict, runas=runas)\n    if isinstance(snap_ids, six.string_types):\n        snap_ids = [snap_ids]\n\n    # Delete snapshot(s)\n    ret = {}\n    for snap_id in snap_ids:\n        snap_id = snap_id.strip('{}')\n        # Construct argument list\n        args = [name, '--id', snap_id]\n\n        # Execute command\n        ret[snap_id] = prlctl('snapshot-delete', args, runas=runas)\n\n    # Return results\n    ret_keys = list(ret.keys())\n    if len(ret_keys) == 1:\n        return ret[ret_keys[0]]\n    else:\n        return ret", "output": "Delete a snapshot\n\n    .. note::\n\n        Deleting a snapshot from which other snapshots are dervied will not\n        delete the derived snapshots\n\n    :param str name:\n        Name/ID of VM whose snapshot will be deleted\n\n    :param str snap_name:\n        Name/ID of snapshot to delete\n\n    :param str runas:\n        The user that the prlctl command will be run as\n\n    :param bool all:\n        Delete all snapshots having the name given\n\n        .. versionadded:: 2016.11.0\n\n    Example:\n\n    .. code-block:: bash\n\n        salt '*' parallels.delete_snapshot macvm 'unneeded snapshot' runas=macdev\n        salt '*' parallels.delete_snapshot macvm 'Snapshot for linked clone' all=True runas=macdev", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_rfc3339(cls, stamp):\n        \"\"\"\n        \"\"\"\n        with_nanos = _RFC3339_NANOS.match(stamp)\n        if with_nanos is None:\n            raise ValueError(\n                \"Timestamp: {}, does not match pattern: {}\".format(\n                    stamp, _RFC3339_NANOS.pattern\n                )\n            )\n        bare = datetime.datetime.strptime(\n            with_nanos.group(\"no_fraction\"), _RFC3339_NO_FRACTION\n        )\n        fraction = with_nanos.group(\"nanos\")\n        if fraction is None:\n            nanos = 0\n        else:\n            scale = 9 - len(fraction)\n            nanos = int(fraction) * (10 ** scale)\n        return cls(\n            bare.year,\n            bare.month,\n            bare.day,\n            bare.hour,\n            bare.minute,\n            bare.second,\n            nanosecond=nanos,\n            tzinfo=pytz.UTC,\n        )", "output": "Parse RFC 3339-compliant timestamp, preserving nanoseconds.\n\n        Args:\n            stamp (str): RFC 3339 stamp, with up to nanosecond precision\n\n        Returns:\n            :class:`DatetimeWithNanoseconds`:\n                an instance matching the timestamp string\n\n        Raises:\n            ValueError: if `stamp` does not match the expected format", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def iterate(self, resource_type=None):\n        \"\"\"\n        \n        \"\"\"\n\n        for logicalId, resource_dict in self.resources.items():\n\n            resource = SamResource(resource_dict)\n            needs_filter = resource.valid()\n            if resource_type:\n                needs_filter = needs_filter and resource.type == resource_type\n\n            if needs_filter:\n                yield logicalId, resource", "output": "Iterate over all resources within the SAM template, optionally filtering by type\n\n        :param string resource_type: Optional type to filter the resources by\n        :yields (string, SamResource): Tuple containing LogicalId and the resource", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _StructPackDecoder(wire_type, format):\n  \"\"\"\n  \"\"\"\n\n  value_size = struct.calcsize(format)\n  local_unpack = struct.unpack\n\n  # Reusing _SimpleDecoder is slightly slower than copying a bunch of code, but\n  # not enough to make a significant difference.\n\n  # Note that we expect someone up-stack to catch struct.error and convert\n  # it to _DecodeError -- this way we don't have to set up exception-\n  # handling blocks every time we parse one value.\n\n  def InnerDecode(buffer, pos):\n    new_pos = pos + value_size\n    result = local_unpack(format, buffer[pos:new_pos])[0]\n    return (result, new_pos)\n  return _SimpleDecoder(wire_type, InnerDecode)", "output": "Return a constructor for a decoder for a fixed-width field.\n\n  Args:\n      wire_type:  The field's wire type.\n      format:  The format string to pass to struct.unpack().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def infer_dtype_from(val, pandas_dtype=False):\n    \"\"\"\n    \n    \"\"\"\n    if is_scalar(val):\n        return infer_dtype_from_scalar(val, pandas_dtype=pandas_dtype)\n    return infer_dtype_from_array(val, pandas_dtype=pandas_dtype)", "output": "interpret the dtype from a scalar or array. This is a convenience\n    routines to infer dtype from a scalar or an array\n\n    Parameters\n    ----------\n    pandas_dtype : bool, default False\n        whether to infer dtype including pandas extension types.\n        If False, scalar/array belongs to pandas extension types is inferred as\n        object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_distribution_id(vm_):\n    \n    '''\n    distributions = _query('avail', 'distributions')['DATA']\n    vm_image_name = config.get_cloud_config_value('image', vm_, __opts__)\n\n    distro_id = ''\n\n    for distro in distributions:\n        if vm_image_name == distro['LABEL']:\n            distro_id = distro['DISTRIBUTIONID']\n            return distro_id\n\n    if not distro_id:\n        raise SaltCloudNotFound(\n            'The DistributionID for the \\'{0}\\' profile could not be found.\\n'\n            'The \\'{1}\\' instance could not be provisioned. The following distributions '\n            'are available:\\n{2}'.format(\n                vm_image_name,\n                vm_['name'],\n                pprint.pprint(sorted([distro['LABEL'].encode(__salt_system_encoding__) for distro in distributions]))\n            )\n        )", "output": "r'''\n    Returns the distribution ID for a VM\n\n    vm\\_\n        The VM to get the distribution ID for", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_invoke_config(self, function):\n        \"\"\"\n        \n        \"\"\"\n\n        env_vars = self._make_env_vars(function)\n        code_abs_path = resolve_code_path(self.cwd, function.codeuri)\n\n        LOG.debug(\"Resolved absolute path to code is %s\", code_abs_path)\n\n        function_timeout = function.timeout\n\n        # The Runtime container handles timeout inside the container. When debugging with short timeouts, this can\n        # cause the container execution to stop. When in debug mode, we set the timeout in the container to a max 10\n        # hours. This will ensure the container doesn't unexpectedly stop while debugging function code\n        if self.is_debugging():\n            function_timeout = self.MAX_DEBUG_TIMEOUT\n\n        return FunctionConfig(name=function.name,\n                              runtime=function.runtime,\n                              handler=function.handler,\n                              code_abs_path=code_abs_path,\n                              layers=function.layers,\n                              memory=function.memory,\n                              timeout=function_timeout,\n                              env_vars=env_vars)", "output": "Returns invoke configuration to pass to Lambda Runtime to invoke the given function\n\n        :param samcli.commands.local.lib.provider.Function function: Lambda function to generate the configuration for\n        :return samcli.local.lambdafn.config.FunctionConfig: Function configuration to pass to Lambda runtime", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, path, local_path):\n        \"\"\"\n        \n        \"\"\"\n        normpath = os.path.normpath(local_path)\n        folder = os.path.dirname(normpath)\n        if folder and not os.path.exists(folder):\n            os.makedirs(folder)\n\n        tmp_local_path = local_path + '-luigi-tmp-%09d' % random.randrange(0, 1e10)\n\n        # download file\n        self._connect()\n\n        if self.sftp:\n            self._sftp_get(path, tmp_local_path)\n        else:\n            self._ftp_get(path, tmp_local_path)\n\n        self._close()\n\n        os.rename(tmp_local_path, local_path)", "output": "Download file from (s)FTP to local filesystem.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(self, qname):\n        '''\n        \n        '''\n        try:\n            # First if not exists() -> exit\n            if self.conn.queue_exists(qname):\n                return True\n            return False\n        except pyrax.exceptions as err_msg:\n            log.error('RackSpace API got some problems during '\n                      'existing queue check: %s',\n                      err_msg)\n        return False", "output": "Check to see if a Queue exists.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_idx(self, idx, buf):\n        \"\"\"\n        \"\"\"\n        key = self.key_type(idx)\n        pos = self.tell()\n        self.write(buf)\n        self.fidx.write('%s\\t%d\\n'%(str(key), pos))\n        self.idx[key] = pos\n        self.keys.append(key)", "output": "Inserts input record at given index.\n\n        Examples\n        ---------\n        >>> for i in range(5):\n        ...     record.write_idx(i, 'record_%d'%i)\n        >>> record.close()\n\n        Parameters\n        ----------\n        idx : int\n            Index of a file.\n        buf :\n            Record to write.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dump(obj, file, protocol=None):\n    \"\"\"\n    \"\"\"\n    CloudPickler(file, protocol=protocol).dump(obj)", "output": "Serialize obj as bytes streamed into file\n\n    protocol defaults to cloudpickle.DEFAULT_PROTOCOL which is an alias to\n    pickle.HIGHEST_PROTOCOL. This setting favors maximum communication speed\n    between processes running the same Python version.\n\n    Set protocol=pickle.DEFAULT_PROTOCOL instead if you need to ensure\n    compatibility with older versions of Python.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _GetRequestClass(self, method_descriptor):\n    \"\"\"\n    \"\"\"\n    if method_descriptor.containing_service != self.descriptor:\n      raise RuntimeError(\n          'GetRequestClass() given method descriptor for wrong service type.')\n    return method_descriptor.input_type._concrete_class", "output": "Returns the class of the request protocol message.\n\n    Args:\n      method_descriptor: Descriptor of the method for which to return the\n        request protocol message class.\n\n    Returns:\n      A class that represents the input protocol message of the specified\n      method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_multiple_sources_to_consumable_types (self, project, prop_set, sources):\n        \"\"\" \n        \"\"\"\n        if __debug__:\n            from .targets import ProjectTarget\n\n            assert isinstance(project, ProjectTarget)\n            assert isinstance(prop_set, property_set.PropertySet)\n            assert is_iterable_typed(sources, virtual_target.VirtualTarget)\n        if not self.source_types_:\n            return list(sources)\n\n        acceptable_types = set()\n        for t in self.source_types_:\n            acceptable_types.update(type.all_derived(t))\n\n        result = []\n        for source in sources:\n            if source.type() not in acceptable_types:\n                transformed = construct_types(\n                    project, None,self.source_types_, prop_set, [source])\n                # construct_types returns [prop_set, [targets]]\n                for t in transformed[1]:\n                    if t.type() in self.source_types_:\n                        result.append(t)\n                if not transformed:\n                    project.manager().logger().log(__name__, \"  failed to convert \", source)\n            else:\n                result.append(source)\n\n        result = sequence.unique(result, stable=True)\n        return result", "output": "Converts several files to consumable types.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_wordwrap(environment, s, width=79, break_long_words=True,\n                wrapstring=None):\n    \"\"\"\n    \n    \"\"\"\n    if not wrapstring:\n        wrapstring = environment.newline_sequence\n    import textwrap\n    return wrapstring.join(textwrap.wrap(s, width=width, expand_tabs=False,\n                                   replace_whitespace=False,\n                                   break_long_words=break_long_words))", "output": "Return a copy of the string passed to the filter wrapped after\n    ``79`` characters.  You can override this default using the first\n    parameter.  If you set the second parameter to `false` Jinja will not\n    split words apart if they are longer than `width`. By default, the newlines\n    will be the default newlines for the environment, but this can be changed\n    using the wrapstring keyword argument.\n\n    .. versionadded:: 2.7\n       Added support for the `wrapstring` parameter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attack(self, imgs, targets):\n    \"\"\"\n    \n    \"\"\"\n\n    r = []\n    for i in range(0, len(imgs), self.batch_size):\n      _logger.debug(\n          (\"Running CWL2 attack on instance %s of %s\", i, len(imgs)))\n      r.extend(\n          self.attack_batch(imgs[i:i + self.batch_size],\n                            targets[i:i + self.batch_size]))\n    return np.array(r)", "output": "Perform the L_2 attack on the given instance for the given targets.\n\n    If self.targeted is true, then the targets represents the target labels\n    If self.targeted is false, then targets are the original class labels", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_pb(self):\n        \"\"\"\n        \"\"\"\n        condition_kwargs = {\"predicate_filter\": self.base_filter.to_pb()}\n        if self.true_filter is not None:\n            condition_kwargs[\"true_filter\"] = self.true_filter.to_pb()\n        if self.false_filter is not None:\n            condition_kwargs[\"false_filter\"] = self.false_filter.to_pb()\n        condition = data_v2_pb2.RowFilter.Condition(**condition_kwargs)\n        return data_v2_pb2.RowFilter(condition=condition)", "output": "Converts the row filter to a protobuf.\n\n        :rtype: :class:`.data_v2_pb2.RowFilter`\n        :returns: The converted current object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_column(self, column, where=None, start=None, stop=None):\n        \"\"\"\n        \"\"\"\n\n        # validate the version\n        self.validate_version()\n\n        # infer the data kind\n        if not self.infer_axes():\n            return False\n\n        if where is not None:\n            raise TypeError(\"read_column does not currently accept a where \"\n                            \"clause\")\n\n        # find the axes\n        for a in self.axes:\n            if column == a.name:\n\n                if not a.is_data_indexable:\n                    raise ValueError(\n                        \"column [{column}] can not be extracted individually; \"\n                        \"it is not data indexable\".format(column=column))\n\n                # column must be an indexable or a data column\n                c = getattr(self.table.cols, column)\n                a.set_info(self.info)\n                return Series(_set_tz(a.convert(c[start:stop],\n                                                nan_rep=self.nan_rep,\n                                                encoding=self.encoding,\n                                                errors=self.errors\n                                                ).take_data(),\n                                      a.tz, True), name=column)\n\n        raise KeyError(\n            \"column [{column}] not found in the table\".format(column=column))", "output": "return a single column from the table, generally only indexables\n        are interesting", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(self, decoration):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            self._decorations.remove(decoration)\n            self.update()\n            return True\n        except ValueError:\n            return False\n        except RuntimeError:\n            # This is needed to fix issue 9173\n            pass", "output": "Removes a text decoration from the editor.\n\n        :param decoration: Text decoration to remove\n        :type decoration: spyder.api.TextDecoration", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def triangle_wave(frequency):\n  \"\"\"\"\"\"\n  xs = tf.reshape(tf.range(_samples(), dtype=tf.float32), [1, _samples(), 1])\n  ts = xs / FLAGS.sample_rate\n  #\n  # A triangle wave looks like this:\n  #\n  #      /\\      /\\\n  #     /  \\    /  \\\n  #         \\  /    \\  /\n  #          \\/      \\/\n  #\n  # If we look at just half a period (the first four slashes in the\n  # diagram above), we can see that it looks like a transformed absolute\n  # value function.\n  #\n  # Let's start by computing the times relative to the start of each\n  # half-wave pulse (each individual \"mountain\" or \"valley\", of which\n  # there are four in the above diagram).\n  half_pulse_index = ts * (frequency * 2)\n  half_pulse_angle = half_pulse_index % 1.0  # in [0, 1]\n  #\n  # Now, we can see that each positive half-pulse (\"mountain\") has\n  # amplitude given by A(z) = 0.5 - abs(z - 0.5), and then normalized:\n  absolute_amplitude = (0.5 - tf.abs(half_pulse_angle - 0.5)) / 0.5\n  #\n  # But every other half-pulse is negative, so we should invert these.\n  half_pulse_parity = tf.sign(1 - (half_pulse_index % 2.0))\n  amplitude = half_pulse_parity * absolute_amplitude\n  #\n  # This is precisely the desired result, so we're done!\n  return amplitude", "output": "Emit a triangle wave at the given frequency.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_graph(dists, scheme='default'):\n    \"\"\"\n    \"\"\"\n    scheme = get_scheme(scheme)\n    graph = DependencyGraph()\n    provided = {}  # maps names to lists of (version, dist) tuples\n\n    # first, build the graph and find out what's provided\n    for dist in dists:\n        graph.add_distribution(dist)\n\n        for p in dist.provides:\n            name, version = parse_name_and_version(p)\n            logger.debug('Add to provided: %s, %s, %s', name, version, dist)\n            provided.setdefault(name, []).append((version, dist))\n\n    # now make the edges\n    for dist in dists:\n        requires = (dist.run_requires | dist.meta_requires |\n                    dist.build_requires | dist.dev_requires)\n        for req in requires:\n            try:\n                matcher = scheme.matcher(req)\n            except UnsupportedVersionError:\n                # XXX compat-mode if cannot read the version\n                logger.warning('could not read version %r - using name only',\n                               req)\n                name = req.split()[0]\n                matcher = scheme.matcher(name)\n\n            name = matcher.key   # case-insensitive\n\n            matched = False\n            if name in provided:\n                for version, provider in provided[name]:\n                    try:\n                        match = matcher.match(version)\n                    except UnsupportedVersionError:\n                        match = False\n\n                    if match:\n                        graph.add_edge(dist, provider, req)\n                        matched = True\n                        break\n            if not matched:\n                graph.add_missing(dist, req)\n    return graph", "output": "Makes a dependency graph from the given distributions.\n\n    :parameter dists: a list of distributions\n    :type dists: list of :class:`distutils2.database.InstalledDistribution` and\n                 :class:`distutils2.database.EggInfoDistribution` instances\n    :rtype: a :class:`DependencyGraph` instance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_dacl(path, objectType):\n    '''\n    \n    '''\n    try:\n        dacl = win32security.GetNamedSecurityInfo(\n            path, objectType, win32security.DACL_SECURITY_INFORMATION\n            ).GetSecurityDescriptorDacl()\n    except Exception:\n        dacl = None\n    return dacl", "output": "Gets the DACL of a path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close(self):\n        \"\"\"\n        \n        \"\"\"\n        if hasattr(self, \"thread\"):\n            self.thread._exit = True\n            self.thread.join(1000)\n        if self._conn is not None:\n            self._conn.close()", "output": "Close the current connection and terminate the agent\n        Should be called manually", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _expect_extra(expected, present, exc_unexpected, exc_missing, exc_args):\n    \"\"\"\n    \n    \"\"\"\n    if present:\n        if not expected:\n            raise exc_unexpected(*exc_args)\n    elif expected and expected is not Argument.ignore:\n        raise exc_missing(*exc_args)", "output": "Checks for the presence of an extra to the argument list. Raises expections\n    if this is unexpected or if it is missing and expected.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _rand_cpu_str(cpu):\n    '''\n    \n    '''\n    cpu = int(cpu)\n    avail = __salt__['status.nproc']()\n    if cpu < avail:\n        return '0-{0}'.format(avail)\n    to_set = set()\n    while len(to_set) < cpu:\n        choice = random.randint(0, avail - 1)\n        if choice not in to_set:\n            to_set.add(six.text_type(choice))\n    return ','.join(sorted(to_set))", "output": "Return a random subset of cpus for the cpuset config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def print_dict(d, show_missing=True):\n  \"\"\"\n  \"\"\"\n  for k, v in sorted(d.items()):\n    if (not v) and show_missing:\n      # No instances of the key, so print missing symbol.\n      print('{} -'.format(k))\n    elif isinstance(v, list):\n      # Value is a list, so print each item of the list.\n      print(k)\n      for item in v:\n        print('   {}'.format(item))\n    elif isinstance(v, dict):\n      # Value is a dict, so print each (key, value) pair of the dict.\n      print(k)\n      for kk, vv in sorted(v.items()):\n        print('   {:<20} {}'.format(kk, vv))", "output": "Prints a shallow dict to console.\n\n  Args:\n    d: Dict to print.\n    show_missing: Whether to show keys with empty values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def inception_v3(pretrained=False, ctx=cpu(),\n                 root=os.path.join(base.data_dir(), 'models'), **kwargs):\n    \n    \"\"\"\n    net = Inception3(**kwargs)\n    if pretrained:\n        from ..model_store import get_model_file\n        net.load_parameters(get_model_file('inceptionv3', root=root), ctx=ctx)\n    return net", "output": "r\"\"\"Inception v3 model from\n    `\"Rethinking the Inception Architecture for Computer Vision\"\n    <http://arxiv.org/abs/1512.00567>`_ paper.\n\n    Parameters\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    ctx : Context, default CPU\n        The context in which to load the pretrained weights.\n    root : str, default $MXNET_HOME/models\n        Location for keeping the model parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_feather(df, path):\n    \"\"\"\n    \n\n    \"\"\"\n    path = _stringify_path(path)\n    if not isinstance(df, DataFrame):\n        raise ValueError(\"feather only support IO with DataFrames\")\n\n    feather = _try_import()[0]\n    valid_types = {'string', 'unicode'}\n\n    # validate index\n    # --------------\n\n    # validate that we have only a default index\n    # raise on anything else as we don't serialize the index\n\n    if not isinstance(df.index, Int64Index):\n        raise ValueError(\"feather does not support serializing {} \"\n                         \"for the index; you can .reset_index()\"\n                         \"to make the index into column(s)\".format(\n                             type(df.index)))\n\n    if not df.index.equals(RangeIndex.from_range(range(len(df)))):\n        raise ValueError(\"feather does not support serializing a \"\n                         \"non-default index for the index; you \"\n                         \"can .reset_index() to make the index \"\n                         \"into column(s)\")\n\n    if df.index.name is not None:\n        raise ValueError(\"feather does not serialize index meta-data on a \"\n                         \"default index\")\n\n    # validate columns\n    # ----------------\n\n    # must have value column names (strings only)\n    if df.columns.inferred_type not in valid_types:\n        raise ValueError(\"feather must have string column names\")\n\n    feather.write_feather(df, path)", "output": "Write a DataFrame to the feather-format\n\n    Parameters\n    ----------\n    df : DataFrame\n    path : string file path, or file-like object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _handle_input_request(self, msg):\n        \"\"\"\"\"\"\n        if self._hidden:\n            raise RuntimeError('Request for raw input during hidden execution.')\n\n        # Make sure that all output from the SUB channel has been processed\n        # before entering readline mode.\n        self.kernel_client.iopub_channel.flush()\n\n        def callback(line):\n            # Save history to browse it later\n            if not (len(self._control.history) > 0\n                    and self._control.history[-1] == line):\n                # do not save pdb commands\n                cmd = line.split(\" \")[0]\n                if \"do_\" + cmd not in dir(pdb.Pdb):\n                    self._control.history.append(line)\n\n            # This is the Spyder addition: add a %plot magic to display\n            # plots while debugging\n            if line.startswith('%plot '):\n                line = line.split()[-1]\n                code = \"__spy_code__ = get_ipython().run_cell('%s')\" % line\n                self.kernel_client.input(code)\n            else:\n                self.kernel_client.input(line)\n        if self._reading:\n            self._reading = False\n        self._readline(msg['content']['prompt'], callback=callback,\n                       password=msg['content']['password'])", "output": "Save history and add a %plot magic.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def PlaceVOffsetT(self, x):\n        \"\"\"\n        \"\"\"\n        N.enforce_number(x, N.VOffsetTFlags)\n        self.head = self.head - N.VOffsetTFlags.bytewidth\n        encode.Write(packer.voffset, self.Bytes, self.Head(), x)", "output": "PlaceVOffsetT prepends a VOffsetT to the Builder, without checking\n        for space.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clean_params(self):\n        \"\"\"\n        \"\"\"\n        result = self.params.copy()\n        if self.cog is not None:\n            # first parameter is self\n            result.popitem(last=False)\n\n        try:\n            # first/second parameter is context\n            result.popitem(last=False)\n        except Exception:\n            raise ValueError('Missing context parameter') from None\n\n        return result", "output": "Retrieves the parameter OrderedDict without the context or self parameters.\n\n        Useful for inspecting signature.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_state(self, x):\n    \"\"\"\n    \n    \"\"\"\n    optim_state = {}\n    optim_state[\"t\"] = 0.\n    optim_state[\"m\"] = [tf.zeros_like(v) for v in x]\n    optim_state[\"u\"] = [tf.zeros_like(v) for v in x]\n    return optim_state", "output": "Initialize t, m, and u", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def touch(self):\n        \"\"\"\n        \n        \"\"\"\n        self.create_marker_index()\n        self.es.index(index=self.marker_index, doc_type=self.marker_doc_type,\n                      id=self.marker_index_document_id(), body={\n                          'update_id': self.update_id,\n                          'target_index': self.index,\n                          'target_doc_type': self.doc_type,\n                          'date': datetime.datetime.now()})\n        self.es.indices.flush(index=self.marker_index)\n        self.ensure_hist_size()", "output": "Mark this update as complete.\n\n        The document id would be sufficent but,\n        for documentation,\n        we index the parameters `update_id`, `target_index`, `target_doc_type` and `date` as well.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_priority_class(self, name, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_priority_class_with_http_info(name, **kwargs)\n        else:\n            (data) = self.read_priority_class_with_http_info(name, **kwargs)\n            return data", "output": "read the specified PriorityClass\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_priority_class(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PriorityClass (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'. Deprecated. Planned for removal in 1.18.\n        :param bool export: Should this value be exported.  Export strips fields that a user can not specify. Deprecated. Planned for removal in 1.18.\n        :return: V1PriorityClass\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _concat_datetime(to_concat, axis=0, typs=None):\n    \"\"\"\n    \n    \"\"\"\n\n    if typs is None:\n        typs = get_dtype_kinds(to_concat)\n\n    # multiple types, need to coerce to object\n    if len(typs) != 1:\n        return _concatenate_2d([_convert_datetimelike_to_object(x)\n                                for x in to_concat],\n                               axis=axis)\n\n    # must be single dtype\n    if any(typ.startswith('datetime') for typ in typs):\n\n        if 'datetime' in typs:\n            to_concat = [x.astype(np.int64, copy=False) for x in to_concat]\n            return _concatenate_2d(to_concat, axis=axis).view(_NS_DTYPE)\n        else:\n            # when to_concat has different tz, len(typs) > 1.\n            # thus no need to care\n            return _concat_datetimetz(to_concat)\n\n    elif 'timedelta' in typs:\n        return _concatenate_2d([x.view(np.int64) for x in to_concat],\n                               axis=axis).view(_TD_DTYPE)\n\n    elif any(typ.startswith('period') for typ in typs):\n        assert len(typs) == 1\n        cls = to_concat[0]\n        new_values = cls._concat_same_type(to_concat)\n        return new_values", "output": "provide concatenation of an datetimelike array of arrays each of which is a\n    single M8[ns], datetimet64[ns, tz] or m8[ns] dtype\n\n    Parameters\n    ----------\n    to_concat : array of arrays\n    axis : axis to provide concatenation\n    typs : set of to_concat dtypes\n\n    Returns\n    -------\n    a single array, preserving the combined dtypes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _context_string_to_dict(context):\n    '''\n    \n    '''\n    if not re.match('[^:]+:[^:]+:[^:]+:[^:]+$', context):\n        raise SaltInvocationError('Invalid SELinux context string: {0}. ' +\n                                  'Expected \"sel_user:sel_role:sel_type:sel_level\"')\n    context_list = context.split(':', 3)\n    ret = {}\n    for index, value in enumerate(['sel_user', 'sel_role', 'sel_type', 'sel_level']):\n        ret[value] = context_list[index]\n    return ret", "output": ".. versionadded:: 2017.7.0\n\n    Converts an SELinux file context from string to dict.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def open(self):\n        \"\"\"\"\"\"\n        if self.flag == \"w\":\n            check_call(_LIB.MXRecordIOWriterCreate(self.uri, ctypes.byref(self.handle)))\n            self.writable = True\n        elif self.flag == \"r\":\n            check_call(_LIB.MXRecordIOReaderCreate(self.uri, ctypes.byref(self.handle)))\n            self.writable = False\n        else:\n            raise ValueError(\"Invalid flag %s\"%self.flag)\n        self.pid = current_process().pid\n        self.is_open = True", "output": "Opens the record file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sanitize_for_serialization(self, obj):\n        \"\"\"\n        \n        \"\"\"\n        if obj is None:\n            return None\n        elif isinstance(obj, self.PRIMITIVE_TYPES):\n            return obj\n        elif isinstance(obj, list):\n            return [self.sanitize_for_serialization(sub_obj)\n                    for sub_obj in obj]\n        elif isinstance(obj, tuple):\n            return tuple(self.sanitize_for_serialization(sub_obj)\n                         for sub_obj in obj)\n        elif isinstance(obj, (datetime, date)):\n            return obj.isoformat()\n\n        if isinstance(obj, dict):\n            obj_dict = obj\n        else:\n            # Convert model obj to dict except\n            # attributes `swagger_types`, `attribute_map`\n            # and attributes which value is not None.\n            # Convert attribute name to json key in\n            # model definition for request.\n            obj_dict = {obj.attribute_map[attr]: getattr(obj, attr)\n                        for attr, _ in iteritems(obj.swagger_types)\n                        if getattr(obj, attr) is not None}\n\n        return {key: self.sanitize_for_serialization(val)\n                for key, val in iteritems(obj_dict)}", "output": "Builds a JSON POST object.\n\n        If obj is None, return None.\n        If obj is str, int, long, float, bool, return directly.\n        If obj is datetime.datetime, datetime.date\n            convert to string in iso8601 format.\n        If obj is list, sanitize each element in the list.\n        If obj is dict, return the dict.\n        If obj is swagger model, return the properties dict.\n\n        :param obj: The data to serialize.\n        :return: The serialized form of data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cumsum(self, axis=0, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        nv.validate_cumsum(args, kwargs)\n        # Validate axis\n        if axis is not None:\n            self._get_axis_number(axis)\n\n        new_array = self.values.cumsum()\n\n        return self._constructor(\n            new_array, index=self.index,\n            sparse_index=new_array.sp_index).__finalize__(self)", "output": "Cumulative sum of non-NA/null values.\n\n        When performing the cumulative summation, any non-NA/null values will\n        be skipped. The resulting SparseSeries will preserve the locations of\n        NaN values, but the fill value will be `np.nan` regardless.\n\n        Parameters\n        ----------\n        axis : {0}\n\n        Returns\n        -------\n        cumsum : SparseSeries", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def author(self):\n        \"\"\"\n            \n        \"\"\"\n        author = namedtuple('Author', 'name email')\n        return author(name=self._package['author'],\n                      email=self._package['author_email'])", "output": ">>> package = yarg.get('yarg')\n            >>> package.author\n            Author(name=u'Kura', email=u'kura@kura.io')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def num_discarded(self):\n    \"\"\"\"\"\"\n    if not self._data:\n      return 0\n    n = 0\n    while n < len(self._data):\n      if not isinstance(self._data[n], _TensorValueDiscarded):\n        break\n      n += 1\n    return n", "output": "Get the number of values discarded due to exceeding both limits.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_conn(self, timeout=None):\n        \"\"\"\n        \n        \"\"\"\n        conn = None\n        try:\n            conn = self.pool.get(block=self.block, timeout=timeout)\n\n        except AttributeError:  # self.pool is None\n            raise ClosedPoolError(self, \"Pool is closed.\")\n\n        except queue.Empty:\n            if self.block:\n                raise EmptyPoolError(self,\n                                     \"Pool reached maximum size and no more \"\n                                     \"connections are allowed.\")\n            pass  # Oh well, we'll create a new connection then\n\n        # If this is a persistent connection, check if it got disconnected\n        if conn and is_connection_dropped(conn):\n            log.debug(\"Resetting dropped connection: %s\", self.host)\n            conn.close()\n            if getattr(conn, 'auto_open', 1) == 0:\n                # This is a proxied connection that has been mutated by\n                # httplib._tunnel() and cannot be reused (since it would\n                # attempt to bypass the proxy)\n                conn = None\n\n        return conn or self._new_conn()", "output": "Get a connection. Will return a pooled connection if one is available.\n\n        If no connections are available and :prop:`.block` is ``False``, then a\n        fresh connection is returned.\n\n        :param timeout:\n            Seconds to wait before giving up and raising\n            :class:`urllib3.exceptions.EmptyPoolError` if the pool is empty and\n            :prop:`.block` is ``True``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def restart_kernel(self):\r\n        \"\"\"\"\"\"\r\n        client = self.get_current_client()\r\n        if client is not None:\r\n            self.switch_to_plugin()\r\n            client.restart_kernel()", "output": "Restart kernel of current client.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def redact_netloc(netloc):\n    # type: (str) -> str\n    \"\"\"\n    \n    \"\"\"\n    netloc, (user, password) = split_auth_from_netloc(netloc)\n    if user is None:\n        return netloc\n    password = '' if password is None else ':****'\n    return '{user}{password}@{netloc}'.format(user=urllib_parse.quote(user),\n                                              password=password,\n                                              netloc=netloc)", "output": "Replace the password in a netloc with \"****\", if it exists.\n\n    For example, \"user:pass@example.com\" returns \"user:****@example.com\".", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode_step(self, step_input, states):\n        \"\"\"\n        \"\"\"\n        step_output, states, step_additional_outputs =\\\n            self.decoder(self.tgt_embed(step_input), states)\n        step_output = self.tgt_proj(step_output)\n        return step_output, states, step_additional_outputs", "output": "One step decoding of the translation model.\n\n        Parameters\n        ----------\n        step_input : NDArray\n            Shape (batch_size,)\n        states : list of NDArrays\n\n        Returns\n        -------\n        step_output : NDArray\n            Shape (batch_size, C_out)\n        states : list\n        step_additional_outputs : list\n            Additional outputs of the step, e.g, the attention weights", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _start(self):\n        \"\"\"\"\"\"\n        if not self._fired:\n            self._partial_ouput = None\n            self._process.start(self._cmd_list[0], self._cmd_list[1:])\n            self._timer.start()", "output": "Start process.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy(self, source_path, destination_path, threads=DEFAULT_THREADS, start_time=None, end_time=None,\n             part_size=DEFAULT_PART_SIZE, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n\n        # don't allow threads to be less than 3\n        threads = 3 if threads < 3 else threads\n\n        if self.isdir(source_path):\n            return self._copy_dir(source_path, destination_path, threads=threads,\n                                  start_time=start_time, end_time=end_time, part_size=part_size, **kwargs)\n\n        # If the file isn't a directory just perform a simple copy\n        else:\n            return self._copy_file(source_path, destination_path, threads=threads, part_size=part_size, **kwargs)", "output": "Copy object(s) from one S3 location to another. Works for individual keys or entire directories.\n        When files are larger than `part_size`, multipart uploading will be used.\n        :param source_path: The `s3://` path of the directory or key to copy from\n        :param destination_path: The `s3://` path of the directory or key to copy to\n        :param threads: Optional argument to define the number of threads to use when copying (min: 3 threads)\n        :param start_time: Optional argument to copy files with modified dates after start_time\n        :param end_time: Optional argument to copy files with modified dates before end_time\n        :param part_size: Part size in bytes\n        :param kwargs: Keyword arguments are passed to the boto function `copy` as ExtraArgs\n        :returns tuple (number_of_files_copied, total_size_copied_in_bytes)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def disassembler(co, lasti= -1):\n    \"\"\"\n    \n    \"\"\"\n\n    code = co.co_code\n    labels = dis.findlabels(code)\n    linestarts = dict(dis.findlinestarts(co))\n    i = 0\n    extended_arg = 0\n    lineno = 0\n    free = None\n    for i, op, oparg in _walk_ops(co):\n        if i in linestarts:\n            lineno = linestarts[i]\n        instr = Instruction(i=i, op=op, lineno=lineno)\n        instr.linestart = i in linestarts\n\n        if i == lasti:\n            instr.lasti = True\n        else:\n            instr.lasti = False\n\n        if i in labels:\n            instr.label = True\n        else:\n            instr.label = False\n\n        instr.oparg = oparg\n        extended_arg = 0\n        if op == dis.EXTENDED_ARG:\n            extended_arg = oparg * 65536\n        instr.extended_arg = extended_arg\n        if op >= dis.HAVE_ARGUMENT:\n            if op in dis.hasconst:\n                instr.arg = co.co_consts[oparg]\n            elif op in dis.hasname:\n                instr.arg = co.co_names[oparg]\n            elif op in dis.hasjrel:\n                instr.arg = i + oparg\n            elif op in dis.haslocal:\n                instr.arg = co.co_varnames[oparg]\n            elif op in dis.hascompare:\n                instr.arg = dis.cmp_op[oparg]\n            elif op in dis.hasfree:\n                if free is None:\n                    free = co.co_cellvars + co.co_freevars\n                instr.arg = free[oparg]\n\n        yield instr", "output": "Disassemble a code object. \n    \n    :param co: code object\n    :param lasti: internal\n    :yields: Instructions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(self):\n        \"\"\"\n        \"\"\"\n        with self._operational_lock:\n            if self.is_alive:\n                return\n\n            self._thread = threading.Thread(\n                target=self._thread_main, name=_WORKER_THREAD_NAME\n            )\n            self._thread.daemon = True\n            self._thread.start()\n            atexit.register(self._main_thread_terminated)", "output": "Starts the background thread.\n\n        Additionally, this registers a handler for process exit to attempt\n        to send any pending log entries before shutdown.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _consolidate(blocks):\n    \"\"\"\n    \n    \"\"\"\n\n    # sort by _can_consolidate, dtype\n    gkey = lambda x: x._consolidate_key\n    grouper = itertools.groupby(sorted(blocks, key=gkey), gkey)\n\n    new_blocks = []\n    for (_can_consolidate, dtype), group_blocks in grouper:\n        merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n                                      _can_consolidate=_can_consolidate)\n        new_blocks = _extend_blocks(merged_blocks, new_blocks)\n    return new_blocks", "output": "Merge blocks having same dtype, exclude non-consolidating blocks", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sys_version(version_tuple):\n    \"\"\"\n    \n    \"\"\"\n\n    old_version = sys.version_info\n    sys.version_info = version_tuple\n    yield\n    sys.version_info = old_version", "output": "Set a temporary sys.version_info tuple\n\n    :param version_tuple: a fake sys.version_info tuple", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_for_driver_task(self):\n        \"\"\"\n        \"\"\"\n        return all(\n            len(x) == 0\n            for x in [self.module_name, self.class_name, self.function_name])", "output": "See whether this function descriptor is for a driver or not.\n\n        Returns:\n            True if this function descriptor is for driver tasks.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, resource, client):\n        \"\"\"\n        \"\"\"\n        job_ref_properties = resource.get(\"jobReference\", {\"projectId\": client.project})\n        job_ref = _JobReference._from_api_repr(job_ref_properties)\n        job = cls(job_ref, client)\n        # Populate the job reference with the project, even if it has been\n        # redacted, because we know it should equal that of the request.\n        resource[\"jobReference\"] = job_ref_properties\n        job._properties = resource\n        return job", "output": "Construct an UnknownJob from the JSON representation.\n\n        Args:\n            resource (dict): JSON representation of a job.\n            client (google.cloud.bigquery.client.Client):\n                Client connected to BigQuery API.\n\n        Returns:\n            UnknownJob: Job corresponding to the resource.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_batch_begin(self, last_input, last_target, **kwargs):\n        \"\"\n        self.acc_samples += last_input.shape[0]\n        self.acc_batches += 1", "output": "accumulate samples and batches", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_application_tags(self):\n        \"\"\"\n        \"\"\"\n        application_tags = {}\n        if isinstance(self.Location, dict):\n            if (self.APPLICATION_ID_KEY in self.Location.keys() and\n                    self.Location[self.APPLICATION_ID_KEY] is not None):\n                application_tags[self._SAR_APP_KEY] = self.Location[self.APPLICATION_ID_KEY]\n            if (self.SEMANTIC_VERSION_KEY in self.Location.keys() and\n                    self.Location[self.SEMANTIC_VERSION_KEY] is not None):\n                application_tags[self._SAR_SEMVER_KEY] = self.Location[self.SEMANTIC_VERSION_KEY]\n        return application_tags", "output": "Adds tags to the stack if this resource is using the serverless app repo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_logger(self, logger):\n        \"\"\"\n        \n        \"\"\"\n        handler = CommandHandler(self)\n        handler.setFormatter(CommandFormatter())\n        logger.handlers = [handler]\n        logger.propagate = False\n\n        output = self.output\n        level = logging.WARNING\n        if output.is_debug():\n            level = logging.DEBUG\n        elif output.is_very_verbose() or output.is_verbose():\n            level = logging.INFO\n\n        logger.setLevel(level)", "output": "Register a new logger.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def teardown(cluster_config_file, yes, workers_only, cluster_name):\n    \"\"\"\"\"\"\n    teardown_cluster(cluster_config_file, yes, workers_only, cluster_name)", "output": "Tear down the Ray cluster.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dispatch(self, input_batch: List[SingleQuery]):\n        \"\"\"\"\"\"\n        method = getattr(self, self.serve_method)\n        if hasattr(method, \"ray_serve_batched_input\"):\n            batch = [inp.data for inp in input_batch]\n            result = _execute_and_seal_error(method, batch, self.serve_method)\n            for res, inp in zip(result, input_batch):\n                ray.worker.global_worker.put_object(inp.result_object_id, res)\n        else:\n            for inp in input_batch:\n                result = _execute_and_seal_error(method, inp.data,\n                                                 self.serve_method)\n                ray.worker.global_worker.put_object(inp.result_object_id,\n                                                    result)", "output": "Helper method to dispatch a batch of input to self.serve_method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_extensions(default, extensions, strict, environ, reload=False):\n    \"\"\"\n    \"\"\"\n    if default:\n        default_extension_path = pth.default_extension(environ=environ)\n        pth.ensure_file(default_extension_path)\n        # put the default extension first so other extensions can depend on\n        # the order they are loaded\n        extensions = concatv([default_extension_path], extensions)\n\n    for ext in extensions:\n        if ext in _loaded_extensions and not reload:\n            continue\n        try:\n            # load all of the zipline extensionss\n            if ext.endswith('.py'):\n                with open(ext) as f:\n                    ns = {}\n                    six.exec_(compile(f.read(), ext, 'exec'), ns, ns)\n            else:\n                __import__(ext)\n        except Exception as e:\n            if strict:\n                # if `strict` we should raise the actual exception and fail\n                raise\n            # without `strict` we should just log the failure\n            warnings.warn(\n                'Failed to load extension: %r\\n%s' % (ext, e),\n                stacklevel=2\n            )\n        else:\n            _loaded_extensions.add(ext)", "output": "Load all of the given extensions. This should be called by run_algo\n    or the cli.\n\n    Parameters\n    ----------\n    default : bool\n        Load the default exension (~/.zipline/extension.py)?\n    extension : iterable[str]\n        The paths to the extensions to load. If the path ends in ``.py`` it is\n        treated as a script and executed. If it does not end in ``.py`` it is\n        treated as a module to be imported.\n    strict : bool\n        Should failure to load an extension raise. If this is false it will\n        still warn.\n    environ : mapping\n        The environment to use to find the default extension path.\n    reload : bool, optional\n        Reload any extensions that have already been loaded.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_quota(mount, opts):\n    '''\n    \n    '''\n    cmd = 'repquota -vp {0} {1}'.format(opts, mount)\n    out = __salt__['cmd.run'](cmd, python_shell=False).splitlines()\n    mode = 'header'\n\n    if '-u' in opts:\n        quotatype = 'Users'\n    elif '-g' in opts:\n        quotatype = 'Groups'\n    ret = {quotatype: {}}\n\n    for line in out:\n        if not line:\n            continue\n        comps = line.split()\n        if mode == 'header':\n            if 'Block grace time' in line:\n                blockg, inodeg = line.split(';')\n                blockgc = blockg.split(': ')\n                inodegc = inodeg.split(': ')\n                ret['Block Grace Time'] = blockgc[-1:]\n                ret['Inode Grace Time'] = inodegc[-1:]\n            elif line.startswith('-'):\n                mode = 'quotas'\n        elif mode == 'quotas':\n            if len(comps) < 8:\n                continue\n            if not comps[0] in ret[quotatype]:\n                ret[quotatype][comps[0]] = {}\n            ret[quotatype][comps[0]]['block-used'] = comps[2]\n            ret[quotatype][comps[0]]['block-soft-limit'] = comps[3]\n            ret[quotatype][comps[0]]['block-hard-limit'] = comps[4]\n            ret[quotatype][comps[0]]['block-grace'] = comps[5]\n            ret[quotatype][comps[0]]['file-used'] = comps[6]\n            ret[quotatype][comps[0]]['file-soft-limit'] = comps[7]\n            ret[quotatype][comps[0]]['file-hard-limit'] = comps[8]\n            ret[quotatype][comps[0]]['file-grace'] = comps[9]\n    return ret", "output": "Parse the output from repquota. Requires that -u -g are passed in", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zoom_cv(x,z):\n    \"\"\"  \"\"\"\n    if z==0: return x\n    r,c,*_ = x.shape\n    M = cv2.getRotationMatrix2D((c/2,r/2),0,z+1.)\n    return cv2.warpAffine(x,M,(c,r))", "output": "Zoom the center of image x by a factor of z+1 while retaining the original image size and proportion.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_style_from_font(font):\n    \"\"\"\n    \n    \"\"\"\n\n    if font is None:\n        return ''\n\n    if type(font) is str:\n        _font = font.split(' ')\n    else:\n        _font = font\n\n    style = ''\n    style += 'font-family: %s;\\n' % _font[0]\n    style += 'font-size: %spt;\\n' % _font[1]\n    font_items = ''\n    for item in _font[2:]:\n        if item == 'underline':\n            style += 'text-decoration: underline;\\n'\n        else:\n            font_items += item + ' '\n    if font_items != '':\n        style += 'font: %s;\\n' % (font_items)\n    return style", "output": "Convert from font string/tyuple into a Qt style sheet string\n    :param font: \"Arial 10 Bold\" or ('Arial', 10, 'Bold)\n    :return: style string that can be combined with other style strings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filter_roidb(self):\n        \"\"\"\"\"\"\n        num_roidb = len(self._roidb)\n        self._roidb = [roi_rec for roi_rec in self._roidb if len(roi_rec['gt_classes'])]\n        num_after = len(self._roidb)\n        logger.info('filter roidb: {} -> {}'.format(num_roidb, num_after))", "output": "Remove images without usable rois", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rename_pipe(self, old_name, new_name):\n        \"\"\"\n        \"\"\"\n        if old_name not in self.pipe_names:\n            raise ValueError(Errors.E001.format(name=old_name, opts=self.pipe_names))\n        if new_name in self.pipe_names:\n            raise ValueError(Errors.E007.format(name=new_name, opts=self.pipe_names))\n        i = self.pipe_names.index(old_name)\n        self.pipeline[i] = (new_name, self.pipeline[i][1])", "output": "Rename a pipeline component.\n\n        old_name (unicode): Name of the component to rename.\n        new_name (unicode): New name of the component.\n\n        DOCS: https://spacy.io/api/language#rename_pipe", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def versions(runas=None):\n    '''\n    \n    '''\n    ret = _rbenv_exec(['versions', '--bare'], runas=runas)\n    return [] if ret is False else ret.splitlines()", "output": "List the installed versions of ruby\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rbenv.versions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rse(label, pred):\n    \"\"\"\"\"\"\n    numerator = np.sqrt(np.mean(np.square(label - pred), axis = None))\n    denominator = np.std(label, axis = None)\n    return numerator / denominator", "output": "computes the root relative squared error (condensed using standard deviation formula)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_dict2tuple(value):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(value, dict):\n        for _keys in value:\n            value[_keys] = convert_dict2tuple(value[_keys])\n        return tuple(sorted(value.items()))\n    else:\n        return value", "output": "convert dict type to tuple to solve unhashable problem.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cover_time(date):\n    \"\"\"\n    \n    \"\"\"\n    datestr = str(date)[0:8]\n    date = time.mktime(time.strptime(datestr, '%Y%m%d'))\n    return date", "output": "\u5b57\u7b26\u4e32 '20180101'  \u8f6c\u53d8\u6210 float \u7c7b\u578b\u65f6\u95f4 \u7c7b\u4f3c time.time() \u8fd4\u56de\u7684\u7c7b\u578b\n    :param date: \u5b57\u7b26\u4e32str -- \u683c\u5f0f\u5fc5\u987b\u662f 20180101 \uff0c\u957f\u5ea68\n    :return: \u7c7b\u578bfloat", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __pred_for_csc(self, csc, num_iteration, predict_type):\n        \"\"\"\"\"\"\n        nrow = csc.shape[0]\n        if nrow > MAX_INT32:\n            return self.__pred_for_csr(csc.tocsr(), num_iteration, predict_type)\n        n_preds = self.__get_num_preds(num_iteration, nrow, predict_type)\n        preds = np.zeros(n_preds, dtype=np.float64)\n        out_num_preds = ctypes.c_int64(0)\n\n        ptr_indptr, type_ptr_indptr, __ = c_int_array(csc.indptr)\n        ptr_data, type_ptr_data, _ = c_float_array(csc.data)\n\n        assert csc.shape[0] <= MAX_INT32\n        csc.indices = csc.indices.astype(np.int32, copy=False)\n\n        _safe_call(_LIB.LGBM_BoosterPredictForCSC(\n            self.handle,\n            ptr_indptr,\n            ctypes.c_int32(type_ptr_indptr),\n            csc.indices.ctypes.data_as(ctypes.POINTER(ctypes.c_int32)),\n            ptr_data,\n            ctypes.c_int(type_ptr_data),\n            ctypes.c_int64(len(csc.indptr)),\n            ctypes.c_int64(len(csc.data)),\n            ctypes.c_int64(csc.shape[0]),\n            ctypes.c_int(predict_type),\n            ctypes.c_int(num_iteration),\n            c_str(self.pred_parameter),\n            ctypes.byref(out_num_preds),\n            preds.ctypes.data_as(ctypes.POINTER(ctypes.c_double))))\n        if n_preds != out_num_preds.value:\n            raise ValueError(\"Wrong length for predict results\")\n        return preds, nrow", "output": "Predict for a CSC data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def force_close(self) -> None:\n        \"\"\"\"\"\"\n        self._force_close = True\n        if self._waiter:\n            self._waiter.cancel()\n        if self.transport is not None:\n            self.transport.close()\n            self.transport = None", "output": "Force close connection", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_builtin(text):\r\n    \"\"\"\"\"\"\r\n    from spyder.py3compat import builtins\r\n    return text in [str(name) for name in dir(builtins)\r\n                    if not name.startswith('_')]", "output": "Test if passed string is the name of a Python builtin object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def underlying_order_book_id(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            return self.__dict__[\"underlying_order_book_id\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'underlying_order_book_id' \".format(self.order_book_id)\n            )", "output": "[str] \u5408\u7ea6\u6807\u7684\u4ee3\u7801\uff0c\u76ee\u524d\u9664\u80a1\u6307\u671f\u8d27(IH, IF, IC)\u4e4b\u5916\u7684\u671f\u8d27\u5408\u7ea6\uff0c\u8fd9\u4e00\u5b57\u6bb5\u5168\u90e8\u4e3a\u2019null\u2019\uff08\u671f\u8d27\u4e13\u7528\uff09", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_trial_add(self, trial_runner, trial):\n        \"\"\"\"\"\"\n\n        cur_bracket = self._state[\"bracket\"]\n        cur_band = self._hyperbands[self._state[\"band_idx\"]]\n        if cur_bracket is None or cur_bracket.filled():\n            retry = True\n            while retry:\n                # if current iteration is filled, create new iteration\n                if self._cur_band_filled():\n                    cur_band = []\n                    self._hyperbands.append(cur_band)\n                    self._state[\"band_idx\"] += 1\n\n                # cur_band will always be less than s_max_1 or else filled\n                s = len(cur_band)\n                assert s < self._s_max_1, \"Current band is filled!\"\n                if self._get_r0(s) == 0:\n                    logger.info(\"Bracket too small - Retrying...\")\n                    cur_bracket = None\n                else:\n                    retry = False\n                    cur_bracket = Bracket(self._time_attr, self._get_n0(s),\n                                          self._get_r0(s), self._max_t_attr,\n                                          self._eta, s)\n                cur_band.append(cur_bracket)\n                self._state[\"bracket\"] = cur_bracket\n\n        self._state[\"bracket\"].add_trial(trial)\n        self._trial_info[trial] = cur_bracket, self._state[\"band_idx\"]", "output": "Adds new trial.\n\n        On a new trial add, if current bracket is not filled,\n        add to current bracket. Else, if current band is not filled,\n        create new bracket, add to current bracket.\n        Else, create new iteration, create new bracket, add to bracket.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_container_service(container):\n    '''\n    \n    '''\n    if 'account_key' in container:\n        account = azure.storage.CloudStorageAccount(container['account_name'], account_key=container['account_key'])\n    elif 'sas_token' in container:\n        account = azure.storage.CloudStorageAccount(container['account_name'], sas_token=container['sas_token'])\n    else:\n        account = azure.storage.CloudStorageAccount(container['account_name'])\n    blob_service = account.create_block_blob_service()\n    return blob_service", "output": "Get the azure block blob service for the container in question\n\n    Try account_key, sas_token, and no auth in that order", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def histograms_route(self, request):\n    \"\"\"\"\"\"\n    tag = request.args.get('tag')\n    run = request.args.get('run')\n    try:\n      (body, mime_type) = self.histograms_impl(\n          tag, run, downsample_to=self.SAMPLE_SIZE)\n      code = 200\n    except ValueError as e:\n      (body, mime_type) = (str(e), 'text/plain')\n      code = 400\n    return http_util.Respond(request, body, mime_type, code=code)", "output": "Given a tag and single run, return array of histogram values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_params(self):\n    \"\"\"\n    \n    \"\"\"\n\n    if self.needs_dummy_fprop:\n      if hasattr(self, \"_dummy_input\"):\n        return\n      self._dummy_input = self.make_input_placeholder()\n      self.fprop(self._dummy_input)", "output": "Create all Variables to be returned later by get_params.\n    By default this is a no-op.\n    Models that need their fprop to be called for their params to be\n    created can set `needs_dummy_fprop=True` in the constructor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pods(namespace='default', **kwargs):\n    '''\n    \n    '''\n    cfg = _setup_conn(**kwargs)\n    try:\n        api_instance = kubernetes.client.CoreV1Api()\n        api_response = api_instance.list_namespaced_pod(namespace)\n\n        return [pod['metadata']['name'] for pod in api_response.to_dict().get('items')]\n    except (ApiException, HTTPError) as exc:\n        if isinstance(exc, ApiException) and exc.status == 404:\n            return None\n        else:\n            log.exception(\n                'Exception when calling '\n                'CoreV1Api->list_namespaced_pod'\n            )\n            raise CommandExecutionError(exc)\n    finally:\n        _cleanup(**cfg)", "output": "Return a list of kubernetes pods defined in the namespace\n\n    CLI Examples::\n\n        salt '*' kubernetes.pods\n        salt '*' kubernetes.pods namespace=default", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def evolved_transformer_big_tpu():\n  \"\"\"\"\"\"\n  hparams = add_evolved_transformer_hparams(transformer.transformer_big_tpu())\n  hparams.learning_rate_constant = 1 / hparams.learning_rate_warmup_steps ** 0.5\n  hparams.learning_rate_schedule = (\n      \"constant*single_cycle_cos_decay\")\n  return hparams", "output": "Big parameters for Evolved Transformer model on TPU.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop_trial(self, trial):\n        \"\"\"\n        \"\"\"\n        error = False\n        error_msg = None\n\n        if trial.status in [Trial.ERROR, Trial.TERMINATED]:\n            return\n        elif trial.status in [Trial.PENDING, Trial.PAUSED]:\n            self._scheduler_alg.on_trial_remove(self, trial)\n            self._search_alg.on_trial_complete(\n                trial.trial_id, early_terminated=True)\n        elif trial.status is Trial.RUNNING:\n            try:\n                result = self.trial_executor.fetch_result(trial)\n                trial.update_last_result(result, terminate=True)\n                self._scheduler_alg.on_trial_complete(self, trial, result)\n                self._search_alg.on_trial_complete(\n                    trial.trial_id, result=result)\n            except Exception:\n                error_msg = traceback.format_exc()\n                logger.exception(\"Error processing event.\")\n                self._scheduler_alg.on_trial_error(self, trial)\n                self._search_alg.on_trial_complete(trial.trial_id, error=True)\n                error = True\n\n        self.trial_executor.stop_trial(trial, error=error, error_msg=error_msg)", "output": "Stops trial.\n\n        Trials may be stopped at any time. If trial is in state PENDING\n        or PAUSED, calls `on_trial_remove`  for scheduler and\n        `on_trial_complete(..., early_terminated=True) for search_alg.\n        Otherwise waits for result for the trial and calls\n        `on_trial_complete` for scheduler and search_alg if RUNNING.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rollback(self, project_id, transaction):\n        \"\"\"\n        \"\"\"\n        request_pb = _datastore_pb2.RollbackRequest(\n            project_id=project_id, transaction=transaction\n        )\n        # Response is empty (i.e. no fields) but we return it anyway.\n        return _rpc(\n            self.client._http,\n            project_id,\n            \"rollback\",\n            self.client._base_url,\n            request_pb,\n            _datastore_pb2.RollbackResponse,\n        )", "output": "Perform a ``rollback`` request.\n\n        :type project_id: str\n        :param project_id: The project to connect to. This is\n                           usually your project name in the cloud console.\n\n        :type transaction: bytes\n        :param transaction: The transaction ID to rollback.\n\n        :rtype: :class:`.datastore_pb2.RollbackResponse`\n        :returns: The returned protobuf response object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def query(self, expr, **kwargs):\n        \"\"\"\n        \"\"\"\n        columns = self.columns\n\n        def query_builder(df, **kwargs):\n            # This is required because of an Arrow limitation\n            # TODO revisit for Arrow error\n            df = df.copy()\n            df.index = pandas.RangeIndex(len(df))\n            df.columns = columns\n            df.query(expr, inplace=True, **kwargs)\n            df.columns = pandas.RangeIndex(len(df.columns))\n            return df\n\n        func = self._prepare_method(query_builder, **kwargs)\n        new_data = self._map_across_full_axis(1, func)\n        # Query removes rows, so we need to update the index\n        new_index = self.compute_index(0, new_data, True)\n\n        return self.__constructor__(new_data, new_index, self.columns, self.dtypes)", "output": "Query columns of the DataManager with a boolean expression.\n\n        Args:\n            expr: Boolean expression to query the columns with.\n\n        Returns:\n            DataManager containing the rows where the boolean expression is satisfied.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def route_tables_list(resource_group, **kwargs):\n    '''\n    \n\n    '''\n    result = {}\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        tables = __utils__['azurearm.paged_object_to_list'](\n            netconn.route_tables.list(\n                resource_group_name=resource_group\n            )\n        )\n\n        for table in tables:\n            result[table['name']] = table\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    List all route tables within a resource group.\n\n    :param resource_group: The resource group name to list route\n        tables within.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.route_tables_list testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def master_config(opts, vm_):\n    '''\n    \n    '''\n    # Let's get a copy of the salt master default options\n    master = copy.deepcopy(salt.config.DEFAULT_MASTER_OPTS)\n    # Some default options are Null, let's set a reasonable default\n    master.update(\n        log_level='info',\n        log_level_logfile='info',\n        hash_type='sha256'\n    )\n\n    # Get ANY defined master setting, merging data, in the following order\n    # 1. VM config\n    # 2. Profile config\n    # 3. Global configuration\n    master.update(\n        salt.config.get_cloud_config_value(\n            'master', vm_, opts, default={}, search_global=True\n        )\n    )\n    return master", "output": "Return a master's configuration for the provided options and VM", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_vqa_v2_image_raw_dataset(directory, image_root_url, image_urls):\n  \"\"\"\"\"\"\n  for url in image_urls:\n    filename = os.path.basename(url)\n    download_url = os.path.join(image_root_url, url)\n    path = generator_utils.maybe_download(directory, filename, download_url)\n    unzip_dir = os.path.join(directory, filename.strip(\".zip\"))\n    if not tf.gfile.Exists(unzip_dir):\n      zipfile.ZipFile(path, \"r\").extractall(directory)", "output": "Extract the VQA V2 image data set to directory unless it's there.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def formatter(self):\n        \"\"\"\n        \n        \"\"\"\n        formatter_chain = [\n            LambdaLogMsgFormatters.colorize_errors,\n\n            # Format JSON \"before\" highlighting the keywords. Otherwise, JSON will be invalid from all the\n            # ANSI color codes and fail to pretty print\n            JSONMsgFormatter.format_json,\n\n            KeywordHighlighter(self._filter_pattern).highlight_keywords,\n        ]\n\n        return LogsFormatter(self.colored, formatter_chain)", "output": "Creates and returns a Formatter capable of nicely formatting Lambda function logs\n\n        Returns\n        -------\n        LogsFormatter", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def provides_distribution(self, name, version=None):\n        \"\"\"\n        \n        \"\"\"\n        matcher = None\n        if version is not None:\n            try:\n                matcher = self._scheme.matcher('%s (%s)' % (name, version))\n            except ValueError:\n                raise DistlibException('invalid name or version: %r, %r' %\n                                      (name, version))\n\n        for dist in self.get_distributions():\n            # We hit a problem on Travis where enum34 was installed and doesn't\n            # have a provides attribute ...\n            if not hasattr(dist, 'provides'):\n                logger.debug('No \"provides\": %s', dist)\n            else:\n                provided = dist.provides\n\n                for p in provided:\n                    p_name, p_ver = parse_name_and_version(p)\n                    if matcher is None:\n                        if p_name == name:\n                            yield dist\n                            break\n                    else:\n                        if p_name == name and matcher.match(p_ver):\n                            yield dist\n                            break", "output": "Iterates over all distributions to find which distributions provide *name*.\n        If a *version* is provided, it will be used to filter the results.\n\n        This function only returns the first result found, since no more than\n        one values are expected. If the directory is not found, returns ``None``.\n\n        :parameter version: a version specifier that indicates the version\n                            required, conforming to the format in ``PEP-345``\n\n        :type name: string\n        :type version: string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cache_key(self, **extra):\n        \"\"\"\n        \n        \"\"\"\n        cache_dict = self.to_dict()\n        cache_dict.update(extra)\n\n        for k in ['from_dttm', 'to_dttm']:\n            del cache_dict[k]\n        if self.time_range:\n            cache_dict['time_range'] = self.time_range\n        json_data = self.json_dumps(cache_dict, sort_keys=True)\n        return hashlib.md5(json_data.encode('utf-8')).hexdigest()", "output": "The cache key is made out of the key/values in `query_obj`, plus any\n        other key/values in `extra`\n        We remove datetime bounds that are hard values, and replace them with\n        the use-provided inputs to bounds, which may be time-relative (as in\n        \"5 days ago\" or \"now\").", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_get(alias=None, userids=None, **kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'user.get'\n            params = {\"output\": \"extend\", \"filter\": {}}\n            if not userids and not alias:\n                return {'result': False, 'comment': 'Please submit alias or userids parameter to retrieve users.'}\n            if alias:\n                params['filter'].setdefault('alias', alias)\n            if userids:\n                params.setdefault('userids', userids)\n            params = _params_extend(params, **kwargs)\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result'] if ret['result'] else False\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": "Retrieve users according to the given parameters.\n\n    .. versionadded:: 2016.3.0\n\n    :param alias: user alias\n    :param userids: return only users with the given IDs\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    :return: Array with details of convenient users, False on failure of if no user found.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.user_get james", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def full_scope(self):\n        \"\"\"\n        \"\"\"\n        maps = [self.temps] + self.resolvers.maps + self.scope.maps\n        return DeepChainMap(*maps)", "output": "Return the full scope for use with passing to engines transparently\n        as a mapping.\n\n        Returns\n        -------\n        vars : DeepChainMap\n            All variables in this scope.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _upstart_is_disabled(name):\n    '''\n    \n    '''\n    files = ['/etc/init/{0}.conf'.format(name), '/etc/init/{0}.override'.format(name)]\n    for file_name in filter(os.path.isfile, files):\n        with salt.utils.files.fopen(file_name) as fp_:\n            if re.search(r'^\\s*manual',\n                         salt.utils.stringutils.to_unicode(fp_.read()),\n                         re.MULTILINE):\n                return True\n    return False", "output": "An Upstart service is assumed disabled if a manual stanza is\n    placed in /etc/init/[name].override.\n    NOTE: An Upstart service can also be disabled by placing \"manual\"\n    in /etc/init/[name].conf.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_identity_pool_ids(name, pool_id, conn):\n    '''\n    \n    '''\n    ids = []\n    if pool_id is None:\n        for pools in __utils__['boto3.paged_call'](conn.list_identity_pools,\n                         marker_flag='NextToken', marker_arg='NextToken', MaxResults=25):\n            for pool in pools['IdentityPools']:\n                if pool['IdentityPoolName'] == name:\n                    ids.append(pool['IdentityPoolId'])\n    else:\n        ids.append(pool_id)\n\n    return ids", "output": "Given identity pool name (or optionally a pool_id and name will be ignored),\n    find and return list of matching identity pool id's.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def atc(jobid):\n    '''\n    \n    '''\n    # Shim to produce output similar to what __virtual__() should do\n    # but __salt__ isn't available in __virtual__()\n    output = _cmd('at', '-c', six.text_type(jobid))\n\n    if output is None:\n        return '\\'at.atc\\' is not available.'\n    elif output == '':\n        return {'error': 'invalid job id \\'{0}\\''.format(jobid)}\n\n    return output", "output": "Print the at(1) script that will run for the passed job\n    id. This is mostly for debugging so the output will\n    just be text.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' at.atc <jobid>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copyProp(self, target):\n        \"\"\" \"\"\"\n        if target is None: target__o = None\n        else: target__o = target._o\n        ret = libxml2mod.xmlCopyProp(target__o, self._o)\n        if ret is None:raise treeError('xmlCopyProp() failed')\n        __tmp = xmlAttr(_obj=ret)\n        return __tmp", "output": "Do a copy of the attribute.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_rewards(self, rewards):\n    \"\"\"\n    \"\"\"\n\n    min_reward, max_reward = self.reward_range\n\n    # Clips at min and max reward.\n    rewards = np.clip(rewards, min_reward, max_reward)\n    # Round to (nearest) int and convert to integral type.\n    rewards = np.around(rewards, decimals=0).astype(np.int64)\n    return rewards", "output": "Clips, rounds, and changes to integer type.\n\n    Args:\n      rewards: numpy array of raw (float) rewards.\n\n    Returns:\n      processed_rewards: numpy array of np.int64", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_event(self, wait=0.25, tag='', full=False):\n        '''\n        \n        '''\n        return self.event.get_event(wait=wait, tag=tag, full=full, auto_reconnect=True)", "output": "Get a single salt event.\n        If no events are available, then block for up to ``wait`` seconds.\n        Return the event if it matches the tag (or ``tag`` is empty)\n        Otherwise return None\n\n        If wait is 0 then block forever or until next event becomes available.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_vcs_root(path):\r\n    \"\"\"\"\"\"\r\n    previous_path = path\r\n    while get_vcs_info(path) is None:\r\n        path = abspardir(path)\r\n        if path == previous_path:\r\n            return\r\n        else:\r\n            previous_path = path\r\n    return osp.abspath(path)", "output": "Return VCS root directory path\r\n    Return None if path is not within a supported VCS repository", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _from_java_impl(cls, java_stage):\n        \"\"\"\n        \n        \"\"\"\n\n        # Load information from java_stage to the instance.\n        estimator = JavaParams._from_java(java_stage.getEstimator())\n        evaluator = JavaParams._from_java(java_stage.getEvaluator())\n        epms = [estimator._transfer_param_map_from_java(epm)\n                for epm in java_stage.getEstimatorParamMaps()]\n        return estimator, epms, evaluator", "output": "Return Python estimator, estimatorParamMaps, and evaluator from a Java ValidatorParams.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def product_path(cls, project, location, product):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/locations/{location}/products/{product}\",\n            project=project,\n            location=location,\n            product=product,\n        )", "output": "Return a fully-qualified product string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_host_advanced(name=None, ipv4addr=None, mac=None, **api_opts):\n    '''\n    \n    '''\n    infoblox = _get_infoblox(**api_opts)\n    host = infoblox.get_host_advanced(name=name, mac=mac, ipv4addr=ipv4addr)\n    return host", "output": "Get all host information\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call infoblox.get_host_advanced hostname.domain.ca", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def step(self)->Number:\n        \"\"\n        self.n += 1\n        return self.func(self.start, self.end, self.n/self.n_iter)", "output": "Return next value along annealed schedule.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def with_metaclass(meta, *bases):\n    \"\"\"\"\"\"\n    # This requires a bit of explanation: the basic idea is to make a dummy\n    # metaclass for one level of class instantiation that replaces itself with\n    # the actual metaclass.\n    class metaclass(type):\n\n        def __new__(cls, name, this_bases, d):\n            return meta(name, bases, d)\n\n        @classmethod\n        def __prepare__(cls, name, this_bases):\n            return meta.__prepare__(name, bases)\n    return type.__new__(metaclass, 'temporary_class', (), {})", "output": "Create a base class with a metaclass.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _call_salt_command(self,\n                           fun,\n                           args,\n                           kwargs,\n                           assertion_section=None):\n        '''\n        \n        '''\n        value = False\n        try:\n            if args and kwargs:\n                value = self.salt_lc.cmd(fun, *args, **kwargs)\n            elif args and not kwargs:\n                value = self.salt_lc.cmd(fun, *args)\n            elif not args and kwargs:\n                value = self.salt_lc.cmd(fun, **kwargs)\n            else:\n                value = self.salt_lc.cmd(fun)\n        except salt.exceptions.SaltException:\n            raise\n        except Exception:\n            raise\n        if isinstance(value, dict) and assertion_section:\n            return value.get(assertion_section, False)\n        else:\n            return value", "output": "Generic call of salt Caller command", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def repeat_last_axis(array, count):\n    \"\"\"\n    \n    \"\"\"\n    return as_strided(array, array.shape + (count,), array.strides + (0,))", "output": "Restride `array` to repeat `count` times along the last axis.\n\n    Parameters\n    ----------\n    array : np.array\n        The array to restride.\n    count : int\n        Number of times to repeat `array`.\n\n    Returns\n    -------\n    result : array\n        Array of shape array.shape + (count,) composed of `array` repeated\n        `count` times along the last axis.\n\n    Example\n    -------\n    >>> from numpy import arange\n    >>> a = arange(3); a\n    array([0, 1, 2])\n    >>> repeat_last_axis(a, 2)\n    array([[0, 0],\n           [1, 1],\n           [2, 2]])\n    >>> repeat_last_axis(a, 4)\n    array([[0, 0, 0, 0],\n           [1, 1, 1, 1],\n           [2, 2, 2, 2]])\n\n    Notes\n    ----\n    The resulting array will share memory with `array`.  If you need to assign\n    to the input or output, you should probably make a copy first.\n\n    See Also\n    --------\n    repeat_last_axis", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attend(x, source, hparams, name):\n  \"\"\"\"\"\"\n  with tf.variable_scope(name):\n    x = tf.squeeze(x, axis=2)\n    if len(source.get_shape()) > 3:\n      source = tf.squeeze(source, axis=2)\n    source = common_attention.add_timing_signal_1d(source)\n    y = common_attention.multihead_attention(\n        common_layers.layer_preprocess(x, hparams), source, None,\n        hparams.attention_key_channels or hparams.hidden_size,\n        hparams.attention_value_channels or hparams.hidden_size,\n        hparams.hidden_size, hparams.num_heads,\n        hparams.attention_dropout)\n    res = common_layers.layer_postprocess(x, y, hparams)\n    return tf.expand_dims(res, axis=2)", "output": "Self-attention layer with source as memory antecedent.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_default_download_dir(self, *subdirs):\n        \"\"\" \n        \"\"\"\n        # Look up value for key \"path\" in the config\n        path = self.get_config_value(self.CONFIG_NAME_PATH)\n\n        # If not set in config, default to present working directory\n        if path is None:\n            return os.getcwd()\n\n        return os.path.join(path, *subdirs)", "output": "Get the download path for a file. If not defined, return default\n            from config.\n\n            Parameters\n            ==========\n            subdirs: a single (or list of) subfolders under the basepath", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def configure(\n        cls, impl: \"Union[None, str, Type[Configurable]]\", **kwargs: Any\n    ) -> None:\n        \"\"\"\n        \"\"\"\n        super(AsyncHTTPClient, cls).configure(impl, **kwargs)", "output": "Configures the `AsyncHTTPClient` subclass to use.\n\n        ``AsyncHTTPClient()`` actually creates an instance of a subclass.\n        This method may be called with either a class object or the\n        fully-qualified name of such a class (or ``None`` to use the default,\n        ``SimpleAsyncHTTPClient``)\n\n        If additional keyword arguments are given, they will be passed\n        to the constructor of each subclass instance created.  The\n        keyword argument ``max_clients`` determines the maximum number\n        of simultaneous `~AsyncHTTPClient.fetch()` operations that can\n        execute in parallel on each `.IOLoop`.  Additional arguments\n        may be supported depending on the implementation class in use.\n\n        Example::\n\n           AsyncHTTPClient.configure(\"tornado.curl_httpclient.CurlAsyncHTTPClient\")", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ret_glob_minions(self):\n        '''\n        \n        '''\n        minions = {}\n        for minion in self.raw:\n            if fnmatch.fnmatch(minion, self.tgt):\n                data = self.get_data(minion)\n                if data:\n                    minions[minion] = data\n        return minions", "output": "Return minions that match via glob", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def type_cast_value(self, ctx, value):\n        \"\"\"\n        \"\"\"\n        if self.type.is_composite:\n            if self.nargs <= 1:\n                raise TypeError('Attempted to invoke composite type '\n                                'but nargs has been set to %s.  This is '\n                                'not supported; nargs needs to be set to '\n                                'a fixed value > 1.' % self.nargs)\n            if self.multiple:\n                return tuple(self.type(x or (), self, ctx) for x in value or ())\n            return self.type(value or (), self, ctx)\n\n        def _convert(value, level):\n            if level == 0:\n                return self.type(value, self, ctx)\n            return tuple(_convert(x, level - 1) for x in value or ())\n        return _convert(value, (self.nargs != 1) + bool(self.multiple))", "output": "Given a value this runs it properly through the type system.\n        This automatically handles things like `nargs` and `multiple` as\n        well as composite types.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_installed_files(self):\n        \"\"\"\n        \n        \"\"\"\n\n        def _md5(path):\n            f = open(path, 'rb')\n            try:\n                content = f.read()\n            finally:\n                f.close()\n            return hashlib.md5(content).hexdigest()\n\n        def _size(path):\n            return os.stat(path).st_size\n\n        record_path = os.path.join(self.path, 'installed-files.txt')\n        result = []\n        if os.path.exists(record_path):\n            with codecs.open(record_path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    p = os.path.normpath(os.path.join(self.path, line))\n                    # \"./\" is present as a marker between installed files\n                    # and installation metadata files\n                    if not os.path.exists(p):\n                        logger.warning('Non-existent file: %s', p)\n                        if p.endswith(('.pyc', '.pyo')):\n                            continue\n                        #otherwise fall through and fail\n                    if not os.path.isdir(p):\n                        result.append((p, _md5(p), _size(p)))\n            result.append((record_path, None, None))\n        return result", "output": "Iterates over the ``installed-files.txt`` entries and returns a tuple\n        ``(path, hash, size)`` for each line.\n\n        :returns: a list of (path, hash, size)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generateColorMap():\n    '''\n    \n    '''\n    Map = cm.jet(np.arange(256))\n    stringColors = []\n    for i in range(Map.shape[0]):\n        rgb = (int(255*Map[i][0]), int(255*Map[i][1]), int(255*Map[i][2]))\n        if (sys.version_info > (3, 0)):\n            stringColors.append((struct.pack('BBB', *rgb).hex())) # python 3\n        else:\n            stringColors.append(\n                struct.pack('BBB', *rgb).encode('hex'))  # python2\n\n    return stringColors", "output": "This function generates a 256 jet colormap of HTML-like\n    hex string colors (e.g. FF88AA)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def execute_async_script(self, script, *args):\n        \"\"\"\n        \n        \"\"\"\n        converted_args = list(args)\n        if self.w3c:\n            command = Command.W3C_EXECUTE_SCRIPT_ASYNC\n        else:\n            command = Command.EXECUTE_ASYNC_SCRIPT\n\n        return self.execute(command, {\n            'script': script,\n            'args': converted_args})['value']", "output": "Asynchronously Executes JavaScript in the current window/frame.\n\n        :Args:\n         - script: The JavaScript to execute.\n         - \\\\*args: Any applicable arguments for your JavaScript.\n\n        :Usage:\n            ::\n\n                script = \"var callback = arguments[arguments.length - 1]; \" \\\\\n                         \"window.setTimeout(function(){ callback('timeout') }, 3000);\"\n                driver.execute_async_script(script)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _maybe_to_sparse(array):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(array, ABCSparseSeries):\n        array = array.values.copy()\n    return array", "output": "array must be SparseSeries or SparseArray", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kernel_restarted_message(self, msg):\r\n        \"\"\"\"\"\"\r\n        if not self.is_error_shown:\r\n            # If there are kernel creation errors, jupyter_client will\r\n            # try to restart the kernel and qtconsole prints a\r\n            # message about it.\r\n            # So we read the kernel's stderr_file and display its\r\n            # contents in the client instead of the usual message shown\r\n            # by qtconsole.\r\n            try:\r\n                stderr = self._read_stderr()\r\n            except Exception:\r\n                stderr = None\r\n            if stderr:\r\n                self.show_kernel_error('<tt>%s</tt>' % stderr)\r\n        else:\r\n            self.shellwidget._append_html(\"<br>%s<hr><br>\" % msg,\r\n                                          before_prompt=False)", "output": "Show kernel restarted/died messages.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_rules_import_paths():\n    \"\"\"\n\n    \"\"\"\n    # Bundled rules:\n    yield Path(__file__).parent.joinpath('rules')\n    # Rules defined by user:\n    yield settings.user_dir.joinpath('rules')\n    # Packages with third-party rules:\n    for path in sys.path:\n        for contrib_module in Path(path).glob('thefuck_contrib_*'):\n            contrib_rules = contrib_module.joinpath('rules')\n            if contrib_rules.is_dir():\n                yield contrib_rules", "output": "Yields all rules import paths.\n\n    :rtype: Iterable[Path]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_state_map(meta_graph, state_ops, unsupported_state_ops,\n                  get_tensor_by_name):\n  \"\"\"\"\"\"\n  state_map = {}\n  for node in meta_graph.graph_def.node:\n    if node.op in state_ops:\n      tensor_name = node.name + \":0\"\n      tensor = get_tensor_by_name(tensor_name)\n      num_outputs = len(tensor.op.outputs)\n      if num_outputs != 1:\n        raise ValueError(\"Stateful op %s has %d outputs, expected 1\" %\n                         (node.op, num_outputs))\n      state_map[tensor_name] = tensor\n    if node.op in unsupported_state_ops:\n      raise ValueError(\"Unsupported stateful op: %s\" % node.op)\n  return state_map", "output": "Returns a map from tensor names to tensors that hold the state.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_model_to_external_data(model, all_tensors_to_one_file=True, location=None):\n    # type: (ModelProto, bool, Optional[Text]) -> None\n    \"\"\"\n    \n    \"\"\"\n    if all_tensors_to_one_file:\n        file_name = Text(uuid.uuid1())\n        if location:\n            file_name = location\n        for tensor in _get_all_tensors(model):\n            set_external_data(tensor, file_name)\n    else:\n        for tensor in _get_all_tensors(model):\n            set_external_data(tensor, tensor.name)", "output": "call to set all tensors as external data. save_model saves all the tensors data as external data after calling this function.\n    @params\n    model: ModelProto to be converted.\n    all_tensors_to_one_file: If true, save all tensors to one external file specified by location.\n                             If false, save each tensor to a file named with the tensor name.\n    location: specify the external file that all tensors to save to.\n              If not specified, will use the model name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ret_glob_minions(self):\n        '''\n        \n        '''\n        fnfilter = functools.partial(fnmatch.filter, pat=self.tgt)\n        return self._ret_minions(fnfilter)", "output": "Return minions that match via glob", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def denoise_v1_m15():\n  \"\"\"\"\"\"\n  hparams = xmoe2_v1()\n  # no local attention\n  # TODO(noam): non-masked version of local-attention\n  hparams.decoder_layers = [\n      \"att\" if l == \"local_att\" else l for l in hparams.decoder_layers]\n  hparams.decoder_type = \"denoising\"\n  hparams.noising_spec_train = {\"type\": \"mask\", \"prob\": 0.15}\n  return hparams", "output": "Denoising experiment.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def present(name,\n            passwd,\n            database=None,\n            user=None,\n            password=None,\n            host=None,\n            port=None):\n    '''\n    \n\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    # check if db does not exist\n    if database and not __salt__['influxdb08.db_exists'](\n            database, user, password, host, port):\n        ret['result'] = False\n        ret['comment'] = 'Database {0} does not exist'.format(database)\n        return ret\n\n    # check if user exists\n    if not __salt__['influxdb08.user_exists'](\n            name, database, user, password, host, port):\n        if __opts__['test']:\n            ret['result'] = None\n            ret['comment'] = 'User {0} is not present and needs to be created'\\\n                .format(name)\n            return ret\n        # The user is not present, make it!\n        if __salt__['influxdb08.user_create'](\n                name, passwd, database, user, password, host, port):\n            ret['comment'] = 'User {0} has been created'.format(name)\n            ret['changes'][name] = 'Present'\n            return ret\n        else:\n            ret['comment'] = 'Failed to create user {0}'.format(name)\n            ret['result'] = False\n            return ret\n\n    # fallback\n    ret['comment'] = 'User {0} is already present'.format(name)\n    return ret", "output": "Ensure that the cluster admin or database user is present.\n\n    name\n        The name of the user to manage\n\n    passwd\n        The password of the user\n\n    database\n        The database to create the user in\n\n    user\n        The user to connect as (must be able to create the user)\n\n    password\n        The password of the user\n\n    host\n        The host to connect to\n\n    port\n        The port to connect to", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def import_dashboards(path, recursive):\n    \"\"\"\"\"\"\n    p = Path(path)\n    files = []\n    if p.is_file():\n        files.append(p)\n    elif p.exists() and not recursive:\n        files.extend(p.glob('*.json'))\n    elif p.exists() and recursive:\n        files.extend(p.rglob('*.json'))\n    for f in files:\n        logging.info('Importing dashboard from file %s', f)\n        try:\n            with f.open() as data_stream:\n                dashboard_import_export.import_dashboards(\n                    db.session, data_stream)\n        except Exception as e:\n            logging.error('Error when importing dashboard from file %s', f)\n            logging.error(e)", "output": "Import dashboards from JSON", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_ssl_context(self, req: 'ClientRequest') -> Optional[SSLContext]:\n        \"\"\"\n        \"\"\"\n        if req.is_ssl():\n            if ssl is None:  # pragma: no cover\n                raise RuntimeError('SSL is not supported.')\n            sslcontext = req.ssl\n            if isinstance(sslcontext, ssl.SSLContext):\n                return sslcontext\n            if sslcontext is not None:\n                # not verified or fingerprinted\n                return self._make_ssl_context(False)\n            sslcontext = self._ssl\n            if isinstance(sslcontext, ssl.SSLContext):\n                return sslcontext\n            if sslcontext is not None:\n                # not verified or fingerprinted\n                return self._make_ssl_context(False)\n            return self._make_ssl_context(True)\n        else:\n            return None", "output": "Logic to get the correct SSL context\n\n        0. if req.ssl is false, return None\n\n        1. if ssl_context is specified in req, use it\n        2. if _ssl_context is specified in self, use it\n        3. otherwise:\n            1. if verify_ssl is not specified in req, use self.ssl_context\n               (will generate a default context according to self.verify_ssl)\n            2. if verify_ssl is True in req, generate a default SSL context\n            3. if verify_ssl is False in req, generate a SSL context that\n               won't verify", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def read(self):\n        \"\"\"\n        \"\"\"\n        if not self._url:\n            raise DiscordException('Invalid asset (no URL provided)')\n\n        if self._state is None:\n            raise DiscordException('Invalid state (no ConnectionState provided)')\n\n        return await self._state.http.get_from_cdn(self._url)", "output": "|coro|\n\n        Retrieves the content of this asset as a :class:`bytes` object.\n\n        .. warning::\n\n            :class:`PartialEmoji` won't have a connection state if user created,\n            and a URL won't be present if a custom image isn't associated with\n            the asset, e.g. a guild with no custom icon.\n\n        .. versionadded:: 1.1.0\n\n        Raises\n        ------\n        DiscordException\n            There was no valid URL or internal connection state.\n        HTTPException\n            Downloading the asset failed.\n        NotFound\n            The asset was deleted.\n\n        Returns\n        -------\n        :class:`bytes`\n            The content of the asset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetch_project(self, project_id):\n        \"\"\"\n        \"\"\"\n        project = self.new_project(project_id)\n        project.reload()\n        return project", "output": "Fetch an existing project and it's relevant metadata by ID.\n\n        .. note::\n\n            If the project does not exist, this will raise a\n            :class:`NotFound <google.cloud.exceptions.NotFound>` error.\n\n        :type project_id: str\n        :param project_id: The ID for this project.\n\n        :rtype: :class:`~google.cloud.resource_manager.project.Project`\n        :returns: A :class:`~google.cloud.resource_manager.project.Project`\n                  with metadata fetched from the API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main():\n    \"\"\"\"\"\"\n    global __verbose\n    # Parse CMD args\n    parser = argparse.ArgumentParser(description='DFU Python Util')\n    #parser.add_argument(\"path\", help=\"file path\")\n    parser.add_argument(\n        \"-l\", \"--list\",\n        help=\"list available DFU devices\",\n        action=\"store_true\",\n        default=False\n    )\n    parser.add_argument(\n        \"-m\", \"--mass-erase\",\n        help=\"mass erase device\",\n        action=\"store_true\",\n        default=False\n    )\n    parser.add_argument(\n        \"-u\", \"--upload\",\n        help=\"read file from DFU device\",\n        dest=\"path\",\n        default=False\n    )\n    parser.add_argument(\n        \"-v\", \"--verbose\",\n        help=\"increase output verbosity\",\n        action=\"store_true\",\n        default=False\n    )\n    args = parser.parse_args()\n\n    __verbose = args.verbose\n\n    if args.list:\n        list_dfu_devices(idVendor=__VID, idProduct=__PID)\n        return\n\n    init()\n\n    if args.mass_erase:\n        print (\"Mass erase...\")\n        mass_erase()\n\n    if args.path:\n        elements = read_dfu_file(args.path)\n        if not elements:\n            return\n        print(\"Writing memory...\")\n        write_elements(elements, args.mass_erase, progress=cli_progress)\n\n        print(\"Exiting DFU...\")\n        exit_dfu()\n        return\n\n    print(\"No command specified\")", "output": "Test program for verifying this files functionality.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def terminate(self, force=False):\n        ''' '''\n\n        if not self.isalive():\n            return True\n        try:\n            self.kill(signal.SIGHUP)\n            time.sleep(self.delayafterterminate)\n            if not self.isalive():\n                return True\n            self.kill(signal.SIGCONT)\n            time.sleep(self.delayafterterminate)\n            if not self.isalive():\n                return True\n            self.kill(signal.SIGINT)\n            time.sleep(self.delayafterterminate)\n            if not self.isalive():\n                return True\n            if force:\n                self.kill(signal.SIGKILL)\n                time.sleep(self.delayafterterminate)\n                if not self.isalive():\n                    return True\n                else:\n                    return False\n            return False\n        except OSError:\n            # I think there are kernel timing issues that sometimes cause\n            # this to happen. I think isalive() reports True, but the\n            # process is dead to the kernel.\n            # Make one last attempt to see if the kernel is up to date.\n            time.sleep(self.delayafterterminate)\n            if not self.isalive():\n                return True\n            else:\n                return False", "output": "This forces a child process to terminate. It starts nicely with\n        SIGHUP and SIGINT. If \"force\" is True then moves onto SIGKILL. This\n        returns True if the child was terminated. This returns False if the\n        child could not be terminated.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, request):\n        \"\"\"\n        \"\"\"\n        # No virtual hosts specified; default behavior\n        if not self.hosts:\n            return self._get(request.path, request.method, \"\")\n        # virtual hosts specified; try to match route to the host header\n        try:\n            return self._get(\n                request.path, request.method, request.headers.get(\"Host\", \"\")\n            )\n        # try default hosts\n        except NotFound:\n            return self._get(request.path, request.method, \"\")", "output": "Get a request handler based on the URL of the request, or raises an\n        error\n\n        :param request: Request object\n        :return: handler, arguments, keyword arguments", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_nodes(**kwargs):\n    '''\n    \n    '''\n    ret = {}\n\n    nodes = list_nodes_full()\n    for node in nodes:\n        ret[node] = {}\n        for prop in 'id', 'image', 'size', 'state', 'private_ips', 'public_ips':\n            ret[node][prop] = nodes[node][prop]\n\n    return ret", "output": "Return basic data on nodes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cast_to_pod(val):\n  \"\"\"\"\"\"\n  bools = {\"True\": True, \"False\": False}\n  if val in bools:\n    return bools[val]\n  try:\n    return int(val)\n  except ValueError:\n    try:\n      return float(val)\n    except ValueError:\n      return tf.compat.as_text(val)", "output": "Try cast to int, float, bool, str, in that order.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def public_ip_address_create_or_update(name, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    if 'location' not in kwargs:\n        rg_props = __salt__['azurearm_resource.resource_group_get'](\n            resource_group, **kwargs\n        )\n\n        if 'error' in rg_props:\n            log.error(\n                'Unable to determine location from resource group specified.'\n            )\n            return False\n        kwargs['location'] = rg_props['location']\n\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n\n    try:\n        pub_ip_model = __utils__['azurearm.create_object_model']('network', 'PublicIPAddress', **kwargs)\n    except TypeError as exc:\n        result = {'error': 'The object model could not be built. ({0})'.format(str(exc))}\n        return result\n\n    try:\n        ip = netconn.public_ip_addresses.create_or_update(\n            resource_group_name=resource_group,\n            public_ip_address_name=name,\n            parameters=pub_ip_model\n        )\n        ip.wait()\n        ip_result = ip.result()\n        result = ip_result.as_dict()\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n    except SerializationError as exc:\n        result = {'error': 'The object model could not be parsed. ({0})'.format(str(exc))}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Create or update a public IP address within a specified resource group.\n\n    :param name: The name of the public IP address to create.\n\n    :param resource_group: The resource group name assigned to the\n        public IP address.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.public_ip_address_create_or_update test-ip-0 testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def project_transfer_config_path(cls, project, transfer_config):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/transferConfigs/{transfer_config}\",\n            project=project,\n            transfer_config=transfer_config,\n        )", "output": "Return a fully-qualified project_transfer_config string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setSample(self, sample):\n        \"\"\"\"\"\"\n        if not isinstance(sample, RDD):\n            raise TypeError(\"samples should be a RDD, received %s\" % type(sample))\n        self._sample = sample", "output": "Set sample points from the population. Should be a RDD", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def event_return(events):\n    '''\n    \n    '''\n    with _get_serv(events, commit=True) as cur:\n        for event in events:\n            tag = event.get('tag', '')\n            data = event.get('data', '')\n            sql = '''INSERT INTO `salt_events` (`tag`, `data`, `master_id`)\n                     VALUES (%s, %s, %s)'''\n            cur.execute(sql, (tag, salt.utils.json.dumps(data), __opts__['id']))", "output": "Return event to mysql server\n\n    Requires that configuration be enabled via 'event_return'\n    option in master config.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unflatten(flat_dict: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    \n    \"\"\"\n    unflat: Dict[str, Any] = {}\n\n    for compound_key, value in flat_dict.items():\n        curr_dict = unflat\n        parts = compound_key.split(\".\")\n        for key in parts[:-1]:\n            curr_value = curr_dict.get(key)\n            if key not in curr_dict:\n                curr_dict[key] = {}\n                curr_dict = curr_dict[key]\n            elif isinstance(curr_value, dict):\n                curr_dict = curr_value\n            else:\n                raise ConfigurationError(\"flattened dictionary is invalid\")\n        if not isinstance(curr_dict, dict) or parts[-1] in curr_dict:\n            raise ConfigurationError(\"flattened dictionary is invalid\")\n        else:\n            curr_dict[parts[-1]] = value\n\n    return unflat", "output": "Given a \"flattened\" dict with compound keys, e.g.\n        {\"a.b\": 0}\n    unflatten it:\n        {\"a\": {\"b\": 0}}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_symbol_train(network, data_shape, **kwargs):\n    \"\"\"\n    \"\"\"\n    if network.startswith('legacy'):\n        logging.warn('Using legacy model.')\n        return symbol_builder.import_module(network).get_symbol_train(**kwargs)\n    config = get_config(network, data_shape, **kwargs).copy()\n    config.update(kwargs)\n    return symbol_builder.get_symbol_train(**config)", "output": "Wrapper for get symbol for train\n\n    Parameters\n    ----------\n    network : str\n        name for the base network symbol\n    data_shape : int\n        input shape\n    kwargs : dict\n        see symbol_builder.get_symbol_train for more details", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_module_source_path(modname, basename=None):\r\n    \"\"\"\"\"\"\r\n    srcpath = get_module_path(modname)\r\n    parentdir = osp.join(srcpath, osp.pardir)\r\n    if osp.isfile(parentdir):\r\n        # Parent directory is not a directory but the 'library.zip' file:\r\n        # this is either a py2exe or a cx_Freeze distribution\r\n        srcpath = osp.abspath(osp.join(osp.join(parentdir, osp.pardir),\r\n                                       modname))\r\n    if basename is not None:\r\n        srcpath = osp.abspath(osp.join(srcpath, basename))\r\n    return srcpath", "output": "Return module *modname* source path\r\n    If *basename* is specified, return *modname.basename* path where \r\n    *modname* is a package containing the module *basename*\r\n    \r\n    *basename* is a filename (not a module name), so it must include the\r\n    file extension: .py or .pyw\r\n    \r\n    Handles py2exe/cx_Freeze distributions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compile_pattern_list(self, patterns):\n        '''\n        '''\n\n        if patterns is None:\n            return []\n        if not isinstance(patterns, list):\n            patterns = [patterns]\n\n        # Allow dot to match \\n\n        compile_flags = re.DOTALL\n        if self.ignorecase:\n            compile_flags = compile_flags | re.IGNORECASE\n        compiled_pattern_list = []\n        for idx, p in enumerate(patterns):\n            if isinstance(p, self.allowed_string_types):\n                p = self._coerce_expect_string(p)\n                compiled_pattern_list.append(re.compile(p, compile_flags))\n            elif p is EOF:\n                compiled_pattern_list.append(EOF)\n            elif p is TIMEOUT:\n                compiled_pattern_list.append(TIMEOUT)\n            elif isinstance(p, type(re.compile(''))):\n                compiled_pattern_list.append(p)\n            else:\n                self._pattern_type_err(p)\n        return compiled_pattern_list", "output": "This compiles a pattern-string or a list of pattern-strings.\n        Patterns must be a StringType, EOF, TIMEOUT, SRE_Pattern, or a list of\n        those. Patterns may also be None which results in an empty list (you\n        might do this if waiting for an EOF or TIMEOUT condition without\n        expecting any pattern).\n\n        This is used by expect() when calling expect_list(). Thus expect() is\n        nothing more than::\n\n             cpl = self.compile_pattern_list(pl)\n             return self.expect_list(cpl, timeout)\n\n        If you are using expect() within a loop it may be more\n        efficient to compile the patterns first and then call expect_list().\n        This avoid calls in a loop to compile_pattern_list()::\n\n             cpl = self.compile_pattern_list(my_pattern)\n             while some_condition:\n                ...\n                i = self.expect_list(cpl, timeout)\n                ...", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __setup_menu(self):\r\n        \"\"\"\"\"\"\r\n        self.menu.clear()\r\n        if self.data:\r\n            actions = self.menu_actions\r\n        else:\r\n            actions = (self.new_action, self.open_action)\r\n            self.setFocus() # --> Editor.__get_focus_editortabwidget\r\n        add_actions(self.menu, list(actions) + self.__get_split_actions())\r\n        self.close_action.setEnabled(self.is_closable)", "output": "Setup tab context menu before showing it", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _process_msg(self, client, state, reward, isOver):\n        \"\"\"\n        \n        \"\"\"\n        # in the first message, only state is valid,\n        # reward&isOver should be discarded\n        if len(client.memory) > 0:\n            client.memory[-1].reward = reward\n            if isOver:\n                # should clear client's memory and put to queue\n                self._parse_memory(0, client, True)\n            else:\n                if len(client.memory) == LOCAL_TIME_MAX + 1:\n                    R = client.memory[-1].value\n                    self._parse_memory(R, client, False)\n        # feed state and return action\n        self._on_state(state, client)", "output": "Process a message sent from some client.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_task_kwargs(self):\n        \"\"\"\n        \n        \"\"\"\n        res = {}\n        for (param_name, param_obj) in self._get_task_cls().get_params():\n            attr = getattr(self.known_args, param_name)\n            if attr:\n                res.update(((param_name, param_obj.parse(attr)),))\n\n        return res", "output": "Get the local task arguments as a dictionary. The return value is in\n        the form ``dict(my_param='my_value', ...)``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hybrid_forward(self, F, inputs, mem_value, mask=None, mem_mask=None):  #pylint: disable=unused-argument\n        #  pylint: disable=arguments-differ\n        \"\"\"\n        \"\"\"\n        outputs, attention_in_outputs =\\\n            self.attention_cell_in(inputs, inputs, inputs, mask)\n        outputs = self.proj_in(outputs)\n        if self._dropout:\n            outputs = self.dropout_layer(outputs)\n        if self._use_residual:\n            outputs = outputs + inputs\n        outputs = self.layer_norm_in(outputs)\n        inputs = outputs\n        outputs, attention_inter_outputs = \\\n            self.attention_cell_inter(inputs, mem_value, mem_value, mem_mask)\n        outputs = self.proj_inter(outputs)\n        if self._dropout:\n            outputs = self.dropout_layer(outputs)\n        if self._use_residual:\n            outputs = outputs + inputs\n        outputs = self.layer_norm_inter(outputs)\n        outputs = self.ffn(outputs)\n        additional_outputs = []\n        if self._output_attention:\n            additional_outputs.append(attention_in_outputs)\n            additional_outputs.append(attention_inter_outputs)\n        return outputs, additional_outputs", "output": "Transformer Decoder Attention Cell.\n\n        Parameters\n        ----------\n        inputs : Symbol or NDArray\n            Input sequence. Shape (batch_size, length, C_in)\n        mem_value : Symbol or NDArrays\n            Memory value, i.e. output of the encoder. Shape (batch_size, mem_length, C_in)\n        mask : Symbol or NDArray or None\n            Mask for inputs. Shape (batch_size, length, length)\n        mem_mask : Symbol or NDArray or None\n            Mask for mem_value. Shape (batch_size, length, mem_length)\n\n        Returns\n        -------\n        decoder_cell_outputs: list\n            Outputs of the decoder cell. Contains:\n\n            - outputs of the transformer decoder cell. Shape (batch_size, length, C_out)\n            - additional_outputs of all the transformer decoder cell", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def quantity(self):\n        \"\"\"\n        \n        \"\"\"\n        if np.isnan(self._quantity):\n            raise RuntimeError(\"Quantity of order {} is not supposed to be nan.\".format(self.order_id))\n        return self._quantity", "output": "[int] \u8ba2\u5355\u6570\u91cf", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_font(section='appearance', option='font', font_size_delta=0):\n    \"\"\"\"\"\"\n    font = FONT_CACHE.get((section, option))\n\n    if font is None:\n        families = CONF.get(section, option+\"/family\", None)\n\n        if families is None:\n            return QFont()\n\n        family = get_family(families)\n        weight = QFont.Normal\n        italic = CONF.get(section, option+'/italic', False)\n\n        if CONF.get(section, option+'/bold', False):\n            weight = QFont.Bold\n\n        size = CONF.get(section, option+'/size', 9) + font_size_delta\n        font = QFont(family, size, weight)\n        font.setItalic(italic)\n        FONT_CACHE[(section, option)] = font\n\n    size = CONF.get(section, option+'/size', 9) + font_size_delta\n    font.setPointSize(size)\n    return font", "output": "Get console font properties depending on OS and user options", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _serve_numerics_alert_report_handler(self, request):\n    \"\"\"\n    \"\"\"\n    if request.method != 'GET':\n      logger.error(\n          '%s requests are forbidden by the debugger plugin.', request.method)\n      return wrappers.Response(status=405)\n\n    report = self._debugger_data_server.numerics_alert_report()\n\n    # Convert the named tuples to dictionaries so we JSON them into objects.\n    response = [r._asdict() for r in report]  # pylint: disable=protected-access\n    return http_util.Respond(request, response, 'application/json')", "output": "A (wrapped) werkzeug handler for serving numerics alert report.\n\n    Accepts GET requests and responds with an array of JSON-ified\n    NumericsAlertReportRow.\n\n    Each JSON-ified NumericsAlertReportRow object has the following format:\n    {\n        'device_name': string,\n        'tensor_name': string,\n        'first_timestamp': float,\n        'nan_event_count': int,\n        'neg_inf_event_count': int,\n        'pos_inf_event_count': int\n    }\n\n    These objects are sorted by ascending order of first_timestamp in the\n    response array.\n\n    Args:\n      request: The request, currently assumed to be empty.\n\n    Returns:\n      A werkzeug BaseResponse object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_stream(self, stream):\n        \"\"\"\n        \n        \"\"\"\n        # load the batches\n        for batch in self.serializer.load_stream(stream):\n            yield batch\n\n        # load the batch order indices\n        num = read_int(stream)\n        batch_order = []\n        for i in xrange(num):\n            index = read_int(stream)\n            batch_order.append(index)\n        yield batch_order", "output": "Load a stream of un-ordered Arrow RecordBatches, where the last iteration yields\n        a list of indices that can be used to put the RecordBatches in the correct order.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_vocab_from_file(self, filename):\n    \"\"\"\n    \"\"\"\n    with tf.gfile.Open(filename) as f:\n      tokens = [token.strip() for token in f.readlines()]\n\n    def token_gen():\n      for token in tokens:\n        yield token\n\n    self._init_vocab(token_gen(), add_reserved_tokens=False)", "output": "Load vocab from a file.\n\n    Args:\n      filename: The file to load vocabulary from.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mtr_tr_dense_local(sz):\n  \"\"\"\"\"\"\n  hparams = mtr_tr_dense(sz)\n  hparams.decoder_layers = [\"local_self_att\", \"enc_att\", \"drd\"] * 6\n  hparams.local_attention_radius = 32\n  return hparams", "output": "With local self-attention in the decoder.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare_data_loader(args, dataset, vocab, test=False):\n    \"\"\"\n    \n    \"\"\"\n    # Preprocess\n    dataset = dataset.transform(lambda s1, s2, label: (vocab(s1), vocab(s2), label),\n                                lazy=False)\n\n    # Batching\n    batchify_fn = btf.Tuple(btf.Pad(), btf.Pad(), btf.Stack(dtype='int32'))\n    data_lengths = [max(len(d[0]), len(d[1])) for d in dataset]\n    batch_sampler = nlp.data.FixedBucketSampler(lengths=data_lengths,\n                                                batch_size=args.batch_size,\n                                                shuffle=(not test))\n    data_loader = gluon.data.DataLoader(dataset=dataset,\n                                        batch_sampler=batch_sampler,\n                                        batchify_fn=batchify_fn)\n    return data_loader", "output": "Read data and build data loader.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def export(export_path, vocabulary, embeddings, num_oov_buckets,\n           preprocess_text):\n  \"\"\"\n  \"\"\"\n  # Write temporary vocab file for module construction.\n  tmpdir = tempfile.mkdtemp()\n  vocabulary_file = os.path.join(tmpdir, \"tokens.txt\")\n  with tf.gfile.GFile(vocabulary_file, \"w\") as f:\n    f.write(\"\\n\".join(vocabulary))\n  vocab_size = len(vocabulary)\n  embeddings_dim = embeddings.shape[1]\n  spec = make_module_spec(vocabulary_file, vocab_size, embeddings_dim,\n                          num_oov_buckets, preprocess_text)\n\n  try:\n    with tf.Graph().as_default():\n      m = hub.Module(spec)\n      # The embeddings may be very large (e.g., larger than the 2GB serialized\n      # Tensor limit).  To avoid having them frozen as constant Tensors in the\n      # graph we instead assign them through the placeholders and feed_dict\n      # mechanism.\n      p_embeddings = tf.placeholder(tf.float32)\n      load_embeddings = tf.assign(m.variable_map[EMBEDDINGS_VAR_NAME],\n                                  p_embeddings)\n\n      with tf.Session() as sess:\n        sess.run([load_embeddings], feed_dict={p_embeddings: embeddings})\n        m.export(export_path, sess)\n  finally:\n    shutil.rmtree(tmpdir)", "output": "Exports a TF-Hub module that performs embedding lookups.\n\n  Args:\n    export_path: Location to export the module.\n    vocabulary: List of the N tokens in the vocabulary.\n    embeddings: Numpy array of shape [N+K,M] the first N rows are the\n      M dimensional embeddings for the respective tokens and the next K\n      rows are for the K out-of-vocabulary buckets.\n    num_oov_buckets: How many out-of-vocabulary buckets to add.\n    preprocess_text: Whether to preprocess the input tensor by removing\n      punctuation and splitting on spaces.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def finder(package):\n    \"\"\"\n    \n    \"\"\"\n    if package in _finder_cache:\n        result = _finder_cache[package]\n    else:\n        if package not in sys.modules:\n            __import__(package)\n        module = sys.modules[package]\n        path = getattr(module, '__path__', None)\n        if path is None:\n            raise DistlibException('You cannot get a finder for a module, '\n                                   'only for a package')\n        loader = getattr(module, '__loader__', None)\n        finder_maker = _finder_registry.get(type(loader))\n        if finder_maker is None:\n            raise DistlibException('Unable to locate finder for %r' % package)\n        result = finder_maker(module)\n        _finder_cache[package] = result\n    return result", "output": "Return a resource finder for a package.\n    :param package: The name of the package.\n    :return: A :class:`ResourceFinder` instance for the package.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def train_model_from_args(args: argparse.Namespace):\n    \"\"\"\n    \n    \"\"\"\n    train_model_from_file(args.param_path,\n                          args.serialization_dir,\n                          args.overrides,\n                          args.file_friendly_logging,\n                          args.recover,\n                          args.force,\n                          args.cache_directory,\n                          args.cache_prefix)", "output": "Just converts from an ``argparse.Namespace`` object to string paths.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_many(queue_, max_items=None, max_latency=0):\n    \"\"\"\n    \"\"\"\n    start = time.time()\n    # Always return at least one item.\n    items = [queue_.get()]\n    while max_items is None or len(items) < max_items:\n        try:\n            elapsed = time.time() - start\n            timeout = max(0, max_latency - elapsed)\n            items.append(queue_.get(timeout=timeout))\n        except queue.Empty:\n            break\n    return items", "output": "Get multiple items from a Queue.\n\n    Gets at least one (blocking) and at most ``max_items`` items\n    (non-blocking) from a given Queue. Does not mark the items as done.\n\n    Args:\n        queue_ (~queue.Queue`): The Queue to get items from.\n        max_items (int): The maximum number of items to get. If ``None``, then\n            all available items in the queue are returned.\n        max_latency (float):  The maximum number of seconds to wait for more\n            than one item from a queue. This number includes the time required\n            to retrieve the first item.\n\n    Returns:\n        Sequence[Any]: A sequence of items retrieved from the queue.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_int(cls, i):\n        \"\"\"\n        \"\"\"\n        i &= FULL_PERMS\n        key = ('', 'x', 'w', 'xw', 'r', 'rx', 'rw', 'rwx')\n        parts = []\n        while i:\n            parts.append(key[i & _SINGLE_FULL_PERM])\n            i >>= 3\n        parts.reverse()\n        return cls(*parts)", "output": "Create a :class:`FilePerms` object from an integer.\n\n        >>> FilePerms.from_int(0o644)  # note the leading zero-oh for octal\n        FilePerms(user='rw', group='r', other='r')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_cluster_role_binding(self, name, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_cluster_role_binding_with_http_info(name, **kwargs)\n        else:\n            (data) = self.read_cluster_role_binding_with_http_info(name, **kwargs)\n            return data", "output": "read the specified ClusterRoleBinding\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_cluster_role_binding(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ClusterRoleBinding (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1ClusterRoleBinding\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_model_from_path(model_path, meta=False, **overrides):\n    \"\"\"\"\"\"\n    if not meta:\n        meta = get_model_meta(model_path)\n    cls = get_lang_class(meta[\"lang\"])\n    nlp = cls(meta=meta, **overrides)\n    pipeline = meta.get(\"pipeline\", [])\n    disable = overrides.get(\"disable\", [])\n    if pipeline is True:\n        pipeline = nlp.Defaults.pipe_names\n    elif pipeline in (False, None):\n        pipeline = []\n    for name in pipeline:\n        if name not in disable:\n            config = meta.get(\"pipeline_args\", {}).get(name, {})\n            component = nlp.create_pipe(name, config=config)\n            nlp.add_pipe(component, name=name)\n    return nlp.from_disk(model_path)", "output": "Load a model from a data directory path. Creates Language class with\n    pipeline from meta.json and then calls from_disk() with path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shuffle_dataset(filenames, extra_fn=None):\n  \"\"\"\n  \"\"\"\n  if outputs_exist(filenames):\n    tf.logging.info(\"Skipping shuffle because output files exist\")\n    return\n  tf.logging.info(\"Shuffling data...\")\n  for filename in filenames:\n    _shuffle_single(filename, extra_fn=extra_fn)\n  tf.logging.info(\"Data shuffled.\")", "output": "Shuffles the dataset.\n\n  Args:\n    filenames: a list of strings\n    extra_fn: an optional function from list of records to list of records\n      to be called after shuffling a file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_jdbc_resource(name, server=None, **kwargs):\n    '''\n    \n    '''\n    # You're not supposed to update jndiName, if you do so, it will crash, silently\n    if 'jndiName' in kwargs:\n        del kwargs['jndiName']\n    return _update_element(name, 'resources/jdbc-resource', kwargs, server)", "output": "Update a JDBC resource", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def position(self, node):\n        \"\"\"\"\"\"\n        rv = 'line %d' % node.lineno\n        if self.name is not None:\n            rv += ' in ' + repr(self.name)\n        return rv", "output": "Return a human readable position for the node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _list_images(self, root):\n        \"\"\"\n        \n        \"\"\"\n        self.labels = []\n        self.items = []\n\n        valid_unseen_sub_idx = [1, 2, 20, 22]\n        skip_sub_idx = [21]\n\n        if self._mode == 'train':\n            sub_idx = ['s' + str(i) for i in range(1, 35) \\\n                             if i not in valid_unseen_sub_idx + skip_sub_idx]\n        elif self._mode == 'valid':\n            sub_idx = ['s' + str(i) for i in valid_unseen_sub_idx]\n\n        folder_path = []\n        for i in sub_idx:\n            folder_path.extend(glob.glob(os.path.join(root, i, \"*\")))\n\n        for folder in folder_path:\n            filename = glob.glob(os.path.join(folder, \"*\"))\n            if len(filename) != self._seq_len:\n                continue\n            filename.sort()\n            label = os.path.split(folder)[-1]\n            self.items.append((filename, label))", "output": "Description : generate list for lip images", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_args(arg):\n    '''\n    \n    '''\n    yaml_args = salt.utils.args.yamlify_arg(arg)\n\n    if yaml_args is None:\n        return []\n    elif not isinstance(yaml_args, list):\n        return [yaml_args]\n    else:\n        return yaml_args", "output": "yamlify `arg` and ensure it's outermost datatype is a list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def all_packages(self):\n        \"\"\"\"\"\"\n        p = dict(self.parsed_pipfile.get(\"dev-packages\", {}))\n        p.update(self.parsed_pipfile.get(\"packages\", {}))\n        return p", "output": "Returns a list of all packages.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_region_from_metadata():\n    '''\n    \n    '''\n    global __Location__\n\n    if __Location__ == 'do-not-get-from-metadata':\n        log.debug('Previously failed to get AWS region from metadata. Not trying again.')\n        return None\n\n    # Cached region\n    if __Location__ != '':\n        return __Location__\n\n    try:\n        # Connections to instance meta-data must fail fast and never be proxied\n        result = requests.get(\n            \"http://169.254.169.254/latest/dynamic/instance-identity/document\",\n            proxies={'http': ''}, timeout=AWS_METADATA_TIMEOUT,\n        )\n    except requests.exceptions.RequestException:\n        log.warning('Failed to get AWS region from instance metadata.', exc_info=True)\n        # Do not try again\n        __Location__ = 'do-not-get-from-metadata'\n        return None\n\n    try:\n        region = result.json()['region']\n        __Location__ = region\n        return __Location__\n    except (ValueError, KeyError):\n        log.warning('Failed to decode JSON from instance metadata.')\n        return None\n\n    return None", "output": "Try to get region from instance identity document and cache it\n\n    .. versionadded:: 2015.5.6", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_episode_end(self, episode, logs):\n        \"\"\"  \"\"\" \n        duration = timeit.default_timer() - self.starts[episode]\n\n        metrics = self.metrics[episode]\n        if np.isnan(metrics).all():\n            mean_metrics = np.array([np.nan for _ in self.metrics_names])\n        else:\n            mean_metrics = np.nanmean(metrics, axis=0)\n        assert len(mean_metrics) == len(self.metrics_names)\n\n        data = list(zip(self.metrics_names, mean_metrics))\n        data += list(logs.items())\n        data += [('episode', episode), ('duration', duration)]\n        for key, value in data:\n            if key not in self.data:\n                self.data[key] = []\n            self.data[key].append(value)\n\n        if self.interval is not None and episode % self.interval == 0:\n            self.save_data()\n\n        # Clean up.\n        del self.metrics[episode]\n        del self.starts[episode]", "output": "Compute and print metrics at the end of each episode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def summary(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.hasSummary:\n            return GaussianMixtureSummary(super(GaussianMixtureModel, self).summary)\n        else:\n            raise RuntimeError(\"No training summary available for this %s\" %\n                               self.__class__.__name__)", "output": "Gets summary (e.g. cluster assignments, cluster sizes) of the model trained on the\n        training set. An exception is thrown if no summary exists.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_func(self, func, *arg, **kwargs):\n        \"\"\"\n        \"\"\"\n\n        return self.groupby(level=1, sort=False).apply(func, *arg, **kwargs)", "output": "QADATASTRUCT\u7684\u6307\u6807/\u51fd\u6570apply\u5165\u53e3\n\n        Arguments:\n            func {[type]} -- [description]\n\n        Returns:\n            [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def recall(self, label=None):\n        \"\"\"\n        \n        \"\"\"\n        if label is None:\n            return self.call(\"recall\")\n        else:\n            return self.call(\"recall\", float(label))", "output": "Returns recall or recall for a given label (category) if specified.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compute_video_metrics_from_png_files(\n    output_dirs, problem_name, video_length, frame_shape):\n  \"\"\"\n  \"\"\"\n  ssim_all_decodes, psnr_all_decodes = [], []\n  for output_dir in output_dirs:\n    output_files, target_files = get_target_and_output_filepatterns(\n        output_dir, problem_name)\n    args = get_zipped_dataset_from_png_files(\n        output_files, target_files, video_length, frame_shape)\n    psnr_single, ssim_single = compute_one_decoding_video_metrics(*args)\n    psnr_all_decodes.append(psnr_single)\n    ssim_all_decodes.append(ssim_single)\n\n  psnr_all_decodes = np.array(psnr_all_decodes)\n  ssim_all_decodes = np.array(ssim_all_decodes)\n  all_results = {\"PSNR\": psnr_all_decodes, \"SSIM\": ssim_all_decodes}\n  return compute_all_metrics_statistics(all_results)", "output": "Computes the average of all the metric for one decoding.\n\n  This function assumes that all the predicted and target frames\n  have been saved on the disk and sorting them by name will result\n  to consecutive frames saved in order.\n\n  Args:\n    output_dirs: directory with all the saved frames.\n    problem_name: prefix of the saved frames usually name of the problem.\n    video_length: length of the videos.\n    frame_shape: shape of each frame in HxWxC format.\n\n  Returns:\n    Dictionary which contains the average of each metric per frame.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def subproc_call(cmd, timeout=None):\n    \"\"\"\n    \n    \"\"\"\n    try:\n        output = subprocess.check_output(\n            cmd, stderr=subprocess.STDOUT,\n            shell=True, timeout=timeout)\n        return output, 0\n    except subprocess.TimeoutExpired as e:\n        logger.warn(\"Command '{}' timeout!\".format(cmd))\n        logger.warn(e.output.decode('utf-8'))\n        return e.output, -1\n    except subprocess.CalledProcessError as e:\n        logger.warn(\"Command '{}' failed, return code={}\".format(cmd, e.returncode))\n        logger.warn(e.output.decode('utf-8'))\n        return e.output, e.returncode\n    except Exception:\n        logger.warn(\"Command '{}' failed to run.\".format(cmd))\n        return \"\", -2", "output": "Execute a command with timeout, and return STDOUT and STDERR\n\n    Args:\n        cmd(str): the command to execute.\n        timeout(float): timeout in seconds.\n\n    Returns:\n        output(bytes), retcode(int). If timeout, retcode is -1.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pkg_version_list(self, pkg_id):\n        '''\n        \n        '''\n        pkg_data = self.__reg_software.get(pkg_id, None)\n        if not pkg_data:\n            return []\n\n        if isinstance(pkg_data, list):\n            # raw data is 'pkgid': [sorted version list]\n            return pkg_data  # already sorted oldest to newest\n\n        # Must be a dict or OrderDict, and contain full details\n        installed_versions = list(pkg_data.get('version').keys())\n        return sorted(installed_versions, key=cmp_to_key(self.__oldest_to_latest_version))", "output": "Returns information on a package.\n\n        Args:\n            pkg_id (str): Package Id of the software/component.\n\n        Returns:\n            list: List of version numbers installed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def metrics_for_mode(self, mode):\n    \"\"\"\"\"\"\n    if mode not in self._values:\n      logging.info(\"Mode %s not found\", mode)\n      return []\n    return sorted(list(self._values[mode].keys()))", "output": "Metrics available for a given mode.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def splitext(path):\n    # type: (str) -> Tuple[str, str]\n    \"\"\"\"\"\"\n    base, ext = posixpath.splitext(path)\n    if base.lower().endswith('.tar'):\n        ext = base[-4:] + ext\n        base = base[:-4]\n    return base, ext", "output": "Like os.path.splitext, but take off .tar too", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def non_empty_lines(path):\n    \"\"\"\n    \n    \"\"\"\n    with open(path) as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                yield line", "output": "Yield non-empty lines from file at path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def roll(ctx, dice: str):\n    \"\"\"\"\"\"\n    try:\n        rolls, limit = map(int, dice.split('d'))\n    except Exception:\n        await ctx.send('Format has to be in NdN!')\n        return\n\n    result = ', '.join(str(random.randint(1, limit)) for r in range(rolls))\n    await ctx.send(result)", "output": "Rolls a dice in NdN format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_sparse(arr):\n    \"\"\"\n    \n    \"\"\"\n    from pandas.core.arrays.sparse import SparseDtype\n\n    dtype = getattr(arr, 'dtype', arr)\n    return isinstance(dtype, SparseDtype)", "output": "Check whether an array-like is a 1-D pandas sparse array.\n\n    Check that the one-dimensional array-like is a pandas sparse array.\n    Returns True if it is a pandas sparse array, not another type of\n    sparse array.\n\n    Parameters\n    ----------\n    arr : array-like\n        Array-like to check.\n\n    Returns\n    -------\n    bool\n        Whether or not the array-like is a pandas sparse array.\n\n    See Also\n    --------\n    DataFrame.to_sparse : Convert DataFrame to a SparseDataFrame.\n    Series.to_sparse : Convert Series to SparseSeries.\n    Series.to_dense : Return dense representation of a Series.\n\n    Examples\n    --------\n    Returns `True` if the parameter is a 1-D pandas sparse array.\n\n    >>> is_sparse(pd.SparseArray([0, 0, 1, 0]))\n    True\n    >>> is_sparse(pd.SparseSeries([0, 0, 1, 0]))\n    True\n\n    Returns `False` if the parameter is not sparse.\n\n    >>> is_sparse(np.array([0, 0, 1, 0]))\n    False\n    >>> is_sparse(pd.Series([0, 1, 0, 0]))\n    False\n\n    Returns `False` if the parameter is not a pandas sparse array.\n\n    >>> from scipy.sparse import bsr_matrix\n    >>> is_sparse(bsr_matrix([0, 1, 0, 0]))\n    False\n\n    Returns `False` if the parameter has more than one dimension.\n\n    >>> df = pd.SparseDataFrame([389., 24., 80.5, np.nan],\n                                columns=['max_speed'],\n                                index=['falcon', 'parrot', 'lion', 'monkey'])\n    >>> is_sparse(df)\n    False\n    >>> is_sparse(df.max_speed)\n    True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def iteritems(self, key=_absent):\n        \"\"\"\n        \n        \"\"\"\n        if key is not _absent:\n            if key in self:\n                items = [(node.key, node.value) for node in self._map[key]]\n                return iter(items)\n            raise KeyError(key)\n        items = six.iteritems(self._map)\n        return iter((key, nodes[0].value) for (key, nodes) in items)", "output": "Parity with dict.iteritems() except the optional <key> parameter has\n        been added. If <key> is provided, only items with the provided key are\n        iterated over. KeyError is raised if <key> is provided and not in the\n        dictionary.\n\n        Example:\n          omd = omdict([(1,1), (1,11), (1,111), (2,2), (3,3)])\n          omd.iteritems(1) -> (1,1) -> (1,11) -> (1,111)\n          omd.iteritems() -> (1,1) -> (2,2) -> (3,3)\n\n        Raises: KeyError if <key> is provided and not in the dictionary.\n        Returns: An iterator over the items() of the dictionary, or only items\n          with the key <key> if <key> is provided.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dict_rpartition(\n        in_dict,\n        keys,\n        delimiter=DEFAULT_TARGET_DELIM,\n        ordered_dict=False):\n    '''\n    \n    '''\n    if delimiter in keys:\n        all_but_last_keys, _, last_key = keys.rpartition(delimiter)\n        ensure_dict_key(in_dict,\n                        all_but_last_keys,\n                        delimiter=delimiter,\n                        ordered_dict=ordered_dict)\n        dict_pointer = salt.utils.data.traverse_dict(in_dict,\n                                                     all_but_last_keys,\n                                                     default=None,\n                                                     delimiter=delimiter)\n    else:\n        dict_pointer = in_dict\n        last_key = keys\n    return dict_pointer, last_key", "output": "Helper function to:\n    - Ensure all but the last key in `keys` exist recursively in `in_dict`.\n    - Return the dict at the one-to-last key, and the last key\n\n    :param dict in_dict: The dict to work with.\n    :param str keys: The delimited string with one or more keys.\n    :param str delimiter: The delimiter to use in `keys`. Defaults to ':'.\n    :param bool ordered_dict: Create OrderedDicts if keys are missing.\n                              Default: create regular dicts.\n\n    :return tuple(dict, str)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keep_absolute_resample__roc_auc(X, y, model_generator, method_name, num_fcounts=11):\n    \"\"\" \n    \"\"\"\n    return __run_measure(measures.keep_resample, X, y, model_generator, method_name, 0, num_fcounts, sklearn.metrics.roc_auc_score)", "output": "Keep Absolute (resample)\n    xlabel = \"Max fraction of features kept\"\n    ylabel = \"ROC AUC\"\n    transform = \"identity\"\n    sort_order = 12", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extract_zip(zip_name, exclude_term=None):\n    \"\"\"\"\"\"\n\n    zip_dir = os.path.dirname(os.path.abspath(zip_name))\n\n    try:\n        with zipfile.ZipFile(zip_name) as z:\n\n            # write each zipped file out if it isn't a directory\n            files = [zip_file for zip_file in z.namelist() if not zip_file.endswith('/')]\n\n            print('Extracting %i files from %r.' % (len(files), zip_name))\n            for zip_file in files:\n\n                # remove any provided extra directory term from zip file\n                if exclude_term:\n                    dest_file = zip_file.replace(exclude_term, '')\n                else:\n                    dest_file = zip_file\n\n                dest_file = os.path.normpath(os.path.join(zip_dir, dest_file))\n                dest_dir = os.path.dirname(dest_file)\n\n                # make directory if it does not exist\n                if not os.path.isdir(dest_dir):\n                    os.makedirs(dest_dir)\n\n                # read file from zip, then write to new directory\n                data = z.read(zip_file)\n                with open(dest_file, 'wb') as f:\n                    f.write(encode_utf8(data))\n\n    except zipfile.error as e:\n        print(\"Bad zipfile (%r): %s\" % (zip_name, e))\n        raise e", "output": "Extracts a zip file to its containing directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _InternalUnpackAny(msg):\n  \"\"\"\n  \"\"\"\n  # TODO(amauryfa): Don't use the factory of generated messages.\n  # To make Any work with custom factories, use the message factory of the\n  # parent message.\n  # pylint: disable=g-import-not-at-top\n  from google.protobuf import symbol_database\n  factory = symbol_database.Default()\n\n  type_url = msg.type_url\n\n  if not type_url:\n    return None\n\n  # TODO(haberman): For now we just strip the hostname.  Better logic will be\n  # required.\n  type_name = type_url.split('/')[-1]\n  descriptor = factory.pool.FindMessageTypeByName(type_name)\n\n  if descriptor is None:\n    return None\n\n  message_class = factory.GetPrototype(descriptor)\n  message = message_class()\n\n  message.ParseFromString(msg.value)\n  return message", "output": "Unpacks Any message and returns the unpacked message.\n\n  This internal method is different from public Any Unpack method which takes\n  the target message as argument. _InternalUnpackAny method does not have\n  target message type and need to find the message type in descriptor pool.\n\n  Args:\n    msg: An Any message to be unpacked.\n\n  Returns:\n    The unpacked message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_(key, value, profile=None):\n    '''\n    \n    '''\n    if '?' in key:\n        __utils__['versions.warn_until'](\n            'Neon',\n            (\n                'Using ? to seperate between the path and key for vault has been deprecated '\n                'and will be removed in {version}.  Please just use a /.'\n            ),\n        )\n        path, key = key.split('?')\n    else:\n        path, key = key.rsplit('/', 1)\n\n    try:\n        url = 'v1/{0}'.format(path)\n        data = {key: value}\n        response = __utils__['vault.make_request'](\n            'POST',\n            url,\n            profile,\n            json=data)\n\n        if response.status_code != 204:\n            response.raise_for_status()\n        return True\n    except Exception as e:\n        log.error('Failed to write secret! %s: %s', type(e).__name__, e)\n        raise salt.exceptions.CommandExecutionError(e)", "output": "Set a key/value pair in the vault service", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sed_esc(string, escape_all=False):\n    '''\n    \n    '''\n    special_chars = \"^.[$()|*+?{\"\n    string = string.replace(\"'\", \"'\\\"'\\\"'\").replace(\"/\", \"\\\\/\")\n    if escape_all is True:\n        for char in special_chars:\n            string = string.replace(char, \"\\\\\" + char)\n    return string", "output": "Escape single quotes and forward slashes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_(key,\n         value,\n         host=DEFAULT_HOST,\n         port=DEFAULT_PORT,\n         time=DEFAULT_TIME,\n         min_compress_len=DEFAULT_MIN_COMPRESS_LEN):\n    '''\n    \n    '''\n    if not isinstance(time, six.integer_types):\n        raise SaltInvocationError('\\'time\\' must be an integer')\n    if not isinstance(min_compress_len, six.integer_types):\n        raise SaltInvocationError('\\'min_compress_len\\' must be an integer')\n    conn = _connect(host, port)\n    _check_stats(conn)\n    return conn.set(key, value, time, min_compress_len)", "output": "Set a key on the memcached server, overwriting the value if it exists.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' memcached.set <key> <value>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_diskgroup(cache_disk_id, data_accessibility=True,\n                     service_instance=None):\n    '''\n    \n    '''\n    log.trace('Validating diskgroup input')\n    host_ref = _get_proxy_target(service_instance)\n    hostname = __proxy__['esxi.get_details']()['esxi_host']\n    diskgroups = \\\n            salt.utils.vmware.get_diskgroups(host_ref,\n                                             cache_disk_ids=[cache_disk_id])\n    if not diskgroups:\n        raise VMwareObjectRetrievalError(\n            'No diskgroup with cache disk id \\'{0}\\' was found in ESXi '\n            'host \\'{1}\\''.format(cache_disk_id, hostname))\n    log.trace('data accessibility = %s', data_accessibility)\n    salt.utils.vsan.remove_diskgroup(\n        service_instance, host_ref, diskgroups[0],\n        data_accessibility=data_accessibility)\n    return True", "output": "Remove the diskgroup with the specified cache disk.\n\n    cache_disk_id\n        The canonical name of the cache disk.\n\n    data_accessibility\n        Specifies whether to ensure data accessibility. Default value is True.\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter/ESXi host.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.remove_diskgroup cache_disk_id='naa.000000000000001'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_images(self, search_file, source_file):\n        \"\"\"\"\"\"\n        self.search_file, self.source_file = search_file, source_file\n        self.im_search, self.im_source = imread(self.search_file), imread(self.source_file)\n        # \u521d\u59cb\u5316\u5bf9\u8c61\n        self.check_macthing_object = CheckKeypointResult(self.im_search, self.im_source)", "output": "\u52a0\u8f7d\u5f85\u5339\u914d\u56fe\u7247.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_index_day_adv(\n        code,\n        start, end=None,\n        if_drop_index=True,\n        # \ud83d\udee0 todo collections \u53c2\u6570\u6ca1\u6709\u7528\u5230\uff0c \u4e14\u6570\u636e\u5e93\u662f\u56fa\u5b9a\u7684\uff0c \u8fd9\u4e2a\u53d8\u91cf\u540e\u671f\u53bb\u6389\n        collections=DATABASE.index_day):\n    '''\n    \n    '''\n    '\u83b7\u53d6\u6307\u6570\u65e5\u7ebf'\n    end = start if end is None else end\n    start = str(start)[0:10]\n    end = str(end)[0:10]\n\n    # \ud83d\udee0 todo \u62a5\u544a\u9519\u8bef \u5982\u679c\u5f00\u59cb\u65f6\u95f4 \u5728 \u7ed3\u675f\u65f6\u95f4\u4e4b\u540e\n    # \ud83d\udee0 todo \u5982\u679c\u76f8\u7b49\n\n    res = QA_fetch_index_day(code, start, end, format='pd')\n    if res is None:\n        print(\"QA Error QA_fetch_index_day_adv parameter code=%s start=%s end=%s call QA_fetch_index_day return None\" % (\n            code, start, end))\n        return None\n    else:\n        res_set_index = res.set_index(['date', 'code'], drop=if_drop_index)\n        # if res_set_index is None:\n        #     print(\"QA Error QA_fetch_index_day_adv set index 'date, code' return None\")\n        #     return None\n        return QA_DataStruct_Index_day(res_set_index)", "output": ":param code: code:  \u5b57\u7b26\u4e32str eg 600085\n    :param start:  \u5b57\u7b26\u4e32str \u5f00\u59cb\u65e5\u671f eg 2011-01-01\n    :param end:  \u5b57\u7b26\u4e32str \u7ed3\u675f\u65e5\u671f eg 2011-05-01\n    :param if_drop_index: Ture False \uff0c dataframe drop index or not\n    :param collections:  mongodb \u6570\u636e\u5e93\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_current(self, panel):\n        \"\"\"\n        \n        \"\"\"\n        where = slice(self._start_index, self._pos)\n        self.buffer.values[:, where, :] = panel.values", "output": "Set the values stored in our current in-view data to be values of the\n        passed panel.  The passed panel must have the same indices as the panel\n        that would be returned by self.get_current.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def benchmark_data(self):\n        \"\"\"\n        \n        \"\"\"\n        return self.fetch[self.benchmark_type](\n            self.benchmark_code,\n            self.account.start_date,\n            self.account.end_date\n        )", "output": "\u57fa\u51c6\u7ec4\u5408\u7684\u884c\u60c5\u6570\u636e(\u4e00\u822c\u662f\u7ec4\u5408,\u53ef\u4ee5\u8c03\u6574)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flavor_create(name,      # pylint: disable=C0103\n                  flavor_id=0,      # pylint: disable=C0103\n                  ram=0,\n                  disk=0,\n                  vcpus=1,\n                  is_public=True,\n                  profile=None, **kwargs):\n    '''\n    \n    '''\n    conn = _auth(profile, **kwargs)\n    return conn.flavor_create(\n        name,\n        flavor_id,\n        ram,\n        disk,\n        vcpus,\n        is_public\n    )", "output": "Add a flavor to nova (nova flavor-create). The following parameters are\n    required:\n\n    name\n        Name of the new flavor (must be first)\n    flavor_id\n        Unique integer ID for the new flavor\n    ram\n        Memory size in MB\n    disk\n        Disk size in GB\n    vcpus\n        Number of vcpus\n    is_public\n        Whether flavor is public. Default is True.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' nova.flavor_create myflavor flavor_id=6 ram=4096 disk=10 vcpus=1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _compute_diff(existing, expected):\n\n    '''\n    \n    '''\n\n    diff = {}\n\n    for key in ['location', 'contact', 'chassis_id']:\n        if existing.get(key) != expected.get(key):\n            _create_diff(diff,\n                         _valid_str,\n                         key,\n                         existing.get(key),\n                         expected.get(key))\n\n    for key in ['community']:  # for the moment only onen\n        if existing.get(key) != expected.get(key):\n            _create_diff(diff,\n                         _valid_dict,\n                         key,\n                         existing.get(key),\n                         expected.get(key))\n\n    return diff", "output": "Computes the differences between the existing and the expected SNMP config.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cleanup_virtualenv(bare=True):\n    \"\"\"\"\"\"\n    if not bare:\n        click.echo(crayons.red(\"Environment creation aborted.\"))\n    try:\n        # Delete the virtualenv.\n        vistir.path.rmtree(project.virtualenv_location)\n    except OSError as e:\n        click.echo(\n            \"{0} An error occurred while removing {1}!\".format(\n                crayons.red(\"Error: \", bold=True),\n                crayons.green(project.virtualenv_location),\n            ),\n            err=True,\n        )\n        click.echo(crayons.blue(e), err=True)", "output": "Removes the virtualenv directory from the system.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_lowstate(**kwargs):\n    '''\n    \n    '''\n    __opts__['grains'] = __grains__\n    opts = salt.utils.state.get_sls_opts(__opts__, **kwargs)\n    st_ = salt.client.ssh.state.SSHHighState(\n            opts,\n            __pillar__,\n            __salt__,\n            __context__['fileclient'])\n    st_.push_active()\n    chunks = st_.compile_low_chunks()\n    _cleanup_slsmod_low_data(chunks)\n    return chunks", "output": "List out the low data that will be applied to this minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.show_lowstate", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unlock_password(name, root=None):\n    '''\n    \n    '''\n    pre_info = info(name, root=root)\n    if not pre_info['name']:\n        return False\n\n    if not pre_info['passwd'].startswith('!'):\n        return True\n\n    cmd = ['passwd']\n\n    if root is not None:\n        cmd.extend(('-R', root))\n\n    cmd.extend(('-u', name))\n\n    __salt__['cmd.run'](cmd, python_shell=False)\n    return not info(name, root=root)['passwd'].startswith('!')", "output": ".. versionadded:: 2016.11.0\n\n    Unlock the password from name user\n\n    name\n        User to unlock\n\n    root\n        Directory to chroot into\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' shadow.unlock_password username", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_privileges(name, **client_args):\n    '''\n    \n    '''\n    client = _client(**client_args)\n\n    res = {}\n    for item in client.get_list_privileges(name):\n        res[item['database']] = item['privilege'].split()[0].lower()\n    return res", "output": "List privileges from a user.\n\n    name\n        Name of the user from whom privileges will be listed.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb.list_privileges <name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def map_exp_ids(self, exp, positions=False):\n        \"\"\"\n        \"\"\"\n        if positions:\n            exp = [('%s_%s' % (\n                self.indexed_string.word(x[0]),\n                '-'.join(\n                    map(str,\n                        self.indexed_string.string_position(x[0])))), x[1])\n                   for x in exp]\n        else:\n            exp = [(self.indexed_string.word(x[0]), x[1]) for x in exp]\n        return exp", "output": "Maps ids to words or word-position strings.\n\n        Args:\n            exp: list of tuples [(id, weight), (id,weight)]\n            positions: if True, also return word positions\n\n        Returns:\n            list of tuples (word, weight), or (word_positions, weight) if\n            examples: ('bad', 1) or ('bad_3-6-12', 1)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_states(self, merge_multi_context=True):\n        \"\"\"\n        \"\"\"\n        assert self.binded and self.params_initialized\n        return self._curr_module.get_states(merge_multi_context=merge_multi_context)", "output": "Gets states from all devices.\n\n        Parameters\n        ----------\n        merge_multi_context : bool\n            Default is `True`. In the case when data-parallelism is used, the states\n            will be collected from multiple devices. A `True` value indicate that we\n            should merge the collected results so that they look like from a single\n            executor.\n\n        Returns\n        -------\n        list of NDArrays or list of list of NDArrays\n            If `merge_multi_context` is ``True``, it is like ``[out1, out2]``. Otherwise, it\n            is like ``[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]``. All the output\n            elements are `NDArray`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def maybe_upcast_for_op(obj):\n    \"\"\"\n    \n    \"\"\"\n    if type(obj) is datetime.timedelta:\n        # GH#22390  cast up to Timedelta to rely on Timedelta\n        # implementation; otherwise operation against numeric-dtype\n        # raises TypeError\n        return pd.Timedelta(obj)\n    elif isinstance(obj, np.timedelta64) and not isna(obj):\n        # In particular non-nanosecond timedelta64 needs to be cast to\n        #  nanoseconds, or else we get undesired behavior like\n        #  np.timedelta64(3, 'D') / 2 == np.timedelta64(1, 'D')\n        # The isna check is to avoid casting timedelta64(\"NaT\"), which would\n        #  return NaT and incorrectly be treated as a datetime-NaT.\n        return pd.Timedelta(obj)\n    elif isinstance(obj, np.ndarray) and is_timedelta64_dtype(obj):\n        # GH#22390 Unfortunately we need to special-case right-hand\n        # timedelta64 dtypes because numpy casts integer dtypes to\n        # timedelta64 when operating with timedelta64\n        return pd.TimedeltaIndex(obj)\n    return obj", "output": "Cast non-pandas objects to pandas types to unify behavior of arithmetic\n    and comparison operations.\n\n    Parameters\n    ----------\n    obj: object\n\n    Returns\n    -------\n    out : object\n\n    Notes\n    -----\n    Be careful to call this *after* determining the `name` attribute to be\n    attached to the result of the arithmetic operation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vm_monitoring(name, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The vm_monitoring action must be called with -a or --action.'\n        )\n\n    server, user, password = _get_xml_rpc()\n    auth = ':'.join([user, password])\n    vm_id = int(get_vm_id(kwargs={'name': name}))\n    response = server.one.vm.monitoring(auth, vm_id)\n\n    if response[0] is False:\n        log.error(\n            'There was an error retrieving the specified VM\\'s monitoring '\n            'information.'\n        )\n        return {}\n    else:\n        info = {}\n        for vm_ in _get_xml(response[1]):\n            info[vm_.find('ID').text] = _xml_to_dict(vm_)\n        return info", "output": "Returns the monitoring records for a given virtual machine. A VM name must be\n    supplied.\n\n    The monitoring information returned is a list of VM elements. Each VM element\n    contains the complete dictionary of the VM with the updated information returned\n    by the poll action.\n\n    .. versionadded:: 2016.3.0\n\n    name\n        The name of the VM for which to gather monitoring records.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a vm_monitoring my-vm", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def minibatch_by_words(items, size, tuples=True, count_words=len):\n    \"\"\"\"\"\"\n    if isinstance(size, int):\n        size_ = itertools.repeat(size)\n    else:\n        size_ = size\n    items = iter(items)\n    while True:\n        batch_size = next(size_)\n        batch = []\n        while batch_size >= 0:\n            try:\n                if tuples:\n                    doc, gold = next(items)\n                else:\n                    doc = next(items)\n            except StopIteration:\n                if batch:\n                    yield batch\n                return\n            batch_size -= count_words(doc)\n            if tuples:\n                batch.append((doc, gold))\n            else:\n                batch.append(doc)\n        if batch:\n            yield batch", "output": "Create minibatches of a given number of words.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stonith_present(name, stonith_id, stonith_device_type, stonith_device_options=None, cibname=None):\n    '''\n    \n    '''\n    return _item_present(name=name,\n                         item='stonith',\n                         item_id=stonith_id,\n                         item_type=stonith_device_type,\n                         extra_args=stonith_device_options,\n                         cibname=cibname)", "output": "Ensure that a fencing resource is created\n\n    Should be run on one cluster node only\n    (there may be races)\n    Can only be run on a node with a functional pacemaker/corosync\n\n    name\n        Irrelevant, not used (recommended: pcs_stonith__created_{{stonith_id}})\n    stonith_id\n        name for the stonith resource\n    stonith_device_type\n        name of the stonith agent fence_eps, fence_xvm f.e.\n    stonith_device_options\n        additional options for creating the stonith resource\n    cibname\n        use a cached CIB-file named like cibname instead of the live CIB\n\n    Example:\n\n    .. code-block:: yaml\n\n        pcs_stonith__created_eps_fence:\n            pcs.stonith_present:\n                - stonith_id: eps_fence\n                - stonith_device_type: fence_eps\n                - stonith_device_options:\n                    - 'pcmk_host_map=node1.example.org:01;node2.example.org:02'\n                    - 'ipaddr=myepsdevice.example.org'\n                    - 'power_wait=5'\n                    - 'verbose=1'\n                    - 'debug=/var/log/pcsd/eps_fence.log'\n                    - 'login=hidden'\n                    - 'passwd=hoonetorg'\n                - cibname: cib_for_stonith", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_css(self):\n        ''' \n\n        '''\n        if self.a == 1.0:\n            return \"rgb(%d, %d, %d)\" % (self.r, self.g, self.b)\n        else:\n            return \"rgba(%d, %d, %d, %s)\" % (self.r, self.g, self.b, self.a)", "output": "Generate the CSS representation of this RGB color.\n\n        Returns:\n            str, ``\"rgb(...)\"`` or ``\"rgba(...)\"``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _list_items(queue):\n    '''\n    \n    '''\n    con = _conn(queue)\n    with con:\n        cur = con.cursor()\n        cmd = 'SELECT name FROM {0}'.format(queue)\n        log.debug('SQL Query: %s', cmd)\n        cur.execute(cmd)\n        contents = cur.fetchall()\n    return contents", "output": "Private function to list contents of a queue", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_frontends(socket=DEFAULT_SOCKET_URL):\n    '''\n    \n    '''\n    ha_conn = _get_conn(socket)\n    ha_cmd = haproxy.cmds.showFrontends()\n    return ha_conn.sendCmd(ha_cmd)", "output": "Show HaProxy frontends\n\n    socket\n        haproxy stats socket, default ``/var/run/haproxy.sock``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' haproxy.show_frontends", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _master_supports_ipv6():\n    '''\n    \n    '''\n    master_fqdn = salt.utils.network.get_fqhostname()\n    pillar_util = salt.utils.master.MasterPillarUtil(master_fqdn,\n                                                     tgt_type='glob',\n                                                     use_cached_grains=False,\n                                                     grains_fallback=False,\n                                                     opts=__opts__)\n    grains_data = pillar_util.get_minion_grains()\n    ipv6_addresses = grains_data[master_fqdn]['ipv6']\n    for address in ipv6_addresses:\n        if _valid_ip6(address):\n            return True\n    return False", "output": "Check if the salt master has a valid and\n    routable IPv6 address available", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_clonespec_for_valid_snapshot(config_spec, object_ref, reloc_spec, template, vm_):\n    '''\n    \n    '''\n    moving = True\n    if QUICK_LINKED_CLONE == vm_['snapshot']['disk_move_type']:\n        reloc_spec.diskMoveType = QUICK_LINKED_CLONE\n    elif CURRENT_STATE_LINKED_CLONE == vm_['snapshot']['disk_move_type']:\n        reloc_spec.diskMoveType = CURRENT_STATE_LINKED_CLONE\n    elif COPY_ALL_DISKS_FULL_CLONE == vm_['snapshot']['disk_move_type']:\n        reloc_spec.diskMoveType = COPY_ALL_DISKS_FULL_CLONE\n    elif FLATTEN_DISK_FULL_CLONE == vm_['snapshot']['disk_move_type']:\n        reloc_spec.diskMoveType = FLATTEN_DISK_FULL_CLONE\n    else:\n        moving = False\n\n    if moving:\n        return build_clonespec(config_spec, object_ref, reloc_spec, template)\n\n    return None", "output": "return clonespec only if values are valid", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def speakerDiarizationEvaluateScript(folder_name, ldas):\n    '''\n        \n    '''\n    types = ('*.wav',  )\n    wavFilesList = []\n    for files in types:\n        wavFilesList.extend(glob.glob(os.path.join(folder_name, files)))    \n    \n    wavFilesList = sorted(wavFilesList)\n\n    # get number of unique speakers per file (from ground-truth)    \n    N = []\n    for wav_file in wavFilesList:        \n        gt_file = wav_file.replace('.wav', '.segments');\n        if os.path.isfile(gt_file):\n            [seg_start, seg_end, seg_labs] = readSegmentGT(gt_file)\n            N.append(len(list(set(seg_labs))))\n        else:\n            N.append(-1)\n    \n    for l in ldas:\n        print(\"LDA = {0:d}\".format(l))\n        for i, wav_file in enumerate(wavFilesList):\n            speakerDiarization(wav_file, N[i], 2.0, 0.2, 0.05, l, plot_res=False)\n        print", "output": "This function prints the cluster purity and speaker purity for\n        each WAV file stored in a provided directory (.SEGMENT files\n         are needed as ground-truth)\n        ARGUMENTS:\n            - folder_name:     the full path of the folder where the WAV and\n                              SEGMENT (ground-truth) files are stored\n            - ldas:           a list of LDA dimensions (0 for no LDA)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def with_pattern(pattern, regex_group_count=None):\n    \"\"\"\n    \"\"\"\n    def decorator(func):\n        func.pattern = pattern\n        func.regex_group_count = regex_group_count\n        return func\n    return decorator", "output": "Attach a regular expression pattern matcher to a custom type converter\n    function.\n\n    This annotates the type converter with the :attr:`pattern` attribute.\n\n    EXAMPLE:\n        >>> import parse\n        >>> @parse.with_pattern(r\"\\d+\")\n        ... def parse_number(text):\n        ...     return int(text)\n\n    is equivalent to:\n\n        >>> def parse_number(text):\n        ...     return int(text)\n        >>> parse_number.pattern = r\"\\d+\"\n\n    :param pattern: regular expression pattern (as text)\n    :param regex_group_count: Indicates how many regex-groups are in pattern.\n    :return: wrapped function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_iam_policy(self):\n        \"\"\"\n        \"\"\"\n        instance_admin_client = self._client.instance_admin_client\n        resp = instance_admin_client.get_iam_policy(resource=self.name)\n        return Policy.from_pb(resp)", "output": "Gets the access control policy for an instance resource.\n\n        For example:\n\n        .. literalinclude:: snippets.py\n            :start-after: [START bigtable_get_iam_policy]\n            :end-before: [END bigtable_get_iam_policy]\n\n        :rtype: :class:`google.cloud.bigtable.policy.Policy`\n        :returns: The current IAM policy of this instance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_config_status():\n    '''\n    \n    '''\n    cmd = 'Get-DscConfigurationStatus | ' \\\n          'Select-Object -Property HostName, Status, MetaData, ' \\\n          '@{Name=\"StartDate\";Expression={Get-Date ($_.StartDate) -Format g}}, ' \\\n          'Type, Mode, RebootRequested, NumberofResources'\n    try:\n        return _pshell(cmd, ignore_retcode=True)\n    except CommandExecutionError as exc:\n        if 'No status information available' in exc.info['stderr']:\n            raise CommandExecutionError('Not Configured')\n        raise", "output": "Get the status of the current DSC Configuration\n\n    Returns:\n        dict: A dictionary representing the status of the current DSC\n            Configuration on the machine\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' dsc.get_config_status", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_line(self):\r\n        \"\"\"\"\"\"\r\n        cursor = self.textCursor()\r\n        if self.has_selected_text():\r\n            self.extend_selection_to_complete_lines()\r\n            start_pos, end_pos = cursor.selectionStart(), cursor.selectionEnd()\r\n            cursor.setPosition(start_pos)\r\n        else:\r\n            start_pos = end_pos = cursor.position()\r\n        cursor.beginEditBlock()\r\n        cursor.setPosition(start_pos)\r\n        cursor.movePosition(QTextCursor.StartOfBlock)\r\n        while cursor.position() <= end_pos:\r\n            cursor.movePosition(QTextCursor.EndOfBlock, QTextCursor.KeepAnchor)\r\n            if cursor.atEnd():\r\n                break\r\n            cursor.movePosition(QTextCursor.NextBlock, QTextCursor.KeepAnchor)\r\n        cursor.removeSelectedText()\r\n        cursor.endEditBlock()\r\n        self.ensureCursorVisible()", "output": "Delete current line", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def var(self):\n        \"\"\"\"\"\"\n        if self._var is None:\n            self._var = symbol.var(self.name, shape=self.shape, dtype=self.dtype,\n                                   lr_mult=self.lr_mult, wd_mult=self.wd_mult,\n                                   init=self.init, stype=self._stype)\n        return self._var", "output": "Returns a symbol representing this parameter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_floatingip(self, floating_network, port=None):\n        '''\n        \n        '''\n        net_id = self._find_network_id(floating_network)\n        body = {'floating_network_id': net_id}\n        if port:\n            port_id = self._find_port_id(port)\n            body['port_id'] = port_id\n\n        return self.network_conn.create_floatingip(body={'floatingip': body})", "output": "Creates a new floatingip", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def retry(target_exception, tries=4, delay_s=1, backoff=2):\n    \"\"\"\n    \"\"\"\n    import time\n    from functools import wraps\n\n    def decorated_retry(f):\n        @wraps(f)\n        def f_retry(*args, **kwargs):\n            mtries, mdelay = tries, delay_s\n            while mtries > 1:\n                try:\n                    return f(*args, **kwargs)\n                except target_exception as e:\n                    logging.warning(\"Exception: %s, Retrying in %d seconds...\", str(e), mdelay)\n                    time.sleep(mdelay)\n                    mtries -= 1\n                    mdelay *= backoff\n            return f(*args, **kwargs)\n\n        return f_retry  # true decorator\n\n    return decorated_retry", "output": "Retry calling the decorated function using an exponential backoff.\n\n    http://www.saltycrane.com/blog/2009/11/trying-out-retry-decorator-python/\n    original from: http://wiki.python.org/moin/PythonDecoratorLibrary#Retry\n\n    :param target_exception: the exception to check. may be a tuple of\n        exceptions to check\n    :type target_exception: Exception or tuple\n    :param tries: number of times to try (not retry) before giving up\n    :type tries: int\n    :param delay_s: initial delay between retries in seconds\n    :type delay_s: int\n    :param backoff: backoff multiplier e.g. value of 2 will double the delay\n        each retry\n    :type backoff: int", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_variable(self, name, initializer):\n    \"\"\"\n    \n    \"\"\"\n    v = tf.get_variable(name, shape=initializer.shape,\n                        initializer=(lambda shape, dtype, partition_info:\n                                     initializer),\n                        trainable=self.training)\n    return v", "output": "Create and initialize a variable using a numpy array and set trainable.\n    :param name: (required str) name of the variable\n    :param initializer: a numpy array or a tensor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        ''''''\n        logger.info(\"processor starting...\")\n\n        while not self._quit:\n            try:\n                task, response = self.inqueue.get(timeout=1)\n                self.on_task(task, response)\n                self._exceptions = 0\n            except Queue.Empty as e:\n                continue\n            except KeyboardInterrupt:\n                break\n            except Exception as e:\n                logger.exception(e)\n                self._exceptions += 1\n                if self._exceptions > self.EXCEPTION_LIMIT:\n                    break\n                continue\n\n        logger.info(\"processor exiting...\")", "output": "Run loop", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def export(self, fn:PathOrStr, **kwargs):\n        \"\"\n        pickle.dump(self.get_state(**kwargs), open(fn, 'wb'))", "output": "Export the minimal state and save it in `fn` to load an empty version for inference.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _smooth(values: List[float], beta: float) -> List[float]:\n    \"\"\"  \"\"\"\n    avg_value = 0.\n    smoothed = []\n    for i, value in enumerate(values):\n        avg_value = beta * avg_value + (1 - beta) * value\n        smoothed.append(avg_value / (1 - beta ** (i + 1)))\n    return smoothed", "output": "Exponential smoothing of values", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def match(version, match_expr):\n    \"\"\"\n    \"\"\"\n    prefix = match_expr[:2]\n    if prefix in ('>=', '<=', '==', '!='):\n        match_version = match_expr[2:]\n    elif prefix and prefix[0] in ('>', '<'):\n        prefix = prefix[0]\n        match_version = match_expr[1:]\n    else:\n        raise ValueError(\"match_expr parameter should be in format <op><ver>, \"\n                         \"where <op> is one of \"\n                         \"['<', '>', '==', '<=', '>=', '!=']. \"\n                         \"You provided: %r\" % match_expr)\n\n    possibilities_dict = {\n        '>': (1,),\n        '<': (-1,),\n        '==': (0,),\n        '!=': (-1, 1),\n        '>=': (0, 1),\n        '<=': (-1, 0)\n    }\n\n    possibilities = possibilities_dict[prefix]\n    cmp_res = compare(version, match_version)\n\n    return cmp_res in possibilities", "output": "Compare two versions through a comparison\n\n    :param str version: a version string\n    :param str match_expr: operator and version; valid operators are\n          <   smaller than\n          >   greater than\n          >=  greator or equal than\n          <=  smaller or equal than\n          ==  equal\n          !=  not equal\n    :return: True if the expression matches the version, otherwise False\n    :rtype: bool\n\n    >>> import semver\n    >>> semver.match(\"2.0.0\", \">=1.0.0\")\n    True\n    >>> semver.match(\"1.0.0\", \">1.0.0\")\n    False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def beacon(config):\n    '''\n    \n    '''\n    log.debug('Executing napalm beacon with config:')\n    log.debug(config)\n    ret = []\n    for mod in config:\n        if not mod:\n            continue\n        event = {}\n        fun = mod.keys()[0]\n        fun_cfg = mod.values()[0]\n        args = fun_cfg.pop('_args', [])\n        kwargs = fun_cfg.pop('_kwargs', {})\n        log.debug('Executing %s with %s and %s', fun, args, kwargs)\n        fun_ret = __salt__[fun](*args, **kwargs)\n        log.debug('Got the reply from the minion:')\n        log.debug(fun_ret)\n        if not fun_ret.get('result', False):\n            log.error('Error whilst executing %s', fun)\n            log.error(fun_ret)\n            continue\n        fun_ret_out = fun_ret['out']\n        log.debug('Comparing to:')\n        log.debug(fun_cfg)\n        try:\n            fun_cmp_result = _compare(fun_cfg, fun_ret_out)\n        except Exception as err:\n            log.error(err, exc_info=True)\n            # catch any exception and continue\n            # to not jeopardise the execution of the next function in the list\n            continue\n        log.debug('Result of comparison: %s', fun_cmp_result)\n        if fun_cmp_result:\n            log.info('Matched %s with %s', fun, fun_cfg)\n            event['tag'] = '{os}/{fun}'.format(os=__grains__['os'], fun=fun)\n            event['fun'] = fun\n            event['args'] = args\n            event['kwargs'] = kwargs\n            event['data'] = fun_ret\n            event['match'] = fun_cfg\n            log.debug('Queueing event:')\n            log.debug(event)\n            ret.append(event)\n    log.debug('NAPALM beacon generated the events:')\n    log.debug(ret)\n    return ret", "output": "Watch napalm function and fire events.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def frustum(left, right, bottom, top, znear, zfar):\n  \"\"\"\"\"\"\n  assert right != left\n  assert bottom != top\n  assert znear != zfar\n\n  M = np.zeros((4, 4), dtype=np.float32)\n  M[0, 0] = +2.0 * znear / (right - left)\n  M[2, 0] = (right + left) / (right - left)\n  M[1, 1] = +2.0 * znear / (top - bottom)\n  M[3, 1] = (top + bottom) / (top - bottom)\n  M[2, 2] = -(zfar + znear) / (zfar - znear)\n  M[3, 2] = -2.0 * znear * zfar / (zfar - znear)\n  M[2, 3] = -1.0\n  return M", "output": "Create view frustum matrix.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sub_datetime_arraylike(self, other):\n        \"\"\"\"\"\"\n        if len(self) != len(other):\n            raise ValueError(\"cannot add indices of unequal length\")\n\n        if isinstance(other, np.ndarray):\n            assert is_datetime64_dtype(other)\n            other = type(self)(other)\n\n        if not self._has_same_tz(other):\n            # require tz compat\n            raise TypeError(\"{cls} subtraction must have the same \"\n                            \"timezones or no timezones\"\n                            .format(cls=type(self).__name__))\n\n        self_i8 = self.asi8\n        other_i8 = other.asi8\n        arr_mask = self._isnan | other._isnan\n        new_values = checked_add_with_arr(self_i8, -other_i8,\n                                          arr_mask=arr_mask)\n        if self._hasnans or other._hasnans:\n            new_values[arr_mask] = iNaT\n        return new_values.view('timedelta64[ns]')", "output": "subtract DatetimeArray/Index or ndarray[datetime64]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __hosting_wechat_img(self, content_info, hosting_callback):\n        \"\"\"\n        \"\"\"\n        assert callable(hosting_callback)\n\n        content_img_list = content_info.pop(\"content_img_list\")\n        content_html = content_info.pop(\"content_html\")\n        for idx, img_url in enumerate(content_img_list):\n            hosting_img_url = hosting_callback(img_url)\n            if not hosting_img_url:\n                # todo \u5b9a\u4e49\u6807\u51c6\u5f02\u5e38\n                raise Exception()\n            content_img_list[idx] = hosting_img_url\n            content_html = content_html.replace(img_url, hosting_img_url)\n\n        return dict(content_img_list=content_img_list, content_html=content_html)", "output": "\u5c06\u5fae\u4fe1\u660e\u7ec6\u4e2d\u56fe\u7247\u6258\u7ba1\u5230\u4e91\u7aef\uff0c\u540c\u65f6\u5c06html\u9875\u9762\u4e2d\u7684\u5bf9\u5e94\u56fe\u7247\u66ff\u6362\n\n        Parameters\n        ----------\n        content_info : dict \u5fae\u4fe1\u6587\u7ae0\u660e\u7ec6\u5b57\u5178\n            {\n                'content_img_list': [], # \u4ece\u5fae\u4fe1\u6587\u7ae0\u89e3\u6790\u51fa\u7684\u539f\u59cb\u56fe\u7247\u5217\u8868\n                'content_html': '', # \u4ece\u5fae\u4fe1\u6587\u7ae0\u89e3\u6790\u51fa\u6587\u7ae0\u7684\u5185\u5bb9\n            }\n        hosting_callback : callable\n            \u6258\u7ba1\u56de\u8c03\u51fd\u6570\uff0c\u4f20\u5165\u5355\u4e2a\u56fe\u7247\u94fe\u63a5\uff0c\u8fd4\u56de\u6258\u7ba1\u540e\u7684\u56fe\u7247\u94fe\u63a5\n\n        Returns\n        -------\n        dict\n            {\n                'content_img_list': '', # \u6258\u7ba1\u540e\u7684\u56fe\u7247\u5217\u8868\n                'content_html': '',  # \u56fe\u7247\u94fe\u63a5\u4e3a\u6258\u7ba1\u540e\u7684\u56fe\u7247\u94fe\u63a5\u5185\u5bb9\n            }", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write(self, s, flush=True):\n        \"\"\"\n        \"\"\"\n        return self._writeb(s, flush=flush)", "output": "Write bytes to the pseudoterminal.\n        \n        Returns the number of bytes written.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bokehjsdir(dev=False):\n    \"\"\" \n    \"\"\"\n    dir1 = join(ROOT_DIR, '..', 'bokehjs', 'build')\n    dir2 = join(serverdir(), 'static')\n    if dev and isdir(dir1):\n        return dir1\n    else:\n        return dir2", "output": "Get the location of the bokehjs source files. If dev is True,\n    the files in bokehjs/build are preferred. Otherwise uses the files\n    in bokeh/server/static.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_derived (type, base):\n    \"\"\" \n    \"\"\"\n    assert isinstance(type, basestring)\n    assert isinstance(base, basestring)\n    # TODO: this isn't very efficient, especially for bases close to type\n    if base in all_bases (type):\n        return True\n    else:\n        return False", "output": "Returns true if 'type' is 'base' or has 'base' as its direct or indirect base.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_event_rules_for_lambda(self, lambda_arn):\n        \"\"\"\n        \n        \"\"\"\n        rule_names = self.get_event_rule_names_for_lambda(lambda_arn=lambda_arn)\n        return [self.events_client.describe_rule(Name=r) for r in rule_names]", "output": "Get all of the rule details associated with this function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _int_values(self, shape):\n        \"\"\"\n        \n        \"\"\"\n        return (self.state.randint(low=0, high=100, size=shape)\n                .astype('int64'))", "output": "Return uniformly-distributed integers between 0 and 100.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_primary_at(source_code, offset, retry=True):\r\n    \"\"\"\r\n    \"\"\"\r\n    obj = ''\r\n    left = re.split(r\"[^0-9a-zA-Z_.]\", source_code[:offset])\r\n    if left and left[-1]:\r\n        obj = left[-1]\r\n    right = re.split(r\"\\W\", source_code[offset:])\r\n    if right and right[0]:\r\n        obj += right[0]\r\n    if obj and obj[0].isdigit():\r\n        obj = ''\r\n    # account for opening chars with no text to the right\r\n    if not obj and retry and offset and source_code[offset - 1] in '([.':\r\n        return get_primary_at(source_code, offset - 1, retry=False)\r\n    return obj", "output": "Return Python object in *source_code* at *offset*\r\n    Periods to the left of the cursor are carried forward \r\n      e.g. 'functools.par^tial' would yield 'functools.partial'\r\n    Retry prevents infinite recursion: retry only once", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_evaluated_individuals_(self, result_score_list, eval_individuals_str, operator_counts, stats_dicts):\n        \"\"\"\n        \"\"\"\n        for result_score, individual_str in zip(result_score_list, eval_individuals_str):\n            if type(result_score) in [float, np.float64, np.float32]:\n                self.evaluated_individuals_[individual_str] = self._combine_individual_stats(operator_counts[individual_str],\n                                                                                             result_score,\n                                                                                             stats_dicts[individual_str])\n            else:\n                raise ValueError('Scoring function does not return a float.')", "output": "Update self.evaluated_individuals_ and error message during pipeline evaluation.\n\n        Parameters\n        ----------\n        result_score_list: list\n            A list of CV scores for evaluated pipelines\n        eval_individuals_str: list\n            A list of strings for evaluated pipelines\n        operator_counts: dict\n            A dict where 'key' is the string representation of an individual and 'value' is the number of operators in the pipeline\n        stats_dicts: dict\n            A dict where 'key' is the string representation of an individual and 'value' is a dict containing statistics about the individual\n\n\n        Returns\n        -------\n        None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sanitize(x: Any) -> Any:  # pylint: disable=invalid-name,too-many-return-statements\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(x, (str, float, int, bool)):\n        # x is already serializable\n        return x\n    elif isinstance(x, torch.Tensor):\n        # tensor needs to be converted to a list (and moved to cpu if necessary)\n        return x.cpu().tolist()\n    elif isinstance(x, numpy.ndarray):\n        # array needs to be converted to a list\n        return x.tolist()\n    elif isinstance(x, numpy.number):  # pylint: disable=no-member\n        # NumPy numbers need to be converted to Python numbers\n        return x.item()\n    elif isinstance(x, dict):\n        # Dicts need their values sanitized\n        return {key: sanitize(value) for key, value in x.items()}\n    elif isinstance(x, (spacy.tokens.Token, allennlp.data.Token)):\n        # Tokens get sanitized to just their text.\n        return x.text\n    elif isinstance(x, (list, tuple)):\n        # Lists and Tuples need their values sanitized\n        return [sanitize(x_i) for x_i in x]\n    elif x is None:\n        return \"None\"\n    elif hasattr(x, 'to_json'):\n        return x.to_json()\n    else:\n        raise ValueError(f\"Cannot sanitize {x} of type {type(x)}. \"\n                         \"If this is your own custom class, add a `to_json(self)` method \"\n                         \"that returns a JSON-like object.\")", "output": "Sanitize turns PyTorch and Numpy types into basic Python types so they\n    can be serialized into JSON.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def embedding(x,\n              vocab_size,\n              dense_size,\n              name=None,\n              reuse=None,\n              multiplier=1.0,\n              symbol_dropout_rate=0.0,\n              embedding_var=None,\n              dtype=tf.float32):\n  \"\"\"\"\"\"\n  with tf.variable_scope(\n      name, default_name=\"embedding\", values=[x], reuse=reuse, dtype=dtype):\n    if embedding_var is None:\n      embedding_var = tf.get_variable(\"kernel\", [vocab_size, dense_size])\n    # On the backwards pass, we want to convert the gradient from\n    # an indexed-slices to a regular tensor before sending it back to the\n    # parameter server. This avoids excess computation on the parameter server.\n    if not tf.executing_eagerly():\n      embedding_var = convert_gradient_to_tensor(embedding_var)\n    x = dropout_no_scaling(x, 1.0 - symbol_dropout_rate)\n    emb_x = gather(embedding_var, x, dtype)\n    if multiplier != 1.0:\n      emb_x *= multiplier\n    static_shape = emb_x.shape.as_list()\n    if len(static_shape) < 5:\n      return emb_x\n    assert len(static_shape) == 5\n    # If we had an extra channel dimension, assume it's 1, i.e. shape[3] == 1.\n    return tf.squeeze(emb_x, 3)", "output": "Embed x of type int64 into dense vectors, reducing to max 4 dimensions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_uri(uri):\n    \"\"\"\n    \"\"\"\n    groups = URI.match(uri).groups()\n    return (groups[1], groups[3], groups[4], groups[6], groups[8])", "output": "Parses a URI using the regex given in Appendix B of RFC 3986.\n\n        (scheme, authority, path, query, fragment) = parse_uri(uri)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_page(self):\n        \"\"\"\"\"\"\n        settings_group = QGroupBox(_(\"Settings\"))\n        hist_spin = self.create_spinbox(\n                            _(\"History depth: \"), _(\" entries\"),\n                            'max_entries', min_=10, max_=10000, step=10,\n                            tip=_(\"Set maximum line count\"))\n\n        sourcecode_group = QGroupBox(_(\"Source code\"))\n        wrap_mode_box = self.create_checkbox(_(\"Wrap lines\"), 'wrap')\n        linenumbers_mode_box = self.create_checkbox(_(\"Show line numbers\"),\n                                                    'line_numbers')\n        go_to_eof_box = self.create_checkbox(\n                        _(\"Scroll automatically to last entry\"), 'go_to_eof')\n\n        settings_layout = QVBoxLayout()\n        settings_layout.addWidget(hist_spin)\n        settings_group.setLayout(settings_layout)\n\n        sourcecode_layout = QVBoxLayout()\n        sourcecode_layout.addWidget(wrap_mode_box)\n        sourcecode_layout.addWidget(linenumbers_mode_box)\n        sourcecode_layout.addWidget(go_to_eof_box)\n        sourcecode_group.setLayout(sourcecode_layout)\n\n        vlayout = QVBoxLayout()\n        vlayout.addWidget(settings_group)\n        vlayout.addWidget(sourcecode_group)\n        vlayout.addStretch(1)\n        self.setLayout(vlayout)", "output": "Setup config page widgets and options.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_func(func, *args, **kwargs):\n    \"\"\"\"\"\"\n    ray.init()\n\n    func = ray.remote(func)\n\n    # NOTE: kwargs not allowed for now\n    result = ray.get(func.remote(*args))\n\n    # Inspect the stack to get calling example\n    caller = inspect.stack()[1][3]\n    print(\"%s: %s\" % (caller, str(result)))\n\n    return result", "output": "Helper function for running examples", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_dark_font_color(color_scheme):\n    \"\"\"\"\"\"\n    color_scheme = get_color_scheme(color_scheme)\n    font_color, fon_fw, fon_fs = color_scheme['normal']\n    return dark_color(font_color)", "output": "Check if the font color used in the color scheme is dark.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def import_module(module_name):\n    \"\"\"\"\"\"\n    import sys, os\n    import importlib\n    sys.path.append(os.path.dirname(__file__))\n    return importlib.import_module(module_name)", "output": "Helper function to import module", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_range_minions(self, expr, greedy):\n        '''\n        \n        '''\n        if not HAS_RANGE:\n            raise CommandExecutionError(\n                'Range matcher unavailable (unable to import seco.range, '\n                'module most likely not installed)'\n            )\n        if not hasattr(self, '_range'):\n            self._range = seco.range.Range(self.opts['range_server'])\n        try:\n            return self._range.expand(expr)\n        except seco.range.RangeException as exc:\n            log.error(\n                'Range exception in compound match: %s', exc\n            )\n            cache_enabled = self.opts.get('minion_data_cache', False)\n            if greedy:\n                mlist = []\n                for fn_ in salt.utils.data.sorted_ignorecase(os.listdir(os.path.join(self.opts['pki_dir'], self.acc))):\n                    if not fn_.startswith('.') and os.path.isfile(os.path.join(self.opts['pki_dir'], self.acc, fn_)):\n                        mlist.append(fn_)\n                return {'minions': mlist,\n                        'missing': []}\n            elif cache_enabled:\n                return {'minions': self.cache.list('minions'),\n                        'missing': []}\n            else:\n                return {'minions': [],\n                        'missing': []}", "output": "Return the minions found by looking via range expression", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_files(directory):\n    '''\n    \n    '''\n    ret = set()\n    ret.add(directory)\n    for root, dirs, files in safe_walk(directory):\n        for name in files:\n            ret.add(os.path.join(root, name))\n        for name in dirs:\n            ret.add(os.path.join(root, name))\n\n    return list(ret)", "output": "Return a list of all files found under directory (and its subdirectories)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_sleep(minutes):\n    '''\n    \n    '''\n    value = _validate_sleep(minutes)\n    cmd = 'systemsetup -setsleep {0}'.format(value)\n    salt.utils.mac_utils.execute_return_success(cmd)\n\n    state = []\n    for check in (get_computer_sleep, get_display_sleep, get_harddisk_sleep):\n        state.append(salt.utils.mac_utils.confirm_updated(\n            value,\n            check,\n        ))\n    return all(state)", "output": "Sets the amount of idle time until the machine sleeps. Sets the same value\n    for Computer, Display, and Hard Disk. Pass \"Never\" or \"Off\" for computers\n    that should never sleep.\n\n    :param minutes: Can be an integer between 1 and 180 or \"Never\" or \"Off\"\n    :ptype: int, str\n\n    :return: True if successful, False if not\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' power.set_sleep 120\n        salt '*' power.set_sleep never", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_nodes(conn=None, call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The list_nodes function must be called with -f or --function.'\n        )\n\n    if not conn:\n        conn = get_conn()\n\n    ret = {}\n    datacenter_id = get_datacenter_id()\n\n    try:\n        nodes = conn.list_servers(datacenter_id=datacenter_id)\n    except PBNotFoundError:\n        log.error('Failed to get nodes list '\n                  'from datacenter: %s', datacenter_id)\n        raise\n\n    for item in nodes['items']:\n        node = {'id': item['id']}\n        node.update(item['properties'])\n        node['state'] = node.pop('vmState')\n        ret[node['name']] = node\n\n    return ret", "output": "Return a list of VMs that are on the provider", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def temporal_latent_to_dist(name, x, hparams, output_channels=None):\n  \"\"\"\n  \"\"\"\n  _, _, width, _, res_channels = common_layers.shape_list(x)\n  if output_channels is None:\n    output_channels = res_channels\n  dilation_rates = get_dilation_rates(hparams, width)\n  with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n    h = x\n    for i in range(hparams.latent_encoder_depth):\n      if hparams.latent_apply_dilations:\n        h2 = dilated_conv_stack(\"dil_latent_3d_res_%d\" % i, h,\n                                mid_channels=hparams.latent_encoder_width,\n                                output_channels=res_channels,\n                                dilation_rates=dilation_rates,\n                                activation=hparams.latent_activation,\n                                dropout=hparams.latent_dropout)\n      else:\n        h2 = conv_stack(\"latent_3d_res_%d\" % i, h,\n                        mid_channels=hparams.latent_encoder_width,\n                        output_channels=res_channels,\n                        activation=hparams.latent_activation,\n                        dropout=hparams.latent_dropout)\n      h += h2\n\n    # take last activation that should capture all context since padding is\n    # on left.\n    h = h[:, -1, :, :, :]\n    h = conv(\"res_final\", h, apply_actnorm=False, conv_init=\"zeros\",\n             output_channels=2*output_channels, filter_size=[1, 1])\n    mean, log_scale = h[:, :, :, 0::2], h[:, :, :, 1::2]\n  return tfp.distributions.Normal(mean, tf.exp(log_scale))", "output": "Network that maps a time-indexed list of 3-D latents to a gaussian.\n\n  Args:\n    name: variable scope.\n    x: List of 4-D Tensors indexed by time, (NHWC)\n    hparams: tf.contrib.training.Hparams.\n    output_channels: int, Number of channels of the output gaussian mean.\n  Returns:\n    dist: tfp.distributions.Normal", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flush_api_stage_cache(restApiId, stageName, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.flush_stage_cache(restApiId=restApiId, stageName=stageName)\n        return {'flushed': True}\n    except ClientError as e:\n        return {'flushed': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Flushes cache for the stage identified by stageName from API identified by restApiId\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_apigateway.flush_api_stage_cache restApiId stageName", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_namespaced_network_policy(self, name, namespace, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_network_policy_with_http_info(name, namespace, body, **kwargs)\n        else:\n            (data) = self.replace_namespaced_network_policy_with_http_info(name, namespace, body, **kwargs)\n            return data", "output": "replace the specified NetworkPolicy\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_network_policy(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the NetworkPolicy (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1beta1NetworkPolicy body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1beta1NetworkPolicy\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_attr(self, **kwargs):\n        \"\"\"\n        \"\"\"\n        for key, value in kwargs.items():\n            if value is not None:\n                if not isinstance(value, string_type):\n                    raise ValueError(\"Only string values are accepted\")\n                self.__attr[key] = value\n            else:\n                self.__attr.pop(key, None)\n        return self", "output": "Set attributes to the Booster.\n\n        Parameters\n        ----------\n        **kwargs\n            The attributes to set.\n            Setting a value to None deletes an attribute.\n\n        Returns\n        -------\n        self : Booster\n            Booster with set attributes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_nd_advanced_indexing(self, key, value):\n        \"\"\"\"\"\"\n        indices = self._get_index_nd(key)\n        vshape = _get_oshape_of_gather_nd_op(self.shape, indices.shape)\n        value_nd = self._prepare_value_nd(value, vshape)\n        _internal._scatter_set_nd(lhs=self, rhs=value_nd, indices=indices,\n                                  shape=self.shape, out=self)", "output": "This function is called by __setitem__ when key is an advanced index.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathNextDescendantOrSelf(self, cur):\n        \"\"\" \"\"\"\n        if cur is None: cur__o = None\n        else: cur__o = cur._o\n        ret = libxml2mod.xmlXPathNextDescendantOrSelf(self._o, cur__o)\n        if ret is None:raise xpathError('xmlXPathNextDescendantOrSelf() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Traversal function for the \"descendant-or-self\" direction\n          the descendant-or-self axis contains the context node and\n          the descendants of the context node in document order; thus\n          the context node is the first node on the axis, and the\n          first child of the context node is the second node on the\n           axis", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_java_worker_command(\n        java_worker_options,\n        redis_address,\n        plasma_store_name,\n        raylet_name,\n        redis_password,\n        temp_dir,\n):\n    \"\"\"\n    \"\"\"\n    assert java_worker_options is not None\n\n    command = \"java \".format(java_worker_options)\n    if redis_address is not None:\n        command += \"-Dray.redis.address={} \".format(redis_address)\n\n    if plasma_store_name is not None:\n        command += (\n            \"-Dray.object-store.socket-name={} \".format(plasma_store_name))\n\n    if raylet_name is not None:\n        command += \"-Dray.raylet.socket-name={} \".format(raylet_name)\n\n    if redis_password is not None:\n        command += \"-Dray.redis.password={} \".format(redis_password)\n\n    command += \"-Dray.home={} \".format(RAY_HOME)\n    # TODO(suquark): We should use temp_dir as the input of a java worker.\n    command += \"-Dray.log-dir={} \".format(os.path.join(temp_dir, \"sockets\"))\n\n    if java_worker_options:\n        # Put `java_worker_options` in the last, so it can overwrite the\n        # above options.\n        command += java_worker_options + \" \"\n    command += \"org.ray.runtime.runner.worker.DefaultWorker\"\n\n    return command", "output": "This method assembles the command used to start a Java worker.\n\n    Args:\n        java_worker_options (str): The command options for Java worker.\n        redis_address (str): Redis address of GCS.\n        plasma_store_name (str): The name of the plasma store socket to connect\n           to.\n        raylet_name (str): The name of the raylet socket to create.\n        redis_password (str): The password of connect to redis.\n        temp_dir (str): The path of the temporary directory Ray will use.\n    Returns:\n        The command string for starting Java worker.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_network(project_id, network_name, service):\n    '''\n    \n    '''\n    return service.networks().get(project=project_id,\n                                  network=network_name).execute()", "output": "Fetch network selfLink from network name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mget(self, body, doc_type=None, index=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, doc_type, \"_mget\"), params=params, body=body\n        )", "output": "Get multiple documents based on an index, type (optional) and ids.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/docs-multi-get.html>`_\n\n        :arg body: Document identifiers; can be either `docs` (containing full\n            document information) or `ids` (when index and type is provided in\n            the URL.\n        :arg index: The name of the index\n        :arg _source: True or false to return the _source field or not, or a\n            list of fields to return\n        :arg _source_exclude: A list of fields to exclude from the returned\n            _source field\n        :arg _source_include: A list of fields to extract and return from the\n            _source field\n        :arg preference: Specify the node or shard the operation should be\n            performed on (default: random)\n        :arg realtime: Specify whether to perform the operation in realtime or\n            search mode\n        :arg refresh: Refresh the shard containing the document before\n            performing the operation\n        :arg routing: Specific routing value\n        :arg stored_fields: A comma-separated list of stored fields to return in\n            the response", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_mat_to_images(args):\n    '''\n    '''\n    dataset = scipy.io.loadmat(\"{}/{}\".format(args.save_path, args.dataset))\n\n    # image pixel data\n    X = dataset['X']\n\n    # image class labels (not used in this project)\n    Y = dataset['Y']\n\n    total_image = X.shape[0]\n\n    h=args.height\n    w=args.width\n\n    for i in range(total_image):\n        img = X[i]\n        img = np.reshape(img, (28, 28))\n        if args.invert:\n            img = (1-img)*255\n        else:\n            img = img*255\n        img = Image.fromarray(img, 'L')\n        img = img.rotate(-90)\n        img = img.resize([h, w], Image.BILINEAR)\n        img.save(args.save_path + '/img' + str(i) + '.png')", "output": "convert the caltech101 mat file to images\n    Examples\n    --------\n    python convert_data.py --dataset /home/ubuntu/datasets/caltech101/data/caltech101_silhouettes_28.mat --save_path /home/ubuntu/datasets/caltech101/data/ --invert --height 32 --width 32", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def socks_password(self, value):\n        \"\"\"\n        \n        \"\"\"\n        self._verify_proxy_type_compatibility(ProxyType.MANUAL)\n        self.proxyType = ProxyType.MANUAL\n        self.socksPassword = value", "output": "Sets socks proxy password setting.\n\n        :Args:\n         - value: The socks proxy password value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def text_array_to_html(text_arr):\n  \"\"\"\n  \"\"\"\n  if not text_arr.shape:\n    # It is a scalar. No need to put it in a table, just apply markdown\n    return plugin_util.markdown_to_safe_html(np.asscalar(text_arr))\n  warning = ''\n  if len(text_arr.shape) > 2:\n    warning = plugin_util.markdown_to_safe_html(WARNING_TEMPLATE\n                                                % len(text_arr.shape))\n    text_arr = reduce_to_2d(text_arr)\n\n  html_arr = [plugin_util.markdown_to_safe_html(x)\n              for x in text_arr.reshape(-1)]\n  html_arr = np.array(html_arr).reshape(text_arr.shape)\n\n  return warning + make_table(html_arr)", "output": "Take a numpy.ndarray containing strings, and convert it into html.\n\n  If the ndarray contains a single scalar string, that string is converted to\n  html via our sanitized markdown parser. If it contains an array of strings,\n  the strings are individually converted to html and then composed into a table\n  using make_table. If the array contains dimensionality greater than 2,\n  all but two of the dimensions are removed, and a warning message is prefixed\n  to the table.\n\n  Args:\n    text_arr: A numpy.ndarray containing strings.\n\n  Returns:\n    The array converted to html.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _normalize_rename_handler(self, mapping, schema, field):\n        \"\"\"  \"\"\"\n        if 'rename_handler' not in schema[field]:\n            return\n        new_name = self.__normalize_coerce(\n            schema[field]['rename_handler'], field, field,\n            False, errors.RENAMING_FAILED)\n        if new_name != field:\n            mapping[new_name] = mapping[field]\n            del mapping[field]", "output": "{'oneof': [\n                {'type': 'callable'},\n                {'type': 'list',\n                 'schema': {'oneof': [{'type': 'callable'},\n                                      {'type': 'string'}]}},\n                {'type': 'string'}\n                ]}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def msearch_template(self, body, index=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"GET\",\n            _make_path(index, \"_msearch\", \"template\"),\n            params=params,\n            body=self._bulk_body(body),\n            headers={\"content-type\": \"application/x-ndjson\"},\n        )", "output": "The /_search/template endpoint allows to use the mustache language to\n        pre render search requests, before they are executed and fill existing\n        templates with template parameters.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html>`_\n\n        :arg body: The request definitions (metadata-search request definition\n            pairs), separated by newlines\n        :arg index: A list of index names, or a string containing a\n            comma-separated list of index names, to use as the default\n        :arg max_concurrent_searches: Controls the maximum number of concurrent\n            searches the multi search api will execute\n        :arg search_type: Search operation type, valid choices are:\n            'query_then_fetch', 'query_and_fetch', 'dfs_query_then_fetch',\n            'dfs_query_and_fetch'\n        :arg typed_keys: Specify whether aggregation and suggester names should\n            be prefixed by their respective types in the response", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def one_batch(self, ds_type:DatasetType=DatasetType.Train, detach:bool=True, denorm:bool=True, cpu:bool=True)->Collection[Tensor]:\n        \"\"\n        dl = self.dl(ds_type)\n        w = self.num_workers\n        self.num_workers = 0\n        try:     x,y = next(iter(dl))\n        finally: self.num_workers = w\n        if detach: x,y = to_detach(x,cpu=cpu),to_detach(y,cpu=cpu)\n        norm = getattr(self,'norm',False)\n        if denorm and norm:\n            x = self.denorm(x)\n            if norm.keywords.get('do_y',False): y = self.denorm(y, do_x=True)\n        return x,y", "output": "Get one batch from the data loader of `ds_type`. Optionally `detach` and `denorm`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zap(target=None, **kwargs):\n    '''\n    \n    '''\n    if target is not None:\n        log.warning(\"Depricated use of function, use kwargs\")\n    target = kwargs.get(\"dev\", target)\n    kwargs[\"dev\"] = target\n    return ceph_cfg.zap(**kwargs)", "output": "Destroy the partition table and content of a given disk.\n\n    .. code-block:: bash\n\n        salt '*' ceph.osd_prepare 'dev'='/dev/vdc' \\\\\n                'cluster_name'='ceph' \\\\\n                'cluster_uuid'='cluster_uuid'\n\n    dev\n        The block device to format.\n\n    cluster_name\n        The cluster name. Defaults to ``ceph``.\n\n    cluster_uuid\n        The cluster UUID. Defaults to value found in ceph config file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_deleted_fs(name, blade):\n    '''\n    \n    '''\n    try:\n        _fs = _get_fs(name, blade)\n        if _fs and _fs.destroyed:\n            return _fs\n    except rest.ApiException:\n        return None", "output": "Private function to check\n    if a file systeem has already been deleted", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_admin_object_resource(name, server=None, **kwargs):\n    '''\n    \n    '''\n    if 'jndiName' in kwargs:\n        del kwargs['jndiName']\n    return _update_element(name, 'resources/admin-object-resource', kwargs, server)", "output": "Update a JMS destination", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _run_cell_text(self, text, line):\r\n        \"\"\"\r\n        \"\"\"\r\n        finfo = self.get_current_finfo()\r\n        editor = self.get_current_editor()\r\n        oe_data = editor.highlighter.get_outlineexplorer_data()\r\n        try:\r\n            cell_name = oe_data.get(line-1).def_name\r\n        except AttributeError:\r\n            cell_name = ''\r\n        if finfo.editor.is_python() and text:\r\n            self.run_cell_in_ipyclient.emit(text, cell_name,\r\n                                            finfo.filename,\r\n                                            self.run_cell_copy)\r\n        editor.setFocus()", "output": "Run cell code in the console.\r\n\r\n        Cell code is run in the console by copying it to the console if\r\n        `self.run_cell_copy` is ``True`` otherwise by using the `run_cell`\r\n        function.\r\n\r\n        Parameters\r\n        ----------\r\n        text : str\r\n            The code in the cell as a string.\r\n        line : int\r\n            The starting line number of the cell in the file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def underlying_symbol(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            return self.__dict__[\"underlying_symbol\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'underlying_symbol' \".format(self.order_book_id)\n            )", "output": "[str] \u5408\u7ea6\u6807\u7684\u4ee3\u7801\uff0c\u76ee\u524d\u9664\u80a1\u6307\u671f\u8d27(IH, IF, IC)\u4e4b\u5916\u7684\u671f\u8d27\u5408\u7ea6\uff0c\u8fd9\u4e00\u5b57\u6bb5\u5168\u90e8\u4e3a\u2019null\u2019\uff08\u671f\u8d27\u4e13\u7528\uff09", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cast_bytes(s, encoding=None):\n    \"\"\"\"\"\"\n    if not isinstance(s, bytes):\n        return encode(s, encoding)\n    return s", "output": "Source: https://github.com/ipython/ipython_genutils", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_get_future_list(ip=None, port=None):\n    \"\"\"\n    \"\"\"\n\n    global extension_market_list\n    extension_market_list = QA_fetch_get_extensionmarket_list(\n    ) if extension_market_list is None else extension_market_list\n\n    return extension_market_list.query('market==42 or market==28 or market==29 or market==30 or market==47')", "output": "[summary]\n\n    Keyword Arguments:\n        ip {[type]} -- [description] (default: {None})\n        port {[type]} -- [description] (default: {None})\n\n    42         3      \u5546\u54c1\u6307\u6570         TI\n    60         3    \u4e3b\u529b\u671f\u8d27\u5408\u7ea6         MA\n    28         3      \u90d1\u5dde\u5546\u54c1         QZ\n    29         3      \u5927\u8fde\u5546\u54c1         QD\n    30         3      \u4e0a\u6d77\u671f\u8d27(\u539f\u6cb9+\u8d35\u91d1\u5c5e)  QS\n    47         3     \u4e2d\u91d1\u6240\u671f\u8d27         CZ\n\n    50         3      \u6e24\u6d77\u5546\u54c1         BH\n    76         3      \u9f50\u9c81\u5546\u54c1         QL\n\n\n    46        11      \u4e0a\u6d77\u9ec4\u91d1(\u4f26\u6566\u91d1T+D)         SG", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_dataframe_localize_timestamps(pdf, timezone):\n    \"\"\"\n    \n    \"\"\"\n    from pyspark.sql.utils import require_minimum_pandas_version\n    require_minimum_pandas_version()\n\n    for column, series in pdf.iteritems():\n        pdf[column] = _check_series_localize_timestamps(series, timezone)\n    return pdf", "output": "Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone\n\n    :param pdf: pandas.DataFrame\n    :param timezone: the timezone to convert. if None then use local timezone\n    :return pandas.DataFrame where any timezone aware columns have been converted to tz-naive", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attention(query: torch.Tensor,\n              key: torch.Tensor,\n              value: torch.Tensor,\n              mask: torch.Tensor = None,\n              dropout: Callable = None) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"'\"\"\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim=-1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn", "output": "Compute 'Scaled Dot Product Attention", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def id_exists(ids, mods, test=None, queue=False, **kwargs):\n    '''\n    \n    '''\n    ids = salt.utils.args.split_input(ids)\n    ids = set(ids)\n    sls_ids = set(x['__id__'] for x in show_low_sls(mods, test=test, queue=queue, **kwargs))\n    return ids.issubset(sls_ids)", "output": "Tests for the existence of a specific ID or list of IDs within the\n    specified SLS file(s). Similar to :py:func:`state.sls_exists\n    <salt.modules.state.sls_exists>`, returns True or False. The default\n    environment is base``, use ``saltenv`` to specify a different environment.\n\n    .. versionadded:: 2019.2.0\n\n    saltenv\n        Specify a salt fileserver environment from which to look for the SLS files\n        specified in the ``mods`` argument\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.id_exists create_myfile,update_template filestate saltenv=dev", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def collect_variables(self, g_scope='gen', d_scope='discrim'):\n        \"\"\"\n        \n        \"\"\"\n        self.g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, g_scope)\n        assert self.g_vars\n        self.d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, d_scope)\n        assert self.d_vars", "output": "Assign `self.g_vars` to the parameters under scope `g_scope`,\n        and same with `self.d_vars`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_value(self, continuous_future, dt, field):\n        \"\"\"\n        \n        \"\"\"\n        rf = self._roll_finders[continuous_future.roll_style]\n        sid = (rf.get_contract_center(continuous_future.root_symbol,\n                                      dt,\n                                      continuous_future.offset))\n        return self._bar_reader.get_value(sid, dt, field)", "output": "Retrieve the value at the given coordinates.\n\n        Parameters\n        ----------\n        sid : int\n            The asset identifier.\n        dt : pd.Timestamp\n            The timestamp for the desired data point.\n        field : string\n            The OHLVC name for the desired data point.\n\n        Returns\n        -------\n        value : float|int\n            The value at the given coordinates, ``float`` for OHLC, ``int``\n            for 'volume'.\n\n        Raises\n        ------\n        NoDataOnDate\n            If the given dt is not a valid market minute (in minute mode) or\n            session (in daily mode) according to this reader's tradingcalendar.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def scheduler(ctx, xmlrpc, xmlrpc_host, xmlrpc_port,\n              inqueue_limit, delete_time, active_tasks, loop_limit, fail_pause_num,\n              scheduler_cls, threads, get_object=False):\n    \"\"\"\n    \n    \"\"\"\n    g = ctx.obj\n    Scheduler = load_cls(None, None, scheduler_cls)\n\n    kwargs = dict(taskdb=g.taskdb, projectdb=g.projectdb, resultdb=g.resultdb,\n                  newtask_queue=g.newtask_queue, status_queue=g.status_queue,\n                  out_queue=g.scheduler2fetcher, data_path=g.get('data_path', 'data'))\n    if threads:\n        kwargs['threads'] = int(threads)\n\n    scheduler = Scheduler(**kwargs)\n    scheduler.INQUEUE_LIMIT = inqueue_limit\n    scheduler.DELETE_TIME = delete_time\n    scheduler.ACTIVE_TASKS = active_tasks\n    scheduler.LOOP_LIMIT = loop_limit\n    scheduler.FAIL_PAUSE_NUM = fail_pause_num\n\n    g.instances.append(scheduler)\n    if g.get('testing_mode') or get_object:\n        return scheduler\n\n    if xmlrpc:\n        utils.run_in_thread(scheduler.xmlrpc_run, port=xmlrpc_port, bind=xmlrpc_host)\n    scheduler.run()", "output": "Run Scheduler, only one scheduler is allowed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def control_message_target(self, slack_user_name, text, loaded_groups, trigger_string):\n        '''\n\n        '''\n\n        # Trim the trigger string from the front\n        # cmdline = _text[1:].split(' ', 1)\n        cmdline = self.commandline_to_list(text, trigger_string)\n        permitted_group = self.can_user_run(slack_user_name, cmdline[0], loaded_groups)\n        log.debug('slack_user_name is %s and the permitted group is %s', slack_user_name, permitted_group)\n\n        if not permitted_group:\n            return (False, None, cmdline[0])\n        if not slack_user_name:\n            return (False, None, cmdline[0])\n\n        # maybe there are aliases, so check on that\n        if cmdline[0] in permitted_group[1].get('aliases', {}).keys():\n            use_cmdline = self.commandline_to_list(permitted_group[1]['aliases'][cmdline[0]].get('cmd', ''), '')\n            # Include any additional elements from cmdline\n            use_cmdline.extend(cmdline[1:])\n        else:\n            use_cmdline = cmdline\n        target = self.get_target(permitted_group, cmdline, use_cmdline)\n\n        # Remove target and tgt_type from commandline\n        # that is sent along to Salt\n        use_cmdline = [item for item\n                       in use_cmdline\n                       if all(not item.startswith(x) for x in ('target', 'tgt_type'))]\n\n        return (True, target, use_cmdline)", "output": "Returns a tuple of (target, cmdline,) for the response\n\n        Raises IndexError if a user can't be looked up from all_slack_users\n\n        Returns (False, False) if the user doesn't have permission\n\n        These are returned together because the commandline and the targeting\n        interact with the group config (specifically aliases and targeting configuration)\n        so taking care of them together works out.\n\n        The cmdline that is returned is the actual list that should be\n        processed by salt, and not the alias.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def active_element(self):\n        \"\"\"\n        \n        \"\"\"\n        if self._driver.w3c:\n            return self._driver.execute(Command.W3C_GET_ACTIVE_ELEMENT)['value']\n        else:\n            return self._driver.execute(Command.GET_ACTIVE_ELEMENT)['value']", "output": "Returns the element with focus, or BODY if nothing has focus.\n\n        :Usage:\n            ::\n\n                element = driver.switch_to.active_element", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def service_enable(s_name, **connection_args):\n    '''\n    \n    '''\n    ret = True\n    service = _service_get(s_name, **connection_args)\n    if service is None:\n        return False\n    nitro = _connect(**connection_args)\n    if nitro is None:\n        return False\n    try:\n        NSService.enable(nitro, service)\n    except NSNitroError as error:\n        log.debug('netscaler module error - NSService.enable() failed: %s', error)\n        ret = False\n    _disconnect(nitro)\n    return ret", "output": "Enable a service\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' netscaler.service_enable 'serviceName'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_proxy_running(proxyname):\n    '''\n    \n    '''\n    cmd = ('ps ax | grep \"salt-proxy --proxyid={0}\" | grep -v grep'\n           .format(salt.ext.six.moves.shlex_quote(proxyname)))\n    cmdout = __salt__['cmd.run_all'](\n        cmd,\n        timeout=5,\n        python_shell=True)\n    if not cmdout['stdout']:\n        return False\n    else:\n        return True", "output": "Check if proxy for this name is running", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_cron_cmdstr(path, user=None):\n    '''\n    \n    '''\n    if user:\n        cmd = 'crontab -u {0}'.format(user)\n    else:\n        cmd = 'crontab'\n    return '{0} {1}'.format(cmd, path)", "output": "Returns a format string, to be used to build a crontab command.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_auto_dict(values, source='auto', convert_to_human=True):\n    '''\n    \n    '''\n    for name, value in values.items():\n        values[name] = to_auto(name, value, source, convert_to_human)\n\n    return values", "output": "Pass an entire dictionary to to_auto\n\n    .. note::\n        The key will be passed as the name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _recursive_compare(v1, v2):\n    '''\n    \n    '''\n    if isinstance(v1, list):\n        if v2 is None:\n            v2 = []\n        if len(v1) != len(v2):\n            return False\n        v1.sort(key=_id_or_key)\n        v2.sort(key=_id_or_key)\n        for x, y in zip(v1, v2):\n            if not _recursive_compare(x, y):\n                return False\n        return True\n    elif isinstance(v1, dict):\n        if v2 is None:\n            v2 = {}\n        v1 = dict(v1)\n        v2 = dict(v2)\n        if sorted(v1) != sorted(v2):\n            return False\n        for k in v1:\n            if not _recursive_compare(v1[k], v2[k]):\n                return False\n        return True\n    else:\n        return v1 == v2", "output": "Return v1 == v2. Compares list, dict, recursively.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self, event):\n        '''\n        \n        '''\n        'QA_WORKER method'\n        if event.event_type is ACCOUNT_EVENT.SETTLE:\n            print('account_settle')\n            self.settle()\n\n        # elif event.event_type is ACCOUNT_EVENT.UPDATE:\n        #     self.receive_deal(event.message)\n        elif event.event_type is ACCOUNT_EVENT.MAKE_ORDER:\n            \"\"\"generate order\n            if callback callback the order\n            if not return back the order\n            \"\"\"\n            data = self.send_order(\n                code=event.code,\n                amount=event.amount,\n                time=event.time,\n                amount_model=event.amount_model,\n                towards=event.towards,\n                price=event.price,\n                order_model=event.order_model\n            )\n            if event.callback:\n                event.callback(data)\n            else:\n                return data\n        elif event.event_type is ENGINE_EVENT.UPCOMING_DATA:\n            \"\"\"update the market_data\n            1. update the inside market_data struct\n            2. tell the on_bar methods\n\n            # \u8fd9\u6837\u6709\u70b9\u6162\n\n\n            \"\"\"\n\n            self._currenttime = event.market_data.datetime[0]\n            if self._market_data is None:\n                self._market_data = event.market_data\n            else:\n                self._market_data = self._market_data + event.market_data\n            self.on_bar(event)\n\n            if event.callback:\n                event.callback(event)", "output": "\u8fd9\u4e2a\u65b9\u6cd5\u662f\u88ab QA_ThreadEngine \u5904\u7406\u961f\u5217\u65f6\u5019\u8c03\u7528\u7684\uff0c QA_Task \u4e2d do \u65b9\u6cd5\u8c03\u7528 run \uff08\u5728\u5176\u5b83\u7ebf\u7a0b\u4e2d\uff09\n       'QA_WORKER method \u91cd\u8f7d'\n        :param event: \u4e8b\u4ef6\u7c7b\u578b QA_Event\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_output(script, expanded):\n    \"\"\"\n\n    \"\"\"\n    if shell_logger.is_available():\n        return shell_logger.get_output(script)\n    if settings.instant_mode:\n        return read_log.get_output(script)\n    else:\n        return rerun.get_output(script, expanded)", "output": "Get output of the script.\n\n    :param script: Console script.\n    :type script: str\n    :param expanded: Console script with expanded aliases.\n    :type expanded: str\n    :rtype: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def closed_issue(issue, after=None):\n    \"\"\"\"\"\"\n    if issue['state'] == 'closed':\n        if after is None or parse_timestamp(issue['closed_at']) > after:\n            return True\n    return False", "output": "Returns True iff this issue was closed after given date. If after not given, only checks if issue is closed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def noam_norm(x, epsilon=1.0, name=None):\n  \"\"\"\"\"\"\n  with tf.name_scope(name, default_name=\"noam_norm\", values=[x]):\n    shape = x.get_shape()\n    ndims = len(shape)\n    return (tf.nn.l2_normalize(x, ndims - 1, epsilon=epsilon) * tf.sqrt(\n        to_float(shape[-1])))", "output": "One version of layer normalization.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def empty_dir_list(saltenv='base', backend=None):\n    '''\n    \n    '''\n    fileserver = salt.fileserver.Fileserver(__opts__)\n    load = {'saltenv': saltenv, 'fsbackend': backend}\n    return fileserver.file_list_emptydirs(load=load)", "output": ".. versionadded:: 2015.5.0\n\n    Return a list of empty directories in the given environment\n\n    saltenv : base\n        The salt fileserver environment to be listed\n\n    backend\n        Narrow fileserver backends to a subset of the enabled ones. If all\n        passed backends start with a minus sign (``-``), then these backends\n        will be excluded from the enabled backends. However, if there is a mix\n        of backends with and without a minus sign (ex:\n        ``backend=-roots,git``) then the ones starting with a minus sign will\n        be disregarded.\n\n        .. note::\n\n            Some backends (such as :mod:`git <salt.fileserver.gitfs>` and\n            :mod:`hg <salt.fileserver.hgfs>`) do not support empty directories.\n            So, passing ``backend=git`` or ``backend=hg`` will result in an\n            empty list being returned.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run fileserver.empty_dir_list\n        salt-run fileserver.empty_dir_list saltenv=prod\n        salt-run fileserver.empty_dir_list backend=roots", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def entry_path(cls, project, location, entry_group, entry):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/locations/{location}/entryGroups/{entry_group}/entries/{entry}\",\n            project=project,\n            location=location,\n            entry_group=entry_group,\n            entry=entry,\n        )", "output": "Return a fully-qualified entry string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lrn(self, depth_radius, bias, alpha, beta):\n        \"\"\"\"\"\"\n        name = \"lrn\" + str(self.counts[\"lrn\"])\n        self.counts[\"lrn\"] += 1\n        self.top_layer = tf.nn.lrn(\n            self.top_layer, depth_radius, bias, alpha, beta, name=name)\n        return self.top_layer", "output": "Adds a local response normalization layer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_widget_shortcuts(self, widget):\n        \"\"\"\n        \n        \"\"\"\n        for qshortcut, context, name in widget.get_shortcut_data():\n            self.register_shortcut(qshortcut, context, name)", "output": "Register widget shortcuts.\n\n        Widget interface must have a method called 'get_shortcut_data'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_pytorch_link(ft)->str:\n    \"\"\n    name = ft.__name__\n    ext = '.html'\n    if name == 'device': return f'{PYTORCH_DOCS}tensor_attributes{ext}#torch-device'\n    if name == 'Tensor': return f'{PYTORCH_DOCS}tensors{ext}#torch-tensor'\n    if name.startswith('torchvision'):\n        doc_path = get_module_name(ft).replace('.', '/')\n        if inspect.ismodule(ft): name = name.replace('.', '-')\n        return f'{PYTORCH_DOCS}{doc_path}{ext}#{name}'\n    if name.startswith('torch.nn') and inspect.ismodule(ft): # nn.functional is special case\n        nn_link = name.replace('.', '-')\n        return f'{PYTORCH_DOCS}nn{ext}#{nn_link}'\n    paths = get_module_name(ft).split('.')\n    if len(paths) == 1: return f'{PYTORCH_DOCS}{paths[0]}{ext}#{paths[0]}.{name}'\n\n    offset = 1 if paths[1] == 'utils' else 0 # utils is a pytorch special case\n    doc_path = paths[1+offset]\n    if inspect.ismodule(ft): return f'{PYTORCH_DOCS}{doc_path}{ext}#module-{name}'\n    fnlink = '.'.join(paths[:(2+offset)]+[name])\n    return f'{PYTORCH_DOCS}{doc_path}{ext}#{fnlink}'", "output": "Returns link to pytorch docs of `ft`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _env_root(repo, saltenv):\n    '''\n    \n    '''\n    # If 'base' is desired, look for the trunk\n    if saltenv == 'base':\n        trunk = os.path.join(repo['repo'], repo['trunk'])\n        if os.path.isdir(trunk):\n            return trunk\n        else:\n            return None\n\n    # Check branches\n    branches = os.path.join(repo['repo'], repo['branches'])\n    if os.path.isdir(branches) and saltenv in os.listdir(branches):\n        return os.path.join(branches, saltenv)\n\n    # Check tags\n    tags = os.path.join(repo['repo'], repo['tags'])\n    if os.path.isdir(tags) and saltenv in os.listdir(tags):\n        return os.path.join(tags, saltenv)\n\n    return None", "output": "Return the root of the directory corresponding to the desired environment,\n    or None if the environment was not found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def server_enable(s_name, **connection_args):\n    '''\n    \n    '''\n    ret = True\n    server = _server_get(s_name, **connection_args)\n    if server is None:\n        return False\n    if server.get_state() == 'ENABLED':\n        return True\n    nitro = _connect(**connection_args)\n    if nitro is None:\n        return False\n    try:\n        NSServer.enable(nitro, server)\n    except NSNitroError as error:\n        log.debug('netscaler module error - NSServer.enable() failed: %s', error)\n        ret = False\n    _disconnect(nitro)\n    return ret", "output": "Enables a server globally\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' netscaler.server_enable 'serverName'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def MAP_ADD(self, instr):\n        key = self.ast_stack.pop()\n        value = self.ast_stack.pop()\n\n        self.ast_stack.append((key, value))\n        ''", "output": "NOP", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_number(dtype):\r\n    \"\"\"\"\"\"\r\n    return is_float(dtype) or ('int' in dtype.name) or ('long' in dtype.name) \\\r\n           or ('short' in dtype.name)", "output": "Return True is datatype dtype is a number kind", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _yellowfin(self):\n    \"\"\"\n    \"\"\"\n    # List for the returned Operations.\n    yellowfin_ops = []\n\n    # Curvature range ops.\n    curv_range_ops = self._curvature_range()\n    yellowfin_ops += curv_range_ops\n    # Estimate of gradient Variance ops.\n    grad_var_ops = self._grad_variance()\n    yellowfin_ops += grad_var_ops\n    # Distance to optimum ops.\n    dist_to_opt_ops = self._dist_to_opt()\n    yellowfin_ops += dist_to_opt_ops\n\n    # Single-Step: minimizes the surrogate for the expected\n    # squared distance from the optimum of a local quadratic\n    # approximation after a single step while keeping all directions in the\n    # robust region.\n    self._mu = tf.identity(tf.cond(self._do_tune,\n                                   self._get_mu_tensor,\n                                   lambda: self._mu_var))\n    with tf.control_dependencies([self._mu]):\n      self._lr = tf.identity(tf.cond(self._do_tune,\n                                     self._get_lr_tensor,\n                                     lambda: self._lr_var))\n\n    # Tune learning rate and momentum.\n    with tf.control_dependencies([self._mu, self._lr]):\n      self._mu = self._beta * self._mu_var + (1 - self._beta) * self._mu\n      self._lr = self._beta * self._lr_var + (1 - self._beta) * self._lr\n      yellowfin_ops.append(tf.assign(self._mu_var, self._mu))\n      yellowfin_ops.append(tf.assign(self._lr_var, self._lr))\n\n    yellowfin_ops = tf.group(*yellowfin_ops)\n    return yellowfin_ops", "output": "YellowFin auto-tuning optimizer based on momentum SGD.\n\n    Returns:\n      YF ops\n        (Curvature range,\n         Grad_variance,\n         Dist_to_opt,\n         Single-Step,\n         Auto-Tuning)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_trial_meta(cls, expr_dir):\n        \"\"\"\n        \"\"\"\n        meta_file = os.path.join(expr_dir, EXPR_META_FILE)\n        meta = parse_json(meta_file)\n\n        if not meta:\n            job_id = expr_dir.split(\"/\")[-2]\n            trial_id = expr_dir[-8:]\n            params = parse_json(os.path.join(expr_dir, EXPR_PARARM_FILE))\n            meta = {\n                \"trial_id\": trial_id,\n                \"job_id\": job_id,\n                \"status\": \"RUNNING\",\n                \"type\": \"TUNE\",\n                \"start_time\": os.path.getctime(expr_dir),\n                \"end_time\": None,\n                \"progress_offset\": 0,\n                \"result_offset\": 0,\n                \"params\": params\n            }\n\n        if not meta.get(\"start_time\", None):\n            meta[\"start_time\"] = os.path.getctime(expr_dir)\n\n        if isinstance(meta[\"start_time\"], float):\n            meta[\"start_time\"] = timestamp2date(meta[\"start_time\"])\n\n        if meta.get(\"end_time\", None):\n            meta[\"end_time\"] = timestamp2date(meta[\"end_time\"])\n\n        meta[\"params\"] = parse_json(os.path.join(expr_dir, EXPR_PARARM_FILE))\n\n        return meta", "output": "Build meta file for trial.\n\n        Args:\n            expr_dir (str): Directory path of the experiment.\n\n        Return:\n            A dict of trial meta info.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_alarms(deployment_id, alert_id=None, metric_name=None, api_key=None, profile='telemetry'):\n    '''\n\n    '''\n    auth = _auth(profile=profile)\n\n    if alert_id is None:\n        # Delete all the alarms associated with this deployment\n        alert_ids = get_alert_config(deployment_id, api_key=api_key, profile=profile)\n    else:\n        alert_ids = [alert_id]\n\n    if not alert_ids:\n        return False, \"failed to find alert associated with deployment: {0}\".format(deployment_id)\n\n    failed_to_delete = []\n    for id in alert_ids:\n        delete_url = _get_telemetry_base(profile) + \"/alerts/{0}\".format(id)\n\n        try:\n            response = requests.delete(delete_url, headers=auth)\n            if metric_name:\n                log.debug(\"updating cache and delete %s key from %s\",\n                          metric_name, deployment_id)\n                _update_cache(deployment_id, metric_name, None)\n\n        except requests.exceptions.RequestException as e:\n            log.error('Delete failed: %s', e)\n\n        if response.status_code != 200:\n            failed_to_delete.append(id)\n\n    if failed_to_delete:\n        return False, \"Failed to delete {0} alarms in deployment: {1}\" .format(', '.join(failed_to_delete), deployment_id)\n\n    return True, \"Successfully deleted {0} alerts in deployment: {1}\".format(', '.join(alert_ids), deployment_id)", "output": "delete an alert specified by alert_id or if not specified blows away all the alerts\n         in the current deployment.\n\n    Returns (bool success, str message) tuple.\n\n    CLI Example:\n\n        salt myminion telemetry.delete_alarms rs-ds033197 profile=telemetry", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def network_security_group_create_or_update(name, resource_group, **kwargs):  # pylint: disable=invalid-name\n    '''\n    \n\n    '''\n    if 'location' not in kwargs:\n        rg_props = __salt__['azurearm_resource.resource_group_get'](\n            resource_group, **kwargs\n        )\n\n        if 'error' in rg_props:\n            log.error(\n                'Unable to determine location from resource group specified.'\n            )\n            return False\n        kwargs['location'] = rg_props['location']\n\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n\n    try:\n        secgroupmodel = __utils__['azurearm.create_object_model']('network', 'NetworkSecurityGroup', **kwargs)\n    except TypeError as exc:\n        result = {'error': 'The object model could not be built. ({0})'.format(str(exc))}\n        return result\n\n    try:\n        secgroup = netconn.network_security_groups.create_or_update(\n            resource_group_name=resource_group,\n            network_security_group_name=name,\n            parameters=secgroupmodel\n        )\n        secgroup.wait()\n        secgroup_result = secgroup.result()\n        result = secgroup_result.as_dict()\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n    except SerializationError as exc:\n        result = {'error': 'The object model could not be parsed. ({0})'.format(str(exc))}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Create or update a network security group.\n\n    :param name: The name of the network security group to create.\n\n    :param resource_group: The resource group name assigned to the\n        network security group.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.network_security_group_create_or_update testnsg testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list(self, verbose=True):\n        \"\"\"\n        \"\"\"\n        self._check()\n\n        for tarinfo in self:\n            if verbose:\n                print(filemode(tarinfo.mode), end=' ')\n                print(\"%s/%s\" % (tarinfo.uname or tarinfo.uid,\n                                 tarinfo.gname or tarinfo.gid), end=' ')\n                if tarinfo.ischr() or tarinfo.isblk():\n                    print(\"%10s\" % (\"%d,%d\" \\\n                                    % (tarinfo.devmajor, tarinfo.devminor)), end=' ')\n                else:\n                    print(\"%10d\" % tarinfo.size, end=' ')\n                print(\"%d-%02d-%02d %02d:%02d:%02d\" \\\n                      % time.localtime(tarinfo.mtime)[:6], end=' ')\n\n            print(tarinfo.name + (\"/\" if tarinfo.isdir() else \"\"), end=' ')\n\n            if verbose:\n                if tarinfo.issym():\n                    print(\"->\", tarinfo.linkname, end=' ')\n                if tarinfo.islnk():\n                    print(\"link to\", tarinfo.linkname, end=' ')\n            print()", "output": "Print a table of contents to sys.stdout. If `verbose' is False, only\n           the names of the members are printed. If it is True, an `ls -l'-like\n           output is produced.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _store_outputs_in_object_store(self, object_ids, outputs):\n        \"\"\"\n        \"\"\"\n        for i in range(len(object_ids)):\n            if isinstance(outputs[i], ray.actor.ActorHandle):\n                raise Exception(\"Returning an actor handle from a remote \"\n                                \"function is not allowed).\")\n            if outputs[i] is ray.experimental.no_return.NoReturn:\n                if not self.plasma_client.contains(\n                        pyarrow.plasma.ObjectID(object_ids[i].binary())):\n                    raise RuntimeError(\n                        \"Attempting to return 'ray.experimental.NoReturn' \"\n                        \"from a remote function, but the corresponding \"\n                        \"ObjectID does not exist in the local object store.\")\n            else:\n                self.put_object(object_ids[i], outputs[i])", "output": "Store the outputs of a remote function in the local object store.\n\n        This stores the values that were returned by a remote function in the\n        local object store. If any of the return values are object IDs, then\n        these object IDs are aliased with the object IDs that the scheduler\n        assigned for the return values. This is called by the worker that\n        executes the remote function.\n\n        Note:\n            The arguments object_ids and outputs should have the same length.\n\n        Args:\n            object_ids (List[ObjectID]): The object IDs that were assigned to\n                the outputs of the remote function call.\n            outputs (Tuple): The value returned by the remote function. If the\n                remote function was supposed to only return one value, then its\n                output was wrapped in a tuple with one element prior to being\n                passed into this function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(\n    version, strict=False  # type: str  # type: bool\n):  # type:(...) -> Union[Version, LegacyVersion]\n    \"\"\"\n    \n    \"\"\"\n    try:\n        return Version(version)\n    except InvalidVersion:\n        if strict:\n            raise\n\n        return LegacyVersion(version)", "output": "Parse the given version string and return either a :class:`Version` object\n    or a LegacyVersion object depending on if the given version is\n    a valid PEP 440 version or a legacy version.\n\n    If strict=True only PEP 440 versions will be accepted.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _value_iterator(self, task_name, param_name):\n        \"\"\"\n        \n        \"\"\"\n        cp_parser = CmdlineParser.get_instance()\n        if cp_parser:\n            dest = self._parser_global_dest(param_name, task_name)\n            found = getattr(cp_parser.known_args, dest, None)\n            yield (self._parse_or_no_value(found), None)\n        yield (self._get_value_from_config(task_name, param_name), None)\n        if self._config_path:\n            yield (self._get_value_from_config(self._config_path['section'], self._config_path['name']),\n                   'The use of the configuration [{}] {} is deprecated. Please use [{}] {}'.format(\n                       self._config_path['section'], self._config_path['name'], task_name, param_name))\n        yield (self._default, None)", "output": "Yield the parameter values, with optional deprecation warning as second tuple value.\n\n        The parameter value will be whatever non-_no_value that is yielded first.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close(self, reason=None):\n        \"\"\"\n        \"\"\"\n        with self._closing:\n            if self._closed:\n                return\n\n            # Stop consuming messages.\n            if self.is_active:\n                _LOGGER.debug(\"Stopping consumer.\")\n                self._consumer.stop()\n            self._consumer = None\n\n            # Shutdown all helper threads\n            _LOGGER.debug(\"Stopping scheduler.\")\n            self._scheduler.shutdown()\n            self._scheduler = None\n            _LOGGER.debug(\"Stopping leaser.\")\n            self._leaser.stop()\n            self._leaser = None\n            _LOGGER.debug(\"Stopping dispatcher.\")\n            self._dispatcher.stop()\n            self._dispatcher = None\n            _LOGGER.debug(\"Stopping heartbeater.\")\n            self._heartbeater.stop()\n            self._heartbeater = None\n\n            self._rpc = None\n            self._closed = True\n            _LOGGER.debug(\"Finished stopping manager.\")\n\n            for callback in self._close_callbacks:\n                callback(self, reason)", "output": "Stop consuming messages and shutdown all helper threads.\n\n        This method is idempotent. Additional calls will have no effect.\n\n        Args:\n            reason (Any): The reason to close this. If None, this is considered\n                an \"intentional\" shutdown. This is passed to the callbacks\n                specified via :meth:`add_close_callback`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def laplacian_pyramid_image(shape, n_levels=4, sd=None):\n    \"\"\"\n    \"\"\"\n    batch_dims = shape[:-3]\n    w, h, ch = shape[-3:]\n    pyramid = 0\n    for n in range(n_levels):\n        k = 2 ** n\n        pyramid += lowres_tensor(shape, batch_dims + (w // k, h // k, ch), sd=sd)\n    return pyramid", "output": "Simple laplacian pyramid paramaterization of an image.\n\n    For more flexibility, use a sum of lowres_tensor()s.\n\n    Args:\n      shape: shape of resulting image, [batch, width, height, channels].\n      n_levels: number of levels of laplacian pyarmid.\n      sd: standard deviation of param initialization.\n\n    Returns:\n      tensor with shape from first argument.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hybrid_forward(self, F, inputs):\n        # pylint: disable=arguments-differ\n        \"\"\"\n        \n        \"\"\"\n        # the character id embedding\n        # (batch_size * sequence_length, max_chars_per_token, embed_dim)\n        character_embedding = self._char_embedding(inputs.reshape((-1, self._max_chars_per_token)))\n\n        character_embedding = F.transpose(character_embedding, axes=(1, 0, 2))\n        token_embedding = self._convolutions(character_embedding)\n\n        out_shape_ref = inputs.slice_axis(axis=-1, begin=0, end=1)\n        out_shape_ref = out_shape_ref.broadcast_axes(axis=(2,),\n                                                     size=(self._output_size))\n\n        return token_embedding.reshape_like(out_shape_ref)", "output": "Compute context insensitive token embeddings for ELMo representations.\n\n        Parameters\n        ----------\n        inputs : NDArray\n            Shape (batch_size, sequence_length, max_character_per_token)\n            of character ids representing the current batch.\n\n        Returns\n        -------\n        token_embedding : NDArray\n            Shape (batch_size, sequence_length, embedding_size) with context\n            insensitive token representations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _item_list(profile=None):\n    '''\n    \n    '''\n    g_client = _auth(profile)\n    ret = []\n    for item in g_client.items.list():\n        ret.append(item.__dict__)\n        #ret[item.name] = {\n        #        'name': item.name,\n        #    }\n    return ret", "output": "Template for writing list functions\n    Return a list of available items (glance items-list)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' glance.item_list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_template_on_contents(\n        contents,\n        template,\n        context,\n        defaults,\n        saltenv):\n    '''\n    \n    '''\n    if template in salt.utils.templates.TEMPLATE_REGISTRY:\n        context_dict = defaults if defaults else {}\n        if context:\n            context_dict = salt.utils.dictupdate.merge(context_dict, context)\n        # Apply templating\n        contents = salt.utils.templates.TEMPLATE_REGISTRY[template](\n            contents,\n            from_str=True,\n            to_str=True,\n            context=context_dict,\n            saltenv=saltenv,\n            grains=__opts__['grains'],\n            pillar=__pillar__,\n            salt=__salt__,\n            opts=__opts__)['data']\n        if six.PY2:\n            contents = contents.encode('utf-8')\n        elif six.PY3 and isinstance(contents, bytes):\n            # bytes -> str\n            contents = contents.decode('utf-8')\n    else:\n        ret = {}\n        ret['result'] = False\n        ret['comment'] = ('Specified template format {0} is not supported'\n                          ).format(template)\n        return ret\n    return contents", "output": "Return the contents after applying the templating engine\n\n    contents\n        template string\n\n    template\n        template format\n\n    context\n        Overrides default context variables passed to the template.\n\n    defaults\n        Default context passed to the template.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.apply_template_on_contents \\\\\n            contents='This is a {{ template }} string.' \\\\\n            template=jinja \\\\\n            \"context={}\" \"defaults={'template': 'cool'}\" \\\\\n            saltenv=base", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def export(self, remote_function):\n        \"\"\"\n        \"\"\"\n        if self._worker.mode is None:\n            # If the worker isn't connected, cache the function\n            # and export it later.\n            self._functions_to_export.append(remote_function)\n            return\n        if self._worker.mode != ray.worker.SCRIPT_MODE:\n            # Don't need to export if the worker is not a driver.\n            return\n        self._do_export(remote_function)", "output": "Export a remote function.\n\n        Args:\n            remote_function: the RemoteFunction object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def subcommand_not_found(self, command, string):\n        \"\"\"\n        \"\"\"\n        if isinstance(command, Group) and len(command.all_commands) > 0:\n            return 'Command \"{0.qualified_name}\" has no subcommand named {1}'.format(command, string)\n        return 'Command \"{0.qualified_name}\" has no subcommands.'.format(command)", "output": "|maybecoro|\n\n        A method called when a command did not have a subcommand requested in the help command.\n        This is useful to override for i18n.\n\n        Defaults to either:\n\n        - ``'Command \"{command.qualified_name}\" has no subcommands.'``\n            - If there is no subcommand in the ``command`` parameter.\n        - ``'Command \"{command.qualified_name}\" has no subcommand named {string}'``\n            - If the ``command`` parameter has subcommands but not one named ``string``.\n\n        Parameters\n        ------------\n        command: :class:`Command`\n            The command that did not have the subcommand requested.\n        string: :class:`str`\n            The string that contains the invalid subcommand. Note that this has\n            had mentions removed to prevent abuse.\n\n        Returns\n        ---------\n        :class:`str`\n            The string to use when the command did not have the subcommand requested.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create(self, cache_file):\n        \"\"\"\"\"\"\n        conn = sqlite3.connect(cache_file)\n        cur = conn.cursor()\n        cur.execute(\"PRAGMA foreign_keys = ON\")\n        cur.execute('''\n            CREATE TABLE jobs(\n                hash TEXT NOT NULL UNIQUE PRIMARY KEY, description TEXT NOT NULL,\n                last_run REAL, next_run REAL, last_run_result INTEGER)''')\n\n        cur.execute('''\n            CREATE TABLE history(\n                hash TEXT, description TEXT, time REAL, result INTEGER,\n                FOREIGN KEY(hash) REFERENCES jobs(hash))''')\n\n        conn.commit()\n        conn.close()", "output": "Create the tables needed to store the information.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            self.stat()\n        except OSError as e:\n            if e.errno not in (ENOENT, ENOTDIR):\n                raise\n            return False\n        return True", "output": "Whether this path exists.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mediatype_delete(mediatypeids, **kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'mediatype.delete'\n            if isinstance(mediatypeids, list):\n                params = mediatypeids\n            else:\n                params = [mediatypeids]\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result']['mediatypeids']\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": "Delete mediatype\n\n\n    :param interfaceids: IDs of the mediatypes to delete\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    :return: ID of deleted mediatype, False on failure.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.mediatype_delete 3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def MergeFrom(self, other):\n    \"\"\"\n    \"\"\"\n    self._values.extend(other._values)\n    self._message_listener.Modified()", "output": "Appends the contents of another repeated field of the same type to this\n    one. We do not check the types of the individual fields.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_logging(\n        self, log_level=logging.INFO, excluded_loggers=EXCLUDED_LOGGER_DEFAULTS, **kw\n    ):\n        \"\"\"\n        \"\"\"\n        handler = self.get_default_handler(**kw)\n        setup_logging(handler, log_level=log_level, excluded_loggers=excluded_loggers)", "output": "Attach default Stackdriver logging handler to the root logger.\n\n        This method uses the default log handler, obtained by\n        :meth:`~get_default_handler`, and attaches it to the root Python\n        logger, so that a call such as ``logging.warn``, as well as all child\n        loggers, will report to Stackdriver logging.\n\n        :type log_level: int\n        :param log_level: (Optional) Python logging log level. Defaults to\n                          :const:`logging.INFO`.\n\n        :type excluded_loggers: tuple\n        :param excluded_loggers: (Optional) The loggers to not attach the\n                                 handler to. This will always include the\n                                 loggers in the path of the logging client\n                                 itself.\n\n        :type kw: dict\n        :param kw: keyword args passed to handler constructor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetch(bank, key):\n    '''\n    \n    '''\n    redis_server = _get_redis_server()\n    redis_key = _get_key_redis_key(bank, key)\n    redis_value = None\n    try:\n        redis_value = redis_server.get(redis_key)\n    except (RedisConnectionError, RedisResponseError) as rerr:\n        mesg = 'Cannot fetch the Redis cache key {rkey}: {rerr}'.format(rkey=redis_key,\n                                                                        rerr=rerr)\n        log.error(mesg)\n        raise SaltCacheError(mesg)\n    if redis_value is None:\n        return {}\n    return __context__['serial'].loads(redis_value)", "output": "Fetch data from the Redis cache.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_storage_policies(policy_names=None, service_instance=None):\n    '''\n    \n    '''\n    profile_manager = salt.utils.pbm.get_profile_manager(service_instance)\n    if not policy_names:\n        policies = salt.utils.pbm.get_storage_policies(profile_manager,\n                                                       get_all_policies=True)\n    else:\n        policies = salt.utils.pbm.get_storage_policies(profile_manager,\n                                                       policy_names)\n    return [_get_policy_dict(p) for p in policies]", "output": "Returns a list of storage policies.\n\n    policy_names\n        Names of policies to list. If None, all policies are listed.\n        Default is None.\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.list_storage_policies\n\n        salt '*' vsphere.list_storage_policy policy_names=[policy_name]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send(self, obj):\n        \"\"\"\"\"\"\n        buf = io.BytesIO()\n        ForkingPickler(buf, pickle.HIGHEST_PROTOCOL).dump(obj)\n        self.send_bytes(buf.getvalue())", "output": "Send object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init():\n    '''\n    \n    '''\n    cache_file = _get_buckets_cache_filename()\n    exp = time.time() - S3_CACHE_EXPIRE\n\n    # check mtime of the buckets files cache\n    metadata = None\n    try:\n        if os.path.getmtime(cache_file) > exp:\n            metadata = _read_buckets_cache_file(cache_file)\n    except OSError:\n        pass\n\n    if metadata is None:\n        # bucket files cache expired or does not exist\n        metadata = _refresh_buckets_cache_file(cache_file)\n\n    return metadata", "output": "Connect to S3 and download the metadata for each file in all buckets\n    specified and cache the data to disk.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pvariance(self):\n        ''\n        res = self.price.groupby(level=1\n                                ).apply(lambda x: statistics.pvariance(x))\n        res.name = 'pvariance'\n        return res", "output": "\u8fd4\u56deDataStruct.price\u7684\u65b9\u5dee variance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_file(self, path, saltenv, back=None):\n        '''\n        \n        '''\n        path = salt.utils.stringutils.to_unicode(path)\n        saltenv = salt.utils.stringutils.to_unicode(saltenv)\n        back = self.backends(back)\n        kwargs = {}\n        fnd = {'path': '',\n               'rel': ''}\n        if os.path.isabs(path):\n            return fnd\n        if '../' in path:\n            return fnd\n        if salt.utils.url.is_escaped(path):\n            # don't attempt to find URL query arguments in the path\n            path = salt.utils.url.unescape(path)\n        else:\n            if '?' in path:\n                hcomps = path.split('?')\n                path = hcomps[0]\n                comps = hcomps[1].split('&')\n                for comp in comps:\n                    if '=' not in comp:\n                        # Invalid option, skip it\n                        continue\n                    args = comp.split('=', 1)\n                    kwargs[args[0]] = args[1]\n\n        if 'env' in kwargs:\n            # \"env\" is not supported; Use \"saltenv\".\n            kwargs.pop('env')\n        if 'saltenv' in kwargs:\n            saltenv = kwargs.pop('saltenv')\n\n        if not isinstance(saltenv, six.string_types):\n            saltenv = six.text_type(saltenv)\n\n        for fsb in back:\n            fstr = '{0}.find_file'.format(fsb)\n            if fstr in self.servers:\n                fnd = self.servers[fstr](path, saltenv, **kwargs)\n                if fnd.get('path'):\n                    fnd['back'] = fsb\n                    return fnd\n        return fnd", "output": "Find the path and return the fnd structure, this structure is passed\n        to other backend interfaces.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fit(self, X, y=None, **fit_params):\n        \"\"\"\n        \"\"\"\n        self.estimator.fit(X, y, **fit_params)\n        return self", "output": "Fit the StackingEstimator meta-transformer.\n\n        Parameters\n        ----------\n        X: array-like of shape (n_samples, n_features)\n            The training input samples.\n        y: array-like, shape (n_samples,)\n            The target values (integers that correspond to classes in classification, real numbers in regression).\n        fit_params:\n            Other estimator-specific parameters.\n\n        Returns\n        -------\n        self: object\n            Returns a copy of the estimator", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _extract_labels(time_series):\n    \"\"\"\"\"\"\n    labels = {\"resource_type\": time_series.resource.type}\n    labels.update(time_series.resource.labels)\n    labels.update(time_series.metric.labels)\n    return labels", "output": "Build the combined resource and metric labels, with resource_type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_params(self, fname):\n        \"\"\"\n        \"\"\"\n        save_dict = ndarray.load(fname)\n        arg_params = {}\n        aux_params = {}\n        for k, value in save_dict.items():\n            arg_type, name = k.split(':', 1)\n            if arg_type == 'arg':\n                arg_params[name] = value\n            elif arg_type == 'aux':\n                aux_params[name] = value\n            else:\n                raise ValueError(\"Invalid param file \" + fname)\n        self.set_params(arg_params, aux_params)", "output": "Loads model parameters from file.\n\n        Parameters\n        ----------\n        fname : str\n            Path to input param file.\n\n        Examples\n        --------\n        >>> # An example of loading module parameters.\n        >>> mod.load_params('myfile')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_second_run():\n    \"\"\"\"\"\"\n    tracker_path = _get_not_configured_usage_tracker_path()\n    if not tracker_path.exists():\n        return False\n\n    current_pid = _get_shell_pid()\n    with tracker_path.open('r') as tracker:\n        try:\n            info = json.load(tracker)\n        except ValueError:\n            return False\n\n    if not (isinstance(info, dict) and info.get('pid') == current_pid):\n        return False\n\n    return (_get_previous_command() == 'fuck' or\n            time.time() - info.get('time', 0) < const.CONFIGURATION_TIMEOUT)", "output": "Returns `True` when we know that `fuck` called second time.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mknfold(dall, nfold, param, seed, evals=(), fpreproc=None, stratified=False,\n            folds=None, shuffle=True):\n    \"\"\"\n    \n    \"\"\"\n    evals = list(evals)\n    np.random.seed(seed)\n\n    if stratified is False and folds is None:\n        # Do standard k-fold cross validation\n        if shuffle is True:\n            idx = np.random.permutation(dall.num_row())\n        else:\n            idx = np.arange(dall.num_row())\n        out_idset = np.array_split(idx, nfold)\n        in_idset = [\n            np.concatenate([out_idset[i] for i in range(nfold) if k != i])\n            for k in range(nfold)\n        ]\n    elif folds is not None:\n        # Use user specified custom split using indices\n        try:\n            in_idset = [x[0] for x in folds]\n            out_idset = [x[1] for x in folds]\n        except TypeError:\n            # Custom stratification using Sklearn KFoldSplit object\n            splits = list(folds.split(X=dall.get_label(), y=dall.get_label()))\n            in_idset = [x[0] for x in splits]\n            out_idset = [x[1] for x in splits]\n        nfold = len(out_idset)\n    else:\n        # Do standard stratefied shuffle k-fold split\n        sfk = XGBStratifiedKFold(n_splits=nfold, shuffle=True, random_state=seed)\n        splits = list(sfk.split(X=dall.get_label(), y=dall.get_label()))\n        in_idset = [x[0] for x in splits]\n        out_idset = [x[1] for x in splits]\n        nfold = len(out_idset)\n\n    ret = []\n    for k in range(nfold):\n        dtrain = dall.slice(in_idset[k])\n        dtest = dall.slice(out_idset[k])\n        # run preprocessing on the data set if needed\n        if fpreproc is not None:\n            dtrain, dtest, tparam = fpreproc(dtrain, dtest, param.copy())\n        else:\n            tparam = param\n        plst = list(tparam.items()) + [('eval_metric', itm) for itm in evals]\n        ret.append(CVPack(dtrain, dtest, plst))\n    return ret", "output": "Make an n-fold list of CVPack from random indices.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def concat(attrs, inputs, proto_obj):\n    \"\"\"  \"\"\"\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'axis': 'dim'})\n    return 'concat', new_attrs, inputs", "output": "Joins input arrays along a given axis.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toggle_highlighting(self, state):\r\n        \"\"\"\"\"\"\r\n        if self.editor is not None:\r\n            if state:\r\n                self.highlight_matches()\r\n            else:\r\n                self.clear_matches()", "output": "Toggle the 'highlight all results' feature", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_image_id(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The get_image_id function must be called with -f or --function.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    name = kwargs.get('name', None)\n    if name is None:\n        raise SaltCloudSystemExit(\n            'The get_image_id function requires a name.'\n        )\n\n    try:\n        ret = avail_images()[name]['id']\n    except KeyError:\n        raise SaltCloudSystemExit(\n            'The image \\'{0}\\' could not be found'.format(name)\n        )\n\n    return ret", "output": "Returns an image's ID from the given image name.\n\n    .. versionadded:: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f get_image_id opennebula name=my-image-name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_grpc_error(rpc_exc):\n    \"\"\"\n    \"\"\"\n    if isinstance(rpc_exc, grpc.Call):\n        return from_grpc_status(\n            rpc_exc.code(), rpc_exc.details(), errors=(rpc_exc,), response=rpc_exc\n        )\n    else:\n        return GoogleAPICallError(str(rpc_exc), errors=(rpc_exc,), response=rpc_exc)", "output": "Create a :class:`GoogleAPICallError` from a :class:`grpc.RpcError`.\n\n    Args:\n        rpc_exc (grpc.RpcError): The gRPC error.\n\n    Returns:\n        GoogleAPICallError: An instance of the appropriate subclass of\n            :class:`GoogleAPICallError`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def batch_to_ids(batch: List[List[str]]) -> torch.Tensor:\n    \"\"\"\n    \n    \"\"\"\n    instances = []\n    indexer = ELMoTokenCharactersIndexer()\n    for sentence in batch:\n        tokens = [Token(token) for token in sentence]\n        field = TextField(tokens,\n                          {'character_ids': indexer})\n        instance = Instance({\"elmo\": field})\n        instances.append(instance)\n\n    dataset = Batch(instances)\n    vocab = Vocabulary()\n    dataset.index_instances(vocab)\n    return dataset.as_tensor_dict()['elmo']['character_ids']", "output": "Converts a batch of tokenized sentences to a tensor representing the sentences with encoded characters\n    (len(batch), max sentence length, max word length).\n\n    Parameters\n    ----------\n    batch : ``List[List[str]]``, required\n        A list of tokenized sentences.\n\n    Returns\n    -------\n        A tensor of padded character ids.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_(path, **kwargs):\n    '''\n    \n    '''\n    kwargs = salt.utils.args.clean_kwargs(**kwargs)\n    hex_ = kwargs.pop('hex', False)\n    if kwargs:\n        salt.utils.args.invalid_kwargs(kwargs)\n\n    cmd = ['xattr', path]\n    try:\n        ret = salt.utils.mac_utils.execute_return_result(cmd)\n    except CommandExecutionError as exc:\n        if 'No such file' in exc.strerror:\n            raise CommandExecutionError('File not found: {0}'.format(path))\n        raise CommandExecutionError('Unknown Error: {0}'.format(exc.strerror))\n\n    if not ret:\n        return {}\n\n    attrs_ids = ret.split(\"\\n\")\n    attrs = {}\n\n    for id_ in attrs_ids:\n        attrs[id_] = read(path, id_, **{'hex': hex_})\n\n    return attrs", "output": "List all of the extended attributes on the given file/directory\n\n    :param str path: The file(s) to get attributes from\n\n    :param bool hex: Return the values with forced hexadecimal values\n\n    :return: A dictionary containing extended attributes and values for the\n        given file\n    :rtype: dict\n\n    :raises: CommandExecutionError on file not found or any other unknown error\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' xattr.list /path/to/file\n        salt '*' xattr.list /path/to/file hex=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unicode_string(string):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(string, six.text_type):\n        return string\n    try:\n        return string.decode(\"utf8\")\n    except UnicodeDecodeError:\n        return '[BASE64-DATA]' + base64.b64encode(string) + '[/BASE64-DATA]'", "output": "Make sure string is unicode, try to default with utf8, or base64 if failed.\n\n    can been decode by `decode_unicode_string`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_scala(app):\n    \"\"\"\"\"\"\n    if any(v in _BUILD_VER for v in ['1.2.', '1.3.', '1.4.']):\n        _run_cmd(\"cd %s/.. && make scalapkg\" % app.builder.srcdir)\n        _run_cmd(\"cd %s/.. && make scalainstall\" % app.builder.srcdir)\n    else:\n        _run_cmd(\"cd %s/../scala-package && mvn -B install -DskipTests\" % app.builder.srcdir)", "output": "build scala for scala docs, java docs, and clojure docs to use", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_all_cookies(self, path: str = \"/\", domain: str = None) -> None:\n        \"\"\"\n        \"\"\"\n        for name in self.request.cookies:\n            self.clear_cookie(name, path=path, domain=domain)", "output": "Deletes all the cookies the user sent with this request.\n\n        See `clear_cookie` for more information on the path and domain\n        parameters.\n\n        Similar to `set_cookie`, the effect of this method will not be\n        seen until the following request.\n\n        .. versionchanged:: 3.2\n\n           Added the ``path`` and ``domain`` parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_default_color_scheme(name, replace=True):\n    \"\"\"\"\"\"\n    assert name in sh.COLOR_SCHEME_NAMES\n    set_color_scheme(name, sh.get_color_scheme(name), replace=replace)", "output": "Reset color scheme to default values", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download_file(url, filename):\n    \"\"\"\"\"\"\n    r = _get_requests_session().get(url, stream=True)\n    if not r.ok:\n        raise IOError(\"Unable to download file\")\n\n    with open(filename, \"wb\") as f:\n        f.write(r.content)", "output": "Downloads file from url to a path with filename", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def append(self, key, _item):  # type: (Union[Key, str], Any) -> InlineTable\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(_item, Item):\n            _item = item(_item)\n\n        if not isinstance(_item, (Whitespace, Comment)):\n            if not _item.trivia.indent and len(self._value) > 0:\n                _item.trivia.indent = \" \"\n            if _item.trivia.comment:\n                _item.trivia.comment = \"\"\n\n        self._value.append(key, _item)\n\n        if isinstance(key, Key):\n            key = key.key\n\n        if key is not None:\n            super(InlineTable, self).__setitem__(key, _item)\n\n        return self", "output": "Appends a (key, item) to the table.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def thread_pool(self, thread_pool_patterns=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request('GET', _make_path('_cat',\n            'thread_pool', thread_pool_patterns), params=params)", "output": "Get information about thread pools.\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-thread-pool.html>`_\n\n        :arg thread_pool_patterns: A comma-separated list of regular-expressions\n            to filter the thread pools in the output\n        :arg format: a short version of the Accept header, e.g. json, yaml\n        :arg h: Comma-separated list of column names to display\n        :arg help: Return help information, default False\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg s: Comma-separated list of column names or column aliases to sort\n            by\n        :arg size: The multiplier in which to display values, valid choices are:\n            '', 'k', 'm', 'g', 't', 'p'\n        :arg v: Verbose mode. Display column headers, default False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _batch_norm_new_params(input_shape, rng, axis=(0, 1, 2),\n                           center=True, scale=True, **kwargs):\n  \"\"\"\"\"\"\n  del rng, kwargs\n  axis = (axis,) if np.isscalar(axis) else axis\n  shape = tuple(d for i, d in enumerate(input_shape) if i not in axis)\n  beta = np.zeros(shape, dtype='float32') if center else ()\n  gamma = np.ones(shape, dtype='float32') if scale else ()\n  return (beta, gamma)", "output": "Helper to initialize batch norm params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_value(value):\n    ''''''\n    if isinstance(value, bool):\n        return 'true' if value else 'false'\n    elif isinstance(value, six.string_types):\n        # parse compacted notation to dict\n        listparser = re.compile(r'''((?:[^,\"']|\"[^\"]*\"|'[^']*')+)''')\n\n        value = value.strip()\n        if value.startswith('[') and value.endswith(']'):\n            return listparser.split(value[1:-1])[1::2]\n        elif value.startswith('(') and value.endswith(')'):\n            rval = {}\n            for pair in listparser.split(value[1:-1])[1::2]:\n                pair = pair.split('=')\n                if '\"' in pair[1]:\n                    pair[1] = pair[1].replace('\"', '')\n                if pair[1].isdigit():\n                    rval[pair[0]] = int(pair[1])\n                elif pair[1] == 'true':\n                    rval[pair[0]] = True\n                elif pair[1] == 'false':\n                    rval[pair[0]] = False\n                else:\n                    rval[pair[0]] = pair[1]\n            return rval\n        else:\n            if '\"' in value:\n                value = value.replace('\"', '')\n            if value.isdigit():\n                return int(value)\n            elif value == 'true':\n                return True\n            elif value == 'false':\n                return False\n            else:\n                return value\n    else:\n        return value", "output": "Internal helper for parsing configuration values into python values", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_all(self, texts:Collection[str]) -> List[List[str]]:\n        \"\"\n        if self.n_cpus <= 1: return self._process_all_1(texts)\n        with ProcessPoolExecutor(self.n_cpus) as e:\n            return sum(e.map(self._process_all_1, partition_by_cores(texts, self.n_cpus)), [])", "output": "Process a list of `texts`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def using(self, client):\n        \"\"\"\n        \n\n        \"\"\"\n        s = self._clone()\n        s._using = client\n        return s", "output": "Associate the search request with an elasticsearch client. A fresh copy\n        will be returned with current instance remaining unchanged.\n\n        :arg client: an instance of ``elasticsearch.Elasticsearch`` to use or\n            an alias to look up in ``elasticsearch_dsl.connections``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transform(instance, tokenizer, max_seq_length, max_predictions_per_seq, do_pad=True):\n    \"\"\"\"\"\"\n    pad = tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n    input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)\n    input_mask = [1] * len(input_ids)\n    segment_ids = list(instance.segment_ids)\n    assert len(input_ids) <= max_seq_length\n    valid_lengths = len(input_ids)\n\n    masked_lm_positions = list(instance.masked_lm_positions)\n    masked_lm_ids = tokenizer.convert_tokens_to_ids(\n        instance.masked_lm_labels)\n    masked_lm_weights = [1.0] * len(masked_lm_ids)\n    masked_lm_valid_lengths = len(masked_lm_ids)\n    if do_pad:\n        while len(input_ids) < max_seq_length:\n            input_ids.append(pad)\n            # Padding index MUST be defined to 0 on input_mask, segment_ids\n            input_mask.append(0)\n            segment_ids.append(0)\n        while len(masked_lm_positions) < max_predictions_per_seq:\n            masked_lm_positions.append(0)\n            masked_lm_ids.append(pad)\n            masked_lm_weights.append(0.0)\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n\n    next_sentence_label = 1 if instance.is_random_next else 0\n\n    features = {}\n    features['input_ids'] = input_ids\n    features['input_mask'] = input_mask\n    features['segment_ids'] = segment_ids\n    features['masked_lm_positions'] = masked_lm_positions\n    features['masked_lm_ids'] = masked_lm_ids\n    features['segment_a_lengths'] = [instance.segment_a_lengths]\n    features['segment_b_lengths'] = [instance.segment_b_lengths]\n    features['masked_lm_ids'] = masked_lm_ids\n    features['masked_lm_weights'] = masked_lm_weights\n    features['next_sentence_labels'] = [next_sentence_label]\n    features['valid_lengths'] = [valid_lengths]\n    features['masked_lm_valid_lengths'] = [masked_lm_valid_lengths]\n\n    return features", "output": "Transform instance to inputs for MLM and NSP.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_line(self):\n        \"\"\"\n        \n        \"\"\"\n        self.cursor.movePosition(self.cursor.StartOfBlock)\n        text = self.cursor.block().text()\n        lindent = len(text) - len(text.lstrip())\n        self.cursor.setPosition(self.cursor.block().position() + lindent)\n        self.cursor.movePosition(self.cursor.EndOfBlock,\n                                 self.cursor.KeepAnchor)", "output": "Select the entire line but starts at the first non whitespace character\n        and stops at the non-whitespace character.\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def layout_route(self, request):\n    \n    \"\"\"\n    body = self.layout_impl()\n    return http_util.Respond(request, body, 'application/json')", "output": "r\"\"\"Fetches the custom layout specified by the config file in the logdir.\n\n    If more than 1 run contains a layout, this method merges the layouts by\n    merging charts within individual categories. If 2 categories with the same\n    name are found, the charts within are merged. The merging is based on the\n    order of the runs to which the layouts are written.\n\n    The response is a JSON object mirroring properties of the Layout proto if a\n    layout for any run is found.\n\n    The response is an empty object if no layout could be found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_example_line(lisp_string: str) -> Dict:\n    \"\"\"\n    \n    \"\"\"\n    id_piece, rest = lisp_string.split(') (utterance \"')\n    example_id = id_piece.split('(id ')[1]\n    question, rest = rest.split('\") (context (graph tables.TableKnowledgeGraph ')\n    table_filename, rest = rest.split(')) (targetValue (list')\n    target_value_strings = rest.strip().split(\"(description\")\n    target_values = []\n    for string in target_value_strings:\n        string = string.replace(\")\", \"\").replace('\"', '').strip()\n        if string != \"\":\n            target_values.append(string)\n    return {'id': example_id,\n            'question': question,\n            'table_filename': table_filename,\n            'target_values': target_values}", "output": "Training data in WikitableQuestions comes with examples in the form of lisp strings in the format:\n        (example (id <example-id>)\n                 (utterance <question>)\n                 (context (graph tables.TableKnowledgeGraph <table-filename>))\n                 (targetValue (list (description <answer1>) (description <answer2>) ...)))\n\n    We parse such strings and return the parsed information here.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_attribute(self, name):\n        \"\"\"\n\n        \"\"\"\n\n        attributeValue = ''\n        if self._w3c:\n            attributeValue = self.parent.execute_script(\n                \"return (%s).apply(null, arguments);\" % getAttribute_js,\n                self, name)\n        else:\n            resp = self._execute(Command.GET_ELEMENT_ATTRIBUTE, {'name': name})\n            attributeValue = resp.get('value')\n            if attributeValue is not None:\n                if name != 'value' and attributeValue.lower() in ('true', 'false'):\n                    attributeValue = attributeValue.lower()\n        return attributeValue", "output": "Gets the given attribute or property of the element.\n\n        This method will first try to return the value of a property with the\n        given name. If a property with that name doesn't exist, it returns the\n        value of the attribute with the same name. If there's no attribute with\n        that name, ``None`` is returned.\n\n        Values which are considered truthy, that is equals \"true\" or \"false\",\n        are returned as booleans.  All other non-``None`` values are returned\n        as strings.  For attributes or properties which do not exist, ``None``\n        is returned.\n\n        :Args:\n            - name - Name of the attribute/property to retrieve.\n\n        Example::\n\n            # Check if the \"active\" CSS class is applied to an element.\n            is_active = \"active\" in target_element.get_attribute(\"class\")", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def initialize_plugin(self):\n        \"\"\"\n        \n        \"\"\"\n        self.create_toggle_view_action()\n\n        self.plugin_actions = self.get_plugin_actions() + [MENU_SEPARATOR,\n                                                           self.undock_action]\n        add_actions(self.options_menu, self.plugin_actions)\n        self.options_button.setMenu(self.options_menu)\n        self.options_menu.aboutToShow.connect(self.refresh_actions)\n\n        self.sig_show_message.connect(self.show_message)\n        self.sig_update_plugin_title.connect(self.update_plugin_title)\n        self.sig_option_changed.connect(self.set_option)\n        self.setWindowTitle(self.get_plugin_title())", "output": "Initialize plugin: connect signals, setup actions, etc.\n\n        It must be run at the end of __init__", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def competition_list_files(self, competition):\n        \"\"\" \n        \"\"\"\n        competition_list_files_result = self.process_response(\n            self.competitions_data_list_files_with_http_info(id=competition))\n        return [File(f) for f in competition_list_files_result]", "output": "list files for competition\n             Parameters\n            ==========\n            competition: the name of the competition", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exec_command(command, cwd=None):\n    \n    '''\n\n    rc = None\n    stdout = stderr = None\n    if ssh_conn is None:\n        ld_library_path = {'LD_LIBRARY_PATH': '.:%s' % os.environ.get('LD_LIBRARY_PATH', '')}\n        p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, env=ld_library_path, cwd=cwd)\n        stdout, stderr = p.communicate()\n        rc = p.returncode\n    else:\n        # environment= requires paramiko >= 2.1 (fails with 2.0.2)\n        final_command = command if cwd is None else 'cd %s && %s %s' % (cwd, 'LD_LIBRARY_PATH=.:$LD_LIBRARY_PATH', command)\n        ssh_stdin, ssh_stdout, ssh_stderr = ssh_conn.exec_command(final_command)\n        stdout = ''.join(ssh_stdout.readlines())\n        stderr = ''.join(ssh_stderr.readlines())\n        rc = ssh_stdout.channel.recv_exit_status()\n\n    return rc, stdout, stderr", "output": "r'''\n    Helper to exec locally (subprocess) or remotely (paramiko)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __get_idxs(self, words):\n        \"\"\"\"\"\"\n        if self.bow:\n            return list(itertools.chain.from_iterable(\n                [self.positions[z] for z in words]))\n        else:\n            return self.positions[words]", "output": "Returns indexes to appropriate words.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def node_label_absent(name, node, **kwargs):\n    '''\n    \n    '''\n\n    ret = {'name': name,\n           'changes': {},\n           'result': False,\n           'comment': ''}\n\n    labels = __salt__['kubernetes.node_labels'](node, **kwargs)\n\n    if name not in labels:\n        ret['result'] = True if not __opts__['test'] else None\n        ret['comment'] = 'The label does not exist'\n        return ret\n\n    if __opts__['test']:\n        ret['comment'] = 'The label is going to be deleted'\n        ret['result'] = None\n        return ret\n\n    __salt__['kubernetes.node_remove_label'](\n        node_name=node,\n        label_name=name,\n        **kwargs)\n\n    ret['result'] = True\n    ret['changes'] = {\n        'kubernetes.node_label': {\n            'new': 'absent', 'old': 'present'}}\n    ret['comment'] = 'Label removed from node'\n\n    return ret", "output": "Ensures that the named label is absent from the node.\n\n    name\n        The name of the label\n\n    node\n        The name of the node", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_monday_or_tuesday(dt):\n    \"\"\"\n    \n    \"\"\"\n    dow = dt.weekday()\n    if dow == 5 or dow == 6:\n        return dt + timedelta(2)\n    elif dow == 0:\n        return dt + timedelta(1)\n    return dt", "output": "For second holiday of two adjacent ones!\n    If holiday falls on Saturday, use following Monday instead;\n    if holiday falls on Sunday or Monday, use following Tuesday instead\n    (because Monday is already taken by adjacent holiday on the day before)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_field_as(value, _as=None):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(value, MountedType):\n        return value\n    elif isinstance(value, UnmountedType):\n        if _as is None:\n            return value\n        return _as.mounted(value)", "output": "Get type mounted", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_to_str(l: Node) -> str:\n    \"\"\"\n        \n    \"\"\"\n    result = \"\"\n    while l:\n        result += str(l.val)\n        l = l.next\n    return result", "output": "converts the non-negative number list into a string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_scale(self, overflow):\n        \"\"\"\"\"\"\n        iter_since_rescale = self._num_steps - self._last_rescale_iter\n        if overflow:\n            self._last_overflow_iter = self._num_steps\n            self._overflows_since_rescale += 1\n            percentage = self._overflows_since_rescale / float(iter_since_rescale)\n            # we tolerate a certrain amount of NaNs before actually scaling it down\n            if percentage >= self.tolerance:\n                self.loss_scale /= self.scale_factor\n                self._last_rescale_iter = self._num_steps\n                self._overflows_since_rescale = 0\n                logging.info('DynamicLossScaler: overflow detected. set loss_scale = %s',\n                             self.loss_scale)\n        elif (self._num_steps - self._last_overflow_iter) % self.scale_window == 0:\n            self.loss_scale *= self.scale_factor\n            self._last_rescale_iter = self._num_steps\n        self._num_steps += 1", "output": "dynamically update loss scale", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _full_reduce(self, axis, map_func, reduce_func=None):\n        \"\"\"\n        \"\"\"\n        if reduce_func is None:\n            reduce_func = map_func\n\n        mapped_parts = self.data.map_across_blocks(map_func)\n        full_frame = mapped_parts.map_across_full_axis(axis, reduce_func)\n        if axis == 0:\n            columns = self.columns\n            return self.__constructor__(\n                full_frame, index=[\"__reduced__\"], columns=columns\n            )\n        else:\n            index = self.index\n            return self.__constructor__(\n                full_frame, index=index, columns=[\"__reduced__\"]\n            )", "output": "Apply function that will reduce the data to a Pandas Series.\n\n        Args:\n            axis: 0 for columns and 1 for rows. Default is 0.\n            map_func: Callable function to map the dataframe.\n            reduce_func: Callable function to reduce the dataframe. If none,\n                then apply map_func twice.\n\n        Return:\n            A new QueryCompiler object containing the results from map_func and\n            reduce_func.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_same_name_files(files_path_list, filename):\r\n    \"\"\"\"\"\"\r\n    same_name_files = []\r\n    for fname in files_path_list:\r\n        if filename == os.path.basename(fname):\r\n            same_name_files.append(path_components(fname))\r\n    return same_name_files", "output": "Get a list of the path components of the files with the same name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write(self, model:nn.Module, tbwriter:SummaryWriter, input_to_model:torch.Tensor)->None:\n        \"\"\n        request = GraphTBRequest(model=model, tbwriter=tbwriter, input_to_model=input_to_model)\n        asyncTBWriter.request_write(request)", "output": "Writes model graph to Tensorboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_batchnorm(builder, layer, input_names, output_names, keras_layer):\n    \"\"\"\n    \n    \"\"\"\n\n    # Get input and output names\n    input_name, output_name = (input_names[0], output_names[0])\n\n    # Currently CoreML supports only per-channel batch-norm\n    if keras_layer.mode != 0:\n        raise NotImplementedError(\n            'Currently supports only per-feature normalization')\n\n    axis = keras_layer.axis\n    nb_channels = keras_layer.input_shape[axis]\n\n\n    # Set parameters\n    # Parameter arrangement in Keras: gamma, beta, mean, variance\n    gamma = keras_layer.get_weights()[0]\n    beta = keras_layer.get_weights()[1]\n    mean = keras_layer.get_weights()[2]\n    std = keras_layer.get_weights()[3]\n    # compute adjusted parameters\n    variance = std * std\n    f = 1.0 / np.sqrt(std + keras_layer.epsilon)\n    gamma1 = gamma*f\n    beta1 = beta - gamma*mean*f\n    mean[:] = 0.0 #mean\n    variance[:] = 1.0 - .00001 #stddev\n\n    builder.add_batchnorm(\n        name = layer,\n        channels = nb_channels,\n        gamma = gamma1,\n        beta = beta1,\n        mean = mean,\n        variance = variance,\n        input_name = input_name,\n        output_name = output_name)", "output": "Parameters\n    keras_layer: layer\n        A keras layer object.\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stat_file(path, saltenv='base', octal=True):\n    '''\n    \n    '''\n    path, senv = salt.utils.url.split_env(path)\n    if senv:\n        saltenv = senv\n\n    stat = _client().hash_and_stat_file(path, saltenv)[1]\n    if stat is None:\n        return stat\n    return salt.utils.files.st_mode_to_octal(stat[0]) if octal is True else stat[0]", "output": "Return the permissions of a file, to get the permissions of a file on the\n    salt master file server prepend the path with salt://<file on server>\n    otherwise, prepend the file with / for a local file.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cp.stat_file salt://path/to/file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetcher_loop_v1(data_queue, data_buffer, pin_memory=False,\n                    pin_device_id=0, data_buffer_lock=None):\n    \"\"\"\"\"\"\n    while True:\n        idx, batch = data_queue.get()\n        if idx is None:\n            break\n        if pin_memory:\n            batch = _as_in_context(batch, context.cpu_pinned(pin_device_id))\n        else:\n            batch = _as_in_context(batch, context.cpu())\n        if data_buffer_lock is not None:\n            with data_buffer_lock:\n                data_buffer[idx] = batch\n        else:\n            data_buffer[idx] = batch", "output": "Fetcher loop for fetching data from queue and put in reorder dict.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_rev_args(receive_msg):\n    \"\"\" \n    \"\"\"\n    global trainloader\n    global testloader\n    global net\n\n    # Loading Data\n    logger.debug(\"Preparing data..\")\n\n    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n    y_train = to_categorical(y_train, 10)\n    y_test = to_categorical(y_test, 10)\n    x_train = x_train.reshape(x_train.shape+(1,)).astype(\"float32\")\n    x_test = x_test.reshape(x_test.shape+(1,)).astype(\"float32\")\n    x_train /= 255.0\n    x_test /= 255.0\n    trainloader = (x_train, y_train)\n    testloader = (x_test, y_test)\n\n    # Model\n    logger.debug(\"Building model..\")\n    net = build_graph_from_json(receive_msg)\n\n    # parallel model\n    try:\n        available_devices = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n        gpus = len(available_devices.split(\",\"))\n        if gpus > 1:\n            net = multi_gpu_model(net, gpus)\n    except KeyError:\n        logger.debug(\"parallel model not support in this config settings\")\n\n    if args.optimizer == \"SGD\":\n        optimizer = SGD(lr=args.learning_rate, momentum=0.9, decay=args.weight_decay)\n    if args.optimizer == \"Adadelta\":\n        optimizer = Adadelta(lr=args.learning_rate, decay=args.weight_decay)\n    if args.optimizer == \"Adagrad\":\n        optimizer = Adagrad(lr=args.learning_rate, decay=args.weight_decay)\n    if args.optimizer == \"Adam\":\n        optimizer = Adam(lr=args.learning_rate, decay=args.weight_decay)\n    if args.optimizer == \"Adamax\":\n        optimizer = Adamax(lr=args.learning_rate, decay=args.weight_decay)\n    if args.optimizer == \"RMSprop\":\n        optimizer = RMSprop(lr=args.learning_rate, decay=args.weight_decay)\n\n    # Compile the model\n    net.compile(\n        loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"]\n    )\n    return 0", "output": "parse reveive msgs to global variable", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attention_lm_small():\n  \"\"\"\n  \"\"\"\n  hparams = attention_lm_base()\n  hparams.num_hidden_layers = 4\n  hparams.hidden_size = 512\n  hparams.filter_size = 2048\n  hparams.layer_prepostprocess_dropout = 0.5\n  return hparams", "output": "Cheap model.\n\n  on lm1b_32k:\n     45M params\n     2 steps/sec on  [GeForce GTX TITAN X]\n\n  Returns:\n    an hparams object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deserialize(stream_or_string, **options):\n    '''\n    \n    '''\n\n    try:\n        if not isinstance(stream_or_string, (bytes, six.string_types)):\n            return toml.load(stream_or_string, **options)\n\n        if isinstance(stream_or_string, bytes):\n            stream_or_string = stream_or_string.decode('utf-8')\n\n        return toml.loads(stream_or_string)\n    except Exception as error:\n        raise DeserializationError(error)", "output": "Deserialize from TOML into Python data structure.\n\n    :param stream_or_string: toml stream or string to deserialize.\n    :param options: options given to lower pytoml module.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def interpolations_to_summary(sample_ind, interpolations, first_frame,\n                              last_frame, hparams, decode_hp):\n  \"\"\"\n  \"\"\"\n  parent_tag = \"sample_%d\" % sample_ind\n  frame_shape = hparams.problem.frame_shape\n  interp_shape = [hparams.batch_size, decode_hp.num_interp] + frame_shape\n  interpolations = np.reshape(interpolations, interp_shape)\n  interp_tag = \"%s/interp/%s\" % (parent_tag, decode_hp.channel_interp)\n  if decode_hp.channel_interp == \"ranked\":\n    interp_tag = \"%s/rank_%d\" % (interp_tag, decode_hp.rank_interp)\n  summaries, _ = common_video.py_gif_summary(\n      interp_tag, interpolations, return_summary_value=True,\n      max_outputs=decode_hp.max_display_outputs,\n      fps=decode_hp.frames_per_second)\n\n  if decode_hp.save_frames:\n    first_frame_summ = image_utils.image_to_tf_summary_value(\n        first_frame, \"%s/first\" % parent_tag)\n    last_frame_summ = image_utils.image_to_tf_summary_value(\n        last_frame, \"%s/last\" % parent_tag)\n    summaries.append(first_frame_summ)\n    summaries.append(last_frame_summ)\n  return summaries", "output": "Converts interpolated frames into tf summaries.\n\n  The summaries consists of:\n    1. Image summary corresponding to the first frame.\n    2. Image summary corresponding to the last frame.\n    3. The interpolated frames as a gif summary.\n\n  Args:\n    sample_ind: int\n    interpolations: Numpy array, shape=(num_interp, H, W, 3)\n    first_frame: Numpy array, shape=(HWC)\n    last_frame: Numpy array, shape=(HWC)\n    hparams: HParams, train hparams\n    decode_hp: HParams, decode hparams\n  Returns:\n    summaries: list of tf Summary Values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attention_lm_decoder(decoder_input,\n                         decoder_self_attention_bias,\n                         hparams,\n                         name=\"decoder\"):\n  \"\"\"\n  \"\"\"\n  x = decoder_input\n  with tf.variable_scope(name):\n    for layer in range(hparams.num_hidden_layers):\n      with tf.variable_scope(\"layer_%d\" % layer):\n        with tf.variable_scope(\"self_attention\"):\n          y = common_attention.multihead_attention(\n              common_layers.layer_preprocess(\n                  x, hparams), None, decoder_self_attention_bias,\n              hparams.attention_key_channels or hparams.hidden_size,\n              hparams.attention_value_channels or hparams.hidden_size,\n              hparams.hidden_size, hparams.num_heads, hparams.attention_dropout)\n          x = common_layers.layer_postprocess(x, y, hparams)\n        with tf.variable_scope(\"ffn\"):\n          y = common_layers.conv_hidden_relu(\n              common_layers.layer_preprocess(x, hparams),\n              hparams.filter_size,\n              hparams.hidden_size,\n              dropout=hparams.relu_dropout)\n          x = common_layers.layer_postprocess(x, y, hparams)\n    return common_layers.layer_preprocess(x, hparams)", "output": "A stack of attention_lm layers.\n\n  Args:\n    decoder_input: a Tensor\n    decoder_self_attention_bias: bias Tensor for self-attention\n      (see common_attention.attention_bias())\n    hparams: hyperparameters for model\n    name: a string\n\n  Returns:\n    y: a Tensors", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _gen_param_header(name, doc, defaultValueStr, typeConverter):\n    \"\"\"\n    \n    \"\"\"\n    template = '''class Has$Name(Params):\n    \"\"\"\n    Mixin for param $name: $doc\n    \"\"\"\n\n    $name = Param(Params._dummy(), \"$name\", \"$doc\", typeConverter=$typeConverter)\n\n    def __init__(self):\n        super(Has$Name, self).__init__()'''\n\n    if defaultValueStr is not None:\n        template += '''\n        self._setDefault($name=$defaultValueStr)'''\n\n    Name = name[0].upper() + name[1:]\n    if typeConverter is None:\n        typeConverter = str(None)\n    return template \\\n        .replace(\"$name\", name) \\\n        .replace(\"$Name\", Name) \\\n        .replace(\"$doc\", doc) \\\n        .replace(\"$defaultValueStr\", str(defaultValueStr)) \\\n        .replace(\"$typeConverter\", typeConverter)", "output": "Generates the header part for shared variables\n\n    :param name: param name\n    :param doc: param doc", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def debug(self, value):\n        \"\"\"\n        \n        \"\"\"\n        self._debug = value\n\n        if self._debug:\n            # Turn on debug logging\n            logging.getLogger().setLevel(logging.DEBUG)", "output": "Turn on debug logging if necessary.\n\n        :param value: Value of debug flag", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def weight_decay(decay_rate, var_list, skip_biases=True):\n  \"\"\"\"\"\"\n  if not decay_rate:\n    return 0.\n\n  tf.logging.info(\"Applying weight decay, decay_rate: %0.5f\", decay_rate)\n\n  weight_decays = []\n  for v in var_list:\n    # Weight decay.\n    # This is a heuristic way to detect biases that works for main tf.layers.\n    is_bias = len(v.shape.as_list()) == 1 and v.name.endswith(\"bias:0\")\n    if not (skip_biases and is_bias):\n      with tf.device(v.device):\n        v_loss = tf.nn.l2_loss(v)\n      weight_decays.append(v_loss)\n\n  return tf.add_n(weight_decays) * decay_rate", "output": "Apply weight decay to vars in var_list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_pascal(image_set, year, devkit_path, shuffle=False):\n    \"\"\"\n    \n    \"\"\"\n    image_set = [y.strip() for y in image_set.split(',')]\n    assert image_set, \"No image_set specified\"\n    year = [y.strip() for y in year.split(',')]\n    assert year, \"No year specified\"\n\n    # make sure (# sets == # years)\n    if len(image_set) > 1 and len(year) == 1:\n        year = year * len(image_set)\n    if len(image_set) == 1 and len(year) > 1:\n        image_set = image_set * len(year)\n    assert len(image_set) == len(year), \"Number of sets and year mismatch\"\n\n    imdbs = []\n    for s, y in zip(image_set, year):\n        imdbs.append(PascalVoc(s, y, devkit_path, shuffle, is_train=True))\n    if len(imdbs) > 1:\n        return ConcatDB(imdbs, shuffle)\n    else:\n        return imdbs[0]", "output": "wrapper function for loading pascal voc dataset\n\n    Parameters:\n    ----------\n    image_set : str\n        train, trainval...\n    year : str\n        2007, 2012 or combinations splitted by comma\n    devkit_path : str\n        root directory of dataset\n    shuffle : bool\n        whether to shuffle initial list\n\n    Returns:\n    ----------\n    Imdb", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_interval_filter(interval):\n  \"\"\"\n  \"\"\"\n  def filter_fn(value):\n    if (not isinstance(value, six.integer_types) and\n        not isinstance(value, float)):\n      raise error.HParamsError(\n          'Cannot use an interval filter for a value of type: %s, Value: %s' %\n          (type(value), value))\n    return interval.min_value <= value and value <= interval.max_value\n\n  return filter_fn", "output": "Returns a function that checkes whether a number belongs to an interval.\n\n  Args:\n    interval: A tensorboard.hparams.Interval protobuf describing the interval.\n  Returns:\n    A function taking a number (a float or an object of a type in\n    six.integer_types) that returns True if the number belongs to (the closed)\n    'interval'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _guess_apiserver(apiserver_url=None):\n    '''\n    '''\n    default_config = \"/etc/kubernetes/config\"\n    if apiserver_url is not None:\n        return apiserver_url\n    if \"KUBERNETES_MASTER\" in os.environ:\n        apiserver_url = os.environ.get(\"KUBERNETES_MASTER\")\n    elif __salt__['config.get']('k8s:master'):\n        apiserver_url = __salt__['config.get']('k8s:master')\n    elif os.path.exists(default_config) or __salt__['config.get']('k8s:config', \"\"):\n        config = __salt__['config.get']('k8s:config', default_config)\n        kubeapi_regex = re.compile(\"\"\"KUBE_MASTER=['\"]--master=(.*)['\"]\"\"\",\n                                   re.MULTILINE)\n        with salt.utils.files.fopen(config) as fh_k8s:\n            for line in fh_k8s.readlines():\n                match_line = kubeapi_regex.match(line)\n            if match_line:\n                apiserver_url = match_line.group(1)\n    else:\n        # we failed to discover, lets use k8s default address\n        apiserver_url = \"http://127.0.0.1:8080\"\n    log.debug(\"Discoverd k8s API server address: %s\", apiserver_url)\n    return apiserver_url", "output": "Try to guees the kubemaster url from environ,\n    then from `/etc/kubernetes/config` file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_positive_mask(X, y, model_generator, method_name, num_fcounts=11):\n    \"\"\" \n    \"\"\"\n    return __run_measure(measures.remove_mask, X, y, model_generator, method_name, 1, num_fcounts, __mean_pred)", "output": "Remove Positive (mask)\n    xlabel = \"Max fraction of features removed\"\n    ylabel = \"Negative mean model output\"\n    transform = \"negate\"\n    sort_order = 7", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_dvportgroup_security_policy(pg_name, sec_policy, sec_policy_conf):\n    '''\n    \n    '''\n    log.trace('Building portgroup\\'s \\'%s\\' security policy ', pg_name)\n    if 'allow_promiscuous' in sec_policy_conf:\n        sec_policy.allowPromiscuous = vim.BoolPolicy()\n        sec_policy.allowPromiscuous.value = \\\n                sec_policy_conf['allow_promiscuous']\n    if 'forged_transmits' in sec_policy_conf:\n        sec_policy.forgedTransmits = vim.BoolPolicy()\n        sec_policy.forgedTransmits.value = sec_policy_conf['forged_transmits']\n    if 'mac_changes' in sec_policy_conf:\n        sec_policy.macChanges = vim.BoolPolicy()\n        sec_policy.macChanges.value = sec_policy_conf['mac_changes']", "output": "Applies the values in sec_policy_conf to a security policy object\n\n    pg_name\n        The name of the portgroup\n\n    sec_policy\n        The vim.DVSTrafficShapingPolicy to apply the config to\n\n    sec_policy_conf\n        The out shaping config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def for_model(self, fn):\n        \"\"\"\n        \"\"\"\n        return ray.get(self.workers[0].for_model.remote(fn))", "output": "Apply the given function to a single model replica.\n\n        Returns:\n            Result from applying the function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_all_topics(region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    cache_key = _cache_get_key()\n    try:\n        return __context__[cache_key]\n    except KeyError:\n        pass\n\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    __context__[cache_key] = {}\n    # TODO: support >100 SNS topics (via NextToken)\n    topics = conn.get_all_topics()\n    for t in topics['ListTopicsResponse']['ListTopicsResult']['Topics']:\n        short_name = t['TopicArn'].split(':')[-1]\n        __context__[cache_key][short_name] = t['TopicArn']\n    return __context__[cache_key]", "output": "Returns a list of the all topics..\n\n    CLI example::\n\n        salt myminion boto_sns.get_all_topics", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def listener(cls, name=None):\n        \"\"\"\n        \"\"\"\n\n        if name is not None and not isinstance(name, str):\n            raise TypeError('Cog.listener expected str but received {0.__class__.__name__!r} instead.'.format(name))\n\n        def decorator(func):\n            actual = func\n            if isinstance(actual, staticmethod):\n                actual = actual.__func__\n            if not inspect.iscoroutinefunction(actual):\n                raise TypeError('Listener function must be a coroutine function.')\n            actual.__cog_listener__ = True\n            to_assign = name or actual.__name__\n            try:\n                actual.__cog_listener_names__.append(to_assign)\n            except AttributeError:\n                actual.__cog_listener_names__ = [to_assign]\n            # we have to return `func` instead of `actual` because\n            # we need the type to be `staticmethod` for the metaclass\n            # to pick it up but the metaclass unfurls the function and\n            # thus the assignments need to be on the actual function\n            return func\n        return decorator", "output": "A decorator that marks a function as a listener.\n\n        This is the cog equivalent of :meth:`.Bot.listen`.\n\n        Parameters\n        ------------\n        name: :class:`str`\n            The name of the event being listened to. If not provided, it\n            defaults to the function's name.\n\n        Raises\n        --------\n        TypeError\n            The function is not a coroutine function or a string was not passed as\n            the name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def me(self):\n        \"\"\"\"\"\"\n        return self.guild.me if self.guild is not None else self.bot.user", "output": "Similar to :attr:`.Guild.me` except it may return the :class:`.ClientUser` in private message contexts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def local_global_attention(x,\n                           self_attention_bias,\n                           hparams,\n                           q_padding=\"LEFT\",\n                           kv_padding=\"LEFT\"):\n  \"\"\"\"\"\"\n  with tf.variable_scope(\"self_local_global_att\"):\n    [x_global, x_local] = tf.split(x, 2, axis=-1)\n    split_hidden_size = int(hparams.hidden_size / 2)\n    split_heads = int(hparams.num_heads / 2)\n    if self_attention_bias is not None:\n      self_attention_bias = get_self_attention_bias(x)\n    y_global = common_attention.multihead_attention(\n        x_global,\n        None,\n        self_attention_bias,\n        hparams.attention_key_channels or split_hidden_size,\n        hparams.attention_value_channels or split_hidden_size,\n        split_hidden_size,\n        split_heads,\n        hparams.attention_dropout,\n        q_filter_width=hparams.q_filter_width,\n        kv_filter_width=hparams.kv_filter_width,\n        q_padding=q_padding,\n        kv_padding=kv_padding,\n        name=\"global_self_att\")\n    y_local = common_attention.multihead_attention(\n        x_local,\n        None,\n        None,\n        hparams.attention_key_channels or split_hidden_size,\n        hparams.attention_value_channels or split_hidden_size,\n        split_hidden_size,\n        split_heads,\n        hparams.attention_dropout,\n        attention_type=\"local_masked\",\n        block_length=hparams.block_length,\n        block_width=hparams.block_width,\n        q_filter_width=hparams.q_filter_width,\n        kv_filter_width=hparams.kv_filter_width,\n        q_padding=q_padding,\n        kv_padding=kv_padding,\n        name=\"local_self_att\")\n    y = tf.concat([y_global, y_local], axis=-1)\n    return y", "output": "Local and global 1d self attention.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve_image_id(name):\n    '''\n    \n    '''\n    try:\n        inspect_result = inspect_image(name)\n        return inspect_result['Id']\n    except CommandExecutionError:\n        # No matching image pulled locally, or inspect_image otherwise failed\n        pass\n    except KeyError:\n        log.error(\n            'Inspecting docker image \\'%s\\' returned an unexpected data '\n            'structure: %s', name, inspect_result\n        )\n    return False", "output": ".. versionadded:: 2018.3.0\n\n    Given an image name (or partial image ID), return the full image ID. If no\n    match is found among the locally-pulled images, then ``False`` will be\n    returned.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt myminion docker.resolve_image_id foo\n        salt myminion docker.resolve_image_id foo:bar\n        salt myminion docker.resolve_image_id 36540f359ca3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tail(self, since, filter_pattern, limit=10000, keep_open=True, colorize=True, http=False, non_http=False, force_colorize=False):\n        \"\"\"\n        \n        \"\"\"\n\n        try:\n            since_stamp = string_to_timestamp(since)\n\n            last_since = since_stamp\n            while True:\n                new_logs = self.zappa.fetch_logs(\n                    self.lambda_name,\n                    start_time=since_stamp,\n                    limit=limit,\n                    filter_pattern=filter_pattern,\n                    )\n\n                new_logs = [ e for e in new_logs if e['timestamp'] > last_since ]\n                self.print_logs(new_logs, colorize, http, non_http, force_colorize)\n\n                if not keep_open:\n                    break\n                if new_logs:\n                    last_since = new_logs[-1]['timestamp']\n                time.sleep(1)\n        except KeyboardInterrupt: # pragma: no cover\n            # Die gracefully\n            try:\n                sys.exit(0)\n            except SystemExit:\n                os._exit(130)", "output": "Tail this function's logs.\n\n        if keep_open, do so repeatedly, printing any new logs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def envs(ignore_cache=False):\n    '''\n    \n    '''\n    if not ignore_cache:\n        env_cache = os.path.join(__opts__['cachedir'], 'svnfs/envs.p')\n        cache_match = salt.fileserver.check_env_cache(__opts__, env_cache)\n        if cache_match is not None:\n            return cache_match\n    ret = set()\n    for repo in init():\n        trunk = os.path.join(repo['repo'], repo['trunk'])\n        if os.path.isdir(trunk):\n            # Add base as the env for trunk\n            ret.add('base')\n        else:\n            log.error(\n                'svnfs trunk path \\'%s\\' does not exist in repo %s, no base '\n                'environment will be provided by this remote',\n                repo['trunk'], repo['url']\n            )\n\n        branches = os.path.join(repo['repo'], repo['branches'])\n        if os.path.isdir(branches):\n            ret.update(os.listdir(branches))\n        else:\n            log.error(\n                'svnfs branches path \\'%s\\' does not exist in repo %s',\n                repo['branches'], repo['url']\n            )\n\n        tags = os.path.join(repo['repo'], repo['tags'])\n        if os.path.isdir(tags):\n            ret.update(os.listdir(tags))\n        else:\n            log.error(\n                'svnfs tags path \\'%s\\' does not exist in repo %s',\n                repo['tags'], repo['url']\n            )\n    return [x for x in sorted(ret) if _env_is_exposed(x)]", "output": "Return a list of refs that can be used as environments", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zone_absent(name, resource_group, connection_auth=None):\n    '''\n    \n    '''\n    ret = {\n        'name': name,\n        'result': False,\n        'comment': '',\n        'changes': {}\n    }\n\n    if not isinstance(connection_auth, dict):\n        ret['comment'] = 'Connection information must be specified via connection_auth dictionary!'\n        return ret\n\n    zone = __salt__['azurearm_dns.zone_get'](\n        name,\n        resource_group,\n        azurearm_log_level='info',\n        **connection_auth\n    )\n\n    if 'error' in zone:\n        ret['result'] = True\n        ret['comment'] = 'DNS zone {0} was not found.'.format(name)\n        return ret\n\n    elif __opts__['test']:\n        ret['comment'] = 'DNS zone {0} would be deleted.'.format(name)\n        ret['result'] = None\n        ret['changes'] = {\n            'old': zone,\n            'new': {},\n        }\n        return ret\n\n    deleted = __salt__['azurearm_dns.zone_delete'](name, resource_group, **connection_auth)\n\n    if deleted:\n        ret['result'] = True\n        ret['comment'] = 'DNS zone {0} has been deleted.'.format(name)\n        ret['changes'] = {\n            'old': zone,\n            'new': {}\n        }\n        return ret\n\n    ret['comment'] = 'Failed to delete DNS zone {0}!'.format(name)\n    return ret", "output": ".. versionadded:: Fluorine\n\n    Ensure a DNS zone does not exist in the resource group.\n\n    :param name:\n        Name of the DNS zone.\n\n    :param resource_group:\n        The resource group assigned to the DNS zone.\n\n    :param connection_auth:\n        A dict with subscription and authentication parameters to be used in connecting to the\n        Azure Resource Manager API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def num_trees(self):\n        \"\"\"\n        \"\"\"\n        num_trees = ctypes.c_int(0)\n        _safe_call(_LIB.LGBM_BoosterNumberOfTotalModel(\n            self.handle,\n            ctypes.byref(num_trees)))\n        return num_trees.value", "output": "Get number of weak sub-models.\n\n        Returns\n        -------\n        num_trees : int\n            The number of weak sub-models.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_timeout_callback(self, callback, timeout_milliseconds, callback_id=None):\n        \"\"\" \"\"\"\n        def wrapper(*args, **kwargs):\n            self.remove_timeout_callback(callback_id)\n            return callback(*args, **kwargs)\n\n        handle = None\n\n        def remover():\n            if handle is not None:\n                self._loop.remove_timeout(handle)\n\n        callback_id = self._assign_remover(callback, callback_id, self._timeout_callback_removers, remover)\n        handle = self._loop.call_later(timeout_milliseconds / 1000.0, wrapper)\n        return callback_id", "output": "Adds a callback to be run once after timeout_milliseconds.\n        Returns an ID that can be used with remove_timeout_callback.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def active_tcp():\n    '''\n    \n    '''\n    ret = {}\n    for statf in ['/proc/net/tcp', '/proc/net/tcp6']:\n        if os.path.isfile(statf):\n            with salt.utils.files.fopen(statf, 'rb') as fp_:\n                for line in fp_:\n                    line = salt.utils.stringutils.to_unicode(line)\n                    if line.strip().startswith('sl'):\n                        continue\n                    iret = _parse_tcp_line(line)\n                    sl = next(iter(iret))\n                    if iret[sl]['state'] == 1:  # 1 is ESTABLISHED\n                        del iret[sl]['state']\n                        ret[len(ret)] = iret[sl]\n    return ret", "output": "Return a dict describing all active tcp connections as quickly as possible", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shadow_hash(crypt_salt=None, password=None, algorithm='sha512'):\n    '''\n    \n    '''\n    return salt.utils.pycrypto.gen_hash(crypt_salt, password, algorithm)", "output": "Generates a salted hash suitable for /etc/shadow.\n\n    crypt_salt : None\n        Salt to be used in the generation of the hash. If one is not\n        provided, a random salt will be generated.\n\n    password : None\n        Value to be salted and hashed. If one is not provided, a random\n        password will be generated.\n\n    algorithm : sha512\n        Hash algorithm to use.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' random.shadow_hash 'My5alT' 'MyP@asswd' md5", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clean_prefix(self):\n        \"\"\"\"\"\"\n        user = self.context.guild.me if self.context.guild else self.context.bot.user\n        # this breaks if the prefix mention is not the bot itself but I\n        # consider this to be an *incredibly* strange use case. I'd rather go\n        # for this common use case rather than waste performance for the\n        # odd one.\n        return self.context.prefix.replace(user.mention, '@' + user.display_name)", "output": "The cleaned up invoke prefix. i.e. mentions are ``@name`` instead of ``<@id>``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_unique_tags(field_to_obs):\n  \"\"\"\n  \"\"\"\n  return {field: sorted(set([x.get('tag', '') for x in observations]))\n          for field, observations in field_to_obs.items()\n          if field in TAG_FIELDS}", "output": "Returns a dictionary of tags that a user could query over.\n\n  Args:\n    field_to_obs: Dict that maps string field to `Observation` list.\n\n  Returns:\n    A dict that maps keys in `TAG_FIELDS` to a list of string tags present in\n    the event files. If the dict does not have any observations of the type,\n    maps to an empty list so that we can render this to console.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lookat(eye, target=[0, 0, 0], up=[0, 1, 0]):\n  \"\"\"\"\"\"\n  eye = np.float32(eye)\n  forward = normalize(target - eye)\n  side = normalize(np.cross(forward, up))\n  up = np.cross(side, forward)\n  M = np.eye(4, dtype=np.float32)\n  R = M[:3, :3]\n  R[:] = [side, up, -forward]\n  M[:3, 3] = -R.dot(eye)\n  return M", "output": "Generate LookAt modelview matrix.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_redis_keys_opts():\n    '''\n    \n    '''\n    return {\n        'bank_prefix': __opts__.get('cache.redis.bank_prefix', _BANK_PREFIX),\n        'bank_keys_prefix': __opts__.get('cache.redis.bank_keys_prefix', _BANK_KEYS_PREFIX),\n        'key_prefix': __opts__.get('cache.redis.key_prefix', _KEY_PREFIX),\n        'separator': __opts__.get('cache.redis.separator', _SEPARATOR)\n    }", "output": "Build the key opts based on the user options.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def train(cls, ratings, rank, iterations=5, lambda_=0.01, blocks=-1, nonnegative=False,\n              seed=None):\n        \"\"\"\n        \n        \"\"\"\n        model = callMLlibFunc(\"trainALSModel\", cls._prepare(ratings), rank, iterations,\n                              lambda_, blocks, nonnegative, seed)\n        return MatrixFactorizationModel(model)", "output": "Train a matrix factorization model given an RDD of ratings by users\n        for a subset of products. The ratings matrix is approximated as the\n        product of two lower-rank matrices of a given rank (number of\n        features). To solve for these features, ALS is run iteratively with\n        a configurable level of parallelism.\n\n        :param ratings:\n          RDD of `Rating` or (userID, productID, rating) tuple.\n        :param rank:\n          Number of features to use (also referred to as the number of latent factors).\n        :param iterations:\n          Number of iterations of ALS.\n          (default: 5)\n        :param lambda_:\n          Regularization parameter.\n          (default: 0.01)\n        :param blocks:\n          Number of blocks used to parallelize the computation. A value\n          of -1 will use an auto-configured number of blocks.\n          (default: -1)\n        :param nonnegative:\n          A value of True will solve least-squares with nonnegativity\n          constraints.\n          (default: False)\n        :param seed:\n          Random seed for initial matrix factorization model. A value\n          of None will use system time as the seed.\n          (default: None)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ParseMessage(descriptor, byte_str):\n  \"\"\"\n  \"\"\"\n  result_class = MakeClass(descriptor)\n  new_msg = result_class()\n  new_msg.ParseFromString(byte_str)\n  return new_msg", "output": "Generate a new Message instance from this Descriptor and a byte string.\n\n  Args:\n    descriptor: Protobuf Descriptor object\n    byte_str: Serialized protocol buffer byte string\n\n  Returns:\n    Newly created protobuf Message object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_filter_pillar(filter_name,\n                      pillar_key='acl',\n                      pillarenv=None,\n                      saltenv=None):\n    '''\n    \n    '''\n    return __salt__['capirca.get_filter_pillar'](filter_name,\n                                                  pillar_key=pillar_key,\n                                                  pillarenv=pillarenv,\n                                                  saltenv=saltenv)", "output": "Helper that can be used inside a state SLS,\n    in order to get the filter configuration given its name.\n\n    filter_name\n        The name of the filter.\n\n    pillar_key\n        The root key of the whole policy config.\n\n    pillarenv\n        Query the master to generate fresh pillar data on the fly,\n        specifically from the requested pillar environment.\n\n    saltenv\n        Included only for compatibility with\n        :conf_minion:`pillarenv_from_saltenv`, and is otherwise ignored.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _bdev(dev=None):\n    '''\n    \n    '''\n    if dev is None:\n        dev = _fssys('cache0')\n    else:\n        dev = _bcpath(dev)\n\n    if not dev:\n        return False\n    else:\n        return _devbase(os.path.dirname(dev))", "output": "Resolve a bcacheX or cache to a real dev\n    :return: basename of bcache dev", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def owner(*paths, **kwargs):\n    '''\n    \n    '''\n    if not paths:\n        return ''\n    ret = {}\n    for path in paths:\n        cmd = ['rpm']\n        if kwargs.get('root'):\n            cmd.extend(['--root', kwargs['root']])\n        cmd.extend(['-qf', '--queryformat', '%{name}', path])\n        ret[path] = __salt__['cmd.run_stdout'](cmd,\n                                               output_loglevel='trace',\n                                               python_shell=False)\n        if 'not owned' in ret[path].lower():\n            ret[path] = ''\n    if len(ret) == 1:\n        return list(ret.values())[0]\n    return ret", "output": "Return the name of the package that owns the file. Multiple file paths can\n    be passed. If a single path is passed, a string will be returned,\n    and if multiple paths are passed, a dictionary of file/package name pairs\n    will be returned.\n\n    If the file is not owned by a package, or is not present on the minion,\n    then an empty string will be returned for that path.\n\n    root\n        use root as top level directory (default: \"/\")\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' lowpkg.owner /usr/bin/apachectl\n        salt '*' lowpkg.owner /usr/bin/apachectl /etc/httpd/conf/httpd.conf", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def url_param(param, default=None):\n    \"\"\"\n    \"\"\"\n    if request.args.get(param):\n        return request.args.get(param, default)\n    # Supporting POST as well as get\n    if request.form.get('form_data'):\n        form_data = json.loads(request.form.get('form_data'))\n        url_params = form_data.get('url_params') or {}\n        return url_params.get(param, default)\n    return default", "output": "Read a url or post parameter and use it in your SQL Lab query\n\n    When in SQL Lab, it's possible to add arbitrary URL \"query string\"\n    parameters, and use those in your SQL code. For instance you can\n    alter your url and add `?foo=bar`, as in\n    `{domain}/superset/sqllab?foo=bar`. Then if your query is something like\n    SELECT * FROM foo = '{{ url_param('foo') }}', it will be parsed at\n    runtime and replaced by the value in the URL.\n\n    As you create a visualization form this SQL Lab query, you can pass\n    parameters in the explore view as well as from the dashboard, and\n    it should carry through to your queries.\n\n    :param param: the parameter to lookup\n    :type param: str\n    :param default: the value to return in the absence of the parameter\n    :type default: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_y(y):\n  \"\"\"\n  \n  \"\"\"\n  if not isinstance(y, np.ndarray):\n    raise TypeError(\"y must be numpy array. Typically y contains \"\n                    \"the entire test set labels. Got \" + str(y) + \" of type \" + str(type(y)))", "output": "Makes sure a `y` argument is a vliad numpy dataset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rm_env(user, name):\n    '''\n    \n    '''\n    lst = list_tab(user)\n    ret = 'absent'\n    rm_ = None\n    for ind in range(len(lst['env'])):\n        if name == lst['env'][ind]['name']:\n            rm_ = ind\n    if rm_ is not None:\n        lst['env'].pop(rm_)\n        ret = 'removed'\n    comdat = _write_cron_lines(user, _render_tab(lst))\n    if comdat['retcode']:\n        # Failed to commit, return the error\n        return comdat['stderr']\n    return ret", "output": "Remove cron environment variable for a specified user.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cron.rm_env root MAILTO", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deprecated(text=\"\", eos=\"\"):\n    \"\"\"\n    \n    \"\"\"\n\n    def get_location():\n        import inspect\n        frame = inspect.currentframe()\n        if frame:\n            callstack = inspect.getouterframes(frame)[-1]\n            return '%s:%i' % (callstack[1], callstack[2])\n        else:\n            stack = inspect.stack(0)\n            entry = stack[2]\n            return '%s:%i' % (entry[1], entry[2])\n\n    def deprecated_inner(func):\n        @functools.wraps(func)\n        def new_func(*args, **kwargs):\n            name = \"{} [{}]\".format(func.__name__, get_location())\n            log_deprecated(name, text, eos)\n            return func(*args, **kwargs)\n        return new_func\n    return deprecated_inner", "output": "Args:\n        text, eos: same as :func:`log_deprecated`.\n\n    Returns:\n        a decorator which deprecates the function.\n\n    Example:\n        .. code-block:: python\n\n            @deprecated(\"Explanation of what to do instead.\", \"2017-11-4\")\n            def foo(...):\n                pass", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lhs(node):\n    '''\n    \n    '''\n\n    gen = ConditionalSymbolVisitor()\n    if isinstance(node, (list, tuple)):\n        gen.visit_list(node)\n    else:\n        gen.visit(node)\n    return gen.lhs", "output": "Return a set of symbols in `node` that are assigned.\n    \n    :param node: ast node \n    \n    :returns: set of strings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def smoothing_cross_entropy_factored_grad(op, dy):\n  \"\"\"\"\"\"\n  a = op.inputs[0]\n  b = op.inputs[1]\n  labels = op.inputs[2]\n  confidence = op.inputs[3]\n  num_splits = 16\n  vocab_size = shape_list(b)[0]\n  labels = approximate_split(labels, num_splits)\n  a = approximate_split(a, num_splits)\n  dy = approximate_split(dy, num_splits)\n  b_grad = None\n  a_grad_parts = []\n  deps = []\n  for part in range(num_splits):\n    with tf.control_dependencies(deps):\n      logits = tf.matmul(a[part], b, transpose_b=True)\n      output_part = smoothing_cross_entropy(logits, labels[part], vocab_size,\n                                            confidence)\n      a_grad_part, b_grad_part = tf.gradients(\n          ys=[output_part], xs=[a[part], b], grad_ys=[dy[part]])\n      a_grad_parts.append(a_grad_part)\n      if part > 0:\n        b_grad += b_grad_part\n      else:\n        b_grad = b_grad_part\n      deps = [b_grad, a_grad_part]\n  a_grad = tf.concat(a_grad_parts, 0)\n  return a_grad, b_grad, None, None", "output": "Gradient function for smoothing_cross_entropy_factored.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def addFile(self, path, recursive=False):\n        \"\"\"\n        \n        \"\"\"\n        self._jsc.sc().addFile(path, recursive)", "output": "Add a file to be downloaded with this Spark job on every node.\n        The C{path} passed can be either a local file, a file in HDFS\n        (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n        FTP URI.\n\n        To access the file in Spark jobs, use\n        L{SparkFiles.get(fileName)<pyspark.files.SparkFiles.get>} with the\n        filename to find its download location.\n\n        A directory can be given if the recursive option is set to True.\n        Currently directories are only supported for Hadoop-supported filesystems.\n\n        .. note:: A path can be added only once. Subsequent additions of the same path are ignored.\n\n        >>> from pyspark import SparkFiles\n        >>> path = os.path.join(tempdir, \"test.txt\")\n        >>> with open(path, \"w\") as testFile:\n        ...    _ = testFile.write(\"100\")\n        >>> sc.addFile(path)\n        >>> def func(iterator):\n        ...    with open(SparkFiles.get(\"test.txt\")) as testFile:\n        ...        fileVal = int(testFile.readline())\n        ...        return [x * fileVal for x in iterator]\n        >>> sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n        [100, 200, 300, 400]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_table(self, table):\n        \"\"\"\n        \"\"\"\n\n        if not self.table_exists(table):\n            return\n\n        self.client.tables().delete(projectId=table.project_id,\n                                    datasetId=table.dataset_id,\n                                    tableId=table.table_id).execute()", "output": "Deletes a table, if it exists.\n\n           :param table:\n           :type table: BQTable", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def num_elements(self):\n        \"\"\"\"\"\"\n        if self.is_fully_defined():\n            size = 1\n            for dim in self._dims:\n                size *= dim.value\n            return size\n        else:\n            return None", "output": "Returns the total number of elements, or none for incomplete shapes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_worker(node_ip_address,\n                 object_store_name,\n                 raylet_name,\n                 redis_address,\n                 worker_path,\n                 temp_dir,\n                 stdout_file=None,\n                 stderr_file=None):\n    \"\"\"\n    \"\"\"\n    command = [\n        sys.executable, \"-u\", worker_path,\n        \"--node-ip-address=\" + node_ip_address,\n        \"--object-store-name=\" + object_store_name,\n        \"--raylet-name=\" + raylet_name,\n        \"--redis-address=\" + str(redis_address), \"--temp-dir=\" + temp_dir\n    ]\n    process_info = start_ray_process(\n        command,\n        ray_constants.PROCESS_TYPE_WORKER,\n        stdout_file=stdout_file,\n        stderr_file=stderr_file)\n    return process_info", "output": "This method starts a worker process.\n\n    Args:\n        node_ip_address (str): The IP address of the node that this worker is\n            running on.\n        object_store_name (str): The socket name of the object store.\n        raylet_name (str): The socket name of the raylet server.\n        redis_address (str): The address that the Redis server is listening on.\n        worker_path (str): The path of the source code which the worker process\n            will run.\n        temp_dir (str): The path of the temp dir.\n        stdout_file: A file handle opened for writing to redirect stdout to. If\n            no redirection should happen, then this should be None.\n        stderr_file: A file handle opened for writing to redirect stderr to. If\n            no redirection should happen, then this should be None.\n\n    Returns:\n        ProcessInfo for the process that was started.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _combine_lines(self, lines):\n        \"\"\"\n        \n        \"\"\"\n        lines = filter(None, map(lambda x: x.strip(), lines))\n        return '[' + ','.join(lines) + ']'", "output": "Combines a list of JSON objects into one JSON object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_get_stock_transaction_realtime(code, ip=None, port=None):\n    ''\n    ip, port = get_mainmarket_ip(ip, port)\n    api = TdxHq_API()\n    try:\n        with api.connect(ip, port):\n            data = pd.DataFrame()\n            data = pd.concat([api.to_df(api.get_transaction_data(\n                _select_market_code(str(code)), code, (2 - i) * 2000, 2000)) for i in range(3)], axis=0)\n            if 'value' in data.columns:\n                data = data.drop(['value'], axis=1)\n            data = data.dropna()\n            day = datetime.date.today()\n            return data.assign(date=str(day)).assign(\n                datetime=pd.to_datetime(data['time'].apply(lambda x: str(day) + ' ' + str(x)))) \\\n                .assign(code=str(code)).assign(order=range(len(data.index))).set_index('datetime', drop=False,\n                                                                                       inplace=False)\n    except:\n        return None", "output": "\u5b9e\u65f6\u5206\u7b14\u6210\u4ea4 \u5305\u542b\u96c6\u5408\u7ade\u4ef7 buyorsell 1--sell 0--buy 2--\u76d8\u524d", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dtype(self):\n        \"\"\"\n        \"\"\"\n        mx_dtype = ctypes.c_int()\n        check_call(_LIB.MXNDArrayGetDType(\n            self.handle, ctypes.byref(mx_dtype)))\n        return _DTYPE_MX_TO_NP[mx_dtype.value]", "output": "Data-type of the array's elements.\n\n        Returns\n        -------\n        numpy.dtype\n            This NDArray's data type.\n\n        Examples\n        --------\n        >>> x = mx.nd.zeros((2,3))\n        >>> x.dtype\n        <type 'numpy.float32'>\n        >>> y = mx.nd.zeros((2,3), dtype='int32')\n        >>> y.dtype\n        <type 'numpy.int32'>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filepattern(self, *args, **kwargs):\n    \"\"\"\"\"\"\n    return [p.filepattern(*args, **kwargs) for p in self.problems]", "output": "Returns a list of filepatterns, one for each problem.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copyDtd(self):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlCopyDtd(self._o)\n        if ret is None:raise treeError('xmlCopyDtd() failed')\n        __tmp = xmlDtd(_obj=ret)\n        return __tmp", "output": "Do a copy of the dtd.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copyHdfsFileToLocal(hdfsFilePath, localFilePath, hdfsClient, override=True):\n    ''''''\n    if not hdfsClient.exists(hdfsFilePath):\n        raise Exception('HDFS file {} does not exist!'.format(hdfsFilePath))\n    try: \n        file_status = hdfsClient.get_file_status(hdfsFilePath)\n        if file_status.type != 'FILE':\n            raise Exception('HDFS file path {} is not a file'.format(hdfsFilePath))\n    except Exception as exception:\n        nni_log(LogType.Error, 'Get hdfs file {0} status error: {1}'.format(hdfsFilePath, str(exception)))\n        raise exception\n\n    if os.path.exists(localFilePath) and override:\n        os.remove(localFilePath)\n    try:\n        hdfsClient.copy_to_local(hdfsFilePath, localFilePath)\n    except Exception as exception:\n        nni_log(LogType.Error, 'Copy hdfs file {0} to {1} error: {2}'.format(hdfsFilePath, localFilePath, str(exception)))\n        raise exception\n    nni_log(LogType.Info, 'Successfully copied hdfs file {0} to {1}, {2} bytes'.format(hdfsFilePath, localFilePath, file_status.length))", "output": "Copy file from HDFS to local", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _initialize(self):\n        \"\"\"\n        \"\"\"\n        if not os.path.exists(self._logdir):\n            raise CollectorError(\"Log directory %s not exists\" % self._logdir)\n\n        self.logger.info(\"Collector started, taking %s as parent directory\"\n                         \"for all job logs.\" % self._logdir)\n\n        # clear old records\n        JobRecord.objects.filter().delete()\n        TrialRecord.objects.filter().delete()\n        ResultRecord.objects.filter().delete()", "output": "Initialize collector worker thread, Log path will be checked first.\n\n        Records in DB backend will be cleared.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sdb(opts, functions=None, whitelist=None, utils=None):\n    '''\n    \n    '''\n    if utils is None:\n        utils = {}\n\n    return LazyLoader(\n        _module_dirs(opts, 'sdb'),\n        opts,\n        tag='sdb',\n        pack={\n            '__sdb__': functions,\n            '__opts__': opts,\n            '__utils__': utils,\n            '__salt__': minion_mods(opts, utils),\n        },\n        whitelist=whitelist,\n    )", "output": "Make a very small database call", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_sugg(self, keyword):\n        \"\"\"\n        \"\"\"\n        url = 'http://w.sugg.sogou.com/sugg/ajaj_json.jsp?key={}&type=wxpub&pr=web'.format(\n            quote(keyword.encode('utf-8')))\n        r = requests.get(url)\n        if not r.ok:\n            raise WechatSogouRequestsException('get_sugg', r)\n\n        sugg = re.findall(u'\\[\"' + keyword + '\",(.*?),\\[\"', r.text)[0]\n        return json.loads(sugg)", "output": "\u83b7\u53d6\u5fae\u4fe1\u641c\u72d7\u641c\u7d22\u5173\u952e\u8bcd\u8054\u60f3\n\n        Parameters\n        ----------\n        keyword : str or unicode\n            \u5173\u952e\u8bcd\n\n        Returns\n        -------\n        list[str]\n            \u8054\u60f3\u5173\u952e\u8bcd\u5217\u8868\n\n        Raises\n        ------\n        WechatSogouRequestsException", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bulleted_list(items, max_count=None, indent=2):\n    \"\"\"\n    \"\"\"\n    if max_count is not None and len(items) > max_count:\n        item_list = list(items)\n        items = item_list[:max_count - 1]\n        items.append('...')\n        items.append(item_list[-1])\n\n    line_template = (\" \" * indent) + \"- {}\"\n    return \"\\n\".join(map(line_template.format, items))", "output": "Format a bulleted list of values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_SU_save_etf_list(client=DATABASE, ui_log=None, ui_progress=None):\n    \"\"\"\n    \"\"\"\n    try:\n        QA_util_log_info(\n            '##JOB16 Now Saving ETF_LIST ====',\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=5000\n        )\n        etf_list_from_tdx = QA_fetch_get_stock_list(type_=\"etf\")\n        pandas_data = QA_util_to_json_from_pandas(etf_list_from_tdx)\n\n        if len(pandas_data) > 0:\n            # \u83b7\u53d6\u5230\u6570\u636e\u540e\u624d\u8fdb\u884cdrop collection \u64cd\u4f5c\n            client.drop_collection('etf_list')\n            coll = client.etf_list\n            coll.create_index('code')\n            coll.insert_many(pandas_data)\n        QA_util_log_info(\n            \"\u5b8c\u6210ETF\u5217\u8868\u83b7\u53d6\",\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=10000\n        )\n    except Exception as e:\n        QA_util_log_info(e, ui_log=ui_log)\n        print(\" Error save_tdx.QA_SU_save_etf_list exception!\")\n        pass", "output": "save etf_list\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sub_all_refs(self, text, handler_method):\n        \"\"\"\n        \n        \"\"\"\n\n        # RegExp to find pattern \"${logicalId.property}\" and return the word inside bracket\n        logical_id_regex = '[A-Za-z0-9\\.]+|AWS::[A-Z][A-Za-z]*'\n        ref_pattern = re.compile(r'\\$\\{(' + logical_id_regex + ')\\}')\n\n        # Find all the pattern, and call the handler to decide how to substitute them.\n        # Do the substitution and return the final text\n        return re.sub(ref_pattern,\n                      # Pass the handler entire string ${logicalId.property} as first parameter and \"logicalId.property\"\n                      # as second parameter. Return value will be substituted\n                      lambda match: handler_method(match.group(0), match.group(1)),\n                      text)", "output": "Substitute references within a string that is using ${key} syntax by calling the `handler_method` on every\n        occurrence of this structure. The value returned by this method directly replaces the reference structure.\n\n        Ex:\n            text = \"${key1}-hello-${key2}\n            def handler_method(full_ref, ref_value):\n                return \"foo\"\n\n            _sub_all_refs(text, handler_method) will output \"foo-hello-foo\"\n\n        :param string text: Input text\n        :param handler_method: Method to be called to handle each occurrence of ${blah} reference structure.\n            First parameter to this method is the full reference structure Ex: ${LogicalId.Property}.\n            Second parameter is just the value of the reference such as \"LogicalId.Property\"\n\n        :return string: Text with all reference structures replaced as necessary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def leaky_relu(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    if 'alpha' in attrs:\n        new_attrs = translation_utils._fix_attribute_names(attrs, {'alpha' : 'slope'})\n    else:\n        new_attrs = translation_utils._add_extra_attributes(attrs, {'slope': 0.01})\n    return 'LeakyReLU', new_attrs, inputs", "output": "Leaky Relu function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _maybe_fill(arr, fill_value=np.nan):\n    \"\"\"\n    \n    \"\"\"\n    if _isna_compat(arr, fill_value):\n        arr.fill(fill_value)\n    return arr", "output": "if we have a compatible fill_value and arr dtype, then fill", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_nodegroup_minions(self, expr, greedy):  # pylint: disable=unused-argument\n        '''\n        \n        '''\n        return self._check_compound_minions(nodegroup_comp(expr, self.opts['nodegroups']),\n            DEFAULT_TARGET_DELIM,\n            greedy)", "output": "Return minions found by looking at nodegroups", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mousePressEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        if event.button() == Qt.LeftButton:\r\n            self.__drag_start_pos = QPoint(event.pos())\r\n        QTabBar.mousePressEvent(self, event)", "output": "Reimplement Qt method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def grains():\n    '''\n    \n    '''\n    if not DETAILS['grains_cache']:\n        ret = system_info()\n        log.debug(ret)\n        DETAILS['grains_cache'].update(ret)\n    return {'onyx': DETAILS['grains_cache']}", "output": "Get grains for proxy minion\n\n    .. code-block: bash\n\n        salt '*' onyx.cmd grains", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _keys(self, pattern):\n        \"\"\"\n        \"\"\"\n        result = []\n        for client in self.redis_clients:\n            result.extend(list(client.scan_iter(match=pattern)))\n        return result", "output": "Execute the KEYS command on all Redis shards.\n\n        Args:\n            pattern: The KEYS pattern to query.\n\n        Returns:\n            The concatenated list of results from all shards.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def file_hash(load, fnd):\n    '''\n    \n    '''\n    if 'env' in load:\n        # \"env\" is not supported; Use \"saltenv\".\n        load.pop('env')\n\n    if not all(x in load for x in ('path', 'saltenv')):\n        return ''\n    saltenv = load['saltenv']\n    if saltenv == 'base':\n        saltenv = 'trunk'\n    ret = {}\n    relpath = fnd['rel']\n    path = fnd['path']\n\n    # If the file doesn't exist, we can't get a hash\n    if not path or not os.path.isfile(path):\n        return ret\n\n    # Set the hash_type as it is determined by config\n    ret['hash_type'] = __opts__['hash_type']\n\n    # Check if the hash is cached\n    # Cache file's contents should be \"hash:mtime\"\n    cache_path = os.path.join(__opts__['cachedir'],\n                              'svnfs',\n                              'hash',\n                              saltenv,\n                              '{0}.hash.{1}'.format(relpath,\n                                                    __opts__['hash_type']))\n    # If we have a cache, serve that if the mtime hasn't changed\n    if os.path.exists(cache_path):\n        with salt.utils.files.fopen(cache_path, 'rb') as fp_:\n            hsum, mtime = fp_.read().split(':')\n            if os.path.getmtime(path) == mtime:\n                # check if mtime changed\n                ret['hsum'] = hsum\n                return ret\n\n    # if we don't have a cache entry-- lets make one\n    ret['hsum'] = salt.utils.hashutils.get_hash(path, __opts__['hash_type'])\n    cache_dir = os.path.dirname(cache_path)\n    # make cache directory if it doesn't exist\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n    # save the cache object \"hash:mtime\"\n    with salt.utils.files.fopen(cache_path, 'w') as fp_:\n        fp_.write('{0}:{1}'.format(ret['hsum'], os.path.getmtime(path)))\n\n    return ret", "output": "Return a file hash, the hash type is set in the master config file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode_example(self, example_data):\n    \"\"\"\"\"\"\n    np_dtype = np.dtype(self._dtype.as_numpy_dtype)\n    # Convert to numpy if possible\n    if not isinstance(example_data, np.ndarray):\n      example_data = np.array(example_data, dtype=np_dtype)\n    # Ensure the shape and dtype match\n    if example_data.dtype != np_dtype:\n      raise ValueError('Dtype {} do not match {}'.format(\n          example_data.dtype, np_dtype))\n    utils.assert_shape_match(example_data.shape, self._shape)\n    # For booleans, convert to integer (tf.train.Example does not support bool)\n    if example_data.dtype == np.bool_:\n      example_data = example_data.astype(int)\n    return example_data", "output": "See base class for details.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def insert(queue, items, backend='sqlite'):\n    '''\n    \n    '''\n    queue_funcs = salt.loader.queues(__opts__)\n    cmd = '{0}.insert'.format(backend)\n    if cmd not in queue_funcs:\n        raise SaltInvocationError('Function \"{0}\" is not available'.format(cmd))\n    ret = queue_funcs[cmd](items=items, queue=queue)\n    return ret", "output": "Add an item or items to a queue\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run queue.insert myqueue myitem\n        salt-run queue.insert myqueue \"['item1', 'item2', 'item3']\"\n        salt-run queue.insert myqueue myitem backend=sqlite\n        salt-run queue.insert myqueue \"['item1', 'item2', 'item3']\" backend=sqlite", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_SU_save_option_commodity_day(\n        client=DATABASE,\n        ui_log=None,\n        ui_progress=None\n):\n    '''\n        \n    '''\n    _save_option_commodity_cu_day(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n    _save_option_commodity_m_day(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n    _save_option_commodity_sr_day(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n\n    _save_option_commodity_ru_day(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n    _save_option_commodity_cf_day(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n    _save_option_commodity_c_day(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )", "output": ":param client:\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_child(self, parent, child):  # type: (str, str) -> bool\n        \"\"\"\n        \n        \"\"\"\n        parent_parts = tuple(self._split_table_name(parent))\n        child_parts = tuple(self._split_table_name(child))\n\n        if parent_parts == child_parts:\n            return False\n\n        return parent_parts == child_parts[: len(parent_parts)]", "output": "Returns whether a key is strictly a child of another key.\n        AoT siblings are not considered children of one another.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_confidence(self):\n        \"\"\"\"\"\"\n        # if we didn't receive any character in our consideration range,\n        # return negative answer\n        if self._total_chars <= 0 or self._freq_chars <= self.MINIMUM_DATA_THRESHOLD:\n            return self.SURE_NO\n\n        if self._total_chars != self._freq_chars:\n            r = (self._freq_chars / ((self._total_chars - self._freq_chars)\n                 * self.typical_distribution_ratio))\n            if r < self.SURE_YES:\n                return r\n\n        # normalize confidence (we don't want to be 100% sure)\n        return self.SURE_YES", "output": "return confidence based on existing data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_svc(svc_path):\n    '''\n    \n    '''\n    run_file = os.path.join(svc_path, 'run')\n    if (os.path.exists(svc_path)\n         and os.path.exists(run_file)\n         and os.access(run_file, os.X_OK)):\n        return True\n    return False", "output": "Return ``True`` if directory <svc_path> is really a service:\n    file <svc_path>/run exists and is executable\n\n    svc_path\n        the (absolute) directory to check for compatibility", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def top(name):\n    '''\n    \n    '''\n    response = _client_wrapper('top', name)\n\n    # Read in column names\n    columns = {}\n    for idx, col_name in enumerate(response['Titles']):\n        columns[idx] = col_name\n\n    # Build return dict\n    ret = []\n    for process in response['Processes']:\n        cur_proc = {}\n        for idx, val in enumerate(process):\n            cur_proc[columns[idx]] = val\n        ret.append(cur_proc)\n    return ret", "output": "Runs the `docker top` command on a specific container\n\n    name\n        Container name or ID\n\n    CLI Example:\n\n\n    **RETURN DATA**\n\n    A list of dictionaries containing information about each process\n\n\n    .. code-block:: bash\n\n        salt myminion docker.top mycontainer\n        salt myminion docker.top 0123456789ab", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_service(self, wait_timeout=10, sleep_wait=1):\n        '''\n        \n        '''\n\n        # Stops/remove the PAExec service and removes the executable\n        log.debug(\"Deleting PAExec service at the end of the process\")\n        wait_start = time.time()\n        while True:\n            try:\n                self._client._service.delete()\n            except SCMRException as exc:\n                log.debug(\"Exception encountered while deleting service %s\", repr(exc))\n                if time.time() - wait_start > wait_timeout:\n                    raise exc\n                time.sleep(sleep_wait)\n                continue\n            break\n\n        # delete the PAExec executable\n        smb_tree = TreeConnect(\n            self._client.session,\n            r\"\\\\{0}\\ADMIN$\".format(self._client.connection.server_name)\n        )\n        log.info(\"Connecting to SMB Tree %s\", smb_tree.share_name)\n        smb_tree.connect()\n\n        wait_start = time.time()\n        while True:\n            try:\n                log.info(\"Creating open to PAExec file with delete on close flags\")\n                self._client._delete_file(smb_tree, self._exe_file)\n            except SMBResponseException as exc:\n                log.debug(\"Exception deleting file %s %s\", self._exe_file, repr(exc))\n                if time.time() - wait_start > wait_timeout:\n                    raise exc\n                time.sleep(sleep_wait)\n                continue\n            break\n        log.info(\"Disconnecting from SMB Tree %s\", smb_tree.share_name)\n        smb_tree.disconnect()", "output": "Removes the PAExec service and executable that was created as part of\n        the create_service function. This does not remove any older executables\n        or services from previous runs, use cleanup() instead for that purpose.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ping(self):\n        \"\"\"\n        \"\"\"\n        while True:\n            try:\n                ping_after, session = self._sessions.get(block=False)\n            except queue.Empty:  # all sessions in use\n                break\n            if ping_after > _NOW():  # oldest session is fresh\n                # Re-add to queue with existing expiration\n                self._sessions.put((ping_after, session))\n                break\n            if not session.exists():  # stale\n                session = self._new_session()\n                session.create()\n            # Re-add to queue with new expiration\n            self.put(session)", "output": "Refresh maybe-expired sessions in the pool.\n\n        This method is designed to be called from a background thread,\n        or during the \"idle\" phase of an event loop.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def count_tokens(tokens, to_lower=False, counter=None):\n    \n\n    \"\"\"\n    if to_lower:\n        tokens = [t.lower() for t in tokens]\n\n    if counter is None:\n        return Counter(tokens)\n    else:\n        counter.update(tokens)\n        return counter", "output": "r\"\"\"Counts tokens in the specified string.\n\n    For token_delim='(td)' and seq_delim='(sd)', a specified string of two sequences of tokens may\n    look like::\n\n        (td)token1(td)token2(td)token3(td)(sd)(td)token4(td)token5(td)(sd)\n\n\n    Parameters\n    ----------\n    tokens : list of str\n        A source list of tokens.\n    to_lower : bool, default False\n        Whether to convert the source source_str to the lower case.\n    counter : Counter or None, default None\n        The Counter instance to be updated with the counts of `tokens`. If\n        None, return a new Counter instance counting tokens from `tokens`.\n\n    Returns\n    -------\n    The `counter` Counter instance after being updated with the token\n    counts of `source_str`. If `counter` is None, return a new Counter\n    instance counting tokens from `source_str`.\n\n    Examples\n    --------\n    >>> import re\n    >>> source_str = ' Life is great ! \\n life is good . \\n'\n    >>> source_str_tokens = filter(None, re.split(' |\\n', source_str))\n    >>> gluonnlp.data.count_tokens(source_str_tokens)\n    Counter({'is': 2, 'Life': 1, 'great': 1, '!': 1, 'life': 1, 'good': 1, '.': 1})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def network_hosts(value, options=None, version=None):\n    '''\n    \n    '''\n    ipaddr_filter_out = _filter_ipaddr(value, options=options, version=version)\n    if not ipaddr_filter_out:\n        return\n    if not isinstance(value, (list, tuple, types.GeneratorType)):\n        return _network_hosts(ipaddr_filter_out[0])\n    return [\n        _network_hosts(ip_a)\n        for ip_a in ipaddr_filter_out\n    ]", "output": "Return the list of hosts within a network.\n\n    .. note::\n\n        When running this command with a large IPv6 network, the command will\n        take a long time to gather all of the hosts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_only_column_of_type(sframe, target_type, type_name, col_name):\n    \"\"\"\n    \n    \"\"\"\n    image_column_name = None\n    if type(target_type) != list:\n        target_type = [target_type]\n    for name, ctype in zip(sframe.column_names(), sframe.column_types()):\n        if ctype in target_type:\n            if image_column_name is not None:\n                raise ToolkitError('No \"{col_name}\" column specified and more than one {type_name} column in \"dataset\". Can not infer correct {col_name} column.'.format(col_name=col_name, type_name=type_name))\n            image_column_name = name\n    if image_column_name is None:\n        raise ToolkitError('No %s column in \"dataset\".' % type_name)\n    return image_column_name", "output": "Finds the only column in `SFrame` with a type specified by `target_type`.\n    If there are zero or more than one such columns, an exception will be\n    raised. The name and type of the target column should be provided as\n    strings for the purpose of error feedback.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attributes(self):\n        \"\"\"\n        \"\"\"\n        length = c_bst_ulong()\n        sarr = ctypes.POINTER(ctypes.c_char_p)()\n        _check_call(_LIB.XGBoosterGetAttrNames(self.handle,\n                                               ctypes.byref(length),\n                                               ctypes.byref(sarr)))\n        attr_names = from_cstr_to_pystr(sarr, length)\n        return {n: self.attr(n) for n in attr_names}", "output": "Get attributes stored in the Booster as a dictionary.\n\n        Returns\n        -------\n        result : dictionary of  attribute_name: attribute_value pairs of strings.\n            Returns an empty dict if there's no attributes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def removeID(self, attr):\n        \"\"\" \"\"\"\n        if attr is None: attr__o = None\n        else: attr__o = attr._o\n        ret = libxml2mod.xmlRemoveID(self._o, attr__o)\n        return ret", "output": "Remove the given attribute from the ID table maintained\n           internally.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_object(name):\n    \"\"\"\"\"\"\n\n    if \".\" not in name:\n        raise Exception('load object need module.object')\n\n    module_name, object_name = name.rsplit('.', 1)\n    if six.PY2:\n        module = __import__(module_name, globals(), locals(), [utf8(object_name)], -1)\n    else:\n        module = __import__(module_name, globals(), locals(), [object_name])\n    return getattr(module, object_name)", "output": "Load object from module", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_preferences(self, node):\n        '''\n        \n        '''\n        pref = etree.SubElement(node, 'preferences')\n        pacman = etree.SubElement(pref, 'packagemanager')\n        pacman.text = self._get_package_manager()\n        p_version = etree.SubElement(pref, 'version')\n        p_version.text = '0.0.1'\n        p_type = etree.SubElement(pref, 'type')\n        p_type.set('image', 'vmx')\n\n        for disk_id, disk_data in self._data.system.get('disks', {}).items():\n            if disk_id.startswith('/dev'):\n                p_type.set('filesystem', disk_data.get('type') or 'ext3')\n                break\n\n        p_type.set('installiso', 'true')\n        p_type.set('boot', \"vmxboot/suse-leap42.1\")\n        p_type.set('format', self.format)\n        p_type.set('bootloader', 'grub2')\n        p_type.set('timezone', __salt__['timezone.get_zone']())\n        p_type.set('hwclock', __salt__['timezone.get_hwclock']())\n\n        return pref", "output": "Set preferences.\n\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def last_metric_eval(multiplexer, session_name, metric_name):\n  \"\"\"\n  \"\"\"\n  try:\n    run, tag = run_tag_from_session_and_metric(session_name, metric_name)\n    tensor_events = multiplexer.Tensors(run=run, tag=tag)\n  except KeyError as e:\n    raise KeyError(\n        'Can\\'t find metric %s for session: %s. Underlying error message: %s'\n        % (metric_name, session_name, e))\n  last_event = tensor_events[-1]\n  # TODO(erez): Raise HParamsError if the tensor is not a 0-D real scalar.\n  return (last_event.wall_time,\n          last_event.step,\n          tf.make_ndarray(last_event.tensor_proto).item())", "output": "Returns the last evaluations of the given metric at the given session.\n\n  Args:\n    multiplexer: The EventMultiplexer instance allowing access to\n        the exported summary data.\n    session_name: String. The session name for which to get the metric\n        evaluations.\n    metric_name: api_pb2.MetricName proto. The name of the metric to use.\n\n  Returns:\n    A 3-tuples, of the form [wall-time, step, value], denoting\n    the last evaluation of the metric, where wall-time denotes the wall time\n    in seconds since UNIX epoch of the time of the evaluation, step denotes\n    the training step at which the model is evaluated, and value denotes the\n    (scalar real) value of the metric.\n\n  Raises:\n    KeyError if the given session does not have the metric.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_indicator_WR(DataFrame, N, N1):\n    ''\n    HIGH = DataFrame['high']\n    LOW = DataFrame['low']\n    CLOSE = DataFrame['close']\n    WR1 = 100 * (HHV(HIGH, N) - CLOSE) / (HHV(HIGH, N) - LLV(LOW, N))\n    WR2 = 100 * (HHV(HIGH, N1) - CLOSE) / (HHV(HIGH, N1) - LLV(LOW, N1))\n    DICT = {'WR1': WR1, 'WR2': WR2}\n\n    return pd.DataFrame(DICT)", "output": "\u5a01\u5ec9\u6307\u6807", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_property_dict_item(obj, prop, key, value):\n    ''' \n    '''\n    attr = getattr(obj, prop)\n    if prop == 'devices':\n        device_type = value['type']\n\n        if device_type == 'disk':\n\n            if 'path' not in value:\n                raise SaltInvocationError(\n                    \"path must be given as parameter\"\n                )\n\n            if value['path'] != '/' and 'source' not in value:\n                raise SaltInvocationError(\n                    \"source must be given as parameter\"\n                )\n\n        for k in value.keys():\n            if k.startswith('__'):\n                del value[k]\n\n        attr[key] = value\n\n    else:  # config\n        attr[key] = six.text_type(value)\n\n    pylxd_save_object(obj)\n\n    return _pylxd_model_to_dict(obj)", "output": "Sets the dict item key of the attr from obj.\n\n        Basicaly it does getattr(obj, prop)[key] = value.\n\n\n        For the disk device we added some checks to make\n        device changes on the CLI saver.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def valid(self, inplace=False, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        warnings.warn(\"Method .valid will be removed in a future version. \"\n                      \"Use .dropna instead.\", FutureWarning, stacklevel=2)\n        return self.dropna(inplace=inplace, **kwargs)", "output": "Return Series without null values.\n\n        .. deprecated:: 0.23.0\n            Use :meth:`Series.dropna` instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _save_new_defaults(self, defaults, new_version, subfolder):\r\n        \"\"\"\"\"\"\r\n        new_defaults = DefaultsConfig(name='defaults-'+new_version,\r\n                                      subfolder=subfolder)\r\n        if not osp.isfile(new_defaults.filename()):\r\n            new_defaults.set_defaults(defaults)\r\n            new_defaults._save()", "output": "Save new defaults", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_version(self):\n        \"\"\"  \"\"\"\n        version = _ensure_decoded(\n            getattr(self.group._v_attrs, 'pandas_version', None))\n        try:\n            self.version = tuple(int(x) for x in version.split('.'))\n            if len(self.version) == 2:\n                self.version = self.version + (0,)\n        except AttributeError:\n            self.version = (0, 0, 0)", "output": "compute and set our version", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def write(self, writer: Any,\n                    close_boundary: bool=True) -> None:\n        \"\"\"\"\"\"\n        if not self._parts:\n            return\n\n        for part, encoding, te_encoding in self._parts:\n            await writer.write(b'--' + self._boundary + b'\\r\\n')\n            await writer.write(part._binary_headers)\n\n            if encoding or te_encoding:\n                w = MultipartPayloadWriter(writer)\n                if encoding:\n                    w.enable_compression(encoding)\n                if te_encoding:\n                    w.enable_encoding(te_encoding)\n                await part.write(w)  # type: ignore\n                await w.write_eof()\n            else:\n                await part.write(writer)\n\n            await writer.write(b'\\r\\n')\n\n        if close_boundary:\n            await writer.write(b'--' + self._boundary + b'--\\r\\n')", "output": "Write body.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_reward(self, new_reward):\n        \"\"\"\n        \n        \"\"\"\n        self.sess.run(self.model.update_reward,\n                      feed_dict={self.model.new_reward: new_reward})", "output": "Updates reward value for policy.\n        :param new_reward: New reward to save.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_create_batch_env_fun(batch_env_fn, time_limit):\n  \"\"\"\n  \"\"\"\n\n  def create_env_fun(game_name=None, sticky_actions=None):\n    del game_name, sticky_actions\n    batch_env = batch_env_fn(in_graph=False)\n    batch_env = ResizeBatchObservation(batch_env)  # pylint: disable=redefined-variable-type\n    batch_env = DopamineBatchEnv(batch_env, max_episode_steps=time_limit)\n    return batch_env\n\n  return create_env_fun", "output": "Factory for dopamine environment initialization function.\n\n  Args:\n    batch_env_fn: function(in_graph: bool) -> batch environment.\n    time_limit: time steps limit for environment.\n\n  Returns:\n    function (with optional, unused parameters) initializing environment.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_quoted_key(self):  # type: () -> Key\n        \"\"\"\n        \n        \"\"\"\n        quote_style = self._current\n        key_type = None\n        dotted = False\n        for t in KeyType:\n            if t.value == quote_style:\n                key_type = t\n                break\n\n        if key_type is None:\n            raise RuntimeError(\"Should not have entered _parse_quoted_key()\")\n\n        self.inc()\n        self.mark()\n\n        while self._current != quote_style and self.inc():\n            pass\n\n        key = self.extract()\n\n        if self._current == \".\":\n            self.inc()\n            dotted = True\n            key += \".\" + self._parse_key().as_string()\n            key_type = KeyType.Bare\n        else:\n            self.inc()\n\n        return Key(key, key_type, \"\", dotted)", "output": "Parses a key enclosed in either single or double quotes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_epoch_number(batch: Batch, epoch: int) -> Batch:\n    \"\"\"\n    \n    \"\"\"\n    for instance in batch.instances:\n        instance.fields['epoch_num'] = MetadataField(epoch)\n    return batch", "output": "Add the epoch number to the batch instances as a MetadataField.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_label_filter(category, *args, **kwargs):\n    \"\"\"\"\"\"\n    terms = list(args)\n    for key, value in six.iteritems(kwargs):\n        if value is None:\n            continue\n\n        suffix = None\n        if key.endswith(\n            (\"_prefix\", \"_suffix\", \"_greater\", \"_greaterequal\", \"_less\", \"_lessequal\")\n        ):\n            key, suffix = key.rsplit(\"_\", 1)\n\n        if category == \"resource\" and key == \"resource_type\":\n            key = \"resource.type\"\n        else:\n            key = \".\".join((category, \"label\", key))\n\n        if suffix == \"prefix\":\n            term = '{key} = starts_with(\"{value}\")'\n        elif suffix == \"suffix\":\n            term = '{key} = ends_with(\"{value}\")'\n        elif suffix == \"greater\":\n            term = \"{key} > {value}\"\n        elif suffix == \"greaterequal\":\n            term = \"{key} >= {value}\"\n        elif suffix == \"less\":\n            term = \"{key} < {value}\"\n        elif suffix == \"lessequal\":\n            term = \"{key} <= {value}\"\n        else:\n            term = '{key} = \"{value}\"'\n\n        terms.append(term.format(key=key, value=value))\n\n    return \" AND \".join(sorted(terms))", "output": "Construct a filter string to filter on metric or resource labels.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read_from_definition_body(self):\n        \"\"\"\n        \n        \"\"\"\n\n        # Let's try to parse it as AWS::Include Transform first. If not, then fall back to assuming the Swagger document\n        # was inclined directly into the body\n        location = parse_aws_include_transform(self.definition_body)\n        if location:\n            LOG.debug(\"Trying to download Swagger from %s\", location)\n            return self._download_swagger(location)\n\n        # Inline Swagger, just return the contents which should already be a dictionary\n        LOG.debug(\"Detected Inline Swagger definition\")\n        return self.definition_body", "output": "Read the Swagger document from DefinitionBody. It could either be an inline Swagger dictionary or an\n        AWS::Include macro that contains location of the included Swagger. In the later case, we will download and\n        parse the Swagger document.\n\n        Returns\n        -------\n        dict\n            Swagger document, if we were able to parse. None, otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def discretize(self, data):\n        \"\"\"\n        \"\"\"\n        ret = data.copy()\n        for feature in self.lambdas:\n            if len(data.shape) == 1:\n                ret[feature] = int(self.lambdas[feature](ret[feature]))\n            else:\n                ret[:, feature] = self.lambdas[feature](\n                    ret[:, feature]).astype(int)\n        return ret", "output": "Discretizes the data.\n        Args:\n            data: numpy 2d or 1d array\n        Returns:\n            numpy array of same dimension, discretized.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_acl(auth_list, opts=None):\n    '''\n    \n    '''\n    ou_names = []\n    for item in auth_list:\n        if isinstance(item, six.string_types):\n            continue\n        ou_names.extend([potential_ou for potential_ou in item.keys() if potential_ou.startswith('ldap(')])\n    if ou_names:\n        auth_list = __expand_ldap_entries(auth_list, opts)\n    return auth_list", "output": "Query LDAP, retrieve list of minion_ids from an OU or other search.\n    For each minion_id returned from the LDAP search, copy the perms\n    matchers into the auth dictionary\n    :param auth_list:\n    :param opts: __opts__ for when __opts__ is not injected\n    :return: Modified auth list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dmidecoder(args=None):\n    '''\n    \n    '''\n    dmidecoder = salt.utils.path.which_bin(['dmidecode', 'smbios'])\n\n    if not args:\n        out = salt.modules.cmdmod._run_quiet(dmidecoder)\n    else:\n        out = salt.modules.cmdmod._run_quiet('{0} {1}'.format(dmidecoder, args))\n\n    return out", "output": "Call DMIdecode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def match(tgt, opts=None):\n    '''\n    \n    '''\n\n    if not opts:\n        opts = __opts__\n    try:\n        if ',' + opts['id'] + ',' in tgt \\\n                or tgt.startswith(opts['id'] + ',') \\\n                or tgt.endswith(',' + opts['id']):\n            return True\n        # tgt is a string, which we know because the if statement above did not\n        # cause one of the exceptions being caught. Therefore, look for an\n        # exact match. (e.g. salt -L foo test.ping)\n        return opts['id'] == tgt\n    except (AttributeError, TypeError):\n        # tgt is not a string, maybe it's a sequence type?\n        try:\n            return opts['id'] in tgt\n        except Exception:\n            # tgt was likely some invalid type\n            return False\n\n    # We should never get here based on the return statements in the logic\n    # above. If we do, it is because something above changed, and should be\n    # considered as a bug. Log a warning to help us catch this.\n    log.warning(\n        'List matcher unexpectedly did not return, for target %s, '\n        'this is probably a bug.', tgt\n    )\n    return False", "output": "Determines if this host is on the list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ext_pillar(minion_id,\n               pillar,  # pylint: disable=W0613\n               key=None,\n               only=()):\n    '''\n    \n    '''\n    url = __opts__['foreman.url']\n    user = __opts__['foreman.user']\n    password = __opts__['foreman.password']\n    api = __opts__['foreman.api']\n    verify = __opts__['foreman.verifyssl']\n    certfile = __opts__['foreman.certfile']\n    keyfile = __opts__['foreman.keyfile']\n    cafile = __opts__['foreman.cafile']\n    lookup_parameters = __opts__['foreman.lookup_parameters']\n\n    log.info(\"Querying Foreman at %r for information for %r\", url, minion_id)\n    try:\n        # Foreman API version 1 is currently not supported\n        if api != 2:\n            log.error('Foreman API v2 is supported only, please specify'\n                    'version 2 in your Salt master config')\n            raise Exception\n\n        headers = {'accept': 'version=' + six.text_type(api) + ',application/json'}\n\n        if verify and cafile is not None:\n            verify = cafile\n\n        resp = requests.get(\n                url + '/hosts/' + minion_id,\n                auth=(user, password),\n                headers=headers,\n                verify=verify,\n                cert=(certfile, keyfile)\n                )\n        result = resp.json()\n\n        log.debug('Raw response of the Foreman request is %r', result)\n\n        if lookup_parameters:\n            parameters = dict()\n            for param in result['all_parameters']:\n                parameters.update({param['name']: param['value']})\n\n            result['parameters'] = parameters\n\n        if only:\n            result = dict((k, result[k]) for k in only if k in result)\n\n    except Exception:\n        log.exception(\n            'Could not fetch host data via Foreman API:'\n        )\n        return {}\n\n    if key:\n        result = {key: result}\n\n    return result", "output": "Read pillar data from Foreman via its API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def c_str(string):\n    \"\"\"\"\"\"\"\n    if not isinstance(string, str):\n        string = string.decode('ascii')\n    return ctypes.c_char_p(string.encode('utf-8'))", "output": "Convert a python string to C string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hash_args(*args, **kwargs):\n    \"\"\"\"\"\"\n    arg_string = '_'.join([str(arg) for arg in args])\n    kwarg_string = '_'.join([str(key) + '=' + str(value)\n                             for key, value in iteritems(kwargs)])\n    combined = ':'.join([arg_string, kwarg_string])\n\n    hasher = md5()\n    hasher.update(b(combined))\n    return hasher.hexdigest()", "output": "Define a unique string for any set of representable args.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def match_box_with_gt(self, boxes, iou_threshold):\n        \"\"\"\n        \n        \"\"\"\n        if self.is_training:\n            with tf.name_scope('match_box_with_gt_{}'.format(iou_threshold)):\n                iou = pairwise_iou(boxes, self.gt_boxes)  # NxM\n                max_iou_per_box = tf.reduce_max(iou, axis=1)  # N\n                best_iou_ind = tf.argmax(iou, axis=1)  # N\n                labels_per_box = tf.gather(self.gt_labels, best_iou_ind)\n                fg_mask = max_iou_per_box >= iou_threshold\n                fg_inds_wrt_gt = tf.boolean_mask(best_iou_ind, fg_mask)\n                labels_per_box = tf.stop_gradient(labels_per_box * tf.cast(fg_mask, tf.int64))\n                return BoxProposals(boxes, labels_per_box, fg_inds_wrt_gt)\n        else:\n            return BoxProposals(boxes)", "output": "Args:\n            boxes: Nx4\n        Returns:\n            BoxProposals", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_eager_metrics_internal(metric_fns,\n                                  weights_fn=common_layers.weights_all):\n  \"\"\"\n  \"\"\"\n  tfe_metrics = {}\n\n  for name in metric_fns:\n    tfe_metrics[name] = tfe.metrics.Mean(name=name)\n\n  def metric_accum(predictions, targets):\n    for name, metric_fn in metric_fns.items():\n      val, weight = metric_fn(predictions, targets,\n                              weights_fn=weights_fn)\n      tfe_metrics[name](np.squeeze(val), np.squeeze(weight))\n\n  def metric_means():\n    avgs = {}\n    for name in metric_fns:\n      avgs[name] = tfe_metrics[name].result().numpy()\n    return avgs\n\n  return metric_accum, metric_means", "output": "Create metrics accumulators and averager for Eager mode.\n\n  Args:\n    metric_fns: dict<metric name, metric function>\n    weights_fn: function that takes labels and returns a weights mask. Defaults\n      to weights of all 1, i.e. common_layers.weights_all. Use\n      common_layers.weights_nonzero if labels have 0-padding.\n\n  Returns:\n    (accum_fn(predictions, targets) => None,\n     result_fn() => dict<str metric_name, float avg_val>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _save_lang(self):\n        \"\"\"\n        \n        \"\"\"\n        for combobox, (option, _default) in list(self.comboboxes.items()):\n            if option == 'interface_language':\n                data = combobox.itemData(combobox.currentIndex())\n                value = from_qvariant(data, to_text_string)\n                break\n        try:\n            save_lang_conf(value)\n            self.set_option('interface_language', value)\n        except Exception:\n            QMessageBox.critical(\n                self, _(\"Error\"),\n                _(\"We're sorry but the following error occurred while trying \"\n                  \"to set your selected language:<br><br>\"\n                  \"<tt>{}</tt>\").format(traceback.format_exc()),\n                QMessageBox.Ok)\n            return", "output": "Get selected language setting and save to language configuration file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def detach_volume(volume_id, instance_id=None, device=None, force=False,\n                  wait_for_detachement=False, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        ret = conn.detach_volume(volume_id, instance_id, device, force)\n        if ret and wait_for_detachement and not _wait_for_volume_available(conn, volume_id):\n            timeout_msg = 'Timed out waiting for the volume status \"available\".'\n            log.error(timeout_msg)\n            return False\n        return ret\n    except boto.exception.BotoServerError as e:\n        log.error(e)\n        return False", "output": "Detach an EBS volume from an EC2 instance.\n\n    .. versionadded:: 2016.11.0\n\n    volume_id\n        (string) \u2013 The ID of the EBS volume to be detached.\n    instance_id\n        (string) \u2013 The ID of the EC2 instance from which it will be detached.\n    device\n        (string) \u2013 The device on the instance through which the volume is exposted (e.g. /dev/sdh)\n    force\n        (bool) \u2013 Forces detachment if the previous detachment attempt did not occur cleanly.\n                 This option can lead to data loss or a corrupted file system. Use this option\n                 only as a last resort to detach a volume from a failed instance. The instance\n                 will not have an opportunity to flush file system caches nor file system meta data.\n                 If you use this option, you must perform file system check and repair procedures.\n    wait_for_detachement\n       (bool) - Whether or not to wait for volume detachement to complete.\n\n    returns\n        (bool) - True on success, False on failure.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call boto_ec2.detach_volume vol-12345678 i-87654321", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def symlink_to(orig, dest):\n    \"\"\"\n    \"\"\"\n    if is_windows:\n        import subprocess\n\n        subprocess.check_call(\n            [\"mklink\", \"/d\", path2str(orig), path2str(dest)], shell=True\n        )\n    else:\n        orig.symlink_to(dest)", "output": "Create a symlink. Used for model shortcut links.\n\n    orig (unicode / Path): The origin path.\n    dest (unicode / Path): The destination path of the symlink.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hash_file(path, hashobj, conn=None):\n    '''\n    \n    '''\n    if os.path.isdir(path):\n        return ''\n\n    with salt.utils.files.fopen(path, 'r') as f:\n        hashobj.update(salt.utils.stringutils.to_bytes(f.read()))\n        return hashobj.hexdigest()", "output": "Get the hexdigest hash value of a file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __track_job(self):\n        \"\"\"\"\"\"\n        while not self.__verify_job_has_started():\n            time.sleep(self.__POLL_TIME)\n            self.__logger.debug(\"Waiting for Kubernetes job \" + self.uu_name + \" to start\")\n        self.__print_kubectl_hints()\n\n        status = self.__get_job_status()\n        while status == \"RUNNING\":\n            self.__logger.debug(\"Kubernetes job \" + self.uu_name + \" is running\")\n            time.sleep(self.__POLL_TIME)\n            status = self.__get_job_status()\n\n        assert status != \"FAILED\", \"Kubernetes job \" + self.uu_name + \" failed\"\n\n        # status == \"SUCCEEDED\"\n        self.__logger.info(\"Kubernetes job \" + self.uu_name + \" succeeded\")\n        self.signal_complete()", "output": "Poll job status while active", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _fingerprint(public_key, fingerprint_hash_type):\n    '''\n    \n\n    '''\n    if fingerprint_hash_type:\n        hash_type = fingerprint_hash_type.lower()\n    else:\n        hash_type = 'sha256'\n\n    try:\n        hash_func = getattr(hashlib, hash_type)\n    except AttributeError:\n        raise CommandExecutionError(\n            'The fingerprint_hash_type {0} is not supported.'.format(\n                hash_type\n            )\n        )\n\n    try:\n        if six.PY2:\n            raw_key = public_key.decode('base64')\n        else:\n            raw_key = base64.b64decode(public_key, validate=True)  # pylint: disable=E1123\n    except binascii.Error:\n        return None\n\n    ret = hash_func(raw_key).hexdigest()\n\n    chunks = [ret[i:i + 2] for i in range(0, len(ret), 2)]\n    return ':'.join(chunks)", "output": "Return a public key fingerprint based on its base64-encoded representation\n\n    The fingerprint string is formatted according to RFC 4716 (ch.4), that is,\n    in the form \"xx:xx:...:xx\"\n\n    If the key is invalid (incorrect base64 string), return None\n\n    public_key\n        The public key to return the fingerprint for\n\n    fingerprint_hash_type\n        The public key fingerprint hash type that the public key fingerprint\n        was originally hashed with. This defaults to ``sha256`` if not specified.\n\n        .. versionadded:: 2016.11.4\n        .. versionchanged:: 2017.7.0: default changed from ``md5`` to ``sha256``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_params(self, inputs, overwrite=False):\n        \"\"\"\"\"\"\n        inputs = [x if isinstance(x, DataDesc) else DataDesc(*x) for x in inputs]\n        input_shapes = {item.name: item.shape for item in inputs}\n        arg_shapes, _, aux_shapes = self.symbol.infer_shape(**input_shapes)\n        assert arg_shapes is not None\n        input_dtypes = {item.name: item.dtype for item in inputs}\n        arg_dtypes, _, aux_dtypes = self.symbol.infer_type(**input_dtypes)\n        assert arg_dtypes is not None\n\n        arg_names = self.symbol.list_arguments()\n        input_names = input_shapes.keys()\n        param_names = [key for key in arg_names if key not in input_names]\n        aux_names = self.symbol.list_auxiliary_states()\n\n        param_name_attrs = [x for x in zip(arg_names, arg_shapes, arg_dtypes)\n                            if x[0] in param_names]\n        arg_params = {k : nd.zeros(shape=s, dtype=t)\n                      for k, s, t in param_name_attrs}\n        aux_name_attrs = [x for x in zip(aux_names, aux_shapes, aux_dtypes)\n                          if x[0] in aux_names]\n        aux_params = {k : nd.zeros(shape=s, dtype=t)\n                      for k, s, t in aux_name_attrs}\n\n        for k, v in arg_params.items():\n            if self.arg_params and k in self.arg_params and (not overwrite):\n                arg_params[k][:] = self.arg_params[k][:]\n            else:\n                self.initializer(k, v)\n\n        for k, v in aux_params.items():\n            if self.aux_params and k in self.aux_params and (not overwrite):\n                aux_params[k][:] = self.aux_params[k][:]\n            else:\n                self.initializer(k, v)\n\n        self.arg_params = arg_params\n        self.aux_params = aux_params\n        return (arg_names, list(param_names), aux_names)", "output": "Initialize weight parameters and auxiliary states.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validated(self, *args, **kwargs):\n        \"\"\"  \"\"\"\n        always_return_document = kwargs.pop('always_return_document', False)\n        self.validate(*args, **kwargs)\n        if self._errors and not always_return_document:\n            return None\n        else:\n            return self.document", "output": "Wrapper around :meth:`~cerberus.Validator.validate` that returns\n            the normalized and validated document or :obj:`None` if validation\n            failed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_key(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        log.error(\n            'The create_key function must be called with -f or --function.'\n        )\n        return False\n\n    try:\n        result = query(\n            method='account',\n            command='keys',\n            args={'name': kwargs['name'],\n                  'public_key': kwargs['public_key']},\n            http_method='post'\n        )\n    except KeyError:\n        log.info('`name` and `public_key` arguments must be specified')\n        return False\n\n    return result", "output": "Upload a public key", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_default(self, obj):\n        ''' \n\n        '''\n        if self.name in obj._property_values:\n            # this shouldn't happen because we should have checked before _get_default()\n            raise RuntimeError(\"Bokeh internal error, does not handle the case of self.name already in _property_values\")\n\n        is_themed = obj.themed_values() is not None and self.name in obj.themed_values()\n\n        default = self.instance_default(obj)\n\n        if is_themed:\n            unstable_dict = obj._unstable_themed_values\n        else:\n            unstable_dict = obj._unstable_default_values\n\n        if self.name in unstable_dict:\n            return unstable_dict[self.name]\n\n        if self.property._may_have_unstable_default():\n            if isinstance(default, PropertyValueContainer):\n                default._register_owner(obj, self)\n            unstable_dict[self.name] = default\n\n        return default", "output": "Internal implementation of instance attribute access for default\n        values.\n\n        Handles bookeeping around |PropertyContainer| value, etc.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_raylet_monitor(redis_address,\n                         stdout_file=None,\n                         stderr_file=None,\n                         redis_password=None,\n                         config=None):\n    \"\"\"\n    \"\"\"\n    gcs_ip_address, gcs_port = redis_address.split(\":\")\n    redis_password = redis_password or \"\"\n    config = config or {}\n    config_str = \",\".join([\"{},{}\".format(*kv) for kv in config.items()])\n    command = [\n        RAYLET_MONITOR_EXECUTABLE,\n        \"--redis_address={}\".format(gcs_ip_address),\n        \"--redis_port={}\".format(gcs_port),\n        \"--config_list={}\".format(config_str),\n    ]\n    if redis_password:\n        command += [redis_password]\n    process_info = start_ray_process(\n        command,\n        ray_constants.PROCESS_TYPE_RAYLET_MONITOR,\n        stdout_file=stdout_file,\n        stderr_file=stderr_file)\n    return process_info", "output": "Run a process to monitor the other processes.\n\n    Args:\n        redis_address (str): The address that the Redis server is listening on.\n        stdout_file: A file handle opened for writing to redirect stdout to. If\n            no redirection should happen, then this should be None.\n        stderr_file: A file handle opened for writing to redirect stderr to. If\n            no redirection should happen, then this should be None.\n        redis_password (str): The password of the redis server.\n        config (dict|None): Optional configuration that will\n            override defaults in RayConfig.\n\n    Returns:\n        ProcessInfo for the process that was started.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Update(self, menu=None, tooltip=None,filename=None, data=None, data_base64=None,):\n        '''\n        \n        '''\n        # Menu\n        if menu is not None:\n            self.TaskBarIcon.menu = menu\n        if filename:\n            self.icon = wx.Icon(filename, wx.BITMAP_TYPE_ANY)\n        elif data_base64:\n            self.icon = PyEmbeddedImage(data_base64).GetIcon()\n        elif not self.icon:\n            self.icon = PyEmbeddedImage(DEFAULT_BASE64_ICON).GetIcon()\n        if self.icon:\n            self.Tooltip = tooltip or self.Tooltip\n            self.TaskBarIcon.SetIcon(self.icon, tooltip=self.Tooltip)", "output": "Updates the menu, tooltip or icon\n        :param menu: menu defintion\n        :param tooltip: string representing tooltip\n        :param filename:  icon filename\n        :param data:  icon raw image\n        :param data_base64: icon base 64 image\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def b_cubed(clusters, mention_to_gold):\n        \"\"\"\n        \n        \"\"\"\n        numerator, denominator = 0, 0\n        for cluster in clusters:\n            if len(cluster) == 1:\n                continue\n            gold_counts = Counter()\n            correct = 0\n            for mention in cluster:\n                if mention in mention_to_gold:\n                    gold_counts[tuple(mention_to_gold[mention])] += 1\n            for cluster2, count in gold_counts.items():\n                if len(cluster2) != 1:\n                    correct += count * count\n            numerator += correct / float(len(cluster))\n            denominator += len(cluster)\n        return numerator, denominator", "output": "Averaged per-mention precision and recall.\n        <https://pdfs.semanticscholar.org/cfe3/c24695f1c14b78a5b8e95bcbd1c666140fd1.pdf>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def boto_client(self, service, *args, **kwargs):\n        \"\"\"\"\"\"\n        return self.boto_session.client(service, *args, **self.configure_boto_session_method_kwargs(service, kwargs))", "output": "A wrapper to apply configuration options to boto clients", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_anagram(s, t):\n    \"\"\"\n    \n    \"\"\"\n    maps = {}\n    mapt = {}\n    for i in s:\n        maps[i] = maps.get(i, 0) + 1\n    for i in t:\n        mapt[i] = mapt.get(i, 0) + 1\n    return maps == mapt", "output": ":type s: str\n    :type t: str\n    :rtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_data_tick_resample(tick, type_='1min'):\n    \"\"\"\n    \"\"\"\n    tick = tick.assign(amount=tick.price * tick.vol)\n    resx = pd.DataFrame()\n    _temp = set(tick.index.date)\n\n    for item in _temp:\n        _data = tick.loc[str(item)]\n        _data1 = _data[time(9,\n                            31):time(11,\n                                     30)].resample(\n                                         type_,\n                                         closed='right',\n                                         base=30,\n                                         loffset=type_\n                                     ).apply(\n                                         {\n                                             'price': 'ohlc',\n                                             'vol': 'sum',\n                                             'code': 'last',\n                                             'amount': 'sum'\n                                         }\n                                     )\n\n        _data2 = _data[time(13,\n                            1):time(15,\n                                    0)].resample(\n                                        type_,\n                                        closed='right',\n                                        loffset=type_\n                                    ).apply(\n                                        {\n                                            'price': 'ohlc',\n                                            'vol': 'sum',\n                                            'code': 'last',\n                                            'amount': 'sum'\n                                        }\n                                    )\n\n        resx = resx.append(_data1).append(_data2)\n    resx.columns = resx.columns.droplevel(0)\n    return resx.reset_index().drop_duplicates().set_index(['datetime', 'code'])", "output": "tick\u91c7\u6837\u6210\u4efb\u610f\u7ea7\u522b\u5206\u949f\u7ebf\n\n    Arguments:\n        tick {[type]} -- transaction\n\n    Returns:\n        [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reread(user=None, conf_file=None, bin_env=None):\n    '''\n    \n    '''\n    ret = __salt__['cmd.run_all'](\n        _ctl_cmd('reread', None, conf_file, bin_env),\n        runas=user,\n        python_shell=False,\n    )\n    return _get_return(ret)", "output": "Reload the daemon's configuration files\n\n    user\n        user to run supervisorctl as\n    conf_file\n        path to supervisord config file\n    bin_env\n        path to supervisorctl bin or path to virtualenv with supervisor\n        installed\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' supervisord.reread", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_gaussian_weight(self, anchor):\n        \"\"\"\n        \n        \"\"\"\n        ret = np.zeros(self.shape, dtype='float32')\n\n        y, x = np.mgrid[:self.shape[0], :self.shape[1]]\n        y = y.astype('float32') / ret.shape[0] - anchor[0]\n        x = x.astype('float32') / ret.shape[1] - anchor[1]\n        g = np.exp(-(x**2 + y ** 2) / self.sigma)\n        # cv2.imshow(\" \", g)\n        # cv2.waitKey()\n        return g", "output": "Args:\n            anchor: coordinate of the center", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def detach_usage_plan_from_apis(plan_id, apis, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    return _update_usage_plan_apis(plan_id, apis, 'remove', region=region, key=key, keyid=keyid, profile=profile)", "output": "Detaches given usage plan from each of the apis provided in a list of apiId and stage value\n\n    .. versionadded:: 2017.7.0\n\n    apis\n        a list of dictionaries, where each dictionary contains the following:\n\n        apiId\n            a string, which is the id of the created API in AWS ApiGateway\n\n        stage\n            a string, which is the stage that the created API is deployed to.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_apigateway.detach_usage_plan_to_apis plan_id='usage plan id' apis='[{\"apiId\": \"some id 1\", \"stage\": \"some stage 1\"}]'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def restart_process(self, pid):\n        '''\n        \n        '''\n        if self._restart_processes is False:\n            return\n        log.info(\n            'Process %s (%s) died with exit status %s, restarting...',\n            self._process_map[pid]['tgt'],\n            pid,\n            self._process_map[pid]['Process'].exitcode\n        )\n        # don't block, the process is already dead\n        self._process_map[pid]['Process'].join(1)\n\n        self.add_process(self._process_map[pid]['tgt'],\n                         self._process_map[pid]['args'],\n                         self._process_map[pid]['kwargs'])\n\n        del self._process_map[pid]", "output": "Create new process (assuming this one is dead), then remove the old one", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_batch(data_source, i, seq_len=None):\n    \"\"\"\n    \"\"\"\n    seq_len = min(seq_len if seq_len else args.bptt, len(data_source) - 1 - i)\n    data = data_source[i:i+seq_len]\n    target = data_source[i+1:i+1+seq_len]\n    return data, target", "output": "Get mini-batches of the dataset.\n\n    Parameters\n    ----------\n    data_source : NDArray\n        The dataset is evaluated on.\n    i : int\n        The index of the batch, starting from 0.\n    seq_len : int\n        The length of each sample in the batch.\n\n    Returns\n    -------\n    data: NDArray\n        The context\n    target: NDArray\n        The words to predict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def adaptive_universal_transformer_base_range(rhp):\n  \"\"\"\"\"\"\n  # After starting from base, set intervals for some parameters.\n  rhp.set_discrete(\"act_max_steps\", [8, 16, 32])\n  rhp.set_float(\"act_loss_weight\", 0.0, 0.5)\n  rhp.set_discrete(\"hidden_size\", [1024, 2048, 4096])\n  rhp.set_discrete(\"filter_size\", [2048, 4096, 8192])\n  rhp.set_discrete(\"num_heads\", [8, 16, 32])\n  rhp.set_discrete(\"transformer_ffn_type\", [\"sepconv\", \"fc\"])\n  rhp.set_float(\"learning_rate\", 0.3, 3.0, scale=rhp.LOG_SCALE)\n  rhp.set_float(\"weight_decay\", 0.0, 2.0)", "output": "Range of hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ensure_list(iterable: Iterable[A]) -> List[A]:\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(iterable, list):\n        return iterable\n    else:\n        return list(iterable)", "output": "An Iterable may be a list or a generator.\n    This ensures we get a list without making an unnecessary copy.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_remote_added(remote):\n    '''\n    \n    '''\n    out = __salt__['cmd.run_all'](FLATPAK_BINARY_NAME + ' remotes')\n\n    lines = out.splitlines()\n    for item in lines:\n        i = re.split(r'\\t+', item.rstrip('\\t'))\n        if i[0] == remote:\n            return True\n    return False", "output": "Determines if a remote exists.\n\n    Args:\n        remote (str): The remote's name.\n\n    Returns:\n        bool: True if the remote has already been added.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' flatpak.is_remote_added flathub", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dfu_devices(*args, **kwargs):\n    \"\"\"\n    \"\"\"\n    # convert to list for compatibility with newer pyusb\n    return list(usb.core.find(*args, find_all=True,\n                              custom_match=FilterDFU(), **kwargs))", "output": "Returns a list of USB device which are currently in DFU mode.\n    Additional filters (like idProduct and idVendor) can be passed in to\n    refine the search.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def group_update(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    if 'new_name' in kwargs:\n        kwargs['name'] = kwargs.pop('new_name')\n    return cloud.update_group(**kwargs)", "output": "Update a group\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystoneng.group_update name=group1 description='new description'\n        salt '*' keystoneng.group_create name=group2 domain_id=b62e76fbeeff4e8fb77073f591cf211e new_name=newgroupname\n        salt '*' keystoneng.group_create name=0e4febc2a5ab4f2c8f374b054162506d new_name=newgroupname", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathNextParent(self, cur):\n        \"\"\" \"\"\"\n        if cur is None: cur__o = None\n        else: cur__o = cur._o\n        ret = libxml2mod.xmlXPathNextParent(self._o, cur__o)\n        if ret is None:raise xpathError('xmlXPathNextParent() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Traversal function for the \"parent\" direction The parent\n          axis contains the parent of the context node, if there is\n           one.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def change_exteditor(self):\r\n        \"\"\"\"\"\"\r\n        path, valid = QInputDialog.getText(self, _('External editor'),\r\n                          _('External editor executable path:'),\r\n                          QLineEdit.Normal,\r\n                          self.get_option('external_editor/path'))\r\n        if valid:\r\n            self.set_option('external_editor/path', to_text_string(path))", "output": "Change external editor path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def _retrieve_messages_around_strategy(self, retrieve):\n        \"\"\"\"\"\"\n        if self.around:\n            around = self.around.id if self.around else None\n            data = await self.logs_from(self.channel.id, retrieve, around=around)\n            self.around = None\n            return data\n        return []", "output": "Retrieve messages using around parameter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_message(self, message, binary=False, locked=True):\n        ''' \n\n        '''\n        if locked:\n            with (yield self.write_lock.acquire()):\n                yield super(WSHandler, self).write_message(message, binary)\n        else:\n            yield super(WSHandler, self).write_message(message, binary)", "output": "Override parent write_message with a version that acquires a\n        write lock before writing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pre_attention(self, segment, query_antecedent, memory_antecedent, bias):\n    \"\"\"\n    \"\"\"\n    del segment\n    return None, query_antecedent, memory_antecedent, bias", "output": "Called prior to self-attention, to incorporate memory items.\n\n    Args:\n      segment: an integer Tensor with shape [batch]\n      query_antecedent: a Tensor with shape [batch, length_q, channels]\n      memory_antecedent: must be None. Attention normally allows this to be a\n        Tensor with shape [batch, length_m, channels], but we currently only\n        support memory for decoder-side self-attention.\n      bias: bias Tensor (see attention_bias())\n    Returns:\n      (data, new_query_antecedent, new_memory_antecedent, new_bias)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_path_from_sys_path(self):\r\n        \"\"\"\"\"\"\r\n        for path in self.path + self.project_path:\r\n            while path in sys.path:\r\n                sys.path.remove(path)", "output": "Remove Spyder path from sys.path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _config(key, mandatory=True, opts=None):\n    '''\n    \n    '''\n    try:\n        if opts:\n            value = opts['auth.ldap.{0}'.format(key)]\n        else:\n            value = __opts__['auth.ldap.{0}'.format(key)]\n    except KeyError:\n        try:\n            value = __defopts__['auth.ldap.{0}'.format(key)]\n        except KeyError:\n            if mandatory:\n                msg = 'missing auth.ldap.{0} in master config'.format(key)\n                raise SaltInvocationError(msg)\n            return False\n    return value", "output": "Return a value for 'name' from master config file options or defaults.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def fetch_emojis(self):\n        \n        \"\"\"\n        data = await self._state.http.get_all_custom_emojis(self.id)\n        return [Emoji(guild=self, state=self._state, data=d) for d in data]", "output": "r\"\"\"|coro|\n\n        Retrieves all custom :class:`Emoji`\\s from the guild.\n\n        .. note::\n\n            This method is an API call. For general usage, consider :attr:`emojis` instead.\n\n        Raises\n        ---------\n        HTTPException\n            An error occurred fetching the emojis.\n\n        Returns\n        --------\n        List[:class:`Emoji`]\n            The retrieved emojis.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Place(self, x, flags):\n        \"\"\"\n        \n        \"\"\"\n\n        N.enforce_number(x, flags)\n        self.head = self.head - flags.bytewidth\n        encode.Write(flags.packer_type, self.Bytes, self.Head(), x)", "output": "Place prepends a value specified by `flags` to the Builder,\n        without checking for available space.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def human_and_11(X, y, model_generator, method_name):\n    \"\"\" \n    \"\"\"\n    return _human_and(X, model_generator, method_name, True, True)", "output": "AND (true/true)\n\n    This tests how well a feature attribution method agrees with human intuition\n    for an AND operation combined with linear effects. This metric deals\n    specifically with the question of credit allocation for the following function\n    when all three inputs are true:\n    if fever: +2 points\n    if cough: +2 points\n    if fever and cough: +6 points\n\n    transform = \"identity\"\n    sort_order = 2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_load(jid):\n    '''\n    \n    '''\n    serv = _get_serv(ret=None)\n    sql = \"select load from jids where jid = '{0}'\".format(jid)\n\n    log.debug(\">> Now in get_load %s\", jid)\n    data = serv.query(sql)\n    log.debug(\">> Now Data: %s\", data)\n    if data:\n        return data\n    return {}", "output": "Return the load data that marks a specified jid", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def adaptive_avgmax_pool2d(x, pool_type='avg', padding=0, count_include_pad=False):\n    \"\"\"\n    \"\"\"\n    if pool_type == 'avgmaxc':\n        x = torch.cat([\n            F.avg_pool2d(\n                x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad),\n            F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n        ], dim=1)\n    elif pool_type == 'avgmax':\n        x_avg = F.avg_pool2d(\n                x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad)\n        x_max = F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n        x = 0.5 * (x_avg + x_max)\n    elif pool_type == 'max':\n        x = F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n    else:\n        if pool_type != 'avg':\n            print('Invalid pool type %s specified. Defaulting to average pooling.' % pool_type)\n        x = F.avg_pool2d(\n            x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad)\n    return x", "output": "Selectable global pooling function with dynamic input kernel size", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_rest_server(rest_port):\n    ''''''\n    retry_count = 5\n    for _ in range(retry_count):\n        response = rest_get(check_status_url(rest_port), REST_TIME_OUT)\n        if response:\n            if response.status_code == 200:\n                return True, response\n            else:\n                return False, response\n        else:\n            time.sleep(3)\n    return  False, response", "output": "Check if restful server is ready", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __generate_actors(self, operator, upstream_channels,\n                          downstream_channels):\n        \"\"\"\n        \"\"\"\n        num_instances = operator.num_instances\n        logger.info(\"Generating {} actors of type {}...\".format(\n            num_instances, operator.type))\n        in_channels = upstream_channels.pop(\n            operator.id) if upstream_channels else []\n        handles = []\n        for i in range(num_instances):\n            # Collect input and output channels for the particular instance\n            ip = [\n                channel for channel in in_channels\n                if channel.dst_instance_id == i\n            ] if in_channels else []\n            op = [\n                channel for channels_list in downstream_channels.values()\n                for channel in channels_list if channel.src_instance_id == i\n            ]\n            log = \"Constructed {} input and {} output channels \"\n            log += \"for the {}-th instance of the {} operator.\"\n            logger.debug(log.format(len(ip), len(op), i, operator.type))\n            input_gate = DataInput(ip)\n            output_gate = DataOutput(op, operator.partitioning_strategies)\n            handle = self.__generate_actor(i, operator, input_gate,\n                                           output_gate)\n            if handle:\n                handles.append(handle)\n        return handles", "output": "Generates one actor for each instance of the given logical\n        operator.\n\n        Attributes:\n            operator (Operator): The logical operator metadata.\n            upstream_channels (list): A list of all upstream channels for\n            all instances of the operator.\n            downstream_channels (list): A list of all downstream channels\n            for all instances of the operator.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_weight_histograms(self, iteration:int)->None:\n        \"\"\n        self.hist_writer.write(model=self.learn.model, iteration=iteration, tbwriter=self.tbwriter)", "output": "Writes model weight histograms to Tensorboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_windowsfeatures():\n    '''\n    \n    '''\n    choc_path = _find_chocolatey(__context__, __salt__)\n    cmd = [choc_path, 'list', '--source', 'windowsfeatures']\n    result = __salt__['cmd.run_all'](cmd, python_shell=False)\n\n    if result['retcode'] != 0:\n        raise CommandExecutionError(\n            'Running chocolatey failed: {0}'.format(result['stdout'])\n        )\n\n    return result['stdout']", "output": "Instructs Chocolatey to pull a full package list from the Windows Features\n    list, via the Deployment Image Servicing and Management tool.\n\n    Returns:\n        str: List of Windows Features\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' chocolatey.list_windowsfeatures", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def normalize_path(path, resolve_symlinks=True):\n    # type: (str, bool) -> str\n    \"\"\"\n    \n\n    \"\"\"\n    path = expanduser(path)\n    if resolve_symlinks:\n        path = os.path.realpath(path)\n    else:\n        path = os.path.abspath(path)\n    return os.path.normcase(path)", "output": "Convert a path to its canonical, case-normalized, absolute version.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _handle_force_timeout(self) -> None:\n        \"\"\"\n        \"\"\"\n        while True:\n            try:\n                ret, num_handles = self._multi.socket_all()\n            except pycurl.error as e:\n                ret = e.args[0]\n            if ret != pycurl.E_CALL_MULTI_PERFORM:\n                break\n        self._finish_pending_requests()", "output": "Called by IOLoop periodically to ask libcurl to process any\n        events it may have forgotten about.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forward_ocr(self, img_):\n        \"\"\"\n        \"\"\"\n        img_ = cv2.resize(img_, (80, 30))\n        img_ = img_.transpose(1, 0)\n        print(img_.shape)\n        img_ = img_.reshape((1, 80, 30))\n        print(img_.shape)\n        # img_ = img_.reshape((80 * 30))\n        img_ = np.multiply(img_, 1 / 255.0)\n        self.predictor.forward(data=img_, **self.init_state_dict)\n        prob = self.predictor.get_output(0)\n        label_list = []\n        for p in prob:\n            print(np.argsort(p))\n            max_index = np.argsort(p)[::-1][0]\n            label_list.append(max_index)\n        return self.__get_string(label_list)", "output": "Forward the image through the LSTM network model\n\n        Parameters\n        ----------\n        img_: int of array\n\n        Returns\n        ----------\n        label_list: string of list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def competition_submissions_cli(self,\n                                    competition=None,\n                                    competition_opt=None,\n                                    csv_display=False,\n                                    quiet=False):\n        \"\"\" \n        \"\"\"\n        competition = competition or competition_opt\n        if competition is None:\n            competition = self.get_config_value(self.CONFIG_NAME_COMPETITION)\n            if competition is not None and not quiet:\n                print('Using competition: ' + competition)\n\n        if competition is None:\n            raise ValueError('No competition specified')\n        else:\n            submissions = self.competition_submissions(competition)\n            fields = [\n                'fileName', 'date', 'description', 'status', 'publicScore',\n                'privateScore'\n            ]\n            if submissions:\n                if csv_display:\n                    self.print_csv(submissions, fields)\n                else:\n                    self.print_table(submissions, fields)\n            else:\n                print('No submissions found')", "output": "wrapper to competition_submission, will return either json or csv\n            to the user. Additional parameters are listed below, see\n            competition_submissions for rest.\n\n            Parameters\n            ==========\n            competition: the name of the competition. If None, look to config\n            competition_opt: an alternative competition option provided by cli\n            csv_display: if True, print comma separated values\n            quiet: suppress verbose output (default is False)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _FindFileContainingSymbolInDb(self, symbol):\n    \"\"\"\n    \"\"\"\n    try:\n      file_proto = self._internal_db.FindFileContainingSymbol(symbol)\n    except KeyError as error:\n      if self._descriptor_db:\n        file_proto = self._descriptor_db.FindFileContainingSymbol(symbol)\n      else:\n        raise error\n    if not file_proto:\n      raise KeyError('Cannot find a file containing %s' % symbol)\n    return self._ConvertFileProtoToFileDescriptor(file_proto)", "output": "Finds the file in descriptor DB containing the specified symbol.\n\n    Args:\n      symbol: The name of the symbol to search for.\n\n    Returns:\n      A FileDescriptor that contains the specified symbol.\n\n    Raises:\n      KeyError: if the file cannot be found in the descriptor database.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cleanup_global_logging(stdout_handler: logging.FileHandler) -> None:\n    \"\"\"\n    \n    \"\"\"\n    stdout_handler.close()\n    logging.getLogger().removeHandler(stdout_handler)\n\n    if isinstance(sys.stdout, TeeLogger):\n        sys.stdout = sys.stdout.cleanup()\n    if isinstance(sys.stderr, TeeLogger):\n        sys.stderr = sys.stderr.cleanup()", "output": "This function closes any open file handles and logs set up by `prepare_global_logging`.\n\n    Parameters\n    ----------\n    stdout_handler : ``logging.FileHandler``, required.\n        The file handler returned from `prepare_global_logging`, attached to the global logger.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _element_equal(x, y):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(x, _np.ndarray) or isinstance(y, _np.ndarray):\n        try:\n            return (abs(_np.asarray(x) - _np.asarray(y)) < 1e-5).all()\n        except:\n            return False\n    elif isinstance(x, dict):\n        return (isinstance(y, dict)\n                and _element_equal(x.keys(), y.keys())\n                and all(_element_equal(x[k], y[k]) for k in x.keys()))\n    elif isinstance(x, float):\n        return abs(x - y) < 1e-5 * (abs(x) + abs(y))\n    elif isinstance(x, (list, tuple)):\n        return x == y\n    else:\n        return bool(x == y)", "output": "Performs a robust equality test between elements.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _objects_touch_each_other(self, object1: Object, object2: Object) -> bool:\n        \"\"\"\n        \n        \"\"\"\n        in_vertical_range = object1.y_loc <= object2.y_loc + object2.size and \\\n                            object1.y_loc + object1.size >= object2.y_loc\n        in_horizantal_range = object1.x_loc <= object2.x_loc + object2.size and \\\n                            object1.x_loc + object1.size >= object2.x_loc\n        touch_side = object1.x_loc + object1.size == object2.x_loc or \\\n                     object2.x_loc + object2.size == object1.x_loc\n        touch_top_or_bottom = object1.y_loc + object1.size == object2.y_loc or \\\n                              object2.y_loc + object2.size == object1.y_loc\n        return (in_vertical_range and touch_side) or (in_horizantal_range and touch_top_or_bottom)", "output": "Returns true iff the objects touch each other.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def quick_layout_save(self):\r\n        \"\"\"\"\"\"\r\n        get = CONF.get\r\n        set_ = CONF.set\r\n        names = get('quick_layouts', 'names')\r\n        order = get('quick_layouts', 'order')\r\n        active = get('quick_layouts', 'active')\r\n\r\n        dlg = self.dialog_layout_save(self, names)\r\n\r\n        if dlg.exec_():\r\n            name = dlg.combo_box.currentText()\r\n\r\n            if name in names:\r\n                answer = QMessageBox.warning(self, _(\"Warning\"),\r\n                                             _(\"Layout <b>%s</b> will be \\\r\n                                               overwritten. Do you want to \\\r\n                                               continue?\") % name,\r\n                                             QMessageBox.Yes | QMessageBox.No)\r\n                index = order.index(name)\r\n            else:\r\n                answer = True\r\n                if None in names:\r\n                    index = names.index(None)\r\n                    names[index] = name\r\n                else:\r\n                    index = len(names)\r\n                    names.append(name)\r\n                order.append(name)\r\n\r\n            # Always make active a new layout even if it overwrites an inactive\r\n            # layout\r\n            if name not in active:\r\n                active.append(name)\r\n\r\n            if answer:\r\n                self.save_current_window_settings('layout_{}/'.format(index),\r\n                                                  section='quick_layouts')\r\n                set_('quick_layouts', 'names', names)\r\n                set_('quick_layouts', 'order', order)\r\n                set_('quick_layouts', 'active', active)\r\n                self.quick_layout_set_menu()", "output": "Save layout dialog", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def split_by_valid_func(self, func:Callable)->'ItemLists':\n        \"\"\n        valid_idx = [i for i,o in enumerate(self.items) if func(o)]\n        return self.split_by_idx(valid_idx)", "output": "Split the data by result of `func` (which returns `True` for validation set).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_trial_config(experiment_config, port, config_file_name):\n    ''''''\n    request_data = dict()\n    request_data['trial_config'] = experiment_config['trial']\n    response = rest_put(cluster_metadata_url(port), json.dumps(request_data), REST_TIME_OUT)\n    if check_response(response):\n        return True\n    else:\n        print('Error message is {}'.format(response.text))\n        _, stderr_full_path = get_log_path(config_file_name)\n        if response:\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(response.text), indent=4, sort_keys=True, separators=(',', ':')))\n        return False", "output": "set trial configuration", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def traceParseAction(f):\n    \"\"\"\n    \"\"\"\n    f = _trim_arity(f)\n    def z(*paArgs):\n        thisFunc = f.__name__\n        s,l,t = paArgs[-3:]\n        if len(paArgs)>3:\n            thisFunc = paArgs[0].__class__.__name__ + '.' + thisFunc\n        sys.stderr.write( \">>entering %s(line: '%s', %d, %r)\\n\" % (thisFunc,line(l,s),l,t) )\n        try:\n            ret = f(*paArgs)\n        except Exception as exc:\n            sys.stderr.write( \"<<leaving %s (exception: %s)\\n\" % (thisFunc,exc) )\n            raise\n        sys.stderr.write( \"<<leaving %s (ret: %r)\\n\" % (thisFunc,ret) )\n        return ret\n    try:\n        z.__name__ = f.__name__\n    except AttributeError:\n        pass\n    return z", "output": "Decorator for debugging parse actions.\n\n    When the parse action is called, this decorator will print\n    ``\">> entering method-name(line:<current_source_line>, <parse_location>, <matched_tokens>)\"``.\n    When the parse action completes, the decorator will print\n    ``\"<<\"`` followed by the returned value, or any exception that the parse action raised.\n\n    Example::\n\n        wd = Word(alphas)\n\n        @traceParseAction\n        def remove_duplicate_chars(tokens):\n            return ''.join(sorted(set(''.join(tokens))))\n\n        wds = OneOrMore(wd).setParseAction(remove_duplicate_chars)\n        print(wds.parseString(\"slkdjs sld sldd sdlf sdljf\"))\n\n    prints::\n\n        >>entering remove_duplicate_chars(line: 'slkdjs sld sldd sdlf sdljf', 0, (['slkdjs', 'sld', 'sldd', 'sdlf', 'sdljf'], {}))\n        <<leaving remove_duplicate_chars (ret: 'dfjkls')\n        ['dfjkls']", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cut_for_search(self, sentence, HMM=True):\n        \"\"\"\n        \n        \"\"\"\n        words = self.cut(sentence, HMM=HMM)\n        for w in words:\n            if len(w) > 2:\n                for i in xrange(len(w) - 1):\n                    gram2 = w[i:i + 2]\n                    if self.FREQ.get(gram2):\n                        yield gram2\n            if len(w) > 3:\n                for i in xrange(len(w) - 2):\n                    gram3 = w[i:i + 3]\n                    if self.FREQ.get(gram3):\n                        yield gram3\n            yield w", "output": "Finer segmentation for search engines.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def evaluate(self, dataset):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(dataset, DataFrame):\n            raise ValueError(\"dataset must be a DataFrame but got %s.\" % type(dataset))\n        java_lr_summary = self._call_java(\"evaluate\", dataset)\n        return LinearRegressionSummary(java_lr_summary)", "output": "Evaluates the model on a test dataset.\n\n        :param dataset:\n          Test dataset to evaluate model on, where dataset is an\n          instance of :py:class:`pyspark.sql.DataFrame`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_extras(self):\n        # type: () -> None\n        \"\"\"\n        \n        \"\"\"\n\n        extras = None\n        if \"@\" in self.line or self.is_vcs or self.is_url:\n            line = \"{0}\".format(self.line)\n            uri = URI.parse(line)\n            name = uri.name\n            if name:\n                self._name = name\n            if uri.host and uri.path and uri.scheme:\n                self.line = uri.to_string(\n                    escape_password=False, direct=False, strip_ssh=uri.is_implicit_ssh\n                )\n            else:\n                self.line, extras = pip_shims.shims._strip_extras(self.line)\n        else:\n            self.line, extras = pip_shims.shims._strip_extras(self.line)\n        extras_set = set()  # type: Set[STRING_TYPE]\n        if extras is not None:\n            extras_set = set(parse_extras(extras))\n        if self._name:\n            self._name, name_extras = pip_shims.shims._strip_extras(self._name)\n            if name_extras:\n                name_extras = set(parse_extras(name_extras))\n                extras_set |= name_extras\n        if extras_set is not None:\n            self.extras = tuple(sorted(extras_set))", "output": "Parse extras from *self.line* and set them on the current object\n        :returns: Nothing\n        :rtype: None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathNextAttribute(self, cur):\n        \"\"\" \"\"\"\n        if cur is None: cur__o = None\n        else: cur__o = cur._o\n        ret = libxml2mod.xmlXPathNextAttribute(self._o, cur__o)\n        if ret is None:raise xpathError('xmlXPathNextAttribute() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Traversal function for the \"attribute\" direction TODO:\n           support DTD inherited default attributes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enabled(job_label, runas=None):\n    '''\n    \n    '''\n    overrides_data = dict(plistlib.readPlist(\n        '/var/db/launchd.db/com.apple.launchd/overrides.plist'\n    ))\n    if overrides_data.get(job_label, False):\n        if overrides_data[job_label]['Disabled']:\n            return False\n        else:\n            return True\n    else:\n        return False", "output": "Return True if the named service is enabled, false otherwise\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.enabled <service label>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def split_markers_from_line(line):\n    # type: (AnyStr) -> Tuple[AnyStr, Optional[AnyStr]]\n    \"\"\"\"\"\"\n    if not any(line.startswith(uri_prefix) for uri_prefix in SCHEME_LIST):\n        marker_sep = \";\"\n    else:\n        marker_sep = \"; \"\n    markers = None\n    if marker_sep in line:\n        line, markers = line.split(marker_sep, 1)\n        markers = markers.strip() if markers else None\n    return line, markers", "output": "Split markers from a dependency", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def intranges_from_list(list_):\n    \"\"\"\n    \"\"\"\n\n    sorted_list = sorted(list_)\n    ranges = []\n    last_write = -1\n    for i in range(len(sorted_list)):\n        if i+1 < len(sorted_list):\n            if sorted_list[i] == sorted_list[i+1]-1:\n                continue\n        current_range = sorted_list[last_write+1:i+1]\n        ranges.append(_encode_range(current_range[0], current_range[-1] + 1))\n        last_write = i\n\n    return tuple(ranges)", "output": "Represent a list of integers as a sequence of ranges:\n    ((start_0, end_0), (start_1, end_1), ...), such that the original\n    integers are exactly those x such that start_i <= x < end_i for some i.\n\n    Ranges are encoded as single integers (start << 32 | end), not as tuples.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def video_l2_loss(top_out, targets, model_hparams, vocab_size, weights_fn):\n  \"\"\"\"\"\"\n  del vocab_size  # unused arg\n  logits = top_out\n  logits = tf.reshape(logits, [-1] + common_layers.shape_list(logits)[2:-1])\n  targets = tf.reshape(targets, [-1] + common_layers.shape_list(targets)[2:])\n  weights = weights_fn(targets)\n  # Shift targets by 0.5 so later just casting to int gives the prediction.\n  # So for int targets, say 0 and 7, we actually train to predict 0.5 and 7.5.\n  # Later (in merics or infer) this is cast to int anyway. Also, we have no\n  # loss beyond cutoff = 0.2 as these are already correct predictions.\n  targets = tf.to_float(targets) + 0.5\n  loss = video_l2_internal_loss(logits, targets, model_hparams)\n  return tf.reduce_sum(loss * weights), tf.reduce_sum(weights)", "output": "Compute loss numerator and denominator for one shard of output.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_context_menu_actions(self):\r\n        \"\"\"\"\"\"\r\n        actions = []\r\n        fnames = self.get_selected_filenames()\r\n        new_actions = self.create_file_new_actions(fnames)\r\n        if len(new_actions) > 1:\r\n            # Creating a submenu only if there is more than one entry\r\n            new_act_menu = QMenu(_('New'), self)\r\n            add_actions(new_act_menu, new_actions)\r\n            actions.append(new_act_menu)\r\n        else:\r\n            actions += new_actions\r\n        import_actions = self.create_file_import_actions(fnames)\r\n        if len(import_actions) > 1:\r\n            # Creating a submenu only if there is more than one entry\r\n            import_act_menu = QMenu(_('Import'), self)\r\n            add_actions(import_act_menu, import_actions)\r\n            actions.append(import_act_menu)\r\n        else:\r\n            actions += import_actions\r\n        if actions:\r\n            actions.append(None)\r\n        if fnames:\r\n            actions += self.create_file_manage_actions(fnames)\r\n        if actions:\r\n            actions.append(None)\r\n        if fnames and all([osp.isdir(_fn) for _fn in fnames]):\r\n            actions += self.create_folder_manage_actions(fnames)\r\n        return actions", "output": "Create context menu actions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_disk(name=None, kwargs=None, call=None):  # pylint: disable=W0613\n    '''\n    \n    '''\n    if not kwargs or 'disk_name' not in kwargs:\n        log.error(\n            'Must specify disk_name.'\n        )\n        return False\n\n    conn = get_conn()\n    return _expand_disk(conn.ex_get_volume(kwargs['disk_name']))", "output": "Show the details of an existing disk.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a show_disk myinstance disk_name=mydisk\n        salt-cloud -f show_disk gce disk_name=mydisk", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _split_generators(self, dl_manager):\n    \"\"\"\"\"\"\n    paths = dl_manager.download_and_extract(_URLS)\n    # Load labels from CSVs:\n    def load(names):\n      csv_positions = [0] * len(names)\n      return functools.partial(_load_objects, [paths[name] for name in names],\n                               csv_positions)\n    train_objects = load(['train_human_labels', 'train_machine_labels'])\n    test_objects = load(['test_human_labels', 'test_machine_labels'])\n    validation_objects = load(['validation_human_labels',\n                               'validation_machine_labels'])\n    def load_boxes(name):\n      csv_positions = [0]\n      return functools.partial(_load_bboxes, paths[name], csv_positions)\n    train_bbox = load_boxes('train-annotations-bbox')\n    test_bbox = load_boxes('test-annotations-bbox')\n    validation_bbox = load_boxes('validation-annotations-bbox')\n    return [\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TRAIN,\n            num_shards=512,\n            gen_kwargs=dict(archive_paths=paths['train_images'],\n                            objects_getter=train_objects,\n                            bboxes_getter=train_bbox,\n                            prefixes='0123456789abcdef'),\n        ),\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TEST,\n            num_shards=36,\n            gen_kwargs=dict(archive_paths=[paths['test_images']],\n                            objects_getter=test_objects,\n                            bboxes_getter=test_bbox),\n        ),\n        tfds.core.SplitGenerator(\n            name=tfds.Split.VALIDATION,\n            num_shards=12,\n            gen_kwargs=dict(archive_paths=[paths['validation_images']],\n                            objects_getter=validation_objects,\n                            bboxes_getter=validation_bbox),\n        ),\n    ]", "output": "Returns SplitGenerators.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def open(self, fnames=None):\r\n        \"\"\"\"\"\"\r\n        if fnames is None:\r\n            fnames = self.get_selected_filenames()\r\n        for fname in fnames:\r\n            if osp.isfile(fname) and encoding.is_text_file(fname):\r\n                self.parent_widget.sig_open_file.emit(fname)\r\n            else:\r\n                self.open_outside_spyder([fname])", "output": "Open files with the appropriate application", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def object_absent(container, name, profile):\n    '''\n    \n    '''\n    existing_object = __salt__['libcloud_storage.get_container_object'](container, name, profile)\n    if existing_object is None:\n        return state_result(True, \"Object already absent\", name, {})\n    else:\n        result = __salt__['libcloud_storage.delete_object'](container, name, profile)\n        return state_result(result, \"Deleted object\", name, {})", "output": "Ensures a object is absent.\n\n    :param container: Container name\n    :type  container: ``str``\n\n    :param name: Object name in cloud\n    :type  name: ``str``\n\n    :param profile: The profile key\n    :type  profile: ``str``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def properties_with_values(self, include_defaults=True):\n        ''' \n\n        '''\n        return self.query_properties_with_values(lambda prop: prop.serialized, include_defaults)", "output": "Collect a dict mapping property names to their values.\n\n        This method *always* traverses the class hierarchy and includes\n        properties defined on any parent classes.\n\n        Non-serializable properties are skipped and property values are in\n        \"serialized\" format which may be slightly different from the values\n        you would normally read from the properties; the intent of this method\n        is to return the information needed to losslessly reconstitute the\n        object instance.\n\n        Args:\n            include_defaults (bool, optional) :\n                Whether to include properties that haven't been explicitly set\n                since the object was created. (default: True)\n\n        Returns:\n           dict : mapping from property names to their values", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_uid(name):\n    \"\"\"\"\"\"\n    if getpwnam is None or name is None:\n        return None\n    try:\n        result = getpwnam(name)\n    except KeyError:\n        result = None\n    if result is not None:\n        return result[2]\n    return None", "output": "Returns an uid, given a user name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_global(self, obj, name=None, pack=struct.pack):\n        \"\"\"\n        \n        \"\"\"\n        if obj.__module__ == \"__builtin__\" or obj.__module__ == \"builtins\":\n            if obj in _BUILTIN_TYPE_NAMES:\n                return self.save_reduce(_builtin_type, (_BUILTIN_TYPE_NAMES[obj],), obj=obj)\n\n        if name is None:\n            name = obj.__name__\n\n        modname = getattr(obj, \"__module__\", None)\n        if modname is None:\n            try:\n                # whichmodule() could fail, see\n                # https://bitbucket.org/gutworth/six/issues/63/importing-six-breaks-pickling\n                modname = pickle.whichmodule(obj, name)\n            except Exception:\n                modname = '__main__'\n\n        if modname == '__main__':\n            themodule = None\n        else:\n            __import__(modname)\n            themodule = sys.modules[modname]\n            self.modules.add(themodule)\n\n        if hasattr(themodule, name) and getattr(themodule, name) is obj:\n            return Pickler.save_global(self, obj, name)\n\n        typ = type(obj)\n        if typ is not obj and isinstance(obj, (type, _class_type)):\n            self.save_dynamic_class(obj)\n        else:\n            raise pickle.PicklingError(\"Can't pickle %r\" % obj)", "output": "Save a \"global\".\n\n        The name of this method is somewhat misleading: all types get\n        dispatched here.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copyFileToHdfs(localFilePath, hdfsFilePath, hdfsClient, override=True):\n    ''''''\n    if not os.path.exists(localFilePath):\n        raise Exception('Local file Path does not exist!')\n    if os.path.isdir(localFilePath):\n        raise Exception('localFile should not a directory!')\n    if hdfsClient.exists(hdfsFilePath):\n        if override:\n            hdfsClient.delete(hdfsFilePath)\n        else:\n            return False\n    try:\n        hdfsClient.copy_from_local(localFilePath, hdfsFilePath)\n        return True\n    except Exception as exception:\n        nni_log(LogType.Error, 'Copy local file {0} to hdfs file {1} error: {2}'.format(localFilePath, hdfsFilePath, str(exception)))\n        return False", "output": "Copy a local file to HDFS directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _maybe_convert_usecols(usecols):\n    \"\"\"\n    \n    \"\"\"\n    if usecols is None:\n        return usecols\n\n    if is_integer(usecols):\n        warnings.warn((\"Passing in an integer for `usecols` has been \"\n                       \"deprecated. Please pass in a list of int from \"\n                       \"0 to `usecols` inclusive instead.\"),\n                      FutureWarning, stacklevel=2)\n        return lrange(usecols + 1)\n\n    if isinstance(usecols, str):\n        return _range2cols(usecols)\n\n    return usecols", "output": "Convert `usecols` into a compatible format for parsing in `parsers.py`.\n\n    Parameters\n    ----------\n    usecols : object\n        The use-columns object to potentially convert.\n\n    Returns\n    -------\n    converted : object\n        The compatible format of `usecols`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dropout_mask(x, sz, dropout):\n    \"\"\" \n    \"\"\"\n    return x.new(*sz).bernoulli_(1-dropout)/(1-dropout)", "output": "Applies a dropout mask whose size is determined by passed argument 'sz'.\n    Args:\n        x (nn.Variable): A torch Variable object\n        sz (tuple(int, int, int)): The expected size of the new tensor\n        dropout (float): The dropout fraction to apply\n\n    This method uses the bernoulli distribution to decide which activations to keep.\n    Additionally, the sampled activations is rescaled is using the factor 1/(1 - dropout).\n\n    In the example given below, one can see that approximately .8 fraction of the\n    returned tensors are zero. Rescaling with the factor 1/(1 - 0.8) returns a tensor\n    with 5's in the unit places.\n\n    The official link to the pytorch bernoulli function is here:\n        http://pytorch.org/docs/master/torch.html#torch.bernoulli\n\n    Examples:\n        >>> a_Var = torch.autograd.Variable(torch.Tensor(2, 3, 4).uniform_(0, 1), requires_grad=False)\n        >>> a_Var\n            Variable containing:\n            (0 ,.,.) =\n              0.6890  0.5412  0.4303  0.8918\n              0.3871  0.7944  0.0791  0.5979\n              0.4575  0.7036  0.6186  0.7217\n            (1 ,.,.) =\n              0.8354  0.1690  0.1734  0.8099\n              0.6002  0.2602  0.7907  0.4446\n              0.5877  0.7464  0.4257  0.3386\n            [torch.FloatTensor of size 2x3x4]\n        >>> a_mask = dropout_mask(a_Var.data, (1,a_Var.size(1),a_Var.size(2)), dropout=0.8)\n        >>> a_mask\n            (0 ,.,.) =\n              0  5  0  0\n              0  0  0  5\n              5  0  5  0\n            [torch.FloatTensor of size 1x3x4]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render_tile(cells, ti, tj, render, params, metadata, layout, summary):\n  \"\"\"\n    \n  \"\"\"\n  image_size = params[\"cell_size\"] * params[\"n_tile\"]\n  tile = Image.new(\"RGB\", (image_size, image_size), (255,255,255))\n  keys = cells.keys()\n  for i,key in enumerate(keys):\n    print(\"cell\", i+1, \"/\", len(keys), end='\\r')\n    cell_image = render(cells[key], params, metadata, layout, summary)\n    # stitch this rendering into the tile image\n    ci = key[0] % params[\"n_tile\"]\n    cj = key[1] % params[\"n_tile\"]\n    xmin = ci*params[\"cell_size\"]\n    ymin = cj*params[\"cell_size\"]\n    xmax = (ci+1)*params[\"cell_size\"]\n    ymax = (cj+1)*params[\"cell_size\"]\n\n    if params.get(\"scale_density\", False):\n      density = len(cells[key][\"gi\"])\n      # scale = density/summary[\"max_density\"]\n      scale = math.log(density)/(math.log(summary[\"max_density\"]) or 1)\n      owidth = xmax - xmin\n      width = int(round(owidth * scale))\n      if(width < 1):\n        width = 1\n      offsetL = int(round((owidth - width)/2))\n      offsetR = owidth - width - offsetL # handle odd numbers\n      # print(\"\\n\")\n      # print(\"width\", width, offsetL, offsetR)\n      box = [xmin + offsetL, ymin + offsetL, xmax - offsetR, ymax - offsetR]\n      resample = params.get(\"scale_type\", Image.NEAREST)\n      cell_image = cell_image.resize(size=(width,width), resample=resample)\n      # print(cell_image)\n    else:\n      box = [xmin, ymin, xmax, ymax]\n\n    # print(\"box\", box)\n    tile.paste(cell_image, box)\n  print(\"\\n\")\n  return tile", "output": "Render each cell in the tile and stitch it into a single image", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rpc(command, **kwargs):\n    '''\n    \n    '''\n    default_map = {\n        'junos': 'napalm.junos_rpc',\n        'eos': 'napalm.pyeapi_run_commands',\n        'nxos': 'napalm.nxos_api_rpc'\n    }\n    napalm_map = __salt__['config.get']('napalm_rpc_map', {})\n    napalm_map.update(default_map)\n    fun = napalm_map.get(__grains__['os'], 'napalm.netmiko_commands')\n    return __salt__[fun](command, **kwargs)", "output": ".. versionadded:: 2019.2.0\n\n    This is a wrapper to execute RPC requests on various network operating\n    systems supported by NAPALM, invoking the following functions for the NAPALM\n    native drivers:\n\n    - :py:func:`napalm.junos_rpc <salt.modules.napalm_mod.junos_rpc>` for ``junos``\n    - :py:func:`napalm.pyeapi_run_commands <salt.modules.napalm_mod.pyeapi_run_commands>`\n      for ``eos``\n    - :py:func:`napalm.nxos_api_rpc <salt.modules.napalm_mod.nxos_api_rpc>` for\n      ``nxos``\n    - :py:func:`napalm.netmiko_commands <salt.modules.napalm_mod.netmiko_commands>`\n      for ``ios``, ``iosxr``, and ``nxos_ssh``\n\n    command\n        The RPC command to execute. This depends on the nature of the operating\n        system.\n\n    kwargs\n        Key-value arguments to be sent to the underlying Execution function.\n\n    The function capabilities are extensible in the user environment via the\n    ``napalm_rpc_map`` configuration option / Pillar, e.g.,\n\n    .. code-block:: yaml\n\n        napalm_rpc_map:\n          f5: napalm.netmiko_commands\n          panos: panos.call\n\n    The mapping above reads: when the NAPALM ``os`` Grain is ``f5``, then call\n    ``napalm.netmiko_commands`` for RPC requests.\n\n    By default, if the user does not specify any map, non-native NAPALM drivers\n    will invoke the ``napalm.netmiko_commands`` Execution function.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm.rpc 'show version'\n        salt '*' napalm.rpc get-interfaces", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _rgb_randomize(x, channel:int=None, thresh:float=0.3):\n    \"\"\n    if channel is None: channel = np.random.randint(0, x.shape[0] - 1)\n    x[channel] = torch.rand(x.shape[1:]) * np.random.uniform(0, thresh)\n    return x", "output": "Randomize one of the channels of the input image", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def insert_item(self):\r\n        \"\"\"\"\"\"\r\n        index = self.currentIndex()\r\n        if not index.isValid():\r\n            row = self.model.rowCount()\r\n        else:\r\n            row = index.row()\r\n        data = self.model.get_data()\r\n        if isinstance(data, list):\r\n            key = row\r\n            data.insert(row, '')\r\n        elif isinstance(data, dict):\r\n            key, valid = QInputDialog.getText(self, _( 'Insert'), _( 'Key:'),\r\n                                              QLineEdit.Normal)\r\n            if valid and to_text_string(key):\r\n                key = try_to_eval(to_text_string(key))\r\n            else:\r\n                return\r\n        else:\r\n            return\r\n        value, valid = QInputDialog.getText(self, _('Insert'), _('Value:'),\r\n                                            QLineEdit.Normal)\r\n        if valid and to_text_string(value):\r\n            self.new_value(key, try_to_eval(to_text_string(value)))", "output": "Insert item", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_time_limit_wrapper(env):\n  \"\"\"\n  \"\"\"\n  if isinstance(env, gym.wrappers.TimeLimit):\n    env = env.env\n  env_ = env\n  while isinstance(env_, gym.Wrapper):\n    if isinstance(env_, gym.wrappers.TimeLimit):\n      raise ValueError(\"Can remove only top-level TimeLimit gym.Wrapper.\")\n    env_ = env_.env\n  return env", "output": "Removes top level TimeLimit Wrapper.\n\n  Removes TimeLimit Wrapper from top level if exists, throws error if any other\n  TimeLimit Wrapper is present in stack.\n\n  Args:\n    env: environment\n\n  Returns:\n    the env with removed time limit wrapper.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_node(conn, name):\n    '''\n    \n    '''\n    nodes = conn.list_nodes()\n    for node in nodes:\n        if node.name == name:\n            __utils__['cloud.cache_node'](salt.utils.data.simple_types_filter(node.__dict__), __active_provider_name__, __opts__)\n            return node", "output": "Return a libcloud node for the named VM", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_all_from_datastore(self):\n    \"\"\"\"\"\"\n    self._work = {}\n    client = self._datastore_client\n    parent_key = client.key(KIND_WORK_TYPE, self._work_type_entity_id)\n    for entity in client.query_fetch(kind=KIND_WORK, ancestor=parent_key):\n      work_id = entity.key.flat_path[-1]\n      self.work[work_id] = dict(entity)", "output": "Reads all work pieces from the datastore.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def random_targets(gt, nb_classes):\n  \"\"\"\n  \n  \"\"\"\n  # If the ground truth labels are encoded as one-hot, convert to labels.\n  if len(gt.shape) == 2:\n    gt = np.argmax(gt, axis=1)\n\n  # This vector will hold the randomly selected labels.\n  result = np.zeros(gt.shape, dtype=np.int32)\n\n  for class_ind in xrange(nb_classes):\n    # Compute all indices in that class.\n    in_cl = gt == class_ind\n    size = np.sum(in_cl)\n\n    # Compute the set of potential targets for this class.\n    potential_targets = other_classes(nb_classes, class_ind)\n\n    # Draw with replacement random targets among the potential targets.\n    result[in_cl] = np.random.choice(potential_targets, size=size)\n\n  # Encode vector of random labels as one-hot labels.\n  result = to_categorical(result, nb_classes)\n  result = result.astype(np.int32)\n\n  return result", "output": "Take in an array of correct labels and randomly select a different label\n  for each label in the array. This is typically used to randomly select a\n  target class in targeted adversarial examples attacks (i.e., when the\n  search algorithm takes in both a source class and target class to compute\n  the adversarial example).\n  :param gt: the ground truth (correct) labels. They can be provided as a\n             1D vector or 2D array of one-hot encoded labels.\n  :param nb_classes: The number of classes for this task. The random class\n                     will be chosen between 0 and nb_classes such that it\n                     is different from the correct class.\n  :return: A numpy array holding the randomly-selected target classes\n           encoded as one-hot labels.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def insert(self, loc, item, value, allow_duplicates=False):\n        \"\"\"\n        \n\n        \"\"\"\n        if not allow_duplicates and item in self.items:\n            # Should this be a different kind of error??\n            raise ValueError('cannot insert {}, already exists'.format(item))\n\n        if not isinstance(loc, int):\n            raise TypeError(\"loc must be int\")\n\n        # insert to the axis; this could possibly raise a TypeError\n        new_axis = self.items.insert(loc, item)\n\n        block = make_block(values=value, ndim=self.ndim,\n                           placement=slice(loc, loc + 1))\n\n        for blkno, count in _fast_count_smallints(self._blknos[loc:]):\n            blk = self.blocks[blkno]\n            if count == len(blk.mgr_locs):\n                blk.mgr_locs = blk.mgr_locs.add(1)\n            else:\n                new_mgr_locs = blk.mgr_locs.as_array.copy()\n                new_mgr_locs[new_mgr_locs >= loc] += 1\n                blk.mgr_locs = new_mgr_locs\n\n        if loc == self._blklocs.shape[0]:\n            # np.append is a lot faster, let's use it if we can.\n            self._blklocs = np.append(self._blklocs, 0)\n            self._blknos = np.append(self._blknos, len(self.blocks))\n        else:\n            self._blklocs = np.insert(self._blklocs, loc, 0)\n            self._blknos = np.insert(self._blknos, loc, len(self.blocks))\n\n        self.axes[0] = new_axis\n        self.blocks += (block,)\n        self._shape = None\n\n        self._known_consolidated = False\n\n        if len(self.blocks) > 100:\n            self._consolidate_inplace()", "output": "Insert item at selected position.\n\n        Parameters\n        ----------\n        loc : int\n        item : hashable\n        value : array_like\n        allow_duplicates: bool\n            If False, trying to insert non-unique item will raise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def categorical_case(pmf, fns, rand=None):\n  \"\"\"\n  \"\"\"\n  rand = tf.random_uniform([]) if rand is None else rand\n  cmf = tf.pad(tf.cumsum(pmf), [(1, 0)])\n  cmf = [cmf[i] for i in range(len(fns) + 1)]\n  preds = [(rand >= a) & (rand < b) for a, b in zip(cmf[:-1], cmf[1:])]\n  return tf.case(list(zip(preds, fns)), exclusive=True)", "output": "Returns the outputs of fns[i] with probability pmf[i].\n\n  Args:\n    pmf: A 1-D tensor of probabilities, the probability mass function.\n    fns: A list of callables that return tensors, same length as pmf.\n    rand: An optional scalar between 0.0 and 1.0, the output of an RNG.\n\n  Returns:\n    A tensor, the output of fns[i] with probability pmf[i].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def login_exists(login, domain='', **kwargs):\n    '''\n    \n    '''\n    if domain:\n        login = '{0}\\\\{1}'.format(domain, login)\n    try:\n        # We should get one, and only one row\n        return len(tsql_query(query=\"SELECT name FROM sys.syslogins WHERE name='{0}'\".format(login), **kwargs)) == 1\n\n    except Exception as e:\n        return 'Could not find the login: {0}'.format(e)", "output": "Find if a login exists in the MS SQL server.\n    domain, if provided, will be prepended to login\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minion mssql.login_exists 'LOGIN'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def service_add(service_rootid=None, service_name=None, triggerid=None, **kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'service.create'\n            params = {'name': service_name}\n            params = _params_extend(params, _ignore_name=True, **kwargs)\n            # Ensure that we have required params.\n            params.setdefault('algorithm', 1)\n            params.setdefault('showsla', 1)\n            params.setdefault('goodsla', 99.7)\n            params.setdefault('sortorder', 1)\n            if service_rootid:\n                params.setdefault('parentid', service_rootid)\n            if triggerid:\n                params.setdefault('triggerid', triggerid)\n\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result'] if ret['result'] else False\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": ".. versionadded:: Fluorine\n\n    Create service under service with id specified as parameter.\n\n    .. note::\n        https://www.zabbix.com/documentation/3.4/manual/api/reference/service/create\n\n    :param service_rootid: Service id under which service should be added\n    :param service_name: Name of new service\n    :param triggerid: Optional - ID of trigger which should be watched in service\n\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    :return: Service details, False if service could not be added or on failure.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.service_add 11 'My service' 11111", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def weights_multi_problem(labels, taskid=-1):\n  \"\"\"\n  \"\"\"\n  taskid = check_nonnegative(taskid)\n  past_taskid = tf.cumsum(to_float(tf.equal(labels, taskid)), axis=1)\n  # Additionally zero out the task id location\n  past_taskid *= to_float(tf.not_equal(labels, taskid))\n  non_taskid = to_float(labels)\n  return to_float(tf.not_equal(past_taskid * non_taskid, 0))", "output": "Assign weight 1.0 to only the \"targets\" portion of the labels.\n\n  Weight 1.0 is assigned to all labels past the taskid.\n\n  Args:\n    labels: A Tensor of int32s.\n    taskid: an int32 representing the task id for a problem.\n\n  Returns:\n    A Tensor of floats.\n\n  Raises:\n    ValueError: The Task ID must be valid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_examples(self, data_file):\n    \"\"\"\"\"\"\n    with tf.io.gfile.GFile(data_file) as f:\n      reader = csv.DictReader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n      for row in reader:\n        # Everything in the row except for 'talk_name' will be a translation.\n        # Missing/incomplete translations will contain the string \"__NULL__\" or\n        # \"_ _ NULL _ _\".\n        yield {\n            'translations': {\n                lang: text\n                for lang, text in six.iteritems(row)\n                if lang != 'talk_name' and _is_translation_complete(text)\n            },\n            'talk_name': row['talk_name']\n        }", "output": "This function returns the examples in the raw (text) form.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def normalized(self, document, schema=None, always_return_document=False):\n        \"\"\" \n        \"\"\"\n        self.__init_processing(document, schema)\n        self.__normalize_mapping(self.document, self.schema)\n        self.error_handler.end(self)\n        if self._errors and not always_return_document:\n            return None\n        else:\n            return self.document", "output": "Returns the document normalized according to the specified rules\n        of a schema.\n\n        :param document: The document to normalize.\n        :type document: any :term:`mapping`\n        :param schema: The validation schema. Defaults to :obj:`None`. If not\n                       provided here, the schema must have been provided at\n                       class instantiation.\n        :type schema: any :term:`mapping`\n        :param always_return_document: Return the document, even if an error\n                                       occurred. Defaults to: ``False``.\n        :type always_return_document: :class:`bool`\n        :return: A normalized copy of the provided mapping or :obj:`None` if an\n                 error occurred during normalization.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _interpolate(self, method=None, index=None, values=None,\n                     fill_value=None, axis=0, limit=None,\n                     limit_direction='forward', limit_area=None,\n                     inplace=False, downcast=None, **kwargs):\n        \"\"\"  \"\"\"\n\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        data = self.values if inplace else self.values.copy()\n\n        # only deal with floats\n        if not self.is_float:\n            if not self.is_integer:\n                return self\n            data = data.astype(np.float64)\n\n        if fill_value is None:\n            fill_value = self.fill_value\n\n        if method in ('krogh', 'piecewise_polynomial', 'pchip'):\n            if not index.is_monotonic:\n                raise ValueError(\"{0} interpolation requires that the \"\n                                 \"index be monotonic.\".format(method))\n        # process 1-d slices in the axis direction\n\n        def func(x):\n\n            # process a 1-d slice, returning it\n            # should the axis argument be handled below in apply_along_axis?\n            # i.e. not an arg to missing.interpolate_1d\n            return missing.interpolate_1d(index, x, method=method, limit=limit,\n                                          limit_direction=limit_direction,\n                                          limit_area=limit_area,\n                                          fill_value=fill_value,\n                                          bounds_error=False, **kwargs)\n\n        # interp each column independently\n        interp_values = np.apply_along_axis(func, axis, data)\n\n        blocks = [self.make_block_same_class(interp_values)]\n        return self._maybe_downcast(blocks, downcast)", "output": "interpolate using scipy wrappers", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def saveAsPickleFile(self, path, batchSize=10):\n        \"\"\"\n        \n        \"\"\"\n        if batchSize == 0:\n            ser = AutoBatchedSerializer(PickleSerializer())\n        else:\n            ser = BatchedSerializer(PickleSerializer(), batchSize)\n        self._reserialize(ser)._jrdd.saveAsObjectFile(path)", "output": "Save this RDD as a SequenceFile of serialized objects. The serializer\n        used is L{pyspark.serializers.PickleSerializer}, default batch size\n        is 10.\n\n        >>> tmpFile = NamedTemporaryFile(delete=True)\n        >>> tmpFile.close()\n        >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n        >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n        ['1', '2', 'rdd', 'spark']", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def selectExpr(self, *expr):\n        \"\"\"\n        \"\"\"\n        if len(expr) == 1 and isinstance(expr[0], list):\n            expr = expr[0]\n        jdf = self._jdf.selectExpr(self._jseq(expr))\n        return DataFrame(jdf, self.sql_ctx)", "output": "Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n\n        This is a variant of :func:`select` that accepts SQL expressions.\n\n        >>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\n        [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def opened_files_list_changed(self):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        # Refresh Python file dependent actions:\r\n        editor = self.get_current_editor()\r\n        if editor:\r\n            python_enable = editor.is_python()\r\n            cython_enable = python_enable or (\r\n                programs.is_module_installed('Cython') and editor.is_cython())\r\n            for action in self.pythonfile_dependent_actions:\r\n                if action in self.cythonfile_compatible_actions:\r\n                    enable = cython_enable\r\n                else:\r\n                    enable = python_enable\r\n                if action is self.winpdb_action:\r\n                    action.setEnabled(enable and WINPDB_PATH is not None)\r\n                else:\r\n                    action.setEnabled(enable)\r\n            self.open_file_update.emit(self.get_current_filename())", "output": "Opened files list has changed:\r\n        --> open/close file action\r\n        --> modification ('*' added to title)\r\n        --> current edited file has changed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_ips(linode_id=None):\n    '''\n    \n    '''\n    if linode_id:\n        ips = _query('linode', 'ip.list', args={'LinodeID': linode_id})\n    else:\n        ips = _query('linode', 'ip.list')\n\n    ips = ips['DATA']\n    ret = {}\n\n    for item in ips:\n        node_id = six.text_type(item['LINODEID'])\n        if item['ISPUBLIC'] == 1:\n            key = 'public_ips'\n        else:\n            key = 'private_ips'\n\n        if ret.get(node_id) is None:\n            ret.update({node_id: {'public_ips': [], 'private_ips': []}})\n        ret[node_id][key].append(item['IPADDRESS'])\n\n    # If linode_id was specified, only return the ips, and not the\n    # dictionary based on the linode ID as a key.\n    if linode_id:\n        _all_ips = {'public_ips': [], 'private_ips': []}\n        matching_id = ret.get(six.text_type(linode_id))\n        if matching_id:\n            _all_ips['private_ips'] = matching_id['private_ips']\n            _all_ips['public_ips'] = matching_id['public_ips']\n\n        ret = _all_ips\n\n    return ret", "output": "Returns public and private IP addresses.\n\n    linode_id\n        Limits the IP addresses returned to the specified Linode ID.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def filter_commands(self, commands, *, sort=False, key=None):\n        \"\"\"\n        \"\"\"\n\n        if sort and key is None:\n            key = lambda c: c.name\n\n        iterator = commands if self.show_hidden else filter(lambda c: not c.hidden, commands)\n\n        if not self.verify_checks:\n            # if we do not need to verify the checks then we can just\n            # run it straight through normally without using await.\n            return sorted(iterator, key=key) if sort else list(iterator)\n\n        # if we're here then we need to check every command if it can run\n        async def predicate(cmd):\n            try:\n                return await cmd.can_run(self.context)\n            except CommandError:\n                return False\n\n        ret = []\n        for cmd in iterator:\n            valid = await predicate(cmd)\n            if valid:\n                ret.append(cmd)\n\n        if sort:\n            ret.sort(key=key)\n        return ret", "output": "|coro|\n\n        Returns a filtered list of commands and optionally sorts them.\n\n        This takes into account the :attr:`verify_checks` and :attr:`show_hidden`\n        attributes.\n\n        Parameters\n        ------------\n        commands: Iterable[:class:`Command`]\n            An iterable of commands that are getting filtered.\n        sort: :class:`bool`\n            Whether to sort the result.\n        key: Optional[Callable[:class:`Command`, Any]]\n            An optional key function to pass to :func:`py:sorted` that\n            takes a :class:`Command` as its sole parameter. If ``sort`` is\n            passed as ``True`` then this will default as the command name.\n\n        Returns\n        ---------\n        List[:class:`Command`]\n            A list of commands that passed the filter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_util_is_trade(date, code, client):\n    \"\"\"\n    \n    \"\"\"\n    coll = client.quantaxis.stock_day\n    date = str(date)[0:10]\n    is_trade = coll.find_one({'code': code, 'date': date})\n    try:\n        len(is_trade)\n        return True\n    except:\n        return False", "output": "\u5224\u65ad\u662f\u5426\u662f\u4ea4\u6613\u65e5\n    \u4ece\u6570\u636e\u5e93\u4e2d\u67e5\u8be2\n    :param date: str\u7c7b\u578b -- 1999-12-11 \u8fd9\u79cd\u683c\u5f0f    10\u4f4d\u5b57\u7b26\u4e32\n    :param code: str\u7c7b\u578b -- \u80a1\u7968\u4ee3\u7801 \u4f8b\u5982 603658 \uff0c 6\u4f4d\u5b57\u7b26\u4e32\n    :param client: pymongo.MongoClient\u7c7b\u578b    -- mongodb \u6570\u636e\u5e93 \u4ece QA_util_sql_mongo_setting \u4e2d QA_util_sql_mongo_setting \u83b7\u53d6\n    :return:  Boolean -- \u662f\u5426\u662f\u4ea4\u6613\u65f6\u95f4", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _infer_attrs(self, infer_fn, attr, *args):\n        \"\"\"\"\"\"\n        inputs, out = self._get_graph(*args)\n        args, _ = _flatten(args, \"input\")\n        with warnings.catch_warnings(record=True) as w:\n            arg_attrs, _, aux_attrs = getattr(out, infer_fn)(\n                **{i.name: getattr(j, attr) for i, j in zip(inputs, args)})\n            if arg_attrs is None:\n                raise ValueError(w[0].message)\n        sdict = {i: j for i, j in zip(out.list_arguments(), arg_attrs)}\n        sdict.update({name : attr for name, attr in \\\n             zip(out.list_auxiliary_states(), aux_attrs)})\n        for i in self.collect_params().values():\n            setattr(i, attr, sdict[i.name])", "output": "Generic infer attributes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _pca(x, k=2):\n    \"\"\n    x = x-torch.mean(x,0)\n    U,S,V = torch.svd(x.t())\n    return torch.mm(x,U[:,:k])", "output": "Compute PCA of `x` with `k` dimensions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _collect_data(directory, input_ext, transcription_ext):\n  \"\"\"\"\"\"\n  # Directory from string to tuple pair of strings\n  # key: the filepath to a datafile including the datafile's basename. Example,\n  #   if the datafile was \"/path/to/datafile.wav\" then the key would be\n  #   \"/path/to/datafile\"\n  # value: a pair of strings (media_filepath, label)\n  data_files = {}\n  for root, _, filenames in os.walk(directory):\n    transcripts = [filename for filename in filenames\n                   if transcription_ext in filename]\n    for transcript in transcripts:\n      transcript_path = os.path.join(root, transcript)\n      with open(transcript_path, \"r\") as transcript_file:\n        for transcript_line in transcript_file:\n          line_contents = transcript_line.strip().split(\" \", 1)\n          media_base, label = line_contents\n          key = os.path.join(root, media_base)\n          assert key not in data_files\n          media_name = \"%s.%s\"%(media_base, input_ext)\n          media_path = os.path.join(root, media_name)\n          data_files[key] = (media_base, media_path, label)\n  return data_files", "output": "Traverses directory collecting input and target files.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        \"\"\"\n        \"\"\"\n        for line in iter(self.pipeReader.readline, ''):\n            self.orig_stdout.write(line.rstrip() + '\\n')\n            self.orig_stdout.flush()\n            if self.log_collection == 'none':\n                # If not match metrics, do not put the line into queue\n                if not self.log_pattern.match(line):\n                    continue\n            self.queue.put(line)\n            \n        self.pipeReader.close()", "output": "Run the thread, logging everything.\n           If the log_collection is 'none', the log content will not be enqueued", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def closeEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        self.threadmanager.close_all_threads()\r\n        self.analysis_timer.timeout.disconnect(self.analyze_script)\r\n\r\n        # Remove editor references from the outline explorer settings\r\n        if self.outlineexplorer is not None:\r\n            for finfo in self.data:\r\n                self.outlineexplorer.remove_editor(finfo.editor.oe_proxy)\r\n\r\n        QWidget.closeEvent(self, event)", "output": "Overrides QWidget closeEvent().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deployment_operations_list(name, resource_group, result_limit=10, **kwargs):\n    '''\n    \n\n    '''\n    result = {}\n    resconn = __utils__['azurearm.get_client']('resource', **kwargs)\n    try:\n        operations = __utils__['azurearm.paged_object_to_list'](\n            resconn.deployment_operations.list(\n                resource_group_name=resource_group,\n                deployment_name=name,\n                top=result_limit\n            )\n        )\n\n        for oper in operations:\n            result[oper['operation_id']] = oper\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('resource', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    List all deployment operations within a deployment.\n\n    :param name: The name of the deployment to query.\n\n    :param resource_group: The resource group name assigned to the\n        deployment.\n\n    :param result_limit: (Default: 10) The limit on the list of deployment\n        operations.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_resource.deployment_operations_list testdeploy testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def label_const(self, const:Any=0, label_cls:Callable=None, **kwargs)->'LabelList':\n        \"\"\n        return self.label_from_func(func=lambda o: const, label_cls=label_cls, **kwargs)", "output": "Label every item with `const`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_features(self, data):\n        \"\"\"\n        \n        \"\"\"\n        if self.feature_names is None:\n            self.feature_names = data.feature_names\n            self.feature_types = data.feature_types\n        else:\n            # Booster can't accept data with different feature names\n            if self.feature_names != data.feature_names:\n                msg = 'feature_names mismatch: {0} {1}'\n                raise ValueError(msg.format(self.feature_names,\n                                            data.feature_names))", "output": "Validate Booster and data's feature_names are identical.\n        Set feature_names and feature_types from DMatrix", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_period_arraylike(arr):\n    \"\"\"\n    \n    \"\"\"\n\n    if isinstance(arr, (ABCPeriodIndex, ABCPeriodArray)):\n        return True\n    elif isinstance(arr, (np.ndarray, ABCSeries)):\n        return is_period_dtype(arr.dtype)\n    return getattr(arr, 'inferred_type', None) == 'period'", "output": "Check whether an array-like is a periodical array-like or PeriodIndex.\n\n    Parameters\n    ----------\n    arr : array-like\n        The array-like to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array-like is a periodical array-like or\n        PeriodIndex instance.\n\n    Examples\n    --------\n    >>> is_period_arraylike([1, 2, 3])\n    False\n    >>> is_period_arraylike(pd.Index([1, 2, 3]))\n    False\n    >>> is_period_arraylike(pd.PeriodIndex([\"2017-01-01\"], freq=\"D\"))\n    True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _spill(self):\n        \"\"\"  \"\"\"\n        global MemoryBytesSpilled, DiskBytesSpilled\n        if self._file is None:\n            self._open_file()\n\n        used_memory = get_used_memory()\n        pos = self._file.tell()\n        self._ser.dump_stream(self.values, self._file)\n        self.values = []\n        gc.collect()\n        DiskBytesSpilled += self._file.tell() - pos\n        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20", "output": "dump the values into disk", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_command(command, broken, matched):\n    \"\"\"\"\"\"\n    new_cmds = get_close_matches(broken, matched, cutoff=0.1)\n    return [replace_argument(command.script, broken, new_cmd.strip())\n            for new_cmd in new_cmds]", "output": "Helper for *_no_command rules.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def schema_of_csv(csv, options={}):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(csv, basestring):\n        col = _create_column_from_literal(csv)\n    elif isinstance(csv, Column):\n        col = _to_java_column(csv)\n    else:\n        raise TypeError(\"schema argument should be a column or string\")\n\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.schema_of_csv(col, options)\n    return Column(jc)", "output": "Parses a CSV string and infers its schema in DDL format.\n\n    :param col: a CSV string or a string literal containing a CSV string.\n    :param options: options to control parsing. accepts the same options as the CSV datasource\n\n    >>> df = spark.range(1)\n    >>> df.select(schema_of_csv(lit('1|a'), {'sep':'|'}).alias(\"csv\")).collect()\n    [Row(csv=u'struct<_c0:int,_c1:string>')]\n    >>> df.select(schema_of_csv('1|a', {'sep':'|'}).alias(\"csv\")).collect()\n    [Row(csv=u'struct<_c0:int,_c1:string>')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_external_data_tensors(model, filepath):  # type: (ModelProto, Text) -> ModelProto\n    \"\"\"\n    \n    \"\"\"\n    for tensor in _get_all_tensors(model):\n        if uses_external_data(tensor):\n            save_external_data(tensor, filepath)\n            tensor.ClearField(str('raw_data'))\n\n    return model", "output": "Write external data of all tensors to files on disk.\n\n    Note: This function also strips basepath information from all tensors' external_data fields.\n\n    @params\n    model: Model object which is the source of tensors to serialize.\n    filepath: System path to the directory which should be treated as base path for external data.\n\n    @return\n    The modified model object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace(key,\n            value,\n            host=DEFAULT_HOST,\n            port=DEFAULT_PORT,\n            time=DEFAULT_TIME,\n            min_compress_len=DEFAULT_MIN_COMPRESS_LEN):\n    '''\n    \n    '''\n    if not isinstance(time, six.integer_types):\n        raise SaltInvocationError('\\'time\\' must be an integer')\n    if not isinstance(min_compress_len, six.integer_types):\n        raise SaltInvocationError('\\'min_compress_len\\' must be an integer')\n    conn = _connect(host, port)\n    stats = conn.get_stats()\n    return conn.replace(\n        key,\n        value,\n        time=time,\n        min_compress_len=min_compress_len\n    )", "output": "Replace a key on the memcached server. This only succeeds if the key\n    already exists. This is the opposite of :mod:`memcached.add\n    <salt.modules.memcached.add>`\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' memcached.replace <key> <value>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def accept_key(pki_dir, pub, id_):\n    '''\n    \n    '''\n    for key_dir in 'minions', 'minions_pre', 'minions_rejected':\n        key_path = os.path.join(pki_dir, key_dir)\n        if not os.path.exists(key_path):\n            os.makedirs(key_path)\n\n    key = os.path.join(pki_dir, 'minions', id_)\n    with salt.utils.files.fopen(key, 'w+') as fp_:\n        fp_.write(salt.utils.stringutils.to_str(pub))\n\n    oldkey = os.path.join(pki_dir, 'minions_pre', id_)\n    if os.path.isfile(oldkey):\n        with salt.utils.files.fopen(oldkey) as fp_:\n            if fp_.read() == pub:\n                os.remove(oldkey)", "output": "If the master config was available then we will have a pki_dir key in\n    the opts directory, this method places the pub key in the accepted\n    keys dir and removes it from the unaccepted keys dir if that is the case.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, resource, zone):\n        \"\"\"\n        \"\"\"\n        changes = cls(zone=zone)\n        changes._set_properties(resource)\n        return changes", "output": "Factory:  construct a change set given its API representation\n\n        :type resource: dict\n        :param resource: change set representation returned from the API.\n\n        :type zone: :class:`google.cloud.dns.zone.ManagedZone`\n        :param zone: A zone which holds zero or more change sets.\n\n        :rtype: :class:`google.cloud.dns.changes.Changes`\n        :returns: RRS parsed from ``resource``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_ids(cls, path:PathOrStr, vocab:Vocab, train_ids:Collection[Collection[int]], valid_ids:Collection[Collection[int]],\n                 test_ids:Collection[Collection[int]]=None, train_lbls:Collection[Union[int,float]]=None,\n                 valid_lbls:Collection[Union[int,float]]=None, classes:Collection[Any]=None,\n                 processor:PreProcessor=None, **kwargs) -> DataBunch:\n        \"\"\n        src = ItemLists(path, TextList(train_ids, vocab, path=path, processor=[]),\n                        TextList(valid_ids, vocab, path=path, processor=[]))\n        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_lists(train_lbls, valid_lbls, classes=classes, processor=[])\n        if not is1d(train_lbls): src.train.y.one_hot,src.valid.y.one_hot = True,True\n        if test_ids is not None: src.add_test(TextList(test_ids, vocab, path=path), label=train_lbls[0])\n        src.valid.x.processor = ifnone(processor, [TokenizeProcessor(), NumericalizeProcessor(vocab=vocab)])\n        return src.databunch(**kwargs)", "output": "Create a `TextDataBunch` from ids, labels and a `vocab`. `kwargs` are passed to the dataloader creation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def argsort(self, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        result = self.asi8\n        if result is None:\n            result = np.array(self)\n        return result.argsort(*args, **kwargs)", "output": "Return the integer indices that would sort the index.\n\n        Parameters\n        ----------\n        *args\n            Passed to `numpy.ndarray.argsort`.\n        **kwargs\n            Passed to `numpy.ndarray.argsort`.\n\n        Returns\n        -------\n        numpy.ndarray\n            Integer indices that would sort the index if used as\n            an indexer.\n\n        See Also\n        --------\n        numpy.argsort : Similar method for NumPy arrays.\n        Index.sort_values : Return sorted copy of Index.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['b', 'a', 'd', 'c'])\n        >>> idx\n        Index(['b', 'a', 'd', 'c'], dtype='object')\n\n        >>> order = idx.argsort()\n        >>> order\n        array([1, 0, 3, 2])\n\n        >>> idx[order]\n        Index(['a', 'b', 'c', 'd'], dtype='object')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _TryPrintAsAnyMessage(self, message):\n    \"\"\"\"\"\"\n    packed_message = _BuildMessageFromTypeName(message.TypeName(),\n                                               self.descriptor_pool)\n    if packed_message:\n      packed_message.MergeFromString(message.value)\n      self.out.write('%s[%s]' % (self.indent * ' ', message.type_url))\n      self._PrintMessageFieldValue(packed_message)\n      self.out.write(' ' if self.as_one_line else '\\n')\n      return True\n    else:\n      return False", "output": "Serializes if message is a google.protobuf.Any field.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def findSynonyms(self, word, num):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(word, basestring):\n            word = _convert_to_vector(word)\n        return self._call_java(\"findSynonyms\", word, num)", "output": "Find \"num\" number of words closest in similarity to \"word\".\n        word can be a string or vector representation.\n        Returns a dataframe with two fields word and similarity (which\n        gives the cosine similarity).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_decorators(func, decorators):\n    \"\"\"\n    \"\"\"\n    decorators = filter(_is_not_none_or_false, reversed(decorators))\n\n    for decorator in decorators:\n        func = decorator(func)\n\n    return func", "output": "Apply a list of decorators to a given function.\n\n    ``decorators`` may contain items that are ``None`` or ``False`` which will\n    be ignored.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def feed(self, data):\n        \"\"\"\n        \n        \"\"\"\n        self._lock.acquire()\n        try:\n            if self._event is not None:\n                self._event.set()\n            self._buffer_frombytes(b(data))\n            self._cv.notifyAll()\n        finally:\n            self._lock.release()", "output": "Feed new data into this pipe.  This method is assumed to be called\n        from a separate thread, so synchronization is done.\n\n        :param data: the data to add, as a ``str`` or ``bytes``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def summarize_features(features, num_shards=1):\n  \"\"\"\"\"\"\n  if not common_layers.should_generate_summaries():\n    return\n\n  with tf.name_scope(\"input_stats\"):\n    for (k, v) in sorted(six.iteritems(features)):\n      if (isinstance(v, tf.Tensor) and (v.get_shape().ndims > 1) and\n          (v.dtype != tf.string)):\n        tf.summary.scalar(\"%s_batch\" % k, tf.shape(v)[0] // num_shards)\n        tf.summary.scalar(\"%s_length\" % k, tf.shape(v)[1])\n        nonpadding = tf.to_float(tf.not_equal(v, 0))\n        nonpadding_tokens = tf.reduce_sum(nonpadding)\n        tf.summary.scalar(\"%s_nonpadding_tokens\" % k, nonpadding_tokens)\n        tf.summary.scalar(\"%s_nonpadding_fraction\" % k,\n                          tf.reduce_mean(nonpadding))", "output": "Generate summaries for features.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def invites(self):\n        \"\"\"\n        \"\"\"\n\n        state = self._state\n        data = await state.http.invites_from_channel(self.id)\n        result = []\n\n        for invite in data:\n            invite['channel'] = self\n            invite['guild'] = self.guild\n            result.append(Invite(state=state, data=invite))\n\n        return result", "output": "|coro|\n\n        Returns a list of all active instant invites from this channel.\n\n        You must have :attr:`~.Permissions.manage_guild` to get this information.\n\n        Raises\n        -------\n        Forbidden\n            You do not have proper permissions to get the information.\n        HTTPException\n            An error occurred while fetching the information.\n\n        Returns\n        -------\n        List[:class:`Invite`]\n            The list of invites that are currently active.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_scsi_devices(scsi_devices):\n    '''\n    \n    '''\n    keys = range(-1000, -1050, -1)\n    scsi_specs = []\n    if scsi_devices:\n        devs = [scsi['adapter'] for scsi in scsi_devices]\n        log.trace('Creating SCSI devices %s', devs)\n        # unitNumber for disk attachment, 0:0 1st 0 is the controller busNumber,\n        # 2nd is the unitNumber\n        for (key, scsi_controller) in zip(keys, scsi_devices):\n            # create the SCSI controller\n            scsi_spec = _apply_scsi_controller(scsi_controller['adapter'],\n                                               scsi_controller['type'],\n                                               scsi_controller['bus_sharing'],\n                                               key,\n                                               scsi_controller['bus_number'],\n                                               'add')\n            scsi_specs.append(scsi_spec)\n    return scsi_specs", "output": "Returns a list of vim.vm.device.VirtualDeviceSpec objects representing\n    SCSI controllers\n\n    scsi_devices:\n        List of SCSI device properties", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close(self) -> Awaitable[None]:\n        \"\"\"\"\"\"\n        for ev in self._throttle_dns_events.values():\n            ev.cancel()\n\n        return super().close()", "output": "Close all ongoing DNS calls.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def match_config(regex):\n    '''\n    \n    '''\n    if _TRAFFICCTL:\n        cmd = _traffic_ctl('config', 'match', regex)\n    else:\n        cmd = _traffic_line('-m', regex)\n\n    return _subprocess(cmd)", "output": "Display the current values of all configuration variables whose\n    names match the given regular expression.\n\n    .. versionadded:: 2016.11.0\n\n    .. code-block:: bash\n\n        salt '*' trafficserver.match_config regex", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tfms_from_stats(stats, sz, aug_tfms=None, max_zoom=None, pad=0, crop_type=CropType.RANDOM,\n                    tfm_y=None, sz_y=None, pad_mode=cv2.BORDER_REFLECT, norm_y=True, scale=None):\n    \"\"\" \n    \"\"\"\n    if aug_tfms is None: aug_tfms=[]\n    tfm_norm = Normalize(*stats, tfm_y=tfm_y if norm_y else TfmType.NO) if stats is not None else None\n    tfm_denorm = Denormalize(*stats) if stats is not None else None\n    val_crop = CropType.CENTER if crop_type in (CropType.RANDOM,CropType.GOOGLENET) else crop_type\n    val_tfm = image_gen(tfm_norm, tfm_denorm, sz, pad=pad, crop_type=val_crop,\n            tfm_y=tfm_y, sz_y=sz_y, scale=scale)\n    trn_tfm = image_gen(tfm_norm, tfm_denorm, sz, pad=pad, crop_type=crop_type,\n            tfm_y=tfm_y, sz_y=sz_y, tfms=aug_tfms, max_zoom=max_zoom, pad_mode=pad_mode, scale=scale)\n    return trn_tfm, val_tfm", "output": "Given the statistics of the training image sets, returns separate training and validation transform functions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_market_trade_type(self, ttype):\n        \"\"\"\"\"\"\n        selects = self._main.child_window(\n            control_id=self._config.TRADE_MARKET_TYPE_CONTROL_ID,\n            class_name=\"ComboBox\",\n        )\n        for i, text in selects.texts():\n            # skip 0 index, because 0 index is current select index\n            if i == 0:\n                continue\n            if ttype in text:\n                selects.select(i - 1)\n                break\n        else:\n            raise TypeError(\"\u4e0d\u652f\u6301\u5bf9\u5e94\u7684\u5e02\u4ef7\u7c7b\u578b: {}\".format(ttype))", "output": "\u6839\u636e\u9009\u62e9\u7684\u5e02\u4ef7\u4ea4\u6613\u7c7b\u578b\u9009\u62e9\u5bf9\u5e94\u7684\u4e0b\u62c9\u9009\u9879", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setVisible(self, value):\n        \"\"\"\"\"\"\n        if self.timer is not None:\n            if value:\n                self.timer.start(self._interval)\n            else:\n                self.timer.stop()\n        super(BaseTimerStatus, self).setVisible(value)", "output": "Override Qt method to stops timers if widget is not visible.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode_seq(self, inputs, states, valid_length=None):\n        \"\"\"\n        \"\"\"\n        outputs, states, additional_outputs =\\\n            self.decoder.decode_seq(inputs=self.tgt_embed(inputs),\n                                    states=states,\n                                    valid_length=valid_length)\n        outputs = self.tgt_proj(outputs)\n        return outputs, states, additional_outputs", "output": "Decode given the input sequence.\n\n        Parameters\n        ----------\n        inputs : NDArray\n        states : list of NDArrays\n        valid_length : NDArray or None, default None\n\n        Returns\n        -------\n        output : NDArray\n            The output of the decoder. Shape is (batch_size, length, tgt_word_num)\n        states: list\n            The new states of the decoder\n        additional_outputs : list\n            Additional outputs of the decoder, e.g, the attention weights", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_default_assets_zip_provider():\n  \"\"\"\n  \"\"\"\n  path = os.path.join(os.path.dirname(inspect.getfile(sys._getframe(1))),\n                      'webfiles.zip')\n  if not os.path.exists(path):\n    logger.warning('webfiles.zip static assets not found: %s', path)\n    return None\n  return lambda: open(path, 'rb')", "output": "Opens stock TensorBoard web assets collection.\n\n  Returns:\n    Returns function that returns a newly opened file handle to zip file\n    containing static assets for stock TensorBoard, or None if webfiles.zip\n    could not be found. The value the callback returns must be closed. The\n    paths inside the zip file are considered absolute paths on the web server.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_cursor_on_last_line(self):\r\n        \"\"\"\"\"\"\r\n        cursor = self.textCursor()\r\n        cursor.movePosition(QTextCursor.EndOfBlock)\r\n        return cursor.atEnd()", "output": "Return True if cursor is on the last line", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def AddLeafNodes(self, prefix, node):\n    \"\"\"\"\"\"\n    if not node:\n      self.AddPath(prefix)\n    for name in node:\n      child_path = prefix + '.' + name\n      self.AddLeafNodes(child_path, node[name])", "output": "Adds leaf nodes begin with prefix to this tree.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def newTextLen(content, len):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlNewTextLen(content, len)\n    if ret is None:raise treeError('xmlNewTextLen() failed')\n    return xmlNode(_obj=ret)", "output": "Creation of a new text node with an extra parameter for the\n       content's length", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def num_samples(self, dataset_split):\n    \"\"\"\n    \"\"\"\n    return {\n        problem.DatasetSplit.TRAIN: 1000000,\n        problem.DatasetSplit.EVAL: 10000,\n        problem.DatasetSplit.TEST: 10000\n    }[dataset_split]", "output": "Determine the dataset sized given a dataset_split.\n\n    Args:\n      dataset_split: A problem.DatasetSplit.\n\n    Returns:\n      The desired number of samples for this dataset_split.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pod_absent(name, namespace='default', **kwargs):\n    '''\n    \n    '''\n\n    ret = {'name': name,\n           'changes': {},\n           'result': False,\n           'comment': ''}\n\n    pod = __salt__['kubernetes.show_pod'](name, namespace, **kwargs)\n\n    if pod is None:\n        ret['result'] = True if not __opts__['test'] else None\n        ret['comment'] = 'The pod does not exist'\n        return ret\n\n    if __opts__['test']:\n        ret['comment'] = 'The pod is going to be deleted'\n        ret['result'] = None\n        return ret\n\n    res = __salt__['kubernetes.delete_pod'](name, namespace, **kwargs)\n    if res['code'] == 200 or res['code'] is None:\n        ret['result'] = True\n        ret['changes'] = {\n            'kubernetes.pod': {\n                'new': 'absent', 'old': 'present'}}\n        if res['code'] is None:\n            ret['comment'] = 'In progress'\n        else:\n            ret['comment'] = res['message']\n    else:\n        ret['comment'] = 'Something went wrong, response: {0}'.format(res)\n\n    return ret", "output": "Ensures that the named pod is absent from the given namespace.\n\n    name\n        The name of the pod\n\n    namespace\n        The name of the namespace", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_as_underlined(self, color=Qt.blue):\n        \"\"\"\n        \n        \"\"\"\n        self.format.setUnderlineStyle(\n            QTextCharFormat.SingleUnderline)\n        self.format.setUnderlineColor(color)", "output": "Underlines the text.\n\n        :param color: underline color.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render_search_template(self, id=None, body=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_render\", \"template\", id), params=params, body=body\n        )", "output": "`<http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-template.html>`_\n\n        :arg id: The id of the stored search template\n        :arg body: The search definition template and its params", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(setname=None, entry=None, family='ipv4', **kwargs):\n    '''\n    \n\n    '''\n    if not setname:\n        return 'Error: Set needs to be specified'\n    if not entry:\n        return 'Error: Entry needs to be specified'\n\n    setinfo = _find_set_info(setname)\n    if not setinfo:\n        return 'Error: Set {0} does not exist'.format(setname)\n\n    settype = setinfo['Type']\n\n    cmd = '{0}'.format(entry)\n\n    if 'timeout' in kwargs:\n        if 'timeout' not in setinfo['Header']:\n            return 'Error: Set {0} not created with timeout support'.format(setname)\n\n    if 'packets' in kwargs or 'bytes' in kwargs:\n        if 'counters' not in setinfo['Header']:\n            return 'Error: Set {0} not created with counters support'.format(setname)\n\n    if 'comment' in kwargs:\n        if 'comment' not in setinfo['Header']:\n            return 'Error: Set {0} not created with comment support'.format(setname)\n        if 'comment' not in entry:\n            cmd = '{0} comment \"{1}\"'.format(cmd, kwargs['comment'])\n\n    if set(['skbmark', 'skbprio', 'skbqueue']) & set(kwargs):\n        if 'skbinfo' not in setinfo['Header']:\n            return 'Error: Set {0} not created with skbinfo support'.format(setname)\n\n    for item in _ADD_OPTIONS[settype]:\n        if item in kwargs:\n            cmd = '{0} {1} {2}'.format(cmd, item, kwargs[item])\n\n    current_members = _find_set_members(setname)\n    if cmd in current_members:\n        return 'Warn: Entry {0} already exists in set {1}'.format(cmd, setname)\n\n    # Using -exist to ensure entries are updated if the comment changes\n    cmd = '{0} add -exist {1} {2}'.format(_ipset_cmd(), setname, cmd)\n    out = __salt__['cmd.run'](cmd, python_shell=False)\n\n    if not out:\n        return 'Success'\n    return 'Error: {0}'.format(out)", "output": "Append an entry to the specified set.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ipset.add setname 192.168.1.26\n\n        salt '*' ipset.add setname 192.168.0.3,AA:BB:CC:DD:EE:FF", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compress_encoder(inputs,\n                     hparams,\n                     strides=(2, 2),\n                     kernel_size=(3, 3),\n                     name=None):\n  \"\"\"\n  \"\"\"\n  with tf.variable_scope(name, default_name=\"compress\"):\n    x = inputs\n    for i in range(hparams.num_compress_steps // 2):\n      with tf.variable_scope(\"compress_conv_%d\" % i):\n        y = common_layers.conv_block(\n            common_layers.layer_norm(\n                x, hparams.hidden_size, name=\"lnorm\"),\n            hparams.hidden_size,\n            dilation_rates_and_kernel_sizes=[((1, 1), kernel_size)],\n            strides=strides,\n            padding=\"SAME\",\n            name=\"compress_conv_%d\" % i)\n        y = tf.nn.dropout(y, 1.0 - hparams.dropout)\n        if hparams.do_compress_attend:\n          y = compress_self_attention_layer(\n              x, hparams, name=\"compress_selfatt_%d\" % i)\n          y += x\n        x = y\n\n    x = residual_block_layer(x, hparams)\n\n    # If using multiple copies of latents, blow up the hidden size and then\n    # reshape to increase by num_latents.\n    shape_x = common_layers.shape_list(x)\n    x = tf.layers.dense(x,\n                        hparams.num_latents * hparams.hidden_size,\n                        name=name + \"_dense\")\n    return tf.reshape(x, [shape_x[0],\n                          shape_x[1] * shape_x[2] * hparams.num_latents,\n                          hparams.hidden_size])", "output": "Encoder that compresses 2-D inputs by 2**num_compress_steps.\n\n  Args:\n    inputs: Tensor of shape [batch, height, width, channels].\n    hparams: HParams.\n    strides: Tuple, strides for conv block.\n    kernel_size: Tuple, kernel window size for conv block.\n    name: string, variable scope.\n\n  Returns:\n    Tensor of shape [batch, latent_length, hparams.hidden_size], where\n      latent_length is\n      hparams.num_latents * (height*width) / 2**(hparams.num_compress_steps).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unescape(self):\n        \"\"\"\n        \"\"\"\n        from ._constants import HTML_ENTITIES\n\n        def handle_match(m):\n            name = m.group(1)\n            if name in HTML_ENTITIES:\n                return unichr(HTML_ENTITIES[name])\n            try:\n                if name[:2] in (\"#x\", \"#X\"):\n                    return unichr(int(name[2:], 16))\n                elif name.startswith(\"#\"):\n                    return unichr(int(name[1:]))\n            except ValueError:\n                pass\n            # Don't modify unexpected input.\n            return m.group()\n\n        return _entity_re.sub(handle_match, text_type(self))", "output": "Convert escaped markup back into a text string. This replaces\n        HTML entities with the characters they represent.\n\n        >>> Markup('Main &raquo; <em>About</em>').unescape()\n        'Main \u00bb <em>About</em>'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_quickref(self):\r\n        \"\"\"\"\"\"\r\n        from IPython.core.usage import quick_reference\r\n        self.main.help.show_plain_text(quick_reference)", "output": "Show IPython Cheat Sheet", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_restart_freeze():\n    '''\n    \n    '''\n    ret = salt.utils.mac_utils.execute_return_result(\n        'systemsetup -getrestartfreeze')\n    return salt.utils.mac_utils.validate_enabled(\n        salt.utils.mac_utils.parse_return(ret)) == 'on'", "output": "Displays whether 'restart on freeze' is on or off if supported\n\n    :return: A string value representing the \"restart on freeze\" settings\n    :rtype: string\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' power.get_restart_freeze", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def maybe_convert_ix(*args):\n    \"\"\"\n    \n    \"\"\"\n\n    ixify = True\n    for arg in args:\n        if not isinstance(arg, (np.ndarray, list, ABCSeries, Index)):\n            ixify = False\n\n    if ixify:\n        return np.ix_(*args)\n    else:\n        return args", "output": "We likely want to take the cross-product", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def link_type(arg_type, arg_name=None, include_bt:bool=True):\n    \"\"\n    arg_name = arg_name or fn_name(arg_type)\n    if include_bt: arg_name = code_esc(arg_name)\n    if belongs_to_module(arg_type, 'torch') and ('Tensor' not in arg_name): return f'[{arg_name}]({get_pytorch_link(arg_type)})'\n    if is_fastai_class(arg_type): return f'[{arg_name}]({get_fn_link(arg_type)})'\n    return arg_name", "output": "Create link to documentation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_flat_path(path_pb):\n    \"\"\"\n    \"\"\"\n    num_elts = len(path_pb.element)\n    last_index = num_elts - 1\n\n    result = []\n    for index, element in enumerate(path_pb.element):\n        result.append(element.type)\n        _add_id_or_name(result, element, index == last_index)\n\n    return tuple(result)", "output": "Convert a legacy \"Path\" protobuf to a flat path.\n\n    For example\n\n        Element {\n          type: \"parent\"\n          id: 59\n        }\n        Element {\n          type: \"child\"\n          name: \"naem\"\n        }\n\n    would convert to ``('parent', 59, 'child', 'naem')``.\n\n    :type path_pb: :class:`._app_engine_key_pb2.Path`\n    :param path_pb: Legacy protobuf \"Path\" object (from a \"Reference\").\n\n    :rtype: tuple\n    :returns: The path parts from ``path_pb``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tabbed_parsing_token_generator(data_dir, tmp_dir, train, prefix,\n                                   source_vocab_size, target_vocab_size):\n  \"\"\"\"\"\"\n  filename = \"parsing_{0}.pairs\".format(\"train\" if train else \"dev\")\n  source_vocab = generator_utils.get_or_generate_tabbed_vocab(\n      data_dir, tmp_dir, filename, 0,\n      prefix + \"_source.tokens.vocab.%d\" % source_vocab_size, source_vocab_size)\n  target_vocab = generator_utils.get_or_generate_tabbed_vocab(\n      data_dir, tmp_dir, filename, 1,\n      prefix + \"_target.tokens.vocab.%d\" % target_vocab_size, target_vocab_size)\n  pair_filepath = os.path.join(tmp_dir, filename)\n  return text_problems.text2text_generate_encoded(\n      text_problems.text2text_txt_tab_iterator(pair_filepath), source_vocab,\n      target_vocab)", "output": "Generate source and target data from a single file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_big():\n  \"\"\"\"\"\"\n  hparams = transformer_base()\n  hparams.hidden_size = 1024\n  hparams.filter_size = 4096\n  # Reduce batch size to 2048 from 4096 to be able to train the model on a GPU\n  # with 12 GB memory. For example, NVIDIA TITAN V GPU.\n  hparams.batch_size = 2048\n  hparams.num_heads = 16\n  hparams.layer_prepostprocess_dropout = 0.3\n  return hparams", "output": "HParams for transformer big model on WMT.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def print_total_timer():\n    \"\"\"\n    \n    \"\"\"\n    if len(_TOTAL_TIMER_DATA) == 0:\n        return\n    for k, v in six.iteritems(_TOTAL_TIMER_DATA):\n        logger.info(\"Total Time: {} -> {:.2f} sec, {} times, {:.3g} sec/time\".format(\n            k, v.sum, v.count, v.average))", "output": "Print the content of the TotalTimer, if it's not empty. This function will automatically get\n    called when program exits.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_visible_commands_starting_with(ctx, starts_with):\n    \"\"\"\n    \n    \"\"\"\n    for c in ctx.command.list_commands(ctx):\n        if c.startswith(starts_with):\n            command = ctx.command.get_command(ctx, c)\n            if not command.hidden:\n                yield command", "output": ":param ctx: context associated with the parsed command\n    :starts_with: string that visible commands must start with.\n    :return: all visible (not hidden) commands that start with starts_with.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_loc(data, attr={'lr_mult':'0.01'}):\n    \"\"\"\n    \n    \"\"\"\n    loc = mx.symbol.Convolution(data=data, num_filter=30, kernel=(5, 5), stride=(2,2))\n    loc = mx.symbol.Activation(data = loc, act_type='relu')\n    loc = mx.symbol.Pooling(data=loc, kernel=(2, 2), stride=(2, 2), pool_type='max')\n    loc = mx.symbol.Convolution(data=loc, num_filter=60, kernel=(3, 3), stride=(1,1), pad=(1, 1))\n    loc = mx.symbol.Activation(data = loc, act_type='relu')\n    loc = mx.symbol.Pooling(data=loc, global_pool=True, kernel=(2, 2), pool_type='avg')\n    loc = mx.symbol.Flatten(data=loc)\n    loc = mx.symbol.FullyConnected(data=loc, num_hidden=6, name=\"stn_loc\", attr=attr)\n    return loc", "output": "the localisation network in lenet-stn, it will increase acc about more than 1%,\n    when num-epoch >=15", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ParseOrMerge(self, lines, message):\n    \"\"\"\n    \"\"\"\n    tokenizer = Tokenizer(lines)\n    while not tokenizer.AtEnd():\n      self._MergeField(tokenizer, message)", "output": "Converts a text representation of a protocol message into a message.\n\n    Args:\n      lines: Lines of a message's text representation.\n      message: A protocol buffer message to merge into.\n\n    Raises:\n      ParseError: On text parsing problems.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy(self, file, mode='w', propindexes=True, keys=None, complib=None,\n             complevel=None, fletcher32=False, overwrite=True):\n        \"\"\" \n\n        \"\"\"\n        new_store = HDFStore(\n            file,\n            mode=mode,\n            complib=complib,\n            complevel=complevel,\n            fletcher32=fletcher32)\n        if keys is None:\n            keys = list(self.keys())\n        if not isinstance(keys, (tuple, list)):\n            keys = [keys]\n        for k in keys:\n            s = self.get_storer(k)\n            if s is not None:\n\n                if k in new_store:\n                    if overwrite:\n                        new_store.remove(k)\n\n                data = self.select(k)\n                if s.is_table:\n\n                    index = False\n                    if propindexes:\n                        index = [a.name for a in s.axes if a.is_indexed]\n                    new_store.append(\n                        k, data, index=index,\n                        data_columns=getattr(s, 'data_columns', None),\n                        encoding=s.encoding\n                    )\n                else:\n                    new_store.put(k, data, encoding=s.encoding)\n\n        return new_store", "output": "copy the existing store to a new file, upgrading in place\n\n            Parameters\n            ----------\n            propindexes: restore indexes in copied file (defaults to True)\n            keys       : list of keys to include in the copy (defaults to all)\n            overwrite  : overwrite (remove and replace) existing nodes in the\n                new store (default is True)\n            mode, complib, complevel, fletcher32 same as in HDFStore.__init__\n\n            Returns\n            -------\n            open file handle of the new store", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def closing_plugin(self, cancelable=False):\r\n        \"\"\"\"\"\"\r\n        self.save_history()\r\n        self.set_option('zoom_factor',\r\n                        self.pydocbrowser.webview.get_zoom_factor())\r\n        return True", "output": "Perform actions before parent main window is closed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_salt_interface(vm_, opts):\n    '''\n    \n    '''\n    salt_host = salt.config.get_cloud_config_value(\n        'salt_interface', vm_, opts, default=False,\n        search_global=False\n    )\n\n    if salt_host is False:\n        salt_host = salt.config.get_cloud_config_value(\n            'ssh_interface', vm_, opts, default='public_ips',\n            search_global=False\n        )\n\n    return salt_host", "output": "Return the salt_interface type to connect to. Either 'public_ips' (default)\n    or 'private_ips'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gpu_mem_get(id=None):\n    \"\"\n    if not use_gpu: return GPUMemory(0, 0, 0)\n    if id is None: id = torch.cuda.current_device()\n    try:\n        handle = pynvml.nvmlDeviceGetHandleByIndex(id)\n        info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n        return GPUMemory(*(map(b2mb, [info.total, info.free, info.used])))\n    except:\n        return GPUMemory(0, 0, 0)", "output": "get total, used and free memory (in MBs) for gpu `id`. if `id` is not passed, currently selected torch device is used", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def call(conn=None, call=None, kwargs=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The call function must be called with '\n            '-f or --function.'\n        )\n\n    if 'func' not in kwargs:\n        raise SaltCloudSystemExit(\n            'No `func` argument passed'\n        )\n\n    if conn is None:\n        conn = get_conn()\n\n    func = kwargs.pop('func')\n    for key, value in kwargs.items():\n        try:\n            kwargs[key] = __utils__['json.loads'](value)\n        except ValueError:\n            continue\n    try:\n        return getattr(conn, func)(**kwargs)\n    except shade.exc.OpenStackCloudException as exc:\n        log.error('Error running %s: %s', func, exc)\n        raise SaltCloudSystemExit(six.text_type(exc))", "output": "Call function from shade.\n\n    func\n\n        function to call from shade.openstackcloud library\n\n    CLI Example\n\n    .. code-block:: bash\n\n        salt-cloud -f call myopenstack func=list_images\n        t sujksalt-cloud -f call myopenstack func=create_network name=mysubnet", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __get_size(conn, vm_):\n    '''\n    \n    '''\n    size = config.get_cloud_config_value(\n        'size', vm_, __opts__, default='n1-standard-1', search_global=False)\n    return conn.ex_get_size(size, __get_location(conn, vm_))", "output": "Need to override libcloud to find the machine type in the proper zone.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_min(environment, value, case_sensitive=False, attribute=None):\n    \"\"\"\n    \"\"\"\n    return _min_or_max(environment, value, min, case_sensitive, attribute)", "output": "Return the smallest item from the sequence.\n\n    .. sourcecode:: jinja\n\n        {{ [1, 2, 3]|min }}\n            -> 1\n\n    :param case_sensitive: Treat upper and lower case strings as distinct.\n    :param attribute: Get the object with the max value of this attribute.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def as_categorical_frame(self, index, columns, name=None):\n        \"\"\"\n        \n        \"\"\"\n        if len(self.shape) != 2:\n            raise ValueError(\n                \"Can't convert a non-2D LabelArray into a DataFrame.\"\n            )\n\n        expected_shape = (len(index), len(columns))\n        if expected_shape != self.shape:\n            raise ValueError(\n                \"Can't construct a DataFrame with provided indices:\\n\\n\"\n                \"LabelArray shape is {actual}, but index and columns imply \"\n                \"that shape should be {expected}.\".format(\n                    actual=self.shape,\n                    expected=expected_shape,\n                )\n            )\n\n        return pd.Series(\n            index=pd.MultiIndex.from_product([index, columns]),\n            data=self.ravel().as_categorical(),\n            name=name,\n        ).unstack()", "output": "Coerce self into a pandas DataFrame of Categoricals.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pivot(self, column_):\n        \"\"\"\"\"\"\n        if isinstance(column_, str):\n            try:\n                return self.data.reset_index().pivot(\n                    index='datetime',\n                    columns='code',\n                    values=column_\n                )\n            except:\n                return self.data.reset_index().pivot(\n                    index='date',\n                    columns='code',\n                    values=column_\n                )\n        elif isinstance(column_, list):\n            try:\n                return self.data.reset_index().pivot_table(\n                    index='datetime',\n                    columns='code',\n                    values=column_\n                )\n            except:\n                return self.data.reset_index().pivot_table(\n                    index='date',\n                    columns='code',\n                    values=column_\n                )", "output": "\u589e\u52a0\u5bf9\u4e8e\u591a\u5217\u7684\u652f\u6301", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_dummy_vars():\n  \"\"\"\"\"\"\n  var_names = set([v.name for v in tf.global_variables()])\n  if \"losses_avg/problem_0/total_loss:0\" in var_names:\n    return\n  with tf.variable_scope(\"losses_avg\"):\n    with tf.variable_scope(\"problem_0\"):\n      for var_name in [\"total\", \"extra\", \"training\"]:\n        tf.get_variable(\n            \"%s_loss\" % var_name, initializer=100.0, trainable=False)\n  with tf.variable_scope(\"train_stats\"):\n    tf.get_variable(\"problem_0_steps\", initializer=0, trainable=False)", "output": "Dummy vars for restore to work when not using TPU codepath.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absent(name, auth=None, **kwargs):\n    '''\n    \n\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    kwargs = __utils__['args.clean_kwargs'](**kwargs)\n\n    __salt__['neutronng.setup_clouds'](auth)\n\n    kwargs['project_id'] = __salt__['keystoneng.project_get'](\n        name=kwargs['project_name'])\n\n    secgroup = __salt__['neutronng.security_group_get'](\n        name=name,\n        filters={'project_id': kwargs['project_id']}\n    )\n\n    if secgroup:\n        if __opts__['test'] is True:\n            ret['result'] = None\n            ret['changes'] = {'id': secgroup.id}\n            ret['comment'] = 'Security group will be deleted.'\n            return ret\n\n        __salt__['neutronng.security_group_delete'](name=secgroup)\n        ret['changes']['id'] = name\n        ret['comment'] = 'Deleted security group'\n\n    return ret", "output": "Ensure a security group does not exist\n\n    name\n        Name of the security group", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_firewall_rule(self, protocol, action, **kwargs):\n        '''\n        \n        '''\n        body = {'protocol': protocol, 'action': action}\n        if 'tenant_id' in kwargs:\n            body['tenant_id'] = kwargs['tenant_id']\n        if 'name' in kwargs:\n            body['name'] = kwargs['name']\n        if 'description' in kwargs:\n            body['description'] = kwargs['description']\n        if 'ip_version' in kwargs:\n            body['ip_version'] = kwargs['ip_version']\n        if 'source_ip_address' in kwargs:\n            body['source_ip_address'] = kwargs['source_ip_address']\n        if 'destination_port' in kwargs:\n            body['destination_port'] = kwargs['destination_port']\n        if 'shared' in kwargs:\n            body['shared'] = kwargs['shared']\n        if 'enabled' in kwargs:\n            body['enabled'] = kwargs['enabled']\n        return self.network_conn.create_firewall_rule(body={'firewall_rule': body})", "output": "Create a new firlwall rule", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait_lock(lk_fn, dest, wait_timeout=0):\n    '''\n    \n    '''\n    if not os.path.exists(lk_fn):\n        return False\n    if not os.path.exists(dest):\n        # The dest is not here, sleep for a bit, if the dest is not here yet\n        # kill the lockfile and start the write\n        time.sleep(1)\n        if not os.path.isfile(dest):\n            _unlock_cache(lk_fn)\n            return False\n    timeout = None\n    if wait_timeout:\n        timeout = time.time() + wait_timeout\n    # There is a lock file, the dest is there, stat the dest, sleep and check\n    # that the dest is being written, if it is not being written kill the lock\n    # file and continue. Also check if the lock file is gone.\n    s_count = 0\n    s_size = os.stat(dest).st_size\n    while True:\n        time.sleep(1)\n        if not os.path.exists(lk_fn):\n            return False\n        size = os.stat(dest).st_size\n        if size == s_size:\n            s_count += 1\n            if s_count >= 3:\n                # The file is not being written to, kill the lock and proceed\n                _unlock_cache(lk_fn)\n                return False\n        else:\n            s_size = size\n        if timeout:\n            if time.time() > timeout:\n                raise ValueError(\n                    'Timeout({0}s) for {1} (lock: {2}) elapsed'.format(\n                        wait_timeout, dest, lk_fn\n                    )\n                )\n    return False", "output": "If the write lock is there, check to see if the file is actually being\n    written. If there is no change in the file size after a short sleep,\n    remove the lock and move forward.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_pkgs(*packages, **kwargs):\n    '''\n    \n    '''\n    pkgs = {}\n    cmd = 'dpkg -l {0}'.format(' '.join(packages))\n    out = __salt__['cmd.run_all'](cmd, python_shell=False)\n    if out['retcode'] != 0:\n        msg = 'Error:  ' + out['stderr']\n        log.error(msg)\n        return msg\n    out = out['stdout']\n\n    for line in out.splitlines():\n        if line.startswith('ii '):\n            comps = line.split()\n            pkgs[comps[1]] = comps[2]\n    return pkgs", "output": "List the packages currently installed in a dict::\n\n        {'<package_name>': '<version>'}\n\n    External dependencies::\n\n        Virtual package resolution requires aptitude. Because this function\n        uses dpkg, virtual packages will be reported as not installed.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' lowpkg.list_pkgs\n        salt '*' lowpkg.list_pkgs httpd", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_norm(x, norm_type, depth, epsilon, layer_collection=None):\n  \"\"\"\"\"\"\n  if layer_collection is not None:\n    assert norm_type == \"layer\"\n  if norm_type == \"layer\":\n    return layer_norm(\n        x, filters=depth, epsilon=epsilon, layer_collection=layer_collection)\n  if norm_type == \"group\":\n    return group_norm(x, filters=depth, epsilon=epsilon)\n  if norm_type == \"batch\":\n    return layers().BatchNormalization(epsilon=epsilon)(x)\n  if norm_type == \"noam\":\n    return noam_norm(x, epsilon)\n  if norm_type == \"l2\":\n    return l2_norm(x, filters=depth, epsilon=epsilon)\n  if norm_type == \"none\":\n    return x\n  raise ValueError(\"Parameter normalizer_fn must be one of: 'layer', 'batch',\"\n                   \"'noam', 'lr', 'none'.\")", "output": "Apply Normalization.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def destroy(self, names):\n        '''\n        \n        '''\n        mapper = salt.cloud.Map(self._opts_defaults(destroy=True))\n        if isinstance(names, six.string_types):\n            names = names.split(',')\n        return salt.utils.data.simple_types_filter(\n            mapper.destroy(names)\n        )", "output": "Destroy the named VMs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def summary(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.hasSummary:\n            return KMeansSummary(super(KMeansModel, self).summary)\n        else:\n            raise RuntimeError(\"No training summary available for this %s\" %\n                               self.__class__.__name__)", "output": "Gets summary (e.g. cluster assignments, cluster sizes) of the model trained on the\n        training set. An exception is thrown if no summary exists.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_numeric_features_to_observed_range(examples):\n  \"\"\"\n  \"\"\"\n  observed_features = collections.defaultdict(list)  # name -> [value, ]\n  for example in examples:\n    for feature_name in get_numeric_feature_names(example):\n      original_feature = parse_original_feature_from_example(\n          example, feature_name)\n      observed_features[feature_name].extend(original_feature.original_value)\n  return {\n      feature_name: {\n          'observedMin': min(feature_values),\n          'observedMax': max(feature_values),\n      }\n      for feature_name, feature_values in iteritems(observed_features)\n  }", "output": "Returns numerical features and their observed ranges.\n\n  Args:\n    examples: Examples to read to get ranges.\n\n  Returns:\n    A dict mapping feature_name -> {'observedMin': 'observedMax': } dicts,\n    with a key for each numerical feature.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _backward_impl(self):\n        \"\"\"\n        \"\"\"\n        if self._grad_func is not None:\n            grad = self._grad_func(self._scores, self._labels)\n            if not isinstance(grad, nd.NDArray):\n                grad = nd.array(grad)\n            self._scores_grad = grad\n        else:\n            raise NotImplementedError()", "output": "Actual implementation of the backward computation. The computation\n        should take ``self._scores`` and ``self._labels`` and then compute the\n        gradients with respect to the scores, store it as an `NDArray` in\n        ``self._scores_grad``.\n\n        Instead of defining a subclass and overriding this function,\n        a more convenient way is to pass in a `grad_func` when constructing\n        the module object. Then it will be called to compute the gradients.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _compute(self, arrays, dates, assets, mask):\n        \"\"\"\n        \n        \"\"\"\n        out = full(mask.shape, self.missing_value, dtype=self.dtype)\n        # This writes directly into our output buffer.\n        numexpr.evaluate(\n            self._expr,\n            local_dict={\n                \"x_%d\" % idx: array\n                for idx, array in enumerate(arrays)\n            },\n            global_dict={'inf': inf},\n            out=out,\n        )\n        return out", "output": "Compute our stored expression string with numexpr.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_metrics_api(client):\n    \"\"\"\n    \"\"\"\n    generated = MetricsServiceV2Client(\n        credentials=client._credentials, client_info=_CLIENT_INFO\n    )\n    return _MetricsAPI(generated, client)", "output": "Create an instance of the Metrics API adapter.\n\n    :type client: :class:`~google.cloud.logging.client.Client`\n    :param client: The client that holds configuration details.\n\n    :rtype: :class:`_MetricsAPI`\n    :returns: A metrics API instance with the proper credentials.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_image(self, batch_id, image_id, image_properties=None):\n    \"\"\"\"\"\"\n    if batch_id not in self._data:\n      raise KeyError('Batch with ID \"{0}\" does not exist'.format(batch_id))\n    if image_properties is None:\n      image_properties = {}\n    if not isinstance(image_properties, dict):\n      raise ValueError('image_properties has to be dict, however it was: '\n                       + str(type(image_properties)))\n    self._data[batch_id]['images'][image_id] = image_properties.copy()", "output": "Adds image to given batch.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_trial_complete(self,\n                          trial_id,\n                          result=None,\n                          error=False,\n                          early_terminated=False):\n        \"\"\"\n        \"\"\"\n        if result:\n            self.conn.experiments(self.experiment.id).observations().create(\n                suggestion=self._live_trial_mapping[trial_id].id,\n                value=result[self._reward_attr],\n            )\n            # Update the experiment object\n            self.experiment = self.conn.experiments(self.experiment.id).fetch()\n        elif error or early_terminated:\n            # Reports a failed Observation\n            self.conn.experiments(self.experiment.id).observations().create(\n                failed=True, suggestion=self._live_trial_mapping[trial_id].id)\n        del self._live_trial_mapping[trial_id]", "output": "Passes the result to SigOpt unless early terminated or errored.\n\n        If a trial fails, it will be reported as a failed Observation, telling\n        the optimizer that the Suggestion led to a metric failure, which\n        updates the feasible region and improves parameter recommendation.\n\n        Creates SigOpt Observation object for trial.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def segments(self, index=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request('GET', _make_path('_cat',\n            'segments', index), params=params)", "output": "The segments command is the detailed view of Lucene segments per index.\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-segments.html>`_\n\n        :arg index: A comma-separated list of index names to limit the returned\n            information\n        :arg bytes: The unit in which to display byte values, valid choices are:\n            'b', 'k', 'kb', 'm', 'mb', 'g', 'gb', 't', 'tb', 'p', 'pb'\n        :arg format: a short version of the Accept header, e.g. json, yaml\n        :arg h: Comma-separated list of column names to display\n        :arg help: Return help information, default False\n        :arg s: Comma-separated list of column names or column aliases to sort\n            by\n        :arg v: Verbose mode. Display column headers, default False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _hash_categorical(c, encoding, hash_key):\n    \"\"\"\n    \n    \"\"\"\n    # Convert ExtensionArrays to ndarrays\n    values = np.asarray(c.categories.values)\n    hashed = hash_array(values, encoding, hash_key,\n                        categorize=False)\n\n    # we have uint64, as we don't directly support missing values\n    # we don't want to use take_nd which will coerce to float\n    # instead, directly construct the result with a\n    # max(np.uint64) as the missing value indicator\n    #\n    # TODO: GH 15362\n\n    mask = c.isna()\n    if len(hashed):\n        result = hashed.take(c.codes)\n    else:\n        result = np.zeros(len(mask), dtype='uint64')\n\n    if mask.any():\n        result[mask] = np.iinfo(np.uint64).max\n\n    return result", "output": "Hash a Categorical by hashing its categories, and then mapping the codes\n    to the hashes\n\n    Parameters\n    ----------\n    c : Categorical\n    encoding : string, default 'utf8'\n    hash_key : string key to encode, default to _default_hash_key\n\n    Returns\n    -------\n    ndarray of hashed values array, same size as len(c)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_folder(cls, path:PathOrStr='.', extensions:Collection[str]=text_extensions, vocab:Vocab=None,\n                    processor:PreProcessor=None, **kwargs)->'TextList':\n        \"\"\n        processor = ifnone(processor, [OpenFileProcessor(), TokenizeProcessor(), NumericalizeProcessor(vocab=vocab)])\n        return super().from_folder(path=path, extensions=extensions, processor=processor, **kwargs)", "output": "Get the list of files in `path` that have a text suffix. `recurse` determines if we search subfolders.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_tfms(self, tfms:TfmList, do_resolve:bool=True, xtra:Optional[Dict[Callable,dict]]=None,\n                   size:Optional[Union[int,TensorImageSize]]=None, resize_method:ResizeMethod=None,\n                   mult:int=None, padding_mode:str='reflection', mode:str='bilinear', remove_out:bool=True)->TensorImage:\n        \"\"\n        if not (tfms or xtra or size): return self\n        tfms = listify(tfms)\n        xtra = ifnone(xtra, {})\n        default_rsz = ResizeMethod.SQUISH if (size is not None and is_listy(size)) else ResizeMethod.CROP\n        resize_method = ifnone(resize_method, default_rsz)\n        if resize_method <= 2 and size is not None: tfms = self._maybe_add_crop_pad(tfms)\n        tfms = sorted(tfms, key=lambda o: o.tfm.order)\n        if do_resolve: _resolve_tfms(tfms)\n        x = self.clone()\n        x.set_sample(padding_mode=padding_mode, mode=mode, remove_out=remove_out)\n        if size is not None:\n            crop_target = _get_crop_target(size, mult=mult)\n            if resize_method in (ResizeMethod.CROP,ResizeMethod.PAD):\n                target = _get_resize_target(x, crop_target, do_crop=(resize_method==ResizeMethod.CROP))\n                x.resize(target)\n            elif resize_method==ResizeMethod.SQUISH: x.resize((x.shape[0],) + crop_target)\n        else: size = x.size\n        size_tfms = [o for o in tfms if isinstance(o.tfm,TfmCrop)]\n        for tfm in tfms:\n            if tfm.tfm in xtra: x = tfm(x, **xtra[tfm.tfm])\n            elif tfm in size_tfms:\n                if resize_method in (ResizeMethod.CROP,ResizeMethod.PAD):\n                    x = tfm(x, size=_get_crop_target(size,mult=mult), padding_mode=padding_mode)\n            else: x = tfm(x)\n        return x.refresh()", "output": "Apply all `tfms` to the `Image`, if `do_resolve` picks value for random args.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def usages_list(location, **kwargs):\n    '''\n    \n\n    '''\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        result = __utils__['azurearm.paged_object_to_list'](netconn.usages.list(location))\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    List subscription network usage for a location.\n\n    :param location: The Azure location to query for network usage.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.usages_list westus", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_certs(keychain=\"/Library/Keychains/System.keychain\"):\n    '''\n    \n    '''\n    cmd = 'security find-certificate -a {0} | grep -o \"alis\".*\\\\\" | ' \\\n          'grep -o \\'\\\\\"[-A-Za-z0-9.:() ]*\\\\\"\\''.format(_quote(keychain))\n    out = __salt__['cmd.run'](cmd, python_shell=True)\n    return out.replace('\"', '').split('\\n')", "output": "List all of the installed certificates\n\n    keychain\n        The keychain to install the certificate to, this defaults to\n        /Library/Keychains/System.keychain\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keychain.list_certs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy_random_pointer_v2(head):\n    \"\"\"\n    \n    \"\"\"\n    copy = defaultdict(lambda: RandomListNode(0))\n    copy[None] = None\n    node = head\n    while node:\n        copy[node].label = node.label\n        copy[node].next = copy[node.next]\n        copy[node].random = copy[node.random]\n        node = node.next\n    return copy[head]", "output": ":type head: RandomListNode\n    :rtype: RandomListNode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def accuracy_lcs(self, label, pred):\n        \"\"\" \"\"\"\n        hit = 0.\n        total = 0.\n        batch_size = label.shape[0]\n        for i in range(batch_size):\n            l = self._remove_blank(label[i])\n            p = []\n            for k in range(self.seq_len):\n                p.append(np.argmax(pred[k * batch_size + i]))\n            p = self.ctc_label(p)\n            hit += self._lcs(p, l) * 1.0 / len(l)\n            total += 1.0\n        assert total == batch_size\n        return hit / total", "output": "Longest Common Subsequence accuracy measure: calculate accuracy of each prediction as LCS/length", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cumulative_mean(self):\n        \"\"\"\n        \n        \"\"\"\n        from .. import extensions\n        agg_op = \"__builtin__cum_avg__\"\n        return SArray(_proxy = self.__proxy__.builtin_cumulative_aggregate(agg_op))", "output": "Return the cumulative mean of the elements in the SArray.\n\n        Returns an SArray where each element in the output corresponds to the\n        mean value of all the elements preceding and including it. The SArray\n        is expected to be of numeric type (int, float), or a numeric vector\n        type.\n\n        Returns\n        -------\n        out : Sarray[float, array.array]\n\n        Notes\n        -----\n         - Missing values are ignored while performing the cumulative\n           aggregate operation.\n         - For SArray's of type array.array, all entries are expected to\n           be of the same size.\n\n        Examples\n        --------\n        >>> sa = SArray([1, 2, 3, 4, 5])\n        >>> sa.cumulative_mean()\n        dtype: float\n        rows: 3\n        [1, 1.5, 2, 2.5, 3]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_images(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_images function must be called with '\n            '-f or --function, or with the --list-images option'\n        )\n\n    if not isinstance(kwargs, dict):\n        kwargs = {}\n\n    params = {\n        'action': 'DescribeImages',\n        'provider': 'system',\n        'zone': _get_specified_zone(kwargs, get_configured_provider()),\n    }\n    items = query(params=params)\n\n    result = {}\n    for image in items['image_set']:\n        result[image['image_id']] = {}\n        for key in image:\n            result[image['image_id']][key] = image[key]\n\n    return result", "output": "Return a list of the images that are on the provider.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-cloud --list-images my-qingcloud\n        salt-cloud -f avail_images my-qingcloud zone=gd1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sample_prior(batch_size):\n    cat, _ = get_distributions(DIST_PRIOR_PARAM[:NUM_CLASS], DIST_PRIOR_PARAM[NUM_CLASS:])\n    sample_cat = tf.one_hot(cat.sample(batch_size), NUM_CLASS)\n\n    \"\"\"\n    \n    \"\"\"\n    sample_uni = tf.random_uniform([batch_size, NUM_UNIFORM], -1, 1)\n    samples = tf.concat([sample_cat, sample_uni], axis=1)\n    return samples", "output": "OpenAI official code actually models the \"uniform\" latent code as\n    a Gaussian distribution, but obtain the samples from a uniform distribution.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(self):\n        \"\"\"\"\"\"\n        self.t = threading.Thread(target=self._run, name=\"ray_import_thread\")\n        # Making the thread a daemon causes it to exit\n        # when the main thread exits.\n        self.t.daemon = True\n        self.t.start()", "output": "Start the import thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def try_pick_piece_of_work(self, worker_id, submission_id=None):\n    \"\"\"\n    \"\"\"\n    client = self._datastore_client\n    unclaimed_work_ids = None\n    if submission_id:\n      unclaimed_work_ids = [\n          k for k, v in iteritems(self.work)\n          if is_unclaimed(v) and (v['submission_id'] == submission_id)\n      ]\n    if not unclaimed_work_ids:\n      unclaimed_work_ids = [k for k, v in iteritems(self.work)\n                            if is_unclaimed(v)]\n    if unclaimed_work_ids:\n      next_work_id = random.choice(unclaimed_work_ids)\n    else:\n      return None\n    try:\n      with client.transaction() as transaction:\n        work_key = client.key(KIND_WORK_TYPE, self._work_type_entity_id,\n                              KIND_WORK, next_work_id)\n        work_entity = client.get(work_key, transaction=transaction)\n        if not is_unclaimed(work_entity):\n          return None\n        work_entity['claimed_worker_id'] = worker_id\n        work_entity['claimed_worker_start_time'] = get_integer_time()\n        transaction.put(work_entity)\n    except Exception:\n      return None\n    return next_work_id", "output": "Tries pick next unclaimed piece of work to do.\n\n    Attempt to claim work piece is done using Cloud Datastore transaction, so\n    only one worker can claim any work piece at a time.\n\n    Args:\n      worker_id: ID of current worker\n      submission_id: if not None then this method will try to pick\n        piece of work for this submission\n\n    Returns:\n      ID of the claimed work piece", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def truncate(self, before=None, after=None):\n        \"\"\"\n        \n        \"\"\"\n        if after and before and after < before:\n            raise ValueError('after < before')\n\n        i, j = self.levels[0].slice_locs(before, after)\n        left, right = self.slice_locs(before, after)\n\n        new_levels = list(self.levels)\n        new_levels[0] = new_levels[0][i:j]\n\n        new_codes = [level_codes[left:right] for level_codes in self.codes]\n        new_codes[0] = new_codes[0] - i\n\n        return MultiIndex(levels=new_levels, codes=new_codes,\n                          verify_integrity=False)", "output": "Slice index between two labels / tuples, return new MultiIndex\n\n        Parameters\n        ----------\n        before : label or tuple, can be partial. Default None\n            None defaults to start\n        after : label or tuple, can be partial. Default None\n            None defaults to end\n\n        Returns\n        -------\n        truncated : MultiIndex", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_locations(call=None):\n    '''\n    \n\n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_locations function must be called with '\n            '-f or --function, or with the --list-locations option.'\n        )\n\n    server, user, password = _get_xml_rpc()\n    auth = ':'.join([user, password])\n    host_pool = server.one.hostpool.info(auth)[1]\n\n    locations = {}\n    for host in _get_xml(host_pool):\n        locations[host.find('NAME').text] = _xml_to_dict(host)\n\n    return locations", "output": "Return available OpenNebula locations.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud --list-locations opennebula\n        salt-cloud --function avail_locations opennebula\n        salt-cloud -f avail_locations opennebula", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def values(self, dtype, dates, sids):\n        \"\"\"\n        \n        \"\"\"\n        shape = (len(dates), len(sids))\n        return {\n            datetime64ns_dtype: self._datetime_values,\n            float64_dtype: self._float_values,\n            int64_dtype: self._int_values,\n            bool_dtype: self._bool_values,\n            object_dtype: self._object_values,\n        }[dtype](shape)", "output": "Make a random array of shape (len(dates), len(sids)) with ``dtype``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def children_and_parameters(m:nn.Module):\n    \"\"\n    children = list(m.children())\n    children_p = sum([[id(p) for p in c.parameters()] for c in m.children()],[])\n    for p in m.parameters():\n        if id(p) not in children_p: children.append(ParameterModule(p))\n    return children", "output": "Return the children of `m` and its direct parameters not registered in modules.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_scope_highlight_color(self):\n        \"\"\"\n        \n        \"\"\"\n        color = self.editor.sideareas_color\n        if color.lightness() < 128:\n            color = drift_color(color, 130)\n        else:\n            color = drift_color(color, 105)\n        return color", "output": "Gets the base scope highlight color (derivated from the editor\n        background)\n\n        For lighter themes will be a darker color, \n        and for darker ones will be a lighter color", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_session(self, capabilities, browser_profile=None):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(capabilities, dict):\n            raise InvalidArgumentException(\"Capabilities must be a dictionary\")\n        if browser_profile:\n            if \"moz:firefoxOptions\" in capabilities:\n                capabilities[\"moz:firefoxOptions\"][\"profile\"] = browser_profile.encoded\n            else:\n                capabilities.update({'firefox_profile': browser_profile.encoded})\n        w3c_caps = _make_w3c_caps(capabilities)\n        parameters = {\"capabilities\": w3c_caps,\n                      \"desiredCapabilities\": capabilities}\n        response = self.execute(Command.NEW_SESSION, parameters)\n        if 'sessionId' not in response:\n            response = response['value']\n        self.session_id = response['sessionId']\n        self.capabilities = response.get('value')\n\n        # if capabilities is none we are probably speaking to\n        # a W3C endpoint\n        if self.capabilities is None:\n            self.capabilities = response.get('capabilities')\n\n        # Double check to see if we have a W3C Compliant browser\n        self.w3c = response.get('status') is None\n        self.command_executor.w3c = self.w3c", "output": "Creates a new session with the desired capabilities.\n\n        :Args:\n         - browser_name - The name of the browser to request.\n         - version - Which browser version to request.\n         - platform - Which platform to request the browser on.\n         - javascript_enabled - Whether the new session should support JavaScript.\n         - browser_profile - A selenium.webdriver.firefox.firefox_profile.FirefoxProfile object. Only used if Firefox is requested.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cbow_batch(centers, contexts, num_tokens, dtype, index_dtype):\n    \"\"\"\"\"\"\n    contexts_data, contexts_row, contexts_col = contexts\n    centers = mx.nd.array(centers, dtype=index_dtype)\n    contexts = mx.nd.sparse.csr_matrix(\n        (contexts_data, (contexts_row, contexts_col)),\n        dtype=dtype, shape=(len(centers), num_tokens))  # yapf: disable\n    return centers, contexts", "output": "Create a batch for CBOW training objective.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_buckets_cache_filename(bucket, prefix):\n    '''\n    \n    '''\n\n    cache_dir = _get_cache_dir()\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return os.path.join(cache_dir, '{0}-{1}-files.cache'.format(bucket, prefix))", "output": "Return the filename of the cache for bucket contents.\n    Create the path if it does not exist.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def targets(self, tgt, tgt_type):\n        '''\n        \n        '''\n        targets = {}\n        for back in self._gen_back():\n            f_str = '{0}.targets'.format(back)\n            if f_str not in self.rosters:\n                continue\n            try:\n                targets.update(self.rosters[f_str](tgt, tgt_type))\n            except salt.exceptions.SaltRenderError as exc:\n                log.error('Unable to render roster file: %s', exc)\n            except IOError as exc:\n                log.error(\"Can't access roster for backend %s: %s\", back, exc)\n\n        log.debug('Matched minions: %s', targets)\n        return targets", "output": "Return a dict of {'id': {'ipv4': <ipaddr>}} data sets to be used as\n        targets given the passed tgt and tgt_type", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def homogenize(series_dict):\n    \"\"\"\n    \n    \"\"\"\n    index = None\n\n    need_reindex = False\n\n    for _, series in series_dict.items():\n        if not np.isnan(series.fill_value):\n            raise TypeError('this method is only valid with NaN fill values')\n\n        if index is None:\n            index = series.sp_index\n        elif not series.sp_index.equals(index):\n            need_reindex = True\n            index = index.intersect(series.sp_index)\n\n    if need_reindex:\n        output = {}\n        for name, series in series_dict.items():\n            if not series.sp_index.equals(index):\n                series = series.sparse_reindex(index)\n\n            output[name] = series\n    else:\n        output = series_dict\n\n    return output", "output": "Conform a set of SparseSeries (with NaN fill_value) to a common SparseIndex\n    corresponding to the locations where they all have data\n\n    Parameters\n    ----------\n    series_dict : dict or DataFrame\n\n    Notes\n    -----\n    Using the dumbest algorithm I could think of. Should put some more thought\n    into this\n\n    Returns\n    -------\n    homogenized : dict of SparseSeries", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_purge(bare=False, downloads=False, allow_global=False):\n    \"\"\"\"\"\"\n\n    if downloads:\n        if not bare:\n            click.echo(crayons.normal(fix_utf8(\"Clearing out downloads directory\u2026\"), bold=True))\n        vistir.path.rmtree(project.download_location)\n        return\n\n    # Remove comments from the output, if any.\n    installed = set([\n        pep423_name(pkg.project_name) for pkg in project.environment.get_installed_packages()\n    ])\n    bad_pkgs = set([pep423_name(pkg) for pkg in BAD_PACKAGES])\n    # Remove setuptools, pip, etc from targets for removal\n    to_remove = installed - bad_pkgs\n\n    # Skip purging if there is no packages which needs to be removed\n    if not to_remove:\n        if not bare:\n            click.echo(\"Found 0 installed package, skip purging.\")\n            click.echo(crayons.green(\"Environment now purged and fresh!\"))\n        return installed\n\n    if not bare:\n        click.echo(\n            fix_utf8(\"Found {0} installed package(s), purging\u2026\".format(len(to_remove)))\n        )\n\n    command = \"{0} uninstall {1} -y\".format(\n        escape_grouped_arguments(which_pip(allow_global=allow_global)),\n        \" \".join(to_remove),\n    )\n    if environments.is_verbose():\n        click.echo(\"$ {0}\".format(command))\n    c = delegator.run(command)\n    if c.return_code != 0:\n        raise exceptions.UninstallError(installed, command, c.out + c.err, c.return_code)\n    if not bare:\n        click.echo(crayons.blue(c.out))\n        click.echo(crayons.green(\"Environment now purged and fresh!\"))\n    return installed", "output": "Executes the purge functionality.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main(argv=None):\n  \"\"\"\n  \n  \"\"\"\n  try:\n    _name_of_script, filepath = argv\n  except ValueError:\n    raise ValueError(argv)\n  print(filepath)\n  make_confidence_report_bundled(filepath=filepath,\n                                 test_start=FLAGS.test_start,\n                                 test_end=FLAGS.test_end,\n                                 which_set=FLAGS.which_set,\n                                 recipe=FLAGS.recipe,\n                                 report_path=FLAGS.report_path, batch_size=FLAGS.batch_size)", "output": "Make a confidence report and save it to disk.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _wait_for_files(path):\n    \"\"\"\n    \n    \"\"\"\n    timeout = 0.001\n    remaining = []\n    while timeout < 1.0:\n        remaining = []\n        if os.path.isdir(path):\n            L = os.listdir(path)\n            for target in L:\n                _remaining = _wait_for_files(target)\n                if _remaining:\n                    remaining.extend(_remaining)\n            continue\n        try:\n            os.unlink(path)\n        except FileNotFoundError as e:\n            if e.errno == errno.ENOENT:\n                return\n        except (OSError, IOError, PermissionError):\n            time.sleep(timeout)\n            timeout *= 2\n            remaining.append(path)\n        else:\n            return\n    return remaining", "output": "Retry with backoff up to 1 second to delete files from a directory.\n\n    :param str path: The path to crawl to delete files from\n    :return: A list of remaining paths or None\n    :rtype: Optional[List[str]]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_html_patterns():\r\n    \"\"\" \"\"\"\r\n    tags = any(\"builtin\", [r\"<\", r\"[\\?/]?>\", r\"(?<=<).*?(?=[ >])\"])\r\n    keywords = any(\"keyword\", [r\" [\\w:-]*?(?==)\"])\r\n    string = any(\"string\", [r'\".*?\"'])\r\n    comment = any(\"comment\", [r\"<!--.*?-->\"])\r\n    multiline_comment_start = any(\"multiline_comment_start\", [r\"<!--\"])\r\n    multiline_comment_end = any(\"multiline_comment_end\", [r\"-->\"])\r\n    return \"|\".join([comment, multiline_comment_start,\r\n                     multiline_comment_end, tags, keywords, string])", "output": "Strongly inspired from idlelib.ColorDelegator.make_pat", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sequenceFile(self, path, keyClass=None, valueClass=None, keyConverter=None,\n                     valueConverter=None, minSplits=None, batchSize=0):\n        \"\"\"\n        \n        \"\"\"\n        minSplits = minSplits or min(self.defaultParallelism, 2)\n        jrdd = self._jvm.PythonRDD.sequenceFile(self._jsc, path, keyClass, valueClass,\n                                                keyConverter, valueConverter, minSplits, batchSize)\n        return RDD(jrdd, self)", "output": "Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,\n        a local file system (available on all nodes), or any Hadoop-supported file system URI.\n        The mechanism is as follows:\n\n            1. A Java RDD is created from the SequenceFile or other InputFormat, and the key\n               and value Writable classes\n            2. Serialization is attempted via Pyrolite pickling\n            3. If this fails, the fallback is to call 'toString' on each key and value\n            4. C{PickleSerializer} is used to deserialize pickled objects on the Python side\n\n        :param path: path to sequncefile\n        :param keyClass: fully qualified classname of key Writable class\n               (e.g. \"org.apache.hadoop.io.Text\")\n        :param valueClass: fully qualified classname of value Writable class\n               (e.g. \"org.apache.hadoop.io.LongWritable\")\n        :param keyConverter:\n        :param valueConverter:\n        :param minSplits: minimum splits in dataset\n               (default min(2, sc.defaultParallelism))\n        :param batchSize: The number of Python objects represented as a single\n               Java object. (default 0, choose batchSize automatically)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def subplots(rows:int, cols:int, imgsize:int=4, figsize:Optional[Tuple[int,int]]=None, title=None, **kwargs):\n    \"\"\n    figsize = ifnone(figsize, (imgsize*cols, imgsize*rows))\n    fig, axs = plt.subplots(rows,cols,figsize=figsize)\n    if rows==cols==1: axs = [[axs]] # subplots(1,1) returns Axes, not [Axes]\n    elif (rows==1 and cols!=1) or (cols==1 and rows!=1): axs = [axs]\n    if title is not None: fig.suptitle(title, **kwargs)\n    return array(axs)", "output": "Like `plt.subplots` but with consistent axs shape, `kwargs` passed to `fig.suptitle` with `title`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def listGetString(self, doc, inLine):\n        \"\"\" \"\"\"\n        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        ret = libxml2mod.xmlNodeListGetString(doc__o, self._o, inLine)\n        return ret", "output": "Build the string equivalent to the text contained in the\n           Node list made of TEXTs and ENTITY_REFs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _quote_username(name):\n    '''\n    \n    '''\n    if not isinstance(name, six.string_types):\n        return str(name)  # future lint: disable=blacklisted-function\n    else:\n        return salt.utils.stringutils.to_str(name)", "output": "Usernames can only contain ascii chars, so make sure we return a str type", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def evaluate():\n    \"\"\"  \"\"\"\n    print(eval_model)\n    eval_model.initialize(mx.init.Xavier(), ctx=context[0])\n    eval_model.hybridize(static_alloc=True, static_shape=True)\n    epoch = args.from_epoch if args.from_epoch else 0\n    while epoch < args.epochs:\n        checkpoint_name = '%s.%s'%(args.save, format(epoch, '02d'))\n        if not os.path.exists(checkpoint_name):\n            print('Wait for a new checkpoint...')\n            # check again after 600 seconds\n            time.sleep(600)\n            continue\n        eval_model.load_parameters(checkpoint_name)\n        print('Loaded parameters from checkpoint %s'%(checkpoint_name))\n        start_epoch_time = time.time()\n        final_test_L = test(test_data, test_batch_size, ctx=context[0])\n        end_epoch_time = time.time()\n        print('[Epoch %d] test loss %.2f, test ppl %.2f'%\n              (epoch, final_test_L, math.exp(final_test_L)))\n        print('Epoch %d took %.2f seconds.'%(epoch, end_epoch_time - start_epoch_time))\n        sys.stdout.flush()\n        epoch += 1", "output": "Evaluate loop for the trained model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_datastore_id(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The get_datastore_id function must be called with -f or --function.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    name = kwargs.get('name', None)\n    if name is None:\n        raise SaltCloudSystemExit(\n            'The get_datastore_id function requires a name.'\n        )\n\n    try:\n        ret = list_datastores()[name]['id']\n    except KeyError:\n        raise SaltCloudSystemExit(\n            'The datastore \\'{0}\\' could not be found.'.format(name)\n        )\n\n    return ret", "output": "Returns a data store's ID from the given data store name.\n\n    .. versionadded:: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f get_datastore_id opennebula name=my-datastore-name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def values(self, key=_absent):\n        \"\"\"\n        \n        \"\"\"\n        if key is not _absent and key in self._map:\n            return self.getlist(key)\n        return list(self.itervalues())", "output": "Raises: KeyError if <key> is provided and not in the dictionary.\n        Returns: List created from itervalues(<key>).If <key> is provided and\n          is a dictionary key, only values of items with key <key> are\n          returned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_trailing_spaces(self, index=None):\r\n        \"\"\"\"\"\"\r\n        if index is None:\r\n            index = self.get_stack_index()\r\n        finfo = self.data[index]\r\n        finfo.editor.remove_trailing_spaces()", "output": "Remove trailing spaces", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keras_dropout(layer, rate):\n    '''\n    '''\n\n    from keras import layers\n\n    input_dim = len(layer.input.shape)\n    if input_dim == 2:\n        return layers.SpatialDropout1D(rate)\n    elif input_dim == 3:\n        return layers.SpatialDropout2D(rate)\n    elif input_dim == 4:\n        return layers.SpatialDropout3D(rate)\n    else:\n        return layers.Dropout(rate)", "output": "keras dropout layer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists_locally(cls, path):\n    \"\"\"\"\"\"\n    # If INFO file doesn't exist, consider resource does NOT exist, as it would\n    # prevent guessing the `extract_method`.\n    return (tf.io.gfile.exists(path) and\n            tf.io.gfile.exists(_get_info_path(path)))", "output": "Returns whether the resource exists locally, at `resource.path`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_persistent_module(mod):\n    '''\n    \n    '''\n    if not mod or mod in mod_list(True) or mod not in \\\n            available():\n        return set()\n    __salt__['file.append'](_LOADER_CONF, _LOAD_MODULE.format(mod))\n    return set([mod])", "output": "Add a module to loader.conf to make it persistent.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def refresh_db(**kwargs):\n    '''\n    \n    '''\n    # Remove rtag file to keep multiple refreshes from happening in pkg states\n    salt.utils.pkg.clear_rtag(__opts__)\n    cmd = 'xbps-install -Sy'\n    call = __salt__['cmd.run_all'](cmd, output_loglevel='trace')\n    if call['retcode'] != 0:\n        comment = ''\n        if 'stderr' in call:\n            comment += call['stderr']\n\n        raise CommandExecutionError(comment)\n\n    return True", "output": "Update list of available packages from installed repos\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.refresh_db", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def not_send_status(func):\n    \"\"\"\n    \n    \"\"\"\n    @functools.wraps(func)\n    def wrapper(self, response, task):\n        self._extinfo['not_send_status'] = True\n        function = func.__get__(self, self.__class__)\n        return self._run_func(function, response, task)\n    return wrapper", "output": "Do not send process status package back to scheduler.\n\n    It's used by callbacks like on_message, on_result etc...", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save(self, index=None, force=False):\r\n        \"\"\"\"\"\"\r\n        editorstack = self.get_current_editorstack()\r\n        return editorstack.save(index=index, force=force)", "output": "Save file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ToCamelCase(name):\n  \"\"\"\"\"\"\n  capitalize_next = False\n  result = []\n\n  for c in name:\n    if c == '_':\n      if result:\n        capitalize_next = True\n    elif capitalize_next:\n      result.append(c.upper())\n      capitalize_next = False\n    else:\n      result += c\n\n  # Lower-case the first letter.\n  if result and result[0].isupper():\n    result[0] = result[0].lower()\n  return ''.join(result)", "output": "Converts name to camel-case and returns it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def info(name):\n    '''\n    \n    '''\n    ret = {}\n    try:\n        data = pwd.getpwnam(name)\n        ret['gid'] = data.pw_gid\n        ret['groups'] = list_groups(name)\n        ret['home'] = data.pw_dir\n        ret['name'] = data.pw_name\n        ret['passwd'] = data.pw_passwd\n        ret['shell'] = data.pw_shell\n        ret['uid'] = data.pw_uid\n        # Put GECOS info into a list\n        gecos_field = data.pw_gecos.split(',', 3)\n        # Assign empty strings for any unspecified GECOS fields\n        while len(gecos_field) < 4:\n            gecos_field.append('')\n        ret['fullname'] = gecos_field[0]\n        ret['roomnumber'] = gecos_field[1]\n        ret['workphone'] = gecos_field[2]\n        ret['homephone'] = gecos_field[3]\n    except KeyError:\n        return {}\n    return ret", "output": "Return user information\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.info root", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_pipeline(self, id=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request('GET', _make_path('_ingest',\n            'pipeline', id), params=params)", "output": "`<https://www.elastic.co/guide/en/elasticsearch/plugins/current/ingest.html>`_\n\n        :arg id: Comma separated list of pipeline ids. Wildcards supported\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read_undone_shard_from_datastore(self, shard_id=None):\n    \"\"\"\"\"\"\n    self._work = {}\n    client = self._datastore_client\n    parent_key = client.key(KIND_WORK_TYPE, self._work_type_entity_id)\n    filters = [('is_completed', '=', False)]\n    if shard_id is not None:\n      filters.append(('shard_id', '=', shard_id))\n    for entity in client.query_fetch(kind=KIND_WORK, ancestor=parent_key,\n                                     filters=filters):\n      work_id = entity.key.flat_path[-1]\n      self.work[work_id] = dict(entity)\n      if len(self._work) >= MAX_WORK_RECORDS_READ:\n        break", "output": "Reads undone worke pieces which are assigned to shard with given id.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def removed(name, ruby=None, user=None, gem_bin=None):\n    '''\n    \n    '''\n    ret = {'name': name, 'result': None, 'comment': '', 'changes': {}}\n\n    if name not in __salt__['gem.list'](name, ruby, gem_bin=gem_bin, runas=user):\n        ret['result'] = True\n        ret['comment'] = 'Gem is not installed.'\n        return ret\n\n    if __opts__['test']:\n        ret['comment'] = 'The gem {0} would have been removed'.format(name)\n        return ret\n    if __salt__['gem.uninstall'](name, ruby, gem_bin=gem_bin, runas=user):\n        ret['result'] = True\n        ret['changes'][name] = 'Removed'\n        ret['comment'] = 'Gem was successfully removed.'\n    else:\n        ret['result'] = False\n        ret['comment'] = 'Could not remove gem.'\n    return ret", "output": "Make sure that a gem is not installed.\n\n    name\n        The name of the gem to uninstall\n\n    gem_bin : None\n        Full path to ``gem`` binary to use.\n\n    ruby : None\n        If RVM or rbenv are installed, the ruby version and gemset to use.\n        Ignored if ``gem_bin`` is specified.\n\n    user: None\n        The user under which to run the ``gem`` command\n\n        .. versionadded:: 0.17.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def relaxNGValidateFullElement(self, ctxt, elem):\n        \"\"\" \"\"\"\n        if ctxt is None: ctxt__o = None\n        else: ctxt__o = ctxt._o\n        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        ret = libxml2mod.xmlRelaxNGValidateFullElement(ctxt__o, self._o, elem__o)\n        return ret", "output": "Validate a full subtree when\n          xmlRelaxNGValidatePushElement() returned 0 and the content\n           of the node has been expanded.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bokeh_tree(name, rawtext, text, lineno, inliner, options=None, content=None):\n    ''' \n\n    '''\n    app = inliner.document.settings.env.app\n\n    tag = app.env.config['version']\n    if '-' in tag:\n        tag = 'master'\n\n    url = \"%s/tree/%s/%s\" % (_BOKEH_GH, tag, text)\n    options = options or {}\n    set_classes(options)\n    node = nodes.reference(rawtext, text, refuri=url, **options)\n    return [node], []", "output": "Link to a URL in the Bokeh GitHub tree, pointing to appropriate tags\n    for releases, or to master otherwise.\n\n    The link text is simply the URL path supplied, so typical usage might\n    look like:\n\n    .. code-block:: none\n\n        All of the examples are located in the :bokeh-tree:`examples`\n        subdirectory of your Bokeh checkout.\n\n    Returns 2 part tuple containing list of nodes to insert into the\n    document and a list of system messages.  Both are allowed to be\n    empty.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_cluster(datacenter=None, cluster=None, service_instance=None):\n    '''\n    \n    '''\n    proxy_type = get_proxy_type()\n    if proxy_type == 'esxdatacenter':\n        dc_ref = _get_proxy_target(service_instance)\n        if not cluster:\n            raise ArgumentValueError('\\'cluster\\' needs to be specified')\n        cluster_ref = salt.utils.vmware.get_cluster(dc_ref, cluster)\n    elif proxy_type == 'esxcluster':\n        cluster_ref = _get_proxy_target(service_instance)\n        cluster = __salt__['esxcluster.get_details']()['cluster']\n    log.trace('Retrieving representation of cluster \\'%s\\' in a %s proxy',\n              cluster, proxy_type)\n    return _get_cluster_dict(cluster, cluster_ref)", "output": "Returns a dict representation of an ESX cluster.\n\n    datacenter\n        Name of datacenter containing the cluster.\n        Ignored if already contained by proxy details.\n        Default value is None.\n\n    cluster\n        Name of cluster.\n        Ignored if already contained by proxy details.\n        Default value is None.\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter.\n        Default is None.\n\n    .. code-block:: bash\n\n        # vcenter proxy\n        salt '*' vsphere.list_cluster datacenter=dc1 cluster=cl1\n\n        # esxdatacenter proxy\n        salt '*' vsphere.list_cluster cluster=cl1\n\n        # esxcluster proxy\n        salt '*' vsphere.list_cluster", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _highlight_extrema(data, color='yellow', max_=True):\n        \"\"\"\n        \n        \"\"\"\n        attr = 'background-color: {0}'.format(color)\n        if data.ndim == 1:  # Series from .apply\n            if max_:\n                extrema = data == data.max()\n            else:\n                extrema = data == data.min()\n            return [attr if v else '' for v in extrema]\n        else:  # DataFrame from .tee\n            if max_:\n                extrema = data == data.max().max()\n            else:\n                extrema = data == data.min().min()\n            return pd.DataFrame(np.where(extrema, attr, ''),\n                                index=data.index, columns=data.columns)", "output": "Highlight the min or max in a Series or DataFrame.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check(self, mode=None):\n        \"\"\"\n        \"\"\"\n        if self.closed:\n            raise IOError(\"%s is closed\" % self.__class__.__name__)\n        if mode is not None and self.mode not in mode:\n            raise IOError(\"bad operation for mode %r\" % self.mode)", "output": "Check if TarFile is still open, and if the operation's mode\n           corresponds to TarFile's mode.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_dateaxis(subplot, freq, index):\n    \"\"\"\n    \n    \"\"\"\n\n    # handle index specific formatting\n    # Note: DatetimeIndex does not use this\n    # interface. DatetimeIndex uses matplotlib.date directly\n    if isinstance(index, ABCPeriodIndex):\n\n        majlocator = TimeSeries_DateLocator(freq, dynamic_mode=True,\n                                            minor_locator=False,\n                                            plot_obj=subplot)\n        minlocator = TimeSeries_DateLocator(freq, dynamic_mode=True,\n                                            minor_locator=True,\n                                            plot_obj=subplot)\n        subplot.xaxis.set_major_locator(majlocator)\n        subplot.xaxis.set_minor_locator(minlocator)\n\n        majformatter = TimeSeries_DateFormatter(freq, dynamic_mode=True,\n                                                minor_locator=False,\n                                                plot_obj=subplot)\n        minformatter = TimeSeries_DateFormatter(freq, dynamic_mode=True,\n                                                minor_locator=True,\n                                                plot_obj=subplot)\n        subplot.xaxis.set_major_formatter(majformatter)\n        subplot.xaxis.set_minor_formatter(minformatter)\n\n        # x and y coord info\n        subplot.format_coord = functools.partial(_format_coord, freq)\n\n    elif isinstance(index, ABCTimedeltaIndex):\n        subplot.xaxis.set_major_formatter(\n            TimeSeries_TimedeltaFormatter())\n    else:\n        raise TypeError('index type not supported')\n\n    pylab.draw_if_interactive()", "output": "Pretty-formats the date axis (x-axis).\n\n    Major and minor ticks are automatically set for the frequency of the\n    current underlying series.  As the dynamic mode is activated by\n    default, changing the limits of the x axis will intelligently change\n    the positions of the ticks.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chunks(l:Collection, n:int)->Iterable:\n    \"\"\n    for i in range(0, len(l), n): yield l[i:i+n]", "output": "Yield successive `n`-sized chunks from `l`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def path_to_url(path):\n    # type: (Union[str, Text]) -> str\n    \"\"\"\n    \n    \"\"\"\n    path = os.path.normpath(os.path.abspath(path))\n    url = urllib_parse.urljoin('file:', urllib_request.pathname2url(path))\n    return url", "output": "Convert a path to a file: URL.  The path will be made absolute and have\n    quoted path parts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_alternative (self, target):\n        \"\"\" \n        \"\"\"\n        assert isinstance(target, BasicTarget)\n        d = target.default_build ()\n\n        if self.alternatives_ and self.default_build_ != d:\n            get_manager().errors()(\"default build must be identical in all alternatives\\n\"\n              \"main target is '%s'\\n\"\n              \"with '%s'\\n\"\n              \"differing from previous default build: '%s'\" % (self.full_name (), d.raw (), self.default_build_.raw ()))\n\n        else:\n            self.default_build_ = d\n\n        self.alternatives_.append (target)", "output": "Add a new alternative for this target.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def splash_url_as(self, *, format='webp', size=2048):\n        \"\"\"\"\"\"\n        return Asset._from_guild_image(self._state, self.id, self.splash, 'splashes', format=format, size=size)", "output": ":class:`Asset`: The same operation as :meth:`Guild.splash_url_as`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_view(self, path: str, handler: AbstractView,\n                 **kwargs: Any) -> AbstractRoute:\n        \"\"\"\n        \n        \"\"\"\n        return self.add_route(hdrs.METH_ANY, path, handler, **kwargs)", "output": "Shortcut for add_route with ANY methods for a class-based view", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_on_path(importer, path_item, only=False):\n    \"\"\"\"\"\"\n    path_item = _normalize_cached(path_item)\n\n    if _is_unpacked_egg(path_item):\n        yield Distribution.from_filename(\n            path_item, metadata=PathMetadata(\n                path_item, os.path.join(path_item, 'EGG-INFO')\n            )\n        )\n        return\n\n    entries = safe_listdir(path_item)\n\n    # for performance, before sorting by version,\n    # screen entries for only those that will yield\n    # distributions\n    filtered = (\n        entry\n        for entry in entries\n        if dist_factory(path_item, entry, only)\n    )\n\n    # scan for .egg and .egg-info in directory\n    path_item_entries = _by_version_descending(filtered)\n    for entry in path_item_entries:\n        fullpath = os.path.join(path_item, entry)\n        factory = dist_factory(path_item, entry, only)\n        for dist in factory(fullpath):\n            yield dist", "output": "Yield distributions accessible on a sys.path directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode_single_feature_from_dict(\n    feature_k,\n    feature,\n    tfexample_dict):\n  \"\"\"\n  \"\"\"\n  # Singleton case\n  if not feature.serialized_keys:\n    data_to_decode = tfexample_dict[feature_k]\n  # Feature contains sub features\n  else:\n    # Extract the sub-features from the global feature dict\n    data_to_decode = {\n        k: tfexample_dict[posixpath.join(feature_k, k)]\n        for k in feature.serialized_keys\n    }\n  return feature.decode_example(data_to_decode)", "output": "Decode the given feature from the tfexample_dict.\n\n  Args:\n    feature_k (str): Feature key in the tfexample_dict\n    feature (FeatureConnector): Connector object to use to decode the field\n    tfexample_dict (dict): Dict containing the data to decode.\n\n  Returns:\n    decoded_feature: The output of the feature.decode_example", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_version_to_install(self, name):\n        \"\"\"\n        \"\"\"\n        version = Version.parse(name)\n        if version.patch is not None:\n            return name\n        try:\n            best_match = max((\n                inst_version\n                for inst_version in self.iter_installable_versions()\n                if inst_version.matches_minor(version)\n            ), key=operator.attrgetter('cmpkey'))\n        except ValueError:\n            raise ValueError(\n                'no installable version found for {0!r}'.format(name),\n            )\n        return best_match", "output": "Find a version in pyenv from the version supplied.\n\n        A ValueError is raised if a matching version cannot be found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_SU_save_order_queue(order_queue, client=DATABASE):\n    \"\"\"\n    \"\"\"\n    collection = client.order_queue\n    collection.create_index(\n        [('account_cookie',\n          ASCENDING),\n         ('order_id',\n          ASCENDING)],\n        unique=True\n    )\n    for order in order_queue.values():\n        order_json = order.to_dict()\n        try:\n            collection.update_one(\n                {\n                    'account_cookie': order_json.get('account_cookie'),\n                    'order_id': order_json.get('order_id')\n                },\n                {'$set': order_json},\n                upsert=True\n            )\n        except Exception as e:\n            print(e)", "output": "\u589e\u91cf\u5b58\u50a8order_queue\n\n    Arguments:\n        order_queue {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_broadly_matched_key(self, broad, string):\n        \"\"\"\"\"\"\n        keys = list(broad.keys())\n        for i in range(0, len(keys)):\n            key = keys[i]\n            if string.find(key) >= 0:\n                return key\n        return None", "output": "A helper method for matching error strings exactly vs broadly", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def change_presence(self, *, activity=None, status=None, afk=False):\n        \"\"\"\n        \"\"\"\n\n        if status is None:\n            status = 'online'\n            status_enum = Status.online\n        elif status is Status.offline:\n            status = 'invisible'\n            status_enum = Status.offline\n        else:\n            status_enum = status\n            status = str(status)\n\n        await self.ws.change_presence(activity=activity, status=status, afk=afk)\n\n        for guild in self._connection.guilds:\n            me = guild.me\n            if me is None:\n                continue\n\n            me.activities = (activity,)\n            me.status = status_enum", "output": "|coro|\n\n        Changes the client's presence.\n\n        The activity parameter is a :class:`.Activity` object (not a string) that represents\n        the activity being done currently. This could also be the slimmed down versions,\n        :class:`.Game` and :class:`.Streaming`.\n\n        Example\n        ---------\n\n        .. code-block:: python3\n\n            game = discord.Game(\"with the API\")\n            await client.change_presence(status=discord.Status.idle, activity=game)\n\n        Parameters\n        ----------\n        activity: Optional[Union[:class:`.Game`, :class:`.Streaming`, :class:`.Activity`]]\n            The activity being done. ``None`` if no currently active activity is done.\n        status: Optional[:class:`.Status`]\n            Indicates what status to change to. If None, then\n            :attr:`.Status.online` is used.\n        afk: :class:`bool`\n            Indicates if you are going AFK. This allows the discord\n            client to know how to handle push notifications better\n            for you in case you are actually idle and not lying.\n\n        Raises\n        ------\n        InvalidArgument\n            If the ``activity`` parameter is not the proper type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tensor(x:Any, *rest)->Tensor:\n    \"\"\n    if len(rest): x = (x,)+rest\n    # XXX: Pytorch bug in dataloader using num_workers>0; TODO: create repro and report\n    if is_listy(x) and len(x)==0: return tensor(0)\n    res = torch.tensor(x) if is_listy(x) else as_tensor(x)\n    if res.dtype is torch.int32:\n        warn('Tensor is int32: upgrading to int64; for better performance use int64 input')\n        return res.long()\n    return res", "output": "Like `torch.as_tensor`, but handle lists too, and can pass multiple vector elements directly.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getattr(self, obj, attribute):\n        \"\"\"\n        \"\"\"\n        try:\n            value = getattr(obj, attribute)\n        except AttributeError:\n            try:\n                return obj[attribute]\n            except (TypeError, LookupError):\n                pass\n        else:\n            if self.is_safe_attribute(obj, attribute, value):\n                return value\n            return self.unsafe_undefined(obj, attribute)\n        return self.undefined(obj=obj, name=attribute)", "output": "Subscribe an object from sandboxed code and prefer the\n        attribute.  The attribute passed *must* be a bytestring.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _draw_rect(self, rect, painter):\n        \"\"\"\n        \n        \"\"\"\n        c = self.editor.sideareas_color\n        grad = QLinearGradient(rect.topLeft(),\n                                     rect.topRight())\n        if sys.platform == 'darwin':\n            grad.setColorAt(0, c.lighter(100))\n            grad.setColorAt(1, c.lighter(110))\n            outline = c.darker(110)\n        else:\n            grad.setColorAt(0, c.lighter(110))\n            grad.setColorAt(1, c.lighter(130))\n            outline = c.darker(100)\n        painter.fillRect(rect, grad)\n        painter.setPen(QPen(outline))\n        painter.drawLine(rect.topLeft() +\n                         QPointF(1, 0),\n                         rect.topRight() -\n                         QPointF(1, 0))\n        painter.drawLine(rect.bottomLeft() +\n                         QPointF(1, 0),\n                         rect.bottomRight() -\n                         QPointF(1, 0))\n        painter.drawLine(rect.topRight() +\n                         QPointF(0, 1),\n                         rect.bottomRight() -\n                         QPointF(0, 1))\n        painter.drawLine(rect.topLeft() +\n                         QPointF(0, 1),\n                         rect.bottomLeft() -\n                         QPointF(0, 1))", "output": "Draw the background rectangle using the current style primitive color.\n\n        :param rect: The fold zone rect to draw\n\n        :param painter: The widget's painter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply(self, func, **kwargs):\n        \"\"\"\n        \"\"\"\n        oid = self.oid\n        self.call_queue.append((func, kwargs))\n\n        def call_queue_closure(oid_obj, call_queues):\n            for func, kwargs in call_queues:\n                if isinstance(func, ray.ObjectID):\n                    func = ray.get(func)\n                if isinstance(kwargs, ray.ObjectID):\n                    kwargs = ray.get(kwargs)\n\n                oid_obj = func(oid_obj, **kwargs)\n\n            return oid_obj\n\n        oid = deploy_ray_func.remote(\n            call_queue_closure, oid, kwargs={\"call_queues\": self.call_queue}\n        )\n        self.call_queue = []\n\n        return PyarrowOnRayFramePartition(oid)", "output": "Apply a function to the object stored in this partition.\n\n        Note: It does not matter if func is callable or an ObjectID. Ray will\n            handle it correctly either way. The keyword arguments are sent as a\n            dictionary.\n\n        Args:\n            func: The function to apply.\n\n        Returns:\n            A RayRemotePartition object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def session_path(cls, project, instance, database, session):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/instances/{instance}/databases/{database}/sessions/{session}\",\n            project=project,\n            instance=instance,\n            database=database,\n            session=session,\n        )", "output": "Return a fully-qualified session string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_all_zones_by_id(region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    ret = describe_hosted_zones(region=region, key=key, keyid=keyid,\n                                profile=profile)\n    return [r['Id'].replace('/hostedzone/', '') for r in ret]", "output": "List, by their IDs, all hosted zones in the bound account.\n\n    region\n        Region to connect to.\n\n    key\n        Secret key to be used.\n\n    keyid\n        Access key to be used.\n\n    profile\n        A dict with region, key and keyid, or a pillar key (string) that\n        contains a dict with region, key and keyid.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_route53.list_all_zones_by_id", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_template(self, name, parent=None, globals=None):\n        \"\"\"\n        \"\"\"\n        if isinstance(name, Template):\n            return name\n        if parent is not None:\n            name = self.join_path(name, parent)\n        return self._load_template(name, self.make_globals(globals))", "output": "Load a template from the loader.  If a loader is configured this\n        method asks the loader for the template and returns a :class:`Template`.\n        If the `parent` parameter is not `None`, :meth:`join_path` is called\n        to get the real template name before loading.\n\n        The `globals` parameter can be used to provide template wide globals.\n        These variables are available in the context at render time.\n\n        If the template does not exist a :exc:`TemplateNotFound` exception is\n        raised.\n\n        .. versionchanged:: 2.4\n           If `name` is a :class:`Template` object it is returned from the\n           function unchanged.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def garbage_collect_exports(export_dir_base, exports_to_keep):\n  \"\"\"\n  \"\"\"\n  if exports_to_keep is None:\n    return\n  version_paths = []  # List of tuples (version, path)\n  for filename in tf_v1.gfile.ListDirectory(export_dir_base):\n    path = os.path.join(\n        tf.compat.as_bytes(export_dir_base),\n        tf.compat.as_bytes(filename))\n    if len(filename) == 10 and filename.isdigit():\n      version_paths.append((int(filename), path))\n\n  oldest_version_path = sorted(version_paths)[:-exports_to_keep]\n  for _, path in oldest_version_path:\n    try:\n      tf_v1.gfile.DeleteRecursively(path)\n    except tf.errors.NotFoundError as e:\n      logging.warn(\"Can not delete %s recursively: %s\", path, e)", "output": "Deletes older exports, retaining only a given number of the most recent.\n\n  Export subdirectories are assumed to be named with monotonically increasing\n  integers; the most recent are taken to be those with the largest values.\n\n  Args:\n    export_dir_base: the base directory under which each export is in a\n      versioned subdirectory.\n    exports_to_keep: Number of exports to keep. Older exports will be garbage\n      collected. Set to None to disable.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_nat_big():\n  \"\"\"\"\"\"\n  hparams = transformer_nat_small()\n  hparams.batch_size = 2048\n  hparams.hidden_size = 1024\n  hparams.filter_size = 4096\n  hparams.num_hidden_layers = 6\n  hparams.num_heads = 16\n  hparams.layer_prepostprocess_dropout = 0.3\n  return hparams", "output": "Set of hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unique(self):\n        \"\"\"\n        \n        \"\"\"\n        from .sframe import SFrame as _SFrame\n\n        tmp_sf = _SFrame()\n        tmp_sf.add_column(self, 'X1', inplace=True)\n\n        res = tmp_sf.groupby('X1',{})\n\n        return SArray(_proxy=res['X1'].__proxy__)", "output": "Get all unique values in the current SArray.\n\n        Raises a TypeError if the SArray is of dictionary type. Will not\n        necessarily preserve the order of the given SArray in the new SArray.\n\n\n        Returns\n        -------\n        out : SArray\n            A new SArray that contains the unique values of the current SArray.\n\n        See Also\n        --------\n        SFrame.unique", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def example_path(cls, project, dataset, annotated_dataset, example):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/datasets/{dataset}/annotatedDatasets/{annotated_dataset}/examples/{example}\",\n            project=project,\n            dataset=dataset,\n            annotated_dataset=annotated_dataset,\n            example=example,\n        )", "output": "Return a fully-qualified example string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_perioddelta(self, freq):\n        \"\"\"\n        \n        \"\"\"\n        # TODO: consider privatizing (discussion in GH#23113)\n        from pandas.core.arrays.timedeltas import TimedeltaArray\n        i8delta = self.asi8 - self.to_period(freq).to_timestamp().asi8\n        m8delta = i8delta.view('m8[ns]')\n        return TimedeltaArray(m8delta)", "output": "Calculate TimedeltaArray of difference between index\n        values and index converted to PeriodArray at specified\n        freq. Used for vectorized offsets\n\n        Parameters\n        ----------\n        freq : Period frequency\n\n        Returns\n        -------\n        TimedeltaArray/Index", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def union(self, *dstreams):\n        \"\"\"\n        \n        \"\"\"\n        if not dstreams:\n            raise ValueError(\"should have at least one DStream to union\")\n        if len(dstreams) == 1:\n            return dstreams[0]\n        if len(set(s._jrdd_deserializer for s in dstreams)) > 1:\n            raise ValueError(\"All DStreams should have same serializer\")\n        if len(set(s._slideDuration for s in dstreams)) > 1:\n            raise ValueError(\"All DStreams should have same slide duration\")\n        cls = SparkContext._jvm.org.apache.spark.streaming.api.java.JavaDStream\n        jdstreams = SparkContext._gateway.new_array(cls, len(dstreams))\n        for i in range(0, len(dstreams)):\n            jdstreams[i] = dstreams[i]._jdstream\n        return DStream(self._jssc.union(jdstreams), self, dstreams[0]._jrdd_deserializer)", "output": "Create a unified DStream from multiple DStreams of the same\n        type and same slide duration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def random_normal(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    try:\n        from onnx.mapping import TENSOR_TYPE_TO_NP_TYPE\n    except ImportError:\n        raise ImportError(\"Onnx and protobuf need to be installed. \"\n                          \"Instructions to install - https://github.com/onnx/onnx\")\n    new_attr = translation_utils._remove_attributes(attrs, ['seed'])\n    new_attr = translation_utils._fix_attribute_names(new_attr, {'mean': 'loc'})\n    new_attr['dtype'] = TENSOR_TYPE_TO_NP_TYPE[int(new_attr.get('dtype', 1))]\n    return 'random_normal', new_attr, inputs", "output": "Draw random samples from a Gaussian distribution.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cov(self, other, min_periods=None):\n        \"\"\"\n        \n        \"\"\"\n        this, other = self.align(other, join='inner', copy=False)\n        if len(this) == 0:\n            return np.nan\n        return nanops.nancov(this.values, other.values,\n                             min_periods=min_periods)", "output": "Compute covariance with Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n            Series with which to compute the covariance.\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result.\n\n        Returns\n        -------\n        float\n            Covariance between Series and other normalized by N-1\n            (unbiased estimator).\n\n        Examples\n        --------\n        >>> s1 = pd.Series([0.90010907, 0.13484424, 0.62036035])\n        >>> s2 = pd.Series([0.12528585, 0.26962463, 0.51111198])\n        >>> s1.cov(s2)\n        -0.01685762652715874", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def print_network_spec(mlmodel_spec, interface_only=False):\n    \"\"\" \n    \"\"\"\n    inputs, outputs, layers_info = summarize_neural_network_spec(mlmodel_spec)\n\n    print('Inputs:')\n    for i in inputs:\n        name, description = i\n        print('  {} {}'.format(name, description))\n\n    print('Outputs:')\n    for o in outputs:\n        name, description = o\n        print('  {} {}'.format(name, description))\n\n    if layers_info is None:\n        print('\\n(This MLModel is not a neural network model or does not contain any layers)')\n\n    if layers_info and not interface_only:\n        print('\\nLayers:')\n        for idx, l in enumerate(layers_info):\n            layer_type, name, in_blobs, out_blobs, params_info = l\n            print('[{}] ({}) {}'.format(idx, layer_type, name))\n            print('  Input blobs: {}'.format(in_blobs))\n            print('  Output blobs: {}'.format(out_blobs))\n            if len(params_info) > 0:\n                print('  Parameters: ')\n            for param in params_info:\n                print('    {} = {}'.format(param[0], param[1]))\n\n    print('\\n')", "output": "Print the network information summary.\n    Args:\n    mlmodel_spec : the mlmodel spec\n    interface_only : Shows only the input and output of the network", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(self, tab_index):\r\n        \"\"\"\"\"\"\r\n        _id = id(self.editor.tabs.widget(tab_index))\r\n        if _id in self.history:\r\n            self.history.remove(_id)", "output": "Remove the widget at the corresponding tab_index.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def modify_ack_deadline(self, seconds):\n        \"\"\"\n        \"\"\"\n        self._request_queue.put(\n            requests.ModAckRequest(ack_id=self._ack_id, seconds=seconds)\n        )", "output": "Resets the deadline for acknowledgement.\n\n        New deadline will be the given value of seconds from now.\n\n        The default implementation handles this for you; you should not need\n        to manually deal with setting ack deadlines. The exception case is\n        if you are implementing your own custom subclass of\n        :class:`~.pubsub_v1.subcriber._consumer.Consumer`.\n\n        Args:\n            seconds (int): The number of seconds to set the lease deadline\n                to. This should be between 0 and 600. Due to network latency,\n                values below 10 are advised against.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render(self, doc, context=None, math_option=False, img_path='',\n               css_path=CSS_PATH):\n        \"\"\"\"\"\"\n        # If the thread is already running wait for it to finish before\n        # starting it again.\n        if self.wait():\n            self.doc = doc\n            self.context = context\n            self.math_option = math_option\n            self.img_path = img_path\n            self.css_path = css_path\n            # This causes run() to be executed in separate thread\n            self.start()", "output": "Start thread to render a given documentation", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_nat(self):\n        \"\"\"\n        \n        \"\"\"\n        if is_period_dtype(self):\n            raise TypeError('Cannot add {cls} and {typ}'\n                            .format(cls=type(self).__name__,\n                                    typ=type(NaT).__name__))\n\n        # GH#19124 pd.NaT is treated like a timedelta for both timedelta\n        # and datetime dtypes\n        result = np.zeros(len(self), dtype=np.int64)\n        result.fill(iNaT)\n        return type(self)(result, dtype=self.dtype, freq=None)", "output": "Add pd.NaT to self", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_sparse(self, fill_value=None, kind='block'):\n        \"\"\"\n        \n        \"\"\"\n        from pandas.core.sparse.api import SparseDataFrame\n        return SparseDataFrame(self._series, index=self.index,\n                               columns=self.columns, default_kind=kind,\n                               default_fill_value=fill_value)", "output": "Convert to SparseDataFrame.\n\n        Implement the sparse version of the DataFrame meaning that any data\n        matching a specific value it's omitted in the representation.\n        The sparse DataFrame allows for a more efficient storage.\n\n        Parameters\n        ----------\n        fill_value : float, default None\n            The specific value that should be omitted in the representation.\n        kind : {'block', 'integer'}, default 'block'\n            The kind of the SparseIndex tracking where data is not equal to\n            the fill value:\n\n            - 'block' tracks only the locations and sizes of blocks of data.\n            - 'integer' keeps an array with all the locations of the data.\n\n            In most cases 'block' is recommended, since it's more memory\n            efficient.\n\n        Returns\n        -------\n        SparseDataFrame\n            The sparse representation of the DataFrame.\n\n        See Also\n        --------\n        DataFrame.to_dense :\n            Converts the DataFrame back to the its dense form.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([(np.nan, np.nan),\n        ...                    (1., np.nan),\n        ...                    (np.nan, 1.)])\n        >>> df\n             0    1\n        0  NaN  NaN\n        1  1.0  NaN\n        2  NaN  1.0\n        >>> type(df)\n        <class 'pandas.core.frame.DataFrame'>\n\n        >>> sdf = df.to_sparse()\n        >>> sdf\n             0    1\n        0  NaN  NaN\n        1  1.0  NaN\n        2  NaN  1.0\n        >>> type(sdf)\n        <class 'pandas.core.sparse.frame.SparseDataFrame'>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def union(self, other):\n        \"\"\"\n        \n        \"\"\"\n        if self._slideDuration != other._slideDuration:\n            raise ValueError(\"the two DStream should have same slide duration\")\n        return self.transformWith(lambda a, b: a.union(b), other, True)", "output": "Return a new DStream by unifying data of another DStream with this DStream.\n\n        @param other: Another DStream having the same interval (i.e., slideDuration)\n                     as this DStream.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy_file_clipboard(self, fnames=None):\r\n        \"\"\"\"\"\"\r\n        if fnames is None:\r\n            fnames = self.get_selected_filenames()\r\n        if not isinstance(fnames, (tuple, list)):\r\n            fnames = [fnames]\r\n        try:\r\n            file_content = QMimeData()\r\n            file_content.setUrls([QUrl.fromLocalFile(_fn) for _fn in fnames])\r\n            cb = QApplication.clipboard()\r\n            cb.setMimeData(file_content, mode=cb.Clipboard)\r\n        except Exception as e:\r\n            QMessageBox.critical(self,\r\n                                 _('File/Folder copy error'),\r\n                                 _(\"Cannot copy this type of file(s) or \"\r\n                                     \"folder(s). The error was:\\n\\n\")\r\n                                 + to_text_string(e))", "output": "Copy file(s)/folders(s) to clipboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_similar_commands(name):\n    \"\"\"\"\"\"\n    from difflib import get_close_matches\n\n    name = name.lower()\n\n    close_commands = get_close_matches(name, commands_dict.keys())\n\n    if close_commands:\n        return close_commands[0]\n    else:\n        return False", "output": "Command name auto-correct.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait(self):\n        '''\n        '''\n\n        ptyproc = self.ptyproc\n        with _wrap_ptyprocess_err():\n            # exception may occur if \"Is some other process attempting\n            # \"job control with our child pid?\"\n            exitstatus = ptyproc.wait()\n        self.status = ptyproc.status\n        self.exitstatus = ptyproc.exitstatus\n        self.signalstatus = ptyproc.signalstatus\n        self.terminated = True\n\n        return exitstatus", "output": "This waits until the child exits. This is a blocking call. This will\n        not read any data from the child, so this will block forever if the\n        child has unread output and has terminated. In other words, the child\n        may have printed output then called exit(), but, the child is\n        technically still alive until its output is read by the parent.\n\n        This method is non-blocking if :meth:`wait` has already been called\n        previously or :meth:`isalive` method returns False.  It simply returns\n        the previously determined exit status.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_rest_server_quick(rest_port):\n    ''''''\n    response = rest_get(check_status_url(rest_port), 5)\n    if response and response.status_code == 200:\n        return True, response\n    return False, None", "output": "Check if restful server is ready, only check once", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_from_storage_write_to_datastore(self,\n                                           batch_size=100,\n                                           allowed_epsilon=None,\n                                           skip_image_ids=None,\n                                           max_num_images=None):\n    \"\"\"\n    \"\"\"\n    if allowed_epsilon is None:\n      allowed_epsilon = copy.copy(DEFAULT_EPSILON)\n    # init dataset batches from data in storage\n    self._dataset_batches = {}\n    # read all blob names from storage\n    images = self._read_image_list(skip_image_ids)\n    if max_num_images:\n      images = images[:max_num_images]\n    for batch_idx, batch_start in enumerate(range(0, len(images), batch_size)):\n      batch = images[batch_start:batch_start+batch_size]\n      batch_id = DATASET_BATCH_ID_PATTERN.format(batch_idx)\n      batch_epsilon = allowed_epsilon[batch_idx % len(allowed_epsilon)]\n      self.add_batch(batch_id, {'epsilon': batch_epsilon})\n      for image_id, image_path in batch:\n        self.add_image(batch_id, image_id,\n                       {'dataset_image_id': os.path.basename(image_path)[:-4],\n                        'image_path': image_path})\n    # write data to datastore\n    self.write_to_datastore()", "output": "Initializes dataset batches from the list of images in the datastore.\n\n    Args:\n      batch_size: batch size\n      allowed_epsilon: list of allowed epsilon or None to use default\n      skip_image_ids: list of image ids to skip\n      max_num_images: maximum number of images to read", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _date_to_json(value):\n    \"\"\"\"\"\"\n    if isinstance(value, datetime.date):\n        value = value.isoformat()\n    return value", "output": "Coerce 'value' to an JSON-compatible representation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def auth_add(user, auth):\n    '''\n    \n    '''\n    ret = {}\n\n    ## validate auths\n    auths = auth.split(',')\n    known_auths = auth_list().keys()\n    valid_auths = [r for r in auths if r in known_auths]\n    log.debug(\n        'rbac.auth_add - auths=%s, known_auths=%s, valid_auths=%s',\n        auths,\n        known_auths,\n        valid_auths,\n    )\n\n    ## update user auths\n    if valid_auths:\n        res = __salt__['cmd.run_all']('usermod -A \"{auths}\" {login}'.format(\n            login=user,\n            auths=','.join(set(auth_get(user, False) + valid_auths)),\n        ))\n        if res['retcode'] > 0:\n            ret['Error'] = {\n                'retcode': res['retcode'],\n                'message': res['stderr'] if 'stderr' in res else res['stdout']\n            }\n            return ret\n\n    ## update return value\n    active_auths = auth_get(user, False)\n    for a in auths:\n        if a not in valid_auths:\n            ret[a] = 'Unknown'\n        elif a in active_auths:\n            ret[a] = 'Added'\n        else:\n            ret[a] = 'Failed'\n\n    return ret", "output": "Add authorization to user\n\n    user : string\n        username\n    auth : string\n        authorization name\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rbac.auth_add martine solaris.zone.manage\n        salt '*' rbac.auth_add martine solaris.zone.manage,solaris.mail.mailq", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __convert_enum(node):\n    \"\"\"\"\"\"\n    name = __get_attribute(node, 'Name')\n    logging.debug('Found EnumProperty named %s', name)\n\n    converted_values = []\n\n    for value in node.getElementsByTagName('EnumValue'):\n        converted = __convert_node(value)\n\n        converted['value'] = converted['name']\n        converted['name'] = name\n\n        # Modify flags when there is an argument child\n        __with_argument(value, converted)\n\n        converted_values.append(converted)\n\n    return converted_values", "output": "Converts an EnumProperty node to JSON format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_pretrained_file_names(embedding_name=None):\n    \"\"\"\n    \"\"\"\n\n    text_embedding_reg = registry.get_registry(_TokenEmbedding)\n\n    if embedding_name is not None:\n        if embedding_name not in text_embedding_reg:\n            raise KeyError('Cannot find `embedding_name` %s. Use '\n                           '`get_pretrained_file_names('\n                           'embedding_name=None).keys()` to get all the valid embedding '\n                           'names.' % embedding_name)\n        return list(text_embedding_reg[embedding_name].pretrained_file_name_sha1.keys())\n    else:\n        return {embedding_name: list(embedding_cls.pretrained_file_name_sha1.keys())\n                for embedding_name, embedding_cls in registry.get_registry(_TokenEmbedding).items()}", "output": "Get valid token embedding names and their pre-trained file names.\n\n\n    To load token embedding vectors from an externally hosted pre-trained token embedding file,\n    such as those of GloVe and FastText, one should use\n    `mxnet.contrib.text.embedding.create(embedding_name, pretrained_file_name)`.\n    This method returns all the valid names of `pretrained_file_name` for the specified\n    `embedding_name`. If `embedding_name` is set to None, this method returns all the valid\n    names of `embedding_name` with their associated `pretrained_file_name`.\n\n\n    Parameters\n    ----------\n    embedding_name : str or None, default None\n        The pre-trained token embedding name.\n\n\n    Returns\n    -------\n    dict or list:\n        A list of all the valid pre-trained token embedding file names (`pretrained_file_name`)\n        for the specified token embedding name (`embedding_name`). If the text embeding name is\n        set to None, returns a dict mapping each valid token embedding name to a list of valid\n        pre-trained files (`pretrained_file_name`). They can be plugged into\n        `mxnet.contrib.text.embedding.create(embedding_name,\n        pretrained_file_name)`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dmol_neg_log_perplexity(predictions,\n                            labels,\n                            weights_fn=None):\n  \"\"\"\"\"\"\n  del weights_fn  # Unused\n  num, den = common_layers.dml_loss(\n      predictions, labels, reduce_sum=False)\n  return (-num, den)", "output": "Average log-perplexity excluding padding 0s. No smoothing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_options(ret=None):\n    '''\n    \n    '''\n    defaults = {'host': 'salt',\n                'user': 'salt',\n                'pass': 'salt',\n                'db': 'salt',\n                'port': 3306,\n                'ssl_ca': None,\n                'ssl_cert': None,\n                'ssl_key': None}\n\n    attrs = {'host': 'host',\n             'user': 'user',\n             'pass': 'pass',\n             'db': 'db',\n             'port': 'port',\n             'ssl_ca': 'ssl_ca',\n             'ssl_cert': 'ssl_cert',\n             'ssl_key': 'ssl_key'}\n\n    _options = salt.returners.get_returner_options(__virtualname__,\n                                                   ret,\n                                                   attrs,\n                                                   __salt__=__salt__,\n                                                   __opts__=__opts__,\n                                                   defaults=defaults)\n    # post processing\n    for k, v in six.iteritems(_options):\n        if isinstance(v, six.string_types) and v.lower() == 'none':\n            # Ensure 'None' is rendered as None\n            _options[k] = None\n        if k == 'port':\n            # Ensure port is an int\n            _options[k] = int(v)\n\n    return _options", "output": "Returns options used for the MySQL connection.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def debugDumpString(output, str):\n    \"\"\" \"\"\"\n    if output is not None: output.flush()\n    libxml2mod.xmlDebugDumpString(output, str)", "output": "Dumps informations about the string, shorten it if necessary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def PrintIndented(self, file, ident, code):\n        \"\"\"\"\"\"\n        for entry in code:\n            print >>file, '%s%s' % (ident, entry)", "output": "Takes an array, add indentation to each entry and prints it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write(self, output):\n        \"\"\"\n        \n        \"\"\"\n        self._stream.write(output)\n\n        if self._auto_flush:\n            self._stream.flush()", "output": "Writes specified text to the underlying stream\n\n        Parameters\n        ----------\n        output bytes-like object\n            Bytes to write", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_sari(source_ids, prediction_ids, target_ids, max_gram_size=4):\n  \"\"\"\n  \"\"\"\n\n  def get_sari_numpy(source_ids, prediction_ids, target_ids):\n    \"\"\"Iterate over elements in the batch and call the SARI function.\"\"\"\n    sari_scores = []\n    keep_scores = []\n    add_scores = []\n    deletion_scores = []\n    # Iterate over elements in the batch.\n    for source_ids_i, prediction_ids_i, target_ids_i in zip(\n        source_ids, prediction_ids, target_ids):\n      sari, keep, add, deletion = get_sari_score(\n          source_ids_i, prediction_ids_i, target_ids_i, max_gram_size,\n          BETA_FOR_SARI_DELETION_F_MEASURE)\n      sari_scores.append(sari)\n      keep_scores.append(keep)\n      add_scores.append(add)\n      deletion_scores.append(deletion)\n    return (np.asarray(sari_scores), np.asarray(keep_scores),\n            np.asarray(add_scores), np.asarray(deletion_scores))\n\n  sari, keep, add, deletion = tf.py_func(\n      get_sari_numpy,\n      [source_ids, prediction_ids, target_ids],\n      [tf.float64, tf.float64, tf.float64, tf.float64])\n  return sari, keep, add, deletion", "output": "Computes the SARI scores from the given source, prediction and targets.\n\n  Args:\n    source_ids: A 2D tf.Tensor of size (batch_size , sequence_length)\n    prediction_ids: A 2D tf.Tensor of size (batch_size, sequence_length)\n    target_ids: A 3D tf.Tensor of size (batch_size, number_of_targets,\n        sequence_length)\n    max_gram_size: int. largest n-gram size we care about (e.g. 3 for unigrams,\n        bigrams, and trigrams)\n\n  Returns:\n    A 4-tuple of 1D float Tensors of size (batch_size) for the SARI score and\n        the keep, addition and deletion scores.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(self, client=None):\n        \"\"\"\n        \"\"\"\n        client = self._require_client(client)\n        client.sinks_api.sink_delete(self.project, self.name)", "output": "API call:  delete a sink via a DELETE request\n\n        See\n        https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.sinks/delete\n\n        :type client: :class:`~google.cloud.logging.client.Client` or\n                      ``NoneType``\n        :param client: the client to use.  If not passed, falls back to the\n                       ``client`` stored on the current sink.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _handle_event_socket_recv(self, raw):\n        '''\n        \n        '''\n        mtag, data = self.event.unpack(raw, self.event.serial)\n\n        # see if we have any futures that need this info:\n        for (tag, matcher), futures in six.iteritems(self.tag_map):\n            try:\n                is_matched = matcher(mtag, tag)\n            except Exception:\n                log.error('Failed to run a matcher.', exc_info=True)\n                is_matched = False\n\n            if not is_matched:\n                continue\n\n            for future in futures:\n                if future.done():\n                    continue\n                future.set_result({'data': data, 'tag': mtag})\n                self.tag_map[(tag, matcher)].remove(future)\n                if future in self.timeout_map:\n                    tornado.ioloop.IOLoop.current().remove_timeout(self.timeout_map[future])\n                    del self.timeout_map[future]", "output": "Callback for events on the event sub socket", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Get(self, flags, off):\n        \"\"\"\n        \n        \"\"\"\n        N.enforce_number(off, N.UOffsetTFlags)\n        return flags.py_type(encode.Get(flags.packer_type, self.Bytes, off))", "output": "Get retrieves a value of the type specified by `flags`  at the\n        given offset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_port_pin(name_str):\n    \"\"\"\"\"\"\n    if len(name_str) < 3:\n        raise ValueError(\"Expecting pin name to be at least 3 charcters.\")\n    if name_str[0] != 'P':\n        raise ValueError(\"Expecting pin name to start with P\")\n    if name_str[1] < 'A' or name_str[1] > 'K':\n        raise ValueError(\"Expecting pin port to be between A and K\")\n    port = ord(name_str[1]) - ord('A')\n    pin_str = name_str[2:]\n    if not pin_str.isdigit():\n        raise ValueError(\"Expecting numeric pin number.\")\n    return (port, int(pin_str))", "output": "Parses a string and returns a (port-num, pin-num) tuple.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_set(set=None, family='ipv4'):\n    '''\n    \n\n    '''\n    if not set:\n        return 'Error: Set needs to be specified'\n\n    setinfo = _find_set_info(set)\n    if not setinfo:\n        return False\n    return True", "output": "Check that given ipset set exists.\n\n    .. versionadded:: 2014.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ipset.check_set setname", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write(text, filename, encoding='utf-8', mode='wb'):\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    text, encoding = encode(text, encoding)\r\n    if 'a' in mode:\r\n        with open(filename, mode) as textfile:\r\n            textfile.write(text)\r\n    else:\r\n        with atomic_write(filename,\r\n                          overwrite=True,\r\n                          mode=mode) as textfile:\r\n            textfile.write(text)\r\n    return encoding", "output": "Write 'text' to file ('filename') assuming 'encoding' in an atomic way\r\n    Return (eventually new) encoding", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _rotate(degrees:uniform):\n    \"\"\n    angle = degrees * math.pi / 180\n    return [[cos(angle), -sin(angle), 0.],\n            [sin(angle),  cos(angle), 0.],\n            [0.        ,  0.        , 1.]]", "output": "Rotate image by `degrees`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_namespaced_service(self, name, namespace, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_service_with_http_info(name, namespace, body, **kwargs)\n        else:\n            (data) = self.replace_namespaced_service_with_http_info(name, namespace, body, **kwargs)\n            return data", "output": "replace the specified Service\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_service(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Service (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1Service body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1Service\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def route_filter_rules_list(route_filter, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    result = {}\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        rules = __utils__['azurearm.paged_object_to_list'](\n            netconn.route_filter_rules.list_by_route_filter(\n                resource_group_name=resource_group,\n                route_filter_name=route_filter\n            )\n        )\n\n        for rule in rules:\n            result[rule['name']] = rule\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    List all routes within a route filter.\n\n    :param route_filter: The route filter to query.\n\n    :param resource_group: The resource group name assigned to the\n        route filter.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.route_filter_rules_list test-filter testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def traverse (target, include_roots = False, include_sources = False):\n    \"\"\" \n    \"\"\"\n    assert isinstance(target, VirtualTarget)\n    assert isinstance(include_roots, (int, bool))\n    assert isinstance(include_sources, (int, bool))\n    result = []\n\n    if target.action ():\n        action = target.action ()\n\n        # This includes 'target' as well\n        result += action.targets ()\n\n        for t in action.sources ():\n\n            # FIXME:\n            # TODO: see comment in Manager.register_object ()\n            #if not isinstance (t, VirtualTarget):\n            #    t = target.project_.manager_.get_object (t)\n\n            if not t.root ():\n                result += traverse (t, include_roots, include_sources)\n\n            elif include_roots:\n                result.append (t)\n\n    elif include_sources:\n        result.append (target)\n\n    return result", "output": "Traverses the dependency graph of 'target' and return all targets that will\n        be created before this one is created. If root of some dependency graph is\n        found during traversal, it's either included or not, dependencing of the\n        value of 'include_roots'. In either case, sources of root are not traversed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pair(address, key):\n    '''\n    \n    '''\n    if not salt.utils.validate.net.mac(address):\n        raise CommandExecutionError(\n            'Invalid BD address passed to bluetooth.pair'\n        )\n\n    try:\n        int(key)\n    except Exception:\n        raise CommandExecutionError(\n            'bluetooth.pair requires a numerical key to be used'\n        )\n\n    addy = address_()\n    cmd = 'echo {0} | bluez-simple-agent {1} {2}'.format(\n        _cmd_quote(addy['device']), _cmd_quote(address), _cmd_quote(key)\n    )\n    out = __salt__['cmd.run'](cmd, python_shell=True).splitlines()\n    return out", "output": "Pair the bluetooth adapter with a device\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bluetooth.pair DE:AD:BE:EF:CA:FE 1234\n\n    Where DE:AD:BE:EF:CA:FE is the address of the device to pair with, and 1234\n    is the passphrase.\n\n    TODO: This function is currently broken, as the bluez-simple-agent program\n    no longer ships with BlueZ >= 5.0. It needs to be refactored.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check(\n    state,\n    unused=False,\n    style=False,\n    ignore=None,\n    args=None,\n    **kwargs\n):\n    \"\"\"\"\"\"\n    from ..core import do_check\n\n    do_check(\n        three=state.three,\n        python=state.python,\n        system=state.system,\n        unused=unused,\n        ignore=ignore,\n        args=args,\n        pypi_mirror=state.pypi_mirror,\n    )", "output": "Checks for security vulnerabilities and against PEP 508 markers provided in Pipfile.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sample_categorical(prob, rng):\n    \"\"\"\n    \"\"\"\n    ret = numpy.empty(prob.shape[0], dtype=numpy.float32)\n    for ind in range(prob.shape[0]):\n        ret[ind] = numpy.searchsorted(numpy.cumsum(prob[ind]), rng.rand()).clip(min=0.0,\n                                                                                max=prob.shape[\n                                                                                        1] - 0.5)\n    return ret", "output": "Sample from independent categorical distributions\n\n    Each batch is an independent categorical distribution.\n\n    Parameters\n    ----------\n    prob : numpy.ndarray\n      Probability of the categorical distribution. Shape --> (batch_num, category_num)\n    rng : numpy.random.RandomState\n\n    Returns\n    -------\n    ret : numpy.ndarray\n      Sampling result. Shape --> (batch_num,)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bitonic_sort(arr, reverse=False):\n    \"\"\"\n    \n    \"\"\"\n    def compare(arr, reverse):\n        n = len(arr)//2\n        for i in range(n):\n            if reverse != (arr[i] > arr[i+n]):\n                arr[i], arr[i+n] = arr[i+n], arr[i]\n        return arr\n\n    def bitonic_merge(arr, reverse):\n        n = len(arr)\n        \n        if n <= 1:\n            return arr\n        \n        arr = compare(arr, reverse)\n        left = bitonic_merge(arr[:n // 2], reverse)\n        right = bitonic_merge(arr[n // 2:], reverse)\n        return left + right\n    \n    #end of function(compare and bitionic_merge) definition\n    n = len(arr)\n    if n <= 1:\n        return arr\n    # checks if n is power of two\n    if not (n and (not(n & (n - 1))) ):\n        raise ValueError(\"the size of input should be power of two\")\n    \n    left = bitonic_sort(arr[:n // 2], True)\n    right = bitonic_sort(arr[n // 2:], False)\n\n    arr = bitonic_merge(left + right, reverse)\n        \n    return arr", "output": "bitonic sort is sorting algorithm to use multiple process, but this code not containing parallel process\n    It can sort only array that sizes power of 2\n    It can sort array in both increasing order and decreasing order by giving argument true(increasing) and false(decreasing)\n    \n    Worst-case in parallel: O(log(n)^2)\n    Worst-case in non-parallel: O(nlog(n)^2)\n    \n    reference: https://en.wikipedia.org/wiki/Bitonic_sorter", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_dtype_equal(source, target):\n    \"\"\"\n    \n    \"\"\"\n\n    try:\n        source = _get_dtype(source)\n        target = _get_dtype(target)\n        return source == target\n    except (TypeError, AttributeError):\n\n        # invalid comparison\n        # object == category will hit this\n        return False", "output": "Check if two dtypes are equal.\n\n    Parameters\n    ----------\n    source : The first dtype to compare\n    target : The second dtype to compare\n\n    Returns\n    ----------\n    boolean\n        Whether or not the two dtypes are equal.\n\n    Examples\n    --------\n    >>> is_dtype_equal(int, float)\n    False\n    >>> is_dtype_equal(\"int\", int)\n    True\n    >>> is_dtype_equal(object, \"category\")\n    False\n    >>> is_dtype_equal(CategoricalDtype(), \"category\")\n    True\n    >>> is_dtype_equal(DatetimeTZDtype(), \"datetime64\")\n    False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def member_present(ip, port, balancer_id, profile, **libcloud_kwargs):\n    '''\n    \n    '''\n    existing_members = __salt__['libcloud_loadbalancer.list_balancer_members'](balancer_id, profile)\n    for member in existing_members:\n        if member['ip'] == ip and member['port'] == port:\n            return state_result(True, \"Member already present\", balancer_id)\n    member = __salt__['libcloud_loadbalancer.balancer_attach_member'](balancer_id, ip, port, profile, **libcloud_kwargs)\n    return state_result(True, \"Member added to balancer, id: {0}\".format(member['id']), balancer_id, member)", "output": "Ensure a load balancer member is present\n\n    :param ip: IP address for the new member\n    :type  ip: ``str``\n\n    :param port: Port for the new member\n    :type  port: ``int``\n\n    :param balancer_id: id of a load balancer you want to attach the member to\n    :type  balancer_id: ``str``\n\n    :param profile: The profile key\n    :type  profile: ``str``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, timeout=None):  # pylint: disable=arguments-differ\n        \"\"\"\n        \"\"\"\n        if timeout is None:\n            timeout = self.default_timeout\n\n        session = self._sessions.get(block=True, timeout=timeout)\n\n        if not session.exists():\n            session = self._database.session()\n            session.create()\n\n        return session", "output": "Check a session out from the pool.\n\n        :type timeout: int\n        :param timeout: seconds to block waiting for an available session\n\n        :rtype: :class:`~google.cloud.spanner_v1.session.Session`\n        :returns: an existing session from the pool, or a newly-created\n                  session.\n        :raises: :exc:`six.moves.queue.Empty` if the queue is empty.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(s):\n        \"\"\"\n        \n        \"\"\"\n        start = s.find('(')\n        if start == -1:\n            raise ValueError(\"Tuple should start with '('\")\n        end = s.find(')')\n        if end == -1:\n            raise ValueError(\"Tuple should end with ')'\")\n        s = s[start + 1: end].strip()\n\n        size = s[: s.find(',')]\n        try:\n            size = int(size)\n        except ValueError:\n            raise ValueError(\"Cannot parse size %s.\" % size)\n\n        ind_start = s.find('[')\n        if ind_start == -1:\n            raise ValueError(\"Indices array should start with '['.\")\n        ind_end = s.find(']')\n        if ind_end == -1:\n            raise ValueError(\"Indices array should end with ']'\")\n        new_s = s[ind_start + 1: ind_end]\n        ind_list = new_s.split(',')\n        try:\n            indices = [int(ind) for ind in ind_list if ind]\n        except ValueError:\n            raise ValueError(\"Unable to parse indices from %s.\" % new_s)\n        s = s[ind_end + 1:].strip()\n\n        val_start = s.find('[')\n        if val_start == -1:\n            raise ValueError(\"Values array should start with '['.\")\n        val_end = s.find(']')\n        if val_end == -1:\n            raise ValueError(\"Values array should end with ']'.\")\n        val_list = s[val_start + 1: val_end].split(',')\n        try:\n            values = [float(val) for val in val_list if val]\n        except ValueError:\n            raise ValueError(\"Unable to parse values from %s.\" % s)\n        return SparseVector(size, indices, values)", "output": "Parse string representation back into the SparseVector.\n\n        >>> SparseVector.parse(' (4, [0,1 ],[ 4.0,5.0] )')\n        SparseVector(4, {0: 4.0, 1: 5.0})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ping(self, data: bytes = b\"\") -> None:\n        \"\"\"\n\n        \"\"\"\n        data = utf8(data)\n        if self.protocol is None:\n            raise WebSocketClosedError()\n        self.protocol.write_ping(data)", "output": "Send ping frame to the remote end.\n\n        The data argument allows a small amount of data (up to 125\n        bytes) to be sent as a part of the ping message. Note that not\n        all websocket implementations expose this data to\n        applications.\n\n        Consider using the ``ping_interval`` argument to\n        `websocket_connect` instead of sending pings manually.\n\n        .. versionadded:: 5.1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_json(cls, json_str):\n        \"\"\"\n        \"\"\"\n        vocab_dict = json.loads(json_str)\n\n        unknown_token = vocab_dict.get('unknown_token')\n        bert_vocab = cls(unknown_token=unknown_token)\n        bert_vocab._idx_to_token = vocab_dict.get('idx_to_token')\n        bert_vocab._token_to_idx = vocab_dict.get('token_to_idx')\n        if unknown_token:\n            bert_vocab._token_to_idx = DefaultLookupDict(bert_vocab._token_to_idx[unknown_token],\n                                                         bert_vocab._token_to_idx)\n        bert_vocab._reserved_tokens = vocab_dict.get('reserved_tokens')\n        bert_vocab._padding_token = vocab_dict.get('padding_token')\n        bert_vocab._bos_token = vocab_dict.get('bos_token')\n        bert_vocab._eos_token = vocab_dict.get('eos_token')\n        bert_vocab._mask_token = vocab_dict.get('mask_token')\n        bert_vocab._sep_token = vocab_dict.get('sep_token')\n        bert_vocab._cls_token = vocab_dict.get('cls_token')\n\n        return bert_vocab", "output": "Deserialize BERTVocab object from json string.\n\n        Parameters\n        ----------\n        json_str : str\n            Serialized json string of a BERTVocab object.\n\n        Returns\n        -------\n        BERTVocab", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_app(opts):\n    '''\n    \n    '''\n    apiopts = opts.get(__name__.rsplit('.', 2)[-2], {})  # rest_cherrypy opts\n\n    # Add Salt and salt-api config options to the main CherryPy config dict\n    cherrypy.config['saltopts'] = opts\n    cherrypy.config['apiopts'] = apiopts\n\n    root = API()  # cherrypy app\n    cpyopts = root.get_conf()  # cherrypy app opts\n\n    return root, apiopts, cpyopts", "output": "Returns a WSGI app and a configuration dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def most_by_uncertain(self, y):\n        \"\"\" \n        \"\"\"\n        return self.most_uncertain_by_mask((self.ds.y == y), y)", "output": "Extracts the predicted classes which correspond to the selected class (y) and have probabilities nearest to 1/number_of_classes (eg. 0.5 for 2 classes, 0.33 for 3 classes) for the selected class.\n\n            Arguments:\n                y (int): the selected class\n\n            Returns:\n                idxs (numpy.ndarray): An array of indexes (numpy.ndarray)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dml_loss(pred, labels, weights_fn=_weights_one_third, reduce_sum=True):\n  \"\"\"\n  \"\"\"\n  real_labels = convert_rgb_to_symmetric_real(labels)\n  dml_loss_value = discretized_mix_logistic_loss(pred=pred, labels=real_labels)\n  weights = weights_fn(labels)\n  loss_num = weights * dml_loss_value\n  loss_den = weights_nonzero(weights)\n  if reduce_sum:\n    loss_num = tf.reduce_sum(loss_num)\n    loss_den = tf.reduce_sum(loss_den)\n  return loss_num, loss_den", "output": "Discretized mixture of logistics loss.\n\n  Args:\n    pred: A [batch, height, width, num_mixtures*10] tensor of floats\n      comprising one unconstrained mixture probability, three means\n      (one per channel), three standard deviations (one per channel),\n      and three coefficients which linearly parameterize dependence across\n      channels.\n    labels: A [batch, height, width, channels] tensor of 8-bit pixel\n      intensities. The computation assumes channels is 3.\n    weights_fn: A function of labels, returning a Tensor of shape\n      [batch, height, width] which weights each loss term. Default is to scale\n      each loss term by 1/3 so that they capture the average across channels.\n    reduce_sum: A boolean, to return scalar loss instead of per position.\n\n  Returns:\n    Tuple of loss tensors for numerator and denominator, each a scalar if\n    reduce_sum else of shape [batch, height, width]. The sum of their divisions\n    is the number of nats for each pixel in labels.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nested_map(x, f):\n  \"\"\"\"\"\"\n  if isinstance(x, list):\n    return [nested_map(y, f) for y in x]\n  if isinstance(x, tuple):\n    return tuple([nested_map(y, f) for y in x])\n  if isinstance(x, dict):\n    return {k: nested_map(x[k], f) for k in x}\n  return f(x)", "output": "Map the function f to the nested structure x (dicts, tuples, lists).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main_target_sources (self, sources, main_target_name, no_renaming=0):\n        \"\"\"\"\"\"\n        assert is_iterable_typed(sources, basestring)\n        assert isinstance(main_target_name, basestring)\n        assert isinstance(no_renaming, (int, bool))\n        result = []\n\n        for t in sources:\n\n            t = b2.util.jam_to_value_maybe(t)\n\n            if isinstance (t, AbstractTarget):\n                name = t.name ()\n\n                if not no_renaming:\n                    name = main_target_name + '__' + name\n                    t.rename (name)\n\n                # Inline targets are not built by default.\n                p = t.project()\n                p.mark_targets_as_explicit([name])\n                result.append(name)\n\n            else:\n                result.append (t)\n\n        return result", "output": "Return the list of sources to use, if main target rule is invoked\n        with 'sources'. If there are any objects in 'sources', they are treated\n        as main target instances, and the name of such targets are adjusted to\n        be '<name_of_this_target>__<name_of_source_target>'. Such renaming\n        is disabled is non-empty value is passed for 'no-renaming' parameter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fail(self, msg, lineno):\n        \"\"\"\"\"\"\n        raise TemplateAssertionError(msg, lineno, self.name, self.filename)", "output": "Fail with a :exc:`TemplateAssertionError`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rnn_unroll(cell, length, inputs=None, begin_state=None, input_prefix='', layout='NTC'):\n    \"\"\"\"\"\"\n    warnings.warn('rnn_unroll is deprecated. Please call cell.unroll directly.')\n    return cell.unroll(length=length, inputs=inputs, begin_state=begin_state,\n                       input_prefix=input_prefix, layout=layout)", "output": "Deprecated. Please use cell.unroll instead", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def scale_flow(flow, to_unit=True):\n    \"\"\n    s = tensor([flow.size[0]/2,flow.size[1]/2])[None]\n    if to_unit: flow.flow = flow.flow/s-1\n    else:       flow.flow = (flow.flow+1)*s\n    return flow", "output": "Scale the coords in `flow` to -1/1 or the image size depending on `to_unit`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tenant_path(cls, project, tenant):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/tenants/{tenant}\", project=project, tenant=tenant\n        )", "output": "Return a fully-qualified tenant string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_future_result(self):\n        \"\"\"\"\"\"\n        # This must be done in a lock to prevent the polling thread\n        # and main thread from both executing the completion logic\n        # at the same time.\n        with self._completion_lock:\n            # If the operation isn't complete or if the result has already been\n            # set, do not call set_result/set_exception again.\n            # Note: self._result_set is set to True in set_result and\n            # set_exception, in case those methods are invoked directly.\n            if self.state != _DONE_STATE or self._result_set:\n                return\n\n            if self.error_result is not None:\n                exception = _error_result_to_exception(self.error_result)\n                self.set_exception(exception)\n            else:\n                self.set_result(self)", "output": "Set the result or exception from the job if it is complete.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_httplib(ResponseCls, r, **response_kw):\n        \"\"\"\n        \n        \"\"\"\n        headers = r.msg\n\n        if not isinstance(headers, HTTPHeaderDict):\n            if PY3:  # Python 3\n                headers = HTTPHeaderDict(headers.items())\n            else:  # Python 2\n                headers = HTTPHeaderDict.from_httplib(headers)\n\n        # HTTPResponse objects in Python 3 don't have a .strict attribute\n        strict = getattr(r, 'strict', 0)\n        resp = ResponseCls(body=r,\n                           headers=headers,\n                           status=r.status,\n                           version=r.version,\n                           reason=r.reason,\n                           strict=strict,\n                           original_response=r,\n                           **response_kw)\n        return resp", "output": "Given an :class:`httplib.HTTPResponse` instance ``r``, return a\n        corresponding :class:`urllib3.response.HTTPResponse` object.\n\n        Remaining parameters are passed to the HTTPResponse constructor, along\n        with ``original_response=r``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def predict(self, text, k=1, threshold=0.0, on_unicode_error='strict'):\n        \"\"\"\n        \n        \"\"\"\n\n        def check(entry):\n            if entry.find('\\n') != -1:\n                raise ValueError(\n                    \"predict processes one line at a time (remove \\'\\\\n\\')\"\n                )\n            entry += \"\\n\"\n            return entry\n\n        if type(text) == list:\n            text = [check(entry) for entry in text]\n            predictions = self.f.multilinePredict(text, k, threshold, on_unicode_error)\n            dt = np.dtype([('probability', 'float64'), ('label', 'object')])\n            result_as_pair = np.array(predictions, dtype=dt)\n\n            return result_as_pair['label'].tolist(), result_as_pair['probability']\n        else:\n            text = check(text)\n            predictions = self.f.predict(text, k, threshold, on_unicode_error)\n            probs, labels = zip(*predictions)\n\n            return labels, np.array(probs, copy=False)", "output": "Given a string, get a list of labels and a list of\n        corresponding probabilities. k controls the number\n        of returned labels. A choice of 5, will return the 5\n        most probable labels. By default this returns only\n        the most likely label and probability. threshold filters\n        the returned labels by a threshold on probability. A\n        choice of 0.5 will return labels with at least 0.5\n        probability. k and threshold will be applied together to\n        determine the returned labels.\n\n        This function assumes to be given\n        a single line of text. We split words on whitespace (space,\n        newline, tab, vertical tab) and the control characters carriage\n        return, formfeed and the null character.\n\n        If the model is not supervised, this function will throw a ValueError.\n\n        If given a list of strings, it will return a list of results as usually\n        received for a single line of text.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_examples(self, filepath):\n    \"\"\"\n    \"\"\"\n    # Simultaneously iterating through the different data sets in the hdf5\n    # file will be slow with a single file. Instead, we first load everything\n    # into memory before yielding the samples.\n    image_array, values_array = _load_data(filepath)\n\n    # We need to calculate the class labels from the float values in the file.\n    labels_array = np.zeros_like(values_array, dtype=np.int64)\n    for i in range(values_array.shape[1]):\n      labels_array[:, i] = _discretize(values_array[:, i])  # pylint: disable=unsupported-assignment-operation\n\n    for image, labels, values in moves.zip(image_array, labels_array,\n                                           values_array):\n      yield {\n          \"image\": image,\n          \"label_floor_hue\": labels[0],\n          \"label_wall_hue\": labels[1],\n          \"label_object_hue\": labels[2],\n          \"label_scale\": labels[3],\n          \"label_shape\": labels[4],\n          \"label_orientation\": labels[5],\n          \"value_floor_hue\": values[0],\n          \"value_wall_hue\": values[1],\n          \"value_object_hue\": values[2],\n          \"value_scale\": values[3],\n          \"value_shape\": values[4],\n          \"value_orientation\": values[5],\n      }", "output": "Generate examples for the Shapes3d dataset.\n\n    Args:\n      filepath: path to the Shapes3d hdf5 file.\n\n    Yields:\n      Dictionaries with images and the different labels.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def concept_names(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            return self.__dict__[\"concept_names\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'concept_names' \".format(self.order_book_id)\n            )", "output": "[str] \u6982\u5ff5\u80a1\u5206\u7c7b\uff0c\u4f8b\u5982\uff1a\u2019\u94c1\u8def\u57fa\u5efa\u2019\uff0c\u2019\u57fa\u91d1\u91cd\u4ed3\u2019\u7b49\uff08\u80a1\u7968\u4e13\u7528\uff09", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_auth_list(self, load, token=None):\n        '''\n        \n        '''\n        # Get auth list from token\n        if token and self.opts['keep_acl_in_token'] and 'auth_list' in token:\n            return token['auth_list']\n        # Get acl from eauth module.\n        auth_list = self.__get_acl(load)\n        if auth_list is not None:\n            return auth_list\n\n        eauth = token['eauth'] if token else load['eauth']\n        if eauth not in self.opts['external_auth']:\n            # No matching module is allowed in config\n            log.debug('The eauth system \"%s\" is not enabled', eauth)\n            log.warning('Authorization failure occurred.')\n            return None\n\n        if token:\n            name = token['name']\n            groups = token.get('groups')\n        else:\n            name = self.load_name(load)  # The username we are attempting to auth with\n            groups = self.get_groups(load)  # The groups this user belongs to\n        eauth_config = self.opts['external_auth'][eauth]\n        if not eauth_config:\n            log.debug('eauth \"%s\" configuration is empty', eauth)\n\n        if not groups:\n            groups = []\n\n        # We now have an authenticated session and it is time to determine\n        # what the user has access to.\n        auth_list = self.ckminions.fill_auth_list(\n                eauth_config,\n                name,\n                groups)\n\n        auth_list = self.__process_acl(load, auth_list)\n\n        log.trace('Compiled auth_list: %s', auth_list)\n\n        return auth_list", "output": "Retrieve access list for the user specified in load.\n        The list is built by eauth module or from master eauth configuration.\n        Return None if current configuration doesn't provide any ACL for the user. Return an empty\n        list if the user has no rights to execute anything on this master and returns non-empty list\n        if user is allowed to execute particular functions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toLocalIterator(self):\n        \"\"\"\n        \n        \"\"\"\n        with SCCallSiteSync(self.context) as css:\n            sock_info = self.ctx._jvm.PythonRDD.toLocalIteratorAndServe(self._jrdd.rdd())\n        return _load_from_socket(sock_info, self._jrdd_deserializer)", "output": "Return an iterator that contains all of the elements in this RDD.\n        The iterator will consume as much memory as the largest partition in this RDD.\n\n        >>> rdd = sc.parallelize(range(10))\n        >>> [x for x in rdd.toLocalIterator()]\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shap_values(self, X, ranked_outputs=None, output_rank_order='max'):\n        \"\"\" \n        \"\"\"\n        return self.explainer.shap_values(X, ranked_outputs, output_rank_order)", "output": "Return approximate SHAP values for the model applied to the data given by X.\n\n        Parameters\n        ----------\n        X : list,\n            if framework == 'tensorflow': numpy.array, or pandas.DataFrame\n            if framework == 'pytorch': torch.tensor\n            A tensor (or list of tensors) of samples (where X.shape[0] == # samples) on which to\n            explain the model's output.\n\n        ranked_outputs : None or int\n            If ranked_outputs is None then we explain all the outputs in a multi-output model. If\n            ranked_outputs is a positive integer then we only explain that many of the top model\n            outputs (where \"top\" is determined by output_rank_order). Note that this causes a pair\n            of values to be returned (shap_values, indexes), where shap_values is a list of numpy\n            arrays for each of the output ranks, and indexes is a matrix that indicates for each sample\n            which output indexes were choses as \"top\".\n\n        output_rank_order : \"max\", \"min\", or \"max_abs\"\n            How to order the model outputs when using ranked_outputs, either by maximum, minimum, or\n            maximum absolute value.\n\n        Returns\n        -------\n        For a models with a single output this returns a tensor of SHAP values with the same shape\n        as X. For a model with multiple outputs this returns a list of SHAP value tensors, each of\n        which are the same shape as X. If ranked_outputs is None then this list of tensors matches\n        the number of model outputs. If ranked_outputs is a positive integer a pair is returned\n        (shap_values, indexes), where shap_values is a list of tensors with a length of\n        ranked_outputs, and indexes is a matrix that indicates for each sample which output indexes\n        were chosen as \"top\".", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expand (properties):\n    \"\"\" \n    \"\"\"\n    if __debug__:\n        from .property import Property\n        assert is_iterable_typed(properties, Property)\n    expanded = expand_subfeatures(properties)\n    return expand_composites (expanded)", "output": "Given a property set which may consist of composite and implicit\n        properties and combined subfeature values, returns an expanded,\n        normalized property set with all implicit features expressed\n        explicitly, all subfeature values individually expressed, and all\n        components of composite properties expanded. Non-free features\n        directly expressed in the input properties cause any values of\n        those features due to composite feature expansion to be dropped. If\n        two values of a given non-free feature are directly expressed in the\n        input, an error is issued.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def last_date_in_output_for_sid(self, sid):\n        \"\"\"\n        \n        \"\"\"\n        sizes_path = \"{0}/close/meta/sizes\".format(self.sidpath(sid))\n        if not os.path.exists(sizes_path):\n            return pd.NaT\n        with open(sizes_path, mode='r') as f:\n            sizes = f.read()\n        data = json.loads(sizes)\n        # use integer division so that the result is an int\n        # for pandas index later https://github.com/pandas-dev/pandas/blob/master/pandas/tseries/base.py#L247 # noqa\n        num_days = data['shape'][0] // self._minutes_per_day\n        if num_days == 0:\n            # empty container\n            return pd.NaT\n        return self._session_labels[num_days - 1]", "output": "Parameters\n        ----------\n        sid : int\n            Asset identifier.\n\n        Returns\n        -------\n        out : pd.Timestamp\n            The midnight of the last date written in to the output for the\n            given sid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_pipeline(pipeline_id, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    client = _get_client(region, key, keyid, profile)\n    r = {}\n    try:\n        client.delete_pipeline(pipelineId=pipeline_id)\n        r['result'] = True\n    except (botocore.exceptions.BotoCoreError, botocore.exceptions.ClientError) as e:\n        r['error'] = six.text_type(e)\n    return r", "output": "Delete a pipeline, its pipeline definition, and its run history. This function is idempotent.\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt myminion boto_datapipeline.delete_pipeline my_pipeline_id", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _slice_pad_length(num_items, length, overlap=0):\n    \"\"\"\n\n    \"\"\"\n    if length <= overlap:\n        raise ValueError('length needs to be larger than overlap')\n\n    step = length - overlap\n    span = num_items - length\n    residual = span % step\n    if residual:\n        return step - residual\n    else:\n        return 0", "output": "Calculate the padding length needed for sliced samples in order not to discard data.\n\n    Parameters\n    ----------\n    num_items : int\n        Number of items in dataset before collating.\n    length : int\n        The length of each of the samples.\n    overlap : int, default 0\n        The extra number of items in current sample that should overlap with the\n        next sample.\n\n    Returns\n    -------\n    Length of paddings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_params(self):\n    \"\"\"\n    \n    \"\"\"\n\n    if hasattr(self, 'params'):\n      return list(self.params)\n\n    # Catch eager execution and assert function overload.\n    try:\n      if tf.executing_eagerly():\n        raise NotImplementedError(\"For Eager execution - get_params \"\n                                  \"must be overridden.\")\n    except AttributeError:\n      pass\n\n    # For graph-based execution\n    scope_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n                                   self.scope + \"/\")\n\n    if len(scope_vars) == 0:\n      self.make_params()\n      scope_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n                                     self.scope + \"/\")\n      assert len(scope_vars) > 0\n\n    # Make sure no parameters have been added or removed\n    if hasattr(self, \"num_params\"):\n      if self.num_params != len(scope_vars):\n        print(\"Scope: \", self.scope)\n        print(\"Expected \" + str(self.num_params) + \" variables\")\n        print(\"Got \" + str(len(scope_vars)))\n        for var in scope_vars:\n          print(\"\\t\" + str(var))\n        assert False\n    else:\n      self.num_params = len(scope_vars)\n\n    return scope_vars", "output": "Provides access to the model's parameters.\n    :return: A list of all Variables defining the model parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shift_right_2d(x, pad_value=None):\n  \"\"\"\"\"\"\n  if pad_value is None:\n    shifted_targets = tf.pad(x, [[0, 0], [1, 0]])[:, :-1]\n  else:\n    shifted_targets = tf.concat([pad_value, x], axis=1)[:, :-1]\n  return shifted_targets", "output": "Shift the second dimension of x right by one.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_defaults(self, defaults):\n        \"\"\"\"\"\"\n\n        # Accumulate complex default state.\n        self.values = optparse.Values(self.defaults)\n        late_eval = set()\n        # Then set the options with those values\n        for key, val in self._get_ordered_configuration_items():\n            # '--' because configuration supports only long names\n            option = self.get_option('--' + key)\n\n            # Ignore options not present in this parser. E.g. non-globals put\n            # in [global] by users that want them to apply to all applicable\n            # commands.\n            if option is None:\n                continue\n\n            if option.action in ('store_true', 'store_false', 'count'):\n                try:\n                    val = strtobool(val)\n                except ValueError:\n                    error_msg = invalid_config_error_message(\n                        option.action, key, val\n                    )\n                    self.error(error_msg)\n\n            elif option.action == 'append':\n                val = val.split()\n                val = [self.check_default(option, key, v) for v in val]\n            elif option.action == 'callback':\n                late_eval.add(option.dest)\n                opt_str = option.get_opt_string()\n                val = option.convert_value(opt_str, val)\n                # From take_action\n                args = option.callback_args or ()\n                kwargs = option.callback_kwargs or {}\n                option.callback(option, opt_str, val, self, *args, **kwargs)\n            else:\n                val = self.check_default(option, key, val)\n\n            defaults[option.dest] = val\n\n        for key in late_eval:\n            defaults[key] = getattr(self.values, key)\n        self.values = None\n        return defaults", "output": "Updates the given defaults with values from the config files and\n        the environ. Does a little special handling for certain types of\n        options (lists).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def adjust_cells(self):\n        \"\"\"\"\"\"\n        self.resizeColumnsToContents()\n        fm = self.horizontalHeader().fontMetrics()\n        names = [fm.width(s.cmd) for s in self.source_model.servers]\n        if names:\n            self.setColumnWidth(CMD, max(names))\n        self.horizontalHeader().setStretchLastSection(True)", "output": "Adjust column size based on contents.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _iter_service_names():\n    '''\n    \n    '''\n    found = set()\n    for line in glob.glob('/etc/init.d/*'):\n        name = os.path.basename(line)\n        found.add(name)\n        yield name\n\n    # This walk method supports nested services as per the init man page\n    # definition 'For example a configuration file /etc/init/rc-sysinit.conf\n    # is named rc-sysinit, while a configuration file /etc/init/net/apache.conf\n    # is named net/apache'\n    init_root = '/etc/init/'\n    for root, dirnames, filenames in salt.utils.path.os_walk(init_root):\n        relpath = os.path.relpath(root, init_root)\n        for filename in fnmatch.filter(filenames, '*.conf'):\n            if relpath == '.':\n                # service is defined in the root, no need to append prefix.\n                name = filename[:-5]\n            else:\n                # service is nested, append its relative path prefix.\n                name = os.path.join(relpath, filename[:-5])\n            if name in found:\n                continue\n            yield name", "output": "Detect all of the service names available to upstart via init configuration\n    files and via classic sysv init scripts", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download_url(url, destination=None, progress_bar=True):\n    \"\"\"\n    \"\"\"\n\n    def my_hook(t):\n        last_b = [0]\n\n        def inner(b=1, bsize=1, tsize=None):\n            if tsize is not None:\n                t.total = tsize\n            if b > 0:\n                t.update((b - last_b[0]) * bsize)\n            last_b[0] = b\n\n        return inner\n\n    if progress_bar:\n        with tqdm(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:\n            filename, _ = urlretrieve(url, filename=destination, reporthook=my_hook(t))\n    else:\n        filename, _ = urlretrieve(url, filename=destination)", "output": "Download a URL to a local file.\n\n    Parameters\n    ----------\n    url : str\n        The URL to download.\n    destination : str, None\n        The destination of the file. If None is given the file is saved to a temporary directory.\n    progress_bar : bool\n        Whether to show a command-line progress bar while downloading.\n\n    Returns\n    -------\n    filename : str\n        The location of the downloaded file.\n\n    Notes\n    -----\n    Progress bar use/example adapted from tqdm documentation: https://github.com/tqdm/tqdm", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_ndarray_file(nd_bytes):\n    \"\"\"\n    \"\"\"\n    handle = NDListHandle()\n    olen = mx_uint()\n    nd_bytes = bytearray(nd_bytes)\n    ptr = (ctypes.c_char * len(nd_bytes)).from_buffer(nd_bytes)\n    _check_call(_LIB.MXNDListCreate(\n        ptr, len(nd_bytes),\n        ctypes.byref(handle), ctypes.byref(olen)))\n    keys = []\n    arrs = []\n\n    for i in range(olen.value):\n        key = ctypes.c_char_p()\n        cptr = mx_float_p()\n        pdata = ctypes.POINTER(mx_uint)()\n        ndim = mx_uint()\n        _check_call(_LIB.MXNDListGet(\n            handle, mx_uint(i), ctypes.byref(key),\n            ctypes.byref(cptr), ctypes.byref(pdata), ctypes.byref(ndim)))\n        shape = tuple(pdata[:ndim.value])\n        dbuffer = (mx_float * np.prod(shape)).from_address(ctypes.addressof(cptr.contents))\n        ret = np.frombuffer(dbuffer, dtype=np.float32).reshape(shape)\n        ret = np.array(ret, dtype=np.float32)\n        keys.append(py_str(key.value))\n        arrs.append(ret)\n    _check_call(_LIB.MXNDListFree(handle))\n\n    if len(keys) == 0 or len(keys[0]) == 0:\n        return arrs\n    else:\n        return {keys[i] : arrs[i] for i in range(len(keys))}", "output": "Load ndarray file and return as list of numpy array.\n\n    Parameters\n    ----------\n    nd_bytes : str or bytes\n        The internal ndarray bytes\n\n    Returns\n    -------\n    out : dict of str to numpy array or list of numpy array\n        The output list or dict, depending on whether the saved type is list or dict.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _hash_scalar(val, encoding='utf8', hash_key=None):\n    \"\"\"\n    \n    \"\"\"\n\n    if isna(val):\n        # this is to be consistent with the _hash_categorical implementation\n        return np.array([np.iinfo(np.uint64).max], dtype='u8')\n\n    if getattr(val, 'tzinfo', None) is not None:\n        # for tz-aware datetimes, we need the underlying naive UTC value and\n        # not the tz aware object or pd extension type (as\n        # infer_dtype_from_scalar would do)\n        if not isinstance(val, tslibs.Timestamp):\n            val = tslibs.Timestamp(val)\n        val = val.tz_convert(None)\n\n    dtype, val = infer_dtype_from_scalar(val)\n    vals = np.array([val], dtype=dtype)\n\n    return hash_array(vals, hash_key=hash_key, encoding=encoding,\n                      categorize=False)", "output": "Hash scalar value\n\n    Returns\n    -------\n    1d uint64 numpy array of hash value, of length 1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rehash(path, blocksize=1 << 20):\n    # type: (str, int) -> Tuple[str, str]\n    \"\"\"\"\"\"\n    h = hashlib.sha256()\n    length = 0\n    with open(path, 'rb') as f:\n        for block in read_chunks(f, size=blocksize):\n            length += len(block)\n            h.update(block)\n    digest = 'sha256=' + urlsafe_b64encode(\n        h.digest()\n    ).decode('latin1').rstrip('=')\n    # unicode/str python2 issues\n    return (digest, str(length))", "output": "Return (hash, length) for path using hashlib.sha256()", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _unflatten_dict(flat_dict, prefixes):\n  \"\"\"\n\n  \"\"\"\n  original_dict = {}\n  for key, value in flat_dict.items():\n    prefix_found = False\n    for prefix in prefixes:\n      full_prefix = \"__\" + prefix + \"_\"\n      if key.startswith(full_prefix):\n        # Add a dict to the original dict with key=prefix\n        if prefix not in original_dict:\n          original_dict[prefix] = {}\n        original_dict[prefix][key[len(full_prefix):]] = value\n        prefix_found = True\n        break\n    if not prefix_found:\n      # No key matched a prefix in the for loop.\n      original_dict[key] = value\n\n  return original_dict", "output": "Returns a dict of dicts if any prefixes match keys in the flat dict.\n\n    The function handles the case where the prefix may not be a dict.\n\n  Args:\n    flat_dict: A dict without any nesting.\n    prefixes: A list of strings which may have been dicts in the\n      original structure.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_splash(self, message):\r\n        \"\"\"\"\"\"\r\n        if self.splash is None:\r\n            return\r\n        if message:\r\n            logger.info(message)\r\n        self.splash.show()\r\n        self.splash.showMessage(message, Qt.AlignBottom | Qt.AlignCenter |\r\n                                Qt.AlignAbsolute, QColor(Qt.white))\r\n        QApplication.processEvents()", "output": "Set splash message", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read(self, amt=None):\n        \"\"\"\n        \"\"\"\n        chunk = self._raw_stream.read(amt)\n        self._amount_read += len(chunk)\n        return chunk", "output": "Read at most amt bytes from the stream.\n        If the amt argument is omitted, read all data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_state_denotations(self, state: GrammarBasedState, worlds: List[NlvrLanguage]) -> List[bool]:\n        \"\"\"\n        \n        \"\"\"\n        assert state.is_finished(), \"Cannot compute denotations for unfinished states!\"\n        # Since this is a finished state, its group size must be 1.\n        batch_index = state.batch_indices[0]\n        instance_label_strings = state.extras[batch_index]\n        history = state.action_history[0]\n        all_actions = state.possible_actions[0]\n        action_sequence = [all_actions[action][0] for action in history]\n        return self._check_denotation(action_sequence, instance_label_strings, worlds)", "output": "Returns whether action history in the state evaluates to the correct denotations over all\n        worlds. Only defined when the state is finished.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def FromTimedelta(self, td):\n    \"\"\"\"\"\"\n    self._NormalizeDuration(td.seconds + td.days * _SECONDS_PER_DAY,\n                            td.microseconds * _NANOS_PER_MICROSECOND)", "output": "Convertd timedelta to Duration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_servers(*servers, **options):\n\n    '''\n    \n    '''\n\n    test = options.pop('test', False)\n    commit = options.pop('commit', True)\n\n    return __salt__['net.load_template']('delete_ntp_servers',\n                                         servers=servers,\n                                         test=test,\n                                         commit=commit,\n                                         inherit_napalm_device=napalm_device)", "output": "Removes NTP servers configured on the device.\n\n    :param servers: list of IP Addresses/Domain Names to be removed as NTP\n        servers\n    :param test (bool): discard loaded config. By default ``test`` is False\n        (will not dicard the changes)\n    :param commit (bool): commit loaded config. By default ``commit`` is True\n        (will commit the changes). Useful when the user does not want to commit\n        after each change, but after a couple.\n\n    By default this function will commit the config changes (if any). To load\n    without committing, use the ``commit`` option. For dry run use the ``test``\n    argument.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ntp.delete_servers 8.8.8.8 time.apple.com\n        salt '*' ntp.delete_servers 172.17.17.1 test=True  # only displays the diff\n        salt '*' ntp.delete_servers 192.168.0.1 commit=False  # preserves the changes, but does not commit", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolvePublic(self, pubID):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlACatalogResolvePublic(self._o, pubID)\n        return ret", "output": "Try to lookup the catalog local reference associated to a\n           public ID in that catalog", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read_buckets_cache_file(cache_file):\n    '''\n    \n    '''\n\n    log.debug('Reading buckets cache file')\n\n    with salt.utils.files.fopen(cache_file, 'rb') as fp_:\n        data = pickle.load(fp_)\n\n    return data", "output": "Return the contents of the buckets cache file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forward(self, inputs, token_types, valid_length=None):  # pylint: disable=arguments-differ\n        \"\"\"\n        \"\"\"\n        _, pooler_out = self.bert(inputs, token_types, valid_length)\n        return self.classifier(pooler_out)", "output": "Generate the unnormalized score for the given the input sequences.\n\n        Parameters\n        ----------\n        inputs : NDArray, shape (batch_size, seq_length)\n            Input words for the sequences.\n        token_types : NDArray, shape (batch_size, seq_length)\n            Token types for the sequences, used to indicate whether the word belongs to the\n            first sentence or the second one.\n        valid_length : NDArray or None, shape (batch_size)\n            Valid length of the sequence. This is used to mask the padded tokens.\n\n        Returns\n        -------\n        outputs : NDArray\n            Shape (batch_size, num_classes)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def equals(self, rhs):\n    \"\"\"\n    \"\"\"\n\n    try:\n      return isinstance(rhs, self._class_name)\n    except TypeError:\n      # Check raw types if there was a type error.  This is helpful for\n      # things like cStringIO.StringIO.\n      return type(rhs) == type(self._class_name)", "output": "Check to see if the RHS is an instance of class_name.\n\n    Args:\n      # rhs: the right hand side of the test\n      rhs: object\n\n    Returns:\n      bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put_request_payment(Bucket, Payer,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.put_bucket_request_payment(Bucket=Bucket, RequestPaymentConfiguration={\n                'Payer': Payer,\n        })\n        return {'updated': True, 'name': Bucket}\n    except ClientError as e:\n        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Given a valid config, update the request payment configuration for a bucket.\n\n    Returns {updated: true} if request payment configuration was updated and returns\n    {updated: False} if request payment configuration was not updated.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.put_request_payment my_bucket Requester", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_visual_observation_encoder(self, image_input, h_size, activation, num_layers, scope,\n                                          reuse):\n        \"\"\"\n        \n        \"\"\"\n        with tf.variable_scope(scope):\n            conv1 = tf.layers.conv2d(image_input, 16, kernel_size=[8, 8], strides=[4, 4],\n                                     activation=tf.nn.elu, reuse=reuse, name=\"conv_1\")\n            conv2 = tf.layers.conv2d(conv1, 32, kernel_size=[4, 4], strides=[2, 2],\n                                     activation=tf.nn.elu, reuse=reuse, name=\"conv_2\")\n            hidden = c_layers.flatten(conv2)\n\n        with tf.variable_scope(scope + '/' + 'flat_encoding'):\n            hidden_flat = self.create_vector_observation_encoder(hidden, h_size, activation,\n                                                                 num_layers, scope, reuse)\n        return hidden_flat", "output": "Builds a set of visual (CNN) encoders.\n        :param reuse: Whether to re-use the weights within the same scope.\n        :param scope: The scope of the graph within which to create the ops.\n        :param image_input: The placeholder for the image input to use.\n        :param h_size: Hidden layer size.\n        :param activation: What type of activation function to use for layers.\n        :param num_layers: number of hidden layers to create.\n        :return: List of hidden layer tensors.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def valid (names):\n    \"\"\" \n    \"\"\"\n    if isinstance(names, str):\n        names = [names]\n        assert is_iterable_typed(names, basestring)\n\n    return all(name in __all_features for name in names)", "output": "Returns true iff all elements of names are valid features.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dataframe_to_vertex_list(df):\n    \"\"\"\n    \n    \"\"\"\n    cols = df.columns\n    if len(cols):\n        assert _VID_COLUMN in cols, \"Vertex DataFrame must contain column %s\" % _VID_COLUMN\n        df = df[cols].T\n        ret = [Vertex(None, _series=df[col]) for col in df]\n        return ret\n    else:\n        return []", "output": "Convert dataframe into list of vertices, assuming that vertex ids are stored in _VID_COLUMN.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_hash(self, ireq, ireq_hashes=None):\n        \"\"\"\n        \n        \"\"\"\n\n        # We _ALWAYS MUST PRIORITIZE_ the inclusion of hashes from local sources\n        # PLEASE *DO NOT MODIFY THIS* TO CHECK WHETHER AN IREQ ALREADY HAS A HASH\n        # RESOLVED. The resolver will pull hashes from PyPI and only from PyPI.\n        # The entire purpose of this approach is to include missing hashes.\n        # This fixes a race condition in resolution for missing dependency caches\n        # see pypa/pipenv#3289\n        if not self._should_include_hash(ireq):\n            return set()\n        elif self._should_include_hash(ireq) and (\n            not ireq_hashes or ireq.link.scheme == \"file\"\n        ):\n            if not ireq_hashes:\n                ireq_hashes = set()\n            new_hashes = self.resolver.repository._hash_cache.get_hash(ireq.link)\n            ireq_hashes = add_to_set(ireq_hashes, new_hashes)\n        else:\n            ireq_hashes = set(ireq_hashes)\n        # The _ONLY CASE_ where we flat out set the value is if it isn't present\n        # It's a set, so otherwise we *always* need to do a union update\n        if ireq not in self.hashes:\n            return ireq_hashes\n        else:\n            return self.hashes[ireq] | ireq_hashes", "output": "Retrieve hashes for a specific ``InstallRequirement`` instance.\n\n        :param ireq: An ``InstallRequirement`` to retrieve hashes for\n        :type ireq: :class:`~pip_shims.InstallRequirement`\n        :return: A set of hashes.\n        :rtype: Set", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_snapshot_delete(call=None, kwargs=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The image_snapshot_delete function must be called with -f or --function.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    image_id = kwargs.get('image_id', None)\n    image_name = kwargs.get('image_name', None)\n    snapshot_id = kwargs.get('snapshot_id', None)\n\n    if snapshot_id is None:\n        raise SaltCloudSystemExit(\n            'The image_snapshot_delete function requires a \\'snapshot_id\\' to be provided.'\n        )\n\n    if image_id:\n        if image_name:\n            log.warning(\n                'Both the \\'image_id\\' and \\'image_name\\' arguments were provided. '\n                '\\'image_id\\' will take precedence.'\n            )\n    elif image_name:\n        image_id = get_image_id(kwargs={'name': image_name})\n    else:\n        raise SaltCloudSystemExit(\n            'The image_snapshot_delete function requires either an \\'image_id\\' '\n            'or a \\'image_name\\' to be provided.'\n        )\n\n    server, user, password = _get_xml_rpc()\n    auth = ':'.join([user, password])\n    response = server.one.image.snapshotdelete(auth, int(image_id), int(snapshot_id))\n\n    data = {\n        'action': 'image.snapshotdelete',\n        'deleted': response[0],\n        'snapshot_id': response[1],\n        'error_code': response[2],\n    }\n\n    return data", "output": "Deletes a snapshot from the image.\n\n    .. versionadded:: 2016.3.0\n\n    image_id\n        The ID of the image from which to delete the snapshot. Can be used instead of\n        ``image_name``.\n\n    image_name\n        The name of the image from which to delete the snapshot. Can be used instead\n        of ``image_id``.\n\n    snapshot_id\n        The ID of the snapshot to delete.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f image_snapshot_delete vm_id=106 snapshot_id=45\n        salt-cloud -f image_snapshot_delete vm_name=my-vm snapshot_id=111", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_table(self, table, retry=DEFAULT_RETRY, not_found_ok=False):\n        \"\"\"\n        \"\"\"\n        table = _table_arg_to_table_ref(table, default_project=self.project)\n        if not isinstance(table, TableReference):\n            raise TypeError(\"Unable to get TableReference for table '{}'\".format(table))\n\n        try:\n            self._call_api(retry, method=\"DELETE\", path=table.path)\n        except google.api_core.exceptions.NotFound:\n            if not not_found_ok:\n                raise", "output": "Delete a table\n\n        See\n        https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/delete\n\n        Args:\n            table (Union[ \\\n                :class:`~google.cloud.bigquery.table.Table`, \\\n                :class:`~google.cloud.bigquery.table.TableReference`, \\\n                str, \\\n            ]):\n                A reference to the table to delete. If a string is passed in,\n                this method attempts to create a table reference from a\n                string using\n                :func:`google.cloud.bigquery.table.TableReference.from_string`.\n            retry (:class:`google.api_core.retry.Retry`):\n                (Optional) How to retry the RPC.\n            not_found_ok (bool):\n                Defaults to ``False``. If ``True``, ignore \"not found\" errors\n                when deleting the table.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def file_list(saltenv='base', backend=None):\n    '''\n    \n    '''\n    fileserver = salt.fileserver.Fileserver(__opts__)\n    load = {'saltenv': saltenv, 'fsbackend': backend}\n    return fileserver.file_list(load=load)", "output": "Return a list of files from the salt fileserver\n\n    saltenv : base\n        The salt fileserver environment to be listed\n\n    backend\n        Narrow fileserver backends to a subset of the enabled ones. If all\n        passed backends start with a minus sign (``-``), then these backends\n        will be excluded from the enabled backends. However, if there is a mix\n        of backends with and without a minus sign (ex:\n        ``backend=-roots,git``) then the ones starting with a minus sign will\n        be disregarded.\n\n        .. versionadded:: 2015.5.0\n\n    .. note:\n        Keep in mind that executing this function spawns a new process,\n        separate from the master. This means that if the fileserver\n        configuration has been changed in some way since the master has been\n        restarted (e.g. if :conf_master:`fileserver_backend`,\n        :conf_master:`gitfs_remotes`, :conf_master:`hgfs_remotes`, etc. have\n        been updated), then the results of this runner will not accurately\n        reflect what files are available to minions.\n\n        When in doubt, use :py:func:`cp.list_master\n        <salt.modules.cp.list_master>` to see what files the minion can see,\n        and always remember to restart the salt-master daemon when updating\n        the fileserver configuration.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-run fileserver.file_list\n        salt-run fileserver.file_list saltenv=prod\n        salt-run fileserver.file_list saltenv=dev backend=git\n        salt-run fileserver.file_list base hg,roots\n        salt-run fileserver.file_list -git", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_apis_with_config(self, logical_id):\n        \"\"\"\n        \n        \"\"\"\n\n        properties = self._get_properties(logical_id)\n\n        # These configs need to be applied to each API\n        binary_media = sorted(list(properties.binary_media_types))  # Also sort the list to keep the ordering stable\n        cors = properties.cors\n\n        result = []\n        for api in properties.apis:\n            # Create a copy of the API with updated configuration\n            updated_api = api._replace(binary_media_types=binary_media,\n                                       cors=cors)\n            result.append(updated_api)\n\n        return result", "output": "Returns the list of APIs in this resource along with other extra configuration such as binary media types,\n        cors etc. Additional configuration is merged directly into the API data because these properties, although\n        defined globally, actually apply to each API.\n\n        Parameters\n        ----------\n        logical_id : str\n            Logical ID of the resource to fetch data for\n\n        Returns\n        -------\n        list of samcli.commands.local.lib.provider.Api\n            List of APIs with additional configurations for the resource with given logicalId. If there are no APIs,\n            then it returns an empty list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ctype_dict(param_dict):\n    \"\"\"\n    \n    \"\"\"\n    assert(isinstance(param_dict, dict)), \\\n        \"unexpected type for param_dict: \" + str(type(param_dict))\n    c_keys = c_array(ctypes.c_char_p, [c_str(k) for k in param_dict.keys()])\n    c_vals = c_array(ctypes.c_char_p, [c_str(str(v)) for v in param_dict.values()])\n    return (c_keys, c_vals)", "output": "Returns ctype arrays for keys and values(converted to strings) in a dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(name, vmid=None, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The start action must be called with -a or --action.'\n        )\n\n    log.debug('Start: %s (%s) = Start', name, vmid)\n    if not set_vm_status('start', name, vmid=vmid):\n        log.error('Unable to bring VM %s (%s) up..', name, vmid)\n        raise SaltCloudExecutionFailure\n\n    # xxx: TBD: Check here whether the status was actually changed to 'started'\n\n    return {'Started': '{0} was started.'.format(name)}", "output": "Start a node.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a start mymachine", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _locate_index(self, index):\n        \"\"\"\n        \n        \"\"\"\n        assert index >= 0 and index < self.num_images, \"index out of range\"\n        pos = self.image_set_index[index]\n        for k, v in enumerate(self.imdbs):\n            if pos >= v.num_images:\n                pos -= v.num_images\n            else:\n                return (k, pos)", "output": "given index, find out sub-db and sub-index\n\n        Parameters\n        ----------\n        index : int\n            index of a specific image\n\n        Returns\n        ----------\n        a tuple (sub-db, sub-index)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reboot(name, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudException(\n            'The reboot action must be called with -a or --action.'\n        )\n    my_info = _get_my_info(name)\n    profile_name = my_info[name]['profile']\n    profile = __opts__['profiles'][profile_name]\n    host = profile['host']\n    local = salt.client.LocalClient()\n    return local.cmd(host, 'vagrant.reboot', [name])", "output": "Reboot a vagrant minion.\n\n    name\n        The name of the VM to reboot.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a reboot vm_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_logits_name(self):\n    \"\"\"\n    \n    \"\"\"\n    softmax_name = self._get_softmax_name()\n    softmax_layer = self.model.get_layer(softmax_name)\n\n    if not isinstance(softmax_layer, Activation):\n      # In this case, the activation is part of another layer\n      return softmax_name\n\n    if not hasattr(softmax_layer, '_inbound_nodes'):\n      raise RuntimeError(\"Please update keras to version >= 2.1.3\")\n\n    node = softmax_layer._inbound_nodes[0]\n\n    logits_name = node.inbound_layers[0].name\n\n    return logits_name", "output": "Looks for the name of the layer producing the logits.\n    :return: name of layer producing the logits", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bytenet_internal(inputs, targets, hparams):\n  \"\"\"\"\"\"\n  with tf.variable_scope(\"bytenet\"):\n    # Flatten inputs and extend length by 50%.\n    inputs = tf.expand_dims(common_layers.flatten4d3d(inputs), axis=2)\n    extend_length = tf.to_int32(0.5 * tf.to_float(tf.shape(inputs)[1]))\n    inputs_shape = inputs.shape.as_list()\n    inputs = tf.pad(inputs, [[0, 0], [0, extend_length], [0, 0], [0, 0]])\n    inputs_shape[1] = None\n    inputs.set_shape(inputs_shape)  # Don't lose the other shapes when padding.\n    # Pad inputs and targets to be the same length, divisible by 50.\n    inputs, targets = common_layers.pad_to_same_length(\n        inputs, targets, final_length_divisible_by=50)\n    final_encoder = residual_dilated_conv(inputs, hparams.num_block_repeat,\n                                          \"SAME\", \"encoder\", hparams)\n\n    shifted_targets = common_layers.shift_right(targets)\n    kernel = (hparams.kernel_height, hparams.kernel_width)\n    decoder_start = common_layers.conv_block(\n        tf.concat([final_encoder, shifted_targets], axis=3),\n        hparams.hidden_size, [((1, 1), kernel)],\n        padding=\"LEFT\")\n\n    return residual_dilated_conv(decoder_start, hparams.num_block_repeat,\n                                 \"LEFT\", \"decoder\", hparams)", "output": "ByteNet, main step used for training.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        ''''''\n        logger.info(\"result_worker starting...\")\n\n        while not self._quit:\n            try:\n                task, result = self.inqueue.get(timeout=1)\n                self.on_result(task, result)\n            except Queue.Empty as e:\n                continue\n            except KeyboardInterrupt:\n                break\n            except AssertionError as e:\n                logger.error(e)\n                continue\n            except Exception as e:\n                logger.exception(e)\n                continue\n\n        logger.info(\"result_worker exiting...\")", "output": "Run loop", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_rename_function(mapper):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(mapper, (abc.Mapping, ABCSeries)):\n\n        def f(x):\n            if x in mapper:\n                return mapper[x]\n            else:\n                return x\n    else:\n        f = mapper\n\n    return f", "output": "Returns a function that will map names/labels, dependent if mapper\n    is a dict, Series or just a function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe_function(FunctionName, region=None, key=None,\n                      keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        func = _find_function(FunctionName,\n                              region=region, key=key, keyid=keyid, profile=profile)\n        if func:\n            keys = ('FunctionName', 'Runtime', 'Role', 'Handler', 'CodeSha256',\n                    'CodeSize', 'Description', 'Timeout', 'MemorySize',\n                    'FunctionArn', 'LastModified', 'VpcConfig', 'Environment')\n            return {'function': dict([(k, func.get(k)) for k in keys])}\n        else:\n            return {'function': None}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "Given a function name describe its properties.\n\n    Returns a dictionary of interesting properties.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_lambda.describe_function myfunction", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def below(self, objects: Set[Object]) -> Set[Object]:\n        \"\"\"\n        \n        \"\"\"\n        objects_per_box = self._separate_objects_by_boxes(objects)\n        return_set = set()\n        for box in objects_per_box:\n            # max_y_loc corresponds to the bottom-most object.\n            max_y_loc = max([obj.y_loc for obj in objects_per_box[box]])\n            for candidate_obj in box.objects:\n                if candidate_obj.y_loc > max_y_loc:\n                    return_set.add(candidate_obj)\n        return return_set", "output": "Returns the set of objects in the same boxes that are below the given objects. That is, if\n        the input is a set of two objects, one in each box, we will return a union of the objects\n        below the first object in the first box, and those below the second object in the second box.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _output_to_dict(cmdoutput, values_mapper=None):\n    '''\n    \n    '''\n    if isinstance(cmdoutput, dict):\n        if cmdoutput['retcode'] != 0 or cmdoutput['stderr']:\n            raise CommandExecutionError(\n                'RabbitMQ command failed: {0}'.format(cmdoutput['stderr'])\n            )\n        cmdoutput = cmdoutput['stdout']\n\n    ret = {}\n    if values_mapper is None:\n        values_mapper = lambda string: string.split('\\t')\n\n    # remove first and last line: Listing ... - ...done\n    data_rows = _strip_listing_to_done(cmdoutput.splitlines())\n\n    for row in data_rows:\n        try:\n            key, values = row.split('\\t', 1)\n        except ValueError:\n            # If we have reached this far, we've hit an edge case where the row\n            # only has one item: the key. The key doesn't have any values, so we\n            # set it to an empty string to preserve rabbitmq reporting behavior.\n            # e.g. A user's permission string for '/' is set to ['', '', ''],\n            # Rabbitmq reports this only as '/' from the rabbitmqctl command.\n            log.debug('Could not find any values for key \\'%s\\'. '\n                      'Setting to \\'%s\\' to an empty string.', row, row)\n            ret[row] = ''\n            continue\n        ret[key] = values_mapper(values)\n    return ret", "output": "Convert rabbitmqctl output to a dict of data\n    cmdoutput: string output of rabbitmqctl commands\n    values_mapper: function object to process the values part of each line", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _count_spaces_startswith(line):\n    '''\n    \n    '''\n    if line.split('#')[0].strip() == \"\":\n        return None\n\n    spaces = 0\n    for i in line:\n        if i.isspace():\n            spaces += 1\n        else:\n            return spaces", "output": "Count the number of spaces before the first character", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_custom_models(models):\n    \"\"\"\"\"\"\n    if models is None:\n        models = Model.model_class_reverse_map.values()\n\n    custom_models = OrderedDict()\n    for cls in models:\n        impl = getattr(cls, \"__implementation__\", None)\n\n        if impl is not None:\n            model = CustomModel(cls)\n            custom_models[model.full_name] = model\n\n    if not custom_models:\n        return None\n    return custom_models", "output": "Returns CustomModels for models with a custom `__implementation__`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def touch_object(self, objects: Set[Object]) -> Set[Object]:\n        \"\"\"\n        \n        \"\"\"\n        objects_per_box = self._separate_objects_by_boxes(objects)\n        return_set = set()\n        for box, box_objects in objects_per_box.items():\n            candidate_objects = box.objects\n            for object_ in box_objects:\n                for candidate_object in candidate_objects:\n                    if self._objects_touch_each_other(object_, candidate_object):\n                        return_set.add(candidate_object)\n        return return_set", "output": "Returns all objects that touch the given set of objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset ():\n    \"\"\" \n    \"\"\"\n    global __all_attributes, __all_features, __implicit_features, __composite_properties\n    global __subfeature_from_value, __all_top_features, __free_features\n    global __all_subfeatures\n\n    # sets the default value of False for each valid attribute\n    for attr in VALID_ATTRIBUTES:\n        setattr(Feature, attr.replace(\"-\", \"_\"), False)\n\n    # A map containing all features. The key is the feature name.\n    # The value is an instance of Feature class.\n    __all_features = {}\n\n    # All non-subfeatures.\n    __all_top_features = []\n\n    # Maps valus to the corresponding implicit feature\n    __implicit_features = {}\n\n    # A map containing all composite properties. The key is a Property instance,\n    # and the value is a list of Property instances\n    __composite_properties = {}\n\n    # Maps a value to the corresponding subfeature name.\n    __subfeature_from_value = {}\n\n    # All free features\n    __free_features = []\n\n    __all_subfeatures = []", "output": "Clear the module state. This is mainly for testing purposes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_python_script_action(parent, text, icon, package, module, args=[]):\r\n    \"\"\"\"\"\"\r\n    if is_text_string(icon):\r\n        icon = get_icon(icon)\r\n    if programs.python_script_exists(package, module):\r\n        return create_action(parent, text, icon=icon,\r\n                             triggered=lambda:\r\n                             programs.run_python_script(package, module, args))", "output": "Create action to run a GUI based Python script", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_itemgetter(self, obj):\n        \"\"\"\"\"\"\n        class Dummy:\n            def __getitem__(self, item):\n                return item\n        items = obj(Dummy())\n        if not isinstance(items, tuple):\n            items = (items,)\n        return self.save_reduce(operator.itemgetter, items)", "output": "itemgetter serializer (needed for namedtuple support)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def newProp(self, name, value):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlNewProp(self._o, name, value)\n        if ret is None:raise treeError('xmlNewProp() failed')\n        __tmp = xmlAttr(_obj=ret)\n        return __tmp", "output": "Create a new property carried by a node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def push_error_to_driver(worker, error_type, message, driver_id=None):\n    \"\"\"\n    \"\"\"\n    if driver_id is None:\n        driver_id = ray.DriverID.nil()\n    worker.raylet_client.push_error(driver_id, error_type, message,\n                                    time.time())", "output": "Push an error message to the driver to be printed in the background.\n\n    Args:\n        worker: The worker to use.\n        error_type (str): The type of the error.\n        message (str): The message that will be printed in the background\n            on the driver.\n        driver_id: The ID of the driver to push the error message to. If this\n            is None, then the message will be pushed to all drivers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def long_banner(self):\n        \"\"\"\"\"\"\n        # Default banner\n        try:\n            from IPython.core.usage import quick_guide\n        except Exception:\n            quick_guide = ''\n        banner_parts = [\n            'Python %s\\n' % self.interpreter_versions['python_version'],\n            'Type \"copyright\", \"credits\" or \"license\" for more information.\\n\\n',\n            'IPython %s -- An enhanced Interactive Python.\\n' % \\\n            self.interpreter_versions['ipython_version'],\n            quick_guide\n        ]\n        banner = ''.join(banner_parts)\n\n        # Pylab additions\n        pylab_o = self.additional_options['pylab']\n        autoload_pylab_o = self.additional_options['autoload_pylab']\n        mpl_installed = programs.is_module_installed('matplotlib')\n        if mpl_installed and (pylab_o and autoload_pylab_o):\n            pylab_message = (\"\\nPopulating the interactive namespace from \"\n                             \"numpy and matplotlib\\n\")\n            banner = banner + pylab_message\n\n        # Sympy additions\n        sympy_o = self.additional_options['sympy']\n        if sympy_o:\n            lines = \"\"\"\nThese commands were executed:\n>>> from __future__ import division\n>>> from sympy import *\n>>> x, y, z, t = symbols('x y z t')\n>>> k, m, n = symbols('k m n', integer=True)\n>>> f, g, h = symbols('f g h', cls=Function)\n\"\"\"\n            banner = banner + lines\n        if (pylab_o and sympy_o):\n            lines = \"\"\"\nWarning: pylab (numpy and matplotlib) and symbolic math (sympy) are both \nenabled at the same time. Some pylab functions are going to be overrided by \nthe sympy module (e.g. plot)\n\"\"\"\n            banner = banner + lines\n        return banner", "output": "Banner for IPython widgets with pylab message", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_datafeeds(self, datafeed_id=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_ml\", \"datafeeds\", datafeed_id), params=params\n        )", "output": "`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-datafeed.html>`_\n\n        :arg datafeed_id: The ID of the datafeeds to fetch\n        :arg allow_no_datafeeds: Whether to ignore if a wildcard expression\n            matches no datafeeds. (This includes `_all` string or when no\n            datafeeds have been specified)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_powershell_complete(cli, prog_name):\n    \"\"\"\n    \"\"\"\n    commandline = os.environ['COMMANDLINE']\n    args = split_args(commandline)[1:]\n    quote = single_quote\n    incomplete = ''\n    if args and not commandline.endswith(' '):\n        incomplete = args[-1]\n        args = args[:-1]\n        quote_pos = commandline.rfind(incomplete) - 1\n        if quote_pos >= 0 and commandline[quote_pos] == '\"':\n            quote = double_quote\n\n    for item, help in get_choices(cli, prog_name, args, incomplete):\n        echo(quote(item))\n\n    return True", "output": "Do the powershell completion\n\n    Parameters\n    ----------\n    cli : click.Command\n        The main click Command of the program\n    prog_name : str\n        The program name on the command line\n\n    Returns\n    -------\n    bool\n        True if the completion was successful, False otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_documents(self, page_size=None):\n        \"\"\"\n        \"\"\"\n        parent, _ = self._parent_info()\n\n        iterator = self._client._firestore_api.list_documents(\n            parent,\n            self.id,\n            page_size=page_size,\n            show_missing=True,\n            metadata=self._client._rpc_metadata,\n        )\n        iterator.collection = self\n        iterator.item_to_value = _item_to_document_ref\n        return iterator", "output": "List all subdocuments of the current collection.\n\n        Args:\n            page_size (Optional[int]]): The maximum number of documents\n            in each page of results from this request. Non-positive values\n            are ignored. Defaults to a sensible value set by the API.\n\n        Returns:\n            Sequence[~.firestore_v1beta1.collection.DocumentReference]:\n                iterator of subdocuments of the current collection. If the\n                collection does not exist at the time of `snapshot`, the\n                iterator will be empty", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def default(session, django_dep=('django',)):\n    \"\"\"\n    \"\"\"\n\n    # Install all test dependencies, then install this package in-place.\n    deps = UNIT_TEST_DEPS\n    deps += django_dep\n\n    session.install(*deps)\n    for local_dep in LOCAL_DEPS:\n        session.install('-e', local_dep)\n    session.install('-e', '.')\n\n    # Run py.test against the unit tests.\n    session.run(\n        'py.test',\n        '--quiet',\n        '--cov=google.cloud.logging',\n        '--cov=tests.unit',\n        '--cov-append',\n        '--cov-config=.coveragerc',\n        '--cov-report=',\n        '--cov-fail-under=97',\n        'tests/unit',\n        *session.posargs\n    )", "output": "Default unit test session.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_output(script):\n    \"\"\"\"\"\"\n    with logs.debug_time(u'Read output from external shell logger'):\n        commands = _get_last_n(const.SHELL_LOGGER_LIMIT)\n        for command in commands:\n            if command['command'] == script:\n                lines = _get_output_lines(command['output'])\n                output = '\\n'.join(lines).strip()\n                return output\n            else:\n                logs.warn(\"Output isn't available in shell logger\")\n                return None", "output": "Gets command output from shell logger.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _serve_environment(self, request):\n    \"\"\"\n    \"\"\"\n    return http_util.Respond(\n        request,\n        {\n            'data_location': self._logdir or self._db_uri,\n            'mode': 'db' if self._db_uri else 'logdir',\n            'window_title': self._window_title,\n        },\n        'application/json')", "output": "Serve a JSON object containing some base properties used by the frontend.\n\n    * data_location is either a path to a directory or an address to a\n      database (depending on which mode TensorBoard is running in).\n    * window_title is the title of the TensorBoard web page.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate(config):\n    '''\n    \n    '''\n    # Configuration for pkg beacon should be a list\n    if not isinstance(config, list):\n        return False, ('Configuration for pkg beacon must be a list.')\n\n    # Configuration for pkg beacon should contain pkgs\n    pkgs_found = False\n    pkgs_not_list = False\n    for config_item in config:\n        if 'pkgs' in config_item:\n            pkgs_found = True\n            if isinstance(config_item['pkgs'], list):\n                pkgs_not_list = True\n\n    if not pkgs_found or not pkgs_not_list:\n        return False, 'Configuration for pkg beacon requires list of pkgs.'\n    return True, 'Valid beacon configuration'", "output": "Validate the beacon configuration", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __managed_policy_map(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            iam_client = boto3.client('iam')\n            return ManagedPolicyLoader(iam_client).load()\n        except Exception as ex:\n\n            if self._offline_fallback:\n                # If offline flag is set, then fall back to the list of default managed policies\n                # This should be sufficient for most cases\n                with open(self._DEFAULT_MANAGED_POLICIES_FILE, 'r') as fp:\n                    return json.load(fp)\n\n            # Offline is not enabled. So just raise the exception\n            raise ex", "output": "This method is unused and a Work In Progress", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def new_value(self, key, value):\r\n        \"\"\"\"\"\"\r\n        data = self.model.get_data()\r\n        data[key] = value\r\n        self.set_data(data)", "output": "Create new value in data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keep_absolute_resample__r2(X, y, model_generator, method_name, num_fcounts=11):\n    \"\"\" \n    \"\"\"\n    return __run_measure(measures.keep_resample, X, y, model_generator, method_name, 0, num_fcounts, sklearn.metrics.r2_score)", "output": "Keep Absolute (resample)\n    xlabel = \"Max fraction of features kept\"\n    ylabel = \"R^2\"\n    transform = \"identity\"\n    sort_order = 12", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def newDocFragment(self):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlNewDocFragment(self._o)\n        if ret is None:raise treeError('xmlNewDocFragment() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Creation of a new Fragment node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tic(self):\n        \"\"\"\"\"\"\n        if self.step % self.interval == 0:\n            for exe in self.exes:\n                for array in exe.arg_arrays:\n                    array.wait_to_read()\n                for array in exe.aux_arrays:\n                    array.wait_to_read()\n            self.queue = []\n            self.activated = True\n        self.step += 1", "output": "Start collecting stats for current batch.\n        Call before calling forward.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def destroy(instance_id, call=None):\n    '''\n    \n    '''\n    if call == 'function':\n        raise SaltCloudSystemExit(\n            'The destroy action must be called with -d, --destroy, '\n            '-a or --action.'\n        )\n\n    instance_data = show_instance(instance_id, call='action')\n    name = instance_data['instance_name']\n\n    __utils__['cloud.fire_event'](\n        'event',\n        'destroying instance',\n        'salt/cloud/{0}/destroying'.format(name),\n        args={'name': name},\n        sock_dir=__opts__['sock_dir'],\n        transport=__opts__['transport']\n    )\n\n    params = {\n        'action': 'TerminateInstances',\n        'zone': _get_specified_zone(provider=get_configured_provider()),\n        'instances.1': instance_id,\n    }\n    result = query(params)\n\n    __utils__['cloud.fire_event'](\n        'event',\n        'destroyed instance',\n        'salt/cloud/{0}/destroyed'.format(name),\n        args={'name': name},\n        sock_dir=__opts__['sock_dir'],\n        transport=__opts__['transport']\n    )\n\n    return result", "output": "Destroy an instance.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a destroy i-2f733r5n\n        salt-cloud -d i-2f733r5n", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _rect_to_css(rect):\n    \"\"\"\n    \n    \"\"\"\n    return rect.top(), rect.right(), rect.bottom(), rect.left()", "output": "Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\n\n    :param rect: a dlib 'rect' object\n    :return: a plain tuple representation of the rect in (top, right, bottom, left) order", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def argmax(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        nv.validate_argmax(args, kwargs)\n        nv.validate_minmax_axis(axis)\n\n        i8 = self.asi8\n        if self.hasnans:\n            mask = self._isnan\n            if mask.all() or not skipna:\n                return -1\n            i8 = i8.copy()\n            i8[mask] = 0\n        return i8.argmax()", "output": "Returns the indices of the maximum values along an axis.\n\n        See `numpy.ndarray.argmax` for more information on the\n        `axis` parameter.\n\n        See Also\n        --------\n        numpy.ndarray.argmax", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _on_session_destroyed(session_context):\n    '''\n    \n    '''\n    callbacks = session_context._document.session_destroyed_callbacks\n    session_context._document.session_destroyed_callbacks = set()\n    for callback in callbacks:\n        try:\n            callback(session_context)\n        except Exception as e:\n            log.warning('DocumentLifeCycleHandler on_session_destroyed '\n                        'callback %s failed with following error: %s'\n                        % (callback, e))\n    if callbacks:\n        # If any session callbacks were defined garbage collect after deleting all references\n        del callback\n        del callbacks\n\n        import gc\n        gc.collect()", "output": "Calls any on_session_destroyed callbacks defined on the Document", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def confusion_matrix(self, slice_size:int=1):\n        \"\"\n        x=torch.arange(0,self.data.c)\n        if slice_size is None: cm = ((self.pred_class==x[:,None]) & (self.y_true==x[:,None,None])).sum(2)\n        else:\n            cm = torch.zeros(self.data.c, self.data.c, dtype=x.dtype)\n            for i in range(0, self.y_true.shape[0], slice_size):\n                cm_slice = ((self.pred_class[i:i+slice_size]==x[:,None])\n                            & (self.y_true[i:i+slice_size]==x[:,None,None])).sum(2)\n                torch.add(cm, cm_slice, out=cm)\n        return to_np(cm)", "output": "Confusion matrix as an `np.ndarray`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def global_max(col_vals, index):\r\n    \"\"\"\"\"\"\r\n    col_vals_without_None = [x for x in col_vals if x is not None]\r\n    max_col, min_col = zip(*col_vals_without_None)\r\n    return max(max_col), min(min_col)", "output": "Returns the global maximum and minimum.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rddToFileName(prefix, suffix, timestamp):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(timestamp, datetime):\n        seconds = time.mktime(timestamp.timetuple())\n        timestamp = int(seconds * 1000) + timestamp.microsecond // 1000\n    if suffix is None:\n        return prefix + \"-\" + str(timestamp)\n    else:\n        return prefix + \"-\" + str(timestamp) + \".\" + suffix", "output": "Return string prefix-time(.suffix)\n\n    >>> rddToFileName(\"spark\", None, 12345678910)\n    'spark-12345678910'\n    >>> rddToFileName(\"spark\", \"tmp\", 12345678910)\n    'spark-12345678910.tmp'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def at_time(self, time, asof=False, axis=None):\n        \"\"\"\n        \n        \"\"\"\n        if axis is None:\n            axis = self._stat_axis_number\n        axis = self._get_axis_number(axis)\n\n        index = self._get_axis(axis)\n        try:\n            indexer = index.indexer_at_time(time, asof=asof)\n        except AttributeError:\n            raise TypeError('Index must be DatetimeIndex')\n\n        return self._take(indexer, axis=axis)", "output": "Select values at particular time of day (e.g. 9:30AM).\n\n        Parameters\n        ----------\n        time : datetime.time or str\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        Series or DataFrame\n\n        Raises\n        ------\n        TypeError\n            If the index is not  a :class:`DatetimeIndex`\n\n        See Also\n        --------\n        between_time : Select values between particular times of the day.\n        first : Select initial periods of time series based on a date offset.\n        last : Select final periods of time series based on a date offset.\n        DatetimeIndex.indexer_at_time : Get just the index locations for\n            values at particular time of the day.\n\n        Examples\n        --------\n        >>> i = pd.date_range('2018-04-09', periods=4, freq='12H')\n        >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n        >>> ts\n                             A\n        2018-04-09 00:00:00  1\n        2018-04-09 12:00:00  2\n        2018-04-10 00:00:00  3\n        2018-04-10 12:00:00  4\n\n        >>> ts.at_time('12:00')\n                             A\n        2018-04-09 12:00:00  2\n        2018-04-10 12:00:00  4", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hybrid_forward(self, F, inputs):  # pylint: disable=arguments-differ\n        # pylint: disable=unused-argument\n        \"\"\"\n        \"\"\"\n        outputs = self.ffn_1(inputs)\n        if self.activation:\n            outputs = self.activation(outputs)\n        outputs = self.ffn_2(outputs)\n        if self._dropout:\n            outputs = self.dropout_layer(outputs)\n        if self._use_residual:\n            outputs = outputs + inputs\n        outputs = self.layer_norm(outputs)\n        return outputs", "output": "Position-wise encoding of the inputs.\n\n        Parameters\n        ----------\n        inputs : Symbol or NDArray\n            Input sequence. Shape (batch_size, length, C_in)\n\n        Returns\n        -------\n        outputs : Symbol or NDArray\n            Shape (batch_size, length, C_out)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def process_commands(self, message):\n        \"\"\"\n        \"\"\"\n        if message.author.bot:\n            return\n\n        ctx = await self.get_context(message)\n        await self.invoke(ctx)", "output": "|coro|\n\n        This function processes the commands that have been registered\n        to the bot and other groups. Without this coroutine, none of the\n        commands will be triggered.\n\n        By default, this coroutine is called inside the :func:`.on_message`\n        event. If you choose to override the :func:`.on_message` event, then\n        you should invoke this coroutine as well.\n\n        This is built using other low level tools, and is equivalent to a\n        call to :meth:`~.Bot.get_context` followed by a call to :meth:`~.Bot.invoke`.\n\n        This also checks if the message's author is a bot and doesn't\n        call :meth:`~.Bot.get_context` or :meth:`~.Bot.invoke` if so.\n\n        Parameters\n        -----------\n        message: :class:`discord.Message`\n            The message to process commands for.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def status(name, sig=None):\n    '''\n    \n    '''\n    if sig:\n        # usual way to do by others (debian_service, netbsdservice).\n        # XXX probably does not work here (check 'runsv sshd' instead of 'sshd' ?)\n        return bool(__salt__['status.pid'](sig))\n\n    svc_path = _service_path(name)\n    if not os.path.exists(svc_path):\n        # service does not exist\n        return False\n\n    # sv return code is not relevant to get a service status.\n    # Check its output instead.\n    cmd = 'sv status {0}'.format(svc_path)\n    try:\n        out = __salt__['cmd.run_stdout'](cmd)\n        return out.startswith('run: ')\n    except Exception:\n        # sv (as a command) returned an error\n        return False", "output": "Return ``True`` if service is running\n\n    name\n        the service's name\n\n    sig\n        signature to identify with ps\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' runit.status <service name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init(textCNN, vocab, model_mode, context, lr):\n    \"\"\"\"\"\"\n\n    textCNN.initialize(mx.init.Xavier(), ctx=context, force_reinit=True)\n    if model_mode != 'rand':\n        textCNN.embedding.weight.set_data(vocab.embedding.idx_to_vec)\n    if model_mode == 'multichannel':\n        textCNN.embedding_extend.weight.set_data(vocab.embedding.idx_to_vec)\n    if model_mode == 'static' or model_mode == 'multichannel':\n        # Parameters of textCNN.embedding are not updated during training.\n        textCNN.embedding.collect_params().setattr('grad_req', 'null')\n    trainer = gluon.Trainer(textCNN.collect_params(), 'adam', {'learning_rate': lr})\n    return textCNN, trainer", "output": "Initialize parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_distributions(self):\n        \"\"\"\n        \n        \"\"\"\n        if not self._cache_enabled:\n            for dist in self._yield_distributions():\n                yield dist\n        else:\n            self._generate_cache()\n\n            for dist in self._cache.path.values():\n                yield dist\n\n            if self._include_egg:\n                for dist in self._cache_egg.path.values():\n                    yield dist", "output": "Provides an iterator that looks for distributions and returns\n        :class:`InstalledDistribution` or\n        :class:`EggInfoDistribution` instances for each one of them.\n\n        :rtype: iterator of :class:`InstalledDistribution` and\n                :class:`EggInfoDistribution` instances", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _contains_blinded_text(stats_xml):\n    \"\"\"  \"\"\"\n    tree = ET.parse(stats_xml)\n    root = tree.getroot()\n    total_tokens = int(root.find('size/total/tokens').text)\n    unique_lemmas = int(root.find('lemmas').get('unique'))\n\n    # assume the corpus is largely blinded when there are less than 1% unique tokens\n    return (unique_lemmas / total_tokens) < 0.01", "output": "Heuristic to determine whether the treebank has blinded texts or not", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ip_addrs(interface=None, include_loopback=False, interface_data=None, proto='inet'):\n    '''\n    \n    '''\n    ret = set()\n\n    ifaces = interface_data \\\n        if isinstance(interface_data, dict) \\\n        else interfaces()\n    if interface is None:\n        target_ifaces = ifaces\n    else:\n        target_ifaces = dict([(k, v) for k, v in six.iteritems(ifaces)\n                              if k == interface])\n        if not target_ifaces:\n            log.error('Interface %s not found.', interface)\n    for ip_info in six.itervalues(target_ifaces):\n        addrs = ip_info.get(proto, [])\n        addrs.extend([addr for addr in ip_info.get('secondary', []) if addr.get('type') == proto])\n\n        for addr in addrs:\n            addr = ipaddress.ip_address(addr.get('address'))\n            if not addr.is_loopback or include_loopback:\n                ret.add(addr)\n    return [six.text_type(addr) for addr in sorted(ret)]", "output": "Return the full list of IP adresses matching the criteria\n\n    proto = inet|inet6", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def norm(self):\n        \"\"\"\n        \n        \"\"\"\n        for (k, _) in self.entries.items():\n            labelingLen = len(self.entries[k].labeling)\n            self.entries[k].prText = self.entries[k].prText ** (1.0 / (labelingLen if labelingLen else 1.0))", "output": "length-normalise LM score", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def python_like_exts():\r\n    \"\"\"\"\"\"\r\n    exts = []\r\n    for lang in languages.PYTHON_LIKE_LANGUAGES:\r\n        exts.extend(list(languages.ALL_LANGUAGES[lang]))\r\n    return ['.' + ext for ext in exts]", "output": "Return a list of all python-like extensions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def MergeAllSummaries(period=0, run_alone=False, key=None):\n    \"\"\"\n    \n    \"\"\"\n    if key is None:\n        key = tf.GraphKeys.SUMMARIES\n    period = int(period)\n    if run_alone:\n        return MergeAllSummaries_RunAlone(period, key)\n    else:\n        return MergeAllSummaries_RunWithOp(period, key)", "output": "This callback is enabled by default.\n    Evaluate all summaries by ``tf.summary.merge_all``, and write them to logs.\n\n    Args:\n        period (int): by default the callback summarizes once every epoch.\n            This option (if not set to 0) makes it additionally summarize every ``period`` steps.\n        run_alone (bool): whether to evaluate the summaries alone.\n            If True, summaries will be evaluated after each epoch alone.\n            If False, summaries will be evaluated together with the\n            `sess.run` calls, in the last step of each epoch.\n            For :class:`SimpleTrainer`, it needs to be False because summary may\n            depend on inputs.\n        key (str): the collection of summary tensors. Same as in ``tf.summary.merge_all``.\n            Default is ``tf.GraphKeys.SUMMARIES``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def returner(ret):\n    '''\n    \n    '''\n    try:\n        with _get_serv(ret, commit=True) as cur:\n            sql = '''INSERT INTO salt_returns\n                    (fun, jid, return, id, success, full_ret)\n                    VALUES (%s, %s, %s, %s, %s, %s)'''\n            cur.execute(\n                sql, (\n                    ret['fun'],\n                    ret['jid'],\n                    salt.utils.json.dumps(ret['return']),\n                    ret['id'],\n                    ret.get('success', False),\n                    salt.utils.json.dumps(ret)))\n    except salt.exceptions.SaltMasterError:\n        log.critical('Could not store return with postgres returner. PostgreSQL server unavailable.')", "output": "Return data to a postgres server", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def arg_lookup(fun, aspec=None):\n    '''\n    \n    '''\n    ret = {'kwargs': {}}\n    if aspec is None:\n        aspec = get_function_argspec(fun)\n    if aspec.defaults:\n        ret['kwargs'] = dict(zip(aspec.args[::-1], aspec.defaults[::-1]))\n    ret['args'] = [arg for arg in aspec.args if arg not in ret['kwargs']]\n    return ret", "output": "Return a dict containing the arguments and default arguments to the\n    function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def status_search(self, status):\n        \"\"\"\"\"\"\n        json = self._fetch_json()\n        jobs = json['response']\n        for job in jobs:\n            job_info = jobs[job]\n            if job_info['status'].lower() == status.lower():\n                yield self._build_results(jobs, job)", "output": "Searches for jobs matching the given ``status``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def license_present(name):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': False,\n           'comment': ''}\n\n    if not __salt__['powerpath.has_powerpath']():\n        ret['result'] = False\n        ret['comment'] = 'PowerPath is not installed.'\n        return ret\n\n    licenses = [l['key'] for l in __salt__['powerpath.list_licenses']()]\n\n    if name in licenses:\n        ret['result'] = True\n        ret['comment'] = 'License key {0} already present'.format(name)\n        return ret\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'License key {0} is set to be added'.format(name)\n        return ret\n\n    data = __salt__['powerpath.add_license'](name)\n    if data['result']:\n        ret['changes'] = {name: 'added'}\n        ret['result'] = True\n        ret['comment'] = data['output']\n        return ret\n    else:\n        ret['result'] = False\n        ret['comment'] = data['output']\n        return ret", "output": "Ensures that the specified PowerPath license key is present\n    on the host.\n\n    name\n        The license key to ensure is present", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def autoencoder_residual_text():\n  \"\"\"\"\"\"\n  hparams = autoencoder_residual()\n  hparams.bottleneck_bits = 32\n  hparams.batch_size = 1024\n  hparams.hidden_size = 64\n  hparams.max_hidden_size = 512\n  hparams.bottleneck_noise = 0.0\n  hparams.bottom = {\n      \"inputs\": modalities.identity_bottom,\n      \"targets\": modalities.identity_bottom,\n  }\n  hparams.top = {\n      \"targets\": modalities.identity_top,\n  }\n  hparams.autoregressive_mode = \"none\"\n  hparams.sample_width = 1\n  return hparams", "output": "Residual autoencoder model for text.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_platform(platform):\n    '''\n    \n    '''\n    nb_platform = get_('dcim', 'platforms', slug=slugify(platform))\n    if nb_platform:\n        return False\n    else:\n        payload = {'name': platform, 'slug': slugify(platform)}\n        plat = _add('dcim', 'platforms', payload)\n        if plat:\n            return {'dcim': {'platforms': payload}}\n        else:\n            return False", "output": ".. versionadded:: 2019.2.0\n\n    Create a new device platform\n\n    platform\n        String of device platform, e.g., ``junos``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion netbox.create_platform junos", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parseEntityRef(self):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlParseEntityRef(self._o)\n        if ret is None:raise parserError('xmlParseEntityRef() failed')\n        __tmp = xmlEntity(_obj=ret)\n        return __tmp", "output": "parse ENTITY references declarations  [68] EntityRef ::=\n          '&' Name ';'  [ WFC: Entity Declared ] In a document\n          without any DTD, a document with only an internal DTD\n          subset which contains no parameter entity references, or a\n          document with \"standalone='yes'\", the Name given in the\n          entity reference must match that in an entity declaration,\n          except that well-formed documents need not declare any of\n          the following entities: amp, lt, gt, apos, quot.  The\n          declaration of a parameter entity must precede any\n          reference to it.  Similarly, the declaration of a general\n          entity must precede any reference to it which appears in a\n          default value in an attribute-list declaration. Note that\n          if entities are declared in the external subset or in\n          external parameter entities, a non-validating processor is\n          not obligated to read and process their declarations; for\n          such documents, the rule that an entity must be declared is\n          a well-formedness constraint only if standalone='yes'.  [\n          WFC: Parsed Entity ] An entity reference must not contain\n           the name of an unparsed entity", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def autoencoder_basic_discrete():\n  \"\"\"\"\"\"\n  hparams = autoencoder_autoregressive()\n  hparams.num_hidden_layers = 5\n  hparams.hidden_size = 64\n  hparams.bottleneck_bits = 1024\n  hparams.bottleneck_noise = 0.1\n  hparams.add_hparam(\"discretize_warmup_steps\", 16000)\n  return hparams", "output": "Basic autoencoder model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dbg(self, level, msg):\n        \"\"\"\n        \"\"\"\n        if level <= self.debug:\n            print(msg, file=sys.stderr)", "output": "Write debugging output to sys.stderr.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def PreparePairedSequenceBatch(source, target_in, pad=0):\n  \"\"\"\n  \"\"\"\n  target = target_in[:, :-1]\n  target_y = target_in[:, 1:]\n  source_mask = np.reshape(source != pad,\n                           (source.shape[0], 1, 1, source.shape[-1]))\n  target_mask = MakeTargetMask(target, pad)\n  memory_mask = (\n      np.reshape(np.arange(target.shape[-1]) < source.shape[-1], [-1, 1]))\n  ntokens = np.sum(target_y != pad)\n  return (source, target, target_y,\n          source_mask, target_mask, memory_mask, ntokens)", "output": "Build masks for this batch.\n\n  Args:\n    source: (batch, source_len) array of integer-coded symbols for inputs\n    target_in: (batch, batch_len) array of integer-coded symbols for targets\n    pad: int: the padding symbol used to pad the above\n\n  Returns:\n    Prepared batch of tuple of arrays: source, input-target, shifted-target,\n    source mask, target mask, source-target \"memory\" mask, minibatch token count", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def alter_subprocess_kwargs_by_platform(**kwargs):\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    kwargs.setdefault('close_fds', os.name == 'posix')\r\n    if os.name == 'nt':\r\n        CONSOLE_CREATION_FLAGS = 0  # Default value\r\n        # See: https://msdn.microsoft.com/en-us/library/windows/desktop/ms684863%28v=vs.85%29.aspx\r\n        CREATE_NO_WINDOW = 0x08000000\r\n        # We \"or\" them together\r\n        CONSOLE_CREATION_FLAGS |= CREATE_NO_WINDOW\r\n        kwargs.setdefault('creationflags', CONSOLE_CREATION_FLAGS)\r\n    return kwargs", "output": "Given a dict, populate kwargs to create a generally\r\n    useful default setup for running subprocess processes\r\n    on different platforms. For example, `close_fds` is\r\n    set on posix and creation of a new console window is\r\n    disabled on Windows.\r\n\r\n    This function will alter the given kwargs and return\r\n    the modified dict.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_conn(profile):\n    '''\n    \n    '''\n    DEFAULT_BASE_URL = _construct_uri(profile) or 'http://localhost:5984'\n\n    server = couchdb.Server()\n    if profile['database'] not in server:\n        server.create(profile['database'])\n    return server", "output": "Get a connection to CouchDB", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stChromaFeaturesInit(nfft, fs):\n    \"\"\"\n    \n    \"\"\"\n    freqs = numpy.array([((f + 1) * fs) / (2 * nfft) for f in range(nfft)])    \n    Cp = 27.50    \n    nChroma = numpy.round(12.0 * numpy.log2(freqs / Cp)).astype(int)\n\n    nFreqsPerChroma = numpy.zeros((nChroma.shape[0], ))\n\n    uChroma = numpy.unique(nChroma)\n    for u in uChroma:\n        idx = numpy.nonzero(nChroma == u)\n        nFreqsPerChroma[idx] = idx[0].shape\n    \n    return nChroma, nFreqsPerChroma", "output": "This function initializes the chroma matrices used in the calculation of the chroma features", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kill_all_jobs():\n    '''\n    \n    '''\n    # Some OS's (Win32) don't have SIGKILL, so use salt_SIGKILL which is set to\n    # an appropriate value for the operating system this is running on.\n    ret = []\n    for data in running():\n        ret.append(signal_job(data['jid'], salt_SIGKILL))\n    return ret", "output": "Sends a kill signal (SIGKILL 9) to all currently running jobs\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.kill_all_jobs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def construct_latent_tower(self, images, time_axis):\n    \"\"\"\"\"\"\n    # No latent in the first phase\n    first_phase = tf.less(\n        self.get_iteration_num(), self.hparams.num_iterations_1st_stage)\n\n    # use all frames by default but this allows more\n    # predicted frames at inference time\n    latent_num_frames = self.hparams.latent_num_frames\n    tf.logging.info(\"Creating latent tower with %d frames.\" % latent_num_frames)\n    if latent_num_frames > 0:\n      images = images[:, :latent_num_frames]\n\n    return common_video.conv_latent_tower(\n        images=images,\n        time_axis=time_axis,\n        latent_channels=self.hparams.latent_channels,\n        min_logvar=self.hparams.latent_std_min,\n        is_training=self.is_training,\n        random_latent=first_phase,\n        tiny_mode=self.hparams.tiny_mode,\n        small_mode=self.hparams.small_mode)", "output": "Create the latent tower.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_estimator_input_fn(self,\n                              mode,\n                              hparams,\n                              data_dir=None,\n                              force_repeat=False,\n                              prevent_repeat=False,\n                              dataset_kwargs=None):\n    \"\"\"\"\"\"\n\n    def estimator_input_fn(params, config):\n      return self.input_fn(\n          mode,\n          hparams,\n          data_dir=data_dir,\n          params=params,\n          config=config,\n          force_repeat=force_repeat,\n          prevent_repeat=prevent_repeat,\n          dataset_kwargs=dataset_kwargs)\n\n    return estimator_input_fn", "output": "Return input_fn wrapped for Estimator.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def naive_grouped_rowwise_apply(data,\n                                group_labels,\n                                func,\n                                func_args=(),\n                                out=None):\n    \"\"\"\n    \n    \"\"\"\n    if out is None:\n        out = np.empty_like(data)\n\n    for (row, label_row, out_row) in zip(data, group_labels, out):\n        for label in np.unique(label_row):\n            locs = (label_row == label)\n            out_row[locs] = func(row[locs], *func_args)\n    return out", "output": "Simple implementation of grouped row-wise function application.\n\n    Parameters\n    ----------\n    data : ndarray[ndim=2]\n        Input array over which to apply a grouped function.\n    group_labels : ndarray[ndim=2, dtype=int64]\n        Labels to use to bucket inputs from array.\n        Should be the same shape as array.\n    func : function[ndarray[ndim=1]] -> function[ndarray[ndim=1]]\n        Function to apply to pieces of each row in array.\n    func_args : tuple\n        Additional positional arguments to provide to each row in array.\n    out : ndarray, optional\n        Array into which to write output.  If not supplied, a new array of the\n        same shape as ``data`` is allocated and returned.\n\n    Examples\n    --------\n    >>> data = np.array([[1., 2., 3.],\n    ...                  [2., 3., 4.],\n    ...                  [5., 6., 7.]])\n    >>> labels = np.array([[0, 0, 1],\n    ...                    [0, 1, 0],\n    ...                    [1, 0, 2]])\n    >>> naive_grouped_rowwise_apply(data, labels, lambda row: row - row.min())\n    array([[ 0.,  1.,  0.],\n           [ 0.,  0.,  2.],\n           [ 0.,  0.,  0.]])\n    >>> naive_grouped_rowwise_apply(data, labels, lambda row: row / row.sum())\n    array([[ 0.33333333,  0.66666667,  1.        ],\n           [ 0.33333333,  1.        ,  0.66666667],\n           [ 1.        ,  1.        ,  1.        ]])", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def path_to_url(path):\n    \"\"\"\n    \n    \"\"\"\n    path = os.path.normpath(os.path.abspath(path))\n    url = urlparse.urljoin(\"file:\", urllib2.pathname2url(path))\n    return url", "output": "Convert a path to a file: URL.  The path will be made absolute and have\n    quoted path parts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def new_value(self, name, value):\r\n        \"\"\"\"\"\"\r\n        try:\r\n            # We need to enclose values in a list to be able to send\r\n            # them to the kernel in Python 2\r\n            svalue = [cloudpickle.dumps(value, protocol=PICKLE_PROTOCOL)]\r\n\r\n            # Needed to prevent memory leaks. See issue 7158\r\n            if len(svalue) < MAX_SERIALIZED_LENGHT:\r\n                self.shellwidget.set_value(name, svalue)\r\n            else:\r\n                QMessageBox.warning(self, _(\"Warning\"),\r\n                                    _(\"The object you are trying to modify is \"\r\n                                      \"too big to be sent back to the kernel. \"\r\n                                      \"Therefore, your modifications won't \"\r\n                                      \"take place.\"))\r\n        except TypeError as e:\r\n            QMessageBox.critical(self, _(\"Error\"),\r\n                                 \"TypeError: %s\" % to_text_string(e))\r\n        self.shellwidget.refresh_namespacebrowser()", "output": "Create new value in data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def date_from_relative_day(base_date, time, dow):\n    \"\"\"\n    \n    \"\"\"\n    # Reset date to start of the day\n    base_date = datetime(base_date.year, base_date.month, base_date.day)\n    time = time.lower()\n    dow = dow.lower()\n    if time == 'this' or time == 'coming':\n        # Else day of week\n        num = HASHWEEKDAYS[dow]\n        return this_week_day(base_date, num)\n    elif time == 'last' or time == 'previous':\n        # Else day of week\n        num = HASHWEEKDAYS[dow]\n        return previous_week_day(base_date, num)\n    elif time == 'next' or time == 'following':\n        # Else day of week\n        num = HASHWEEKDAYS[dow]\n        return next_week_day(base_date, num)", "output": "Converts relative day to time\n    Ex: this tuesday, last tuesday", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(path):\n        \"\"\"\n        \"\"\"\n        with open(path, \"r\") as fobj:\n            analytics = Analytics(info=json.load(fobj))\n        os.unlink(path)\n        return analytics", "output": "Loads analytics report from json file specified by path.\n\n        Args:\n            path (str): path to json file with analytics report.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode_dict(values_dict):\n    \"\"\"\n    \"\"\"\n    return {key: encode_value(value) for key, value in six.iteritems(values_dict)}", "output": "Encode a dictionary into protobuf ``Value``-s.\n\n    Args:\n        values_dict (dict): The dictionary to encode as protobuf fields.\n\n    Returns:\n        Dict[str, ~google.cloud.firestore_v1beta1.types.Value]: A\n        dictionary of string keys and ``Value`` protobufs as dictionary\n        values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def config_tree(source='running', with_tags=False):\n    '''\n    \n    '''\n    config_txt = __salt__['net.config'](source=source)['out'][source]\n    return __salt__['iosconfig.tree'](config=config_txt)", "output": ".. versionadded:: 2019.2.0\n\n    Transform Cisco IOS style configuration to structured Python dictionary.\n    Depending on the value of the ``with_tags`` argument, this function may\n    provide different views, valuable in different situations.\n\n    source: ``running``\n        The configuration type to retrieve from the network device. Default:\n        ``running``. Available options: ``running``, ``startup``, ``candidate``.\n\n    with_tags: ``False``\n        Whether this function should return a detailed view, with tags.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm.config_tree", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def groups(self):\n        \"\"\"  \"\"\"\n\n        # this is mainly for compat\n        # GH 3881\n        result = {key: value for key, value in zip(self.binlabels, self.bins)\n                  if key is not NaT}\n        return result", "output": "dict {group name -> group labels}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_csrs():\n    '''\n    \n    '''\n    data = salt.utils.http.query(\n        '{0}/certificaterequests'.format(_base_url()),\n        status=True,\n        decode=True,\n        decode_type='json',\n        header_dict={\n            'tppl-api-key': _api_key(),\n        },\n    )\n    status = data['status']\n    if six.text_type(status).startswith('4') or six.text_type(status).startswith('5'):\n        raise CommandExecutionError(\n            'There was an API error: {0}'.format(data['error'])\n        )\n    return data.get('dict', {})", "output": "Show certificate requests for this API key\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run digicert.show_csrs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_metadata(self, data_dir, feature_name=None):\n    \"\"\"\"\"\"\n    # Save names if defined\n    if self._str2int is not None:\n      names_filepath = _get_names_filepath(data_dir, feature_name)\n      _write_names_to_file(names_filepath, self.names)", "output": "See base class for details.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _file_changed_nilrt(full_filepath):\n    '''\n    \n    '''\n    rs_state_dir = \"/var/lib/salt/restartcheck_state\"\n    base_filename = os.path.basename(full_filepath)\n    timestamp_file = os.path.join(rs_state_dir, '{0}.timestamp'.format(base_filename))\n    md5sum_file = os.path.join(rs_state_dir, '{0}.md5sum'.format(base_filename))\n\n    if not os.path.exists(timestamp_file) or not os.path.exists(md5sum_file):\n        return True\n\n    prev_timestamp = __salt__['file.read'](timestamp_file).rstrip()\n    # Need timestamp in seconds so floor it using int()\n    cur_timestamp = str(int(os.path.getmtime(full_filepath)))\n\n    if prev_timestamp != cur_timestamp:\n        return True\n\n    return bool(__salt__['cmd.retcode']('md5sum -cs {0}'.format(md5sum_file), output_loglevel=\"quiet\"))", "output": "Detect whether a file changed in an NILinuxRT system using md5sum and timestamp\n    files from a state directory.\n\n    Returns:\n             - False if md5sum/timestamp state files don't exist\n             - True/False depending if ``base_filename`` got modified/touched", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_keywords(lexer):\n    \"\"\"\n    \"\"\"\n    if not hasattr(lexer, 'tokens'):\n        return []\n    if 'keywords' in lexer.tokens:\n        try:\n            return lexer.tokens['keywords'][0][0].words\n        except:\n            pass\n    keywords = []\n    for vals in lexer.tokens.values():\n        for val in vals:\n            try:\n                if isinstance(val[0], words):\n                    keywords.extend(val[0].words)\n                else:\n                    ini_val = val[0]\n                    if ')\\\\b' in val[0] or ')(\\\\s+)' in val[0]:\n                        val = re.sub(r'\\\\.', '', val[0])\n                        val = re.sub(r'[^0-9a-zA-Z|]+', '', val)\n                        if '|' in ini_val:\n                            keywords.extend(val.split('|'))\n                        else:\n                            keywords.append(val)\n            except Exception:\n                continue\n    return keywords", "output": "Get the keywords for a given lexer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_value(self, scalar_data_blob, dtype_enum):\n    \"\"\"\n    \"\"\"\n    tensorflow_dtype = tf.DType(dtype_enum)\n    buf = np.frombuffer(scalar_data_blob, dtype=tensorflow_dtype.as_numpy_dtype)\n    return np.asscalar(buf)", "output": "Obtains value for scalar event given blob and dtype enum.\n\n    Args:\n      scalar_data_blob: The blob obtained from the database.\n      dtype_enum: The enum representing the dtype.\n\n    Returns:\n      The scalar value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def diff(candidate, running, *models):\n    '''\n    \n    '''\n    if isinstance(models, tuple) and isinstance(models[0], list):\n        models = models[0]\n\n    first = _get_root_object(models)\n    first.load_dict(candidate)\n    second = _get_root_object(models)\n    second.load_dict(running)\n    return napalm_yang.utils.diff(first, second)", "output": "Returns the difference between two configuration entities structured\n    according to the YANG model.\n\n    .. note::\n        This function is recommended to be used mostly as a state helper.\n\n    candidate\n        First model to compare.\n\n    running\n        Second model to compare.\n\n    models\n        A list of models to be used when comparing.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm_yang.diff {} {} models.openconfig_interfaces\n\n    Output Example:\n\n    .. code-block:: python\n\n        {\n            \"interfaces\": {\n                \"interface\": {\n                    \"both\": {\n                        \"Port-Channel1\": {\n                            \"config\": {\n                                \"mtu\": {\n                                    \"first\": \"0\",\n                                    \"second\": \"9000\"\n                                }\n                            }\n                        }\n                    },\n                    \"first_only\": [\n                        \"Loopback0\"\n                    ],\n                    \"second_only\": [\n                        \"Loopback1\"\n                    ]\n                }\n            }\n        }", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_posix_path(code_path):\n    \"\"\"\n    \n    \"\"\"\n\n    return re.sub(\"^([A-Za-z])+:\",\n                  lambda match: posixpath.sep + match.group().replace(\":\", \"\").lower(),\n                  pathlib.PureWindowsPath(code_path).as_posix()) if os.name == \"nt\" else code_path", "output": "Change the code_path to be of unix-style if running on windows when supplied with an absolute windows path.\n\n    Parameters\n    ----------\n    code_path : str\n        Directory in the host operating system that should be mounted within the container.\n    Returns\n    -------\n    str\n        Posix equivalent of absolute windows style path.\n    Examples\n    --------\n    >>> to_posix_path('/Users/UserName/sam-app')\n    /Users/UserName/sam-app\n    >>> to_posix_path('C:\\\\\\\\Users\\\\\\\\UserName\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Temp\\\\\\\\mydir')\n    /c/Users/UserName/AppData/Local/Temp/mydir", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _FloatDecoder():\n  \"\"\"\n  \"\"\"\n\n  local_unpack = struct.unpack\n\n  def InnerDecode(buffer, pos):\n    # We expect a 32-bit value in little-endian byte order.  Bit 1 is the sign\n    # bit, bits 2-9 represent the exponent, and bits 10-32 are the significand.\n    new_pos = pos + 4\n    float_bytes = buffer[pos:new_pos]\n\n    # If this value has all its exponent bits set, then it's non-finite.\n    # In Python 2.4, struct.unpack will convert it to a finite 64-bit value.\n    # To avoid that, we parse it specially.\n    if (float_bytes[3:4] in b'\\x7F\\xFF' and float_bytes[2:3] >= b'\\x80'):\n      # If at least one significand bit is set...\n      if float_bytes[0:3] != b'\\x00\\x00\\x80':\n        return (_NAN, new_pos)\n      # If sign bit is set...\n      if float_bytes[3:4] == b'\\xFF':\n        return (_NEG_INF, new_pos)\n      return (_POS_INF, new_pos)\n\n    # Note that we expect someone up-stack to catch struct.error and convert\n    # it to _DecodeError -- this way we don't have to set up exception-\n    # handling blocks every time we parse one value.\n    result = local_unpack('<f', float_bytes)[0]\n    return (result, new_pos)\n  return _SimpleDecoder(wire_format.WIRETYPE_FIXED32, InnerDecode)", "output": "Returns a decoder for a float field.\n\n  This code works around a bug in struct.unpack for non-finite 32-bit\n  floating-point values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_trial_remove(self, trial_runner, trial):\n        \"\"\"\"\"\"\n        if trial.status is Trial.PAUSED and trial in self._results:\n            self._completed_trials.add(trial)", "output": "Marks trial as completed if it is paused and has previously ran.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_label(user: User) -> Optional[str]:\n    \"\"\"\"\"\"\n    if user:\n        if user.first_name and user.last_name:\n            return user.first_name + ' ' + user.last_name\n        else:\n            return user.username\n\n    return None", "output": "Given a user ORM FAB object, returns a label", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getFragment(self):\n        \"\"\"\"\"\"\n        # assert self.innerHTML\n        fragment = self.fragmentClass()\n        self.openElements[0].reparentChildren(fragment)\n        return fragment", "output": "Return the final fragment", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _process_bracket(self, trial_runner, bracket, trial):\n        \"\"\"\"\"\"\n\n        action = TrialScheduler.PAUSE\n        if bracket.cur_iter_done():\n            if bracket.finished():\n                bracket.cleanup_full(trial_runner)\n                return TrialScheduler.STOP\n\n            good, bad = bracket.successive_halving(self._reward_attr)\n            # kill bad trials\n            self._num_stopped += len(bad)\n            for t in bad:\n                if t.status == Trial.PAUSED:\n                    trial_runner.stop_trial(t)\n                elif t.status == Trial.RUNNING:\n                    bracket.cleanup_trial(t)\n                    action = TrialScheduler.STOP\n                else:\n                    raise Exception(\"Trial with unexpected status encountered\")\n\n            # ready the good trials - if trial is too far ahead, don't continue\n            for t in good:\n                if t.status not in [Trial.PAUSED, Trial.RUNNING]:\n                    raise Exception(\"Trial with unexpected status encountered\")\n                if bracket.continue_trial(t):\n                    if t.status == Trial.PAUSED:\n                        trial_runner.trial_executor.unpause_trial(t)\n                    elif t.status == Trial.RUNNING:\n                        action = TrialScheduler.CONTINUE\n        return action", "output": "This is called whenever a trial makes progress.\n\n        When all live trials in the bracket have no more iterations left,\n        Trials will be successively halved. If bracket is done, all\n        non-running trials will be stopped and cleaned up,\n        and during each halving phase, bad trials will be stopped while good\n        trials will return to \"PENDING\".", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def follow(\n        self,\n        users,\n        strategies,\n        track_interval=1,\n        trade_cmd_expire_seconds=120,\n        cmd_cache=True,\n        slippage: float = 0.0,\n        **kwargs\n    ):\n        \"\"\"\n        \"\"\"\n        self.slippage = slippage", "output": "\u8ddf\u8e2a\u5e73\u53f0\u5bf9\u5e94\u7684\u6a21\u62df\u4ea4\u6613\uff0c\u652f\u6301\u591a\u7528\u6237\u591a\u7b56\u7565\n\n        :param users: \u652f\u6301easytrader\u7684\u7528\u6237\u5bf9\u8c61\uff0c\u652f\u6301\u4f7f\u7528 [] \u6307\u5b9a\u591a\u4e2a\u7528\u6237\n        :param strategies: \u96ea\u7403\u7ec4\u5408\u540d, \u7c7b\u4f3c ZH123450\n        :param total_assets: \u96ea\u7403\u7ec4\u5408\u5bf9\u5e94\u7684\u603b\u8d44\u4ea7\uff0c \u683c\u5f0f [ \u7ec4\u54081\u5bf9\u5e94\u8d44\u91d1, \u7ec4\u54082\u5bf9\u5e94\u8d44\u91d1 ]\n            \u82e5 strategies=['ZH000001', 'ZH000002'] \u8bbe\u7f6e total_assets=[10000, 10000], \u5219\u8868\u660e\u6bcf\u4e2a\u7ec4\u5408\u5bf9\u5e94\u7684\u8d44\u4ea7\u4e3a 1w \u5143\uff0c\n            \u5047\u8bbe\u7ec4\u5408 ZH000001 \u52a0\u4ed3 \u4ef7\u683c\u4e3a p \u80a1\u7968 A 10%, \u5219\u5bf9\u5e94\u7684\u4ea4\u6613\u6307\u4ee4\u4e3a \u4e70\u5165 \u80a1\u7968 A \u4ef7\u683c P \u80a1\u6570 1w * 10% / p \u5e76\u6309 100 \u53d6\u6574\n        :param initial_assets:\u96ea\u7403\u7ec4\u5408\u5bf9\u5e94\u7684\u521d\u59cb\u8d44\u4ea7, \u683c\u5f0f [ \u7ec4\u54081\u5bf9\u5e94\u8d44\u91d1, \u7ec4\u54082\u5bf9\u5e94\u8d44\u91d1 ]\n            \u603b\u8d44\u4ea7\u7531 \u521d\u59cb\u8d44\u4ea7 \u00d7 \u7ec4\u5408\u51c0\u503c \u7b97\u5f97\uff0c total_assets \u4f1a\u8986\u76d6\u6b64\u53c2\u6570\n        :param track_interval: \u8f6e\u8be2\u6a21\u62df\u4ea4\u6613\u65f6\u95f4\uff0c\u5355\u4f4d\u4e3a\u79d2\n        :param trade_cmd_expire_seconds: \u4ea4\u6613\u6307\u4ee4\u8fc7\u671f\u65f6\u95f4, \u5355\u4f4d\u4e3a\u79d2\n        :param cmd_cache: \u662f\u5426\u8bfb\u53d6\u5b58\u50a8\u5386\u53f2\u6267\u884c\u8fc7\u7684\u6307\u4ee4\uff0c\u9632\u6b62\u91cd\u542f\u65f6\u91cd\u590d\u6267\u884c\u5df2\u7ecf\u4ea4\u6613\u8fc7\u7684\u6307\u4ee4\n        :param slippage: \u6ed1\u70b9\uff0c0.0 \u8868\u793a\u65e0\u6ed1\u70b9, 0.05 \u8868\u793a\u6ed1\u70b9\u4e3a 5%", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_ctc_loss(pred, seq_len, num_label, loss_type):\n    \"\"\"  \"\"\"\n    label = mx.sym.Variable('label')\n    if loss_type == 'warpctc':\n        print(\"Using WarpCTC Loss\")\n        sm = _add_warp_ctc_loss(pred, seq_len, num_label, label)\n    else:\n        print(\"Using MXNet CTC Loss\")\n        assert loss_type == 'ctc'\n        sm = _add_mxnet_ctc_loss(pred, seq_len, label)\n    return sm", "output": "Adds CTC loss on top of pred symbol and returns the resulting symbol", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_statements(self, end_tokens, drop_needle=False):\n        \"\"\"\n        \"\"\"\n        # the first token may be a colon for python compatibility\n        self.stream.skip_if('colon')\n\n        # in the future it would be possible to add whole code sections\n        # by adding some sort of end of statement token and parsing those here.\n        self.stream.expect('block_end')\n        result = self.subparse(end_tokens)\n\n        # we reached the end of the template too early, the subparser\n        # does not check for this, so we do that now\n        if self.stream.current.type == 'eof':\n            self.fail_eof(end_tokens)\n\n        if drop_needle:\n            next(self.stream)\n        return result", "output": "Parse multiple statements into a list until one of the end tokens\n        is reached.  This is used to parse the body of statements as it also\n        parses template data if appropriate.  The parser checks first if the\n        current token is a colon and skips it if there is one.  Then it checks\n        for the block end and parses until if one of the `end_tokens` is\n        reached.  Per default the active token in the stream at the end of\n        the call is the matched end token.  If this is not wanted `drop_needle`\n        can be set to `True` and the end token is removed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_out_of_order(list_of_numbers):\n  \"\"\"\n  \"\"\"\n  # TODO: Consider changing this to only check for out-of-order\n  # steps within a particular tag.\n  result = []\n  # pylint: disable=consider-using-enumerate\n  for i in range(len(list_of_numbers)):\n    if i == 0:\n      continue\n    if list_of_numbers[i] < list_of_numbers[i - 1]:\n      result.append((list_of_numbers[i - 1], list_of_numbers[i]))\n  return result", "output": "Returns elements that break the monotonically non-decreasing trend.\n\n  This is used to find instances of global step values that are \"out-of-order\",\n  which may trigger TensorBoard event discarding logic.\n\n  Args:\n    list_of_numbers: A list of numbers.\n\n  Returns:\n    A list of tuples in which each tuple are two elements are adjacent, but the\n    second element is lower than the first.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_one(self, selector):\n        ''' \n\n        '''\n        result = list(self.select(selector))\n        if len(result) > 1:\n            raise ValueError(\"Found more than one model matching %s: %r\" % (selector, result))\n        if len(result) == 0:\n            return None\n        return result[0]", "output": "Query this document for objects that match the given selector.\n        Raises an error if more than one object is found.  Returns\n        single matching object, or None if nothing is found\n\n        Args:\n            selector (JSON-like query dictionary) : you can query by type or by\n                name, e.g. ``{\"type\": HoverTool}``, ``{\"name\": \"mycircle\"}``\n\n        Returns:\n            Model or None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sizeHint(self):\n        \"\"\"\n        \"\"\"\n        fm = QFontMetrics(self.editor.font())\n        size_hint = QSize(fm.height(), fm.height())\n        if size_hint.width() > 16:\n            size_hint.setWidth(16)\n        return size_hint", "output": "Override Qt method.\n\n        Returns the widget size hint (based on the editor font size).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _construct_result(left, result, index, name, dtype=None):\n    \"\"\"\n    \n    \"\"\"\n    out = left._constructor(result, index=index, dtype=dtype)\n    out = out.__finalize__(left)\n    out.name = name\n    return out", "output": "If the raw op result has a non-None name (e.g. it is an Index object) and\n    the name argument is None, then passing name to the constructor will\n    not be enough; we still need to override the name attribute.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_linode(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The get_linode function must be called with -f or --function.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    name = kwargs.get('name', None)\n    linode_id = kwargs.get('linode_id', None)\n    if name is None and linode_id is None:\n        raise SaltCloudSystemExit(\n            'The get_linode function requires either a \\'name\\' or a \\'linode_id\\'.'\n        )\n\n    if linode_id is None:\n        linode_id = get_linode_id_from_name(name)\n\n    result = _query('linode', 'list', args={'LinodeID': linode_id})\n\n    return result['DATA'][0]", "output": "Returns data for a single named Linode.\n\n    name\n        The name of the Linode for which to get data. Can be used instead\n        ``linode_id``. Note this will induce an additional API call\n        compared to using ``linode_id``.\n\n    linode_id\n        The ID of the Linode for which to get data. Can be used instead of\n        ``name``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f get_linode my-linode-config name=my-instance\n        salt-cloud -f get_linode my-linode-config linode_id=1234567", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_channel_embeddings_bottom(x, model_hparams, vocab_size):\n  \"\"\"\"\"\"\n  del vocab_size  # unused arg\n  inputs = tf.to_int32(x)\n  io_depth = model_hparams.num_channels\n  tshape = common_layers.shape_list(inputs)\n  hidden_size = model_hparams.hidden_size\n  target_embeddings = cia.get_channel_embeddings(\n      io_depth, inputs, hidden_size, \"input_bottom\")\n  return tf.reshape(target_embeddings,\n                    [tshape[0], tshape[1], tshape[2] * io_depth, hidden_size])", "output": "Bottom transformation for image targets.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_dns_name_availability(name, region, **kwargs):\n    '''\n    \n\n    '''\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        check_dns_name = netconn.check_dns_name_availability(\n            location=region,\n            domain_name_label=name\n        )\n        result = check_dns_name.as_dict()\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Check whether a domain name in the current zone is available for use.\n\n    :param name: The DNS name to query.\n\n    :param region: The region to query for the DNS name in question.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.check_dns_name_availability testdnsname westus", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _nxapi_request(commands, method='cli_conf', **kwargs):\n    '''\n    \n    '''\n    if CONNECTION == 'ssh':\n        return '_nxapi_request is not available for ssh proxy'\n    conn_args = DEVICE_DETAILS['conn_args']\n    conn_args.update(kwargs)\n    data = __utils__['nxos.nxapi_request'](commands, method=method, **conn_args)\n    return data", "output": "Executes an nxapi_request request over NX-API.\n\n    commands\n        The exec or config commands to be sent.\n\n    method: ``cli_show``\n        ``cli_show_ascii``: Return raw test or unstructured output.\n        ``cli_show``: Return structured output.\n        ``cli_conf``: Send configuration commands to the device.\n        Defaults to ``cli_conf``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_pooling_layers(self, start_node_id, end_node_id):\n        \"\"\"\"\"\"\n        layer_list = []\n        node_list = [start_node_id]\n        assert self._depth_first_search(end_node_id, layer_list, node_list)\n        ret = []\n        for layer_id in layer_list:\n            layer = self.layer_list[layer_id]\n            if is_layer(layer, \"Pooling\"):\n                ret.append(layer)\n            elif is_layer(layer, \"Conv\") and layer.stride != 1:\n                ret.append(layer)\n        return ret", "output": "Given two node IDs, return all the pooling layers between them.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def datapath4file(filename, ext:str='.tgz', archive=True):\n    \"\"\n    local_path = URLs.LOCAL_PATH/'data'/filename\n    if local_path.exists() or local_path.with_suffix(ext).exists(): return local_path\n    elif archive: return Config.data_archive_path() / filename\n    else: return Config.data_path() / filename", "output": "Return data path to `filename`, checking locally first then in the config file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getitem_column_array(self, key):\n        \"\"\"\n        \"\"\"\n        # Convert to list for type checking\n        numeric_indices = list(self.columns.get_indexer_for(key))\n\n        # Internal indices is left blank and the internal\n        # `apply_func_to_select_indices` will do the conversion and pass it in.\n        def getitem(df, internal_indices=[]):\n            return df.iloc[:, internal_indices]\n\n        result = self.data.apply_func_to_select_indices(\n            0, getitem, numeric_indices, keep_remaining=False\n        )\n        # We can't just set the columns to key here because there may be\n        # multiple instances of a key.\n        new_columns = self.columns[numeric_indices]\n        new_dtypes = self.dtypes[numeric_indices]\n        return self.__constructor__(result, self.index, new_columns, new_dtypes)", "output": "Get column data for target labels.\n\n        Args:\n            key: Target labels by which to retrieve data.\n\n        Returns:\n            A new QueryCompiler.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def create_text_channel(self, name, *, overwrites=None, reason=None, **options):\n        \"\"\"\n        \"\"\"\n        return await self.guild.create_text_channel(name, overwrites=overwrites, category=self, reason=reason, **options)", "output": "|coro|\n\n        A shortcut method to :meth:`Guild.create_text_channel` to create a :class:`TextChannel` in the category.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_hostname(name, **kwargs):\n    '''\n    \n\n    '''\n    ret = {'name': name, 'changes': {}, 'result': True, 'comment': ''}\n    ret['changes'] = __salt__['junos.set_hostname'](name, **kwargs)\n    return ret", "output": "Changes the hostname of the device.\n\n    .. code-block:: yaml\n\n            device_name:\n              junos:\n                - set_hostname\n                - comment: \"Host-name set via saltstack.\"\n\n\n    Parameters:\n     Required\n        * hostname: The name to be set. (default = None)\n     Optional\n        * kwargs: Keyworded arguments which can be provided like-\n            * timeout:\n              Set NETCONF RPC timeout. Can be used for commands\n              which take a while to execute. (default = 30 seconds)\n            * comment:\n              Provide a comment to the commit. (default = None)\n            * confirm:\n              Provide time in minutes for commit confirmation. \\\n              If this option is specified, the commit will be rollbacked in \\\n              the given time unless the commit is confirmed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convertFsDirWavToWav(dirName, Fs, nC):\n    '''\n    \n    '''\n\n    types = (dirName+os.sep+'*.wav',) # the tuple of file types\n    filesToProcess = []\n\n    for files in types:\n        filesToProcess.extend(glob.glob(files))     \n\n    newDir = dirName + os.sep + \"Fs\" + str(Fs) + \"_\" + \"NC\"+str(nC)\n    if os.path.exists(newDir) and newDir!=\".\":\n        shutil.rmtree(newDir)   \n    os.makedirs(newDir) \n\n    for f in filesToProcess:    \n        _, wavFileName = ntpath.split(f)    \n        command = \"avconv -i \\\"\" + f + \"\\\" -ar \" +str(Fs) + \" -ac \" + str(nC) + \" \\\"\" + newDir + os.sep + wavFileName + \"\\\"\";\n        print(command)\n        os.system(command)", "output": "This function converts the WAV files stored in a folder to WAV using a different sampling freq and number of channels.\n    ARGUMENTS:\n     - dirName:     the path of the folder where the WAVs are stored\n     - Fs:          the sampling rate of the generated WAV files\n     - nC:          the number of channesl of the generated WAV files", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def last_modified(self) -> Optional[datetime.datetime]:\n        \"\"\"\n        \"\"\"\n        httpdate = self._headers.get(hdrs.LAST_MODIFIED)\n        if httpdate is not None:\n            timetuple = parsedate(httpdate)\n            if timetuple is not None:\n                return datetime.datetime(*timetuple[:6],\n                                         tzinfo=datetime.timezone.utc)\n        return None", "output": "The value of Last-Modified HTTP header, or None.\n\n        This header is represented as a `datetime` object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def preferred_ip(vm_, ips):\n    '''\n    \n    '''\n    proto = config.get_cloud_config_value(\n        'protocol', vm_, __opts__, default='ipv4', search_global=False\n    )\n\n    family = socket.AF_INET\n    if proto == 'ipv6':\n        family = socket.AF_INET6\n    for ip in ips:\n        try:\n            socket.inet_pton(family, ip)\n            return ip\n        except Exception:\n            continue\n    return False", "output": "Return the preferred Internet protocol. Either 'ipv4' (default) or 'ipv6'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _process_graph(self):\n        \"\"\"\n        \n        \"\"\"\n        all_nodes = [x.name for x in self.graph.as_graph_def().node]\n        nodes = [x for x in all_nodes if x in self.possible_output_nodes]\n        logger.info('List of nodes to export for brain :' + self.brain.brain_name)\n        for n in nodes:\n            logger.info('\\t' + n)\n        return nodes", "output": "Gets the list of the output nodes present in the graph for inference\n        :return: list of node names", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_interfaces(zone, permanent=True):\n    '''\n    \n    '''\n    cmd = '--zone={0} --list-interfaces'.format(zone)\n\n    if permanent:\n        cmd += ' --permanent'\n\n    return __firewall_cmd(cmd).split()", "output": "List interfaces bound to a zone\n\n    .. versionadded:: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' firewalld.get_interfaces zone", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_logsoftmax(node, **kwargs):\n    \"\"\"\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    # Converting to int\n    axis = int(attrs.get(\"axis\", -1))\n    temp = attrs.get(\"temperature\", 'None')\n    if temp != 'None':\n        raise AttributeError(\"LogSoftMax: ONNX supports only temperature=None\")\n\n    node = onnx.helper.make_node(\n        'LogSoftmax',\n        input_nodes,\n        [name],\n        axis=axis,\n        name=name\n    )\n    return [node]", "output": "Map MXNet's log_softmax operator attributes to onnx's LogSoftMax operator\n    and return the created node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def predict_array(self, arr):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(arr, np.ndarray): raise OSError(f'Not valid numpy array')\n        self.model.eval()\n        return to_np(self.model(to_gpu(V(T(arr)))))", "output": "Args:\n            arr: a numpy array to be used as input to the model for prediction purposes\n        Returns:\n            a numpy array containing the predictions from the model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mtf_resnet_tiny():\n  \"\"\"\"\"\"\n  hparams = mtf_resnet_base()\n  hparams.num_layers = 2\n  hparams.hidden_size = 64\n  hparams.filter_size = 64\n  hparams.batch_size = 16\n  # data parallelism and model-parallelism\n  hparams.col_blocks = 1\n  hparams.mesh_shape = \"batch:2\"\n  hparams.layout = \"batch:batch\"\n  hparams.layer_sizes = [1, 2, 3]\n  hparams.filter_sizes = [64, 64, 64]\n  return hparams", "output": "Catch bugs locally...", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def open_mask_rle(mask_rle:str, shape:Tuple[int, int])->ImageSegment:\n    \"\"\n    x = FloatTensor(rle_decode(str(mask_rle), shape).astype(np.uint8))\n    x = x.view(shape[1], shape[0], -1)\n    return ImageSegment(x.permute(2,0,1))", "output": "Return `ImageSegment` object create from run-length encoded string in `mask_lre` with size in `shape`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def createTable(self, tableName, path=None, source=None, schema=None, **options):\n        \"\"\"\n        \"\"\"\n        if path is not None:\n            options[\"path\"] = path\n        if source is None:\n            source = self._sparkSession._wrapped._conf.defaultDataSourceName()\n        if schema is None:\n            df = self._jcatalog.createTable(tableName, source, options)\n        else:\n            if not isinstance(schema, StructType):\n                raise TypeError(\"schema should be StructType\")\n            scala_datatype = self._jsparkSession.parseDataType(schema.json())\n            df = self._jcatalog.createTable(tableName, source, scala_datatype, options)\n        return DataFrame(df, self._sparkSession._wrapped)", "output": "Creates a table based on the dataset in a data source.\n\n        It returns the DataFrame associated with the table.\n\n        The data source is specified by the ``source`` and a set of ``options``.\n        If ``source`` is not specified, the default data source configured by\n        ``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is\n        created from the data at the given path. Otherwise a managed table is created.\n\n        Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n        created table.\n\n        :return: :class:`DataFrame`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def model_loss(y, model, mean=True):\n  \"\"\"\n  \n  \"\"\"\n  warnings.warn(\"This function is deprecated and will be removed on or after\"\n                \" 2019-04-05. Switch to cleverhans.train.train.\")\n  op = model.op\n  if op.type == \"Softmax\":\n    logits, = op.inputs\n  else:\n    logits = model\n\n  out = softmax_cross_entropy_with_logits(logits=logits, labels=y)\n\n  if mean:\n    out = reduce_mean(out)\n  return out", "output": "Define loss of TF graph\n  :param y: correct labels\n  :param model: output of the model\n  :param mean: boolean indicating whether should return mean of loss\n               or vector of losses for each input of the batch\n  :return: return mean of loss if True, otherwise return vector with per\n           sample loss", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_cached_roles(self, name, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if name in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n        return self.transport.perform_request(\n            \"POST\", _make_path(\"_security\", \"role\", name, \"_clear_cache\"), params=params\n        )", "output": "`<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-clear-role-cache.html>`_\n\n        :arg name: Role name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_pooler(self, units, prefix):\n        \"\"\" \n\n        \"\"\"\n        with self.name_scope():\n            pooler = nn.Dense(units=units, flatten=False, activation='tanh',\n                              prefix=prefix)\n        return pooler", "output": "Construct pooler.\n\n        The pooler slices and projects the hidden output of first token\n        in the sequence for segment level classification.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _handle_get_application_request(self, app_id, semver, key, logical_id):\n        \"\"\"\n        \n        \"\"\"\n        get_application = (lambda app_id, semver: self._sar_client.get_application(\n                                   ApplicationId=self._sanitize_sar_str_param(app_id),\n                                   SemanticVersion=self._sanitize_sar_str_param(semver)))\n        try:\n            self._sar_service_call(get_application, logical_id, app_id, semver)\n            self._applications[key] = {'Available'}\n        except EndpointConnectionError as e:\n            # No internet connection. Don't break verification, but do show a warning.\n            warning_message = \"{}. Unable to verify access to {}/{}.\".format(e, app_id, semver)\n            logging.warning(warning_message)\n            self._applications[key] = {'Unable to verify'}", "output": "Method that handles the get_application API call to the serverless application repo\n\n        This method puts something in the `_applications` dictionary because the plugin expects\n        something there in a later event.\n\n        :param string app_id: ApplicationId\n        :param string semver: SemanticVersion\n        :param string key: The dictionary key consisting of (ApplicationId, SemanticVersion)\n        :param string logical_id: the logical_id of this application resource", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cummin(self, axis=0, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        if axis != 0:\n            return self.apply(lambda x: np.minimum.accumulate(x, axis))\n\n        return self._cython_transform('cummin', numeric_only=False)", "output": "Cumulative min for each group.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def detach(dev=None):\n    '''\n    \n\n    '''\n    if dev is None:\n        res = {}\n        for dev, data in status(alldevs=True).items():\n            if 'cache' in data:\n                res[dev] = detach(dev)\n\n        return res if res else None\n\n    log.debug('Detaching %s', dev)\n    if not _bcsys(dev, 'detach', 'goaway', 'error', 'Error detaching {0}'.format(dev)):\n        return False\n    return _wait(lambda: uuid(dev) is False, 'error', '{0} received detach, but did not comply'.format(dev), 300)", "output": "Detach a backing device(s) from a cache set\n    If no dev is given, all backing devices will be attached.\n\n    Detaching a backing device will flush it's write cache.\n    This should leave the underlying device in a consistent state, but might take a while.\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt '*' bcache.detach sdc\n        salt '*' bcache.detach bcache1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def maybe_load_model(savedir, container):\n  \"\"\"\"\"\"\n  if savedir is None:\n    return\n\n  state_path = os.path.join(os.path.join(savedir, 'training_state.pkl.zip'))\n  if container is not None:\n    logger.log(\"Attempting to download model from Azure\")\n    found_model = container.get(savedir, 'training_state.pkl.zip')\n  else:\n    found_model = os.path.exists(state_path)\n  if found_model:\n    state = pickle_load(state_path, compression=True)\n    model_dir = \"model-{}\".format(state[\"num_iters\"])\n    if container is not None:\n      container.get(savedir, model_dir)\n    U.load_state(os.path.join(savedir, model_dir, \"saved\"))\n    logger.log(\"Loaded models checkpoint at {} iterations\".format(\n        state[\"num_iters\"]))\n    return state", "output": "Load model if present at the specified path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def eventFilter(self, widget, event):\n        \"\"\"\n        \n        \"\"\"\n        if event.type() == QEvent.MouseButtonPress:\n            if event.button() == Qt.LeftButton:\n                self.sig_canvas_clicked.emit(self)\n        return super(FigureThumbnail, self).eventFilter(widget, event)", "output": "A filter that is used to send a signal when the figure canvas is\n        clicked.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_perms_changes(name, newperms, runas=None, existing=None):\n    '''\n    \n    '''\n    if not newperms:\n        return False\n\n    if existing is None:\n        try:\n            existing = __salt__['rabbitmq.list_user_permissions'](name, runas=runas)\n        except CommandExecutionError as err:\n            log.error('Error: %s', err)\n            return False\n\n    perm_need_change = False\n    for vhost_perms in newperms:\n        for vhost, perms in six.iteritems(vhost_perms):\n            if vhost in existing:\n                existing_vhost = existing[vhost]\n                if perms != existing_vhost:\n                    # This checks for setting permissions to nothing in the state,\n                    # when previous state runs have already set permissions to\n                    # nothing. We don't want to report a change in this case.\n                    if existing_vhost == '' and perms == ['', '', '']:\n                        continue\n                    perm_need_change = True\n            else:\n                perm_need_change = True\n\n    return perm_need_change", "output": "Check whether Rabbitmq user's permissions need to be changed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def split_by_idx(idxs, *a):\n    \"\"\"\n    \n    \"\"\"\n    mask = np.zeros(len(a[0]),dtype=bool)\n    mask[np.array(idxs)] = True\n    return [(o[mask],o[~mask]) for o in a]", "output": "Split each array passed as *a, to a pair of arrays like this (elements selected by idxs,  the remaining elements)\n    This can be used to split multiple arrays containing training data to validation and training set.\n\n    :param idxs [int]: list of indexes selected\n    :param a list: list of np.array, each array should have same amount of elements in the first dimension\n    :return: list of tuples, each containing a split of corresponding array from *a.\n            First element of each tuple is an array composed from elements selected by idxs,\n            second element is an array of remaining elements.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_adapter_class(validate_class, adapter_class):\n    \"\"\"\n    \n    \"\"\"\n    from chatterbot.adapters import Adapter\n\n    # If a dictionary was passed in, check if it has an import_path attribute\n    if isinstance(validate_class, dict):\n\n        if 'import_path' not in validate_class:\n            raise Adapter.InvalidAdapterTypeException(\n                'The dictionary {} must contain a value for \"import_path\"'.format(\n                    str(validate_class)\n                )\n            )\n\n        # Set the class to the import path for the next check\n        validate_class = validate_class.get('import_path')\n\n    if not issubclass(import_module(validate_class), adapter_class):\n        raise Adapter.InvalidAdapterTypeException(\n            '{} must be a subclass of {}'.format(\n                validate_class,\n                adapter_class.__name__\n            )\n        )", "output": "Raises an exception if validate_class is not a\n    subclass of adapter_class.\n\n    :param validate_class: The class to be validated.\n    :type validate_class: class\n\n    :param adapter_class: The class type to check against.\n    :type adapter_class: class\n\n    :raises: Adapter.InvalidAdapterTypeException", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_tensor_info(self):\n    \"\"\"\"\"\"\n    return {\n        feature_key: feature.get_tensor_info()\n        for feature_key, feature in self._feature_dict.items()\n    }", "output": "See base class for details.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare_installed_requirement(self, req, require_hashes, skip_reason):\n        # type: (InstallRequirement, bool, Optional[str]) -> DistAbstraction\n        \"\"\"\n        \"\"\"\n        assert req.satisfied_by, \"req should have been satisfied but isn't\"\n        assert skip_reason is not None, (\n            \"did not get skip reason skipped but req.satisfied_by \"\n            \"is set to %r\" % (req.satisfied_by,)\n        )\n        logger.info(\n            'Requirement %s: %s (%s)',\n            skip_reason, req, req.satisfied_by.version\n        )\n        with indent_log():\n            if require_hashes:\n                logger.debug(\n                    'Since it is already installed, we are trusting this '\n                    'package without checking its hash. To ensure a '\n                    'completely repeatable environment, install into an '\n                    'empty virtualenv.'\n                )\n            abstract_dist = Installed(req)\n\n        return abstract_dist", "output": "Prepare an already-installed requirement", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def where(cond, a, b, use_numexpr=True):\n    \"\"\" \n        \"\"\"\n\n    if use_numexpr:\n        return _where(cond, a, b)\n    return _where_standard(cond, a, b)", "output": "evaluate the where condition cond on a and b\n\n        Parameters\n        ----------\n\n        cond : a boolean array\n        a :    return if cond is True\n        b :    return if cond is False\n        use_numexpr : whether to try to use numexpr (default True)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sample_mog(prob, mean, var, rng):\n    \"\"\"\n    \"\"\"\n    gaussian_inds = sample_categorical(prob, rng).astype(numpy.int32)\n    mean = mean[numpy.arange(mean.shape[0]), gaussian_inds, :]\n    var = var[numpy.arange(mean.shape[0]), gaussian_inds, :]\n    ret = sample_normal(mean=mean, var=var, rng=rng)\n    return ret", "output": "Sample from independent mixture of gaussian (MoG) distributions\n\n    Each batch is an independent MoG distribution.\n\n    Parameters\n    ----------\n    prob : numpy.ndarray\n      mixture probability of each gaussian. Shape --> (batch_num, center_num)\n    mean : numpy.ndarray\n      mean of each gaussian. Shape --> (batch_num, center_num, sample_dim)\n    var : numpy.ndarray\n      variance of each gaussian. Shape --> (batch_num, center_num, sample_dim)\n    rng : numpy.random.RandomState\n\n    Returns\n    -------\n    ret : numpy.ndarray\n      sampling result. Shape --> (batch_num, sample_dim)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bool(self):\r\n        \"\"\"\r\n        \"\"\"\r\n        shape = self.shape\r\n        if shape != (1,) and shape != (1, 1):\r\n            raise ValueError(\r\n                \"\"\"The PandasObject does not have exactly\r\n                                1 element. Return the bool of a single\r\n                                element PandasObject. The truth value is\r\n                                ambiguous. Use a.empty, a.item(), a.any()\r\n                                or a.all().\"\"\"\r\n            )\r\n        else:\r\n            return self._to_pandas().bool()", "output": "Return the bool of a single element PandasObject.\r\n\r\n        This must be a boolean scalar value, either True or False.  Raise a\r\n        ValueError if the PandasObject does not have exactly 1 element, or that\r\n        element is not boolean", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def checkout(self):\n        '''\n        \n        '''\n        self.pillar_dirs = OrderedDict()\n        self.pillar_linked_dirs = []\n        for repo in self.remotes:\n            cachedir = self.do_checkout(repo)\n            if cachedir is not None:\n                # Figure out which environment this remote should be assigned\n                if repo.branch == '__env__' and hasattr(repo, 'all_saltenvs'):\n                    env = self.opts.get('pillarenv') \\\n                        or self.opts.get('saltenv') \\\n                        or self.opts.get('git_pillar_base')\n                elif repo.env:\n                    env = repo.env\n                else:\n                    if repo.branch == repo.base:\n                        env = 'base'\n                    else:\n                        tgt = repo.get_checkout_target()\n                        env = 'base' if tgt == repo.base else tgt\n                if repo._mountpoint:\n                    if self.link_mountpoint(repo):\n                        self.pillar_dirs[repo.linkdir] = env\n                        self.pillar_linked_dirs.append(repo.linkdir)\n                else:\n                    self.pillar_dirs[cachedir] = env", "output": "Checkout the targeted branches/tags from the git_pillar remotes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pack(o, stream, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    packer = Packer(**kwargs)\n    stream.write(packer.pack(o))", "output": "Pack object `o` and write it to `stream`\n\n    See :class:`Packer` for options.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _shape(self, df):\n        \"\"\"\n        \n        \"\"\"\n\n        row, col = df.shape\n        return row + df.columns.nlevels, col + df.index.nlevels", "output": "Calculate table chape considering index levels.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Graph(self):\n    \"\"\"\n    \"\"\"\n    graph = graph_pb2.GraphDef()\n    if self._graph is not None:\n      graph.ParseFromString(self._graph)\n      return graph\n    raise ValueError('There is no graph in this EventAccumulator')", "output": "Return the graph definition, if there is one.\n\n    If the graph is stored directly, return that.  If no graph is stored\n    directly but a metagraph is stored containing a graph, return that.\n\n    Raises:\n      ValueError: If there is no graph for this run.\n\n    Returns:\n      The `graph_def` proto.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sysv_enable(name):\n    '''\n    \n    '''\n    if not _service_is_chkconfig(name) and not _chkconfig_add(name):\n        return False\n    cmd = '/sbin/chkconfig {0} on'.format(name)\n    return not __salt__['cmd.retcode'](cmd, python_shell=False)", "output": "Enable the named sysv service to start at boot.  The service will be enabled\n    using chkconfig with default run-levels if the service is chkconfig\n    compatible.  If chkconfig is not available, then this will fail.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _finish_futures(self, responses):\n        \"\"\"\n        \"\"\"\n        # If a bad status occurs, we track it, but don't raise an exception\n        # until all futures have been populated.\n        exception_args = None\n\n        if len(self._target_objects) != len(responses):\n            raise ValueError(\"Expected a response for every request.\")\n\n        for target_object, subresponse in zip(self._target_objects, responses):\n            if not 200 <= subresponse.status_code < 300:\n                exception_args = exception_args or subresponse\n            elif target_object is not None:\n                try:\n                    target_object._properties = subresponse.json()\n                except ValueError:\n                    target_object._properties = subresponse.content\n\n        if exception_args is not None:\n            raise exceptions.from_http_response(exception_args)", "output": "Apply all the batch responses to the futures created.\n\n        :type responses: list of (headers, payload) tuples.\n        :param responses: List of headers and payloads from each response in\n                          the batch.\n\n        :raises: :class:`ValueError` if no requests have been deferred.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install_toolset(self, toolset):\n        '''\n        \n        '''\n        info = toolset_info[toolset]\n        if sys.platform.startswith('linux'):\n            os.chdir(self.work_dir)\n            if 'ppa' in info:\n                for ppa in info['ppa']:\n                    utils.check_call(\n                        'sudo','add-apt-repository','--yes',ppa)\n            if 'deb' in info:\n                utils.make_file('sources.list',\n                    \"deb %s\"%(' '.join(info['deb'])),\n                    \"deb-src %s\"%(' '.join(info['deb'])))\n                utils.check_call('sudo','bash','-c','cat sources.list >> /etc/apt/sources.list')\n            if 'apt-key' in info:\n                for key in info['apt-key']:\n                    utils.check_call('wget',key,'-O','apt.key')\n                    utils.check_call('sudo','apt-key','add','apt.key')\n            utils.check_call(\n                'sudo','apt-get','update','-qq')\n            utils.check_call(\n                'sudo','apt-get','install','-qq',info['package'])\n            if 'debugpackage' in info and info['debugpackage']:\n                utils.check_call(\n                    'sudo','apt-get','install','-qq',info['debugpackage'])", "output": "Installs specific toolset on CI system.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_fifo(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            return S_ISFIFO(self.stat().st_mode)\n        except OSError as e:\n            if e.errno not in (ENOENT, ENOTDIR):\n                raise\n            # Path doesn't exist or is a broken symlink\n            # (see https://bitbucket.org/pitrou/pathlib/issue/12/)\n            return False", "output": "Whether this path is a FIFO.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _file_read(path):\n    '''\n    \n    '''\n    content = False\n    if os.path.exists(path):\n        with salt.utils.files.fopen(path, 'r+') as fp_:\n            content = salt.utils.stringutils.to_unicode(fp_.read())\n        fp_.close()\n    return content", "output": "Read a file and return content", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def undefinedImageType(self):\n        \"\"\"\n        \n        \"\"\"\n\n        if self._undefinedImageType is None:\n            ctx = SparkContext._active_spark_context\n            self._undefinedImageType = \\\n                ctx._jvm.org.apache.spark.ml.image.ImageSchema.undefinedImageType()\n        return self._undefinedImageType", "output": "Returns the name of undefined image type for the invalid image.\n\n        .. versionadded:: 2.3.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_source(self, source):\n        \"\"\"\n        \n        \"\"\"\n        if not is_valid_url(source):\n            try:\n                source = self.get_source(name=source)\n            except SourceNotFound:\n                source = self.get_source(url=source)\n        else:\n            source = self.get_source(url=source)\n        return source", "output": "Given a source, find it.\n\n        source can be a url or an index name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_folders(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The list_folders function must be called with '\n            '-f or --function.'\n        )\n\n    return {'Folders': salt.utils.vmware.list_folders(_get_si())}", "output": "List all the folders for this VMware environment\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f list_folders my-vmware-config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next(self):\n        \"\"\"\n        \n        \"\"\"\n        v=self.queue.pop(0)\n        ret=v.pop(0)\n        if v: self.queue.append(v)\n        return ret", "output": ":rtype: int", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flatten_output(task):\n    \"\"\"\n    \n    \"\"\"\n    r = flatten(task.output())\n    if not r:\n        for dep in flatten(task.requires()):\n            r += flatten_output(dep)\n    return r", "output": "Lists all output targets by recursively walking output-less (wrapper) tasks.\n\n    FIXME order consistently.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def example8():\n    \"\"\"\"\"\"\n\n    # See cython_blas.pyx for argument documentation\n    mat = np.array([[[2.0, 2.0], [2.0, 2.0]], [[2.0, 2.0], [2.0, 2.0]]],\n                   dtype=np.float32)\n    result = np.zeros((2, 2), np.float32, order=\"C\")\n\n    run_func(cyth.compute_kernel_matrix,\n             \"L\",\n             \"T\",\n             2,\n             2,\n             1.0,\n             mat,\n             0,\n             2,\n             1.0,\n             result,\n             2\n             )", "output": "Cython with blas. NOTE: requires scipy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select(self, field_paths):\n        \"\"\"\n        \"\"\"\n        field_paths = list(field_paths)\n        for field_path in field_paths:\n            field_path_module.split_field_path(field_path)  # raises\n\n        new_projection = query_pb2.StructuredQuery.Projection(\n            fields=[\n                query_pb2.StructuredQuery.FieldReference(field_path=field_path)\n                for field_path in field_paths\n            ]\n        )\n        return self.__class__(\n            self._parent,\n            projection=new_projection,\n            field_filters=self._field_filters,\n            orders=self._orders,\n            limit=self._limit,\n            offset=self._offset,\n            start_at=self._start_at,\n            end_at=self._end_at,\n        )", "output": "Project documents matching query to a limited set of fields.\n\n        See :meth:`~.firestore_v1beta1.client.Client.field_path` for\n        more information on **field paths**.\n\n        If the current query already has a projection set (i.e. has already\n        called :meth:`~.firestore_v1beta1.query.Query.select`), this\n        will overwrite it.\n\n        Args:\n            field_paths (Iterable[str, ...]): An iterable of field paths\n                (``.``-delimited list of field names) to use as a projection\n                of document fields in the query results.\n\n        Returns:\n            ~.firestore_v1beta1.query.Query: A \"projected\" query. Acts as\n            a copy of the current query, modified with the newly added\n            projection.\n        Raises:\n            ValueError: If any ``field_path`` is invalid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _MergeMessage(\n    node, source, destination, replace_message, replace_repeated):\n  \"\"\"\"\"\"\n  source_descriptor = source.DESCRIPTOR\n  for name in node:\n    child = node[name]\n    field = source_descriptor.fields_by_name[name]\n    if field is None:\n      raise ValueError('Error: Can\\'t find field {0} in message {1}.'.format(\n          name, source_descriptor.full_name))\n    if child:\n      # Sub-paths are only allowed for singular message fields.\n      if (field.label == FieldDescriptor.LABEL_REPEATED or\n          field.cpp_type != FieldDescriptor.CPPTYPE_MESSAGE):\n        raise ValueError('Error: Field {0} in message {1} is not a singular '\n                         'message field and cannot have sub-fields.'.format(\n                             name, source_descriptor.full_name))\n      _MergeMessage(\n          child, getattr(source, name), getattr(destination, name),\n          replace_message, replace_repeated)\n      continue\n    if field.label == FieldDescriptor.LABEL_REPEATED:\n      if replace_repeated:\n        destination.ClearField(_StrConvert(name))\n      repeated_source = getattr(source, name)\n      repeated_destination = getattr(destination, name)\n      if field.cpp_type == FieldDescriptor.CPPTYPE_MESSAGE:\n        for item in repeated_source:\n          repeated_destination.add().MergeFrom(item)\n      else:\n        repeated_destination.extend(repeated_source)\n    else:\n      if field.cpp_type == FieldDescriptor.CPPTYPE_MESSAGE:\n        if replace_message:\n          destination.ClearField(_StrConvert(name))\n        if source.HasField(name):\n          getattr(destination, name).MergeFrom(getattr(source, name))\n      else:\n        setattr(destination, name, getattr(source, name))", "output": "Merge all fields specified by a sub-tree from source to destination.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_output_dataframe(data_subset, defaults):\n    \"\"\"\n    \n    \"\"\"\n    # The columns provided.\n    cols = set(data_subset.columns)\n    desired_cols = set(defaults)\n\n    # Drop columns with unrecognised headers.\n    data_subset.drop(cols - desired_cols,\n                     axis=1,\n                     inplace=True)\n\n    # Get those columns which we need but\n    # for which no data has been supplied.\n    for col in desired_cols - cols:\n        # write the default value for any missing columns\n        data_subset[col] = defaults[col](data_subset, col)\n\n    return data_subset", "output": "Generates an output dataframe from the given subset of user-provided\n    data, the given column names, and the given default values.\n\n    Parameters\n    ----------\n    data_subset : DataFrame\n        A DataFrame, usually from an AssetData object,\n        that contains the user's input metadata for the asset type being\n        processed\n    defaults : dict\n        A dict where the keys are the names of the columns of the desired\n        output DataFrame and the values are a function from dataframe and\n        column name to the default values to insert in the DataFrame if no user\n        data is provided\n\n    Returns\n    -------\n    DataFrame\n        A DataFrame containing all user-provided metadata, and default values\n        wherever user-provided metadata was missing", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def assemble(name,\n             devices,\n             test_mode=False,\n             **kwargs):\n    '''\n    \n    '''\n    opts = []\n    for key in kwargs:\n        if not key.startswith('__'):\n            opts.append('--{0}'.format(key))\n            if kwargs[key] is not True:\n                opts.append(kwargs[key])\n\n    # Devices may have been written with a blob:\n    if isinstance(devices, six.string_types):\n        devices = devices.split(',')\n\n    cmd = ['mdadm', '-A', name, '-v'] + opts + devices\n\n    if test_mode is True:\n        return cmd\n    elif test_mode is False:\n        return __salt__['cmd.run'](cmd, python_shell=False)", "output": "Assemble a RAID device.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' raid.assemble /dev/md0 ['/dev/xvdd', '/dev/xvde']\n\n    .. note::\n\n        Adding ``test_mode=True`` as an argument will print out the mdadm\n        command that would have been run.\n\n    name\n        The name of the array to assemble.\n\n    devices\n        The list of devices comprising the array to assemble.\n\n    kwargs\n        Optional arguments to be passed to mdadm.\n\n    returns\n        test_mode=True:\n            Prints out the full command.\n        test_mode=False (Default):\n            Executes command on the host(s) and prints out the mdadm output.\n\n    For more info, read the ``mdadm`` manpage.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_groupby(environment, value, attribute):\n    \"\"\"\n    \"\"\"\n    expr = make_attrgetter(environment, attribute)\n    return [_GroupTuple(key, list(values)) for key, values\n            in groupby(sorted(value, key=expr), expr)]", "output": "Group a sequence of objects by a common attribute.\n\n    If you for example have a list of dicts or objects that represent persons\n    with `gender`, `first_name` and `last_name` attributes and you want to\n    group all users by genders you can do something like the following\n    snippet:\n\n    .. sourcecode:: html+jinja\n\n        <ul>\n        {% for group in persons|groupby('gender') %}\n            <li>{{ group.grouper }}<ul>\n            {% for person in group.list %}\n                <li>{{ person.first_name }} {{ person.last_name }}</li>\n            {% endfor %}</ul></li>\n        {% endfor %}\n        </ul>\n\n    Additionally it's possible to use tuple unpacking for the grouper and\n    list:\n\n    .. sourcecode:: html+jinja\n\n        <ul>\n        {% for grouper, list in persons|groupby('gender') %}\n            ...\n        {% endfor %}\n        </ul>\n\n    As you can see the item we're grouping by is stored in the `grouper`\n    attribute and the `list` contains all the objects that have this grouper\n    in common.\n\n    .. versionchanged:: 2.6\n       It's now possible to use dotted notation to group by the child\n       attribute of another attribute.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def plot_lr(self, show_moms=False, skip_start:int=0, skip_end:int=0, return_fig:bool=None)->Optional[plt.Figure]:\n        \"\"\n        lrs = self._split_list(self.lrs, skip_start, skip_end)\n        iterations = self._split_list(range_of(self.lrs), skip_start, skip_end)\n        if show_moms:\n            moms = self._split_list(self.moms, skip_start, skip_end)\n            fig, axs = plt.subplots(1,2, figsize=(12,4))\n            axs[0].plot(iterations, lrs)\n            axs[0].set_xlabel('Iterations')\n            axs[0].set_ylabel('Learning Rate')\n            axs[1].plot(iterations, moms)\n            axs[1].set_xlabel('Iterations')\n            axs[1].set_ylabel('Momentum')\n        else:\n            fig, ax = plt.subplots()\n            ax.plot(iterations, lrs)\n            ax.set_xlabel('Iterations')\n            ax.set_ylabel('Learning Rate')\n        if ifnone(return_fig, defaults.return_fig): return fig\n        if not IN_NOTEBOOK: plot_sixel(fig)", "output": "Plot learning rate, `show_moms` to include momentum.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_all_runs(self, session=None):\n        \"\"\"\n        \n        \"\"\"\n        with self._session(session) as session:\n            return session.query(TaskRecord).all()", "output": "Return all tasks that have been updated.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_max_size(self, commands):\n        \"\"\"\n        \"\"\"\n\n        as_lengths = (\n            discord.utils._string_width(c.name)\n            for c in commands\n        )\n        return max(as_lengths, default=0)", "output": "Returns the largest name length of the specified command list.\n\n        Parameters\n        ------------\n        commands: Sequence[:class:`Command`]\n            A sequence of commands to check for the largest size.\n\n        Returns\n        --------\n        :class:`int`\n            The maximum width of the commands.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_text_file(filename):\n    # type: (str) -> str\n    \"\"\"\n\n    \"\"\"\n    with open(filename, 'rb') as fp:\n        data = fp.read()\n\n    encodings = ['utf-8', locale.getpreferredencoding(False), 'latin1']\n    for enc in encodings:\n        try:\n            # https://github.com/python/mypy/issues/1174\n            data = data.decode(enc)  # type: ignore\n        except UnicodeDecodeError:\n            continue\n        break\n\n    assert not isinstance(data, bytes)  # Latin1 should have worked.\n    return data", "output": "Return the contents of *filename*.\n\n    Try to decode the file contents with utf-8, the preferred system encoding\n    (e.g., cp1252 on some Windows machines), and latin1, in that order.\n    Decoding a byte string with latin1 will never raise an error. In the worst\n    case, the returned string will contain some garbage characters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_parents(folds, linenum):\n    \"\"\"\n    \n    \"\"\"\n    # Note: this might be able to be sped up by finding some kind of\n    # abort-early condition.\n    parents = []\n    for fold in folds:\n        start, end = fold.range\n        if linenum >= start and linenum <= end:\n            parents.append(fold)\n        else:\n            continue\n\n    return parents", "output": "Get the parents at a given linenum.\n\n    If parents is empty, then the linenum belongs to the module.\n\n    Parameters\n    ----------\n    folds : list of :class:`FoldScopeHelper`\n    linenum : int\n        The line number to get parents for. Typically this would be the\n        cursor position.\n\n    Returns\n    -------\n    parents : list of :class:`FoldScopeHelper`\n        A list of :class:`FoldScopeHelper` objects that describe the defintion\n        heirarcy for the given ``linenum``. The 1st index will be the\n        top-level parent defined at the module level while the last index\n        will be the class or funtion that contains ``linenum``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def done(self):\n        \"\"\"\n        \n        \"\"\"\n        logger.info('Marking %s as done', self)\n\n        fn = self.get_path()\n        try:\n            os.makedirs(os.path.dirname(fn))\n        except OSError:\n            pass\n        open(fn, 'w').close()", "output": "Creates temporary file to mark the task as `done`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _run_command(cmd, options=(), env=None):\n    '''\n    \n    '''\n    params = [cmd]\n    params.extend(options)\n    return __salt__['cmd.run_all'](params, env=env, python_shell=False)", "output": "Runs the command cmd with options as its CLI parameters and returns the\n    result as a dictionary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _SetOptions(self, options, options_class_name):\n    \"\"\"\n    \"\"\"\n    self._options = options\n    self._options_class_name = options_class_name\n\n    # Does this descriptor have non-default options?\n    self.has_options = options is not None", "output": "Sets the descriptor's options\n\n    This function is used in generated proto2 files to update descriptor\n    options. It must not be used outside proto2.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_to_style(cls, style_dict, num_format_str=None):\n        \"\"\"\n        \n        \"\"\"\n        import xlwt\n\n        if style_dict:\n            xlwt_stylestr = cls._style_to_xlwt(style_dict)\n            style = xlwt.easyxf(xlwt_stylestr, field_sep=',', line_sep=';')\n        else:\n            style = xlwt.XFStyle()\n        if num_format_str is not None:\n            style.num_format_str = num_format_str\n\n        return style", "output": "converts a style_dict to an xlwt style object\n        Parameters\n        ----------\n        style_dict : style dictionary to convert\n        num_format_str : optional number format string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_postdecode_hooks(decode_hook_args, dataset_split):\n  \"\"\"\"\"\"\n  hooks = decode_hook_args.problem.decode_hooks\n  if not hooks:\n    return\n  global_step = latest_checkpoint_step(decode_hook_args.estimator.model_dir)\n  if global_step is None:\n    tf.logging.info(\n        \"Skipping decode hooks because no checkpoint yet available.\")\n    return\n  tf.logging.info(\"Running decode hooks.\")\n  parent_dir = os.path.join(decode_hook_args.output_dirs[0], os.pardir)\n  child_dir = decode_hook_args.decode_hparams.summaries_log_dir\n  if dataset_split is not None:\n    child_dir += \"_{}\".format(dataset_split)\n  final_dir = os.path.join(parent_dir, child_dir)\n  summary_writer = tf.summary.FileWriter(final_dir)\n\n  for hook in hooks:\n    # Isolate each hook in case it creates TF ops\n    with tf.Graph().as_default():\n      summaries = hook(decode_hook_args)\n    if summaries:\n      summary = tf.Summary(value=list(summaries))\n      summary_writer.add_summary(summary, global_step)\n  summary_writer.close()\n  tf.logging.info(\"Decode hooks done.\")", "output": "Run hooks after decodes have run.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def std(self, bias=False, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        nv.validate_window_func('std', args, kwargs)\n        return _zsqrt(self.var(bias=bias, **kwargs))", "output": "Exponential weighted moving stddev.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathNextChild(self, cur):\n        \"\"\" \"\"\"\n        if cur is None: cur__o = None\n        else: cur__o = cur._o\n        ret = libxml2mod.xmlXPathNextChild(self._o, cur__o)\n        if ret is None:raise xpathError('xmlXPathNextChild() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Traversal function for the \"child\" direction The child axis\n          contains the children of the context node in document order.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_resources(self, cpu, gpu, **kwargs):\n        \"\"\"\n        \"\"\"\n        if self.status is Trial.RUNNING:\n            raise ValueError(\"Cannot update resources while Trial is running.\")\n        self.resources = Resources(cpu, gpu, **kwargs)", "output": "EXPERIMENTAL: Updates the resource requirements.\n\n        Should only be called when the trial is not running.\n\n        Raises:\n            ValueError if trial status is running.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fixpath(path):\r\n    \"\"\"\"\"\"\r\n    norm = osp.normcase if os.name == 'nt' else osp.normpath\r\n    return norm(osp.abspath(osp.realpath(path)))", "output": "Normalize path fixing case, making absolute and removing symlinks", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_image_grad(net, image, class_id=None):\n    \"\"\"\"\"\"\n    return _get_grad(net, image, class_id, image_grad=True)", "output": "Get the gradients of the image.\n\n    Parameters:\n    ----------\n    net: Block\n        Network to use for visualization.\n    image: NDArray\n        Preprocessed image to use for visualization.\n    class_id: int\n        Category ID this image belongs to. If not provided,\n        network's prediction will be used.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _key_opts(self):\n        '''\n        \n        '''\n        options = [\n                   'KbdInteractiveAuthentication=no',\n                   ]\n        if self.passwd:\n            options.append('PasswordAuthentication=yes')\n        else:\n            options.append('PasswordAuthentication=no')\n        if self.opts.get('_ssh_version', (0,)) > (4, 9):\n            options.append('GSSAPIAuthentication=no')\n        options.append('ConnectTimeout={0}'.format(self.timeout))\n        if self.opts.get('ignore_host_keys'):\n            options.append('StrictHostKeyChecking=no')\n        if self.opts.get('no_host_keys'):\n            options.extend(['StrictHostKeyChecking=no',\n                            'UserKnownHostsFile=/dev/null'])\n        known_hosts = self.opts.get('known_hosts_file')\n        if known_hosts and os.path.isfile(known_hosts):\n            options.append('UserKnownHostsFile={0}'.format(known_hosts))\n        if self.port:\n            options.append('Port={0}'.format(self.port))\n        if self.priv and self.priv != 'agent-forwarding':\n            options.append('IdentityFile={0}'.format(self.priv))\n        if self.user:\n            options.append('User={0}'.format(self.user))\n        if self.identities_only:\n            options.append('IdentitiesOnly=yes')\n\n        ret = []\n        for option in options:\n            ret.append('-o {0} '.format(option))\n        return ''.join(ret)", "output": "Return options for the ssh command base for Salt to call", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_timedelta_ticks(x, pos, n_decimals):\n    \"\"\"\n    \n    \"\"\"\n    s, ns = divmod(x, 1e9)\n    m, s = divmod(s, 60)\n    h, m = divmod(m, 60)\n    d, h = divmod(h, 24)\n    decimals = int(ns * 10**(n_decimals - 9))\n    s = r'{:02d}:{:02d}:{:02d}'.format(int(h), int(m), int(s))\n    if n_decimals > 0:\n        s += '.{{:0{:0d}d}}'.format(n_decimals).format(decimals)\n    if d != 0:\n        s = '{:d} days '.format(int(d)) + s\n    return s", "output": "Convert seconds to 'D days HH:MM:SS.F'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_program(program, args=None, **subprocess_kwargs):\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    if 'shell' in subprocess_kwargs and subprocess_kwargs['shell']:\r\n        raise ProgramError(\r\n                \"This function is only for non-shell programs, \"\r\n                \"use run_shell_command() instead.\")\r\n    fullcmd = find_program(program)\r\n    if not fullcmd:\r\n        raise ProgramError(\"Program %s was not found\" % program)\r\n    # As per subprocess, we make a complete list of prog+args\r\n    fullcmd = [fullcmd] + (args or [])\r\n    for stream in ['stdin', 'stdout', 'stderr']:\r\n        subprocess_kwargs.setdefault(stream, subprocess.PIPE)\r\n    subprocess_kwargs = alter_subprocess_kwargs_by_platform(\r\n            **subprocess_kwargs)\r\n    return subprocess.Popen(fullcmd, **subprocess_kwargs)", "output": "Run program in a separate process.\r\n\r\n    NOTE: returns the process object created by\r\n    `subprocess.Popen()`. This can be used with\r\n    `proc.communicate()` for example.\r\n\r\n    If 'shell' appears in the kwargs, it must be False,\r\n    otherwise ProgramError will be raised.\r\n\r\n    If only the program name is given and not the full path,\r\n    a lookup will be performed to find the program. If the\r\n    lookup fails, ProgramError will be raised.\r\n\r\n    Note that stdin, stdout and stderr will be set by default\r\n    to PIPE unless specified in subprocess_kwargs.\r\n\r\n    :str program: The name of the program to run.\r\n    :list args: The program arguments.\r\n    :subprocess_kwargs: These will be passed to subprocess.Popen.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_rule(self, rule_name):\n        \"\"\"\n        \n\n        \"\"\"\n        logger.debug('Deleting existing rule {}'.format(rule_name))\n\n        # All targets must be removed before\n        # we can actually delete the rule.\n        try:\n            targets = self.events_client.list_targets_by_rule(Rule=rule_name)\n        except botocore.exceptions.ClientError as e:\n            # This avoids misbehavior if low permissions, related: https://github.com/Miserlou/Zappa/issues/286\n            error_code = e.response['Error']['Code']\n            if error_code == 'AccessDeniedException':\n                raise\n            else:\n                logger.debug('No target found for this rule: {} {}'.format(rule_name, e.args[0]))\n                return\n\n        if 'Targets' in targets and targets['Targets']:\n            self.events_client.remove_targets(Rule=rule_name, Ids=[x['Id'] for x in targets['Targets']])\n        else:  # pragma: no cover\n            logger.debug('No target to delete')\n\n        # Delete our rule.\n        self.events_client.delete_rule(Name=rule_name)", "output": "Delete a CWE rule.\n\n        This  deletes them, but they will still show up in the AWS console.\n        Annoying.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def segmentation_to_mask(polys, height, width):\n    \"\"\"\n    \n    \"\"\"\n    polys = [p.flatten().tolist() for p in polys]\n    assert len(polys) > 0, \"Polygons are empty!\"\n\n    import pycocotools.mask as cocomask\n    rles = cocomask.frPyObjects(polys, height, width)\n    rle = cocomask.merge(rles)\n    return cocomask.decode(rle)", "output": "Convert polygons to binary masks.\n\n    Args:\n        polys: a list of nx2 float array. Each array contains many (x, y) coordinates.\n\n    Returns:\n        a binary matrix of (height, width)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_ext_pillar_extra_minion_data(self, opts):\n        '''\n        \n        '''\n        def get_subconfig(opts_key):\n            '''\n            Returns a dict containing the opts key subtree, while maintaining\n            the opts structure\n            '''\n            ret_dict = aux_dict = {}\n            config_val = opts\n            subkeys = opts_key.split(':')\n            # Build an empty dict with the opts path\n            for subkey in subkeys[:-1]:\n                aux_dict[subkey] = {}\n                aux_dict = aux_dict[subkey]\n                if not config_val.get(subkey):\n                    # The subkey is not in the config\n                    return {}\n                config_val = config_val[subkey]\n            if subkeys[-1] not in config_val:\n                return {}\n            aux_dict[subkeys[-1]] = config_val[subkeys[-1]]\n            return ret_dict\n\n        extra_data = {}\n        if 'pass_to_ext_pillars' in opts:\n            if not isinstance(opts['pass_to_ext_pillars'], list):\n                log.exception('\\'pass_to_ext_pillars\\' config is malformed.')\n                raise SaltClientError('\\'pass_to_ext_pillars\\' config is '\n                                      'malformed.')\n            for key in opts['pass_to_ext_pillars']:\n                salt.utils.dictupdate.update(extra_data,\n                                             get_subconfig(key),\n                                             recursive_update=True,\n                                             merge_lists=True)\n        log.trace('ext_pillar_extra_data = %s', extra_data)\n        return extra_data", "output": "Returns the extra data from the minion's opts dict (the config file).\n\n        This data will be passed to external pillar functions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main(_):\n  \"\"\"\"\"\"\n  eps = FLAGS.max_epsilon / 255.0\n  batch_shape = [FLAGS.batch_size, FLAGS.image_height, FLAGS.image_width, 3]\n\n  with tf.Graph().as_default():\n    x_input = tf.placeholder(tf.float32, shape=batch_shape)\n    noisy_images = x_input + eps * tf.sign(tf.random_normal(batch_shape))\n    x_output = tf.clip_by_value(noisy_images, 0.0, 1.0)\n\n    with tf.Session(FLAGS.master) as sess:\n      for filenames, images in load_images(FLAGS.input_dir, batch_shape):\n        out_images = sess.run(x_output, feed_dict={x_input: images})\n        save_images(out_images, filenames, FLAGS.output_dir)", "output": "Run the sample attack", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dot_product_single_head(q, k, v, gates_q, gates_k, bi):\n  \"\"\"\n  \"\"\"\n\n  nb_buckets = gates_q.get_shape().as_list()[-1]\n\n  q_dispatcher = expert_utils.SparseDispatcher(nb_buckets, gates_q)\n  k_dispatcher = expert_utils.SparseDispatcher(nb_buckets, gates_k)\n\n  def eventually_dispatch(dispatcher, value):\n    if value is not None:\n      return dispatcher.dispatch(value)\n    return [None] * nb_buckets\n\n  # Iterate over every dispatched group\n  list_v_out = []\n  for (\n      q_i,\n      k_i,\n      v_i,\n      qbc,\n      qbo,\n      kbc,\n      kbo,\n  ) in zip(\n      # Dispatch queries, keys and values\n      q_dispatcher.dispatch(q),\n      k_dispatcher.dispatch(k),\n      k_dispatcher.dispatch(v),\n      # Also dispatch the sequence positions and batch coordinates\n      eventually_dispatch(q_dispatcher, bi.coordinates),\n      eventually_dispatch(q_dispatcher, bi.order),\n      eventually_dispatch(k_dispatcher, bi.coordinates),\n      eventually_dispatch(k_dispatcher, bi.order),\n  ):\n    list_v_out.append(\n        expert_dot_product(\n            q_i,\n            k_i,\n            v_i,\n            info_q=BatchInfo(coordinates=qbc, order=qbo),\n            info_k=BatchInfo(coordinates=kbc, order=kbo)))\n\n  # Combine all buckets together to restore the original length\n  return q_dispatcher.combine(list_v_out)", "output": "Perform a dot product attention on a single sequence on a single head.\n\n  This function dispatch the q, k, v and loop over the buckets to compute the\n  attention dot product on each subsequences.\n\n  Args:\n    q (tf.Tensor): [length_q, depth_q]\n    k (tf.Tensor): [length_k, depth_q]\n    v (tf.Tensor): [length_k, depth_v]\n    gates_q (tf.Tensor): One-hot vector of shape [length_q, nb_buckets]\n    gates_k (tf.Tensor): One-hot vector of shape [length_k, nb_buckets]\n    bi (BatchInfo): Contains the batch coordinates and sequence order\n\n  Returns:\n    tf.Tensor: [length_q, depth_v]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def targets(self):\n        '''\n        \n        '''\n        addrs = ()\n        ret = {}\n        ports = __opts__['ssh_scan_ports']\n        if not isinstance(ports, list):\n            # Comma-separate list of integers\n            ports = list(map(int, six.text_type(ports).split(',')))\n        try:\n            addrs = [ipaddress.ip_address(self.tgt)]\n        except ValueError:\n            try:\n                addrs = ipaddress.ip_network(self.tgt).hosts()\n            except ValueError:\n                pass\n        for addr in addrs:\n            addr = six.text_type(addr)\n            ret[addr] = copy.deepcopy(__opts__.get('roster_defaults', {}))\n            log.trace('Scanning host: %s', addr)\n            for port in ports:\n                log.trace('Scanning port: %s', port)\n                try:\n                    sock = salt.utils.network.get_socket(addr, socket.SOCK_STREAM)\n                    sock.settimeout(float(__opts__['ssh_scan_timeout']))\n                    sock.connect((addr, port))\n                    sock.shutdown(socket.SHUT_RDWR)\n                    sock.close()\n                    ret[addr].update({'host': addr, 'port': port})\n                except socket.error:\n                    pass\n        return ret", "output": "Return ip addrs based on netmask, sitting in the \"glob\" spot because\n        it is the default", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lca(root, p, q):\n    \"\"\"\n    \n    \"\"\"\n    if root is None or root is p or root is q:\n        return root\n    left = lca(root.left, p, q)\n    right = lca(root.right, p, q)\n    if left is not None and right is not None:\n        return root\n    return left if left else right", "output": ":type root: TreeNode\n    :type p: TreeNode\n    :type q: TreeNode\n    :rtype: TreeNode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def snapshot(name, suffix=None, connection=None, username=None, password=None):\n    '''\n    \n    '''\n\n    return _virt_call(name, 'snapshot', 'saved', 'Snapshot has been taken', suffix=suffix,\n                      connection=connection, username=username, password=password)", "output": "Takes a snapshot of a particular VM or by a UNIX-style wildcard.\n\n    .. versionadded:: 2016.3.0\n\n    :param connection: libvirt connection URI, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param username: username to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param password: password to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n\n    .. code-block:: yaml\n\n        domain_name:\n          virt.snapshot:\n            - suffix: periodic\n\n        domain*:\n          virt.snapshot:\n            - suffix: periodic", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def MSELossFlat(*args, axis:int=-1, floatify:bool=True, **kwargs):\n    \"\"\n    return FlattenedLoss(nn.MSELoss, *args, axis=axis, floatify=floatify, is_2d=False, **kwargs)", "output": "Same as `nn.MSELoss`, but flattens input and target.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_util_make_hour_index(day, type_='1h'):\n    \"\"\"\n    \"\"\"\n\n    if QA_util_if_trade(day) is True:\n        return pd.date_range(\n            str(day) + ' 09:30:00',\n            str(day) + ' 11:30:00',\n            freq=type_,\n            closed='right'\n        ).append(\n            pd.date_range(\n                str(day) + ' 13:00:00',\n                str(day) + ' 15:00:00',\n                freq=type_,\n                closed='right'\n            )\n        )\n    else:\n        return pd.DataFrame(['No trade'])", "output": "\u521b\u5efa\u80a1\u7968\u7684\u5c0f\u65f6\u7ebf\u7684index\n\n    Arguments:\n        day {[type]} -- [description]\n\n    Returns:\n        [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dtdQElementDesc(self, name, prefix):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlGetDtdQElementDesc(self._o, name, prefix)\n        if ret is None:raise treeError('xmlGetDtdQElementDesc() failed')\n        __tmp = xmlElement(_obj=ret)\n        return __tmp", "output": "Search the DTD for the description of this element", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, timeout: Union[float, datetime.timedelta] = None) -> Awaitable[_T]:\n        \"\"\"\n\n        \"\"\"\n        future = Future()  # type: Future[_T]\n        try:\n            future.set_result(self.get_nowait())\n        except QueueEmpty:\n            self._getters.append(future)\n            _set_timeout(future, timeout)\n        return future", "output": "Remove and return an item from the queue.\n\n        Returns an awaitable which resolves once an item is available, or raises\n        `tornado.util.TimeoutError` after a timeout.\n\n        ``timeout`` may be a number denoting a time (on the same\n        scale as `tornado.ioloop.IOLoop.time`, normally `time.time`), or a\n        `datetime.timedelta` object for a deadline relative to the\n        current time.\n\n        .. note::\n\n           The ``timeout`` argument of this method differs from that\n           of the standard library's `queue.Queue.get`. That method\n           interprets numeric values as relative timeouts; this one\n           interprets them as absolute deadlines and requires\n           ``timedelta`` objects for relative timeouts (consistent\n           with other timeouts in Tornado).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _decrypt_object(obj, translate_newlines=False):\n    '''\n    \n    '''\n    if salt.utils.stringio.is_readable(obj):\n        return _decrypt_object(obj.getvalue(), translate_newlines)\n    if isinstance(obj, six.string_types):\n        return _decrypt_ciphertexts(obj, translate_newlines=translate_newlines)\n    elif isinstance(obj, dict):\n        for key, value in six.iteritems(obj):\n            obj[key] = _decrypt_object(value,\n                                       translate_newlines=translate_newlines)\n        return obj\n    elif isinstance(obj, list):\n        for key, value in enumerate(obj):\n            obj[key] = _decrypt_object(value,\n                                       translate_newlines=translate_newlines)\n        return obj\n    else:\n        return obj", "output": "Recursively try to decrypt any object. If the object is a six.string_types\n    (string or unicode), and it contains a valid GPG header, decrypt it,\n    otherwise keep going until a string is found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _merge_multi_context(outputs, major_axis):\n    \"\"\"\n    \"\"\"\n    rets = []\n    for tensors, axis in zip(outputs, major_axis):\n        if axis >= 0:\n            # pylint: disable=no-member,protected-access\n            if len(tensors) == 1:\n                rets.append(tensors[0])\n            else:\n                # Concatenate if necessary\n                rets.append(nd.concat(*[tensor.as_in_context(tensors[0].context)\n                                        for tensor in tensors],\n                                      dim=axis))\n            # pylint: enable=no-member,protected-access\n        else:\n            # negative axis means the there is no batch_size axis, and all the\n            # results should be the same on each device. We simply take the\n            # first one, without checking they are actually the same\n            rets.append(tensors[0])\n    return rets", "output": "Merge outputs that lives on multiple context into one, so that they look\n    like living on one context.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def var(self, ddof=1, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        nv.validate_resampler_func('var', args, kwargs)\n        return self._downsample('var', ddof=ddof)", "output": "Compute variance of groups, excluding missing values.\n\n        Parameters\n        ----------\n        ddof : integer, default 1\n            degrees of freedom", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_site(name):\n    '''\n    \n    '''\n    ps_cmd = ['Start-WebSite', r\"'{0}'\".format(name)]\n\n    cmd_ret = _srvmgr(ps_cmd)\n\n    return cmd_ret['retcode'] == 0", "output": "Start a Web Site in IIS.\n\n    .. versionadded:: 2017.7.0\n\n    Args:\n        name (str): The name of the website to start.\n\n    Returns:\n        bool: True if successful, otherwise False\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' win_iis.start_site name='My Test Site'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shared_locations(self):\n        \"\"\"\n        \n        \"\"\"\n        result = {}\n        shared_path = os.path.join(self.path, 'SHARED')\n        if os.path.isfile(shared_path):\n            with codecs.open(shared_path, 'r', encoding='utf-8') as f:\n                lines = f.read().splitlines()\n            for line in lines:\n                key, value = line.split('=', 1)\n                if key == 'namespace':\n                    result.setdefault(key, []).append(value)\n                else:\n                    result[key] = value\n        return result", "output": "A dictionary of shared locations whose keys are in the set 'prefix',\n        'purelib', 'platlib', 'scripts', 'headers', 'data' and 'namespace'.\n        The corresponding value is the absolute path of that category for\n        this distribution, and takes into account any paths selected by the\n        user at installation time (e.g. via command-line arguments). In the\n        case of the 'namespace' key, this would be a list of absolute paths\n        for the roots of namespace packages in this distribution.\n\n        The first time this property is accessed, the relevant information is\n        read from the SHARED file in the .dist-info directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def top_k_with_unique(inputs, k):\n  \"\"\"\n  \"\"\"\n  unique_inputs = _create_make_unique(tf.cast(inputs, tf.float32))\n  top_values, indices = _create_topk_unique(unique_inputs, k)\n  top_values = tf.cast(top_values, inputs.dtype)\n  return top_values, indices", "output": "Finds the values and indices of the k largests entries.\n\n  Instead of doing sort like tf.nn.top_k, this function finds the max value\n  k times. The running time is proportional to k, which is be faster when k\n  is small. The current implementation supports only inputs of rank 2.\n  In addition, iota is used to replace the lower bits of each element, this\n  makes the selection more stable when there are equal elements. The\n  overhead is that output values are approximated.\n\n  Args:\n    inputs: A tensor with rank of 2. [batch_size, original_size].\n    k: An integer, number of top elements to select.\n\n  Returns:\n    top_values: A tensor, the k largest elements in sorted order.\n      [batch_size, k].\n    indices: A tensor, indices of the top_values. [batch_size, k].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nodeListGetRawString(self, list, inLine):\n        \"\"\" \"\"\"\n        if list is None: list__o = None\n        else: list__o = list._o\n        ret = libxml2mod.xmlNodeListGetRawString(self._o, list__o, inLine)\n        return ret", "output": "Builds the string equivalent to the text contained in the\n          Node list made of TEXTs and ENTITY_REFs, contrary to\n          xmlNodeListGetString() this function doesn't do any\n           character encoding handling.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deregister_hook(self, event, hook):\n        \"\"\"\n        \"\"\"\n\n        try:\n            self.hooks[event].remove(hook)\n            return True\n        except ValueError:\n            return False", "output": "Deregister a previously registered hook.\n        Returns True if the hook existed, False if not.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_json_object(col, path):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.get_json_object(_to_java_column(col), path)\n    return Column(jc)", "output": "Extracts json object from a json string based on json path specified, and returns json string\n    of the extracted json object. It will return null if the input json string is invalid.\n\n    :param col: string column in json format\n    :param path: path to the json object to extract\n\n    >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n    >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n    >>> df.select(df.key, get_json_object(df.jstring, '$.f1').alias(\"c0\"), \\\\\n    ...                   get_json_object(df.jstring, '$.f2').alias(\"c1\") ).collect()\n    [Row(key=u'1', c0=u'value1', c1=u'value2'), Row(key=u'2', c0=u'value12', c1=None)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_or_set_hash(uri,\n        length=8,\n        chars='abcdefghijklmnopqrstuvwxyz0123456789!@#$%^&*(-_=+)'):\n    '''\n    \n    '''\n    return salt.utils.sdb.sdb_get_or_set_hash(uri, __opts__, length, chars, __utils__)", "output": "Perform a one-time generation of a hash and write it to sdb.\n    If that value has already been set return the value instead.\n\n    This is useful for generating passwords or keys that are specific to\n    multiple minions that need to be stored somewhere centrally.\n\n    State Example:\n\n    .. code-block:: yaml\n\n        some_mysql_user:\n          mysql_user:\n            - present\n            - host: localhost\n            - password: '{{ salt['sdb.get_or_set_hash']('some_mysql_user_pass') }}'\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sdb.get_or_set_hash 'SECRET_KEY' 50\n\n    .. warning::\n\n        This function could return strings which may contain characters which are reserved\n        as directives by the YAML parser, such as strings beginning with ``%``. To avoid\n        issues when using the output of this function in an SLS file containing YAML+Jinja,\n        surround the call with single quotes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dump_model(self, fout, fmap='', with_stats=False):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(fout, STRING_TYPES):\n            fout = open(fout, 'w')\n            need_close = True\n        else:\n            need_close = False\n        ret = self.get_dump(fmap, with_stats)\n        for i in range(len(ret)):\n            fout.write('booster[{}]:\\n'.format(i))\n            fout.write(ret[i])\n        if need_close:\n            fout.close()", "output": "Dump model into a text file.\n\n        Parameters\n        ----------\n        foout : string\n            Output file name.\n        fmap : string, optional\n            Name of the file containing feature map names.\n        with_stats : bool (optional)\n            Controls whether the split statistics are output.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _FixedSizer(value_size):\n  \"\"\"\"\"\"\n\n  def SpecificSizer(field_number, is_repeated, is_packed):\n    tag_size = _TagSize(field_number)\n    if is_packed:\n      local_VarintSize = _VarintSize\n      def PackedFieldSize(value):\n        result = len(value) * value_size\n        return result + local_VarintSize(result) + tag_size\n      return PackedFieldSize\n    elif is_repeated:\n      element_size = value_size + tag_size\n      def RepeatedFieldSize(value):\n        return len(value) * element_size\n      return RepeatedFieldSize\n    else:\n      field_size = value_size + tag_size\n      def FieldSize(value):\n        return field_size\n      return FieldSize\n\n  return SpecificSizer", "output": "Like _SimpleSizer except for a fixed-size field.  The input is the size\n  of one value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _consume_next(self):\n        \"\"\"\n        \"\"\"\n        response = six.next(self._response_iterator)\n        self._counter += 1\n\n        if self._metadata is None:  # first response\n            metadata = self._metadata = response.metadata\n\n            source = self._source\n            if source is not None and source._transaction_id is None:\n                source._transaction_id = metadata.transaction.id\n\n        if response.HasField(\"stats\"):  # last response\n            self._stats = response.stats\n\n        values = list(response.values)\n        if self._pending_chunk is not None:\n            values[0] = self._merge_chunk(values[0])\n\n        if response.chunked_value:\n            self._pending_chunk = values.pop()\n\n        self._merge_values(values)", "output": "Consume the next partial result set from the stream.\n\n        Parse the result set into new/existing rows in :attr:`_rows`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(name, path=None):\n    '''\n    \n    '''\n\n    _exists = name in ls_(path=path)\n    # container may be just created but we did cached earlier the\n    # lxc-ls results\n    if not _exists:\n        _exists = name in ls_(cache=False, path=path)\n    return _exists", "output": "Returns whether the named container exists.\n\n    path\n        path to the container parent directory (default: /var/lib/lxc)\n\n        .. versionadded:: 2015.8.0\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' lxc.exists name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_rest(args):\n    ''''''\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config('restServerPort')\n    running, _ = check_rest_server_quick(rest_port)\n    if not running:\n        print_normal('Restful server is running...')\n    else:\n        print_normal('Restful server is not running...')", "output": "check if restful server is running", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_session_created(self, session_context):\n        ''' \n\n        '''\n        for h in self._handlers:\n            result = h.on_session_created(session_context)\n            yield yield_for_all_futures(result)\n        raise gen.Return(None)", "output": "Invoked to execute code when a new session is created.\n\n        This method calls ``on_session_created`` on each handler, in order,\n        with the session context passed as the only argument.\n\n        May return a ``Future`` which will delay session creation until the\n        ``Future`` completes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deep_shap(model, data):\n    \"\"\" \n    \"\"\"\n    if isinstance(model, KerasWrap):\n        model = model.model\n    explainer = DeepExplainer(model, kmeans(data, 1).data)\n    def f(X):\n        phi = explainer.shap_values(X)\n        if type(phi) is list and len(phi) == 1:\n            return phi[0]\n        else:\n            return phi\n    \n    return f", "output": "Deep SHAP (DeepLIFT)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def entity_from_protobuf(pb):\n    \"\"\"\n    \"\"\"\n    key = None\n    if pb.HasField(\"key\"):  # Message field (Key)\n        key = key_from_protobuf(pb.key)\n\n    entity_props = {}\n    entity_meanings = {}\n    exclude_from_indexes = []\n\n    for prop_name, value_pb in _property_tuples(pb):\n        value = _get_value_from_value_pb(value_pb)\n        entity_props[prop_name] = value\n\n        # Check if the property has an associated meaning.\n        is_list = isinstance(value, list)\n        meaning = _get_meaning(value_pb, is_list=is_list)\n        if meaning is not None:\n            entity_meanings[prop_name] = (meaning, value)\n\n        # Check if ``value_pb`` was excluded from index. Lists need to be\n        # special-cased and we require all ``exclude_from_indexes`` values\n        # in a list agree.\n        if is_list and len(value) > 0:\n            exclude_values = set(\n                value_pb.exclude_from_indexes\n                for value_pb in value_pb.array_value.values\n            )\n            if len(exclude_values) != 1:\n                raise ValueError(\n                    \"For an array_value, subvalues must either \"\n                    \"all be indexed or all excluded from \"\n                    \"indexes.\"\n                )\n\n            if exclude_values.pop():\n                exclude_from_indexes.append(prop_name)\n        else:\n            if value_pb.exclude_from_indexes:\n                exclude_from_indexes.append(prop_name)\n\n    entity = Entity(key=key, exclude_from_indexes=exclude_from_indexes)\n    entity.update(entity_props)\n    entity._meanings.update(entity_meanings)\n    return entity", "output": "Factory method for creating an entity based on a protobuf.\n\n    The protobuf should be one returned from the Cloud Datastore\n    Protobuf API.\n\n    :type pb: :class:`.entity_pb2.Entity`\n    :param pb: The Protobuf representing the entity.\n\n    :rtype: :class:`google.cloud.datastore.entity.Entity`\n    :returns: The entity derived from the protobuf.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close_project(self):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        if self.current_active_project:\r\n            self.switch_to_plugin()\r\n            if self.main.editor is not None:\r\n                self.set_project_filenames(\r\n                    self.main.editor.get_open_filenames())\r\n            path = self.current_active_project.root_path\r\n            self.current_active_project = None\r\n            self.set_option('current_project_path', None)\r\n            self.setup_menu_actions()\r\n\r\n            self.sig_project_closed.emit(path)\r\n            self.sig_pythonpath_changed.emit()\r\n\r\n            if self.dockwidget is not None:\r\n                self.set_option('visible_if_project_open',\r\n                                self.dockwidget.isVisible())\r\n                self.dockwidget.close()\r\n\r\n            self.explorer.clear()\r\n            self.restart_consoles()", "output": "Close current project and return to a window without an active\r\n        project", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def returner(ret):\n    '''\n    \n    '''\n\n    _options = _get_options(ret)\n\n    user = _options.get('user')\n    device = _options.get('device')\n    token = _options.get('token')\n    title = _options.get('title')\n    priority = _options.get('priority')\n    expire = _options.get('expire')\n    retry = _options.get('retry')\n    sound = _options.get('sound')\n\n    if not token:\n        raise SaltInvocationError('Pushover token is unavailable.')\n\n    if not user:\n        raise SaltInvocationError('Pushover user key is unavailable.')\n\n    if priority and priority == 2:\n        if not expire and not retry:\n            raise SaltInvocationError('Priority 2 requires pushover.expire and pushover.retry options.')\n\n    message = ('id: {0}\\r\\n'\n               'function: {1}\\r\\n'\n               'function args: {2}\\r\\n'\n               'jid: {3}\\r\\n'\n               'return: {4}\\r\\n').format(\n                    ret.get('id'),\n                    ret.get('fun'),\n                    ret.get('fun_args'),\n                    ret.get('jid'),\n                    pprint.pformat(ret.get('return')))\n\n    result = _post_message(user=user,\n                           device=device,\n                           message=message,\n                           title=title,\n                           priority=priority,\n                           expire=expire,\n                           retry=retry,\n                           sound=sound,\n                           token=token)\n\n    log.debug('pushover result %s', result)\n    if not result['res']:\n        log.info('Error: %s', result['message'])\n    return", "output": "Send an PushOver message with the data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_values(values, skipna, fill_value=None, fill_value_typ=None,\n                isfinite=False, copy=True, mask=None):\n    \"\"\" \n    \"\"\"\n\n    if is_datetime64tz_dtype(values):\n        # com.values_from_object returns M8[ns] dtype instead of tz-aware,\n        #  so this case must be handled separately from the rest\n        dtype = values.dtype\n        values = getattr(values, \"_values\", values)\n    else:\n        values = com.values_from_object(values)\n        dtype = values.dtype\n\n    if mask is None:\n        if isfinite:\n            mask = _isfinite(values)\n        else:\n            mask = isna(values)\n\n    if is_datetime_or_timedelta_dtype(values) or is_datetime64tz_dtype(values):\n        # changing timedelta64/datetime64 to int64 needs to happen after\n        #  finding `mask` above\n        values = getattr(values, \"asi8\", values)\n        values = values.view(np.int64)\n\n    dtype_ok = _na_ok_dtype(dtype)\n\n    # get our fill value (in case we need to provide an alternative\n    # dtype for it)\n    fill_value = _get_fill_value(dtype, fill_value=fill_value,\n                                 fill_value_typ=fill_value_typ)\n\n    if skipna:\n        if copy:\n            values = values.copy()\n        if dtype_ok:\n            np.putmask(values, mask, fill_value)\n\n        # promote if needed\n        else:\n            values, changed = maybe_upcast_putmask(values, mask, fill_value)\n\n    elif copy:\n        values = values.copy()\n\n    # return a platform independent precision dtype\n    dtype_max = dtype\n    if is_integer_dtype(dtype) or is_bool_dtype(dtype):\n        dtype_max = np.int64\n    elif is_float_dtype(dtype):\n        dtype_max = np.float64\n\n    return values, mask, dtype, dtype_max, fill_value", "output": "utility to get the values view, mask, dtype\n    if necessary copy and mask using the specified fill_value\n    copy = True will force the copy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_zone(zone, private=False, vpc_id=None, vpc_region=None, region=None,\n                key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    if region is None:\n        region = 'universal'\n\n    if private:\n        if not vpc_id or not vpc_region:\n            msg = 'vpc_id and vpc_region must be specified for a private zone'\n            raise SaltInvocationError(msg)\n\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    _zone = conn.get_zone(zone)\n\n    if _zone:\n        return False\n\n    conn.create_zone(zone, private_zone=private, vpc_id=vpc_id,\n                     vpc_region=vpc_region)\n    return True", "output": "Create a Route53 hosted zone.\n\n    .. versionadded:: 2015.8.0\n\n    zone\n        DNS zone to create\n\n    private\n        True/False if the zone will be a private zone\n\n    vpc_id\n        VPC ID to associate the zone to (required if private is True)\n\n    vpc_region\n        VPC Region (required if private is True)\n\n    region\n        region endpoint to connect to\n\n    key\n        AWS key\n\n    keyid\n        AWS keyid\n\n    profile\n        AWS pillar profile\n\n    CLI Example::\n\n        salt myminion boto_route53.create_zone example.org", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def name(self):\n        \"\"\"\"\"\"\n        parts = self._parts\n        if len(parts) == (1 if (self._drv or self._root) else 0):\n            return ''\n        return parts[-1]", "output": "The final path component, if any.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def output_file(filename, title=\"Bokeh Plot\", mode=\"cdn\", root_dir=None):\n    '''\n\n    '''\n    curstate().output_file(\n        filename,\n        title=title,\n        mode=mode,\n        root_dir=root_dir\n    )", "output": "Configure the default output state to generate output saved\n    to a file when :func:`show` is called.\n\n    Does not change the current ``Document`` from ``curdoc()``. File and notebook\n    output may be active at the same time, so e.g., this does not clear the\n    effects of ``output_notebook()``.\n\n    Args:\n        filename (str) : a filename for saving the HTML document\n\n        title (str, optional) : a title for the HTML document (default: \"Bokeh Plot\")\n\n        mode (str, optional) : how to include BokehJS (default: ``'cdn'``)\n            One of: ``'inline'``, ``'cdn'``, ``'relative(-dev)'`` or\n            ``'absolute(-dev)'``. See :class:`bokeh.resources.Resources` for more details.\n\n        root_dir (str, optional) : root directory to use for 'absolute' resources. (default: None)\n            This value is ignored for other resource types, e.g. ``INLINE`` or\n            ``CDN``.\n\n    Returns:\n        None\n\n    .. note::\n        Generally, this should be called at the beginning of an interactive\n        session or the top of a script.\n\n    .. warning::\n        This output file will be overwritten on every save, e.g., each time\n        show() or save() is invoked.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def loguniform(low, high, random_state):\n    '''\n    \n    '''\n    assert low > 0, 'Lower bound must be positive'\n    return np.exp(uniform(np.log(low), np.log(high), random_state))", "output": "low: an float that represent an lower bound\n    high: an float that represent an upper bound\n    random_state: an object of numpy.random.RandomState", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def schemaNewMemParserCtxt(buffer, size):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlSchemaNewMemParserCtxt(buffer, size)\n    if ret is None:raise parserError('xmlSchemaNewMemParserCtxt() failed')\n    return SchemaParserCtxt(_obj=ret)", "output": "Create an XML Schemas parse context for that memory buffer\n       expected to contain an XML Schemas file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put(self, item, block=True, timeout=None):\n        \"\"\"\n        \"\"\"\n        if self.maxsize <= 0:\n            self.actor.put.remote(item)\n        elif not block:\n            if not ray.get(self.actor.put.remote(item)):\n                raise Full\n        elif timeout is None:\n            # Polling\n            # Use a not_full condition variable or promise?\n            while not ray.get(self.actor.put.remote(item)):\n                # Consider adding time.sleep here\n                pass\n        elif timeout < 0:\n            raise ValueError(\"'timeout' must be a non-negative number\")\n        else:\n            endtime = time.time() + timeout\n            # Polling\n            # Use a condition variable or switch to promise?\n            success = False\n            while not success and time.time() < endtime:\n                success = ray.get(self.actor.put.remote(item))\n            if not success:\n                raise Full", "output": "Adds an item to the queue.\n\n        Uses polling if block=True, so there is no guarantee of order if\n        multiple producers put to the same full queue.\n\n        Raises:\n            Full if the queue is full and blocking is False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def autoencoder_ordered_text():\n  \"\"\"\"\"\"\n  hparams = autoencoder_ordered_discrete()\n  hparams.bottleneck_bits = 1024\n  hparams.bottleneck_shared_bits = 1024-64\n  hparams.bottleneck_shared_bits_start_warmup = 75000\n  hparams.bottleneck_shared_bits_stop_warmup = 275000\n  hparams.num_hidden_layers = 7\n  hparams.batch_size = 1024\n  hparams.autoregressive_mode = \"conv5\"\n  hparams.max_hidden_size = 1024\n  hparams.bottom = {\n      \"inputs\": modalities.identity_bottom,\n      \"targets\": modalities.identity_bottom,\n  }\n  hparams.top = {\n      \"targets\": modalities.identity_top,\n  }\n  hparams.sample_height = 128\n  hparams.sample_width = 1\n  return hparams", "output": "Ordered discrete autoencoder model for text.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def config_find_lines(regex, source='running'):\n    \n    '''\n    config_txt = __salt__['net.config'](source=source)['out'][source]\n    return __salt__['ciscoconfparse.find_lines'](config=config_txt,\n                                                 regex=regex)", "output": "r'''\n    .. versionadded:: 2019.2.0\n\n    Return the configuration lines that match the regular expressions from the\n    ``regex`` argument. The configuration is read from the network device\n    interrogated.\n\n    regex\n        The regular expression to match the configuration lines against.\n\n    source: ``running``\n        The configuration type to retrieve from the network device. Default:\n        ``running``. Available options: ``running``, ``startup``, ``candidate``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm.config_find_lines '^interface Ethernet1\\d'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ret_list_minions(self):\n        '''\n        \n        '''\n        tgt = _tgt_set(self.tgt)\n        return self._ret_minions(tgt.intersection)", "output": "Return minions that match via list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _process_server_headers(\n        self, key: Union[str, bytes], headers: httputil.HTTPHeaders\n    ) -> None:\n        \"\"\"\n        \"\"\"\n        assert headers[\"Upgrade\"].lower() == \"websocket\"\n        assert headers[\"Connection\"].lower() == \"upgrade\"\n        accept = self.compute_accept_value(key)\n        assert headers[\"Sec-Websocket-Accept\"] == accept\n\n        extensions = self._parse_extensions_header(headers)\n        for ext in extensions:\n            if ext[0] == \"permessage-deflate\" and self._compression_options is not None:\n                self._create_compressors(\"client\", ext[1])\n            else:\n                raise ValueError(\"unsupported extension %r\", ext)\n\n        self.selected_subprotocol = headers.get(\"Sec-WebSocket-Protocol\", None)", "output": "Process the headers sent by the server to this client connection.\n\n        'key' is the websocket handshake challenge/response key.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_repo(name, config_path=_DEFAULT_CONFIG_PATH, with_packages=False):\n    '''\n    \n    '''\n    _validate_config(config_path)\n    with_packages = six.text_type(bool(with_packages)).lower()\n\n    ret = dict()\n    cmd = ['repo', 'show', '-config={}'.format(config_path),\n           '-with-packages={}'.format(with_packages), name]\n\n    cmd_ret = _cmd_run(cmd)\n\n    ret = _parse_show_output(cmd_ret=cmd_ret)\n\n    if ret:\n        log.debug('Found repository: %s', name)\n    else:\n        log.debug('Unable to find repository: %s', name)\n\n    return ret", "output": "Get detailed information about a local package repository.\n\n    :param str name: The name of the local repository.\n    :param str config_path: The path to the configuration file for the aptly instance.\n    :param bool with_packages: Return a list of packages in the repo.\n\n    :return: A dictionary containing information about the repository.\n    :rtype: dict\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' aptly.get_repo name=\"test-repo\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def choose_trial_to_run(self, trial_runner):\n        \"\"\"\n        \"\"\"\n\n        for hyperband in self._hyperbands:\n            # band will have None entries if no resources\n            # are to be allocated to that bracket.\n            scrubbed = [b for b in hyperband if b is not None]\n            for bracket in sorted(\n                    scrubbed, key=lambda b: b.completion_percentage()):\n                for trial in bracket.current_trials():\n                    if (trial.status == Trial.PENDING\n                            and trial_runner.has_resources(trial.resources)):\n                        return trial\n        return None", "output": "Fair scheduling within iteration by completion percentage.\n\n        List of trials not used since all trials are tracked as state\n        of scheduler. If iteration is occupied (ie, no trials to run),\n        then look into next iteration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mac_str_to_bytes(mac_str):\n    '''\n    \n    '''\n    if len(mac_str) == 12:\n        pass\n    elif len(mac_str) == 17:\n        sep = mac_str[2]\n        mac_str = mac_str.replace(sep, '')\n    else:\n        raise ValueError('Invalid MAC address')\n    chars = (int(mac_str[s:s+2], 16) for s in range(0, 12, 2))\n    return bytes(chars) if six.PY3 else b''.join(chr(x) for x in chars)", "output": "Convert a MAC address string into bytes. Works with or without separators:\n\n    b1 = mac_str_to_bytes('08:00:27:13:69:77')\n    b2 = mac_str_to_bytes('080027136977')\n    assert b1 == b2\n    assert isinstance(b1, bytes)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_with_ruby(ruby, cmdline, runas=None):\n    '''\n    \n    '''\n    if not cmdline:\n        # This is a positional argument so this should never happen, but this\n        # will handle cases where someone explicitly passes a false value for\n        # cmdline.\n        raise SaltInvocationError('Command must be specified')\n\n    try:\n        cmdline = salt.utils.args.shlex_split(cmdline)\n    except AttributeError:\n        cmdline = salt.utils.args.shlex_split(six.text_type(cmdline))\n\n    env = {}\n    if ruby:\n        env['RBENV_VERSION'] = ruby\n        cmd = cmdline\n    else:\n        cmd = cmdline\n\n    return do(cmd, runas=runas, env=env)", "output": "Execute a ruby command with rbenv's shims using a specific ruby version\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rbenv.do_with_ruby 2.0.0-p0 'gem list bundler'\n        salt '*' rbenv.do_with_ruby 2.0.0-p0 'gem list bundler' runas=deploy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def managed(name, **kwargs):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    # save state\n    saved = __salt__['pdbedit.list'](hashes=True)\n    saved = saved[name] if name in saved else {}\n\n    # call pdbedit.modify\n    kwargs['login'] = name\n    res = __salt__['pdbedit.modify'](**kwargs)\n\n    # calculate changes\n    if res[name] in ['created']:\n        ret['changes'] = res\n    elif res[name] in ['updated']:\n        ret['changes'][name] = salt.utils.data.compare_dicts(\n            saved,\n            __salt__['pdbedit.list'](hashes=True)[name],\n        )\n    elif res[name] not in ['unchanged']:\n        ret['result'] = False\n        ret['comment'] = res[name]\n\n    return ret", "output": "Manage user account\n\n    login : string\n        login name\n    password : string\n        password\n    password_hashed : boolean\n        set if password is a nt hash instead of plain text\n    domain : string\n        users domain\n    profile : string\n        profile path\n    script : string\n        logon script\n    drive : string\n        home drive\n    homedir : string\n        home directory\n    fullname : string\n        full name\n    account_desc : string\n        account description\n    machine_sid : string\n        specify the machines new primary group SID or rid\n    user_sid : string\n        specify the users new primary group SID or rid\n    account_control : string\n        specify user account control properties\n\n        .. note::\n            Only the following can be set:\n            - N: No password required\n            - D: Account disabled\n            - H: Home directory required\n            - L: Automatic Locking\n            - X: Password does not expire\n    reset_login_hours : boolean\n        reset the users allowed logon hours\n    reset_bad_password_count : boolean\n        reset the stored bad login counter", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vserver_delete(v_name, **connection_args):\n    '''\n    \n    '''\n    ret = True\n    vserver = _vserver_get(v_name, **connection_args)\n    if vserver is None:\n        return False\n    nitro = _connect(**connection_args)\n    if nitro is None:\n        return False\n    try:\n        NSLBVServer.delete(nitro, vserver)\n    except NSNitroError as error:\n        log.debug('netscaler module error - NSVServer.delete() failed: %s', error)\n        ret = False\n    _disconnect(nitro)\n    return ret", "output": "Delete a lb vserver\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' netscaler.vserver_delete 'vserverName'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_user(name, api_key=None):\n    '''\n    \n    '''\n    if not api_key:\n        api_key = _get_api_key()\n\n    ret = list_users(api_key)\n    if ret['res']:\n        users = ret['message']\n        if users:\n            for user in range(0, len(users)):\n                if users[user]['name'] == name:\n                    return users[user]\n    return False", "output": "Find a user by name and return it.\n\n    :param name:        The user name.\n    :param api_key:     The Slack admin api key.\n    :return:            The user object.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' slack.find_user name=\"ThomasHatch\"\n\n        salt '*' slack.find_user name=\"ThomasHatch\" api_key=peWcBiMOS9HrZG15peWcBiMOS9HrZG15", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_chinese_text():\n    \"\"\"\"\"\"\n    if not os.path.isdir(\"data/\"):\n        os.system(\"mkdir data/\")\n    if (not os.path.exists('data/pos.txt')) or \\\n       (not os.path.exists('data/neg')):\n        os.system(\"wget -q https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/example/chinese_text.zip \"\n                  \"-P data/\")\n        os.chdir(\"./data\")\n        os.system(\"unzip -u chinese_text.zip\")\n        os.chdir(\"..\")", "output": "Download the chinese_text dataset and unzip it", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show(self, y:Image=None, ax:plt.Axes=None, figsize:tuple=(3,3), title:Optional[str]=None, hide_axis:bool=True,\n        color:str='white', **kwargs):\n        \"\"\n        if ax is None: _,ax = plt.subplots(figsize=figsize)\n        bboxes, lbls = self._compute_boxes()\n        h,w = self.flow.size\n        bboxes.add_(1).mul_(torch.tensor([h/2, w/2, h/2, w/2])).long()\n        for i, bbox in enumerate(bboxes):\n            if lbls is not None: text = str(lbls[i])\n            else: text=None\n            _draw_rect(ax, bb2hw(bbox), text=text, color=color)", "output": "Show the `ImageBBox` on `ax`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_layer(self, layer, input_node_id):\n        \"\"\"\n        \"\"\"\n        if isinstance(input_node_id, Iterable):\n            layer.input = list(map(lambda x: self.node_list[x], input_node_id))\n            output_node_id = self._add_node(Node(layer.output_shape))\n            for node_id in input_node_id:\n                self._add_edge(layer, node_id, output_node_id)\n\n        else:\n            layer.input = self.node_list[input_node_id]\n            output_node_id = self._add_node(Node(layer.output_shape))\n            self._add_edge(layer, input_node_id, output_node_id)\n\n        layer.output = self.node_list[output_node_id]\n        return output_node_id", "output": "Add a layer to the Graph.\n        Args:\n            layer: An instance of the subclasses of StubLayer in layers.py.\n            input_node_id: An integer. The ID of the input node of the layer.\n        Returns:\n            output_node_id: An integer. The ID of the output node of the layer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_labels(self, include_freq=False, on_unicode_error='strict'):\n        \"\"\"\n        \n        \"\"\"\n        a = self.f.getArgs()\n        if a.model == model_name.supervised:\n            pair = self.f.getLabels(on_unicode_error)\n            if include_freq:\n                return (pair[0], np.array(pair[1]))\n            else:\n                return pair[0]\n        else:\n            return self.get_words(include_freq)", "output": "Get the entire list of labels of the dictionary optionally\n        including the frequency of the individual labels. Unsupervised\n        models use words as labels, which is why get_labels\n        will call and return get_words for this type of\n        model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _concat_index_asobject(to_concat, name=None):\n    \"\"\"\n    \n    \"\"\"\n    from pandas import Index\n    from pandas.core.arrays import ExtensionArray\n\n    klasses = (ABCDatetimeIndex, ABCTimedeltaIndex, ABCPeriodIndex,\n               ExtensionArray)\n    to_concat = [x.astype(object) if isinstance(x, klasses) else x\n                 for x in to_concat]\n\n    self = to_concat[0]\n    attribs = self._get_attributes_dict()\n    attribs['name'] = name\n\n    to_concat = [x._values if isinstance(x, Index) else x\n                 for x in to_concat]\n\n    return self._shallow_copy_with_infer(np.concatenate(to_concat), **attribs)", "output": "concat all inputs as object. DatetimeIndex, TimedeltaIndex and\n    PeriodIndex are converted to object dtype before concatenation", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def use(broker, debug=True, **kwargs):\n    \"\"\"\n    \"\"\"\n    if not debug:\n        log.setLevel(logging.INFO)\n    if broker.lower() in [\"xq\", \"\u96ea\u7403\"]:\n        return XueQiuTrader(**kwargs)\n    if broker.lower() in [\"yh_client\", \"\u94f6\u6cb3\u5ba2\u6237\u7aef\"]:\n        from .yh_clienttrader import YHClientTrader\n\n        return YHClientTrader()\n    if broker.lower() in [\"ht_client\", \"\u534e\u6cf0\u5ba2\u6237\u7aef\"]:\n        from .ht_clienttrader import HTClientTrader\n\n        return HTClientTrader()\n    if broker.lower() in [\"gj_client\", \"\u56fd\u91d1\u5ba2\u6237\u7aef\"]:\n        from .gj_clienttrader import GJClientTrader\n\n        return GJClientTrader()\n    if broker.lower() in [\"ths\", \"\u540c\u82b1\u987a\u5ba2\u6237\u7aef\"]:\n        from .clienttrader import ClientTrader\n\n        return ClientTrader()\n\n    raise NotImplementedError", "output": "\u7528\u4e8e\u751f\u6210\u7279\u5b9a\u7684\u5238\u5546\u5bf9\u8c61\n    :param broker:\u5238\u5546\u540d\u652f\u6301 ['yh_client', '\u94f6\u6cb3\u5ba2\u6237\u7aef'] ['ht_client', '\u534e\u6cf0\u5ba2\u6237\u7aef']\n    :param debug: \u63a7\u5236 debug \u65e5\u5fd7\u7684\u663e\u793a, \u9ed8\u8ba4\u4e3a True\n    :param initial_assets: [\u96ea\u7403\u53c2\u6570] \u63a7\u5236\u96ea\u7403\u521d\u59cb\u8d44\u91d1\uff0c\u9ed8\u8ba4\u4e3a\u4e00\u767e\u4e07\n    :return the class of trader\n\n    Usage::\n\n        >>> import easytrader\n        >>> user = easytrader.use('xq')\n        >>> user.prepare('xq.json')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dataset_splits(self):\n    \"\"\"\"\"\"\n    return [{\n        \"split\": problem.DatasetSplit.TRAIN,\n        \"shards\": self.num_train_shards,\n    }, {\n        \"split\": problem.DatasetSplit.EVAL,\n        \"shards\": self.num_eval_shards,\n    }, {\n        \"split\": problem.DatasetSplit.TEST,\n        \"shards\": self.num_test_shards,\n    }]", "output": "Splits of data to produce and number the output shards for each.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_device(device=None):\n    '''\n    \n    '''\n\n    data = [{'uid': '/zport/dmd/Devices', 'params': {}, 'limit': None}]\n    all_devices = _router_request('DeviceRouter', 'getDevices', data=data)\n    for dev in all_devices['devices']:\n        if dev['name'] == device:\n            # We need to save the has for later operations\n            dev['hash'] = all_devices['hash']\n            log.info('Found device %s in Zenoss', device)\n            return dev\n\n    log.info('Unable to find device %s in Zenoss', device)\n    return None", "output": "Find a device in Zenoss. If device not found, returns None.\n\n    Parameters:\n        device:         (Optional) Will use the grain 'fqdn' by default\n\n    CLI Example:\n        salt '*' zenoss.find_device", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generator(ngf, nc, no_bias=True, fix_gamma=True, eps=1e-5 + 1e-12, z_dim=100, activation='sigmoid'):\n    '''\n    '''\n    BatchNorm = mx.sym.BatchNorm\n    rand = mx.sym.Variable('rand')\n\n    rand = mx.sym.Reshape(rand, shape=(-1, z_dim, 1, 1))\n\n    g1 = mx.sym.Deconvolution(rand, name='gen1', kernel=(5,5), stride=(2,2),target_shape=(2,2), num_filter=ngf*8, no_bias=no_bias)\n    gbn1 = BatchNorm(g1, name='genbn1', fix_gamma=fix_gamma, eps=eps)\n    gact1 = mx.sym.Activation(gbn1, name=\"genact1\", act_type=\"relu\")\n\n    g2 = mx.sym.Deconvolution(gact1, name='gen2', kernel=(5,5), stride=(2,2),target_shape=(4,4), num_filter=ngf*4, no_bias=no_bias)\n    gbn2 = BatchNorm(g2, name='genbn2', fix_gamma=fix_gamma, eps=eps)\n    gact2 = mx.sym.Activation(gbn2, name='genact2', act_type='relu')\n\n    g3 = mx.sym.Deconvolution(gact2, name='gen3', kernel=(5,5), stride=(2,2), target_shape=(8,8), num_filter=ngf*2, no_bias=no_bias)\n    gbn3 = BatchNorm(g3, name='genbn3', fix_gamma=fix_gamma, eps=eps)\n    gact3 = mx.sym.Activation(gbn3, name='genact3', act_type='relu')\n\n    g4 = mx.sym.Deconvolution(gact3, name='gen4', kernel=(5,5), stride=(2,2), target_shape=(16,16), num_filter=ngf, no_bias=no_bias)\n    gbn4 = BatchNorm(g4, name='genbn4', fix_gamma=fix_gamma, eps=eps)\n    gact4 = mx.sym.Activation(gbn4, name='genact4', act_type='relu')\n\n    g5 = mx.sym.Deconvolution(gact4, name='gen5', kernel=(5,5), stride=(2,2), target_shape=(32,32), num_filter=nc, no_bias=no_bias)\n    gout = mx.sym.Activation(g5, name='genact5', act_type=activation)\n\n    return gout", "output": "The genrator is a CNN which takes 100 dimensional embedding as input\n    and reconstructs the input image given to the encoder", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_extras(cls: Type[T],\n                  extras: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    \n    \"\"\"\n    subextras: Dict[str, Any] = {}\n    if hasattr(cls, \"from_params\"):\n        from_params_method = cls.from_params  # type: ignore\n    else:\n        # In some rare cases, we get a registered subclass that does _not_ have a\n        # from_params method (this happens with Activations, for instance, where we\n        # register pytorch modules directly).  This is a bit of a hack to make those work,\n        # instead of adding a `from_params` method for them somehow. Then the extras\n        # in the class constructor are what we are looking for, to pass on.\n        from_params_method = cls\n    if takes_kwargs(from_params_method):\n        # If annotation.params accepts **kwargs, we need to pass them all along.\n        # For example, `BasicTextFieldEmbedder.from_params` requires a Vocabulary\n        # object, but `TextFieldEmbedder.from_params` does not.\n        subextras = extras\n    else:\n        # Otherwise, only supply the ones that are actual args; any additional ones\n        # will cause a TypeError.\n        subextras = {k: v for k, v in extras.items()\n                     if takes_arg(from_params_method, k)}\n    return subextras", "output": "Given a dictionary of extra arguments, returns a dictionary of\n    kwargs that actually are a part of the signature of the cls.from_params\n    (or cls) method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def layer(output_shape=None, new_parameters=None):\n  \"\"\"\"\"\"\n  def layer_decorator(call):\n    \"\"\"Decorating the call function.\"\"\"\n    def output_shape_fun(self, input_shape):\n      if output_shape is None:\n        return input_shape\n      kwargs = self._init_kwargs  # pylint: disable=protected-access\n      return output_shape(input_shape, **kwargs)\n\n    def new_parameters_fun(self, input_shape, rng):\n      if new_parameters is None:\n        return ()\n      kwargs = self._init_kwargs  # pylint: disable=protected-access\n      return new_parameters(input_shape, rng, **kwargs)\n\n    def call_fun(self, x, params=(), **kwargs):\n      \"\"\"The call function of the created class, derived from call.\"\"\"\n      # Merge on-call kwargs with class-kwargs.\n      call_kwargs = kwargs.copy()\n      call_kwargs.update(self._init_kwargs)  # pylint: disable=protected-access\n      # Call with the merged kwargs.\n      return call(x, params=params, **call_kwargs)\n\n    # Set doc for python help.\n    call_fun.__doc__ = call.__doc__\n    if output_shape is None:\n      output_shape_fun.__doc__ = output_shape.__doc__\n    if new_parameters is None:\n      new_parameters_fun.__doc__ = new_parameters.__doc__\n\n    # Create the class.\n    cls = type(call.__name__, (Layer,),\n               {'call': call_fun,\n                'output_shape': output_shape_fun,\n                'new_parameters': new_parameters_fun})\n\n    return cls\n  return layer_decorator", "output": "Create a layer class from a function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def relaxNGNewValidCtxt(self):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlRelaxNGNewValidCtxt(self._o)\n        if ret is None:raise treeError('xmlRelaxNGNewValidCtxt() failed')\n        __tmp = relaxNgValidCtxt(_obj=ret)\n        __tmp.schema = self\n        return __tmp", "output": "Create an XML RelaxNGs validation context based on the\n           given schema", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _gluster_output_cleanup(result):\n    '''\n    \n    '''\n    ret = ''\n    for line in result.splitlines():\n        if line.startswith('gluster>'):\n            ret += line[9:].strip()\n        elif line.startswith('Welcome to gluster prompt'):\n            pass\n        else:\n            ret += line.strip()\n\n    return ret", "output": "Gluster versions prior to 6 have a bug that requires tricking\n    isatty. This adds \"gluster> \" to the output. Strip it off and\n    produce clean xml for ElementTree.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_toy_sym(teacher=True, teacher_noise_precision=None):\n    \"\"\"\"\"\"\n    if teacher:\n        net = mx.symbol.Variable('data')\n        net = mx.symbol.FullyConnected(data=net, name='teacher_fc1', num_hidden=100)\n        net = mx.symbol.Activation(data=net, name='teacher_relu1', act_type=\"relu\")\n        net = mx.symbol.FullyConnected(data=net, name='teacher_fc2', num_hidden=1)\n        net = mx.symbol.LinearRegressionOutput(data=net, name='teacher_output',\n                                               grad_scale=teacher_noise_precision)\n    else:\n        net = mx.symbol.Variable('data')\n        net = mx.symbol.FullyConnected(data=net, name='student_fc1', num_hidden=100)\n        net = mx.symbol.Activation(data=net, name='student_relu1', act_type=\"relu\")\n        student_mean = mx.symbol.FullyConnected(data=net, name='student_mean', num_hidden=1)\n        student_var = mx.symbol.FullyConnected(data=net, name='student_var', num_hidden=1)\n        net = mx.symbol.Group([student_mean, student_var])\n    return net", "output": "Get toy symbol", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def optional(validator):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(validator, list):\n        return _OptionalValidator(_AndValidator(validator))\n    return _OptionalValidator(validator)", "output": "A validator that makes an attribute optional.  An optional attribute is one\n    which can be set to ``None`` in addition to satisfying the requirements of\n    the sub-validator.\n\n    :param validator: A validator (or a list of validators) that is used for\n        non-``None`` values.\n    :type validator: callable or :class:`list` of callables.\n\n    .. versionadded:: 15.1.0\n    .. versionchanged:: 17.1.0 *validator* can be a list of validators.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_in_transaction(self, func, *args, **kw):\n        \"\"\"\n        \"\"\"\n        # Sanity check: Is there a transaction already running?\n        # If there is, then raise a red flag. Otherwise, mark that this one\n        # is running.\n        if getattr(self._local, \"transaction_running\", False):\n            raise RuntimeError(\"Spanner does not support nested transactions.\")\n        self._local.transaction_running = True\n\n        # Check out a session and run the function in a transaction; once\n        # done, flip the sanity check bit back.\n        try:\n            with SessionCheckout(self._pool) as session:\n                return session.run_in_transaction(func, *args, **kw)\n        finally:\n            self._local.transaction_running = False", "output": "Perform a unit of work in a transaction, retrying on abort.\n\n        :type func: callable\n        :param func: takes a required positional argument, the transaction,\n                     and additional positional / keyword arguments as supplied\n                     by the caller.\n\n        :type args: tuple\n        :param args: additional positional arguments to be passed to ``func``.\n\n        :type kw: dict\n        :param kw: optional keyword arguments to be passed to ``func``.\n                   If passed, \"timeout_secs\" will be removed and used to\n                   override the default timeout.\n\n        :rtype: :class:`datetime.datetime`\n        :returns: timestamp of committed transaction", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_deps_to_pip(deps, project=None, r=True, include_index=True):\n    \"\"\"\"\"\"\"\n    from .vendor.requirementslib.models.requirements import Requirement\n\n    dependencies = []\n    for dep_name, dep in deps.items():\n        if project:\n            project.clear_pipfile_cache()\n        indexes = getattr(project, \"pipfile_sources\", []) if project is not None else []\n        new_dep = Requirement.from_pipfile(dep_name, dep)\n        if new_dep.index:\n            include_index = True\n        req = new_dep.as_line(sources=indexes if include_index else None).strip()\n        dependencies.append(req)\n    if not r:\n        return dependencies\n\n    # Write requirements.txt to tmp directory.\n    from .vendor.vistir.path import create_tracked_tempfile\n    f = create_tracked_tempfile(suffix=\"-requirements.txt\", delete=False)\n    f.write(\"\\n\".join(dependencies).encode(\"utf-8\"))\n    f.close()\n    return f.name", "output": "Converts a Pipfile-formatted dependency to a pip-formatted one.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_name_re(cls, path:PathOrStr, fnames:FilePathList, pat:str, valid_pct:float=0.2, **kwargs):\n        \"\"\n        pat = re.compile(pat)\n        def _get_label(fn):\n            if isinstance(fn, Path): fn = fn.as_posix()\n            res = pat.search(str(fn))\n            assert res,f'Failed to find \"{pat}\" in \"{fn}\"'\n            return res.group(1)\n        return cls.from_name_func(path, fnames, _get_label, valid_pct=valid_pct, **kwargs)", "output": "Create from list of `fnames` in `path` with re expression `pat`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_daily_message(self, dt, algo, metrics_tracker):\n        \"\"\"\n        \n        \"\"\"\n        perf_message = metrics_tracker.handle_market_close(\n            dt,\n            self.data_portal,\n        )\n        perf_message['daily_perf']['recorded_vars'] = algo.recorded_vars\n        return perf_message", "output": "Get a perf message for the given datetime.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_enabled(limit=''):\n    '''\n    \n    '''\n    limit = limit.lower()\n    if limit == 'upstart':\n        return sorted(name for name in _upstart_services()\n            if _upstart_is_enabled(name))\n    elif limit == 'sysvinit':\n        runlevel = _runlevel()\n        return sorted(name for name in _sysv_services()\n            if _sysv_is_enabled(name, runlevel))\n    else:\n        runlevel = _runlevel()\n        return sorted(\n            [name for name in _upstart_services()\n                if _upstart_is_enabled(name)]\n            + [name for name in _sysv_services()\n            if _sysv_is_enabled(name, runlevel)])", "output": "Return the enabled services. Use the ``limit`` param to restrict results\n    to services of that type.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' service.get_enabled\n        salt '*' service.get_enabled limit=upstart\n        salt '*' service.get_enabled limit=sysvinit", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_python_doc_path():\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    if os.name == 'nt':\r\n        doc_path = osp.join(sys.prefix, \"Doc\")\r\n        if not osp.isdir(doc_path):\r\n            return\r\n        python_chm = [path for path in os.listdir(doc_path)\r\n                      if re.match(r\"(?i)Python[0-9]{3,6}.chm\", path)]\r\n        if python_chm:\r\n            return file_uri(osp.join(doc_path, python_chm[0]))\r\n    else:\r\n        vinf = sys.version_info\r\n        doc_path = '/usr/share/doc/python%d.%d/html' % (vinf[0], vinf[1])\r\n    python_doc = osp.join(doc_path, \"index.html\")\r\n    if osp.isfile(python_doc):\r\n        return file_uri(python_doc)", "output": "Return Python documentation path\r\n    (Windows: return the PythonXX.chm path if available)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _collect_metrics(repo, path, recursive, typ, xpath, branch):\n    \"\"\"\n    \"\"\"\n    outs = [out for stage in repo.stages() for out in stage.outs]\n\n    if path:\n        try:\n            outs = repo.find_outs_by_path(path, outs=outs, recursive=recursive)\n        except OutputNotFoundError:\n            logger.debug(\n                \"stage file not for found for '{}' in branch '{}'\".format(\n                    path, branch\n                )\n            )\n            return []\n\n    res = []\n    for o in outs:\n        if not o.metric:\n            continue\n\n        if not typ and isinstance(o.metric, dict):\n            t = o.metric.get(o.PARAM_METRIC_TYPE, typ)\n            x = o.metric.get(o.PARAM_METRIC_XPATH, xpath)\n        else:\n            t = typ\n            x = xpath\n\n        res.append((o, t, x))\n\n    return res", "output": "Gather all the metric outputs.\n\n    Args:\n        path (str): Path to a metric file or a directory.\n        recursive (bool): If path is a directory, do a recursive search for\n            metrics on the given path.\n        typ (str): The type of metric to search for, could be one of the\n            following (raw|json|tsv|htsv|csv|hcsv).\n        xpath (str): Path to search for.\n        branch (str): Branch to look up for metrics.\n\n    Returns:\n        list(tuple): (output, typ, xpath)\n            - output:\n            - typ:\n            - xpath:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def etree(self):\n        \"\"\"\"\"\"\n        if not hasattr(self, '_elements'):\n            try:\n                parser = lxml.html.HTMLParser(encoding=self.encoding)\n                self._elements = lxml.html.fromstring(self.content, parser=parser)\n            except LookupError:\n                # lxml would raise LookupError when encoding not supported\n                # try fromstring without encoding instead.\n                # on windows, unicode is not availabe as encoding for lxml\n                self._elements = lxml.html.fromstring(self.content)\n        if isinstance(self._elements, lxml.etree._ElementTree):\n            self._elements = self._elements.getroot()\n        return self._elements", "output": "Returns a lxml object of the response's content that can be selected by xpath", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def from_client(cls, client, *, resume=False):\n        \"\"\"\"\"\"\n        gateway = 'wss://' + client.endpoint + '/?v=4'\n        ws = await websockets.connect(gateway, loop=client.loop, klass=cls, compression=None)\n        ws.gateway = gateway\n        ws._connection = client\n        ws._max_heartbeat_timeout = 60.0\n\n        if resume:\n            await ws.resume()\n        else:\n            await ws.identify()\n\n        return ws", "output": "Creates a voice websocket for the :class:`VoiceClient`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def mutual_friends(self):\n        \"\"\"\n        \"\"\"\n        state = self._state\n        mutuals = await state.http.get_mutual_friends(self.id)\n        return [User(state=state, data=friend) for friend in mutuals]", "output": "|coro|\n\n        Gets all mutual friends of this user.\n\n        .. note::\n\n            This only applies to non-bot accounts.\n\n        Raises\n        -------\n        Forbidden\n            Not allowed to get mutual friends of this user.\n        HTTPException\n            Getting mutual friends failed.\n\n        Returns\n        -------\n        List[:class:`User`]\n            The users that are mutual friends.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _status_query(query, hostname, enumerate=None, service=None):\n    '''\n    \n    '''\n    config = _config()\n\n    data = None\n    params = {\n        'hostname': hostname,\n        'query': query,\n    }\n\n    ret = {\n        'result': False\n    }\n\n    if enumerate:\n        params['formatoptions'] = 'enumerate'\n    if service:\n        params['servicedescription'] = service\n\n    if config['username'] and config['password'] is not None:\n        auth = (config['username'], config['password'],)\n    else:\n        auth = None\n\n    try:\n        result = salt.utils.http.query(\n            config['url'],\n            method='GET',\n            params=params,\n            decode=True,\n            data=data,\n            text=True,\n            status=True,\n            header_dict={},\n            auth=auth,\n            backend='requests',\n            opts=__opts__,\n        )\n    except ValueError:\n        ret['error'] = 'Please ensure Nagios is running.'\n        ret['result'] = False\n        return ret\n\n    if result.get('status', None) == salt.ext.six.moves.http_client.OK:\n        try:\n            ret['json_data'] = result['dict']\n            ret['result'] = True\n        except ValueError:\n            ret['error'] = 'Please ensure Nagios is running.'\n    elif result.get('status', None) == salt.ext.six.moves.http_client.UNAUTHORIZED:\n        ret['error'] = 'Authentication failed. Please check the configuration.'\n    elif result.get('status', None) == salt.ext.six.moves.http_client.NOT_FOUND:\n        ret['error'] = 'URL {0} was not found.'.format(config['url'])\n    else:\n        ret['error'] = 'Results: {0}'.format(result.text)\n\n    return ret", "output": "Send query along to Nagios.", "category": "Python"}]