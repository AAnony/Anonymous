[{"instruction": "Summarizing the code.", "input": "def bourgain_embedding_matrix(distance_matrix):\n    \"\"\"\n    \"\"\"\n    distance_matrix = np.array(distance_matrix)\n    n = len(distance_matrix)\n    if n == 1:\n        return distance_matrix\n    np.random.seed(123)\n    distort_elements = []\n    r = range(n)\n    k = int(math.ceil(math.log(n) / math.log(2) - 1))\n    t = int(math.ceil(math.log(n)))\n    counter = 0\n    for i in range(0, k + 1):\n        for t in range(t):\n            s = np.random.choice(r, 2 ** i)\n            for j in r:\n                d = min([distance_matrix[j][s] for s in s])\n                counter += len(s)\n                if i == 0 and t == 0:\n                    distort_elements.append([d])\n                else:\n                    distort_elements[j].append(d)\n    return rbf_kernel(distort_elements, distort_elements)", "output": "Use Bourgain algorithm to embed the neural architectures based on their edit-distance.\n    Args:\n        distance_matrix: A matrix of edit-distances.\n    Returns:\n        A matrix of distances after embedding.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def import_object(name: str) -> Any:\n    \"\"\"\n    \"\"\"\n    if name.count(\".\") == 0:\n        return __import__(name)\n\n    parts = name.split(\".\")\n    obj = __import__(\".\".join(parts[:-1]), fromlist=[parts[-1]])\n    try:\n        return getattr(obj, parts[-1])\n    except AttributeError:\n        raise ImportError(\"No module named %s\" % parts[-1])", "output": "Imports an object by name.\n\n    ``import_object('x')`` is equivalent to ``import x``.\n    ``import_object('x.y.z')`` is equivalent to ``from x.y import z``.\n\n    >>> import tornado.escape\n    >>> import_object('tornado.escape') is tornado.escape\n    True\n    >>> import_object('tornado.escape.utf8') is tornado.escape.utf8\n    True\n    >>> import_object('tornado') is tornado\n    True\n    >>> import_object('tornado.missing_module')\n    Traceback (most recent call last):\n        ...\n    ImportError: No module named missing_module", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def empty(stype, shape, ctx=None, dtype=None):\n    \"\"\"\n    \"\"\"\n    if isinstance(shape, int):\n        shape = (shape, )\n    if ctx is None:\n        ctx = current_context()\n    if dtype is None:\n        dtype = mx_real_t\n    assert(stype is not None)\n    if stype in ('csr', 'row_sparse'):\n        return zeros(stype, shape, ctx=ctx, dtype=dtype)\n    else:\n        raise Exception(\"unknown stype : \" + str(stype))", "output": "Returns a new array of given shape and type, without initializing entries.\n\n    Parameters\n    ----------\n    stype: string\n        The storage type of the empty array, such as 'row_sparse', 'csr', etc\n    shape : int or tuple of int\n        The shape of the empty array.\n    ctx : Context, optional\n        An optional device context (default is the current default context).\n    dtype : str or numpy.dtype, optional\n        An optional value type (default is `float32`).\n\n    Returns\n    -------\n    CSRNDArray or RowSparseNDArray\n        A created array.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _score_for_model(meta):\n    \"\"\"  \"\"\"\n    mean_acc = list()\n    pipes = meta[\"pipeline\"]\n    acc = meta[\"accuracy\"]\n    if \"tagger\" in pipes:\n        mean_acc.append(acc[\"tags_acc\"])\n    if \"parser\" in pipes:\n        mean_acc.append((acc[\"uas\"] + acc[\"las\"]) / 2)\n    if \"ner\" in pipes:\n        mean_acc.append((acc[\"ents_p\"] + acc[\"ents_r\"] + acc[\"ents_f\"]) / 3)\n    return sum(mean_acc) / len(mean_acc)", "output": "Returns mean score between tasks in pipeline that can be used for early stopping.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reduce_sum(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'axes':'axis'})\n    return 'sum', new_attrs, inputs", "output": "Reduce the array along a given axis by sum value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_lanczos_eig(self, compute_m=True, feed_dict=None):\n    \"\"\"\n    \"\"\"\n    if compute_m:\n      min_eig, min_vec = self.sess.run([self.m_min_eig, self.m_min_vec], feed_dict=feed_dict)\n\n    else:\n      min_eig, min_vec = self.sess.run([self.h_min_eig, self.h_min_vec], feed_dict=feed_dict)\n\n    return min_vec, min_eig", "output": "Computes the min eigen value and corresponding vector of matrix M or H\n    using the Lanczos algorithm.\n    Args:\n      compute_m: boolean to determine whether we should compute eig val/vec\n        for M or for H. True for M; False for H.\n      feed_dict: dictionary mapping from TF placeholders to values (optional)\n    Returns:\n      min_eig_vec: Corresponding eigen vector to min eig val\n      eig_val: Minimum eigen value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sdc_mdata(mdata_list=None, mdata_get=None):\n    '''\n    \n    '''\n    grains = {}\n    sdc_text_keys = [\n        'uuid',\n        'server_uuid',\n        'datacenter_name',\n        'hostname',\n        'dns_domain',\n    ]\n    sdc_json_keys = [\n        'resolvers',\n        'nics',\n        'routes',\n    ]\n\n    if not mdata_list:\n        mdata_list = salt.utils.path.which('mdata-list')\n\n    if not mdata_get:\n        mdata_get = salt.utils.path.which('mdata-get')\n\n    if not mdata_list or not mdata_get:\n        return grains\n\n    for mdata_grain in sdc_text_keys+sdc_json_keys:\n        mdata_value = __salt__['cmd.run']('{0} sdc:{1}'.format(mdata_get, mdata_grain), ignore_retcode=True)\n\n        if not mdata_value.startswith('No metadata for '):\n            if 'mdata' not in grains:\n                grains['mdata'] = {}\n            if 'sdc' not in grains['mdata']:\n                grains['mdata']['sdc'] = {}\n\n            log.debug('found mdata entry sdc:%s with value %s', mdata_grain, mdata_value)\n            mdata_grain = mdata_grain.replace('-', '_')\n            mdata_grain = mdata_grain.replace(':', '_')\n            if mdata_grain in sdc_json_keys:\n                grains['mdata']['sdc'][mdata_grain] = salt.utils.json.loads(mdata_value)\n            else:\n                grains['mdata']['sdc'][mdata_grain] = mdata_value\n\n    return grains", "output": "SDC Metadata specified by there specs\n    https://eng.joyent.com/mdata/datadict.html", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe_token_expr(expr):\n    \"\"\"\"\"\"\n    if ':' in expr:\n        type, value = expr.split(':', 1)\n        if type == 'name':\n            return value\n    else:\n        type = expr\n    return _describe_token_type(type)", "output": "Like `describe_token` but for token expressions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nce_loss_subwords(\n        data, label, label_mask, label_weight, embed_weight, vocab_size, num_hidden):\n    \"\"\"\n    \"\"\"\n    # get subword-units embedding.\n    label_units_embed = mx.sym.Embedding(data=label,\n                                         input_dim=vocab_size,\n                                         weight=embed_weight,\n                                         output_dim=num_hidden)\n    # get valid subword-units embedding with the help of label_mask\n    # it's achieved by multiplying zeros to useless units in order to handle variable-length input.\n    label_units_embed = mx.sym.broadcast_mul(lhs=label_units_embed,\n                                             rhs=label_mask,\n                                             name='label_units_embed')\n    # sum over them to get label word embedding.\n    label_embed = mx.sym.sum(label_units_embed, axis=2, name='label_embed')\n\n    # by boardcast_mul and sum you can get prediction scores in all label_embed inputs,\n    # which is easy to feed into LogisticRegressionOutput and make your code more concise.\n    data = mx.sym.Reshape(data=data, shape=(-1, 1, num_hidden))\n    pred = mx.sym.broadcast_mul(data, label_embed)\n    pred = mx.sym.sum(data=pred, axis=2)\n\n    return mx.sym.LogisticRegressionOutput(data=pred,\n                                           label=label_weight)", "output": "NCE-Loss layer under subword-units input.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def accept(self, pub):\n        '''\n        \n        '''\n        try:\n            with salt.utils.files.fopen(self.path, 'r') as fp_:\n                expiry = int(fp_.read())\n        except (OSError, IOError):\n            log.error(\n                'Request to sign key for minion \\'%s\\' on hyper \\'%s\\' '\n                'denied: no authorization', self.id, self.hyper\n            )\n            return False\n        except ValueError:\n            log.error('Invalid expiry data in %s', self.path)\n            return False\n\n        # Limit acceptance window to 10 minutes\n        # TODO: Move this value to the master config file\n        if (time.time() - expiry) > 600:\n            log.warning(\n                'Request to sign key for minion \"%s\" on hyper \"%s\" denied: '\n                'authorization expired', self.id, self.hyper\n            )\n            return False\n\n        pubfn = os.path.join(self.opts['pki_dir'],\n                'minions',\n                self.id)\n        with salt.utils.files.fopen(pubfn, 'w+') as fp_:\n            fp_.write(pub)\n        self.void()\n        return True", "output": "Accept the provided key", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sendline(command):\n    '''\n    \n    '''\n    if ping() is False:\n        init()\n    out, _ = DETAILS[_worker_name()].sendline(command)\n    return out", "output": "Run command through switch's cli\n\n    .. code-block: bash\n\n        salt '*' onyx.cmd sendline 'show run | include\n        \"username admin password 7\"'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_roles(username, **kwargs):\n    '''\n    \n    '''\n    user = get_user(username)\n    if not user:\n        return []\n    command = 'show user-account {0}'.format(username)\n    info = ''\n    info = show(command, **kwargs)\n    if isinstance(info, list):\n        info = info[0]\n    roles = re.search(r'^\\s*roles:(.*)$', info, re.MULTILINE)\n    if roles:\n        roles = roles.group(1).strip().split(' ')\n    else:\n        roles = []\n    return roles", "output": "Get roles assigned to a username.\n\n    .. code-block: bash\n\n        salt '*' nxos.cmd get_roles username=admin", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_action(self, brain_info: BrainInfo) -> ActionInfo:\n        \"\"\"\n        \n        \"\"\"\n        if len(brain_info.agents) == 0:\n            return ActionInfo([], [], [], None, None)\n\n        run_out = self.evaluate(brain_info)\n        return ActionInfo(\n            action=run_out.get('action'),\n            memory=run_out.get('memory_out'),\n            text=None,\n            value=run_out.get('value'),\n            outputs=run_out\n        )", "output": "Decides actions given observations information, and takes them in environment.\n        :param brain_info: A dictionary of brain names and BrainInfo from environment.\n        :return: an ActionInfo containing action, memories, values and an object\n        to be passed to add experiences", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _maybe_read_file(filename):\n  \"\"\"\n  \"\"\"\n  try:\n    with open(filename) as infile:\n      return infile.read()\n  except IOError as e:\n    if e.errno == errno.ENOENT:\n      return None", "output": "Read the given file, if it exists.\n\n  Args:\n    filename: A path to a file.\n\n  Returns:\n    A string containing the file contents, or `None` if the file does\n    not exist.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _merge_values(self, values):\n        \"\"\"\n        \"\"\"\n        width = len(self.fields)\n        for value in values:\n            index = len(self._current_row)\n            field = self.fields[index]\n            self._current_row.append(_parse_value_pb(value, field.type))\n            if len(self._current_row) == width:\n                self._rows.append(self._current_row)\n                self._current_row = []", "output": "Merge values into rows.\n\n        :type values: list of :class:`~google.protobuf.struct_pb2.Value`\n        :param values: non-chunked values from partial result set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gen_key(minion_id, dns_name=None, password=None, key_len=2048):\n    '''\n    \n    '''\n    keygen_type = 'RSA'\n\n    if keygen_type == \"RSA\":\n        if HAS_M2:\n            gen = RSA.gen_key(key_len, 65537)\n            private_key = gen.as_pem(cipher='des_ede3_cbc', callback=lambda x: six.b(password))\n        else:\n            gen = RSA.generate(bits=key_len)\n            private_key = gen.exportKey('PEM', password)\n        if dns_name is not None:\n            bank = 'digicert/domains'\n            cache = salt.cache.Cache(__opts__, syspaths.CACHE_DIR)\n            try:\n                data = cache.fetch(bank, dns_name)\n                data['private_key'] = private_key\n                data['minion_id'] = minion_id\n            except TypeError:\n                data = {'private_key': private_key,\n                        'minion_id': minion_id}\n            cache.store(bank, dns_name, data)\n    return private_key", "output": "Generate and return a private_key. If a ``dns_name`` is passed in, the\n    private_key will be cached under that name.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run digicert.gen_key <minion_id> [dns_name] [password]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def median(self, **kwargs):\n        \"\"\"\n        \"\"\"\n        if self._is_transposed:\n            kwargs[\"axis\"] = kwargs.get(\"axis\", 0) ^ 1\n            return self.transpose().median(**kwargs)\n        # Pandas default is 0 (though not mentioned in docs)\n        axis = kwargs.get(\"axis\", 0)\n        func = self._build_mapreduce_func(pandas.DataFrame.median, **kwargs)\n        return self._full_axis_reduce(axis, func)", "output": "Returns median of each column or row.\n\n        Returns:\n            A new QueryCompiler object containing the median of each column or row.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transform(input_fragment, parameter_values, managed_policy_loader):\n    \"\"\"\n    \"\"\"\n\n    sam_parser = Parser()\n    translator = Translator(managed_policy_loader.load(), sam_parser)\n    return translator.translate(input_fragment, parameter_values=parameter_values)", "output": "Translates the SAM manifest provided in the and returns the translation to CloudFormation.\n\n    :param dict input_fragment: the SAM template to transform\n    :param dict parameter_values: Parameter values provided by the user\n    :returns: the transformed CloudFormation template\n    :rtype: dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _serve_asset(self, path, gzipped_asset_bytes, request):\n    \"\"\"\"\"\"\n    mimetype = mimetypes.guess_type(path)[0] or 'application/octet-stream'\n    return http_util.Respond(\n        request, gzipped_asset_bytes, mimetype, content_encoding='gzip')", "output": "Serves a pre-gzipped static asset from the zip file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_content_range(start: Optional[int], end: Optional[int], total: int) -> str:\n    \"\"\"\n    \"\"\"\n    start = start or 0\n    end = (end or total) - 1\n    return \"bytes %s-%s/%s\" % (start, end, total)", "output": "Returns a suitable Content-Range header:\n\n    >>> print(_get_content_range(None, 1, 4))\n    bytes 0-0/4\n    >>> print(_get_content_range(1, 3, 4))\n    bytes 1-2/4\n    >>> print(_get_content_range(None, None, 4))\n    bytes 0-3/4", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve_resource_type(self, resource_dict):\n        \"\"\"\n        \"\"\"\n        if not self.can_resolve(resource_dict):\n            raise TypeError(\"Resource dict has missing or invalid value for key Type. Event Type is: {}.\".format(\n                    resource_dict.get('Type')))\n        if resource_dict['Type'] not in self.resource_types:\n            raise TypeError(\"Invalid resource type {resource_type}\".format(resource_type=resource_dict['Type']))\n        return self.resource_types[resource_dict['Type']]", "output": "Returns the Resource class corresponding to the 'Type' key in the given resource dict.\n\n        :param dict resource_dict: the resource dict to resolve\n        :returns: the resolved Resource class\n        :rtype: class", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def corpus_page_generator(corpus_files, tmp_dir, max_page_size_exp):\n  \"\"\"\n  \"\"\"\n  for remote_filepath in corpus_files:\n\n    filepath = maybe_copy_file_to_directory(remote_filepath, tmp_dir)\n    tf.logging.info(\"Reading from \" + filepath)\n\n    command = [\"7z\", \"x\", \"-so\", filepath]\n    tf.logging.info(\"Running command: %s\", command)\n\n    p = subprocess.Popen(command, stdout=subprocess.PIPE, bufsize=-1)\n\n    for page in file_page_generator(p.stdout, 2**max_page_size_exp):\n      yield page", "output": "Generate pages from a list of .7z encoded history dumps.\n\n  Args:\n    corpus_files: a list of strings\n    tmp_dir: a string\n    max_page_size_exp: an integer\n\n  Yields:\n    strings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_delete(id=None, name=None, profile=None):  # pylint: disable=C0103\n    '''\n    \n    '''\n    g_client = _auth(profile)\n    image = {'id': False, 'name': None}\n    if name:\n        for image in g_client.images.list():\n            if image.name == name:\n                id = image.id  # pylint: disable=C0103\n                continue\n    if not id:\n        return {\n            'result': False,\n            'comment':\n                'Unable to resolve image id '\n                'for name {0}'.format(name)\n            }\n    elif not name:\n        name = image['name']\n    try:\n        g_client.images.delete(id)\n    except exc.HTTPNotFound:\n        return {\n            'result': False,\n            'comment': 'No image with ID {0}'.format(id)\n            }\n    except exc.HTTPForbidden as forbidden:\n        log.error(six.text_type(forbidden))\n        return {\n            'result': False,\n            'comment': six.text_type(forbidden)\n            }\n    return {\n        'result': True,\n        'comment': 'Deleted image \\'{0}\\' ({1}).'.format(name, id),\n        }", "output": "Delete an image (glance image-delete)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' glance.image_delete c2eb2eb0-53e1-4a80-b990-8ec887eae7df\n        salt '*' glance.image_delete id=c2eb2eb0-53e1-4a80-b990-8ec887eae7df\n        salt '*' glance.image_delete name=f16-jeos", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _save_task_definition(name,\n                          task_folder,\n                          task_definition,\n                          user_name,\n                          password,\n                          logon_type):\n    '''\n    \n    '''\n    try:\n        task_folder.RegisterTaskDefinition(name,\n                                           task_definition,\n                                           TASK_CREATE_OR_UPDATE,\n                                           user_name,\n                                           password,\n                                           logon_type)\n\n        return True\n\n    except pythoncom.com_error as error:\n        hr, msg, exc, arg = error.args  # pylint: disable=W0633\n        fc = {-2147024773: 'The filename, directory name, or volume label syntax is incorrect',\n              -2147024894: 'The system cannot find the file specified',\n              -2147216615: 'Required element or attribute missing',\n              -2147216616: 'Value incorrectly formatted or out of range',\n              -2147352571: 'Access denied'}\n        try:\n            failure_code = fc[exc[5]]\n        except KeyError:\n            failure_code = 'Unknown Failure: {0}'.format(error)\n\n        log.debug('Failed to modify task: %s', failure_code)\n\n        return 'Failed to modify task: {0}'.format(failure_code)", "output": "Internal function to save the task definition.\n\n    :param str name: The name of the task.\n\n    :param str task_folder: The object representing the folder in which to save\n    the task\n\n    :param str task_definition: The object representing the task to be saved\n\n    :param str user_name: The user_account under which to run the task\n\n    :param str password: The password that corresponds to the user account\n\n    :param int logon_type: The logon type for the task.\n\n    :return: True if successful, False if not\n    :rtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sanitize_column(self, key, value, **kwargs):\n        \"\"\"\n        \n\n        \"\"\"\n        def sp_maker(x, index=None):\n            return SparseArray(x, index=index,\n                               fill_value=self._default_fill_value,\n                               kind=self._default_kind)\n        if isinstance(value, SparseSeries):\n            clean = value.reindex(self.index).as_sparse_array(\n                fill_value=self._default_fill_value, kind=self._default_kind)\n\n        elif isinstance(value, SparseArray):\n            if len(value) != len(self.index):\n                raise ValueError('Length of values does not match '\n                                 'length of index')\n            clean = value\n\n        elif hasattr(value, '__iter__'):\n            if isinstance(value, Series):\n                clean = value.reindex(self.index)\n                if not isinstance(value, SparseSeries):\n                    clean = sp_maker(clean)\n            else:\n                if len(value) != len(self.index):\n                    raise ValueError('Length of values does not match '\n                                     'length of index')\n                clean = sp_maker(value)\n\n        # Scalar\n        else:\n            clean = sp_maker(value, self.index)\n\n        # always return a SparseArray!\n        return clean", "output": "Creates a new SparseArray from the input value.\n\n        Parameters\n        ----------\n        key : object\n        value : scalar, Series, or array-like\n        kwargs : dict\n\n        Returns\n        -------\n        sanitized_column : SparseArray", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_request(\n        self,\n        method,\n        url,\n        data=None,\n        content_type=None,\n        headers=None,\n        target_object=None,\n    ):\n        \"\"\"\n        \"\"\"\n        headers = headers or {}\n        headers.update(self._EXTRA_HEADERS)\n        headers[\"Accept-Encoding\"] = \"gzip\"\n\n        if content_type:\n            headers[\"Content-Type\"] = content_type\n\n        headers[\"User-Agent\"] = self.USER_AGENT\n\n        return self._do_request(method, url, headers, data, target_object)", "output": "A low level method to send a request to the API.\n\n        Typically, you shouldn't need to use this method.\n\n        :type method: str\n        :param method: The HTTP method to use in the request.\n\n        :type url: str\n        :param url: The URL to send the request to.\n\n        :type data: str\n        :param data: The data to send as the body of the request.\n\n        :type content_type: str\n        :param content_type: The proper MIME type of the data provided.\n\n        :type headers: dict\n        :param headers: (Optional) A dictionary of HTTP headers to send with\n                        the request. If passed, will be modified directly\n                        here with added headers.\n\n        :type target_object: object\n        :param target_object:\n            (Optional) Argument to be used by library callers.  This can allow\n            custom behavior, for example, to defer an HTTP request and complete\n            initialization of the object at a later time.\n\n        :rtype: :class:`requests.Response`\n        :returns: The HTTP response.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _invoke_request_handler(self, function_name):\n        \"\"\"\n        \n        \"\"\"\n        flask_request = request\n\n        request_data = flask_request.get_data()\n\n        if not request_data:\n            request_data = b'{}'\n\n        request_data = request_data.decode('utf-8')\n\n        stdout_stream = io.BytesIO()\n        stdout_stream_writer = StreamWriter(stdout_stream, self.is_debugging)\n\n        try:\n            self.lambda_runner.invoke(function_name, request_data, stdout=stdout_stream_writer, stderr=self.stderr)\n        except FunctionNotFound:\n            LOG.debug('%s was not found to invoke.', function_name)\n            return LambdaErrorResponses.resource_not_found(function_name)\n\n        lambda_response, lambda_logs, is_lambda_user_error_response = \\\n            LambdaOutputParser.get_lambda_output(stdout_stream)\n\n        if self.stderr and lambda_logs:\n            # Write the logs to stderr if available.\n            self.stderr.write(lambda_logs)\n\n        if is_lambda_user_error_response:\n            return self.service_response(lambda_response,\n                                         {'Content-Type': 'application/json', 'x-amz-function-error': 'Unhandled'},\n                                         200)\n\n        return self.service_response(lambda_response, {'Content-Type': 'application/json'}, 200)", "output": "Request Handler for the Local Lambda Invoke path. This method is responsible for understanding the incoming\n        request and invoking the Local Lambda Function\n\n        Parameters\n        ----------\n        function_name str\n            Name of the function to invoke\n\n        Returns\n        -------\n        A Flask Response response object as if it was returned from Lambda", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write(path, *args, **kwargs):\n    '''\n    \n\n    '''\n    path = os.path.expanduser(path)\n\n    if 'args' in kwargs:\n        if isinstance(kwargs['args'], list):\n            args = kwargs['args']\n        else:\n            args = [kwargs['args']]\n\n    contents = []\n    for line in args:\n        contents.append('{0}\\n'.format(line))\n    with salt.utils.files.fopen(path, \"w\") as ofile:\n        ofile.write(salt.utils.stringutils.to_str(''.join(contents)))\n    return 'Wrote {0} lines to \"{1}\"'.format(len(contents), path)", "output": ".. versionadded:: 2014.7.0\n\n    Write text to a file, overwriting any existing contents.\n\n    path\n        path to file\n\n    `*args`\n        strings to write to the file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.write /etc/motd \\\\\n                \"With all thine offerings thou shalt offer salt.\"\n\n    .. admonition:: Attention\n\n        If you need to pass a string to append and that string contains\n        an equal sign, you **must** include the argument name, args.\n        For example:\n\n        .. code-block:: bash\n\n            salt '*' file.write /etc/motd args='cheese=spam'\n\n            salt '*' file.write /etc/motd args=\"['cheese=spam','spam=cheese']\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def guess_archive_type(name):\n    '''\n    \n    '''\n    name = name.lower()\n    for ending in ('tar', 'tar.gz', 'tgz',\n                   'tar.bz2', 'tbz2', 'tbz',\n                   'tar.xz', 'txz',\n                   'tar.lzma', 'tlz'):\n        if name.endswith('.' + ending):\n            return 'tar'\n    for ending in ('zip', 'rar'):\n        if name.endswith('.' + ending):\n            return ending\n    return None", "output": "Guess an archive type (tar, zip, or rar) by its file extension", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def one_hot_class_label_loss(top_out,\n                             targets,\n                             model_hparams,\n                             vocab_size,\n                             weights_fn):\n  \"\"\"\n  \"\"\"\n  del model_hparams, vocab_size  # unused arg\n  loss_scale = tf.losses.softmax_cross_entropy(\n      onehot_labels=targets, logits=top_out)\n  weights = weights_fn(targets)\n  loss_denom = tf.reduce_sum(weights)\n  return loss_scale, loss_denom", "output": "Apply softmax cross-entropy between outputs and targets.\n\n  Args:\n    top_out: logits Tensor with shape [batch, ?, ?, num_classes]\n    targets: one-hot encoding Tensor with shape [batch, ?, ?, num_classes]\n    model_hparams: HParams, model hyperparmeters.\n    vocab_size: int, vocabulary size.\n    weights_fn:\n\n  Returns:\n    loss_scale (cross-entropy), loss_denom", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bootstrap_container(name, dist=None, version=None):\n    '''\n    \n    '''\n    if not dist:\n        dist = __grains__['os'].lower()\n        log.debug('nspawn.bootstrap: no dist provided, defaulting to \\'%s\\'', dist)\n    try:\n        return globals()['_bootstrap_{0}'.format(dist)](name, version=version)\n    except KeyError:\n        raise CommandExecutionError('Unsupported distribution \"{0}\"'.format(dist))", "output": "Bootstrap a container from package servers, if dist is None the os the\n    minion is running as will be created, otherwise the needed bootstrapping\n    tools will need to be available on the host.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion nspawn.bootstrap_container <name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_env(saltenv='base'):\n    '''\n    \n    '''\n    ret = {}\n    if saltenv not in __opts__['pillar_roots']:\n        return ret\n    for f_root in __opts__['pillar_roots'][saltenv]:\n        ret[f_root] = {}\n        for root, dirs, files in salt.utils.path.os_walk(f_root):\n            sub = ret[f_root]\n            if root != f_root:\n                # grab subroot ref\n                sroot = root\n                above = []\n                # Populate the above dict\n                while not os.path.samefile(sroot, f_root):\n                    base = os.path.basename(sroot)\n                    if base:\n                        above.insert(0, base)\n                    sroot = os.path.dirname(sroot)\n                for aroot in above:\n                    sub = sub[aroot]\n            for dir_ in dirs:\n                sub[dir_] = {}\n            for fn_ in files:\n                sub[fn_] = 'f'\n    return ret", "output": "Return all of the file paths found in an environment", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def restore_scrollbar_position(self):\r\n        \"\"\"\"\"\"\r\n        scrollbar_pos = self.get_option('scrollbar_position', None)\r\n        if scrollbar_pos is not None:\r\n            self.explorer.treewidget.set_scrollbar_position(scrollbar_pos)", "output": "Restoring scrollbar position after main window is visible", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def explore_json(self, datasource_type=None, datasource_id=None):\n        \"\"\"\"\"\"\n        csv = request.args.get('csv') == 'true'\n        query = request.args.get('query') == 'true'\n        results = request.args.get('results') == 'true'\n        samples = request.args.get('samples') == 'true'\n        force = request.args.get('force') == 'true'\n\n        form_data = get_form_data()[0]\n        datasource_id, datasource_type = get_datasource_info(\n            datasource_id, datasource_type, form_data)\n        viz_obj = get_viz(\n            datasource_type=datasource_type,\n            datasource_id=datasource_id,\n            form_data=form_data,\n            force=force,\n        )\n\n        return self.generate_json(\n            viz_obj,\n            csv=csv,\n            query=query,\n            results=results,\n            samples=samples,\n        )", "output": "Serves all request that GET or POST form_data\n\n        This endpoint evolved to be the entry point of many different\n        requests that GETs or POSTs a form_data.\n\n        `self.generate_json` receives this input and returns different\n        payloads based on the request args in the first block\n\n        TODO: break into one endpoint for each return shape", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def table(ax, data, rowLabels=None, colLabels=None, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(data, ABCSeries):\n        data = data.to_frame()\n    elif isinstance(data, ABCDataFrame):\n        pass\n    else:\n        raise ValueError('Input data must be DataFrame or Series')\n\n    if rowLabels is None:\n        rowLabels = data.index\n\n    if colLabels is None:\n        colLabels = data.columns\n\n    cellText = data.values\n\n    import matplotlib.table\n    table = matplotlib.table.table(ax, cellText=cellText,\n                                   rowLabels=rowLabels,\n                                   colLabels=colLabels, **kwargs)\n    return table", "output": "Helper function to convert DataFrame and Series to matplotlib.table\n\n    Parameters\n    ----------\n    ax : Matplotlib axes object\n    data : DataFrame or Series\n        data for table contents\n    kwargs : keywords, optional\n        keyword arguments which passed to matplotlib.table.table.\n        If `rowLabels` or `colLabels` is not specified, data index or column\n        name will be used.\n\n    Returns\n    -------\n    matplotlib table object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reduceByKeyLocally(self, func):\n        \"\"\"\n        \n        \"\"\"\n        func = fail_on_stopiteration(func)\n\n        def reducePartition(iterator):\n            m = {}\n            for k, v in iterator:\n                m[k] = func(m[k], v) if k in m else v\n            yield m\n\n        def mergeMaps(m1, m2):\n            for k, v in m2.items():\n                m1[k] = func(m1[k], v) if k in m1 else v\n            return m1\n        return self.mapPartitions(reducePartition).reduce(mergeMaps)", "output": "Merge the values for each key using an associative and commutative reduce function, but\n        return the results immediately to the master as a dictionary.\n\n        This will also perform the merging locally on each mapper before\n        sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n\n        >>> from operator import add\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n        >>> sorted(rdd.reduceByKeyLocally(add).items())\n        [('a', 2), ('b', 1)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keyPressEvent(self, event):\n        \"\"\"\"\"\"\n        event, text, key, ctrl, shift = restore_keyevent(event)\n\n        if key == Qt.Key_Slash and self.isVisible():\n            self.show_find_widget.emit()", "output": "Reimplement Qt Method - Basic keypress event handler", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _compute_zs_mat(sz:TensorImageSize, scale:float, squish:float,\n                   invert:bool, row_pct:float, col_pct:float)->AffineMatrix:\n    \"\"\n    orig_ratio = math.sqrt(sz[1]/sz[0])\n    for s,r,i in zip(scale,squish, invert):\n        s,r = 1/math.sqrt(s),math.sqrt(r)\n        if s * r <= 1 and s / r <= 1: #Test if we are completely inside the picture\n            w,h = (s/r, s*r) if i else (s*r,s/r)\n            col_c = (1-w) * (2*col_pct - 1)\n            row_c = (1-h) * (2*row_pct - 1)\n            return _get_zoom_mat(w, h, col_c, row_c)\n\n    #Fallback, hack to emulate a center crop without cropping anything yet.\n    if orig_ratio > 1: return _get_zoom_mat(1/orig_ratio**2, 1, 0, 0.)\n    else:              return _get_zoom_mat(1, orig_ratio**2, 0, 0.)", "output": "Utility routine to compute zoom/squish matrix.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def relaxNGValidatePopElement(self, ctxt, elem):\n        \"\"\" \"\"\"\n        if ctxt is None: ctxt__o = None\n        else: ctxt__o = ctxt._o\n        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        ret = libxml2mod.xmlRelaxNGValidatePopElement(ctxt__o, self._o, elem__o)\n        return ret", "output": "Pop the element end from the RelaxNG validation stack.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attention_bias_proximal(length):\n  \"\"\"\n  \"\"\"\n  r = tf.to_float(tf.range(length))\n  diff = tf.expand_dims(r, 0) - tf.expand_dims(r, 1)\n  return tf.expand_dims(tf.expand_dims(-tf.log1p(tf.abs(diff)), 0), 0)", "output": "Bias for self-attention to encourage attention to close positions.\n\n  Args:\n    length: an integer scalar.\n\n  Returns:\n    a Tensor with shape [1, 1, length, length]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Expand(self):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlTextReaderExpand(self._o)\n        if ret is None:raise treeError('xmlTextReaderExpand() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Reads the contents of the current node and the full\n          subtree. It then makes the subtree available until the next\n           xmlTextReaderRead() call", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _load_pretrained_vocab(name, root=os.path.join(get_home_dir(), 'models'), cls=None):\n    \"\"\"\n    \"\"\"\n    file_name = '{name}-{short_hash}'.format(name=name,\n                                             short_hash=short_hash(name))\n    root = os.path.expanduser(root)\n    file_path = os.path.join(root, file_name + '.vocab')\n    sha1_hash = _vocab_sha1[name]\n    if os.path.exists(file_path):\n        if check_sha1(file_path, sha1_hash):\n            return _load_vocab_file(file_path, cls)\n        else:\n            print('Detected mismatch in the content of model vocab file. Downloading again.')\n    else:\n        print('Vocab file is not found. Downloading.')\n\n    if not os.path.exists(root):\n        os.makedirs(root)\n\n    zip_file_path = os.path.join(root, file_name + '.zip')\n    repo_url = _get_repo_url()\n    if repo_url[-1] != '/':\n        repo_url = repo_url + '/'\n    download(_url_format.format(repo_url=repo_url, file_name=file_name),\n             path=zip_file_path,\n             overwrite=True)\n    with zipfile.ZipFile(zip_file_path) as zf:\n        zf.extractall(root)\n    os.remove(zip_file_path)\n\n    if check_sha1(file_path, sha1_hash):\n        return _load_vocab_file(file_path, cls)\n    else:\n        raise ValueError('Downloaded file has different hash. Please try again.')", "output": "Load the accompanying vocabulary object for pre-trained model.\n\n    Parameters\n    ----------\n    name : str\n        Name of the vocabulary, usually the name of the dataset.\n    root : str, default '$MXNET_HOME/models'\n        Location for keeping the model parameters.\n        MXNET_HOME defaults to '~/.mxnet'.\n    cls : nlp.Vocab or nlp.vocab.BERTVocab, default nlp.Vocab\n\n    Returns\n    -------\n    Vocab or nlp.vocab.BERTVocab\n        Loaded vocabulary object for the pre-trained model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_time():\n    '''\n    \n    '''\n    ret = salt.utils.mac_utils.execute_return_result('systemsetup -gettime')\n    return salt.utils.mac_utils.parse_return(ret)", "output": "Get the current system time.\n\n    :return: The current time in 24 hour format\n    :rtype: str\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' timezone.get_time", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def simplify_path(path):\n    \"\"\"\n    \n    \"\"\"\n    skip = {'..', '.', ''}\n    stack = []\n    paths = path.split('/')\n    for tok in paths:\n        if tok == '..':\n            if stack:\n                stack.pop()\n        elif tok not in skip:\n            stack.append(tok)\n    return '/' + '/'.join(stack)", "output": ":type path: str\n    :rtype: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_catalog():\n    '''\n    \n    '''\n    cmd = ['defaults',\n           'read',\n           '/Library/Preferences/com.apple.SoftwareUpdate.plist']\n    out = salt.utils.mac_utils.execute_return_result(cmd)\n\n    if 'AppleCatalogURL' in out:\n        cmd.append('AppleCatalogURL')\n        out = salt.utils.mac_utils.execute_return_result(cmd)\n        return out\n    elif 'CatalogURL' in out:\n        cmd.append('CatalogURL')\n        out = salt.utils.mac_utils.execute_return_result(cmd)\n        return out\n    else:\n        return 'Default'", "output": ".. versionadded:: 2016.3.0\n\n    Get the current catalog being used for update lookups. Will return a url if\n    a custom catalog has been specified. Otherwise the word 'Default' will be\n    returned\n\n    :return: The catalog being used for update lookups\n    :rtype: str\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' softwareupdates.get_catalog", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ufunc_helper(lhs, rhs, fn_array, fn_scalar, lfn_scalar, rfn_scalar=None):\n    \"\"\" \n    \"\"\"\n    if isinstance(lhs, numeric_types):\n        if isinstance(rhs, numeric_types):\n            return fn_scalar(lhs, rhs)\n        else:\n            if rfn_scalar is None:\n                # commutative function\n                return lfn_scalar(rhs, float(lhs))\n            else:\n                return rfn_scalar(rhs, float(lhs))\n    elif isinstance(rhs, numeric_types):\n        return lfn_scalar(lhs, float(rhs))\n    elif isinstance(rhs, NDArray):\n        return fn_array(lhs, rhs)\n    else:\n        raise TypeError('type %s not supported' % str(type(rhs)))", "output": "Helper function for element-wise operation.\n    The function will perform numpy-like broadcasting if needed and call different functions.\n\n    Parameters\n    --------\n    lhs : NDArray or numeric value\n        Left-hand side operand.\n\n    rhs : NDArray or numeric value\n        Right-hand operand,\n\n    fn_array : function\n        Function to be called if both lhs and rhs are of ``NDArray`` type.\n\n    fn_scalar : function\n        Function to be called if both lhs and rhs are numeric values.\n\n    lfn_scalar : function\n        Function to be called if lhs is ``NDArray`` while rhs is numeric value\n\n    rfn_scalar : function\n        Function to be called if lhs is numeric value while rhs is ``NDArray``;\n        if none is provided, then the function is commutative, so rfn_scalar is equal to lfn_scalar\n\n    Returns\n    --------\n    NDArray\n        result array", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def print_config_value(self, name, prefix='- ', separator=': '):\n        \"\"\"\n        \"\"\"\n\n        value_out = 'None'\n        if name in self.config_values and self.config_values[name] is not None:\n            value_out = self.config_values[name]\n        print(prefix + name + separator + value_out)", "output": "print a single configuration value, based on a prefix and separator\n\n           Parameters\n           ==========\n           name: the key of the config valur in self.config_values to print\n           prefix: the prefix to print\n           separator: the separator to use (default is : )", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_netrc_auth(url, raise_errors=False):\n    \"\"\"\"\"\"\n\n    try:\n        from netrc import netrc, NetrcParseError\n\n        netrc_path = None\n\n        for f in NETRC_FILES:\n            try:\n                loc = os.path.expanduser('~/{}'.format(f))\n            except KeyError:\n                # os.path.expanduser can fail when $HOME is undefined and\n                # getpwuid fails. See https://bugs.python.org/issue20164 &\n                # https://github.com/requests/requests/issues/1846\n                return\n\n            if os.path.exists(loc):\n                netrc_path = loc\n                break\n\n        # Abort early if there isn't one.\n        if netrc_path is None:\n            return\n\n        ri = urlparse(url)\n\n        # Strip port numbers from netloc. This weird `if...encode`` dance is\n        # used for Python 3.2, which doesn't support unicode literals.\n        splitstr = b':'\n        if isinstance(url, str):\n            splitstr = splitstr.decode('ascii')\n        host = ri.netloc.split(splitstr)[0]\n\n        try:\n            _netrc = netrc(netrc_path).authenticators(host)\n            if _netrc:\n                # Return with login / password\n                login_i = (0 if _netrc[0] else 1)\n                return (_netrc[login_i], _netrc[2])\n        except (NetrcParseError, IOError):\n            # If there was a parsing error or a permissions issue reading the file,\n            # we'll just skip netrc auth unless explicitly asked to raise errors.\n            if raise_errors:\n                raise\n\n    # AppEngine hackiness.\n    except (ImportError, AttributeError):\n        pass", "output": "Returns the Requests tuple auth for a given url from netrc.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_fbeta_score(true_positives, selected, relevant, beta=1):\n  \"\"\"\n  \"\"\"\n  precision = 1\n  if selected > 0:\n    precision = true_positives / selected\n  if beta == 0:\n    return precision\n  recall = 1\n  if relevant > 0:\n    recall = true_positives / relevant\n  if precision > 0 and recall > 0:\n    beta2 = beta * beta\n    return (1 + beta2) * precision * recall / (beta2 * precision + recall)\n  else:\n    return 0", "output": "Compute Fbeta score.\n\n  Args:\n    true_positives: Number of true positive ngrams.\n    selected: Number of selected ngrams.\n    relevant: Number of relevant ngrams.\n    beta: 0 gives precision only, 1 gives F1 score, and Inf gives recall only.\n\n  Returns:\n    Fbeta score.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_get_hkindex_list(ip=None, port=None):\n    \"\"\"\n\n    \"\"\"\n\n    global extension_market_list\n    extension_market_list = QA_fetch_get_extensionmarket_list(\n    ) if extension_market_list is None else extension_market_list\n\n    return extension_market_list.query('market==27')", "output": "[summary]\n\n    Keyword Arguments:\n        ip {[type]} -- [description] (default: {None})\n        port {[type]} -- [description] (default: {None})\n\n# \u6e2f\u80a1 HKMARKET\n       27         5      \u9999\u6e2f\u6307\u6570         FH\n       31         2      \u9999\u6e2f\u4e3b\u677f         KH\n       48         2     \u9999\u6e2f\u521b\u4e1a\u677f         KG\n       49         2      \u9999\u6e2f\u57fa\u91d1         KT\n       43         1     B\u80a1\u8f6cH\u80a1         HB", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flavor_delete(flavor_id, profile=None, **kwargs):  # pylint: disable=C0103\n    '''\n    \n    '''\n    conn = _auth(profile, **kwargs)\n    return conn.flavor_delete(flavor_id)", "output": "Delete a flavor from nova by id (nova flavor-delete)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' nova.flavor_delete 7", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_take_with_convert(convert, args, kwargs):\n    \"\"\"\n    \n    \"\"\"\n\n    if isinstance(convert, ndarray) or convert is None:\n        args = (convert,) + args\n        convert = True\n\n    validate_take(args, kwargs, max_fname_arg_count=3, method='both')\n    return convert", "output": "If this function is called via the 'numpy' library, the third\n    parameter in its signature is 'axis', which takes either an\n    ndarray or 'None', so check if the 'convert' parameter is either\n    an instance of ndarray or is None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_coco_results(self, _coco, detections):\n        \"\"\" \n        \"\"\"\n        cats = [cat['name'] for cat in _coco.loadCats(_coco.getCatIds())]\n        class_to_coco_ind = dict(zip(cats, _coco.getCatIds()))\n        results = []\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == '__background__':\n                continue\n            logger.info('collecting %s results (%d/%d)' % (cls, cls_ind, self.num_classes - 1))\n            coco_cat_id = class_to_coco_ind[cls]\n            results.extend(self._coco_results_one_category(detections[cls_ind], coco_cat_id))\n        logger.info('writing results json to %s' % self._result_file)\n        with open(self._result_file, 'w') as f:\n            json.dump(results, f, sort_keys=True, indent=4)", "output": "example results\n        [{\"image_id\": 42,\n          \"category_id\": 18,\n          \"bbox\": [258.15,41.29,348.26,243.78],\n          \"score\": 0.236}, ...]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _escape_token(token, alphabet):\n  \"\"\"\n  \"\"\"\n  if not isinstance(token, six.text_type):\n    raise ValueError(\"Expected string type for token, got %s\" % type(token))\n\n  token = token.replace(u\"\\\\\", u\"\\\\\\\\\").replace(u\"_\", u\"\\\\u\")\n  ret = [c if c in alphabet and c != u\"\\n\" else r\"\\%d;\" % ord(c) for c in token]\n  return u\"\".join(ret) + \"_\"", "output": "Escape away underscores and OOV characters and append '_'.\n\n  This allows the token to be expressed as the concatenation of a list\n  of subtokens from the vocabulary. The underscore acts as a sentinel\n  which allows us to invertibly concatenate multiple such lists.\n\n  Args:\n    token: A unicode string to be escaped.\n    alphabet: A set of all characters in the vocabulary's alphabet.\n\n  Returns:\n    escaped_token: An escaped unicode string.\n\n  Raises:\n    ValueError: If the provided token is not unicode.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show(self):\n        \"\"\"\n        \n\n        \"\"\"\n        global _target\n        display = False\n        try:\n            if _target == 'auto' and \\\n               get_ipython().__class__.__name__ == \"ZMQInteractiveShell\":\n                self._repr_javascript_()\n                display = True\n        except NameError:\n            pass\n        finally:\n            if not display:\n                if _sys.platform != 'darwin' and _sys.platform != 'linux2' and _sys.platform != 'linux':\n                     raise NotImplementedError('Visualization is currently supported only on macOS and Linux.')\n\n                path_to_client = _get_client_app_path()\n\n                # TODO: allow autodetection of light/dark mode.\n                # Disabled for now, since the GUI side needs some work (ie. background color).\n                plot_variation = 0x10 # force light mode\n                self.__proxy__.call_function('show', {'path_to_client': path_to_client, 'variation': plot_variation})", "output": "A method for displaying the Plot object\n\n        Notes\n        -----\n        - The plot will render either inline in a Jupyter Notebook, or in a\n          native GUI window, depending on the value provided in\n          `turicreate.visualization.set_target` (defaults to 'auto').\n\n        Examples\n        --------\n        Suppose 'plt' is an Plot Object\n\n        We can view it using:\n\n        >>> plt.show()", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_locations(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudException(\n            'The avail_locations function must be called with -f or --function.'\n        )\n\n    vm_ = get_configured_provider()\n    manager = packet.Manager(auth_token=vm_['token'])\n\n    ret = {}\n\n    for facility in manager.list_facilities():\n        ret[facility.name] = facility.__dict__\n\n    return ret", "output": "Return available Packet datacenter locations.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud --list-locations packet-provider\n        salt-cloud -f avail_locations packet-provider", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log_group_name(self):\n        \"\"\"\n        \n        \"\"\"\n\n        function_id = self._function_name\n        if self._stack_name:\n            function_id = self._get_resource_id_from_stack(self._cfn_client, self._stack_name, self._function_name)\n            LOG.debug(\"Function with LogicalId '%s' in stack '%s' resolves to actual physical ID '%s'\",\n                      self._function_name, self._stack_name, function_id)\n\n        return LogGroupProvider.for_lambda_function(function_id)", "output": "Name of the AWS CloudWatch Log Group that we will be querying. It generates the name based on the\n        Lambda Function name and stack name provided.\n\n        Returns\n        -------\n        str\n            Name of the CloudWatch Log Group", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def patch_namespaced_custom_object(self, group, version, namespace, plural, name, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_namespaced_custom_object_with_http_info(group, version, namespace, plural, name, body, **kwargs)\n        else:\n            (data) = self.patch_namespaced_custom_object_with_http_info(group, version, namespace, plural, name, body, **kwargs)\n            return data", "output": "patch the specified namespace scoped custom object\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_namespaced_custom_object(group, version, namespace, plural, name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str group: the custom resource's group (required)\n        :param str version: the custom resource's version (required)\n        :param str namespace: The custom resource's namespace (required)\n        :param str plural: the custom resource's plural name. For TPRs this would be lowercase plural kind. (required)\n        :param str name: the custom object's name (required)\n        :param object body: The JSON schema of the Resource to patch. (required)\n        :return: object\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def index(self, *index):\n        \"\"\"\n        \n        \"\"\"\n        # .index() resets\n        s = self._clone()\n        if not index:\n            s._index = None\n        else:\n            indexes = []\n            for i in index:\n                if isinstance(i, string_types):\n                    indexes.append(i)\n                elif isinstance(i, list):\n                    indexes += i\n                elif isinstance(i, tuple):\n                    indexes += list(i)\n\n            s._index = (self._index or []) + indexes\n\n        return s", "output": "Set the index for the search. If called empty it will remove all information.\n\n        Example:\n\n            s = Search()\n            s = s.index('twitter-2015.01.01', 'twitter-2015.01.02')\n            s = s.index(['twitter-2015.01.01', 'twitter-2015.01.02'])", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_path_to_api_gateway(path):\n        \"\"\"\n        \n        \"\"\"\n        proxy_sub_path = FLASK_TO_APIGW_REGEX.sub(PROXY_PATH_PARAMS, path)\n\n        # Replace the '<' and '>' with '{' and '}' respectively\n        return proxy_sub_path.replace(LEFT_ANGLE_BRACKET, LEFT_BRACKET).replace(RIGHT_ANGLE_BRACKET, RIGHT_BRACKET)", "output": "Converts a Path from a Flask defined path to one that is accepted by Api Gateway\n\n        Examples:\n\n        '/id/<id>' => '/id/{id}'\n        '/<path:proxy>' => '/{proxy+}'\n\n        :param str path: Path to convert to Api Gateway defined path\n        :return str: Path representing an Api Gateway path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toggle_codecompletion(self, checked):\r\n        \"\"\"\"\"\"\r\n        self.shell.set_codecompletion_auto(checked)\r\n        self.set_option('codecompletion/auto', checked)", "output": "Toggle automatic code completion", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mkdir_temp(mode=0o755):\n    \"\"\"\n    \n\n    \"\"\"\n\n    temp_dir = None\n    try:\n        temp_dir = tempfile.mkdtemp()\n        os.chmod(temp_dir, mode)\n\n        yield temp_dir\n\n    finally:\n        if temp_dir:\n            shutil.rmtree(temp_dir)", "output": "Context manager that makes a temporary directory and yields it name. Directory is deleted\n    after the context exits\n\n    Parameters\n    ----------\n    mode : octal\n        Permissions to apply to the directory. Defaults to '755' because don't want directories world writable\n\n    Returns\n    -------\n    str\n        Path to the directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rm_host(ip, alias):\n    '''\n    \n    '''\n    if not has_pair(ip, alias):\n        return True\n    # Make sure future calls to _list_hosts() will re-read the file\n    __context__.pop('hosts._list_hosts', None)\n    hfn = _get_or_create_hostfile()\n    with salt.utils.files.fopen(hfn, 'rb') as fp_:\n        lines = fp_.readlines()\n    for ind, _ in enumerate(lines):\n        tmpline = lines[ind].strip()\n        if not tmpline:\n            continue\n        if tmpline.startswith(b'#'):\n            continue\n        comps = tmpline.split()\n        b_ip = salt.utils.stringutils.to_bytes(ip)\n        b_alias = salt.utils.stringutils.to_bytes(alias)\n        if comps[0] == b_ip:\n            newline = comps[0] + b'\\t\\t'\n            for existing in comps[1:]:\n                if existing == b_alias:\n                    continue\n                newline += existing + b' '\n            if newline.strip() == b_ip:\n                # No aliases exist for the line, make it empty\n                lines[ind] = b''\n            else:\n                # Only an alias was removed\n                lines[ind] = newline + salt.utils.stringutils.to_bytes(os.linesep)\n    with salt.utils.files.fopen(hfn, 'wb') as ofile:\n        ofile.writelines(lines)\n    return True", "output": "Remove a host entry from the hosts file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' hosts.rm_host <ip> <alias>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def grad_dict(self):\n        \"\"\"\n        \"\"\"\n        if self._grad_dict is None:\n            self._grad_dict = Executor._get_dict(\n                self._symbol.list_arguments(), self.grad_arrays)\n        return self._grad_dict", "output": "Get dictionary representation of gradient arrays.\n\n        Returns\n        -------\n        grad_dict : dict of str to NDArray\n            The dictionary that maps name of arguments to gradient arrays.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _emit_metrics(self, missing_datetimes, finite_start, finite_stop):\n        \"\"\"\n        \n        \"\"\"\n        datetimes = self.finite_datetimes(\n            finite_start if self.start is None else min(finite_start, self.parameter_to_datetime(self.start)),\n            finite_stop if self.stop is None else max(finite_stop, self.parameter_to_datetime(self.stop)))\n\n        delay_in_jobs = len(datetimes) - datetimes.index(missing_datetimes[0]) if datetimes and missing_datetimes else 0\n        self.trigger_event(RangeEvent.DELAY, self.of_cls.task_family, delay_in_jobs)\n\n        expected_count = len(datetimes)\n        complete_count = expected_count - len(missing_datetimes)\n        self.trigger_event(RangeEvent.COMPLETE_COUNT, self.of_cls.task_family, complete_count)\n        self.trigger_event(RangeEvent.COMPLETE_FRACTION, self.of_cls.task_family, float(complete_count) / expected_count if expected_count else 1)", "output": "For consistent metrics one should consider the entire range, but\n        it is open (infinite) if stop or start is None.\n\n        Hence make do with metrics respective to the finite simplification.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def verify_files(files, user):\n    '''\n    \n    '''\n    if salt.utils.platform.is_windows():\n        return True\n    import pwd  # after confirming not running Windows\n    try:\n        pwnam = pwd.getpwnam(user)\n        uid = pwnam[2]\n    except KeyError:\n        err = ('Failed to prepare the Salt environment for user '\n               '{0}. The user is not available.\\n').format(user)\n        sys.stderr.write(err)\n        sys.exit(salt.defaults.exitcodes.EX_NOUSER)\n\n    for fn_ in files:\n        dirname = os.path.dirname(fn_)\n        try:\n            if dirname:\n                try:\n                    os.makedirs(dirname)\n                except OSError as err:\n                    if err.errno != errno.EEXIST:\n                        raise\n            if not os.path.isfile(fn_):\n                with salt.utils.files.fopen(fn_, 'w'):\n                    pass\n\n        except IOError as err:\n            if os.path.isfile(dirname):\n                msg = 'Failed to create path {0}, is {1} a file?'.format(fn_, dirname)\n                raise SaltSystemExit(msg=msg)\n            if err.errno != errno.EACCES:\n                raise\n            msg = 'No permissions to access \"{0}\", are you running as the correct user?'.format(fn_)\n            raise SaltSystemExit(msg=msg)\n\n        except OSError as err:\n            msg = 'Failed to create path \"{0}\" - {1}'.format(fn_, err)\n            raise SaltSystemExit(msg=msg)\n\n        stats = os.stat(fn_)\n        if uid != stats.st_uid:\n            try:\n                os.chown(fn_, uid, -1)\n            except OSError:\n                pass\n    return True", "output": "Verify that the named files exist and are owned by the named user", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_tip(self, tip=\"\"):\r\n        \"\"\"\"\"\"\r\n        QToolTip.showText(self.mapToGlobal(self.pos()), tip, self)", "output": "Show tip", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_tab(self, widget, name, filename=''):\r\n        \"\"\"\"\"\"\r\n        self.clients.append(widget)\r\n        index = self.tabwidget.addTab(widget, name)\r\n        self.filenames.insert(index, filename)\r\n        self.tabwidget.setCurrentIndex(index)\r\n        if self.dockwidget and not self.main.is_setting_up:\r\n            self.switch_to_plugin()\r\n        self.activateWindow()\r\n        widget.get_control().setFocus()\r\n        self.update_tabs_text()", "output": "Add tab", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self, i, o):  # type: () -> int\n        \"\"\"\n        \n        \"\"\"\n        self.input = i\n        self.output = PoetryStyle(i, o)\n\n        for logger in self._loggers:\n            self.register_logger(logging.getLogger(logger))\n\n        return super(BaseCommand, self).run(i, o)", "output": "Initialize command.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_array(self, tensor_proto):\n        \"\"\"\"\"\"\n        try:\n            from onnx.numpy_helper import to_array\n        except ImportError:\n            raise ImportError(\"Onnx and protobuf need to be installed. \"\n                              + \"Instructions to install - https://github.com/onnx/onnx\")\n        if len(tuple(tensor_proto.dims)) > 0:\n            np_array = to_array(tensor_proto).reshape(tuple(tensor_proto.dims))\n        else:\n            # If onnx's params are scalar values without dims mentioned.\n            np_array = np.array([to_array(tensor_proto)])\n        return nd.array(np_array)", "output": "Grab data in TensorProto and convert to numpy array.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _leave_event_hide(self):\n        \"\"\" \n        \"\"\"\n        if (self.hide_timer_on and not self._hide_timer.isActive() and\n            # If Enter events always came after Leave events, we wouldn't need\n            # this check. But on Mac OS, it sometimes happens the other way\n            # around when the tooltip is created.\n            self.app.topLevelAt(QCursor.pos()) != self):\n            self._hide_timer.start(800, self)", "output": "Hides the tooltip after some time has passed (assuming the cursor is\n            not over the tooltip).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def annualize_return(self):\n        \"\"\"\n        \"\"\"\n\n        return round(\n            float(self.calc_annualize_return(self.assets,\n                                             self.time_gap)),\n            2\n        )", "output": "\u5e74\u5316\u6536\u76ca\n\n        Returns:\n            [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _strip_extra(elements):\n    \"\"\"\n    \"\"\"\n    extra_indexes = []\n    for i, element in enumerate(elements):\n        if isinstance(element, list):\n            cancelled = _strip_extra(element)\n            if cancelled:\n                extra_indexes.append(i)\n        elif isinstance(element, tuple) and element[0].value == \"extra\":\n            extra_indexes.append(i)\n    for i in reversed(extra_indexes):\n        del elements[i]\n        if i > 0 and elements[i - 1] == \"and\":\n            # Remove the \"and\" before it.\n            del elements[i - 1]\n        elif elements:\n            # This shouldn't ever happen, but is included for completeness.\n            # If there is not an \"and\" before this element, try to remove the\n            # operator after it.\n            del elements[0]\n    return (not elements)", "output": "Remove the \"extra == ...\" operands from the list.\n\n    This is not a comprehensive implementation, but relies on an important\n    characteristic of metadata generation: The \"extra == ...\" operand is always\n    associated with an \"and\" operator. This means that we can simply remove the\n    operand and the \"and\" operator associated with it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forget(package_id):\n    '''\n    \n    '''\n    cmd = 'pkgutil --forget {0}'.format(package_id)\n    salt.utils.mac_utils.execute_return_success(cmd)\n    return not is_installed(package_id)", "output": ".. versionadded:: 2016.3.0\n\n    Remove the receipt data about the specified package. Does not remove files.\n\n    .. warning::\n        DO NOT use this command to fix broken package design\n\n    :param str package_id: The name of the package to forget\n\n    :return: True if successful, otherwise False\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkgutil.forget com.apple.pkg.gcc4.2Leo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copyNode(self, extended):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlCopyNode(self._o, extended)\n        if ret is None:raise treeError('xmlCopyNode() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Do a copy of the node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def post_message(message,\n                 channel=None,\n                 username=None,\n                 api_url=None,\n                 hook=None):\n    '''\n    \n    '''\n    if not api_url:\n        api_url = _get_api_url()\n\n    if not hook:\n        hook = _get_hook()\n\n    if not username:\n        username = _get_username()\n\n    if not channel:\n        channel = _get_channel()\n\n    if not message:\n        log.error('message is a required option.')\n\n    parameters = dict()\n    if channel:\n        parameters['channel'] = channel\n    if username:\n        parameters['username'] = username\n    parameters['text'] = '```' + message + '```'  # pre-formatted, fixed-width text\n    log.debug('Parameters: %s', parameters)\n    data = salt.utils.json.dumps(parameters)\n    result = salt.utils.mattermost.query(\n        api_url=api_url,\n        hook=hook,\n        data=str('payload={0}').format(data))  # future lint: blacklisted-function\n\n    if result:\n        return True\n    else:\n        return result", "output": "Send a message to a Mattermost channel.\n    :param channel:     The channel name, either will work.\n    :param username:    The username of the poster.\n    :param message:     The message to send to the Mattermost channel.\n    :param api_url:     The Mattermost api url, if not specified in the configuration.\n    :param hook:        The Mattermost hook, if not specified in the configuration.\n    :return:            Boolean if message was sent successfully.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run mattermost.post_message message='Build is done'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_connection_dropped(conn):  # Platform-specific\n    \"\"\"\n    \n    \"\"\"\n    sock = getattr(conn, 'sock', False)\n    if sock is False:  # Platform-specific: AppEngine\n        return False\n    if sock is None:  # Connection already closed (such as by httplib).\n        return True\n    try:\n        # Returns True if readable, which here means it's been dropped\n        return wait_for_read(sock, timeout=0.0)\n    except NoWayToWaitForSocketError:  # Platform-specific: AppEngine\n        return False", "output": "Returns True if the connection is dropped and should be closed.\n\n    :param conn:\n        :class:`httplib.HTTPConnection` object.\n\n    Note: For platforms like AppEngine, this will always return ``False`` to\n    let the platform handle connection recycling transparently for us.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_SU_save_option_commodity_min(\n        client=DATABASE,\n        ui_log=None,\n        ui_progress=None\n):\n    '''\n        \n    '''\n    # \u6d4b\u8bd5\u4e2d\u53d1\u73b0\uff0c \u4e00\u8d77\u56de\u53bb\uff0c\u5bb9\u6613\u51fa\u73b0\u9519\u8bef\uff0c\u6bcf\u6b21\u83b7\u53d6\u4e00\u4e2a\u54c1\u79cd\u540e \uff0c\u66f4\u6362\u670d\u52a1ip\u7ee7\u7eed\u83b7\u53d6 \uff1f\n\n    _save_option_commodity_cu_min(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n    _save_option_commodity_sr_min(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n    _save_option_commodity_m_min(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n\n    _save_option_commodity_ru_min(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n\n    _save_option_commodity_cf_min(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )\n\n    _save_option_commodity_c_min(\n        client=client,\n        ui_log=ui_log,\n        ui_progress=ui_progress\n    )", "output": ":param client:\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chconfig(cmd, *args, **kwargs):\n    '''\n    \n\n    '''\n    # Strip the __pub_ keys...is there a better way to do this?\n    for k in list(kwargs):\n        if k.startswith('__pub_'):\n            kwargs.pop(k)\n\n    # Catch password reset\n    if 'dracr.'+cmd not in __salt__:\n        ret = {'retcode': -1, 'message': 'dracr.' + cmd + ' is not available'}\n    else:\n        ret = __salt__['dracr.'+cmd](*args, **kwargs)\n\n    if cmd == 'change_password':\n        if 'username' in kwargs:\n            __opts__['proxy']['admin_username'] = kwargs['username']\n            DETAILS['admin_username'] = kwargs['username']\n        if 'password' in kwargs:\n            __opts__['proxy']['admin_password'] = kwargs['password']\n            DETAILS['admin_password'] = kwargs['password']\n\n    return ret", "output": "This function is called by the :mod:`salt.modules.chassis.cmd <salt.modules.chassis.cmd>`\n    shim.  It then calls whatever is passed in ``cmd``\n    inside the :mod:`salt.modules.dracr <salt.modules.dracr>`\n    module.\n\n    :param cmd: The command to call inside salt.modules.dracr\n    :param args: Arguments that need to be passed to that command\n    :param kwargs: Keyword arguments that need to be passed to that command\n    :return: Passthrough the return from the dracr module.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_name_and_version(p):\n    \"\"\"\n    \n    \"\"\"\n    m = NAME_VERSION_RE.match(p)\n    if not m:\n        raise DistlibException('Ill-formed name/version string: \\'%s\\'' % p)\n    d = m.groupdict()\n    return d['name'].strip().lower(), d['ver']", "output": "A utility method used to get name and version from a string.\n\n    From e.g. a Provides-Dist value.\n\n    :param p: A value in a form 'foo (1.0)'\n    :return: The name and version as a tuple.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def refill_queue(self):\n        \"\"\"\n        \n        \"\"\"\n        self.thread.pause()     # pause enqueue\n\n        opt = tfv1.RunOptions()\n        opt.timeout_in_ms = 2000   # 2s\n        sess = tfv1.get_default_session()\n        # dequeue until empty\n        try:\n            while True:\n                sess.run(self._dequeue_op, options=opt)\n        except tf.errors.DeadlineExceededError:\n            pass\n\n        # reset dataflow, start thread\n        self.thread.reinitialize_dataflow()\n        self.thread.resume()", "output": "Clear the queue, then call dataflow.__iter__() again and fill into the queue.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kill_dashboard(self, check_alive=True):\n        \"\"\"\n        \"\"\"\n        self._kill_process_type(\n            ray_constants.PROCESS_TYPE_DASHBOARD, check_alive=check_alive)", "output": "Kill the dashboard.\n\n        Args:\n            check_alive (bool): Raise an exception if the process was already\n                dead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_SU_save_index_list(engine, client=DATABASE):\n    \"\"\"\n    \"\"\"\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_index_list(client=client)", "output": "save index_list\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_record_set(self, record_set):\n        \"\"\"\n        \"\"\"\n        if not isinstance(record_set, ResourceRecordSet):\n            raise ValueError(\"Pass a ResourceRecordSet\")\n        self._additions += (record_set,)", "output": "Append a record set to the 'additions' for the change set.\n\n        :type record_set:\n            :class:`google.cloud.dns.resource_record_set.ResourceRecordSet`\n        :param record_set: the record set to append.\n\n        :raises: ``ValueError`` if ``record_set`` is not of the required type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _bin_op(name, doc=\"binary operator\"):\n    \"\"\" \n    \"\"\"\n    def _(self, other):\n        jc = other._jc if isinstance(other, Column) else other\n        njc = getattr(self._jc, name)(jc)\n        return Column(njc)\n    _.__doc__ = doc\n    return _", "output": "Create a method for given binary operator", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def import_data(args):\n    ''''''\n    validate_file(args.filename)\n    validate_dispatcher(args)\n    content = load_search_space(args.filename)\n    args.port = get_experiment_port(args)\n    if args.port is not None:\n        if import_data_to_restful_server(args, content):\n            pass\n        else:\n            print_error('Import data failed!')", "output": "import additional data to the experiment", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pop(queue, quantity=1, is_runner=False):\n    '''\n    \n    '''\n    cmd = 'SELECT name FROM {0}'.format(queue)\n    if quantity != 'all':\n        try:\n            quantity = int(quantity)\n        except ValueError as exc:\n            error_txt = ('Quantity must be an integer or \"all\".\\n'\n                         'Error: \"{0}\".'.format(exc))\n            raise SaltInvocationError(error_txt)\n        cmd = ''.join([cmd, ' LIMIT {0}'.format(quantity)])\n    log.debug('SQL Query: %s', cmd)\n    con = _conn(queue)\n    items = []\n    with con:\n        cur = con.cursor()\n        result = cur.execute(cmd).fetchall()\n        if result:\n            items = [item[0] for item in result]\n            itemlist = '\",\"'.join(items)\n            _quote_escape(itemlist)\n            del_cmd = '''DELETE FROM {0} WHERE name IN (\"{1}\")'''.format(\n                queue, itemlist)\n\n            log.debug('SQL Query: %s', del_cmd)\n\n            cur.execute(del_cmd)\n        con.commit()\n    if is_runner:\n        items = [salt.utils.json.loads(item[0].replace(\"'\", '\"')) for item in result]\n    log.info(items)\n    return items", "output": "Pop one or more or all items from the queue return them.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def draw_proposal_recall(img, proposals, proposal_scores, gt_boxes):\n    \"\"\"\n    \n    \"\"\"\n    box_ious = np_iou(gt_boxes, proposals)    # ng x np\n    box_ious_argsort = np.argsort(-box_ious, axis=1)\n    good_proposals_ind = box_ious_argsort[:, :3]   # for each gt, find 3 best proposals\n    good_proposals_ind = np.unique(good_proposals_ind.ravel())\n\n    proposals = proposals[good_proposals_ind, :]\n    tags = list(map(str, proposal_scores[good_proposals_ind]))\n    img = viz.draw_boxes(img, proposals, tags)\n    return img, good_proposals_ind", "output": "Draw top3 proposals for each gt.\n    Args:\n        proposals: NPx4\n        proposal_scores: NP\n        gt_boxes: NG", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _db_pre_transform(self, train_tfm:List[Callable], valid_tfm:List[Callable]):\n    \"\"\n    self.train_ds.x.after_open = compose(train_tfm)\n    self.valid_ds.x.after_open = compose(valid_tfm)\n    return self", "output": "Call `train_tfm` and `valid_tfm` after opening image, before converting from `PIL.Image`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _clear_context(bin_env=None):\n    '''\n    \n    '''\n    contextkey = 'pip.version'\n    if bin_env is not None:\n        contextkey = '{0}.{1}'.format(contextkey, bin_env)\n    __context__.pop(contextkey, None)", "output": "Remove the cached pip version", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_geometry(self, crect):\n        \"\"\"\n        \"\"\"\n        x0, y0, width, height = self.geometry()\n\n        if width is None:\n            width = crect.width()\n        if height is None:\n            height = crect.height()\n\n        # Calculate editor coordinates with their offsets\n        offset = self.editor.contentOffset()\n        x = self.editor.blockBoundingGeometry(self.editor.firstVisibleBlock())\\\n            .translated(offset.x(), offset.y()).left() \\\n            + self.editor.document().documentMargin() \\\n            + self.editor.panels.margin_size(Panel.Position.LEFT)\n        y = crect.top() + self.editor.panels.margin_size(Panel.Position.TOP)\n\n        self.setGeometry(QRect(x+x0, y+y0, width, height))", "output": "Set geometry for floating panels.\n\n        Normally you don't need to override this method, you should override\n        `geometry` instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def accept(self):\r\n        \"\"\"\"\"\"\r\n        if not self.runconfigoptions.is_valid():\r\n            return\r\n        configurations = _get_run_configurations()\r\n        configurations.insert(0, (self.filename, self.runconfigoptions.get()))\r\n        _set_run_configurations(configurations)\r\n        QDialog.accept(self)", "output": "Reimplement Qt method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_dlpack_for_write(data):\n    \"\"\"\n    \"\"\"\n    check_call(_LIB.MXNDArrayWaitToWrite(data.handle))\n    dlpack = DLPackHandle()\n    check_call(_LIB.MXNDArrayToDLPack(data.handle, ctypes.byref(dlpack)))\n    return ctypes.pythonapi.PyCapsule_New(dlpack, _c_str_dltensor, _c_dlpack_deleter)", "output": "Returns a reference view of NDArray that represents as DLManagedTensor until\n       all previous read/write operations on the current array are finished.\n\n    Parameters\n    ----------\n    data: NDArray\n        input data.\n\n    Returns\n    -------\n    PyCapsule (the pointer of DLManagedTensor)\n        a reference view of NDArray that represents as DLManagedTensor.\n\n    Examples\n    --------\n    >>> x = mx.nd.ones((2,3))\n    >>> w = mx.nd.to_dlpack_for_write(x)\n    >>> type(w)\n    <class 'PyCapsule'>\n    >>> u = mx.nd.from_dlpack(w)\n    >>> u += 1\n    >>> x\n    [[2. 2. 2.]\n     [2. 2. 2.]]\n    <NDArray 2x3 @cpu(0)>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sendcontrol(self, char):\n        '''\n        '''\n        n, byte = self.ptyproc.sendcontrol(char)\n        self._log_control(byte)\n        return n", "output": "Helper method that wraps send() with mnemonic access for sending control\n        character to the child (such as Ctrl-C or Ctrl-D).  For example, to send\n        Ctrl-G (ASCII 7, bell, '\\a')::\n\n            child.sendcontrol('g')\n\n        See also, sendintr() and sendeof().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_per_location_glob(tasks, outputs, regexes):\n    \"\"\"\n    \n    \"\"\"\n    paths = [o.path for o in outputs]\n    # naive, because some matches could be confused by numbers earlier\n    # in path, e.g. /foo/fifa2000k/bar/2000-12-31/00\n    matches = [r.search(p) for r, p in zip(regexes, paths)]\n\n    for m, p, t in zip(matches, paths, tasks):\n        if m is None:\n            raise NotImplementedError(\"Couldn't deduce datehour representation in output path %r of task %s\" % (p, t))\n\n    n_groups = len(matches[0].groups())\n    # the most common position of every group is likely\n    # to be conclusive hit or miss\n    positions = [most_common((m.start(i), m.end(i)) for m in matches)[0] for i in range(1, n_groups + 1)]\n\n    glob = list(paths[0])  # FIXME sanity check that it's the same for all paths\n    for start, end in positions:\n        glob = glob[:start] + ['[0-9]'] * (end - start) + glob[end:]\n    # chop off the last path item\n    # (wouldn't need to if `hadoop fs -ls -d` equivalent were available)\n    return ''.join(glob).rsplit('/', 1)[0]", "output": "Builds a glob listing existing output paths.\n\n    Esoteric reverse engineering, but worth it given that (compared to an\n    equivalent contiguousness guarantee by naive complete() checks)\n    requests to the filesystem are cut by orders of magnitude, and users\n    don't even have to retrofit existing tasks anyhow.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def logging_levels(name, remote=None, local=None):\n    '''\n    \n\n    '''\n\n    ret = _default_ret(name)\n\n    syslog_conf = __salt__['cimc.get_syslog_settings']()\n\n    req_change = False\n\n    try:\n        syslog_dict = syslog_conf['outConfigs']['commSyslog'][0]\n\n        if remote and syslog_dict['remoteSeverity'] != remote:\n            req_change = True\n        elif local and syslog_dict['localSeverity'] != local:\n            req_change = True\n\n        if req_change:\n\n            update = __salt__['cimc.set_logging_levels'](remote, local)\n\n            if update['outConfig']['commSyslog'][0]['status'] != 'modified':\n                ret['result'] = False\n                ret['comment'] = \"Error setting logging levels.\"\n                return ret\n\n            ret['changes']['before'] = syslog_conf\n            ret['changes']['after'] = __salt__['cimc.get_syslog_settings']()\n            ret['comment'] = \"Logging level settings modified.\"\n        else:\n            ret['comment'] = \"Logging level already configured. No changes required.\"\n\n    except Exception as err:\n        ret['result'] = False\n        ret['comment'] = \"Error occurred setting logging level settings.\"\n        log.error(err)\n        return ret\n\n    ret['result'] = True\n\n    return ret", "output": "Ensures that the logging levels are set on the device. The logging levels\n    must match the following options: emergency, alert, critical, error, warning,\n    notice, informational, debug.\n\n    .. versionadded:: 2019.2.0\n\n    name: The name of the module function to execute.\n\n    remote(str): The logging level for SYSLOG logs.\n\n    local(str): The logging level for the local device.\n\n    SLS Example:\n\n    .. code-block:: yaml\n\n        logging_levels:\n          cimc.logging_levels:\n            - remote: informational\n            - local: notice", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def leave_swarm(force=bool):\n    '''\n    \n    '''\n    salt_return = {}\n    __context__['client'].swarm.leave(force=force)\n    output = __context__['server_name'] + ' has left the swarm'\n    salt_return.update({'Comment': output})\n    return salt_return", "output": "Force the minion to leave the swarm\n\n    force\n        Will force the minion/worker/manager to leave the swarm\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' swarm.leave_swarm force=False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_readonly(self, readonly, field, value):\n        \"\"\"  \"\"\"\n        if readonly:\n            if not self._is_normalized:\n                self._error(field, errors.READONLY_FIELD)\n            # If the document was normalized (and therefore already been\n            # checked for readonly fields), we still have to return True\n            # if an error was filed.\n            has_error = errors.READONLY_FIELD in \\\n                self.document_error_tree.fetch_errors_from(\n                    self.document_path + (field,))\n            if self._is_normalized and has_error:\n                self._drop_remaining_rules()", "output": "{'type': 'boolean'}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_arguments(self):\n        \"\"\"\"\"\"\n        if self.argument_checked:\n            return\n\n        assert(self.symbol is not None)\n        self.argument_checked = True\n\n        # check if symbol contain duplicated names.\n        _check_arguments(self.symbol)\n        # rematch parameters to delete useless ones\n        if self.allow_extra_params:\n            if self.arg_params:\n                arg_names = set(self.symbol.list_arguments())\n                self.arg_params = {k : v for k, v in self.arg_params.items()\n                                   if k in arg_names}\n            if self.aux_params:\n                aux_names = set(self.symbol.list_auxiliary_states())\n                self.aux_params = {k : v for k, v in self.aux_params.items()\n                                   if k in aux_names}", "output": "verify the argument of the default symbol and user provided parameters", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_attr(self, **kwargs):\n        \"\"\"\n        \"\"\"\n        for key, value in kwargs.items():\n            if not isinstance(value, string_types):\n                raise ValueError(\"Set Attr only accepts string values\")\n            check_call(_LIB.MXSymbolSetAttr(\n                self.handle, c_str(key), c_str(str(value))))", "output": "Sets an attribute of the symbol.\n\n        For example. A._set_attr(foo=\"bar\") adds the mapping ``\"{foo: bar}\"``\n        to the symbol's attribute dictionary.\n\n        Parameters\n        ----------\n        **kwargs\n            The attributes to set", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_car_price(location, days, age, car_type):\n    \"\"\"\n    \n    \"\"\"\n\n    car_types = ['economy', 'standard', 'midsize', 'full size', 'minivan', 'luxury']\n    base_location_cost = 0\n    for i in range(len(location)):\n        base_location_cost += ord(location.lower()[i]) - 97\n\n    age_multiplier = 1.10 if age < 25 else 1\n    # Select economy is car_type is not found\n    if car_type not in car_types:\n        car_type = car_types[0]\n\n    return days * ((100 + base_location_cost) + ((car_types.index(car_type) * 50) * age_multiplier))", "output": "Generates a number within a reasonable range that might be expected for a flight.\n    The price is fixed for a given pair of locations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge_recursive(obj_a, obj_b, level=False):\n    '''\n    \n    '''\n    return aggregate(obj_a, obj_b, level,\n                     map_class=AggregatedMap,\n                     sequence_class=AggregatedSequence)", "output": "Merge obj_b into obj_a.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filetree(collector, *paths):\n    '''\n    \n    '''\n    _paths = []\n    # Unglob\n    for path in paths:\n        _paths += glob.glob(path)\n    for path in set(_paths):\n        if not path:\n            out.error('Path not defined', ident=2)\n        elif not os.path.exists(path):\n            out.warning('Path {} does not exists'.format(path))\n        else:\n            # The filehandler needs to be explicitly passed here, so PyLint needs to accept that.\n            # pylint: disable=W8470\n            if os.path.isfile(path):\n                filename = os.path.basename(path)\n                try:\n                    file_ref = salt.utils.files.fopen(path)  # pylint: disable=W\n                    out.put('Add {}'.format(filename), indent=2)\n                    collector.add(filename)\n                    collector.link(title=path, path=file_ref)\n                except Exception as err:\n                    out.error(err, ident=4)\n            # pylint: enable=W8470\n            else:\n                try:\n                    for fname in os.listdir(path):\n                        fname = os.path.join(path, fname)\n                        filetree(collector, [fname])\n                except Exception as err:\n                    out.error(err, ident=4)", "output": "Add all files in the tree. If the \"path\" is a file,\n    only that file will be added.\n\n    :param path: File or directory\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def traverse(self,\n                 window_length,\n                 offset=0,\n                 perspective_offset=0):\n        \"\"\"\n        \n        \"\"\"\n        data = self._data.copy()\n        _check_window_params(data, window_length)\n        return self._iterator_type(\n            data,\n            self._view_kwargs,\n            self.adjustments,\n            offset,\n            window_length,\n            perspective_offset,\n            rounding_places=None,\n        )", "output": "Produce an iterator rolling windows rows over our data.\n        Each emitted window will have `window_length` rows.\n\n        Parameters\n        ----------\n        window_length : int\n            The number of rows in each emitted window.\n        offset : int, optional\n            Number of rows to skip before the first window.  Default is 0.\n        perspective_offset : int, optional\n            Number of rows past the end of the current window from which to\n            \"view\" the underlying data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_entry_point(key, value):\n    \"\"\"\n    \"\"\"\n    for entry_point in pkg_resources.iter_entry_points(key):\n        if entry_point.name == value:\n            return entry_point.load()", "output": "Check if registered entry point is available for a given name and\n    load it. Otherwise, return None.\n\n    key (unicode): Entry point name.\n    value (unicode): Name of entry point to load.\n    RETURNS: The loaded entry point or None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_distance_names_to_functions(distance):\n    \"\"\"\n    \n    \"\"\"\n    dist_out = _copy.deepcopy(distance)\n\n    for i, d in enumerate(distance):\n        _, dist, _ = d\n        if isinstance(dist, str):\n            try:\n                dist_out[i][1] = _tc.distances.__dict__[dist]\n            except:\n                raise ValueError(\"Distance '{}' not recognized.\".format(dist))\n\n    return dist_out", "output": "Convert function names in a composite distance function into function\n    handles.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def data(self):\n        \"\"\"\"\"\"\n        content = {\n            'form_data': self.form_data,\n            'token': self.token,\n            'viz_name': self.viz_type,\n            'filter_select_enabled': self.datasource.filter_select_enabled,\n        }\n        return content", "output": "This is the data object serialized to the js layer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _remove_boring_lines(text):\n  \"\"\"\n  \"\"\"\n  lines = text.split(\"\\n\")\n  filtered = [line for line in lines if re.match(\"[a-zA-z\\\"\\']\", line)]\n  return \"\\n\".join(filtered)", "output": "Remove lines that do not start with a letter or a quote.\n\n  From inspecting the data, this seems to leave in most prose and remove\n  most weird stuff.\n\n  Args:\n    text: a string\n  Returns:\n    a string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate(self, references, buffers):\n        ''' \n\n        '''\n        references.update(self.model.references())\n        return { 'kind'  : 'RootAdded',\n                 'model' : self.model.ref }", "output": "Create a JSON representation of this event suitable for sending\n        to clients.\n\n        .. code-block:: python\n\n            {\n                'kind'  : 'RootAdded'\n                'title' : <reference to a Model>\n            }\n\n        Args:\n            references (dict[str, Model]) :\n                If the event requires references to certain models in order to\n                function, they may be collected here.\n\n                **This is an \"out\" parameter**. The values it contains will be\n                modified in-place.\n\n            buffers (set) :\n                If the event needs to supply any additional Bokeh protocol\n                buffers, they may be added to this set.\n\n                **This is an \"out\" parameter**. The values it contains will be\n                modified in-place.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_data(self, data_dir, tmp_dir, task_id=-1):\n    \"\"\"\n    \"\"\"\n    tf.logging.info(\"generate_data task_id=%s\" % task_id)\n    encoder = self.get_or_create_vocab(data_dir, tmp_dir)\n    assert task_id >= 0 and task_id < self.num_generate_tasks\n    if task_id < self.num_train_shards:\n      out_file = self.training_filepaths(\n          data_dir, self.num_train_shards, shuffled=False)[task_id]\n    else:\n      out_file = self.dev_filepaths(\n          data_dir, self.num_dev_shards,\n          shuffled=False)[task_id - self.num_train_shards]\n    generator_utils.generate_files(\n        self.example_generator(encoder, tmp_dir, task_id), [out_file])\n    generator_utils.shuffle_dataset([out_file])", "output": "Generates training/dev data.\n\n    Args:\n      data_dir: a string\n      tmp_dir: a string\n      task_id: an optional integer\n    Returns:\n      shard or shards for which data was generated.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flush_redis_unsafe(redis_client=None):\n    \"\"\"\n    \"\"\"\n    if redis_client is None:\n        ray.worker.global_worker.check_connected()\n        redis_client = ray.worker.global_worker.redis_client\n\n    # Delete the log files from the primary Redis shard.\n    keys = redis_client.keys(\"LOGFILE:*\")\n    if len(keys) > 0:\n        num_deleted = redis_client.delete(*keys)\n    else:\n        num_deleted = 0\n    print(\"Deleted {} log files from Redis.\".format(num_deleted))\n\n    # Delete the event log from the primary Redis shard.\n    keys = redis_client.keys(\"event_log:*\")\n    if len(keys) > 0:\n        num_deleted = redis_client.delete(*keys)\n    else:\n        num_deleted = 0\n    print(\"Deleted {} event logs from Redis.\".format(num_deleted))", "output": "This removes some non-critical state from the primary Redis shard.\n\n    This removes the log files as well as the event log from Redis. This can\n    be used to try to address out-of-memory errors caused by the accumulation\n    of metadata in Redis. However, it will only partially address the issue as\n    much of the data is in the task table (and object table), which are not\n    flushed.\n\n    Args:\n      redis_client: optional, if not provided then ray.init() must have been\n        called.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_activation(self, F, inputs, activation, **kwargs):\n        \"\"\"\"\"\"\n        func = {'tanh': F.tanh,\n                'relu': F.relu,\n                'sigmoid': F.sigmoid,\n                'softsign': F.softsign}.get(activation)\n        if func:\n            return func(inputs, **kwargs)\n        elif isinstance(activation, string_types):\n            return F.Activation(inputs, act_type=activation, **kwargs)\n        elif isinstance(activation, LeakyReLU):\n            return F.LeakyReLU(inputs, act_type='leaky', slope=activation._alpha, **kwargs)\n        return activation(inputs, **kwargs)", "output": "Get activation function. Convert if is string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def present(name):\n    '''\n    \n    '''\n    ret = {'name': name, 'result': True, 'comment': '', 'changes': {}}\n\n    vhost_exists = __salt__['rabbitmq.vhost_exists'](name)\n\n    if vhost_exists:\n        ret['comment'] = 'Virtual Host \\'{0}\\' already exists.'.format(name)\n        return ret\n\n    if not __opts__['test']:\n        result = __salt__['rabbitmq.add_vhost'](name)\n        if 'Error' in result:\n            ret['result'] = False\n            ret['comment'] = result['Error']\n            return ret\n        elif 'Added' in result:\n            ret['comment'] = result['Added']\n\n    # If we've reached this far before returning, we have changes.\n    ret['changes'] = {'old': '', 'new': name}\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Virtual Host \\'{0}\\' will be created.'.format(name)\n\n    return ret", "output": "Ensure the RabbitMQ VHost exists.\n\n    name\n        VHost name\n\n    user\n        Initial user permission to set on the VHost, if present\n\n        .. deprecated:: 2015.8.0\n    owner\n        Initial owner permission to set on the VHost, if present\n\n        .. deprecated:: 2015.8.0\n    conf\n        Initial conf string to apply to the VHost and user. Defaults to .*\n\n        .. deprecated:: 2015.8.0\n    write\n        Initial write permissions to apply to the VHost and user.\n        Defaults to .*\n\n        .. deprecated:: 2015.8.0\n    read\n        Initial read permissions to apply to the VHost and user.\n        Defaults to .*\n\n        .. deprecated:: 2015.8.0\n    runas\n        Name of the user to run the command\n\n        .. deprecated:: 2015.8.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_winexe_command(cmd, args, host, username, password, port=445):\n    '''\n    \n    '''\n    creds = \"-U '{0}%{1}' //{2}\".format(\n        username,\n        password,\n        host\n    )\n    logging_creds = \"-U '{0}%XXX-REDACTED-XXX' //{1}\".format(\n        username,\n        host\n    )\n    cmd = 'winexe {0} {1} {2}'.format(creds, cmd, args)\n    logging_cmd = 'winexe {0} {1} {2}'.format(logging_creds, cmd, args)\n    return win_cmd(cmd, logging_command=logging_cmd)", "output": "Run a command remotly via the winexe executable", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mlp(feature, hparams, name=\"mlp\"):\n  \"\"\"\"\"\"\n  with tf.variable_scope(name, \"mlp\", values=[feature]):\n    num_mlp_layers = hparams.num_mlp_layers\n    mlp_size = hparams.mlp_size\n    for _ in range(num_mlp_layers):\n      feature = common_layers.dense(feature, mlp_size, activation=None)\n      utils.collect_named_outputs(\"norms\", \"mlp_feature\",\n                                  tf.norm(feature, axis=-1))\n      feature = common_layers.layer_norm(feature)\n      feature = tf.nn.relu(feature)\n      feature = tf.nn.dropout(feature, keep_prob=1.-hparams.dropout)\n    return feature", "output": "Multi layer perceptron with dropout and relu activation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_wildcard_address(self, port):\n    \"\"\"\n    \"\"\"\n    fallback_address = '::' if socket.has_ipv6 else '0.0.0.0'\n    if hasattr(socket, 'AI_PASSIVE'):\n      try:\n        addrinfos = socket.getaddrinfo(None, port, socket.AF_UNSPEC,\n                                       socket.SOCK_STREAM, socket.IPPROTO_TCP,\n                                       socket.AI_PASSIVE)\n      except socket.gaierror as e:\n        logger.warn('Failed to auto-detect wildcard address, assuming %s: %s',\n                    fallback_address, str(e))\n        return fallback_address\n      addrs_by_family = defaultdict(list)\n      for family, _, _, _, sockaddr in addrinfos:\n        # Format of the \"sockaddr\" socket address varies by address family,\n        # but [0] is always the IP address portion.\n        addrs_by_family[family].append(sockaddr[0])\n      if hasattr(socket, 'AF_INET6') and addrs_by_family[socket.AF_INET6]:\n        return addrs_by_family[socket.AF_INET6][0]\n      if hasattr(socket, 'AF_INET') and addrs_by_family[socket.AF_INET]:\n        return addrs_by_family[socket.AF_INET][0]\n    logger.warn('Failed to auto-detect wildcard address, assuming %s',\n                fallback_address)\n    return fallback_address", "output": "Returns a wildcard address for the port in question.\n\n    This will attempt to follow the best practice of calling getaddrinfo() with\n    a null host and AI_PASSIVE to request a server-side socket wildcard address.\n    If that succeeds, this returns the first IPv6 address found, or if none,\n    then returns the first IPv4 address. If that fails, then this returns the\n    hardcoded address \"::\" if socket.has_ipv6 is True, else \"0.0.0.0\".", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_cloudformation(self, **kwargs):\n        \"\"\"\n        \"\"\"\n        function = kwargs.get('function')\n\n        if not function:\n            raise TypeError(\"Missing required keyword argument: function\")\n\n        return [self._construct_permission(function, source_arn=self.Topic),\n                self._inject_subscription(function, self.Topic, self.FilterPolicy)]", "output": "Returns the Lambda Permission resource allowing SNS to invoke the function this event source triggers.\n\n        :param dict kwargs: no existing resources need to be modified\n        :returns: a list of vanilla CloudFormation Resources, to which this SNS event expands\n        :rtype: list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tag2id(self, xs):\n        \"\"\"\n        \"\"\"\n        if isinstance(xs, list):\n            return [self._tag2id.get(x, self.UNK) for x in xs]\n        return self._tag2id.get(xs, self.UNK)", "output": "Map tag(s) to id(s)\n\n        Parameters\n        ----------\n        xs : str or list\n            tag or tags\n\n        Returns\n        -------\n        int or list\n            id(s) of tag(s)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_model(self, fname):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(fname, STRING_TYPES):  # assume file name\n            _check_call(_LIB.XGBoosterSaveModel(self.handle, c_str(fname)))\n        else:\n            raise TypeError(\"fname must be a string\")", "output": "Save the model to a file.\n\n        The model is saved in an XGBoost internal binary format which is\n        universal among the various XGBoost interfaces. Auxiliary attributes of\n        the Python Booster object (such as feature_names) will not be saved.\n        To preserve all attributes, pickle the Booster object.\n\n        Parameters\n        ----------\n        fname : string\n            Output file name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_config(self):\n        \"\"\"\n        \"\"\"\n        config = self._kwargs.copy()\n        config.update({\n            'metric': self.__class__.__name__,\n            'name': self.name,\n            'output_names': self.output_names,\n            'label_names': self.label_names})\n        return config", "output": "Save configurations of metric. Can be recreated\n        from configs with metric.create(``**config``)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_interface_child(device_name, interface_name, parent_name):\n    '''\n    \n    '''\n    nb_device = get_('dcim', 'devices', name=device_name)\n    nb_parent = get_('dcim', 'interfaces', device_id=nb_device['id'], name=parent_name)\n    if nb_device and nb_parent:\n        return update_interface(device_name, interface_name, lag=nb_parent['id'])\n    else:\n        return False", "output": ".. versionadded:: 2019.2.0\n\n    Set an interface as part of a LAG.\n\n    device_name\n        The name of the device, e.g., ``edge_router``.\n\n    interface_name\n        The name of the interface to be attached to LAG, e.g., ``xe-1/0/2``.\n\n    parent_name\n        The name of the LAG interface, e.g., ``ae13``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion netbox.make_interface_child xe-1/0/2 ae13", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def top_kth_iterative(x, k):\n  \"\"\"\n  \"\"\"\n  # The iterative computation is as follows:\n  #\n  # cur_x = x\n  # for _ in range(k):\n  #   top_x = maximum of elements of cur_x on the last axis\n  #   cur_x = cur_x where cur_x < top_x and 0 everywhere else (top elements)\n  #\n  # We encode this computation in a TF graph using tf.foldl, so the inner\n  # part of the above loop is called \"next_x\" and tf.foldl does the loop.\n  def next_x(cur_x, _):\n    top_x = tf.reduce_max(cur_x, axis=-1, keep_dims=True)\n    return cur_x * to_float(cur_x < top_x)\n  # We only do k-1 steps of the loop and compute the final max separately.\n  fin_x = tf.foldl(next_x, tf.range(k - 1), initializer=tf.stop_gradient(x),\n                   parallel_iterations=2, back_prop=False)\n  return tf.stop_gradient(tf.reduce_max(fin_x, axis=-1, keep_dims=True))", "output": "Compute the k-th top element of x on the last axis iteratively.\n\n  This assumes values in x are non-negative, rescale if needed.\n  It is often faster than tf.nn.top_k for small k, especially if k < 30.\n  Note: this does not support back-propagation, it stops gradients!\n\n  Args:\n    x: a Tensor of non-negative numbers of type float.\n    k: a python integer.\n\n  Returns:\n    a float tensor of the same shape as x but with 1 on the last axis\n    that contains the k-th largest number in x.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_pad(node, **kwargs):\n    \"\"\"\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    mxnet_pad_width = convert_string_to_list(attrs.get(\"pad_width\"))\n    onnx_pad_width = transform_padding(mxnet_pad_width)\n\n    pad_mode = attrs.get(\"mode\")\n\n    if pad_mode == \"constant\":\n        pad_value = float(attrs.get(\"constant_value\")) \\\n            if \"constant_value\" in attrs else 0.0\n        node = onnx.helper.make_node(\n            'Pad',\n            inputs=input_nodes,\n            outputs=[name],\n            mode='constant',\n            value=pad_value,\n            pads=onnx_pad_width,\n            name=name\n        )\n    else:\n        node = onnx.helper.make_node(\n            'Pad',\n            inputs=input_nodes,\n            outputs=[name],\n            mode=pad_mode,\n            pads=onnx_pad_width,\n            name=name\n        )\n\n    return [node]", "output": "Map MXNet's pad operator attributes to onnx's Pad operator\n    and return the created node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _print_cline(self, buf, i, icol):\n        \"\"\"\n        \n        \"\"\"\n        for cl in self.clinebuf:\n            if cl[0] == i:\n                buf.write('\\\\cline{{{cl:d}-{icol:d}}}\\n'\n                          .format(cl=cl[1], icol=icol))\n        # remove entries that have been written to buffer\n        self.clinebuf = [x for x in self.clinebuf if x[0] != i]", "output": "Print clines after multirow-blocks are finished", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_file_system(name,\n                       performance_mode='generalPurpose',\n                       keyid=None,\n                       key=None,\n                       profile=None,\n                       region=None,\n                       creation_token=None,\n                       **kwargs):\n    '''\n    \n    '''\n\n    if creation_token is None:\n        creation_token = name\n\n    tags = {\"Key\": \"Name\", \"Value\": name}\n\n    client = _get_conn(key=key, keyid=keyid, profile=profile, region=region)\n\n    response = client.create_file_system(CreationToken=creation_token,\n                                         PerformanceMode=performance_mode)\n\n    if 'FileSystemId' in response:\n        client.create_tags(FileSystemId=response['FileSystemId'], Tags=tags)\n\n    if 'Name' in response:\n        response['Name'] = name\n\n    return response", "output": "Creates a new, empty file system.\n\n    name\n        (string) - The name for the new file system\n\n    performance_mode\n        (string) - The PerformanceMode of the file system. Can be either\n        generalPurpose or maxIO\n\n    creation_token\n        (string) - A unique name to be used as reference when creating an EFS.\n        This will ensure idempotency. Set to name if not specified otherwise\n\n    returns\n        (dict) - A dict of the data for the elastic file system\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' boto_efs.create_file_system efs-name generalPurpose", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(code):\n    \"\"\"\n    \"\"\"\n    try:\n        ast_tree = ast.parse(code)\n    except Exception:\n        raise RuntimeError('Bad Python code')\n\n    transformer = Transformer()\n    try:\n        transformer.visit(ast_tree)\n    except AssertionError as exc:\n        raise RuntimeError('%d: %s' % (ast_tree.last_line, exc.args[0]))\n\n    if not transformer.annotated:\n        return None\n\n    last_future_import = -1\n    import_nni = ast.Import(names=[ast.alias(name='nni', asname=None)])\n    nodes = ast_tree.body\n    for i, _ in enumerate(nodes):\n        if type(nodes[i]) is ast.ImportFrom and nodes[i].module == '__future__':\n            last_future_import = i\n    nodes.insert(last_future_import + 1, import_nni)\n\n    return astor.to_source(ast_tree)", "output": "Annotate user code.\n    Return annotated code (str) if annotation detected; return None if not.\n    code: original user code (str)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __args_check(self, valid, args=None, kwargs=None):\n        '''\n        \n        '''\n        if not isinstance(valid, list):\n            valid = [valid]\n        for cond in valid:\n            if not isinstance(cond, dict):\n                # Invalid argument\n                continue\n            # whitelist args, kwargs\n            cond_args = cond.get('args', [])\n            good = True\n            for i, cond_arg in enumerate(cond_args):\n                if args is None or len(args) <= i:\n                    good = False\n                    break\n                if cond_arg is None:  # None == '.*' i.e. allow any\n                    continue\n                if not self.match_check(cond_arg, six.text_type(args[i])):\n                    good = False\n                    break\n            if not good:\n                continue\n            # Check kwargs\n            cond_kwargs = cond.get('kwargs', {})\n            for k, v in six.iteritems(cond_kwargs):\n                if kwargs is None or k not in kwargs:\n                    good = False\n                    break\n                if v is None:  # None == '.*' i.e. allow any\n                    continue\n                if not self.match_check(v, six.text_type(kwargs[k])):\n                    good = False\n                    break\n            if good:\n                return True\n        return False", "output": "valid is a dicts: {'args': [...], 'kwargs': {...}} or a list of such dicts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clone(self, *, method: str=sentinel, rel_url: StrOrURL=sentinel,\n              headers: LooseHeaders=sentinel, scheme: str=sentinel,\n              host: str=sentinel,\n              remote: str=sentinel) -> 'BaseRequest':\n        \"\"\"\n\n        \"\"\"\n\n        if self._read_bytes:\n            raise RuntimeError(\"Cannot clone request \"\n                               \"after reading its content\")\n\n        dct = {}  # type: Dict[str, Any]\n        if method is not sentinel:\n            dct['method'] = method\n        if rel_url is not sentinel:\n            new_url = URL(rel_url)\n            dct['url'] = new_url\n            dct['path'] = str(new_url)\n        if headers is not sentinel:\n            # a copy semantic\n            dct['headers'] = CIMultiDictProxy(CIMultiDict(headers))\n            dct['raw_headers'] = tuple((k.encode('utf-8'), v.encode('utf-8'))\n                                       for k, v in headers.items())\n\n        message = self._message._replace(**dct)\n\n        kwargs = {}\n        if scheme is not sentinel:\n            kwargs['scheme'] = scheme\n        if host is not sentinel:\n            kwargs['host'] = host\n        if remote is not sentinel:\n            kwargs['remote'] = remote\n\n        return self.__class__(\n            message,\n            self._payload,\n            self._protocol,\n            self._payload_writer,\n            self._task,\n            self._loop,\n            client_max_size=self._client_max_size,\n            state=self._state.copy(),\n            **kwargs)", "output": "Clone itself with replacement some attributes.\n\n        Creates and returns a new instance of Request object. If no parameters\n        are given, an exact copy is returned. If a parameter is not passed, it\n        will reuse the one from the current request object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_request(self):\n        ''''''\n        # check _postpone_request first\n        todo = []\n        for task in self._postpone_request:\n            if task['project'] not in self.projects:\n                continue\n            if self.projects[task['project']].task_queue.is_processing(task['taskid']):\n                todo.append(task)\n            else:\n                self.on_request(task)\n        self._postpone_request = todo\n\n        tasks = {}\n        while len(tasks) < self.LOOP_LIMIT:\n            try:\n                task = self.newtask_queue.get_nowait()\n            except Queue.Empty:\n                break\n\n            if isinstance(task, list):\n                _tasks = task\n            else:\n                _tasks = (task, )\n\n            for task in _tasks:\n                if not self.task_verify(task):\n                    continue\n\n                if task['taskid'] in self.projects[task['project']].task_queue:\n                    if not task.get('schedule', {}).get('force_update', False):\n                        logger.debug('ignore newtask %(project)s:%(taskid)s %(url)s', task)\n                        continue\n\n                if task['taskid'] in tasks:\n                    if not task.get('schedule', {}).get('force_update', False):\n                        continue\n\n                tasks[task['taskid']] = task\n\n        for task in itervalues(tasks):\n            self.on_request(task)\n\n        return len(tasks)", "output": "Check new task queue", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _view_use_legacy_sql_getter(table):\n    \"\"\"\n    \"\"\"\n    view = table._properties.get(\"view\")\n    if view is not None:\n        # The server-side default for useLegacySql is True.\n        return view.get(\"useLegacySql\", True)\n    # In some cases, such as in a table list no view object is present, but the\n    # resource still represents a view. Use the type as a fallback.\n    if table.table_type == \"VIEW\":\n        # The server-side default for useLegacySql is True.\n        return True", "output": "bool: Specifies whether to execute the view with Legacy or Standard SQL.\n\n    This boolean specifies whether to execute the view with Legacy SQL\n    (:data:`True`) or Standard SQL (:data:`False`). The client side default is\n    :data:`False`. The server-side default is :data:`True`. If this table is\n    not a view, :data:`None` is returned.\n\n    Raises:\n        ValueError: For invalid value types.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        \"\"\"\n        \"\"\"\n        self.collector.start()\n        if self.standalone:\n            self.collector.join()", "output": "Start the collector worker thread.\n\n        If running in standalone mode, the current thread will wait\n        until the collector thread ends.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mount(name, device, mkmnt=False, fstype='', opts='defaults', user=None, util='mount'):\n    '''\n    \n    '''\n    if util != 'mount':\n        # This functionality used to live in img.mount_image\n        if util == 'guestfs':\n            return __salt__['guestfs.mount'](name, root=device)\n        elif util == 'qemu_nbd':\n            mnt = __salt__['qemu_nbd.init'](name, device)\n            if not mnt:\n                return False\n            first = next(six.iterkeys(mnt))\n            __context__['img.mnt_{0}'.format(first)] = mnt\n            return first\n        return False\n\n    # Darwin doesn't expect defaults when mounting without other options\n    if 'defaults' in opts and __grains__['os'] in ['MacOS', 'Darwin', 'AIX']:\n        opts = None\n\n    if isinstance(opts, six.string_types):\n        opts = opts.split(',')\n\n    if not os.path.exists(name) and mkmnt:\n        __salt__['file.mkdir'](name, user=user)\n\n    args = ''\n    if opts is not None:\n        lopts = ','.join(opts)\n        args = '-o {0}'.format(lopts)\n\n    if fstype:\n        # use of fstype on AIX differs from typical Linux use of -t\n        # functionality AIX uses -v vfsname, -t fstype mounts all with\n        # fstype in /etc/filesystems\n        if 'AIX' in __grains__['os']:\n            args += ' -v {0}'.format(fstype)\n        elif 'solaris' in __grains__['os'].lower():\n            args += ' -F {0}'.format(fstype)\n        else:\n            args += ' -t {0}'.format(fstype)\n\n    cmd = 'mount {0} {1} {2} '.format(args, device, name)\n    out = __salt__['cmd.run_all'](cmd, runas=user, python_shell=False)\n    if out['retcode']:\n        return out['stderr']\n    return True", "output": "Mount a device\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mount.mount /mnt/foo /dev/sdz1 True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_window_size(self, width, height, windowHandle='current'):\n        \"\"\"\n        \n        \"\"\"\n        if self.w3c:\n            if windowHandle != 'current':\n                warnings.warn(\"Only 'current' window is supported for W3C compatibile browsers.\")\n            self.set_window_rect(width=int(width), height=int(height))\n        else:\n            self.execute(Command.SET_WINDOW_SIZE, {\n                'width': int(width),\n                'height': int(height),\n                'windowHandle': windowHandle})", "output": "Sets the width and height of the current window. (window.resizeTo)\n\n        :Args:\n         - width: the width in pixels to set the window to\n         - height: the height in pixels to set the window to\n\n        :Usage:\n            ::\n\n                driver.set_window_size(800,600)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_missing_perms(self):\n        \"\"\"\"\"\"\n        from superset import db\n        from superset.models import core as models\n\n        logging.info(\n            'Fetching a set of all perms to lookup which ones are missing')\n        all_pvs = set()\n        for pv in self.get_session.query(self.permissionview_model).all():\n            if pv.permission and pv.view_menu:\n                all_pvs.add((pv.permission.name, pv.view_menu.name))\n\n        def merge_pv(view_menu, perm):\n            \"\"\"Create permission view menu only if it doesn't exist\"\"\"\n            if view_menu and perm and (view_menu, perm) not in all_pvs:\n                self.merge_perm(view_menu, perm)\n\n        logging.info('Creating missing datasource permissions.')\n        datasources = ConnectorRegistry.get_all_datasources(db.session)\n        for datasource in datasources:\n            merge_pv('datasource_access', datasource.get_perm())\n            merge_pv('schema_access', datasource.schema_perm)\n\n        logging.info('Creating missing database permissions.')\n        databases = db.session.query(models.Database).all()\n        for database in databases:\n            merge_pv('database_access', database.perm)\n\n        logging.info('Creating missing metrics permissions')\n        metrics = []\n        for datasource_class in ConnectorRegistry.sources.values():\n            metrics += list(db.session.query(datasource_class.metric_class).all())\n\n        for metric in metrics:\n            if metric.is_restricted:\n                merge_pv('metric_access', metric.perm)", "output": "Creates missing perms for datasources, schemas and metrics", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def previous_week_day(base_date, weekday):\n    \"\"\"\n    \n    \"\"\"\n    day = base_date - timedelta(days=1)\n    while day.weekday() != weekday:\n        day = day - timedelta(days=1)\n    return day", "output": "Finds previous weekday", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def date_from_duration(base_date, number_as_string, unit, duration, base_time=None):\n    \"\"\"\n    \n    \"\"\"\n    # Check if query is `2 days before yesterday` or `day before yesterday`\n    if base_time is not None:\n        base_date = date_from_adverb(base_date, base_time)\n    num = convert_string_to_number(number_as_string)\n    if unit in day_variations:\n        args = {'days': num}\n    elif unit in minute_variations:\n        args = {'minutes': num}\n    elif unit in week_variations:\n        args = {'weeks': num}\n    elif unit in month_variations:\n        args = {'days': 365 * num / 12}\n    elif unit in year_variations:\n        args = {'years': num}\n    if duration == 'ago' or duration == 'before' or duration == 'earlier':\n        if 'years' in args:\n            return datetime(base_date.year - args['years'], base_date.month, base_date.day)\n        return base_date - timedelta(**args)\n    elif duration == 'after' or duration == 'later' or duration == 'from now':\n        if 'years' in args:\n            return datetime(base_date.year + args['years'], base_date.month, base_date.day)\n        return base_date + timedelta(**args)", "output": "Find dates from duration\n    Eg: 20 days from now\n    Currently does not support strings like \"20 days from last monday\".", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_model(itos_filename, classifier_filename, num_classes):\n    \"\"\"\n    \"\"\"\n\n    # load the int to string mapping file\n    itos = pickle.load(Path(itos_filename).open('rb'))\n    # turn it into a string to int mapping (which is what we need)\n    stoi = collections.defaultdict(lambda:0, {str(v):int(k) for k,v in enumerate(itos)})\n\n    # these parameters aren't used, but this is the easiest way to get a model\n    bptt,em_sz,nh,nl = 70,400,1150,3\n    dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.5\n    vs = len(itos)\n\n    model = get_rnn_classifer(bptt, 20*70, num_classes, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n            layers=[em_sz*3, 50, num_classes], drops=[dps[4], 0.1],\n            dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n\n    # load the trained classifier\n    model.load_state_dict(torch.load(classifier_filename, map_location=lambda storage, loc: storage))\n\n    # put the classifier into evaluation mode\n    model.reset()\n    model.eval()\n\n    return stoi, model", "output": "Load the classifier and int to string mapping\n\n    Args:\n        itos_filename (str): The filename of the int to string mapping file (usually called itos.pkl)\n        classifier_filename (str): The filename of the trained classifier\n\n    Returns:\n        string to int mapping, trained classifer model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _fetch_secret(pass_path):\n    '''\n    \n    '''\n    cmd = \"pass show {0}\".format(pass_path.strip())\n    log.debug('Fetching secret: %s', cmd)\n\n    proc = Popen(cmd.split(' '), stdout=PIPE, stderr=PIPE)\n    pass_data, pass_error = proc.communicate()\n\n    # The version of pass used during development sent output to\n    # stdout instead of stderr even though its returncode was non zero.\n    if proc.returncode or not pass_data:\n        log.warning('Could not fetch secret: %s %s', pass_data, pass_error)\n        pass_data = pass_path\n    return pass_data.strip()", "output": "Fetch secret from pass based on pass_path. If there is\n    any error, return back the original pass_path value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_SU_save_etf_day(engine, client=DATABASE):\n    \"\"\"\n    \"\"\"\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_etf_day(client=client)", "output": "save etf_day\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_user_name(uid, name, **kwargs):\n    '''\n    \n    '''\n    with _IpmiCommand(**kwargs) as s:\n        return s.set_user_name(uid, name)", "output": "Set user name\n\n    :param uid: user number [1:16]\n    :param name: username (limit of 16bytes)\n    :param kwargs:\n        - api_host=127.0.0.1\n        - api_user=admin\n        - api_pass=example\n        - api_port=623\n        - api_kg=None\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-call ipmi.set_user_name uid=2 name='steverweber'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def awaitAnyTermination(self, timeout=None):\n        \"\"\"\n        \"\"\"\n        if timeout is not None:\n            if not isinstance(timeout, (int, float)) or timeout < 0:\n                raise ValueError(\"timeout must be a positive integer or float. Got %s\" % timeout)\n            return self._jsqm.awaitAnyTermination(int(timeout * 1000))\n        else:\n            return self._jsqm.awaitAnyTermination()", "output": "Wait until any of the queries on the associated SQLContext has terminated since the\n        creation of the context, or since :func:`resetTerminated()` was called. If any query was\n        terminated with an exception, then the exception will be thrown.\n        If `timeout` is set, it returns whether the query has terminated or not within the\n        `timeout` seconds.\n\n        If a query has terminated, then subsequent calls to :func:`awaitAnyTermination()` will\n        either return immediately (if the query was terminated by :func:`query.stop()`),\n        or throw the exception immediately (if the query was terminated with exception). Use\n        :func:`resetTerminated()` to clear past terminations and wait for new terminations.\n\n        In the case where multiple queries have terminated since :func:`resetTermination()`\n        was called, if any query has terminated with exception, then :func:`awaitAnyTermination()`\n        will throw any of the exception. For correctly documenting exceptions across multiple\n        queries, users need to stop all of them after any of them terminates with exception, and\n        then check the `query.exception()` for each query.\n\n        throws :class:`StreamingQueryException`, if `this` query has terminated with an exception", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def backward(self, out_grads=None):\n        \"\"\"\n        \"\"\"\n        super(SVRGModule, self).backward(out_grads)\n\n        if self._mod_aux.binded:\n            self._mod_aux.backward(out_grads)", "output": "Backward computation.\n\n        See Also\n        ----------\n        :meth:`BaseModule.backward`.\n\n        Parameters\n        ----------\n        out_grads : NDArray or list of NDArray, optional\n            Gradient on the outputs to be propagated back.\n            This parameter is only needed when bind is called\n            on outputs that are not a loss function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _nullpager(stream, generator, color):\n    \"\"\"\"\"\"\n    for text in generator:\n        if not color:\n            text = strip_ansi(text)\n        stream.write(text)", "output": "Simply print unformatted text.  This is the ultimate fallback.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _use_gl(objs):\n    ''' \n\n    '''\n    from ..models.plots import Plot\n    return _any(objs, lambda obj: isinstance(obj, Plot) and obj.output_backend == \"webgl\")", "output": "Whether a collection of Bokeh objects contains a plot requesting WebGL\n\n    Args:\n        objs (seq[Model or Document]) :\n\n    Returns:\n        bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(self, document_data):\n        \"\"\"\n        \"\"\"\n        batch = self._client.batch()\n        batch.create(self, document_data)\n        write_results = batch.commit()\n        return _first_write_result(write_results)", "output": "Create the current document in the Firestore database.\n\n        Args:\n            document_data (dict): Property names and values to use for\n                creating a document.\n\n        Returns:\n            google.cloud.firestore_v1beta1.types.WriteResult: The\n            write result corresponding to the committed document. A write\n            result contains an ``update_time`` field.\n\n        Raises:\n            ~google.cloud.exceptions.Conflict: If the document already exists.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def preprocess_gold(self, docs_golds):\n        \"\"\"\n        \"\"\"\n        for name, proc in self.pipeline:\n            if hasattr(proc, \"preprocess_gold\"):\n                docs_golds = proc.preprocess_gold(docs_golds)\n        for doc, gold in docs_golds:\n            yield doc, gold", "output": "Can be called before training to pre-process gold data. By default,\n        it handles nonprojectivity and adds missing tags to the tag map.\n\n        docs_golds (iterable): Tuples of `Doc` and `GoldParse` objects.\n        YIELDS (tuple): Tuples of preprocessed `Doc` and `GoldParse` objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def file_dict(*packages, **kwargs):\n    '''\n    \n    '''\n    errors = []\n    ret = {}\n    pkgs = {}\n    cmd = 'dpkg -l {0}'.format(' '.join(packages))\n    out = __salt__['cmd.run_all'](cmd, python_shell=False)\n    if out['retcode'] != 0:\n        msg = 'Error:  ' + out['stderr']\n        log.error(msg)\n        return msg\n    out = out['stdout']\n\n    for line in out.splitlines():\n        if line.startswith('ii '):\n            comps = line.split()\n            pkgs[comps[1]] = {'version': comps[2],\n                              'description': ' '.join(comps[3:])}\n        if 'No packages found' in line:\n            errors.append(line)\n    for pkg in pkgs:\n        files = []\n        cmd = 'dpkg -L {0}'.format(pkg)\n        for line in __salt__['cmd.run'](cmd, python_shell=False).splitlines():\n            files.append(line)\n        ret[pkg] = files\n    return {'errors': errors, 'packages': ret}", "output": "List the files that belong to a package, grouped by package. Not\n    specifying any packages will return a list of _every_ file on the system's\n    package database (not generally recommended).\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' lowpkg.file_list httpd\n        salt '*' lowpkg.file_list httpd postfix\n        salt '*' lowpkg.file_list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_settings(self):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        settings = {}\r\n        for name in REMOTE_SETTINGS:\r\n            settings[name] = self.get_option(name)\r\n\r\n        # dataframe_format is stored without percent sign in config\r\n        # to avoid interference with ConfigParser's interpolation\r\n        name = 'dataframe_format'\r\n        settings[name] = '%{0}'.format(self.get_option(name))\r\n        return settings", "output": "Retrieve all Variable Explorer configuration settings.\r\n        \r\n        Specifically, return the settings in CONF_SECTION with keys in \r\n        REMOTE_SETTINGS, and the setting 'dataframe_format'.\r\n        \r\n        Returns:\r\n            dict: settings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def train_agent(real_env, learner, world_model_dir, hparams, epoch):\n  \"\"\"\"\"\"\n  initial_frame_chooser = rl_utils.make_initial_frame_chooser(\n      real_env, hparams.frame_stack_size, hparams.simulation_random_starts,\n      hparams.simulation_flip_first_random_for_beginning\n  )\n  env_fn = rl.make_simulated_env_fn_from_hparams(\n      real_env, hparams, batch_size=hparams.simulated_batch_size,\n      initial_frame_chooser=initial_frame_chooser, model_dir=world_model_dir,\n      sim_video_dir=os.path.join(\n          learner.agent_model_dir, \"sim_videos_{}\".format(epoch)\n      )\n  )\n  base_algo_str = hparams.base_algo\n  train_hparams = trainer_lib.create_hparams(hparams.base_algo_params)\n  if hparams.wm_policy_param_sharing:\n    train_hparams.optimizer_zero_grads = True\n\n  rl_utils.update_hparams_from_hparams(\n      train_hparams, hparams, base_algo_str + \"_\"\n  )\n\n  final_epoch = hparams.epochs - 1\n  is_special_epoch = (epoch + 3) == final_epoch or (epoch + 7) == final_epoch\n  is_final_epoch = epoch == final_epoch\n  env_step_multiplier = 3 if is_final_epoch else 2 if is_special_epoch else 1\n  learner.train(\n      env_fn, train_hparams, simulated=True, save_continuously=True,\n      epoch=epoch, env_step_multiplier=env_step_multiplier\n  )", "output": "Train the PPO agent in the simulated environment.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def readline(self, size=-1):\n        \"\"\"\n        \"\"\"\n        if self.closed:\n            raise ValueError(\"I/O operation on closed file\")\n\n        pos = self.buffer.find(b\"\\n\") + 1\n        if pos == 0:\n            # no newline found.\n            while True:\n                buf = self.fileobj.read(self.blocksize)\n                self.buffer += buf\n                if not buf or b\"\\n\" in buf:\n                    pos = self.buffer.find(b\"\\n\") + 1\n                    if pos == 0:\n                        # no newline found.\n                        pos = len(self.buffer)\n                    break\n\n        if size != -1:\n            pos = min(size, pos)\n\n        buf = self.buffer[:pos]\n        self.buffer = self.buffer[pos:]\n        self.position += len(buf)\n        return buf", "output": "Read one entire line from the file. If size is present\n           and non-negative, return a string with at most that\n           size, which may be an incomplete line.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def password_present(name, password):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'result': True,\n           'changes': {'old': 'unknown',\n                       'new': '********'},\n           'comment': 'Host password was updated.'}\n    esxi_cmd = 'esxi.cmd'\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Host password will change.'\n        return ret\n    else:\n        try:\n            __salt__[esxi_cmd]('update_host_password',\n                               new_password=password)\n        except CommandExecutionError as err:\n            ret['result'] = False\n            ret['comment'] = 'Error: {0}'.format(err)\n            return ret\n\n    return ret", "output": "Ensures the given password is set on the ESXi host. Passwords cannot be obtained from\n    host, so if a password is set in this state, the ``vsphere.update_host_password``\n    function will always run (except when using test=True functionality) and the state's\n    changes dictionary will always be populated.\n\n    The username for which the password will change is the same username that is used to\n    authenticate against the ESXi host via the Proxy Minion. For example, if the pillar\n    definition for the proxy username is defined as ``root``, then the username that the\n    password will be updated for via this state is ``root``.\n\n    name\n        Name of the state.\n\n    password\n        The new password to change on the host.\n\n    Example:\n\n    .. code-block:: yaml\n\n        configure-host-password:\n          esxi.password_present:\n            - password: 'new-bad-password'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def action(func=None,\n           cloudmap=None,\n           instances=None,\n           provider=None,\n           instance=None,\n           opts=None,\n           **kwargs):\n    '''\n    \n    '''\n    info = {}\n    client = _get_client()\n    if isinstance(opts, dict):\n        client.opts.update(opts)\n    try:\n        info = client.action(\n            func,\n            cloudmap,\n            instances,\n            provider,\n            instance,\n            salt.utils.args.clean_kwargs(**kwargs)\n        )\n    except SaltCloudConfigError as err:\n        log.error(err)\n    return info", "output": "Execute a single action on the given map/provider/instance\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run cloud.action start my-salt-vm", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathNewNodeSet(self):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlXPathNewNodeSet(self._o)\n        if ret is None:raise xpathError('xmlXPathNewNodeSet() failed')\n        return xpathObjectRet(ret)", "output": "Create a new xmlXPathObjectPtr of type NodeSet and\n           initialize it with the single Node @val", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def auth_traps_enabled(name, status=True):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'comment': six.text_type(),\n           'result': None}\n\n    vname = 'EnableAuthenticationTraps'\n    current_status = __salt__['win_snmp.get_auth_traps_enabled']()\n\n    if status == current_status:\n        ret['comment'] = '{0} already contains the provided value.'.format(vname)\n        ret['result'] = True\n    elif __opts__['test']:\n        ret['comment'] = '{0} will be changed.'.format(vname)\n        ret['changes'] = {'old': current_status,\n                          'new': status}\n    else:\n        ret['comment'] = 'Set {0} to contain the provided value.'.format(vname)\n        ret['changes'] = {'old': current_status,\n                          'new': status}\n        ret['result'] = __salt__['win_snmp.set_auth_traps_enabled'](status=status)\n\n    return ret", "output": "Manage the sending of authentication traps.\n\n    :param bool status: The enabled status.\n\n    Example of usage:\n\n    .. code-block:: yaml\n\n        snmp-auth-traps:\n            win_snmp.auth_traps_enabled:\n                - status: True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self):\n        \"\"\"\n        \"\"\"\n        if self.num_inst == 0:\n            return (self.name, float('nan'))\n        else:\n            return (self.name, self.sum_metric / self.num_inst)", "output": "Gets the current evaluation result.\n\n        Returns\n        -------\n        names : list of str\n           Name of the metrics.\n        values : list of float\n           Value of the evaluations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_existing_configs(self, default):\n        '''\n        \n        '''\n        configs = []\n        for cfg in [default, self._config_filename_, 'minion', 'proxy', 'cloud', 'spm']:\n            if not cfg:\n                continue\n            config_path = self.get_config_file_path(cfg)\n            if os.path.exists(config_path):\n                configs.append(cfg)\n\n        if default and default not in configs:\n            raise SystemExit('Unknown configuration unit: {}'.format(default))\n\n        return configs", "output": "Find configuration files on the system.\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def buy_open_order_quantity(self):\n        \"\"\"\n        \n        \"\"\"\n        return sum(order.unfilled_quantity for order in self.open_orders if\n                   order.side == SIDE.BUY and order.position_effect == POSITION_EFFECT.OPEN)", "output": "[int] \u4e70\u65b9\u5411\u6302\u5355\u91cf", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_benchmark_returns(symbol):\n    \"\"\"\n    \n    \"\"\"\n    r = requests.get(\n        'https://api.iextrading.com/1.0/stock/{}/chart/5y'.format(symbol)\n    )\n    data = r.json()\n\n    df = pd.DataFrame(data)\n\n    df.index = pd.DatetimeIndex(df['date'])\n    df = df['close']\n\n    return df.sort_index().tz_localize('UTC').pct_change(1).iloc[1:]", "output": "Get a Series of benchmark returns from IEX associated with `symbol`.\n    Default is `SPY`.\n\n    Parameters\n    ----------\n    symbol : str\n        Benchmark symbol for which we're getting the returns.\n\n    The data is provided by IEX (https://iextrading.com/), and we can\n    get up to 5 years worth of data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_internal_attribute(obj, attr):\n    \"\"\"\n    \"\"\"\n    if isinstance(obj, types.FunctionType):\n        if attr in UNSAFE_FUNCTION_ATTRIBUTES:\n            return True\n    elif isinstance(obj, types.MethodType):\n        if attr in UNSAFE_FUNCTION_ATTRIBUTES or \\\n           attr in UNSAFE_METHOD_ATTRIBUTES:\n            return True\n    elif isinstance(obj, type):\n        if attr == 'mro':\n            return True\n    elif isinstance(obj, (types.CodeType, types.TracebackType, types.FrameType)):\n        return True\n    elif isinstance(obj, types.GeneratorType):\n        if attr in UNSAFE_GENERATOR_ATTRIBUTES:\n            return True\n    elif hasattr(types, 'CoroutineType') and isinstance(obj, types.CoroutineType):\n        if attr in UNSAFE_COROUTINE_ATTRIBUTES:\n            return True\n    elif hasattr(types, 'AsyncGeneratorType') and isinstance(obj, types.AsyncGeneratorType):\n        if attr in UNSAFE_ASYNC_GENERATOR_ATTRIBUTES:\n            return True\n    return attr.startswith('__')", "output": "Test if the attribute given is an internal python attribute.  For\n    example this function returns `True` for the `func_code` attribute of\n    python objects.  This is useful if the environment method\n    :meth:`~SandboxedEnvironment.is_safe_attribute` is overridden.\n\n    >>> from jinja2.sandbox import is_internal_attribute\n    >>> is_internal_attribute(str, \"mro\")\n    True\n    >>> is_internal_attribute(str, \"upper\")\n    False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_as_defaults(self):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        self.defaults = []\r\n        for section in self.sections():\r\n            secdict = {}\r\n            for option, value in self.items(section, raw=self.raw):\r\n                secdict[option] = value\r\n            self.defaults.append( (section, secdict) )", "output": "Set defaults from the current config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mock_decorator_with_params(*oargs, **okwargs):  # pylint: disable=unused-argument\n    '''\n    \n    '''\n    def inner(fn, *iargs, **ikwargs):  # pylint: disable=unused-argument\n        if hasattr(fn, '__call__'):\n            return fn\n        return Mock()\n    return inner", "output": "Optionally mock a decorator that takes parameters\n\n    E.g.:\n\n    @blah(stuff=True)\n    def things():\n        pass", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def WideResnetBlock(channels, strides=(1, 1), channel_mismatch=False):\n  \"\"\"\"\"\"\n  main = layers.Serial(layers.BatchNorm(), layers.Relu(),\n                       layers.Conv(channels, (3, 3), strides, padding='SAME'),\n                       layers.BatchNorm(), layers.Relu(),\n                       layers.Conv(channels, (3, 3), padding='SAME'))\n  shortcut = layers.Identity() if not channel_mismatch else layers.Conv(\n      channels, (3, 3), strides, padding='SAME')\n  return layers.Serial(\n      layers.Branch(), layers.Parallel(main, shortcut), layers.SumBranches())", "output": "WideResnet convolutational block.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def running(concurrent=False):\n    '''\n    \n    '''\n    ret = []\n    if concurrent:\n        return ret\n    active = __salt__['saltutil.is_running']('state.*')\n    for data in active:\n        err = (\n            'The function \"{0}\" is running as PID {1} and was started at '\n            '{2} with jid {3}'\n        ).format(\n            data['fun'],\n            data['pid'],\n            salt.utils.jid.jid_to_time(data['jid']),\n            data['jid'],\n        )\n        ret.append(err)\n    return ret", "output": "Return a list of strings that contain state return data if a state function\n    is already running. This function is used to prevent multiple state calls\n    from being run at the same time.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.running", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_best_match_zone(all_zones, domain):\n        \"\"\"\"\"\"\n\n        # Related: https://github.com/Miserlou/Zappa/issues/459\n        public_zones = [zone for zone in all_zones['HostedZones'] if not zone['Config']['PrivateZone']]\n\n        zones = {zone['Name'][:-1]: zone['Id'] for zone in public_zones if zone['Name'][:-1] in domain}\n        if zones:\n            keys = max(zones.keys(), key=lambda a: len(a))  # get longest key -- best match.\n            return zones[keys]\n        else:\n            return None", "output": "Return zone id which name is closer matched with domain name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait_port_open(server, port, timeout=None):\n    \"\"\" \n    \"\"\"\n    import socket\n    import errno\n    import time\n    sleep_s = 0\n    if timeout:\n        from time import time as now\n        # time module is needed to calc timeout shared between two exceptions\n        end = now() + timeout\n\n    while True:\n        logging.debug(\"Sleeping for %s second(s)\", sleep_s)\n        time.sleep(sleep_s)\n        s = socket.socket()\n        try:\n            if timeout:\n                next_timeout = end - now()\n                if next_timeout < 0:\n                    return False\n                else:\n                    s.settimeout(next_timeout)\n\n            logging.info(\"connect %s %d\", server, port)\n            s.connect((server, port))\n\n        except ConnectionError as err:\n            logging.debug(\"ConnectionError %s\", err)\n            if sleep_s == 0:\n                sleep_s = 1\n\n        except socket.gaierror as err:\n            logging.debug(\"gaierror %s\",err)\n            return False\n\n        except socket.timeout as err:\n            # this exception occurs only if timeout is set\n            if timeout:\n                return False\n\n        except TimeoutError as err:\n            # catch timeout exception from underlying network library\n            # this one is different from socket.timeout\n            raise\n\n        else:\n            s.close()\n            logging.info(\"wait_port_open: port %s:%s is open\", server, port)\n            return True", "output": "Wait for network service to appear\n        @param server: host to connect to (str)\n        @param port: port (int)\n        @param timeout: in seconds, if None or 0 wait forever\n        @return: True of False, if timeout is None may return only True or\n                 throw unhandled network exception", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reference(self):\n        \"\"\"\n        \"\"\"\n        ref = ModelReference()\n        ref._proto = self._proto.model_reference\n        return ref", "output": "A :class:`~google.cloud.bigquery.model.ModelReference` pointing to\n        this model.\n\n        Read-only.\n\n        Returns:\n            google.cloud.bigquery.model.ModelReference: pointer to this model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sector_code_name(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            return self.__dict__[\"sector_code_name\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'sector_code_name' \".format(self.order_book_id)\n            )", "output": "[str] \u4ee5\u5f53\u5730\u8bed\u8a00\u4e3a\u6807\u51c6\u7684\u677f\u5757\u4ee3\u7801\u540d\uff08\u80a1\u7968\u4e13\u7528\uff09", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def template_get(name=None, host=None, templateids=None, **kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'template.get'\n            params = {\"output\": \"extend\", \"filter\": {}}\n            if name:\n                params['filter'].setdefault('name', name)\n            if host:\n                params['filter'].setdefault('host', host)\n            if templateids:\n                params.setdefault('templateids', templateids)\n            params = _params_extend(params, **kwargs)\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result'] if ret['result'] else False\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": "Retrieve templates according to the given parameters.\n\n    Args:\n        host: technical name of the template\n        name: visible name of the template\n        hostids: ids of the templates\n\n        optional kwargs:\n                _connection_user: zabbix user (can also be set in opts or pillar, see module's docstring)\n                _connection_password: zabbix password (can also be set in opts or pillar, see module's docstring)\n                _connection_url: url of zabbix frontend (can also be set in opts or pillar, see module's docstring)\n\n                all optional template.get parameters: keyword argument names depends on your zabbix version, see:\n\n                https://www.zabbix.com/documentation/2.4/manual/api/reference/template/get\n\n    Returns:\n        Array with convenient template details, False if no template found or on failure.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.template_get name='Template OS Linux'\n        salt '*' zabbix.template_get templateids=\"['10050', '10001']\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ensure_categorical(arr):\n    \"\"\"\n    \n    \"\"\"\n\n    if not is_categorical(arr):\n        from pandas import Categorical\n        arr = Categorical(arr)\n    return arr", "output": "Ensure that an array-like object is a Categorical (if not already).\n\n    Parameters\n    ----------\n    arr : array-like\n        The array that we want to convert into a Categorical.\n\n    Returns\n    -------\n    cat_arr : The original array cast as a Categorical. If it already\n              is a Categorical, we return as is.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def echo_via_pager(text_or_generator, color=None):\n    \"\"\"\n    \"\"\"\n    color = resolve_color_default(color)\n\n    if inspect.isgeneratorfunction(text_or_generator):\n        i = text_or_generator()\n    elif isinstance(text_or_generator, string_types):\n        i = [text_or_generator]\n    else:\n        i = iter(text_or_generator)\n\n    # convert every element of i to a text type if necessary\n    text_generator = (el if isinstance(el, string_types) else text_type(el)\n                      for el in i)\n\n    from ._termui_impl import pager\n    return pager(itertools.chain(text_generator, \"\\n\"), color)", "output": "This function takes a text and shows it via an environment specific\n    pager on stdout.\n\n    .. versionchanged:: 3.0\n       Added the `color` flag.\n\n    :param text_or_generator: the text to page, or alternatively, a\n                              generator emitting the text to page.\n    :param color: controls if the pager supports ANSI colors or not.  The\n                  default is autodetection.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_ax_layer(cls, ax, primary=True):\n        \"\"\"\"\"\"\n        if primary:\n            return getattr(ax, 'left_ax', ax)\n        else:\n            return getattr(ax, 'right_ax', ax)", "output": "get left (primary) or right (secondary) axes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def revert_snapshot(name, snap_name, runas=None):\n    '''\n    \n    '''\n    # Validate VM and snapshot names\n    name = salt.utils.data.decode(name)\n    snap_name = _validate_snap_name(name, snap_name, runas=runas)\n\n    # Construct argument list\n    args = [name, '--id', snap_name]\n\n    # Execute command and return output\n    return prlctl('snapshot-switch', args, runas=runas)", "output": "Revert a VM to a snapshot\n\n    :param str name:\n        Name/ID of VM to revert to a snapshot\n\n    :param str snap_name:\n        Name/ID of snapshot to revert to\n\n    :param str runas:\n        The user that the prlctl command will be run as\n\n    Example:\n\n    .. code-block:: bash\n\n        salt '*' parallels.revert_snapshot macvm base-with-updates runas=macdev", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def benchmark_command(cmd, progress):\n    \"\"\"\"\"\"\n    full_cmd = '/usr/bin/time --format=\"%U %M\" {0}'.format(cmd)\n    print '{0:6.2f}% Running {1}'.format(100.0 * progress, full_cmd)\n    (_, err) = subprocess.Popen(\n        ['/bin/sh', '-c', full_cmd],\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE\n    ).communicate('')\n\n    values = err.strip().split(' ')\n    if len(values) == 2:\n        try:\n            return (float(values[0]), float(values[1]))\n        except:  # pylint:disable=I0011,W0702\n            pass  # Handled by the code after the \"if\"\n\n    print err\n    raise Exception('Error during benchmarking')", "output": "Benchmark one command execution", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_images(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudException(\n            'The avail_images function must be called with -f or --function.'\n        )\n\n    response = _query('avail', 'distributions')\n\n    ret = {}\n    for item in response['DATA']:\n        name = item['LABEL']\n        ret[name] = item\n\n    return ret", "output": "Return available Linode images.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud --list-images my-linode-config\n        salt-cloud -f avail_images my-linode-config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_archive_file(name):\n    # type: (str) -> bool\n    \"\"\"\"\"\"\n    ext = splitext(name)[1].lower()\n    if ext in ARCHIVE_EXTENSIONS:\n        return True\n    return False", "output": "Return True if `name` is a considered as an archive file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop(name, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The start action must be called with -a or --action.'\n        )\n\n    log.info('Stopping node %s', name)\n\n    return vm_action(name, kwargs={'action': 'stop'}, call=call)", "output": "Stop a VM.\n\n    .. versionadded:: 2016.3.0\n\n    name\n        The name of the VM to stop.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a stop my-vm", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_df(cls, path:PathOrStr, df:pd.DataFrame, folder:PathOrStr=None, label_delim:str=None, valid_pct:float=0.2,\n                fn_col:IntsOrStrs=0, label_col:IntsOrStrs=1, suffix:str='', **kwargs:Any)->'ImageDataBunch':\n        \"\"\n        src = (ImageList.from_df(df, path=path, folder=folder, suffix=suffix, cols=fn_col)\n                .split_by_rand_pct(valid_pct)\n                .label_from_df(label_delim=label_delim, cols=label_col))\n        return cls.create_from_ll(src, **kwargs)", "output": "Create from a `DataFrame` `df`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get(self, plugin_name):\n        \"\"\"\n        \n        \"\"\"\n\n        for p in self._plugins:\n            if p.name == plugin_name:\n                return p\n\n        return None", "output": "Retrieves the plugin with given name\n\n        :param plugin_name: Name of the plugin to retrieve\n        :return samtranslator.plugins.BasePlugin: Returns the plugin object if found. None, otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_SU_save_future_list(engine, client=DATABASE):\n    \"\"\"\n    \"\"\"\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_future_list(client=client)", "output": "save future_list\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fire_master(self, data, tag, preload=None):\n        '''\n        \n        '''\n        load = {}\n        if preload:\n            load.update(preload)\n\n        load.update({\n            'id': self.opts['id'],\n            'tag': tag,\n            'data': data,\n            'cmd': '_minion_event',\n            'tok': self.auth.gen_token(b'salt'),\n        })\n\n        channel = salt.transport.client.ReqChannel.factory(self.opts)\n        try:\n            channel.send(load)\n        except Exception:\n            pass\n        finally:\n            channel.close()\n        return True", "output": "Fire an event off on the master server\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' event.fire_master 'stuff to be in the event' 'tag'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uimports(code):\n    \"\"\"  \"\"\"\n    for uimport in UIMPORTLIST:\n        uimport = bytes(uimport, 'utf8')\n        code = code.replace(uimport, b'u' + uimport)\n    return code", "output": "converts CPython module names into MicroPython equivalents", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rollover(self, alias, new_index=None, body=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if alias in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'alias'.\")\n        return self.transport.perform_request(\n            \"POST\", _make_path(alias, \"_rollover\", new_index), params=params, body=body\n        )", "output": "The rollover index API rolls an alias over to a new index when the\n        existing index is considered to be too large or too old.\n\n        The API accepts a single alias name and a list of conditions. The alias\n        must point to a single index only. If the index satisfies the specified\n        conditions then a new index is created and the alias is switched to\n        point to the new alias.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-rollover-index.html>`_\n\n        :arg alias: The name of the alias to rollover\n        :arg new_index: The name of the rollover index\n        :arg body: The conditions that needs to be met for executing rollover\n        :arg dry_run: If set to true the rollover action will only be validated\n            but not actually performed even if a condition matches. The default\n            is false\n        :arg master_timeout: Specify timeout for connection to master\n        :arg request_timeout: Explicit operation timeout\n        :arg wait_for_active_shards: Set the number of active shards to wait for\n            on the newly created rollover index before the operation returns.\n        :arg include_type_name: Specify whether requests and responses should include a\n            type name (default: depends on Elasticsearch version).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compute_eager_pipelines(self):\n        \"\"\"\n        \n        \"\"\"\n        for name, pipe in self._pipelines.items():\n            if pipe.eager:\n                self.pipeline_output(name)", "output": "Compute any pipelines attached with eager=True.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_template(path,\n                 dest,\n                 template='jinja',\n                 saltenv='base',\n                 makedirs=False,\n                 **kwargs):\n    '''\n    \n    '''\n    if 'salt' not in kwargs:\n        kwargs['salt'] = __salt__\n    if 'pillar' not in kwargs:\n        kwargs['pillar'] = __pillar__\n    if 'grains' not in kwargs:\n        kwargs['grains'] = __grains__\n    if 'opts' not in kwargs:\n        kwargs['opts'] = __opts__\n    return _client().get_template(\n            path,\n            dest,\n            template,\n            makedirs,\n            saltenv,\n            **kwargs)", "output": "Render a file as a template before setting it down.\n    Warning, order is not the same as in fileclient.cp for\n    non breaking old API.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cp.get_template salt://path/to/template /minion/dest", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_pkgs(*packages, **kwargs):\n    '''\n    \n    '''\n    pkgs = {}\n    cmd = ['rpm']\n    if kwargs.get('root'):\n        cmd.extend(['--root', kwargs['root']])\n    cmd.extend(['-q' if packages else '-qa',\n                '--queryformat', r'%{NAME} %{VERSION}\\n'])\n    if packages:\n        cmd.extend(packages)\n    out = __salt__['cmd.run'](cmd, output_loglevel='trace', python_shell=False)\n    for line in salt.utils.itertools.split(out, '\\n'):\n        if 'is not installed' in line:\n            continue\n        comps = line.split()\n        pkgs[comps[0]] = comps[1]\n    return pkgs", "output": "List the packages currently installed in a dict::\n\n        {'<package_name>': '<version>'}\n\n    root\n        use root as top level directory (default: \"/\")\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' lowpkg.list_pkgs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_module_table_header(modules):\n    \"\"\" \n    \"\"\"\n\n    # Print header file for all external modules.\n    mod_defs = []\n    print(\"// Automatically generated by makemoduledefs.py.\\n\")\n    for module_name, obj_module, enabled_define in modules:\n        mod_def = \"MODULE_DEF_{}\".format(module_name.upper())\n        mod_defs.append(mod_def)\n        print((\n            \"#if ({enabled_define})\\n\"\n            \"    extern const struct _mp_obj_module_t {obj_module};\\n\"\n            \"    #define {mod_def} {{ MP_ROM_QSTR({module_name}), MP_ROM_PTR(&{obj_module}) }},\\n\"\n            \"#else\\n\"\n            \"    #define {mod_def}\\n\"\n            \"#endif\\n\"\n            ).format(module_name=module_name, obj_module=obj_module,\n                     enabled_define=enabled_define, mod_def=mod_def)\n        )\n\n    print(\"\\n#define MICROPY_REGISTERED_MODULES \\\\\")\n\n    for mod_def in mod_defs:\n        print(\"    {mod_def} \\\\\".format(mod_def=mod_def))\n\n    print(\"// MICROPY_REGISTERED_MODULES\")", "output": "Generate header with module table entries for builtin modules.\n\n    :param List[(module_name, obj_module, enabled_define)] modules: module defs\n    :return: None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fix_input_files_for_numbered_seq(sourceDir, suffix, timestamp, containers):\n    \"\"\"\"\"\"\n    # Fix input files for each MPL-container type.\n    for container in containers:\n        files = glob.glob( os.path.join( sourceDir, container, container + '*' + suffix ) )\n        for currentFile in sorted( files ):\n            fix_header_comment( currentFile, timestamp )", "output": "Fixes files used as input when pre-processing MPL-containers in their numbered form.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.file is None:\n            return\n\n        # Close the pickle file.\n        self.file.close()\n        self.file = None\n\n        for f in self.mark_for_delete:\n            error = [False]\n\n            def register_error(*args):\n                error[0] = True\n\n            _shutil.rmtree(f, onerror = register_error)\n\n            if error[0]:\n                _atexit.register(_shutil.rmtree, f, ignore_errors=True)", "output": "Close the pickle file, and the zip archive file. The single zip archive\n        file can now be shipped around to be loaded by the unpickler.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _format_entrypoint_target(ep):\n    '''\n    \n    '''\n    s = ep.module_name\n    if ep.attrs:\n        s += ':' + '.'.join(ep.attrs)\n    return s", "output": "Makes a string describing the target of an EntryPoint object.\n\n    Base strongly on EntryPoint.__str__().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_vsan_cluster_config_system(service_instance):\n    '''\n    \n    '''\n\n    #TODO Replace when better connection mechanism is available\n\n    #For python 2.7.9 and later, the defaul SSL conext has more strict\n    #connection handshaking rule. We may need turn of the hostname checking\n    #and client side cert verification\n    context = None\n    if sys.version_info[:3] > (2, 7, 8):\n        context = ssl.create_default_context()\n        context.check_hostname = False\n        context.verify_mode = ssl.CERT_NONE\n\n    stub = service_instance._stub\n    vc_mos = vsanapiutils.GetVsanVcMos(stub, context=context)\n    return vc_mos['vsan-cluster-config-system']", "output": "Returns a vim.cluster.VsanVcClusterConfigSystem object\n\n    service_instance\n        Service instance to the host or vCenter", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _IsValidPath(message_descriptor, path):\n  \"\"\"\"\"\"\n  parts = path.split('.')\n  last = parts.pop()\n  for name in parts:\n    field = message_descriptor.fields_by_name[name]\n    if (field is None or\n        field.label == FieldDescriptor.LABEL_REPEATED or\n        field.type != FieldDescriptor.TYPE_MESSAGE):\n      return False\n    message_descriptor = field.message_type\n  return last in message_descriptor.fields_by_name", "output": "Checks whether the path is valid for Message Descriptor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def actnorm_scale(name, x, logscale_factor=3., reverse=False, init=False):\n  \"\"\"\"\"\"\n  x_shape = common_layers.shape_list(x)\n  with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n\n    # Variance initialization logic.\n    assert len(x_shape) == 2 or len(x_shape) == 4\n    if len(x_shape) == 2:\n      x_var = tf.reduce_mean(x**2, [0], keepdims=True)\n      logdet_factor = 1\n      var_shape = (1, x_shape[1])\n    elif len(x_shape) == 4:\n      x_var = tf.reduce_mean(x**2, [0, 1, 2], keepdims=True)\n      logdet_factor = x_shape[1]*x_shape[2]\n      var_shape = (1, 1, 1, x_shape[3])\n\n    init_value = tf.log(1.0 / (tf.sqrt(x_var) + 1e-6)) / logscale_factor\n    logs = get_variable_ddi(\"logs\", var_shape, initial_value=init_value,\n                            init=init)\n    logs = logs * logscale_factor\n\n    # Function and reverse function.\n    if not reverse:\n      x = x * tf.exp(logs)\n    else:\n      x = x * tf.exp(-logs)\n\n    # Objective calculation, h * w * sum(log|s|)\n    dlogdet = tf.reduce_sum(logs) * logdet_factor\n    if reverse:\n      dlogdet *= -1\n    return x, dlogdet", "output": "Per-channel scaling of x.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handleEntity(self, ctxt):\n        \"\"\" \"\"\"\n        if ctxt is None: ctxt__o = None\n        else: ctxt__o = ctxt._o\n        libxml2mod.xmlHandleEntity(ctxt__o, self._o)", "output": "Default handling of defined entities, when should we define\n          a new input stream ? When do we just handle that as a set\n           of chars ?  OBSOLETE: to be removed at some point.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _render_template(param, username):\n    '''\n    \n    '''\n    env = Environment()\n    template = env.from_string(param)\n    variables = {'username': username}\n    return template.render(variables)", "output": "Render config template, substituting username where found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_computer_desc(desc=None):\n    '''\n    \n    '''\n    if six.PY2:\n        desc = _to_unicode(desc)\n\n    # Make sure the system exists\n    # Return an object containing current information array for the computer\n    system_info = win32net.NetServerGetInfo(None, 101)\n\n    # If desc is passed, decode it for unicode\n    if desc is None:\n        return False\n\n    system_info['comment'] = desc\n\n    # Apply new settings\n    try:\n        win32net.NetServerSetInfo(None, 101, system_info)\n    except win32net.error as exc:\n        (number, context, message) = exc.args\n        log.error('Failed to update system')\n        log.error('nbr: %s', number)\n        log.error('ctx: %s', context)\n        log.error('msg: %s', message)\n        return False\n\n    return {'Computer Description': get_computer_desc()}", "output": "Set the Windows computer description\n\n    Args:\n\n        desc (str):\n            The computer description\n\n    Returns:\n        str: Description if successful, otherwise ``False``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'minion-id' system.set_computer_desc 'This computer belongs to Dave!'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sysrq(vm, action='nmi', key='uuid'):\n    '''\n    \n    '''\n    ret = {}\n    if key not in ['uuid', 'alias', 'hostname']:\n        ret['Error'] = 'Key must be either uuid, alias or hostname'\n        return ret\n    if action not in ['nmi', 'screenshot']:\n        ret['Error'] = 'Action must be either nmi or screenshot'\n        return ret\n    vm = lookup('{0}={1}'.format(key, vm), one=True)\n    if 'Error' in vm:\n        return vm\n    # vmadm sysrq <uuid> <nmi|screenshot>\n    cmd = 'vmadm sysrq {uuid} {action}'.format(\n        uuid=vm,\n        action=action\n    )\n    res = __salt__['cmd.run_all'](cmd)\n    retcode = res['retcode']\n    if retcode != 0:\n        ret['Error'] = res['stderr'] if 'stderr' in res else _exit_status(retcode)\n        return ret\n    return True", "output": "Send non-maskable interrupt to vm or capture a screenshot\n\n    vm : string\n        vm to be targeted\n    action : string\n        nmi or screenshot -- Default: nmi\n    key : string [uuid|alias|hostname]\n        value type of 'vm' parameter\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' vmadm.sysrq 186da9ab-7392-4f55-91a5-b8f1fe770543 nmi\n        salt '*' vmadm.sysrq 186da9ab-7392-4f55-91a5-b8f1fe770543 screenshot\n        salt '*' vmadm.sysrq nacl nmi key=alias", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def html_page_for_render_items(bundle, docs_json, render_items, title, template=None, template_variables={}):\n    ''' \n\n    '''\n    if title is None:\n        title = DEFAULT_TITLE\n\n    bokeh_js, bokeh_css = bundle\n\n    json_id = make_id()\n    json = escape(serialize_json(docs_json), quote=False)\n    json = wrap_in_script_tag(json, \"application/json\", json_id)\n\n    script = wrap_in_script_tag(script_for_render_items(json_id, render_items))\n\n    context = template_variables.copy()\n\n    context.update(dict(\n        title = title,\n        bokeh_js = bokeh_js,\n        bokeh_css = bokeh_css,\n        plot_script = json + script,\n        docs = render_items,\n        base = FILE,\n        macros = MACROS,\n    ))\n\n    if len(render_items) == 1:\n        context[\"doc\"] = context[\"docs\"][0]\n        context[\"roots\"] = context[\"doc\"].roots\n\n    # XXX: backwards compatibility, remove for 1.0\n    context[\"plot_div\"] = \"\\n\".join(div_for_render_item(item) for item in render_items)\n\n    if template is None:\n        template = FILE\n    elif isinstance(template, string_types):\n        template = _env.from_string(\"{% extends base %}\\n\" + template)\n\n    html = template.render(context)\n    return encode_utf8(html)", "output": "Render an HTML page from a template and Bokeh render items.\n\n    Args:\n        bundle (tuple):\n            a tuple containing (bokehjs, bokehcss)\n\n        docs_json (JSON-like):\n            Serialized Bokeh Document\n\n        render_items (RenderItems)\n            Specific items to render from the document and where\n\n        title (str or None)\n            A title for the HTML page. If None, DEFAULT_TITLE is used\n\n        template (str or Template or None, optional) :\n            A Template to be used for the HTML page. If None, FILE is used.\n\n        template_variables (dict, optional):\n            Any Additional variables to pass to the template\n\n    Returns:\n        str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def go_next_thumbnail(self):\n        \"\"\"\"\"\"\n        if self.current_thumbnail is not None:\n            index = self._thumbnails.index(self.current_thumbnail) + 1\n            index = 0 if index >= len(self._thumbnails) else index\n            self.set_current_index(index)\n            self.scroll_to_item(index)", "output": "Select thumbnail next to the currently selected one.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rect(self):\n        \"\"\"\"\"\"\n        if self._w3c:\n            return self._execute(Command.GET_ELEMENT_RECT)['value']\n        else:\n            rect = self.size.copy()\n            rect.update(self.location)\n            return rect", "output": "A dictionary with the size and location of the element.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_allowed(self, allowed_values, field, value):\n        \"\"\"  \"\"\"\n        if isinstance(value, Iterable) and not isinstance(value, _str_type):\n            unallowed = set(value) - set(allowed_values)\n            if unallowed:\n                self._error(field, errors.UNALLOWED_VALUES, list(unallowed))\n        else:\n            if value not in allowed_values:\n                self._error(field, errors.UNALLOWED_VALUE, value)", "output": "{'type': 'list'}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_api_stage_to_api_key(self, api_key, api_id, stage_name):\n        \"\"\"\n        \n        \"\"\"\n        self.apigateway_client.update_api_key(\n            apiKey=api_key,\n            patchOperations=[\n                {\n                    'op': 'add',\n                    'path': '/stages',\n                    'value': '{}/{}'.format(api_id, stage_name)\n                }\n            ]\n        )", "output": "Add api stage to Api key", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def group(*blueprints, url_prefix=\"\"):\n        \"\"\"\n        \n        \"\"\"\n\n        def chain(nested):\n            \"\"\"itertools.chain() but leaves strings untouched\"\"\"\n            for i in nested:\n                if isinstance(i, (list, tuple)):\n                    yield from chain(i)\n                elif isinstance(i, BlueprintGroup):\n                    yield from i.blueprints\n                else:\n                    yield i\n\n        bps = BlueprintGroup(url_prefix=url_prefix)\n        for bp in chain(blueprints):\n            if bp.url_prefix is None:\n                bp.url_prefix = \"\"\n            bp.url_prefix = url_prefix + bp.url_prefix\n            bps.append(bp)\n        return bps", "output": "Create a list of blueprints, optionally grouping them under a\n        general URL prefix.\n\n        :param blueprints: blueprints to be registered as a group\n        :param url_prefix: URL route to be prepended to all sub-prefixes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_route(route_table_id=None, destination_cidr_block=None,\n                 route_table_name=None, region=None, key=None,\n                 keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    if not _exactly_one((route_table_name, route_table_id)):\n        raise SaltInvocationError('One (but not both) of route_table_id or route_table_name '\n                                  'must be provided.')\n\n    if destination_cidr_block is None:\n        raise SaltInvocationError('destination_cidr_block is required.')\n\n    try:\n        if route_table_name:\n            route_table_id = _get_resource_id('route_table', route_table_name,\n                                              region=region, key=key,\n                                              keyid=keyid, profile=profile)\n            if not route_table_id:\n                return {'created': False,\n                        'error': {'message': 'route table {0} does not exist.'.format(route_table_name)}}\n    except BotoServerError as e:\n        return {'created': False, 'error': __utils__['boto.get_error'](e)}\n\n    return _delete_resource(resource='route', resource_id=route_table_id,\n                            destination_cidr_block=destination_cidr_block,\n                            region=region, key=key,\n                            keyid=keyid, profile=profile)", "output": "Deletes a route.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_vpc.delete_route 'rtb-1f382e7d' '10.0.0.0/16'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_util_log_info(\n        logs,\n        ui_log=None,\n        ui_progress=None,\n        ui_progress_int_value=None,\n):\n    \"\"\"\n    \n    \"\"\"\n    logging.warning(logs)\n\n    # \u7ed9GUI\u4f7f\u7528\uff0c\u66f4\u65b0\u5f53\u524d\u4efb\u52a1\u5230\u65e5\u5fd7\u548c\u8fdb\u5ea6\n    if ui_log is not None:\n        if isinstance(logs, str):\n            ui_log.emit(logs)\n        if isinstance(logs, list):\n            for iStr in logs:\n                ui_log.emit(iStr)\n\n    if ui_progress is not None and ui_progress_int_value is not None:\n        ui_progress.emit(ui_progress_int_value)", "output": "QUANTAXIS Log Module\n    @yutiansut\n\n    QA_util_log_x is under [QAStandard#0.0.2@602-x] Protocol", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def split_arg_string(string):\n    \"\"\"\"\"\"\n    rv = []\n    for match in re.finditer(r\"('([^'\\\\]*(?:\\\\.[^'\\\\]*)*)'\"\n                             r'|\"([^\"\\\\]*(?:\\\\.[^\"\\\\]*)*)\"'\n                             r'|\\S+)\\s*', string, re.S):\n        arg = match.group().strip()\n        if arg[:1] == arg[-1:] and arg[:1] in '\"\\'':\n            arg = arg[1:-1].encode('ascii', 'backslashreplace') \\\n                .decode('unicode-escape')\n        try:\n            arg = type(string)(arg)\n        except UnicodeError:\n            pass\n        rv.append(arg)\n    return rv", "output": "Given an argument string this attempts to split it into small parts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def concat_ws(sep, *cols):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.concat_ws(sep, _to_seq(sc, cols, _to_java_column)))", "output": "Concatenates multiple input string columns together into a single string column,\n    using the given separator.\n\n    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n    >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n    [Row(s=u'abcd-123')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_hashed_rule_name(event, function, lambda_name):\n        \"\"\"\n        \n        \"\"\"\n        event_name = event.get('name', function)\n        name_hash = hashlib.sha1('{}-{}'.format(lambda_name, event_name).encode('UTF-8')).hexdigest()\n        return Zappa.get_event_name(name_hash, function)", "output": "Returns an AWS-valid CloudWatch rule name using a digest of the event name, lambda name, and function.\n        This allows support for rule names that may be longer than the 64 char limit.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_pgid(path, follow_symlinks=True):\n    '''\n    \n    '''\n    if not os.path.exists(path):\n        raise CommandExecutionError('Path not found: {0}'.format(path))\n\n    # Under Windows, if the path is a symlink, the user that owns the symlink is\n    # returned, not the user that owns the file/directory the symlink is\n    # pointing to. This behavior is *different* to *nix, therefore the symlink\n    # is first resolved manually if necessary. Remember symlinks are only\n    # supported on Windows Vista or later.\n    if follow_symlinks and sys.getwindowsversion().major >= 6:\n        path = _resolve_symlink(path)\n\n    group_name = salt.utils.win_dacl.get_primary_group(path)\n    return salt.utils.win_dacl.get_sid_string(group_name)", "output": "Return the id of the primary group that owns a given file (Windows only)\n\n    This function will return the rarely used primary group of a file. This\n    generally has no bearing on permissions unless intentionally configured\n    and is most commonly used to provide Unix compatibility (e.g. Services\n    For Unix, NFS services).\n\n    Ensure you know what you are doing before using this function.\n\n    Args:\n        path (str): The path to the file or directory\n\n        follow_symlinks (bool):\n            If the object specified by ``path`` is a symlink, get attributes of\n            the linked file instead of the symlink itself. Default is True\n\n    Returns:\n        str: The gid of the primary group\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.get_pgid c:\\\\temp\\\\test.txt", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_html(self, classes=None, notebook=False, border=None):\n        \"\"\"\n        \n         \"\"\"\n        from pandas.io.formats.html import HTMLFormatter, NotebookFormatter\n        Klass = NotebookFormatter if notebook else HTMLFormatter\n        html = Klass(self, classes=classes, border=border).render()\n        if hasattr(self.buf, 'write'):\n            buffer_put_lines(self.buf, html)\n        elif isinstance(self.buf, str):\n            with open(self.buf, 'w') as f:\n                buffer_put_lines(f, html)\n        else:\n            raise TypeError('buf is not a file name and it has no write '\n                            ' method')", "output": "Render a DataFrame to a html table.\n\n        Parameters\n        ----------\n        classes : str or list-like\n            classes to include in the `class` attribute of the opening\n            ``<table>`` tag, in addition to the default \"dataframe\".\n        notebook : {True, False}, optional, default False\n            Whether the generated HTML is for IPython Notebook.\n        border : int\n            A ``border=border`` attribute is included in the opening\n            ``<table>`` tag. Default ``pd.options.html.border``.\n\n            .. versionadded:: 0.19.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_disk(self, path, exclude=tuple(), disable=None):\n        \"\"\"\n        \"\"\"\n        if disable is not None:\n            deprecation_warning(Warnings.W014)\n            exclude = disable\n        path = util.ensure_path(path)\n        serializers = OrderedDict()\n        serializers[\"tokenizer\"] = lambda p: self.tokenizer.to_disk(p, exclude=[\"vocab\"])\n        serializers[\"meta.json\"] = lambda p: p.open(\"w\").write(srsly.json_dumps(self.meta))\n        for name, proc in self.pipeline:\n            if not hasattr(proc, \"name\"):\n                continue\n            if name in exclude:\n                continue\n            if not hasattr(proc, \"to_disk\"):\n                continue\n            serializers[name] = lambda p, proc=proc: proc.to_disk(p, exclude=[\"vocab\"])\n        serializers[\"vocab\"] = lambda p: self.vocab.to_disk(p)\n        util.to_disk(path, serializers, exclude)", "output": "Save the current state to a directory.  If a model is loaded, this\n        will include the model.\n\n        path (unicode or Path): Path to a directory, which will be created if\n            it doesn't exist.\n        exclude (list): Names of components or serialization fields to exclude.\n\n        DOCS: https://spacy.io/api/language#to_disk", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dump_session_params(path):\n    \"\"\"\n    \n    \"\"\"\n    # save variables that are GLOBAL, and either TRAINABLE or MODEL\n    var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n    var.extend(tf.get_collection(tf.GraphKeys.MODEL_VARIABLES))\n    # TODO dedup\n    assert len(set(var)) == len(var), \"TRAINABLE and MODEL variables have duplication!\"\n    gvars = set([k.name for k in tf.global_variables()])\n    var = [v for v in var if v.name in gvars]\n    result = {}\n    for v in var:\n        result[v.name] = v.eval()\n    save_chkpt_vars(result, path)", "output": "Dump value of all TRAINABLE + MODEL variables to a dict, and save as\n    npz format (loadable by :func:`sessinit.get_model_loader`).\n\n    Args:\n        path(str): the file name to save the parameters. Must ends with npz.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_verbosity(self, verbose=False, print_func=None):\n        \"\"\"\n        \"\"\"\n        self._verbose = verbose\n        if print_func is None:\n            def asum_stat(x):\n                \"\"\"returns |x|/size(x), async execution.\"\"\"\n                return str((ndarray.norm(x)/sqrt(x.size)).asscalar())\n            print_func = asum_stat\n        self._print_func = print_func\n        return self", "output": "Switch on/off verbose mode\n\n        Parameters\n        ----------\n        verbose : bool\n            switch on/off verbose mode\n        print_func : function\n            A function that computes statistics of initialized arrays.\n            Takes an `NDArray` and returns an `str`. Defaults to mean\n            absolute value str((abs(x)/size(x)).asscalar()).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _next_iter_line(self, row_num):\n        \"\"\"\n        \n        \"\"\"\n\n        try:\n            return next(self.data)\n        except csv.Error as e:\n            if self.warn_bad_lines or self.error_bad_lines:\n                msg = str(e)\n\n                if 'NULL byte' in msg:\n                    msg = ('NULL byte detected. This byte '\n                           'cannot be processed in Python\\'s '\n                           'native csv library at the moment, '\n                           'so please pass in engine=\\'c\\' instead')\n\n                if self.skipfooter > 0:\n                    reason = ('Error could possibly be due to '\n                              'parsing errors in the skipped footer rows '\n                              '(the skipfooter keyword is only applied '\n                              'after Python\\'s csv library has parsed '\n                              'all rows).')\n                    msg += '. ' + reason\n\n                self._alert_malformed(msg, row_num)\n            return None", "output": "Wrapper around iterating through `self.data` (CSV source).\n\n        When a CSV error is raised, we check for specific\n        error messages that allow us to customize the\n        error message displayed to the user.\n\n        Parameters\n        ----------\n        row_num : The row number of the line being parsed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log_metrics(self,\n                    train_metrics: dict,\n                    val_metrics: dict = None,\n                    epoch: int = None,\n                    log_to_console: bool = False) -> None:\n        \"\"\"\n        \n        \"\"\"\n        metric_names = set(train_metrics.keys())\n        if val_metrics is not None:\n            metric_names.update(val_metrics.keys())\n        val_metrics = val_metrics or {}\n\n        # For logging to the console\n        if log_to_console:\n            dual_message_template = \"%s |  %8.3f  |  %8.3f\"\n            no_val_message_template = \"%s |  %8.3f  |  %8s\"\n            no_train_message_template = \"%s |  %8s  |  %8.3f\"\n            header_template = \"%s |  %-10s\"\n            name_length = max([len(x) for x in metric_names])\n            logger.info(header_template, \"Training\".rjust(name_length + 13), \"Validation\")\n\n        for name in metric_names:\n            # Log to tensorboard\n            train_metric = train_metrics.get(name)\n            if train_metric is not None:\n                self.add_train_scalar(name, train_metric, timestep=epoch)\n            val_metric = val_metrics.get(name)\n            if val_metric is not None:\n                self.add_validation_scalar(name, val_metric, timestep=epoch)\n\n            # And maybe log to console\n            if log_to_console and val_metric is not None and train_metric is not None:\n                logger.info(dual_message_template, name.ljust(name_length), train_metric, val_metric)\n            elif log_to_console and val_metric is not None:\n                logger.info(no_train_message_template, name.ljust(name_length), \"N/A\", val_metric)\n            elif log_to_console and train_metric is not None:\n                logger.info(no_val_message_template, name.ljust(name_length), train_metric, \"N/A\")", "output": "Sends all of the train metrics (and validation metrics, if provided) to tensorboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_active_queue(self):\n        \"\"\"\"\"\"\n\n        # Get dict of active queues keyed by name\n        queues = {q['jobQueueName']: q for q in self._client.describe_job_queues()['jobQueues']\n                  if q['state'] == 'ENABLED' and q['status'] == 'VALID'}\n        if not queues:\n            raise Exception('No job queues with state=ENABLED and status=VALID')\n\n        # Pick the first queue as default\n        return list(queues.keys())[0]", "output": "Get name of first active job queue", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _NormalizeDuration(self, seconds, nanos):\n    \"\"\"\"\"\"\n    # Force nanos to be negative if the duration is negative.\n    if seconds < 0 and nanos > 0:\n      seconds += 1\n      nanos -= _NANOS_PER_SECOND\n    self.seconds = seconds\n    self.nanos = nanos", "output": "Set Duration by seconds and nonas.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def aws_encode(x):\n    '''\n    \n\n    '''\n    ret = None\n    try:\n        x.encode('ascii')\n        ret = re.sub(r'\\\\x([a-f0-8]{2})',\n                      _hexReplace, x.encode('unicode_escape'))\n    except UnicodeEncodeError:\n        ret = x.encode('idna')\n    except Exception as e:\n        log.error(\"Couldn't encode %s using either 'unicode_escape' or 'idna' codecs\", x)\n        raise CommandExecutionError(e)\n    log.debug('AWS-encoded result for %s: %s', x, ret)\n    return ret", "output": "An implementation of the encoding required to suport AWS's domain name\n    rules defined here__:\n\n    .. __: http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/DomainNameFormat.html\n\n    While AWS's documentation specifies individual ASCII characters which need\n    to be encoded, we instead just try to force the string to one of\n    escaped unicode or idna depending on whether there are non-ASCII characters\n    present.\n\n    This means that we support things like \u30c9\u30e1\u30a4\u30f3.\u30c6\u30b9\u30c8 as a domain name string.\n\n    More information about IDNA encoding in python is found here__:\n\n    .. __: https://pypi.org/project/idna", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rlmb_ppo_quick():\n  \"\"\"\"\"\"\n  hparams = rlmb_ppo_base()\n  hparams.epochs = 2\n  hparams.model_train_steps = 25000\n  hparams.ppo_epochs_num = 700\n  hparams.ppo_epoch_length = 50\n  return hparams", "output": "Base setting but quicker with only 2 epochs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_stat(cls, obj, filename=None):\n        \"\"\"\n        \n        \"\"\"\n        attr = cls()\n        attr.st_size = obj.st_size\n        attr.st_uid = obj.st_uid\n        attr.st_gid = obj.st_gid\n        attr.st_mode = obj.st_mode\n        attr.st_atime = obj.st_atime\n        attr.st_mtime = obj.st_mtime\n        if filename is not None:\n            attr.filename = filename\n        return attr", "output": "Create an `.SFTPAttributes` object from an existing ``stat`` object (an\n        object returned by `os.stat`).\n\n        :param object obj: an object returned by `os.stat` (or equivalent).\n        :param str filename: the filename associated with this file.\n        :return: new `.SFTPAttributes` object with the same attribute fields.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _match_tag_regex(self, event_tag, search_tag):\n        '''\n        \n        '''\n        return self.cache_regex.get(search_tag).search(event_tag) is not None", "output": "Check if the event_tag matches the search check.\n        Uses regular expression search to check.\n        Return True (matches) or False (no match)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uninstall(packages,\n              cyg_arch='x86_64',\n              mirrors=None):\n    '''\n    \n    '''\n    args = []\n    if packages is not None:\n        args.append('--remove-packages {pkgs}'.format(pkgs=packages))\n        LOG.debug('args: %s', args)\n        if not _check_cygwin_installed(cyg_arch):\n            LOG.debug('We\\'re convinced cygwin isn\\'t installed')\n            return True\n\n    return _run_silent_cygwin(cyg_arch=cyg_arch, args=args, mirrors=mirrors)", "output": "Uninstall one or several packages.\n\n    packages\n        The packages to uninstall.\n\n    cyg_arch : x86_64\n        Specify the architecture to remove the package from\n        Current options are x86 and x86_64\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cyg.uninstall dos2unix\n        salt '*' cyg.uninstall dos2unix mirrors=\"[{'http://mirror': 'http://url/to/public/key}]\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _hex_to_octets(addr):\n    '''\n    \n    '''\n    return '{0}:{1}:{2}:{3}'.format(\n        int(addr[6:8], 16),\n        int(addr[4:6], 16),\n        int(addr[2:4], 16),\n        int(addr[0:2], 16),\n    )", "output": "Convert hex fields from /proc/net/route to octects", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_actions_to_context_menu(self, menu):\r\n        \"\"\"\"\"\"\r\n        inspect_action = create_action(self, _(\"Inspect current object\"),\r\n                                    QKeySequence(get_shortcut('console',\r\n                                                    'inspect current object')),\r\n                                    icon=ima.icon('MessageBoxInformation'),\r\n                                    triggered=self.inspect_object)\r\n\r\n        clear_line_action = create_action(self, _(\"Clear line or block\"),\r\n                                          QKeySequence(get_shortcut(\r\n                                                  'console',\r\n                                                  'clear line')),\r\n                                          triggered=self.clear_line)\r\n\r\n        reset_namespace_action = create_action(self, _(\"Remove all variables\"),\r\n                                               QKeySequence(get_shortcut(\r\n                                                       'ipython_console',\r\n                                                       'reset namespace')),\r\n                                               icon=ima.icon('editdelete'),\r\n                                               triggered=self.reset_namespace)\r\n\r\n        clear_console_action = create_action(self, _(\"Clear console\"),\r\n                                             QKeySequence(get_shortcut('console',\r\n                                                               'clear shell')),\r\n                                             triggered=self.clear_console)\r\n\r\n        quit_action = create_action(self, _(\"&Quit\"), icon=ima.icon('exit'),\r\n                                    triggered=self.exit_callback)\r\n\r\n        add_actions(menu, (None, inspect_action, clear_line_action,\r\n                           clear_console_action, reset_namespace_action,\r\n                           None, quit_action))\r\n        return menu", "output": "Add actions to IPython widget context menu", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hist(self, by=None, bins=10, **kwds):\n        \"\"\"\n        \n        \"\"\"\n        return self(kind='hist', by=by, bins=bins, **kwds)", "output": "Draw one histogram of the DataFrame's columns.\n\n        A histogram is a representation of the distribution of data.\n        This function groups the values of all given Series in the DataFrame\n        into bins and draws all bins in one :class:`matplotlib.axes.Axes`.\n        This is useful when the DataFrame's Series are in a similar scale.\n\n        Parameters\n        ----------\n        by : str or sequence, optional\n            Column in the DataFrame to group by.\n        bins : int, default 10\n            Number of histogram bins to be used.\n        **kwds\n            Additional keyword arguments are documented in\n            :meth:`DataFrame.plot`.\n\n        Returns\n        -------\n        class:`matplotlib.AxesSubplot`\n            Return a histogram plot.\n\n        See Also\n        --------\n        DataFrame.hist : Draw histograms per DataFrame's Series.\n        Series.hist : Draw a histogram with Series' data.\n\n        Examples\n        --------\n        When we draw a dice 6000 times, we expect to get each value around 1000\n        times. But when we draw two dices and sum the result, the distribution\n        is going to be quite different. A histogram illustrates those\n        distributions.\n\n        .. plot::\n            :context: close-figs\n\n            >>> df = pd.DataFrame(\n            ...     np.random.randint(1, 7, 6000),\n            ...     columns = ['one'])\n            >>> df['two'] = df['one'] + np.random.randint(1, 7, 6000)\n            >>> ax = df.plot.hist(bins=12, alpha=0.5)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read(**kwargs):\n    \"\"\"\n    \"\"\"\n    pd_obj = BaseFactory.read_csv(**kwargs)\n    # This happens when `read_csv` returns a TextFileReader object for iterating through\n    if isinstance(pd_obj, pandas.io.parsers.TextFileReader):\n        reader = pd_obj.read\n        pd_obj.read = lambda *args, **kwargs: DataFrame(\n            query_compiler=reader(*args, **kwargs)\n        )\n        return pd_obj\n    return DataFrame(query_compiler=pd_obj)", "output": "Read csv file from local disk.\n    Args:\n        filepath_or_buffer:\n              The filepath of the csv file.\n              We only support local files for now.\n        kwargs: Keyword arguments in pandas.read_csv", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _getUserSid(user):\n    '''\n    \n    '''\n    ret = {}\n\n    sid_pattern = r'^S-1(-\\d+){1,}$'\n\n    if user and re.match(sid_pattern, user, re.I):\n        try:\n            sid = win32security.GetBinarySid(user)\n        except Exception as e:\n            ret['result'] = False\n            ret['comment'] = 'Unable to obtain the binary security identifier for {0}.  The exception was {1}.'.format(\n                user, e)\n        else:\n            try:\n                win32security.LookupAccountSid('', sid)\n                ret['result'] = True\n                ret['sid'] = sid\n            except Exception as e:\n                ret['result'] = False\n                ret['comment'] = 'Unable to lookup the account for the security identifier {0}.  The exception was {1}.'.format(\n                    user, e)\n    else:\n        try:\n            sid = win32security.LookupAccountName('', user)[0] if user else None\n            ret['result'] = True\n            ret['sid'] = sid\n        except Exception as e:\n            ret['result'] = False\n            ret['comment'] = 'Unable to obtain the security identifier for {0}.  The exception was {1}.'.format(\n                user, e)\n    return ret", "output": "return a state error dictionary, with 'sid' as a field if it could be returned\n    if user is None, sid will also be None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dict_to_list_ids(objects):\n    '''\n    \n    '''\n    list_with_ids = []\n    for key, value in six.iteritems(objects):\n        element = {'id': key}\n        element.update(value)\n        list_with_ids.append(element)\n    return list_with_ids", "output": "Convert a dictionary to a list of dictionaries, where each element has\n    a key value pair {'id': key}. This makes it easy to override pillar values\n    while still satisfying the boto api.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait_for_fun(fun, timeout=900, **kwargs):\n    '''\n    \n    '''\n    start = time.time()\n    log.debug('Attempting function %s', fun)\n    trycount = 0\n    while True:\n        trycount += 1\n        try:\n            response = fun(**kwargs)\n            if not isinstance(response, bool):\n                return response\n        except Exception as exc:\n            log.debug('Caught exception in wait_for_fun: %s', exc)\n            time.sleep(1)\n            log.debug('Retrying function %s on  (try %s)', fun, trycount)\n        if time.time() - start > timeout:\n            log.error('Function timed out: %s', timeout)\n            return False", "output": "Wait until a function finishes, or times out", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __assert_true(returned):\n        '''\n        \n        '''\n        result = \"Pass\"\n        try:\n            assert (returned is True), \"{0} not True\".format(returned)\n        except AssertionError as err:\n            result = \"Fail: \" + six.text_type(err)\n        return result", "output": "Test if an boolean is True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_output(self, child_key):\r\n        \"\"\" \r\n        \"\"\"\r\n        data = [x.stats.get(child_key, [0, 0, 0, 0, {}]) for x in self.stats1]\r\n        return (map(self.color_string, islice(zip(*data), 1, 4)))", "output": "Formats the data.\r\n\r\n        self.stats1 contains a list of one or two pstat.Stats() instances, with\r\n        the first being the current run and the second, the saved run, if it\r\n        exists.  Each Stats instance is a dictionary mapping a function to\r\n        5 data points - cumulative calls, number of calls, total time,\r\n        cumulative time, and callers.\r\n\r\n        format_output() converts the number of calls, total time, and\r\n        cumulative time to a string format for the child_key parameter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def argscope(layers, **kwargs):\n    \"\"\"\n    \n\n    \"\"\"\n    if not isinstance(layers, list):\n        layers = [layers]\n\n    # def _check_args_exist(l):\n    #     args = inspect.getargspec(l).args\n    #     for k, v in six.iteritems(kwargs):\n    #         assert k in args, \"No argument {} in {}\".format(k, l.__name__)\n\n    for l in layers:\n        assert hasattr(l, 'symbolic_function'), \"{} is not a registered layer\".format(l.__name__)\n        # _check_args_exist(l.symbolic_function)\n\n    new_scope = copy.copy(get_arg_scope())\n    for l in layers:\n        new_scope[l.__name__].update(kwargs)\n    _ArgScopeStack.append(new_scope)\n    yield\n    del _ArgScopeStack[-1]", "output": "Args:\n        layers (list or layer): layer or list of layers to apply the arguments.\n\n    Returns:\n        a context where all appearance of these layer will by default have the\n        arguments specified by kwargs.\n\n    Example:\n        .. code-block:: python\n\n            with argscope(Conv2D, kernel_shape=3, nl=tf.nn.relu, out_channel=32):\n                x = Conv2D('conv0', x)\n                x = Conv2D('conv1', x)\n                x = Conv2D('conv2', x, out_channel=64)  # override argscope", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fcontext_delete_policy(name, filetype=None, sel_type=None, sel_user=None, sel_level=None):\n    '''\n    \n    '''\n    return _fcontext_add_or_delete_policy('delete', name, filetype, sel_type, sel_user, sel_level)", "output": ".. versionadded:: 2019.2.0\n\n    Deletes the SELinux policy for a given filespec and other optional parameters.\n\n    Returns the result of the call to semanage.\n\n    Note that you don't have to remove an entry before setting a new\n    one for a given filespec and filetype, as adding one with semanage\n    automatically overwrites a previously configured SELinux context.\n\n    name\n        filespec of the file or directory. Regex syntax is allowed.\n\n    file_type\n        The SELinux filetype specification. Use one of [a, f, d, c, b,\n        s, l, p]. See also ``man semanage-fcontext``. Defaults to 'a'\n        (all files).\n\n    sel_type\n        SELinux context type. There are many.\n\n    sel_user\n        SELinux user. Use ``semanage login -l`` to determine which ones\n        are available to you.\n\n    sel_level\n        The MLS range of the SELinux context.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' selinux.fcontext_delete_policy my-policy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def scroll_to_item(self, index):\n        \"\"\"\"\"\"\n        spacing_between_items = self.scene.verticalSpacing()\n        height_view = self.scrollarea.viewport().height()\n        height_item = self.scene.itemAt(index).sizeHint().height()\n        height_view_excluding_item = max(0, height_view - height_item)\n\n        height_of_top_items = spacing_between_items\n        for i in range(index):\n            item = self.scene.itemAt(i)\n            height_of_top_items += item.sizeHint().height()\n            height_of_top_items += spacing_between_items\n\n        pos_scroll = height_of_top_items - height_view_excluding_item // 2\n\n        vsb = self.scrollarea.verticalScrollBar()\n        vsb.setValue(pos_scroll)", "output": "Scroll to the selected item of ThumbnailScrollBar.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def partial(cls, id, token, *, adapter):\n        \"\"\"\n        \"\"\"\n\n        if not isinstance(adapter, WebhookAdapter):\n            raise TypeError('adapter must be a subclass of WebhookAdapter')\n\n        data = {\n            'id': id,\n            'token': token\n        }\n\n        return cls(data, adapter=adapter)", "output": "Creates a partial :class:`Webhook`.\n\n        A partial webhook is just a webhook object with an ID and a token.\n\n        Parameters\n        -----------\n        id: :class:`int`\n            The ID of the webhook.\n        token: :class:`str`\n            The authentication token of the webhook.\n        adapter: :class:`WebhookAdapter`\n            The webhook adapter to use when sending requests. This is\n            typically :class:`AsyncWebhookAdapter` for ``aiohttp`` or\n            :class:`RequestsWebhookAdapter` for ``requests``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_in_jupyter():\n    \"\"\"\n    \"\"\"\n    # https://stackoverflow.com/a/39662359/6400719\n    try:\n        shell = get_ipython().__class__.__name__\n        if shell == \"ZMQInteractiveShell\":\n            return True  # Jupyter notebook or qtconsole\n    except NameError:\n        return False  # Probably standard Python interpreter\n    return False", "output": "Check if user is running spaCy from a Jupyter notebook by detecting the\n    IPython kernel. Mainly used for the displaCy visualizer.\n    RETURNS (bool): True if in Jupyter, False if not.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nasnetalarge(num_classes=1001, pretrained='imagenet'):\n    \n    \"\"\"\n    if pretrained:\n        settings = pretrained_settings['nasnetalarge'][pretrained]\n        assert num_classes == settings['num_classes'], \\\n            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n\n        # both 'imagenet'&'imagenet+background' are loaded from same parameters\n        model = NASNetALarge(num_classes=1001)\n        model.load_state_dict(model_zoo.load_url(settings['url']))\n\n        if pretrained == 'imagenet':\n            new_last_linear = nn.Linear(model.last_linear.in_features, 1000)\n            new_last_linear.weight.data = model.last_linear.weight.data[1:]\n            new_last_linear.bias.data = model.last_linear.bias.data[1:]\n            model.last_linear = new_last_linear\n\n        model.input_space = settings['input_space']\n        model.input_size = settings['input_size']\n        model.input_range = settings['input_range']\n\n        model.mean = settings['mean']\n        model.std = settings['std']\n    else:\n        model = NASNetALarge(num_classes=num_classes)\n    return model", "output": "r\"\"\"NASNetALarge model architecture from the\n    `\"NASNet\" <https://arxiv.org/abs/1707.07012>`_ paper.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def upgrade(name=None,\n            pkgs=None,\n            **kwargs):\n    '''\n    \n    '''\n    old = list_pkgs()\n\n    cmd = ['pkg_add', '-Ix', '-u']\n\n    if kwargs.get('noop', False):\n        cmd.append('-n')\n\n    if pkgs:\n        cmd.extend(pkgs)\n    elif name:\n        cmd.append(name)\n\n    # Now run the upgrade, compare the list of installed packages before and\n    # after and we have all the info we need.\n    result = __salt__['cmd.run_all'](cmd, output_loglevel='trace',\n                                     python_shell=False)\n\n    __context__.pop('pkg.list_pkgs', None)\n    new = list_pkgs()\n    ret = salt.utils.data.compare_dicts(old, new)\n\n    if result['retcode'] != 0:\n        raise CommandExecutionError(\n                'Problem encountered upgrading packages',\n                info={'changes': ret, 'result': result}\n        )\n\n    return ret", "output": "Run a full package upgrade (``pkg_add -u``), or upgrade a specific package\n    if ``name`` or ``pkgs`` is provided.\n    ``name`` is ignored when ``pkgs`` is specified.\n\n    Returns a dictionary containing the changes:\n\n    .. versionadded:: 2019.2.0\n\n    .. code-block:: python\n\n        {'<package>': {'old': '<old-version>',\n                       'new': '<new-version>'}}\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.upgrade\n        salt '*' pkg.upgrade python%2.7", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put(self, x):\n        \"\"\" \"\"\"\n        if self._num_serial > 0 or len(self._threads) == 0:\n            self._num_serial -= 1\n            out = self._parallizable.forward_backward(x)\n            self._out_queue.put(out)\n        else:\n            self._in_queue.put(x)", "output": "Assign input `x` to an available worker and invoke\n        `parallizable.forward_backward` with x.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def subnets_list(virtual_network, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    result = {}\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        subnets = __utils__['azurearm.paged_object_to_list'](\n            netconn.subnets.list(\n                resource_group_name=resource_group,\n                virtual_network_name=virtual_network\n            )\n        )\n\n        for subnet in subnets:\n            result[subnet['name']] = subnet\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    List all subnets within a virtual network.\n\n    :param virtual_network: The virtual network name to list subnets within.\n\n    :param resource_group: The resource group name assigned to the\n        virtual network.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.subnets_list testnet testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prop_set(prop, value, extra_args=None, cibfile=None):\n    '''\n    \n    '''\n    return item_create(item='property',\n                       item_id='{0}={1}'.format(prop, value),\n                       item_type=None,\n                       create='set',\n                       extra_args=extra_args,\n                       cibfile=cibfile)", "output": "Set the value of a cluster property\n\n    prop\n        name of the property\n    value\n        value of the property prop\n    extra_args\n        additional options for the pcs property command\n    cibfile\n        use cibfile instead of the live CIB\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pcs.prop_set prop='no-quorum-policy' value='ignore' cibfile='/tmp/2_node_cluster.cib'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pkginfo(name, version, arch, repoid, install_date=None, install_date_time_t=None):\n    '''\n    \n    '''\n    pkginfo_tuple = collections.namedtuple(\n        'PkgInfo',\n        ('name', 'version', 'arch', 'repoid', 'install_date',\n         'install_date_time_t')\n    )\n    return pkginfo_tuple(name, version, arch, repoid, install_date,\n                         install_date_time_t)", "output": "Build and return a pkginfo namedtuple", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lr_find(self, start_lr=1e-5, end_lr=10, wds=None, linear=False, **kwargs):\n        \"\"\"\n\n        \"\"\"\n        self.save('tmp')\n        layer_opt = self.get_layer_opt(start_lr, wds)\n        self.sched = LR_Finder(layer_opt, len(self.data.trn_dl), end_lr, linear=linear)\n        self.fit_gen(self.model, self.data, layer_opt, 1, **kwargs)\n        self.load('tmp')", "output": "Helps you find an optimal learning rate for a model.\n\n         It uses the technique developed in the 2015 paper\n         `Cyclical Learning Rates for Training Neural Networks`, where\n         we simply keep increasing the learning rate from a very small value,\n         until the loss starts decreasing.\n\n        Args:\n            start_lr (float/numpy array) : Passing in a numpy array allows you\n                to specify learning rates for a learner's layer_groups\n            end_lr (float) : The maximum learning rate to try.\n            wds (iterable/float)\n\n        Examples:\n            As training moves us closer to the optimal weights for a model,\n            the optimal learning rate will be smaller. We can take advantage of\n            that knowledge and provide lr_find() with a starting learning rate\n            1000x smaller than the model's current learning rate as such:\n\n            >> learn.lr_find(lr/1000)\n\n            >> lrs = np.array([ 1e-4, 1e-3, 1e-2 ])\n            >> learn.lr_find(lrs / 1000)\n\n        Notes:\n            lr_find() may finish before going through each batch of examples if\n            the loss decreases enough.\n\n        .. _Cyclical Learning Rates for Training Neural Networks:\n            http://arxiv.org/abs/1506.01186", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dataset(dataset_name):\n    \"\"\"\n    \"\"\"\n    # mnist\n    if dataset == \"mnist\":\n        train_data = gluon.data.DataLoader(\n            gluon.data.vision.MNIST('./data', train=True, transform=transformer),\n            batch_size, shuffle=True, last_batch='discard')\n\n        val_data = gluon.data.DataLoader(\n            gluon.data.vision.MNIST('./data', train=False, transform=transformer),\n            batch_size, shuffle=False)\n    # cifar10\n    elif dataset == \"cifar10\":\n        train_data = gluon.data.DataLoader(\n            gluon.data.vision.CIFAR10('./data', train=True, transform=transformer),\n            batch_size, shuffle=True, last_batch='discard')\n\n        val_data = gluon.data.DataLoader(\n            gluon.data.vision.CIFAR10('./data', train=False, transform=transformer),\n            batch_size, shuffle=False)\n\n    return train_data, val_data", "output": "Load the dataset and split it to train/valid data\n\n    :param dataset_name: string\n\n    Returns:\n    train_data: int array\n        training dataset\n    val_data: int array\n        valid dataset", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cachedir_index_add(minion_id, profile, driver, provider, base=None):\n    '''\n    \n    '''\n    base = init_cachedir(base)\n    index_file = os.path.join(base, 'index.p')\n    lock_file(index_file)\n\n    if os.path.exists(index_file):\n        mode = 'rb' if six.PY3 else 'r'\n        with salt.utils.files.fopen(index_file, mode) as fh_:\n            index = salt.utils.data.decode(\n                salt.utils.msgpack.msgpack.load(\n                    fh_, encoding=MSGPACK_ENCODING))\n    else:\n        index = {}\n\n    prov_comps = provider.split(':')\n\n    index.update({\n        minion_id: {\n            'id': minion_id,\n            'profile': profile,\n            'driver': driver,\n            'provider': prov_comps[0],\n        }\n    })\n\n    mode = 'wb' if six.PY3 else 'w'\n    with salt.utils.files.fopen(index_file, mode) as fh_:\n        salt.utils.msgpack.dump(index, fh_, encoding=MSGPACK_ENCODING)\n\n    unlock_file(index_file)", "output": "Add an entry to the cachedir index. This generally only needs to happen when\n    a new instance is created. This entry should contain:\n\n    .. code-block:: yaml\n\n        - minion_id\n        - profile used to create the instance\n        - provider and driver name\n\n    The intent of this function is to speed up lookups for the cloud roster for\n    salt-ssh. However, other code that makes use of profile information can also\n    make use of this function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _piecewise_learning_rate(step, boundaries, values):\n  \"\"\"\n  \"\"\"\n  values = [1.0] + values\n  boundaries = [float(x) for x in boundaries]\n  return tf.train.piecewise_constant(\n      step, boundaries, values, name=\"piecewise_lr\")", "output": "Scale learning rate according to the given schedule.\n\n  Multipliers are not cumulative.\n\n  Args:\n    step: global step\n    boundaries: List of steps to transition on.\n    values: Multiplier to apply at each boundary transition.\n\n  Returns:\n    Scaled value for the learning rate.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dump_cfg(cfg_file):\n    ''''''\n    if __salt__['file.file_exists'](cfg_file):\n        with salt.utils.files.fopen(cfg_file, 'r') as fp_:\n            log.debug(\n                \"zonecfg - configuration file:\\n%s\",\n                    \"\".join(salt.utils.data.decode(fp_.readlines()))\n            )", "output": "Internal helper for debugging cfg files", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def diabetes(display=False):\n    \"\"\"  \"\"\"\n\n    d = sklearn.datasets.load_diabetes()\n    df = pd.DataFrame(data=d.data, columns=d.feature_names) # pylint: disable=E1101\n    return df, d.target", "output": "Return the diabetes data in a nice package.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nameservers(*ns):\n    '''\n    \n    '''\n    if len(ns) > 2:\n        log.warning('racadm only supports two nameservers')\n        return False\n\n    for i in range(1, len(ns) + 1):\n        if not __execute_cmd('config -g cfgLanNetworking -o \\\n                cfgDNSServer{0} {1}'.format(i, ns[i - 1])):\n            return False\n\n    return True", "output": "Configure the nameservers on the DRAC\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt dell drac.nameservers [NAMESERVERS]\n        salt dell drac.nameservers ns1.example.com ns2.example.com", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_logger(logger_file_path, log_level_name='info'):\n    \"\"\"\n    \"\"\"\n    log_level = log_level_map.get(log_level_name, logging.INFO)\n    logger_file = open(logger_file_path, 'w')\n    fmt = '[%(asctime)s] %(levelname)s (%(name)s/%(threadName)s) %(message)s'\n    logging.Formatter.converter = time.localtime\n    formatter = logging.Formatter(fmt, _time_format)\n    handler = logging.StreamHandler(logger_file)\n    handler.setFormatter(formatter)\n\n    root_logger = logging.getLogger()\n    root_logger.addHandler(handler)\n    root_logger.setLevel(log_level)\n\n    # these modules are too verbose\n    logging.getLogger('matplotlib').setLevel(log_level)\n\n    sys.stdout = _LoggerFileWrapper(logger_file)", "output": "Initialize root logger.\n    This will redirect anything from logging.getLogger() as well as stdout to specified file.\n    logger_file_path: path of logger file (path-like object).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_repositories(self, node):\n        '''\n        \n        '''\n        priority = 99\n\n        for repo_id, repo_data in self._data.software.get('repositories', {}).items():\n            if type(repo_data) == list:\n                repo_data = repo_data[0]\n            if repo_data.get('enabled') or not repo_data.get('disabled'):  # RPM and Debian, respectively\n                uri = repo_data.get('baseurl', repo_data.get('uri'))\n                if not uri:\n                    continue\n                repo = etree.SubElement(node, 'repository')\n                if self.__grains__.get('os_family') in ('Kali', 'Debian'):\n                    repo.set('alias', repo_id)\n                    repo.set('distribution', repo_data['dist'])\n                else:\n                    repo.set('alias', repo_data['alias'])\n                    if self.__grains__.get('os_family', '') == 'Suse':\n                        repo.set('type', 'yast2')  # TODO: Check for options!\n                    repo.set('priority', str(priority))\n                source = etree.SubElement(repo, 'source')\n                source.set('path', uri)  # RPM and Debian, respectively\n                priority -= 1", "output": "Create repositories.\n\n        :param node:\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_episode_begin(self, episode, logs):\n        \"\"\"  \"\"\"\n        self.episode_start[episode] = timeit.default_timer()\n        self.observations[episode] = []\n        self.rewards[episode] = []\n        self.actions[episode] = []\n        self.metrics[episode] = []", "output": "Reset environment variables at beginning of each episode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def missing_fun_string(self, function_name):\n        '''\n        \n        '''\n        mod_name = function_name.split('.')[0]\n        if mod_name in self.loaded_modules:\n            return '\\'{0}\\' is not available.'.format(function_name)\n        else:\n            try:\n                reason = self.missing_modules[mod_name]\n            except KeyError:\n                return '\\'{0}\\' is not available.'.format(function_name)\n            else:\n                if reason is not None:\n                    return '\\'{0}\\' __virtual__ returned False: {1}'.format(mod_name, reason)\n                else:\n                    return '\\'{0}\\' __virtual__ returned False'.format(mod_name)", "output": "Return the error string for a missing function.\n\n        This can range from \"not available' to \"__virtual__\" returned False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def js_link(self, attr, other, other_attr):\n        ''' \n\n        '''\n        if attr not in self.properties():\n            raise ValueError(\"%r is not a property of self (%r)\" % (attr, self))\n\n        if not isinstance(other, Model):\n            raise ValueError(\"'other' is not a Bokeh model: %r\" % other)\n\n        if other_attr not in other.properties():\n            raise ValueError(\"%r is not a property of other (%r)\" % (other_attr, other))\n\n        from bokeh.models.callbacks import CustomJS\n        cb = CustomJS(args=dict(other=other), code=\"other.%s = this.%s\" % (other_attr, attr))\n\n        self.js_on_change(attr, cb)", "output": "Link two Bokeh model properties using JavaScript.\n\n        This is a convenience method that simplifies adding a CustomJS callback\n        to update one Bokeh model property whenever another changes value.\n\n        Args:\n\n            attr (str) :\n                The name of a Bokeh property on this model\n\n            other (Model):\n                A Bokeh model to link to self.attr\n\n            other_attr (str) :\n                The property on ``other`` to link together\n\n        Added in version 1.1\n\n        Raises:\n\n            ValueError\n\n        Examples:\n\n            This code with ``js_link``:\n\n            .. code :: python\n\n                select.js_link('value', plot, 'sizing_mode')\n\n            is equivalent to the following:\n\n            .. code:: python\n\n                from bokeh.models import CustomJS\n                select.js_on_change('value',\n                    CustomJS(args=dict(other=plot),\n                             code=\"other.sizing_mode = this.value\"\n                    )\n                )", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def contextMenuEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        state = self.has_selected_text()\r\n        self.copy_action.setEnabled(state)\r\n        self.cut_action.setEnabled(state)\r\n        self.delete_action.setEnabled(state)\r\n        self.menu.popup(event.globalPos())\r\n        event.accept()", "output": "Reimplement Qt method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def all_table_names_in_schema(self, schema, cache=False,\n                                  cache_timeout=None, force=False):\n        \"\"\"\n        \"\"\"\n        tables = []\n        try:\n            tables = self.db_engine_spec.get_table_names(\n                inspector=self.inspector, schema=schema)\n        except Exception as e:\n            logging.exception(e)\n        return tables", "output": "Parameters need to be passed as keyword arguments.\n\n        For unused parameters, they are referenced in\n        cache_util.memoized_func decorator.\n\n        :param schema: schema name\n        :type schema: str\n        :param cache: whether cache is enabled for the function\n        :type cache: bool\n        :param cache_timeout: timeout in seconds for the cache\n        :type cache_timeout: int\n        :param force: whether to force refresh the cache\n        :type force: bool\n        :return: table list\n        :rtype: list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clean_str(string):\n    \"\"\"\n    \"\"\"\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", r\" \\( \", string)\n    string = re.sub(r\"\\)\", r\" \\) \", string)\n    string = re.sub(r\"\\?\", r\" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    return string.strip().lower()", "output": "Tokenization/string cleaning for all datasets except for SST.\n    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dist_is_editable(dist):\n    # type: (Distribution) -> bool\n    \"\"\"\n    \n    \"\"\"\n    for path_item in sys.path:\n        egg_link = os.path.join(path_item, dist.project_name + '.egg-link')\n        if os.path.isfile(egg_link):\n            return True\n    return False", "output": "Return True if given Distribution is an editable install.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def GlorotNormalInitializer(out_dim=0, in_dim=1, scale=onp.sqrt(2)):\n  \"\"\"\"\"\"\n  def init(shape, rng):\n    fan_in, fan_out = shape[in_dim], shape[out_dim]\n    size = onp.prod(onp.delete(shape, [in_dim, out_dim]))\n    std = scale / np.sqrt((fan_in + fan_out) / 2. * size)\n    return (std * backend.random.normal(rng, shape)).astype('float32')\n  return init", "output": "An initializer function for random Glorot-scaled coefficients.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _netstat_route_netbsd():\n    '''\n    \n    '''\n    ret = []\n    cmd = 'netstat -f inet -rn | tail -n+5'\n    out = __salt__['cmd.run'](cmd, python_shell=True)\n    for line in out.splitlines():\n        comps = line.split()\n        ret.append({\n            'addr_family': 'inet',\n            'destination': comps[0],\n            'gateway': comps[1],\n            'netmask': '',\n            'flags': comps[3],\n            'interface': comps[6]})\n    cmd = 'netstat -f inet6 -rn | tail -n+5'\n    out = __salt__['cmd.run'](cmd, python_shell=True)\n    for line in out.splitlines():\n        comps = line.split()\n        ret.append({\n            'addr_family': 'inet6',\n            'destination': comps[0],\n            'gateway': comps[1],\n            'netmask': '',\n            'flags': comps[3],\n            'interface': comps[6]})\n    return ret", "output": "Return netstat routing information for NetBSD", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _docspec_comments(obj) -> Dict[str, str]:\n    \"\"\"\n    \n    \"\"\"\n    # Sometimes our docstring is on the class, and sometimes it's on the initializer,\n    # so we've got to check both.\n    class_docstring = getattr(obj, '__doc__', None)\n    init_docstring = getattr(obj.__init__, '__doc__', None) if hasattr(obj, '__init__') else None\n\n    docstring = class_docstring or init_docstring or ''\n\n    doc = NumpyDocString(docstring)\n    params = doc[\"Parameters\"]\n    comments: Dict[str, str] = {}\n\n    for line in params:\n        # It looks like when there's not a space after the parameter name,\n        # numpydocstring parses it incorrectly.\n        name_bad = line[0]\n        name = name_bad.split(\":\")[0]\n\n        # Sometimes the line has 3 fields, sometimes it has 4 fields.\n        comment = \"\\n\".join(line[-1])\n\n        comments[name] = comment\n\n    return comments", "output": "Inspect the docstring and get the comments for each parameter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_quota(self, tenant_id, subnet=None, router=None,\n                     network=None, floatingip=None, port=None,\n                     sec_grp=None, sec_grp_rule=None):\n        '''\n        \n        '''\n        body = {}\n        if subnet:\n            body['subnet'] = subnet\n        if router:\n            body['router'] = router\n        if network:\n            body['network'] = network\n        if floatingip:\n            body['floatingip'] = floatingip\n        if port:\n            body['port'] = port\n        if sec_grp:\n            body['security_group'] = sec_grp\n        if sec_grp_rule:\n            body['security_group_rule'] = sec_grp_rule\n        return self.network_conn.update_quota(tenant_id=tenant_id,\n                                              body={'quota': body})", "output": "Update a tenant's quota", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(cls, sc, path):\n        \"\"\"\n        \n        \"\"\"\n        model = cls._load_java(sc, path)\n        wrapper =\\\n            sc._jvm.org.apache.spark.mllib.api.python.PowerIterationClusteringModelWrapper(model)\n        return PowerIterationClusteringModel(wrapper)", "output": "Load a model from the given path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_ipsec_site_connection(self, ipsec_site_connection):\n        '''\n        \n        '''\n        ipsec_site_connection_id = self._find_ipsec_site_connection_id(\n            ipsec_site_connection)\n        ret = self.network_conn.delete_ipsec_site_connection(\n            ipsec_site_connection_id)\n        return ret if ret else True", "output": "Deletes the specified IPsecSiteConnection", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_hparam_extractor(hparam_name):\n  \"\"\"\n  \"\"\"\n  def extractor_fn(session_group):\n    if hparam_name in session_group.hparams:\n      return _value_to_python(session_group.hparams[hparam_name])\n    return None\n\n  return extractor_fn", "output": "Returns an extractor function that extracts an hparam from a session group.\n\n  Args:\n    hparam_name: str. Identies the hparam to extract from the session group.\n  Returns:\n    A function that takes a tensorboard.hparams.SessionGroup protobuffer and\n    returns the value, as a native Python object, of the hparam identified by\n    'hparam_name'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def element_slice(self, start=None, stop=None, step=None):\n        \"\"\"\n        \n\n        \"\"\"\n        if self.dtype not in [str, array.array, list]:\n            raise TypeError(\"SArray must contain strings, arrays or lists\")\n        with cython_context():\n            return SArray(_proxy=self.__proxy__.subslice(start, step, stop))", "output": "This returns an SArray with each element sliced accordingly to the\n        slice specified. This is conceptually equivalent to:\n\n        >>> g.apply(lambda x: x[start:step:stop])\n\n        The SArray must be of type list, vector, or string.\n\n        For instance:\n\n        >>> g = SArray([\"abcdef\",\"qwerty\"])\n        >>> g.element_slice(start=0, stop=2)\n        dtype: str\n        Rows: 2\n        [\"ab\", \"qw\"]\n        >>> g.element_slice(3,-1)\n        dtype: str\n        Rows: 2\n        [\"de\", \"rt\"]\n        >>> g.element_slice(3)\n        dtype: str\n        Rows: 2\n        [\"def\", \"rty\"]\n\n        >>> g = SArray([[1,2,3], [4,5,6]])\n        >>> g.element_slice(0, 1)\n        dtype: str\n        Rows: 2\n        [[1], [4]]\n\n        Parameters\n        ----------\n        start : int or None (default)\n            The start position of the slice\n\n        stop: int or None (default)\n            The stop position of the slice\n\n        step: int or None (default)\n            The step size of the slice\n\n        Returns\n        -------\n        out : SArray\n            Each individual vector/string/list sliced according to the arguments.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_cname(name=None, canonical=None, **api_opts):\n    '''\n    \n    '''\n    cname = get_cname(name=name, canonical=canonical, **api_opts)\n    if cname:\n        return delete_object(cname['_ref'], **api_opts)\n    return True", "output": "Delete CNAME. This is a helper call to delete_object.\n\n    If record is not found, return True\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-call infoblox.delete_cname name=example.example.com\n        salt-call infoblox.delete_cname canonical=example-ha-0.example.com", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_elementwise_name_from_keras_layer(keras_layer):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(keras_layer, _keras.layers.Add):\n        return 'ADD'\n    elif isinstance(keras_layer, _keras.layers.Multiply):\n        return 'MULTIPLY'\n    elif isinstance(keras_layer, _keras.layers.Concatenate):\n        if len(keras_layer.input_shape[0]) == 3 and (keras_layer.axis == 1 or keras_layer.axis == -2):\n            return 'SEQUENCE_CONCAT'\n        elif len(keras_layer.input_shape[0]) == 4 and (keras_layer.axis == 3 or keras_layer.axis == -1):\n            return 'CONCAT'\n        elif len(keras_layer.input_shape[0]) == 2 and (keras_layer.axis == 1 or keras_layer.axis == -1):\n            return 'CONCAT'\n        else:\n            raise ValueError('Only channel and sequence concatenation are supported.')\n    elif isinstance(keras_layer, _keras.layers.Dot):\n        if len(keras_layer.input_shape[0]) == 2:\n            if type(keras_layer.axes) is list or type(keras_layer.axes) is tuple:\n                if len(keras_layer.axes) > 1:\n                    raise ValueError('Only vector dot-product is supported.')\n                else:\n                    axis = keras_layer.axes[0]\n            else:\n                axis = keras_layer.axes\n            if axis != -1 and axis != 1:\n                 raise ValueError('Only vector dot-product is supported.')\n\n            if keras_layer.normalize:\n                return 'COS'\n            else:\n                return 'DOT'\n        else:\n            raise ValueError('Only vector dot-product is supported.')\n    elif isinstance(keras_layer, _keras.layers.Maximum):\n        return 'MAX'\n    elif isinstance(keras_layer, _keras.layers.Average):\n        return 'AVE'\n    else:\n        _utils.raise_error_unsupported_option(str(type(keras_layer)), 'merge',\n                keras_layer.name)", "output": "Get the keras layer name from the activation name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_doc(elt, doc_string:bool=True, full_name:str=None, arg_comments:dict=None, title_level=None, alt_doc_string:str='',\n             ignore_warn:bool=False, markdown=True, show_tests=True):\n    \"\"\n    arg_comments = ifnone(arg_comments, {})\n    anchor_id = get_anchor(elt)\n    elt = getattr(elt, '__func__', elt)\n    full_name = full_name or fn_name(elt)\n    if inspect.isclass(elt):\n        if is_enum(elt.__class__):   name,args = get_enum_doc(elt, full_name)\n        else:                        name,args = get_cls_doc(elt, full_name)\n    elif isinstance(elt, Callable):  name,args = format_ft_def(elt, full_name)\n    else: raise Exception(f'doc definition not supported for {full_name}')\n    source_link = get_function_source(elt) if is_fastai_class(elt) else \"\"\n    test_link, test_modal = get_pytest_html(elt, anchor_id=anchor_id) if show_tests else ('', '')\n    title_level = ifnone(title_level, 2 if inspect.isclass(elt) else 4)\n    doc =  f'<h{title_level} id=\"{anchor_id}\" class=\"doc_header\">{name}{source_link}{test_link}</h{title_level}>'\n    doc += f'\\n\\n> {args}\\n\\n'\n    doc += f'{test_modal}'\n    if doc_string and (inspect.getdoc(elt) or arg_comments):\n        doc += format_docstring(elt, arg_comments, alt_doc_string, ignore_warn) + ' '\n    if markdown: display(Markdown(doc))\n    else: return doc", "output": "Show documentation for element `elt`. Supported types: class, Callable, and enum.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_model(name, **overrides):\n    \"\"\"\n    \"\"\"\n    data_path = get_data_path()\n    if not data_path or not data_path.exists():\n        raise IOError(Errors.E049.format(path=path2str(data_path)))\n    if isinstance(name, basestring_):  # in data dir / shortcut\n        if name in set([d.name for d in data_path.iterdir()]):\n            return load_model_from_link(name, **overrides)\n        if is_package(name):  # installed as package\n            return load_model_from_package(name, **overrides)\n        if Path(name).exists():  # path to model data directory\n            return load_model_from_path(Path(name), **overrides)\n    elif hasattr(name, \"exists\"):  # Path or Path-like to model data\n        return load_model_from_path(name, **overrides)\n    raise IOError(Errors.E050.format(name=name))", "output": "Load a model from a shortcut link, package or data path.\n\n    name (unicode): Package name, shortcut link or model path.\n    **overrides: Specific overrides, like pipeline components to disable.\n    RETURNS (Language): `Language` class with the loaded model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_cors(Bucket,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.delete_bucket_cors(Bucket=Bucket)\n        return {'deleted': True, 'name': Bucket}\n    except ClientError as e:\n        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Delete the CORS configuration for the given bucket\n\n    Returns {deleted: true} if CORS was deleted and returns\n    {deleted: False} if CORS was not deleted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.delete_cors my_bucket", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_new_osx():\n    \"\"\"\"\"\"\n    name = distutils.util.get_platform()\n    if sys.platform != \"darwin\":\n        return False\n    elif name.startswith(\"macosx-10\"):\n        minor_version = int(name.split(\"-\")[1].split(\".\")[1])\n        if minor_version >= 7:\n            return True\n        else:\n            return False\n    else:\n        return False", "output": "Check whether we're on OSX >= 10.10", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pin_in_object_store(obj):\n    \"\"\"\n    \"\"\"\n\n    obj_id = ray.put(_to_pinnable(obj))\n    _pinned_objects.append(ray.get(obj_id))\n    return \"{}{}\".format(PINNED_OBJECT_PREFIX,\n                         base64.b64encode(obj_id.binary()).decode(\"utf-8\"))", "output": "Pin an object in the object store.\n\n    It will be available as long as the pinning process is alive. The pinned\n    object can be retrieved by calling get_pinned_object on the identifier\n    returned by this call.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def json(self):\n        \"\"\"\"\"\"\n        if hasattr(self, '_json'):\n            return self._json\n        try:\n            self._json = json.loads(self.text or self.content)\n        except ValueError:\n            self._json = None\n        return self._json", "output": "Returns the json-encoded content of the response, if any.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_cookie(self, name: str, default: str = None) -> Optional[str]:\n        \"\"\"\n        \"\"\"\n        if self.request.cookies is not None and name in self.request.cookies:\n            return self.request.cookies[name].value\n        return default", "output": "Returns the value of the request cookie with the given name.\n\n        If the named cookie is not present, returns ``default``.\n\n        This method only returns cookies that were present in the request.\n        It does not see the outgoing cookies set by `set_cookie` in this\n        handler.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_elapsed(self, restart=True):\n        '''\n        \n        '''\n        end = time.time()\n        span = end - self.__start\n        if restart:\n            self.__start = end\n        return span", "output": "Calculate time span.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(self, client=None):\n        \"\"\"\n        \"\"\"\n        client = self._require_client(client)\n        # We only need the status code (200 or not) so we seek to\n        # minimize the returned payload.\n        query_params = self._query_params\n        query_params[\"fields\"] = \"name\"\n\n        try:\n            # We intentionally pass `_target_object=None` since fields=name\n            # would limit the local properties.\n            client._connection.api_request(\n                method=\"GET\",\n                path=self.path,\n                query_params=query_params,\n                _target_object=None,\n            )\n            # NOTE: This will not fail immediately in a batch. However, when\n            #       Batch.finish() is called, the resulting `NotFound` will be\n            #       raised.\n            return True\n        except NotFound:\n            return False", "output": "Determines whether or not this blob exists.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: Optional. The client to use.  If not passed, falls back\n                       to the ``client`` stored on the blob's bucket.\n\n        :rtype: bool\n        :returns: True if the blob exists in Cloud Storage.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def action(\n        self,\n        fun=None,\n        cloudmap=None,\n        names=None,\n        provider=None,\n        instance=None,\n        kwargs=None\n    ):\n        '''\n        \n        '''\n        if kwargs is None:\n            kwargs = {}\n\n        mapper = salt.cloud.Map(self._opts_defaults(\n            action=fun,\n            names=names,\n            **kwargs))\n        if instance:\n            if names:\n                raise SaltCloudConfigError(\n                    'Please specify either a list of \\'names\\' or a single '\n                    '\\'instance\\', but not both.'\n                )\n            names = [instance]\n\n        if names and not provider:\n            self.opts['action'] = fun\n            return mapper.do_action(names, kwargs)\n\n        if provider and not names:\n            return mapper.do_function(provider, fun, kwargs)\n        else:\n            # This should not be called without either an instance or a\n            # provider. If both an instance/list of names and a provider\n            # are given, then we also need to exit. We can only have one\n            # or the other.\n            raise SaltCloudConfigError(\n                'Either an instance (or list of names) or a provider must be '\n                'specified, but not both.'\n            )", "output": "Execute a single action via the cloud plugin backend\n\n        Examples:\n\n        .. code-block:: python\n\n            client.action(fun='show_instance', names=['myinstance'])\n            client.action(fun='show_image', provider='my-ec2-config',\n                kwargs={'image': 'ami-10314d79'}\n            )", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hincrby(key, field, increment=1, host=None, port=None, db=None, password=None):\n    '''\n    \n    '''\n    server = _connect(host, port, db, password)\n    return server.hincrby(key, field, amount=increment)", "output": "Increment the integer value of a hash field by the given number.\n\n    .. versionadded:: 2017.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.hincrby foo_hash bar_field 5", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_cli(ctx, template):\n    \"\"\"\n    \n    \"\"\"\n\n    sam_template = _read_sam_file(template)\n\n    iam_client = boto3.client('iam')\n    validator = SamTemplateValidator(sam_template, ManagedPolicyLoader(iam_client))\n\n    try:\n        validator.is_valid()\n    except InvalidSamDocumentException as e:\n        click.secho(\"Template provided at '{}' was invalid SAM Template.\".format(template), bg='red')\n        raise InvalidSamTemplateException(str(e))\n    except NoCredentialsError as e:\n        raise UserException(\"AWS Credentials are required. Please configure your credentials.\")\n\n    click.secho(\"{} is a valid SAM Template\".format(template), fg='green')", "output": "Implementation of the ``cli`` method, just separated out for unit testing purposes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def holding_pnl(self):\n        \"\"\"\n        \n        \"\"\"\n        return sum(position.holding_pnl for position in six.itervalues(self._positions))", "output": "[float] \u6d6e\u52a8\u76c8\u4e8f", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pairwise_intersection(boxlist1, boxlist2):\n    \"\"\"\n    \"\"\"\n    x_min1, y_min1, x_max1, y_max1 = tf.split(boxlist1, 4, axis=1)\n    x_min2, y_min2, x_max2, y_max2 = tf.split(boxlist2, 4, axis=1)\n    all_pairs_min_ymax = tf.minimum(y_max1, tf.transpose(y_max2))\n    all_pairs_max_ymin = tf.maximum(y_min1, tf.transpose(y_min2))\n    intersect_heights = tf.maximum(0.0, all_pairs_min_ymax - all_pairs_max_ymin)\n    all_pairs_min_xmax = tf.minimum(x_max1, tf.transpose(x_max2))\n    all_pairs_max_xmin = tf.maximum(x_min1, tf.transpose(x_min2))\n    intersect_widths = tf.maximum(0.0, all_pairs_min_xmax - all_pairs_max_xmin)\n    return intersect_heights * intersect_widths", "output": "Compute pairwise intersection areas between boxes.\n\n    Args:\n      boxlist1: Nx4 floatbox\n      boxlist2: Mx4\n\n    Returns:\n      a tensor with shape [N, M] representing pairwise intersections", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def concat(*cols):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.concat(_to_seq(sc, cols, _to_java_column)))", "output": "Concatenates multiple input columns together into a single column.\n    The function works with strings, binary and compatible array columns.\n\n    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n    >>> df.select(concat(df.s, df.d).alias('s')).collect()\n    [Row(s=u'abcd123')]\n\n    >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])\n    >>> df.select(concat(df.a, df.b, df.c).alias(\"arr\")).collect()\n    [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send(self, s):\n        \"\"\"\n        \n        \"\"\"\n\n        m = Message()\n        m.add_byte(cMSG_CHANNEL_DATA)\n        m.add_int(self.remote_chanid)\n        return self._send(s, m)", "output": "Send data to the channel.  Returns the number of bytes sent, or 0 if\n        the channel stream is closed.  Applications are responsible for\n        checking that all data has been sent: if only some of the data was\n        transmitted, the application needs to attempt delivery of the remaining\n        data.\n\n        :param str s: data to send\n        :return: number of bytes actually sent, as an `int`\n\n        :raises socket.timeout: if no data could be sent before the timeout set\n            by `settimeout`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lm1b_preprocess(dataset, training,\n                    max_target_length=-1, max_eval_target_length=-1):\n  \"\"\"\"\"\"\n\n  def target_right_length(_, target):\n    return tf.less(tf.shape(target)[0], max_target_length + 1)\n\n  def eval_target_right_length(_, target):\n    return tf.less(tf.shape(target)[0], max_eval_target_length + 1)\n\n  if max_target_length > 0 and training:\n    dataset = dataset.filter(target_right_length)\n\n  if max_eval_target_length > 0 and not training:\n    dataset = dataset.filter(eval_target_right_length)\n\n  return dataset", "output": "Preprocessing for LM1B: filter out targets exceeding maximum length.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def _create_connection(self, req: 'ClientRequest',\n                                 traces: List['Trace'],\n                                 timeout: 'ClientTimeout') -> ResponseHandler:\n        \"\"\"\n        \"\"\"\n        if req.proxy:\n            _, proto = await self._create_proxy_connection(\n                req, traces, timeout)\n        else:\n            _, proto = await self._create_direct_connection(\n                req, traces, timeout)\n\n        return proto", "output": "Create connection.\n\n        Has same keyword arguments as BaseEventLoop.create_connection.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_semod():\n    '''\n    \n    '''\n    helptext = __salt__['cmd.run']('semodule -h').splitlines()\n    semodule_version = ''\n    for line in helptext:\n        if line.strip().startswith('full'):\n            semodule_version = 'new'\n\n    if semodule_version == 'new':\n        mdata = __salt__['cmd.run']('semodule -lfull').splitlines()\n        ret = {}\n        for line in mdata:\n            if not line.strip():\n                continue\n            comps = line.split()\n            if len(comps) == 4:\n                ret[comps[1]] = {'Enabled': False,\n                                 'Version': None}\n            else:\n                ret[comps[1]] = {'Enabled': True,\n                                 'Version': None}\n    else:\n        mdata = __salt__['cmd.run']('semodule -l').splitlines()\n        ret = {}\n        for line in mdata:\n            if not line.strip():\n                continue\n            comps = line.split()\n            if len(comps) == 3:\n                ret[comps[0]] = {'Enabled': False,\n                                 'Version': comps[1]}\n            else:\n                ret[comps[0]] = {'Enabled': True,\n                                 'Version': comps[1]}\n    return ret", "output": "Return a structure listing all of the selinux modules on the system and\n    what state they are in\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' selinux.list_semod\n\n    .. versionadded:: 2016.3.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode(input, fallback_encoding, errors='replace'):\n    \"\"\"\n    \n\n    \"\"\"\n    # Fail early if `encoding` is an invalid label.\n    fallback_encoding = _get_encoding(fallback_encoding)\n    bom_encoding, input = _detect_bom(input)\n    encoding = bom_encoding or fallback_encoding\n    return encoding.codec_info.decode(input, errors)[0], encoding", "output": "Decode a single string.\n\n    :param input: A byte string\n    :param fallback_encoding:\n        An :class:`Encoding` object or a label string.\n        The encoding to use if :obj:`input` does note have a BOM.\n    :param errors: Type of error handling. See :func:`codecs.register`.\n    :raises: :exc:`~exceptions.LookupError` for an unknown encoding label.\n    :return:\n        A ``(output, encoding)`` tuple of an Unicode string\n        and an :obj:`Encoding`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def periodic_ping(self) -> None:\n        \"\"\"\n        \"\"\"\n        if self.is_closing() and self.ping_callback is not None:\n            self.ping_callback.stop()\n            return\n\n        # Check for timeout on pong. Make sure that we really have\n        # sent a recent ping in case the machine with both server and\n        # client has been suspended since the last ping.\n        now = IOLoop.current().time()\n        since_last_pong = now - self.last_pong\n        since_last_ping = now - self.last_ping\n        assert self.ping_interval is not None\n        assert self.ping_timeout is not None\n        if (\n            since_last_ping < 2 * self.ping_interval\n            and since_last_pong > self.ping_timeout\n        ):\n            self.close()\n            return\n\n        self.write_ping(b\"\")\n        self.last_ping = now", "output": "Send a ping to keep the websocket alive\n\n        Called periodically if the websocket_ping_interval is set and non-zero.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def if_unmodified_since(self) -> Optional[datetime.datetime]:\n        \"\"\"\n        \"\"\"\n        return self._http_date(self.headers.get(hdrs.IF_UNMODIFIED_SINCE))", "output": "The value of If-Unmodified-Since HTTP header, or None.\n\n        This header is represented as a `datetime` object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main():\n    \"\"\"\n    \n    \"\"\"\n    source_lines = (line.rstrip() for line in sys.stdin)\n    console = InteractiveInterpreter()\n    console.runsource('import turicreate')\n    source = ''\n    try:\n        while True:\n            source = source_lines.next()\n            more = console.runsource(source)\n            while more:\n                next_line = source_lines.next()\n                print '...', next_line\n                source += '\\n' + next_line\n                more = console.runsource(source)\n    except StopIteration:\n        if more:\n            print '... '\n            more = console.runsource(source + '\\n')", "output": "Print lines of input along with output.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def key_value(minion_id,\n              pillar,  # pylint: disable=W0613\n              pillar_key='redis_pillar'):\n    '''\n    \n    '''\n    # Identify key type and process as needed based on that type\n    key_type = __salt__['redis.key_type'](minion_id)\n    if key_type == 'string':\n        return {pillar_key: __salt__['redis.get_key'](minion_id)}\n    elif key_type == 'hash':\n        return {pillar_key: __salt__['redis.hgetall'](minion_id)}\n    elif key_type == 'list':\n        list_size = __salt__['redis.llen'](minion_id)\n        if not list_size:\n            return {}\n        return {pillar_key: __salt__['redis.lrange'](minion_id, 0,\n                                                     list_size - 1)}\n    elif key_type == 'set':\n        return {pillar_key: __salt__['redis.smembers'](minion_id)}\n    elif key_type == 'zset':\n        set_size = __salt__['redis.zcard'](minion_id)\n        if not set_size:\n            return {}\n        return {pillar_key: __salt__['redis.zrange'](minion_id, 0,\n                                                     set_size - 1)}\n    # Return nothing for unhandled types\n    return {}", "output": "Looks for key in redis matching minion_id, returns a structure based on the\n    data type of the redis key. String for string type, dict for hash type and\n    lists for lists, sets and sorted sets.\n\n    pillar_key\n        Pillar key to return data into", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def graph_repr(self):\n        \"\"\"\"\"\"\n\n        # Replace any floating point numbers in the expression\n        # with their scientific notation\n        final = re.sub(r\"[-+]?\\d*\\.\\d+\",\n                       lambda x: format(float(x.group(0)), '.2E'),\n                       self._expr)\n        # Graphviz interprets `\\l` as \"divide label into lines, left-justified\"\n        return \"Expression:\\\\l  {}\\\\l\".format(\n            final,\n        )", "output": "Short repr to use when rendering Pipeline graphs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_model(PARAMS):\n    ''''''\n    model_dict = {\n        'LinearRegression': LinearRegression(),\n        'SVR': SVR(),\n        'KNeighborsRegressor': KNeighborsRegressor(),\n        'DecisionTreeRegressor': DecisionTreeRegressor()\n    }\n    if not model_dict.get(PARAMS['model_name']):\n        LOG.exception('Not supported model!')\n        exit(1)\n    \n    model = model_dict[PARAMS['model_name']]\n    \n    try:\n        if PARAMS['model_name'] == 'SVR':\n            model.kernel = PARAMS['svr_kernel']\n        elif PARAMS['model_name'] == 'KNeighborsRegressor':\n            model.weights = PARAMS['knr_weights']\n    except Exception as exception:\n        LOG.exception(exception)\n        raise\n    return model", "output": "Get model according to parameters", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy(self, path, dest, raise_if_exists=False):\n        \"\"\"\n        \n        \"\"\"\n        if raise_if_exists and dest in self.get_all_data():\n            raise RuntimeError('Destination exists: %s' % path)\n        contents = self.get_all_data()[path]\n        self.get_all_data()[dest] = contents", "output": "Copies the contents of a single file path to dest", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _extract_header(time_series):\n    \"\"\"\"\"\"\n    return TimeSeries(\n        metric=time_series.metric,\n        resource=time_series.resource,\n        metric_kind=time_series.metric_kind,\n        value_type=time_series.value_type,\n    )", "output": "Return a copy of time_series with the points removed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _to_original_callable(obj):\n        \"\"\"\n        \n        \"\"\"\n        while True:\n            if inspect.isfunction(obj) or inspect.isclass(obj):\n                f = inspect.getfile(obj)\n                if f.startswith('<') and f.endswith('>'):\n                    return None\n                return obj\n            if inspect.ismethod(obj):\n                obj = obj.__func__\n            elif isinstance(obj, functools.partial):\n                obj = obj.func\n            elif isinstance(obj, property):\n                obj = obj.fget\n            else:\n                return None", "output": "Find the Python object that contains the source code of the object.\n\n        This is useful to find the place in the source code (file and line\n        number) where a docstring is defined. It does not currently work for\n        all cases, but it should help find some (properties...).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_cluster_custom_object_scale(self, group, version, plural, name, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_cluster_custom_object_scale_with_http_info(group, version, plural, name, body, **kwargs)\n        else:\n            (data) = self.replace_cluster_custom_object_scale_with_http_info(group, version, plural, name, body, **kwargs)\n            return data", "output": "replace scale of the specified cluster scoped custom object\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_cluster_custom_object_scale(group, version, plural, name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str group: the custom resource's group (required)\n        :param str version: the custom resource's version (required)\n        :param str plural: the custom resource's plural name. For TPRs this would be lowercase plural kind. (required)\n        :param str name: the custom object's name (required)\n        :param object body: (required)\n        :return: object\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_descriptor(ctx):\n    \"\"\"  \"\"\"\n    d_net = gluon.nn.Sequential()\n    with d_net.name_scope():\n\n        d_net.add(SNConv2D(num_filter=64, kernel_size=4, strides=2, padding=1, in_channels=3, ctx=ctx))\n        d_net.add(gluon.nn.LeakyReLU(0.2))\n\n        d_net.add(SNConv2D(num_filter=128, kernel_size=4, strides=2, padding=1, in_channels=64, ctx=ctx))\n        d_net.add(gluon.nn.LeakyReLU(0.2))\n\n        d_net.add(SNConv2D(num_filter=256, kernel_size=4, strides=2, padding=1, in_channels=128, ctx=ctx))\n        d_net.add(gluon.nn.LeakyReLU(0.2))\n\n        d_net.add(SNConv2D(num_filter=512, kernel_size=4, strides=2, padding=1, in_channels=256, ctx=ctx))\n        d_net.add(gluon.nn.LeakyReLU(0.2))\n\n        d_net.add(SNConv2D(num_filter=1, kernel_size=4, strides=1, padding=0, in_channels=512, ctx=ctx))\n\n    return d_net", "output": "construct and return descriptor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_configured_hdfs_client():\n    \"\"\"\n    \n    \"\"\"\n    config = hdfs()\n    custom = config.client\n    conf_usinf_snakebite = [\n        \"snakebite_with_hadoopcli_fallback\",\n        \"snakebite\",\n    ]\n    if six.PY3 and (custom in conf_usinf_snakebite):\n        warnings.warn(\n            \"snakebite client not compatible with python3 at the moment\"\n            \"falling back on hadoopcli\",\n            stacklevel=2\n        )\n        return \"hadoopcli\"\n    return custom", "output": "This is a helper that fetches the configuration value for 'client' in\n    the [hdfs] section. It will return the client that retains backwards\n    compatibility when 'client' isn't configured.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rlmf_tictactoe():\n  \"\"\"\"\"\"\n  hparams = rlmf_original()\n  hparams.game = \"tictactoe\"\n  hparams.rl_env_name = \"T2TEnv-TicTacToeEnv-v0\"\n  # Since we don't have any no-op actions, otherwise we have to have an\n  # attribute called `get_action_meanings`.\n  hparams.eval_max_num_noops = 0\n  hparams.max_num_noops = 0\n  hparams.rl_should_derive_observation_space = False\n\n  hparams.policy_network = \"feed_forward_categorical_policy\"\n  hparams.base_algo_params = \"ppo_ttt_params\"\n\n  # Number of last observations to feed to the agent\n  hparams.frame_stack_size = 1\n  return hparams", "output": "Base set of hparams for model-free PPO.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_operation_job(self, operation, obj, external_id_field_name=None, content_type=None):\n        \"\"\"\n        \n        \"\"\"\n        if not self.has_active_session():\n            self.start_session()\n\n        response = requests.post(self._get_create_job_url(),\n                                 headers=self._get_create_job_headers(),\n                                 data=self._get_create_job_xml(operation, obj, external_id_field_name, content_type))\n        response.raise_for_status()\n\n        root = ET.fromstring(response.text)\n        job_id = root.find('%sid' % self.API_NS).text\n        return job_id", "output": "Creates a new SF job that for doing any operation (insert, upsert, update, delete, query)\n\n        :param operation: delete, insert, query, upsert, update, hardDelete. Must be lowercase.\n        :param obj: Parent SF object\n        :param external_id_field_name: Optional.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def distinguished_name_list_exists(name, items):\n    '''\n    \n\n    '''\n    ret = _default_ret(name)\n    req_change = False\n    try:\n        existing_lists = __salt__['bluecoat_sslv.get_distinguished_name_lists']()\n        if name not in existing_lists:\n            __salt__['bluecoat_sslv.add_distinguished_name_list'](name)\n            req_change = True\n        list_members = __salt__['bluecoat_sslv.get_distinguished_name_list'](name)\n        for item in items:\n            if item not in list_members:\n                __salt__['bluecoat_sslv.add_distinguished_name'](name, item)\n                req_change = True\n        if req_change:\n            ret['changes']['before'] = list_members\n            ret['changes']['after'] = __salt__['bluecoat_sslv.get_distinguished_name_list'](name)\n            ret['comment'] = \"Updated distinguished name list.\"\n        else:\n            ret['comment'] = \"No changes required.\"\n    except salt.exceptions.CommandExecutionError as err:\n        ret['result'] = False\n        ret['comment'] = err\n        log.error(err)\n        return ret\n    ret['result'] = True\n    return ret", "output": "Ensures that a distinguished name list exists with the items provided.\n\n    name: The name of the module function to execute.\n\n    name(str): The name of the distinguished names list.\n\n    items(list): A list of items to ensure exist on the distinguished names list.\n\n    SLS Example:\n\n    .. code-block:: yaml\n\n        MyDistinguishedNameList:\n          bluecoat_sslv.distinguished_name_list_exists:\n            items:\n              - cn=test.com\n              - cn=othersite.com", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def group_create(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(keep_name=True, **kwargs)\n    return cloud.create_group(**kwargs)", "output": "Create a group\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystoneng.group_create name=group1\n        salt '*' keystoneng.group_create name=group2 domain=domain1 description='my group2'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_import_data(self, data):\n        \"\"\"\n        \"\"\"\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(\"Importing data, current processing progress %s / %s\" %(_completed_num, len(data)))\n            _completed_num += 1\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                logger.info(\"Useless trial data, value is %s, skip this trial data.\" %_value)\n                continue\n            budget_exist_flag = False\n            barely_params = dict()\n            for keys in _params:\n                if keys == _KEY:\n                    _budget = _params[keys]\n                    budget_exist_flag = True\n                else:\n                    barely_params[keys] = _params[keys]\n            if not budget_exist_flag:\n                _budget = self.max_budget\n                logger.info(\"Set \\\"TRIAL_BUDGET\\\" value to %s (max budget)\" %self.max_budget)\n            if self.optimize_mode is OptimizeMode.Maximize:\n                reward = -_value\n            else:\n                reward = _value\n            self.cg.new_result(loss=reward, budget=_budget, parameters=barely_params, update_model=True)\n        logger.info(\"Successfully import tuning data to BOHB advisor.\")", "output": "Import additional data for tuning\n\n        Parameters\n        ----------\n        data:\n            a list of dictionarys, each of which has at least two keys, 'parameter' and 'value'\n\n        Raises\n        ------\n        AssertionError\n            data doesn't have required key 'parameter' and 'value'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_cache():\n    '''\n    \n    '''\n    ret = []\n    for fn_ in os.listdir(__opts__['cachedir']):\n        if fn_.endswith('.cache.p'):\n            path = os.path.join(__opts__['cachedir'], fn_)\n            if not os.path.isfile(path):\n                continue\n            os.remove(path)\n            ret.append(fn_)\n    return ret", "output": "Clear out cached state files, forcing even cache runs to refresh the cache\n    on the next state execution.\n\n    Remember that the state cache is completely disabled by default, this\n    execution only applies if cache=True is used in states\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.clear_cache", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def construct_from_string(cls, string):\n        \"\"\"\n        \n        \"\"\"\n        if string == cls.name:\n            return cls()\n        raise TypeError(\"Cannot construct a '{}' from \"\n                        \"'{}'\".format(cls, string))", "output": "Construction from a string, raise a TypeError if not\n        possible", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_tmp_dir(suffix=\"\", prefix=\"tmp\", dir=None):  # pylint: disable=redefined-builtin\n  \"\"\"\"\"\"\n  if dir is None:\n    return tempfile.mkdtemp(suffix, prefix, dir)\n  else:\n    while True:\n      rand_term = random.randint(1, 9999)\n      tmp_dir = os.path.join(dir, \"%s%d%s\" % (prefix, rand_term, suffix))\n      if tf.gfile.Exists(tmp_dir):\n        continue\n      tf.gfile.MakeDirs(tmp_dir)\n      break\n    return tmp_dir", "output": "Make a temporary directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def as_posix(self):\n        \"\"\"\"\"\"\n        f = self._flavour\n        return str(self).replace(f.sep, '/')", "output": "Return the string representation of the path with forward (/)\n        slashes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, logicalId):\n        \"\"\"\n        \n        \"\"\"\n        if logicalId not in self.resources:\n            return None\n\n        return SamResource(self.resources.get(logicalId))", "output": "Gets the resource at the given logicalId if present\n\n        :param string logicalId: Id of the resource\n        :return SamResource: Resource, if available at the Id. None, otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def increment_step_and_update_last_reward(self):\n        \"\"\"\n        \n        \"\"\"\n        if len(self.stats['Environment/Cumulative Reward']) > 0:\n            mean_reward = np.mean(self.stats['Environment/Cumulative Reward'])\n            self.policy.update_reward(mean_reward)\n        self.policy.increment_step()\n        self.step = self.policy.get_current_step()", "output": "Increment the step count of the trainer and Updates the last reward", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gradient_override_map(override_dict):\n  \"\"\"\n\n  \"\"\"\n  override_dict_by_name = {}\n  for (op_name, grad_f) in override_dict.items():\n    if isinstance(grad_f, str):\n       override_dict_by_name[op_name] = grad_f\n    else:\n      override_dict_by_name[op_name] = register_to_random_name(grad_f)\n  with tf.get_default_graph().gradient_override_map(override_dict_by_name):\n    yield", "output": "Convenience wrapper for graph.gradient_override_map().\n\n  This functions provides two conveniences over normal tensorflow gradient\n  overrides: it auomatically uses the default graph instead of you needing to\n  find the graph, and it automatically\n\n  Example:\n\n    def _foo_grad_alt(op, grad): ...\n\n    with gradient_override({\"Foo\": _foo_grad_alt}):\n\n  Args:\n    override_dict: A dictionary describing how to override the gradient.\n      keys: strings correponding to the op type that should have their gradient\n        overriden.\n      values: functions or strings registered to gradient functions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_model_loader(filename):\n    \"\"\"\n    \n    \"\"\"\n    assert isinstance(filename, six.string_types), filename\n    filename = os.path.expanduser(filename)\n    if filename.endswith('.npy'):\n        assert tf.gfile.Exists(filename), filename\n        return DictRestore(np.load(filename, encoding='latin1').item())\n    elif filename.endswith('.npz'):\n        assert tf.gfile.Exists(filename), filename\n        obj = np.load(filename)\n        return DictRestore(dict(obj))\n    else:\n        return SaverRestore(filename)", "output": "Get a corresponding model loader by looking at the file name.\n\n    Returns:\n        SessInit: either a :class:`DictRestore` (if name ends with 'npy/npz') or\n        :class:`SaverRestore` (otherwise).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ensure_proc_terminate(proc):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(proc, list):\n        for p in proc:\n            ensure_proc_terminate(p)\n        return\n\n    def stop_proc_by_weak_ref(ref):\n        proc = ref()\n        if proc is None:\n            return\n        if not proc.is_alive():\n            return\n        proc.terminate()\n        proc.join()\n\n    assert isinstance(proc, mp.Process)\n    atexit.register(stop_proc_by_weak_ref, weakref.ref(proc))", "output": "Make sure processes terminate when main process exit.\n\n    Args:\n        proc (multiprocessing.Process or list)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find(collection, query=None, user=None, password=None,\n         host=None, port=None, database='admin', authdb=None):\n    '''\n    \n\n    '''\n    conn = _connect(user, password, host, port, database, authdb)\n    if not conn:\n        return 'Failed to connect to mongo database'\n\n    try:\n        query = _to_dict(query)\n    except Exception as err:\n        return err\n\n    try:\n        log.info(\"Searching for %r in %s\", query, collection)\n        mdb = pymongo.database.Database(conn, database)\n        col = getattr(mdb, collection)\n        ret = col.find(query)\n        return list(ret)\n    except pymongo.errors.PyMongoError as err:\n        log.error(\"Searching objects failed with error: %s\", err)\n        return err", "output": "Find an object or list of objects in a collection\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mongodb.find mycollection '[{\"foo\": \"FOO\", \"bar\": \"BAR\"}]' <user> <password> <host> <port> <database>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select (features, properties):\n    \"\"\" \n    \"\"\"\n    assert is_iterable_typed(properties, basestring)\n    result = []\n\n    # add any missing angle brackets\n    features = add_grist (features)\n\n    return [p for p in properties if get_grist(p) in features]", "output": "Selects properties which correspond to any of the given features.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def heartbeat(self):\n        \"\"\"\"\"\"\n        while self._manager.is_active and not self._stop_event.is_set():\n            self._manager.heartbeat()\n            _LOGGER.debug(\"Sent heartbeat.\")\n            self._stop_event.wait(timeout=self._period)\n\n        _LOGGER.info(\"%s exiting.\", _HEARTBEAT_WORKER_NAME)", "output": "Periodically send heartbeats.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def loop_until_closed(self):\n        ''' \n\n        '''\n        if isinstance(self._state, NOT_YET_CONNECTED):\n            # we don't use self._transition_to_disconnected here\n            # because _transition is a coroutine\n            self._tell_session_about_disconnect()\n            self._state = DISCONNECTED()\n        else:\n            def closed():\n                return isinstance(self._state, DISCONNECTED)\n            self._loop_until(closed)", "output": "Execute a blocking loop that runs and executes event callbacks\n        until the connection is closed (e.g. by hitting Ctrl-C).\n\n        While this method can be used to run Bokeh application code \"outside\"\n        the Bokeh server, this practice is HIGHLY DISCOURAGED for any real\n        use case.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy(self, deep=True):\n        \"\"\"  \"\"\"\n        values = self.values\n        if deep:\n            values = values.copy()\n        return self.make_block_same_class(values, ndim=self.ndim)", "output": "copy constructor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_next(stepper, metrics, val_iter):\n    \"\"\"\"\"\"\n    stepper.reset(False)\n    with no_grad_context():\n        (*x,y) = val_iter.next()\n        preds,l = stepper.evaluate(VV(x), VV(y))\n        res = [delistify(to_np(l))]\n        res += [f(datafy(preds), datafy(y)) for f in metrics]\n    stepper.reset(True)\n    return res", "output": "Computes the loss on the next minibatch of the validation set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Runs(self):\n    \"\"\"\n    \"\"\"\n    with self._accumulators_mutex:\n      # To avoid nested locks, we construct a copy of the run-accumulator map\n      items = list(six.iteritems(self._accumulators))\n    return {run_name: accumulator.Tags() for run_name, accumulator in items}", "output": "Return all the run names in the `EventMultiplexer`.\n\n    Returns:\n    ```\n      {runName: { scalarValues: [tagA, tagB, tagC],\n                  graph: true, meta_graph: true}}\n    ```", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_traceback(exc_info, source_hint=None):\n    \"\"\"\"\"\"\n    exc_type, exc_value, tb = exc_info\n    if isinstance(exc_value, TemplateSyntaxError):\n        exc_info = translate_syntax_error(exc_value, source_hint)\n        initial_skip = 0\n    else:\n        initial_skip = 1\n    return translate_exception(exc_info, initial_skip)", "output": "Creates a processed traceback object from the exc_info.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def evaluate(self, dataset):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(dataset, DataFrame):\n            raise ValueError(\"dataset must be a DataFrame but got %s.\" % type(dataset))\n        java_glr_summary = self._call_java(\"evaluate\", dataset)\n        return GeneralizedLinearRegressionSummary(java_glr_summary)", "output": "Evaluates the model on a test dataset.\n\n        :param dataset:\n          Test dataset to evaluate model on, where dataset is an\n          instance of :py:class:`pyspark.sql.DataFrame`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_integer(self, key, axis):\n        \"\"\"\n        \n        \"\"\"\n\n        len_axis = len(self.obj._get_axis(axis))\n        if key >= len_axis or key < -len_axis:\n            raise IndexError(\"single positional indexer is out-of-bounds\")", "output": "Check that 'key' is a valid position in the desired axis.\n\n        Parameters\n        ----------\n        key : int\n            Requested position\n        axis : int\n            Desired axis\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        IndexError\n            If 'key' is not a valid position in axis 'axis'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __prep_mod_opts(self, opts):\n        '''\n        \n        '''\n        if '__grains__' not in self.pack:\n            grains = opts.get('grains', {})\n\n            if isinstance(grains, ThreadLocalProxy):\n                grains = ThreadLocalProxy.unproxy(grains)\n\n            self.context_dict['grains'] = grains\n            self.pack['__grains__'] = salt.utils.context.NamespacedDictWrapper(self.context_dict, 'grains')\n\n        if '__pillar__' not in self.pack:\n            pillar = opts.get('pillar', {})\n\n            if isinstance(pillar, ThreadLocalProxy):\n                pillar = ThreadLocalProxy.unproxy(pillar)\n\n            self.context_dict['pillar'] = pillar\n            self.pack['__pillar__'] = salt.utils.context.NamespacedDictWrapper(self.context_dict, 'pillar')\n\n        mod_opts = {}\n        for key, val in list(opts.items()):\n            if key == 'logger':\n                continue\n            mod_opts[key] = val\n        return mod_opts", "output": "Strip out of the opts any logger instance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_user(user,\n                  device,\n                  token):\n    '''\n    \n    '''\n    res = {\n            'message': 'User key is invalid',\n            'result': False\n           }\n\n    parameters = dict()\n    parameters['user'] = user\n    parameters['token'] = token\n    if device:\n        parameters['device'] = device\n\n    response = query(function='validate_user',\n                     method='POST',\n                     header_dict={'Content-Type': 'application/x-www-form-urlencoded'},\n                     data=_urlencode(parameters))\n\n    if response['res']:\n        if 'message' in response:\n            _message = response.get('message', '')\n            if 'status' in _message:\n                if _message.get('dict', {}).get('status', None) == 1:\n                    res['result'] = True\n                    res['message'] = 'User key is valid.'\n                else:\n                    res['result'] = False\n                    res['message'] = ''.join(_message.get('dict', {}).get('errors'))\n    return res", "output": "Send a message to a Pushover user or group.\n    :param user:        The user or group name, either will work.\n    :param device:      The device for the user.\n    :param token:       The PushOver token.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _reset_non_empty(self, indices):\n    \"\"\"\n    \"\"\"\n    reset_video_op = tf.cond(\n        self._video_condition,\n        lambda: tf.py_func(self._video_reset_writer, [], []),\n        tf.no_op)\n    with tf.control_dependencies([reset_video_op]):\n      inc_op = tf.assign_add(self._episode_counter, 1)\n      with tf.control_dependencies([self.history_buffer.reset(indices),\n                                    inc_op]):\n        initial_frame_dump_op = tf.cond(\n            self._video_condition,\n            lambda: tf.py_func(self._video_dump_frames,  # pylint: disable=g-long-lambda\n                               [self.history_buffer.get_all_elements()], []),\n            tf.no_op)\n        observ_assign_op = self._observ.assign(\n            self.history_buffer.get_all_elements()[:, -1, ...])\n        with tf.control_dependencies([observ_assign_op, initial_frame_dump_op]):\n          reset_model_op = tf.assign(self._reset_model, tf.constant(1.0))\n          with tf.control_dependencies([reset_model_op]):\n            return tf.gather(self._observ.read_value(), indices)", "output": "Reset the batch of environments.\n\n    Args:\n      indices: The batch indices of the environments to reset; defaults to all.\n\n    Returns:\n      Batch tensor of the new observations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_temp_filename(prefix):\n    '''\n    \n    '''\n    temp_location = _get_temp_file_location()\n    temp_file_name = '/'.join([temp_location, str(prefix)+str(_uuid.uuid4())])\n    return temp_file_name", "output": "Generate a temporary file that would not live beyond the lifetime of\n    unity_server.\n\n    Caller is expected to clean up the temp file as soon as the file is no\n    longer needed. But temp files created using this method will be cleaned up\n    when unity_server restarts", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def predicates_overlap(tags1: List[str], tags2: List[str]) -> bool:\n    \"\"\"\n    \n    \"\"\"\n    # Get predicate word indices from both predictions\n    pred_ind1 = get_predicate_indices(tags1)\n    pred_ind2 = get_predicate_indices(tags2)\n\n    # Return if pred_ind1 pred_ind2 overlap\n    return any(set.intersection(set(pred_ind1), set(pred_ind2)))", "output": "Tests whether the predicate in BIO tags1 overlap\n    with those of tags2.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_present(name, set_type, family='ipv4', **kwargs):\n    '''\n    \n    '''\n\n    ret = {'name': name,\n           'changes': {},\n           'result': None,\n           'comment': ''}\n\n    set_check = __salt__['ipset.check_set'](name)\n    if set_check is True:\n        ret['result'] = True\n        ret['comment'] = ('ipset set {0} already exists for {1}'\n                          .format(name, family))\n        return ret\n\n    if __opts__['test']:\n        ret['comment'] = 'ipset set {0} would be added for {1}'.format(\n            name,\n            family)\n        return ret\n    command = __salt__['ipset.new_set'](name, set_type, family, **kwargs)\n    if command is True:\n        ret['changes'] = {'locale': name}\n        ret['result'] = True\n        ret['comment'] = ('ipset set {0} created successfully for {1}'\n                          .format(name, family))\n        return ret\n    else:\n        ret['result'] = False\n        ret['comment'] = 'Failed to create set {0} for {2}: {1}'.format(\n            name,\n            command.strip(),\n            family\n        )\n        return ret", "output": ".. versionadded:: 2014.7.0\n\n    Verify the set exists.\n\n    name\n        A user-defined set name.\n\n    set_type\n        The type for the set.\n\n    family\n        Networking family, either ipv4 or ipv6", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _extend_blocks(result, blocks=None):\n    \"\"\"  \"\"\"\n    from pandas.core.internals import BlockManager\n    if blocks is None:\n        blocks = []\n    if isinstance(result, list):\n        for r in result:\n            if isinstance(r, list):\n                blocks.extend(r)\n            else:\n                blocks.append(r)\n    elif isinstance(result, BlockManager):\n        blocks.extend(result.blocks)\n    else:\n        blocks.append(result)\n    return blocks", "output": "return a new extended blocks, givin the result", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def argsort(self, axis=0, kind='quicksort', order=None):\n        \"\"\"\n        \n        \"\"\"\n        values = self._values\n        mask = isna(values)\n\n        if mask.any():\n            result = Series(-1, index=self.index, name=self.name,\n                            dtype='int64')\n            notmask = ~mask\n            result[notmask] = np.argsort(values[notmask], kind=kind)\n            return self._constructor(result,\n                                     index=self.index).__finalize__(self)\n        else:\n            return self._constructor(\n                np.argsort(values, kind=kind), index=self.index,\n                dtype='int64').__finalize__(self)", "output": "Override ndarray.argsort. Argsorts the value, omitting NA/null values,\n        and places the result in the same locations as the non-NA values.\n\n        Parameters\n        ----------\n        axis : int\n            Has no effect but is accepted for compatibility with numpy.\n        kind : {'mergesort', 'quicksort', 'heapsort'}, default 'quicksort'\n            Choice of sorting algorithm. See np.sort for more\n            information. 'mergesort' is the only stable algorithm\n        order : None\n            Has no effect but is accepted for compatibility with numpy.\n\n        Returns\n        -------\n        Series\n            Positions of values within the sort order with -1 indicating\n            nan values.\n\n        See Also\n        --------\n        numpy.ndarray.argsort", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def virtual_network_delete(name, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    result = False\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        vnet = netconn.virtual_networks.delete(\n            virtual_network_name=name,\n            resource_group_name=resource_group\n        )\n        vnet.wait()\n        result = True\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Delete a virtual network.\n\n    :param name: The name of the virtual network to delete.\n\n    :param resource_group: The resource group name assigned to the\n        virtual network\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.virtual_network_delete testnet testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def basicConfig(*args, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    if default_handler is not None:\n        bokeh_logger.removeHandler(default_handler)\n        bokeh_logger.propagate = True\n    return logging.basicConfig(*args, **kwargs)", "output": "A logging.basicConfig() wrapper that also undoes the default\n    Bokeh-specific configuration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pillar_format(ret, keys, value, expand_keys):\n    '''\n    \n    '''\n    # if value is empty in Consul then it's None here - skip it\n    if value is None:\n        return ret\n\n    # If value is not None then it's a string\n    # YAML strips whitespaces unless they're surrounded by quotes\n    # If expand_keys is true, deserialize the YAML data\n    if expand_keys:\n        pillar_value = salt.utils.yaml.safe_load(value)\n    else:\n        pillar_value = value\n\n    keyvalue = keys.pop()\n    pil = {keyvalue: pillar_value}\n    keys.reverse()\n    for k in keys:\n        pil = {k: pil}\n\n    return dict_merge(ret, pil)", "output": "Perform data formatting to be used as pillar data and\n    merge it with the current pillar data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_deconvolution(node, **kwargs):\n    \"\"\"\n    \"\"\"\n    name, inputs, attrs = get_inputs(node, kwargs)\n\n    kernel_dims = list(parse_helper(attrs, \"kernel\"))\n    stride_dims = list(parse_helper(attrs, \"stride\", [1, 1]))\n    pad_dims = list(parse_helper(attrs, \"pad\", [0, 0]))\n    num_group = int(attrs.get(\"num_group\", 1))\n    dilations = list(parse_helper(attrs, \"dilate\", [1, 1]))\n    adj_dims = list(parse_helper(attrs, \"adj\", [0, 0]))\n\n    pad_dims = pad_dims + pad_dims\n\n    deconv_node = onnx.helper.make_node(\n        \"ConvTranspose\",\n        inputs=inputs,\n        outputs=[name],\n        kernel_shape=kernel_dims,\n        strides=stride_dims,\n        dilations=dilations,\n        output_padding=adj_dims,\n        pads=pad_dims,\n        group=num_group,\n        name=name\n    )\n\n    return [deconv_node]", "output": "Map MXNet's deconvolution operator attributes to onnx's ConvTranspose operator\n    and return the created node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def box_plot(x, y, xlabel=LABEL_DEFAULT, ylabel=LABEL_DEFAULT, title=LABEL_DEFAULT):\n    \"\"\"\n    \n    \"\"\"\n    if (not isinstance(x, tc.data_structures.sarray.SArray) or \n        not isinstance(y, tc.data_structures.sarray.SArray) or\n        x.dtype != str or y.dtype not in [int, float]):\n        raise ValueError(\"turicreate.visualization.box_plot supports \" + \n            \"x as SArray of dtype str and y as SArray of dtype: int, float.\" +\n            \"\\nExample: turicreate.visualization.box_plot(tc.SArray(['a','b','c','a','a']),tc.SArray([4.0,3.25,2.1,2.0,1.0]))\")\n    title = _get_title(title)\n    plt_ref = tc.extensions.plot_boxes_and_whiskers(x, y, \n      xlabel, ylabel, title)\n    return Plot(plt_ref)", "output": "Plots the data in `x` on the X axis and the data in `y` on the Y axis\n    in a 2d box and whiskers plot, and returns the resulting Plot object.\n    \n    The function x as SArray of dtype str and y as SArray of dtype: int, float.\n\n    Parameters\n    ----------\n    x : SArray\n      The data to plot on the X axis of the box and whiskers plot. \n      Must be an SArray with dtype string.\n    y : SArray\n      The data to plot on the Y axis of the box and whiskers plot. \n      Must be numeric (int/float) and must be the same length as `x`.\n    xlabel : str (optional)\n      The text label for the X axis. Defaults to \"X\".\n    ylabel : str (optional)\n      The text label for the Y axis. Defaults to \"Y\".\n    title : str (optional)\n      The title of the plot. Defaults to LABEL_DEFAULT. If the value is\n      LABEL_DEFAULT, the title will be \"<xlabel> vs. <ylabel>\". If the value\n      is None, the title will be omitted. Otherwise, the string passed in as the\n      title will be used as the plot title.\n\n    Returns\n    -------\n    out : Plot\n      A :class: Plot object that is the box and whiskers plot.\n\n    Examples\n    --------\n    Make a box and whiskers plot.\n\n    >>> bp = turicreate.visualization.box_plot(tc.SArray(['a','b','c','a','a']),tc.SArray([4.0,3.25,2.1,2.0,1.0]))", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compute_etag(self) -> Optional[str]:\n        \"\"\"\n        \"\"\"\n        assert self.absolute_path is not None\n        version_hash = self._get_cached_version(self.absolute_path)\n        if not version_hash:\n            return None\n        return '\"%s\"' % (version_hash,)", "output": "Sets the ``Etag`` header based on static url version.\n\n        This allows efficient ``If-None-Match`` checks against cached\n        versions, and sends the correct ``Etag`` for a partial response\n        (i.e. the same ``Etag`` as the full file).\n\n        .. versionadded:: 3.1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge_noun_chunks(doc):\n    \"\"\"\n    \"\"\"\n    if not doc.is_parsed:\n        return doc\n    with doc.retokenize() as retokenizer:\n        for np in doc.noun_chunks:\n            attrs = {\"tag\": np.root.tag, \"dep\": np.root.dep}\n            retokenizer.merge(np, attrs=attrs)\n    return doc", "output": "Merge noun chunks into a single token.\n\n    doc (Doc): The Doc object.\n    RETURNS (Doc): The Doc object with merged noun chunks.\n\n    DOCS: https://spacy.io/api/pipeline-functions#merge_noun_chunks", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def report_intermediate_result(metric):\n    \"\"\"\n    \"\"\"\n    global _intermediate_seq\n    assert _params is not None, 'nni.get_next_parameter() needs to be called before report_intermediate_result'\n    metric = json_tricks.dumps({\n        'parameter_id': _params['parameter_id'],\n        'trial_job_id': trial_env_vars.NNI_TRIAL_JOB_ID,\n        'type': 'PERIODICAL',\n        'sequence': _intermediate_seq,\n        'value': metric\n    })\n    _intermediate_seq += 1\n    platform.send_metric(metric)", "output": "Reports intermediate result to Assessor.\n    metric: serializable object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def batch_norm_relu(inputs,\n                    is_training,\n                    relu=True,\n                    init_zero=False,\n                    data_format=\"channels_first\"):\n  \"\"\"\n  \"\"\"\n  if init_zero:\n    gamma_initializer = tf.zeros_initializer()\n  else:\n    gamma_initializer = tf.ones_initializer()\n\n  if data_format == \"channels_first\":\n    axis = 1\n  else:\n    axis = 3\n\n  inputs = layers().BatchNormalization(\n      axis=axis,\n      momentum=BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      center=True,\n      scale=True,\n      fused=True,\n      gamma_initializer=gamma_initializer)(inputs, training=is_training)\n\n  if relu:\n    inputs = tf.nn.relu(inputs)\n  return inputs", "output": "Performs a batch normalization followed by a ReLU.\n\n  Args:\n    inputs: `Tensor` of shape `[batch, channels, ...]`.\n    is_training: `bool` for whether the model is training.\n    relu: `bool` if False, omits the ReLU operation.\n    init_zero: `bool` if True, initializes scale parameter of batch\n        normalization with 0 instead of 1 (default).\n    data_format: `str` either \"channels_first\" for `[batch, channels, height,\n        width]` or \"channels_last for `[batch, height, width, channels]`.\n\n  Returns:\n    A normalized `Tensor` with the same `data_format`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_bytecode(self, f):\n        \"\"\"\"\"\"\n        if self.code is None:\n            raise TypeError('can\\'t write empty bucket')\n        f.write(bc_magic)\n        pickle.dump(self.checksum, f, 2)\n        marshal_dump(self.code, f)", "output": "Dump the bytecode into the file or file like object passed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def time(self):\n        \"\"\"\n        \n        \"\"\"\n        # If the Timestamps have a timezone that is not UTC,\n        # convert them into their i8 representation while\n        # keeping their timezone and not using UTC\n        if self.tz is not None and not timezones.is_utc(self.tz):\n            timestamps = self._local_timestamps()\n        else:\n            timestamps = self.asi8\n\n        return tslib.ints_to_pydatetime(timestamps, box=\"time\")", "output": "Returns numpy array of datetime.time. The time part of the Timestamps.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_network_adapters(interface_old_new, parent):\n    '''\n    \n    '''\n    network_changes = []\n    if interface_old_new:\n        devs = [inter['old']['mac'] for inter in interface_old_new]\n        log.trace('Updating network interfaces %s', devs)\n        for item in interface_old_new:\n            current_interface = item['old']\n            next_interface = item['new']\n            difference = recursive_diff(current_interface, next_interface)\n            difference.ignore_unset_values = False\n            if difference.changed():\n                log.trace('Virtual machine network adapter will be updated '\n                          'switch_type=%s name=%s adapter_type=%s mac=%s',\n                          next_interface['switch_type'],\n                          next_interface['name'],\n                          current_interface['adapter_type'],\n                          current_interface['mac'])\n                device_config_spec = _apply_network_adapter_config(\n                    current_interface['key'],\n                    next_interface['name'],\n                    current_interface['adapter_type'],\n                    next_interface['switch_type'],\n                    operation='edit',\n                    mac=current_interface['mac'],\n                    parent=parent)\n                network_changes.append(device_config_spec)\n    return network_changes", "output": "Returns a list of vim.vm.device.VirtualDeviceSpec specifying\n    configuration(s) for changed network adapters, the adapter type cannot\n    be changed, as input the old and new configs are defined in a dictionary.\n\n    interface_old_new\n        Dictionary with old and new keys which contains the current and the\n        next config for a network device\n\n    parent\n        Parent managed object reference", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def istextfile(fname, blocksize=512):\n    \"\"\" \n    \"\"\"\n    with open(fname, \"rb\") as fobj:\n        block = fobj.read(blocksize)\n\n    if not block:\n        # An empty file is considered a valid text file\n        return True\n\n    if b\"\\x00\" in block:\n        # Files with null bytes are binary\n        return False\n\n    # Use translate's 'deletechars' argument to efficiently remove all\n    # occurrences of TEXT_CHARS from the block\n    nontext = block.translate(None, TEXT_CHARS)\n    return float(len(nontext)) / len(block) <= 0.30", "output": "Uses heuristics to guess whether the given file is text or binary,\n        by reading a single block of bytes from the file.\n        If more than 30% of the chars in the block are non-text, or there\n        are NUL ('\\x00') bytes in the block, assume this is a binary file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_shell(self, arg):\n        \"\"\n        print(\">\", arg)\n        sub_cmd = subprocess.Popen(arg, shell=True, stdout=subprocess.PIPE)\n        print(sub_cmd.communicate()[0])", "output": "run a shell commad", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_flask_context(request):\n    \"\"\"\n    \"\"\"\n    return HTTPContext(\n        url=request.url,\n        method=request.method,\n        user_agent=request.user_agent.string,\n        referrer=request.referrer,\n        remote_ip=request.remote_addr,\n    )", "output": "Builds an HTTP context object from a Flask (Werkzeug) request object.\n\n     This helper method extracts the relevant HTTP context from a Flask request\n     object into an object ready to be sent to Error Reporting.\n\n    .. code-block:: python\n\n        >>> @app.errorhandler(HTTPException)\n        ... def handle_error(exc):\n        ...     client.report_exception(\n        ...         http_context=build_flask_context(request))\n        ...     # rest of error response code here\n\n    :type request: :class:`werkzeug.wrappers.request`\n    :param request: The Flask request object to convert.\n\n    :rtype: :class:`~google.cloud.error_reporting.client.HTTPContext`\n    :returns: An HTTPContext object ready to be sent to the Stackdriver Error\n              Reporting API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_variable(self, variable_name, client=None):\n        \"\"\"\n        \"\"\"\n        client = self._require_client(client)\n        variable = Variable(config=self, name=variable_name)\n        try:\n            variable.reload(client=client)\n            return variable\n        except NotFound:\n            return None", "output": "API call:  get a variable via a ``GET`` request.\n\n        This will return None if the variable doesn't exist::\n\n          >>> from google.cloud import runtimeconfig\n          >>> client = runtimeconfig.Client()\n          >>> config = client.config('my-config')\n          >>> print(config.get_variable('variable-name'))\n          <Variable: my-config, variable-name>\n          >>> print(config.get_variable('does-not-exist'))\n          None\n\n        :type variable_name: str\n        :param variable_name: The name of the variable to retrieve.\n\n        :type client: :class:`~google.cloud.runtimeconfig.client.Client`\n        :param client:\n            (Optional) The client to use.  If not passed, falls back to the\n            ``client`` stored on the current config.\n\n        :rtype: :class:`google.cloud.runtimeconfig.variable.Variable` or None\n        :returns: The variable object if it exists, otherwise None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sync(self, other):\n        \"\"\"\n        \"\"\"\n        assert other.shape == self.shape, \"Shapes don't match!\"\n        self.demean = other.demean\n        self.destd = other.destd\n        self.clip = other.clip\n        self.rs = other.rs.copy()\n        self.buffer = other.buffer.copy()", "output": "Syncs all fields together from other filter.\n\n        Examples:\n            >>> a = MeanStdFilter(())\n            >>> a(1)\n            >>> a(2)\n            >>> print([a.rs.n, a.rs.mean, a.buffer.n])\n            [2, array(1.5), 2]\n            >>> b = MeanStdFilter(())\n            >>> b(10)\n            >>> print([b.rs.n, b.rs.mean, b.buffer.n])\n            [1, array(10.0), 1]\n            >>> a.sync(b)\n            >>> print([a.rs.n, a.rs.mean, a.buffer.n])\n            [1, array(10.0), 1]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dense_relu_dense(inputs,\n                     filter_size,\n                     output_size,\n                     output_activation=None,\n                     dropout=0.0,\n                     dropout_broadcast_dims=None,\n                     layer_collection=None,\n                     name=None):\n  \"\"\"\"\"\"\n  # layer_name is appended with \"conv1\" or \"conv2\" in this method only for\n  # historical reasons. These are in fact dense layers.\n  layer_name = \"%s_{}\" % name if name else \"{}\"\n  h = dense(\n      inputs,\n      filter_size,\n      use_bias=True,\n      activation=tf.nn.relu,\n      layer_collection=layer_collection,\n      name=layer_name.format(\"conv1\"))\n\n  if dropout != 0.0:\n    h = dropout_with_broadcast_dims(\n        h, 1.0 - dropout, broadcast_dims=dropout_broadcast_dims)\n  o = dense(\n      h,\n      output_size,\n      activation=output_activation,\n      use_bias=True,\n      layer_collection=layer_collection,\n      name=layer_name.format(\"conv2\"))\n  return o", "output": "Hidden layer with RELU activation followed by linear projection.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_mvn(self, name, input_name, output_name, across_channels = True, normalize_variance = True, epsilon = 1e-5):\n        \"\"\"\n        \n        \"\"\"\n\n        spec = self.spec\n        nn_spec = self.nn_spec\n\n        # Add a new layer\n        spec_layer = nn_spec.layers.add()\n        spec_layer.name = name\n        spec_layer.input.append(input_name)\n        spec_layer.output.append(output_name)\n\n        spec_layer_params = spec_layer.mvn\n        spec_layer_params.acrossChannels = across_channels\n        spec_layer_params.normalizeVariance = normalize_variance\n        spec_layer_params.epsilon = epsilon", "output": "Add an MVN (mean variance normalization) layer. Computes mean, variance and normalizes the input.\n\n        Parameters\n        ----------\n        name: str\n            The name of this layer.\n\n        input_name: str\n            The input blob name of this layer.\n        output_name: str\n            The output blob name of this layer.\n\n        across_channels: boolean\n            If False, each channel plane is normalized separately\n            If True, mean/variance is computed across all C, H and W dimensions\n\n        normalize_variance: boolean\n            If False, only mean subtraction is performed.\n\n        epsilon: float\n            small bias to avoid division by zero.\n\n\n        See Also\n        --------\n        add_l2_normalize, add_lrn", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def restore_backup(path, backup_id):\n    '''\n    \n    '''\n    path = os.path.expanduser(path)\n\n    # Note: This only supports minion backups, so this function will need to be\n    # modified if/when master backups are implemented.\n    ret = {'result': False,\n           'comment': 'Invalid backup_id \\'{0}\\''.format(backup_id)}\n    try:\n        if len(six.text_type(backup_id)) == len(six.text_type(int(backup_id))):\n            backup = list_backups(path)[int(backup_id)]\n        else:\n            return ret\n    except ValueError:\n        return ret\n    except KeyError:\n        ret['comment'] = 'backup_id \\'{0}\\' does not exist for ' \\\n                         '{1}'.format(backup_id, path)\n        return ret\n\n    salt.utils.files.backup_minion(path, _get_bkroot())\n    try:\n        shutil.copyfile(backup['Location'], path)\n    except IOError as exc:\n        ret['comment'] = \\\n            'Unable to restore {0} to {1}: ' \\\n            '{2}'.format(backup['Location'], path, exc)\n        return ret\n    else:\n        ret['result'] = True\n        ret['comment'] = 'Successfully restored {0} to ' \\\n                         '{1}'.format(backup['Location'], path)\n\n    # Try to set proper ownership\n    if not salt.utils.platform.is_windows():\n        try:\n            fstat = os.stat(path)\n        except (OSError, IOError):\n            ret['comment'] += ', but was unable to set ownership'\n        else:\n            os.chown(path, fstat.st_uid, fstat.st_gid)\n\n    return ret", "output": ".. versionadded:: 0.17.0\n\n    Restore a previous version of a file that was backed up using Salt's\n    :ref:`file state backup <file-state-backups>` system.\n\n    path\n        The path on the minion to check for backups\n    backup_id\n        The numeric id for the backup you wish to restore, as found using\n        :mod:`file.list_backups <salt.modules.file.list_backups>`\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.restore_backup /foo/bar/baz.txt 0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def class_register(cls):\n    \"\"\"\"\"\"\n    cls.handler_registry = {}\n    cls.sender_registry = {}\n    for method_name in dir(cls):\n        method = getattr(cls, method_name)\n        if hasattr(method, '_handle'):\n            cls.handler_registry.update({method._handle: method_name})\n        if hasattr(method, '_sends'):\n            cls.sender_registry.update({method._sends: method_name})\n    return cls", "output": "Class decorator that allows to map LSP method names to class methods.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_partition_boundary(boundary):\n    '''\n    \n    '''\n    boundary = six.text_type(boundary)\n    match = re.search(r'^([\\d.]+)(\\D*)$', boundary)\n    if match:\n        unit = match.group(2)\n        if not unit or unit in VALID_UNITS:\n            return\n    raise CommandExecutionError(\n        'Invalid partition boundary passed: \"{0}\"'.format(boundary)\n    )", "output": "Ensure valid partition boundaries are supplied.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_tensor_summary(x, types, name=None, collections=None,\n                       main_tower_only=True):\n    \"\"\"\n    \n    \"\"\"\n    types = set(types)\n    if name is None:\n        name = x.op.name\n    ctx = get_current_tower_context()\n    if main_tower_only and ctx is not None and not ctx.is_main_training_tower:\n        return\n\n    SUMMARY_TYPES_DIC = {\n        'scalar': lambda: tf.summary.scalar(name + '-summary', x, collections=collections),\n        'histogram': lambda: tf.summary.histogram(name + '-histogram', x, collections=collections),\n        'sparsity': lambda: tf.summary.scalar(\n            name + '-sparsity', tf.nn.zero_fraction(x),\n            collections=collections),\n        'mean': lambda: tf.summary.scalar(\n            name + '-mean', tf.reduce_mean(x),\n            collections=collections),\n        'rms': lambda: tf.summary.scalar(\n            name + '-rms', rms(x), collections=collections)\n    }\n    for typ in types:\n        SUMMARY_TYPES_DIC[typ]()", "output": "Summarize a tensor by different methods.\n\n    Args:\n        x (tf.Tensor): a tensor to summarize\n        types (list[str]): summary types, can be scalar/histogram/sparsity/mean/rms\n        name (str): summary name. Defaults to be the op name.\n        collections (list[str]): collections of the summary ops.\n        main_tower_only (bool): Only run under main training tower. If\n            set to True, calling this function under other TowerContext\n            has no effect.\n\n    Example:\n\n    .. code-block:: python\n\n        with tf.name_scope('mysummaries'):  # to not mess up tensorboard\n            add_tensor_summary(\n                tensor, ['histogram', 'rms', 'sparsity'], name='mytensor')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _seconds_as_string(seconds):\n    \"\"\"\n    \n    \"\"\"\n    TIME_UNITS = [('s', 60), ('m', 60), ('h', 24), ('d', None)]\n    unit_strings = []\n    cur = max(int(seconds), 1)\n    for suffix, size in TIME_UNITS:\n        if size is not None:\n            cur, rest = divmod(cur, size)\n        else:\n            rest = cur\n        if rest > 0:\n            unit_strings.insert(0, '%d%s' % (rest, suffix))\n    return ' '.join(unit_strings)", "output": "Returns seconds as a human-friendly string, e.g. '1d 4h 47m 41s'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_state_multi_precision(self, index, weight):\n        \"\"\"\n        \"\"\"\n        weight_master_copy = None\n        if self.multi_precision and weight.dtype == numpy.float16:\n            weight_master_copy = weight.astype(numpy.float32)\n            return (weight_master_copy,) + (self.create_state(index, weight_master_copy),)\n        if weight.dtype == numpy.float16 and not self.multi_precision:\n            warnings.warn(\"Accumulating with float16 in optimizer can lead to \"\n                          \"poor accuracy or slow convergence. \"\n                          \"Consider using multi_precision=True option of the \"\n                          \"optimizer\")\n        return self.create_state(index, weight)", "output": "Creates auxiliary state for a given weight, including FP32 high\n        precision copy if original weight is FP16.\n\n        This method is provided to perform automatic mixed precision training\n        for optimizers that do not support it themselves.\n\n        Parameters\n        ----------\n        index : int\n            An unique index to identify the weight.\n        weight : NDArray\n            The weight.\n\n        Returns\n        -------\n        state : any obj\n            The state associated with the weight.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_lock(remote=None):\n    '''\n    \n    '''\n    def _do_clear_lock(repo):\n        def _add_error(errlist, repo, exc):\n            msg = ('Unable to remove update lock for {0} ({1}): {2} '\n                   .format(repo['url'], repo['lockfile'], exc))\n            log.debug(msg)\n            errlist.append(msg)\n        success = []\n        failed = []\n        if os.path.exists(repo['lockfile']):\n            try:\n                os.remove(repo['lockfile'])\n            except OSError as exc:\n                if exc.errno == errno.EISDIR:\n                    # Somehow this path is a directory. Should never happen\n                    # unless some wiseguy manually creates a directory at this\n                    # path, but just in case, handle it.\n                    try:\n                        shutil.rmtree(repo['lockfile'])\n                    except OSError as exc:\n                        _add_error(failed, repo, exc)\n                else:\n                    _add_error(failed, repo, exc)\n            else:\n                msg = 'Removed lock for {0}'.format(repo['url'])\n                log.debug(msg)\n                success.append(msg)\n        return success, failed\n\n    if isinstance(remote, dict):\n        return _do_clear_lock(remote)\n\n    cleared = []\n    errors = []\n    for repo in init():\n        if remote:\n            try:\n                if remote not in repo['url']:\n                    continue\n            except TypeError:\n                # remote was non-string, try again\n                if six.text_type(remote) not in repo['url']:\n                    continue\n        success, failed = _do_clear_lock(repo)\n        cleared.extend(success)\n        errors.extend(failed)\n    return cleared, errors", "output": "Clear update.lk\n\n    ``remote`` can either be a dictionary containing repo configuration\n    information, or a pattern. If the latter, then remotes for which the URL\n    matches the pattern will be locked.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rename_key(pki_dir, id_, new_id):\n    '''\n    \n    '''\n    oldkey = os.path.join(pki_dir, 'minions', id_)\n    newkey = os.path.join(pki_dir, 'minions', new_id)\n    if os.path.isfile(oldkey):\n        os.rename(oldkey, newkey)", "output": "Rename a key, when an instance has also been renamed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def assign(name, value):\n    '''\n    \n    '''\n    ret = {}\n    cmd = 'sysctl {0}=\"{1}\"'.format(name, value)\n    data = __salt__['cmd.run_all'](cmd, python_shell=False)\n\n    if data['retcode'] != 0:\n        raise CommandExecutionError('sysctl failed: {0}'.format(\n            data['stderr']))\n    new_name, new_value = data['stdout'].split(':', 1)\n    ret[new_name] = new_value.split(' -> ')[-1]\n    return ret", "output": "Assign a single sysctl parameter for this minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sysctl.assign net.inet.icmp.icmplim 50", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def normalized(self):\n        ''\n        res = self.groupby('code').apply(lambda x: x / x.iloc[0])\n        return res", "output": "\u5f52\u4e00\u5316", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def afx_small():\n  \"\"\"\"\"\"\n  hparams = transformer.transformer_tpu()\n  hparams.filter_size = 1024\n  hparams.num_heads = 4\n  hparams.num_hidden_layers = 3\n  hparams.batch_size = 512\n  return hparams", "output": "Small transformer model with small batch size for fast step times.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_loginclass(name):\n    '''\n    \n\n    '''\n\n    userinfo = __salt__['cmd.run_stdout'](['pw', 'usershow', '-n', name])\n    userinfo = userinfo.split(':')\n\n    return userinfo[4] if len(userinfo) == 10 else ''", "output": "Get the login class of the user\n\n    .. versionadded:: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.get_loginclass foo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_default_response(self, input_statement):\n        \"\"\"\n        \n        \"\"\"\n        from random import choice\n\n        if self.default_responses:\n            response = choice(self.default_responses)\n        else:\n            try:\n                response = self.chatbot.storage.get_random()\n            except StorageAdapter.EmptyDatabaseException:\n                response = input_statement\n\n        self.chatbot.logger.info(\n            'No known response to the input was found. Selecting a random response.'\n        )\n\n        # Set confidence to zero because a random response is selected\n        response.confidence = 0\n\n        return response", "output": "This method is called when a logic adapter is unable to generate any\n        other meaningful response.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_updates(self, startup=False):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        from spyder.workers.updates import WorkerUpdates\r\n\r\n        # Disable check_updates_action while the thread is working\r\n        self.check_updates_action.setDisabled(True)\r\n\r\n        if self.thread_updates is not None:\r\n            self.thread_updates.terminate()\r\n\r\n        self.thread_updates = QThread(self)\r\n        self.worker_updates = WorkerUpdates(self, startup=startup)\r\n        self.worker_updates.sig_ready.connect(self._check_updates_ready)\r\n        self.worker_updates.sig_ready.connect(self.thread_updates.quit)\r\n        self.worker_updates.moveToThread(self.thread_updates)\r\n        self.thread_updates.started.connect(self.worker_updates.start)\r\n        self.thread_updates.start()", "output": "Check for spyder updates on github releases using a QThread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _maybe_dt_array(array):\n    \"\"\"  \"\"\"\n    if not isinstance(array, DataTable) or array is None:\n        return array\n\n    if array.shape[1] > 1:\n        raise ValueError('DataTable for label or weight cannot have multiple columns')\n\n    # below requires new dt version\n    # extract first column\n    array = array.to_numpy()[:, 0].astype('float')\n\n    return array", "output": "Extract numpy array from single column data table", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_to_file(self, fileinfo, filename):\r\n        \"\"\"\r\n        \"\"\"\r\n        txt = to_text_string(fileinfo.editor.get_text_with_eol())\r\n        fileinfo.encoding = encoding.write(txt, filename, fileinfo.encoding)", "output": "Low-level function for writing text of editor to file.\r\n\r\n        Args:\r\n            fileinfo: FileInfo object associated to editor to be saved\r\n            filename: str with filename to save to\r\n\r\n        This is a low-level function that only saves the text to file in the\r\n        correct encoding without doing any error handling.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_config(name, xpath=None, value=None, commit=False):\n    '''\n    \n\n    '''\n    ret = _default_ret(name)\n\n    result, msg = _set_config(xpath, value)\n\n    ret.update({\n        'comment': msg,\n        'result': result\n    })\n\n    if not result:\n        return ret\n\n    if commit is True:\n        ret.update({\n            'commit': __salt__['panos.commit'](),\n            'result': True\n        })\n\n    return ret", "output": "Sets a Palo Alto XPATH to a specific value. This will always overwrite the existing value, even if it is not\n    changed.\n\n    You can add or create a new object at a specified location in the configuration hierarchy. Use the xpath parameter\n    to specify the location of the object in the configuration\n\n    name: The name of the module function to execute.\n\n    xpath(str): The XPATH of the configuration API tree to control.\n\n    value(str): The XML value to set. This must be a child to the XPATH.\n\n    commit(bool): If true the firewall will commit the changes, if false do not commit changes.\n\n    SLS Example:\n\n    .. code-block:: yaml\n\n        panos/hostname:\n            panos.set_config:\n              - xpath: /config/devices/entry[@name='localhost.localdomain']/deviceconfig/system\n              - value: <hostname>foobar</hostname>\n              - commit: True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filter(self, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        from django.db.models import Q\n\n        Statement = self.get_model('statement')\n\n        kwargs.pop('page_size', 1000)\n        order_by = kwargs.pop('order_by', None)\n        tags = kwargs.pop('tags', [])\n        exclude_text = kwargs.pop('exclude_text', None)\n        exclude_text_words = kwargs.pop('exclude_text_words', [])\n        persona_not_startswith = kwargs.pop('persona_not_startswith', None)\n        search_text_contains = kwargs.pop('search_text_contains', None)\n\n        # Convert a single sting into a list if only one tag is provided\n        if type(tags) == str:\n            tags = [tags]\n\n        if tags:\n            kwargs['tags__name__in'] = tags\n\n        statements = Statement.objects.filter(**kwargs)\n\n        if exclude_text:\n            statements = statements.exclude(\n                text__in=exclude_text\n            )\n\n        if exclude_text_words:\n            or_query = [\n                ~Q(text__icontains=word) for word in exclude_text_words\n            ]\n\n            statements = statements.filter(\n                *or_query\n            )\n\n        if persona_not_startswith:\n            statements = statements.exclude(\n                persona__startswith='bot:'\n            )\n\n        if search_text_contains:\n            or_query = Q()\n\n            for word in search_text_contains.split(' '):\n                or_query |= Q(search_text__contains=word)\n\n            statements = statements.filter(\n                or_query\n            )\n\n        if order_by:\n            statements = statements.order_by(*order_by)\n\n        for statement in statements.iterator():\n            yield statement", "output": "Returns a list of statements in the database\n        that match the parameters specified.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ext_pillar(hyper_id, pillar, name, key):\n    '''\n    \n    '''\n    vk = salt.utils.virt.VirtKey(hyper_id, name, __opts__)\n    ok = vk.accept(key)\n    pillar['virtkey'] = {name: ok}\n    return {}", "output": "Accept the key for the VM on the hyper, if authorized.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_check(exc_type, template, pred, actual, funcname):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(funcname, str):\n        def get_funcname(_):\n            return funcname\n    else:\n        get_funcname = funcname\n\n    def _check(func, argname, argvalue):\n        if pred(argvalue):\n            raise exc_type(\n                template % {\n                    'funcname': get_funcname(func),\n                    'argname': argname,\n                    'actual': actual(argvalue),\n                },\n            )\n        return argvalue\n    return _check", "output": "Factory for making preprocessing functions that check a predicate on the\n    input value.\n\n    Parameters\n    ----------\n    exc_type : Exception\n        The exception type to raise if the predicate fails.\n    template : str\n        A template string to use to create error messages.\n        Should have %-style named template parameters for 'funcname',\n        'argname', and 'actual'.\n    pred : function[object -> bool]\n        A function to call on the argument being preprocessed.  If the\n        predicate returns `True`, we raise an instance of `exc_type`.\n    actual : function[object -> object]\n        A function to call on bad values to produce the value to display in the\n        error message.\n    funcname : str or callable\n        Name to use in error messages, or function to call on decorated\n        functions to produce a name.  Passing an explicit name is useful when\n        creating checks for __init__ or __new__ methods when you want the error\n        to refer to the class name instead of the method name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_task_status(self, task):\n        \"\"\"\"\"\"\n        if not self.interactive:\n            super(OneScheduler, self).on_task_status(task)\n\n        try:\n            procesok = task['track']['process']['ok']\n        except KeyError as e:\n            logger.error(\"Bad status pack: %s\", e)\n            return None\n\n        if procesok:\n            ret = self.on_task_done(task)\n        else:\n            ret = self.on_task_failed(task)\n        if task['track']['fetch'].get('time'):\n            self._cnt['5m_time'].event((task['project'], 'fetch_time'),\n                                       task['track']['fetch']['time'])\n        if task['track']['process'].get('time'):\n            self._cnt['5m_time'].event((task['project'], 'process_time'),\n                                       task['track']['process'].get('time'))\n        self.projects[task['project']].active_tasks.appendleft((time.time(), task))\n        return ret", "output": "Ignore not processing error in interactive mode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _encode_gif(images, fps):\n  \"\"\"\n  \"\"\"\n  writer = WholeVideoWriter(fps)\n  writer.write_multi(images)\n  return writer.finish()", "output": "Encodes numpy images into gif string.\n\n  Args:\n    images: A 4-D `uint8` `np.array` (or a list of 3-D images) of shape\n      `[time, height, width, channels]` where `channels` is 1 or 3.\n    fps: frames per second of the animation\n\n  Returns:\n    The encoded gif string.\n\n  Raises:\n    IOError: If the ffmpeg command returns an error.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _environment_sanity_check(environment):\n    \"\"\"\"\"\"\n    assert issubclass(environment.undefined, Undefined), 'undefined must ' \\\n        'be a subclass of undefined because filters depend on it.'\n    assert environment.block_start_string != \\\n        environment.variable_start_string != \\\n        environment.comment_start_string, 'block, variable and comment ' \\\n        'start strings must be different'\n    assert environment.newline_sequence in ('\\r', '\\r\\n', '\\n'), \\\n        'newline_sequence set to unknown line ending string.'\n    return environment", "output": "Perform a sanity check on the environment.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def safecall(func):\n    \"\"\"\"\"\"\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception:\n            pass\n    return wrapper", "output": "Wraps a function so that it swallows exceptions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expr_match(line, expr):\n    '''\n    \n    '''\n    try:\n        if fnmatch.fnmatch(line, expr):\n            return True\n        try:\n            if re.match(r'\\A{0}\\Z'.format(expr), line):\n                return True\n        except re.error:\n            pass\n    except TypeError:\n        log.exception('Value %r or expression %r is not a string', line, expr)\n    return False", "output": "Checks whether or not the passed value matches the specified expression.\n    Tries to match expr first as a glob using fnmatch.fnmatch(), and then tries\n    to match expr as a regular expression. Originally designed to match minion\n    IDs for whitelists/blacklists.\n\n    Note that this also does exact matches, as fnmatch.fnmatch() will return\n    ``True`` when no glob characters are used and the string is an exact match:\n\n    .. code-block:: python\n\n        >>> fnmatch.fnmatch('foo', 'foo')\n        True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_nat_small():\n  \"\"\"\"\"\"\n  hparams = transformer.transformer_small()\n  hparams.batch_size = 2048\n  hparams.learning_rate = 0.2\n  hparams.learning_rate_warmup_steps = 4000\n  hparams.num_hidden_layers = 3\n  hparams.hidden_size = 384\n  hparams.filter_size = 2048\n  hparams.label_smoothing = 0.0\n  hparams.force_full_predict = True\n  hparams.optimizer = \"adam\"\n  hparams.optimizer_adam_epsilon = 1e-9\n  hparams.optimizer_adam_beta1 = 0.9\n  hparams.optimizer_adam_beta2 = 0.997\n  hparams.add_hparam(\"bottleneck_kind\", \"vq\")\n  hparams.add_hparam(\"bottleneck_bits\", 12)\n  hparams.add_hparam(\"num_compress_steps\", 3)\n  hparams.add_hparam(\"beta\", 0.25)\n  hparams.add_hparam(\"epsilon\", 1e-5)\n  hparams.add_hparam(\"decay\", 0.999)\n  hparams.add_hparam(\"num_samples\", 10)\n  hparams.add_hparam(\"mask_startup_steps\", 50000)\n  return hparams", "output": "Set of hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_params(self):\n    \"\"\"\n    \n    \"\"\"\n    assert tf.executing_eagerly()\n    out = []\n\n    # Collecting params from each layer.\n    for layer_name in self.layers:\n      out += self.get_layer_params(layer_name)\n    return out", "output": "Provides access to the model's parameters.\n    Works arounds the non-availability of graph collections in\n                    eager mode.\n    :return: A list of all Variables defining the model parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def license_from_classifiers(self):\n        \"\"\"\n            \n        \"\"\"\n        if len(self.classifiers) > 0:\n            for c in self.classifiers:\n                if c.startswith(\"License\"):\n                    return c.split(\" :: \")[-1]", "output": ">>> package = yarg.get('yarg')\n            >>> package.license_from_classifiers\n            u'MIT License'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def max_number(self, rows: List[Row], column: NumberColumn) -> Number:\n        \"\"\"\n        \n        \"\"\"\n        cell_values = [row.values[column.name] for row in rows]\n        if not cell_values:\n            return 0.0  # type: ignore\n        if not all([isinstance(value, Number) for value in cell_values]):\n            raise ExecutionError(f\"Invalid values for number selection function: {cell_values}\")\n        return max(cell_values)", "output": "Takes a list of rows and a column and returns the max of the values under that column in\n        those rows.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_config(self, cfg=None):\n        '''\n        \n        '''\n        _opts, _args = optparse.OptionParser.parse_args(self)\n        configs = self.find_existing_configs(_opts.support_unit)\n        if configs and cfg not in configs:\n            cfg = configs[0]\n\n        return config.master_config(self.get_config_file_path(cfg))", "output": "Open suitable config file.\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lvremove(lvname, vgname):\n    '''\n    \n    '''\n    cmd = ['lvremove', '-f', '{0}/{1}'.format(vgname, lvname)]\n    out = __salt__['cmd.run'](cmd, python_shell=False)\n    return out.strip()", "output": "Remove a given existing logical volume from a named existing volume group\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' lvm.lvremove lvname vgname force=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _GetResponseClass(self, method_descriptor):\n    \"\"\"\n    \"\"\"\n    if method_descriptor.containing_service != self.descriptor:\n      raise RuntimeError(\n          'GetResponseClass() given method descriptor for wrong service type.')\n    return method_descriptor.output_type._concrete_class", "output": "Returns the class of the response protocol message.\n\n    Args:\n      method_descriptor: Descriptor of the method for which to return the\n        response protocol message class.\n\n    Returns:\n      A class that represents the output protocol message of the specified\n      method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_account(message, collection=DATABASE.account):\n    \"\"\"\n    \"\"\"\n    try:\n        collection.create_index(\n            [(\"account_cookie\", ASCENDING), (\"user_cookie\", ASCENDING), (\"portfolio_cookie\", ASCENDING)], unique=True)\n    except:\n        pass\n    collection.update(\n        {'account_cookie': message['account_cookie'], 'portfolio_cookie':\n            message['portfolio_cookie'], 'user_cookie': message['user_cookie']},\n        {'$set': message},\n        upsert=True\n    )", "output": "save account\n\n    Arguments:\n        message {[type]} -- [description]\n\n    Keyword Arguments:\n        collection {[type]} -- [description] (default: {DATABASE})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_table_row(contents, tag='td'):\n  \"\"\"\n  \"\"\"\n  columns = ('<%s>%s</%s>\\n' % (tag, s, tag) for s in contents)\n  return '<tr>\\n' + ''.join(columns) + '</tr>\\n'", "output": "Given an iterable of string contents, make a table row.\n\n  Args:\n    contents: An iterable yielding strings.\n    tag: The tag to place contents in. Defaults to 'td', you might want 'th'.\n\n  Returns:\n    A string containing the content strings, organized into a table row.\n\n  Example: make_table_row(['one', 'two', 'three']) == '''\n  <tr>\n  <td>one</td>\n  <td>two</td>\n  <td>three</td>\n  </tr>'''", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vq_discrete_bottleneck(x,\n                           bottleneck_bits,\n                           beta=0.25,\n                           decay=0.999,\n                           epsilon=1e-5,\n                           soft_em=False,\n                           num_samples=10):\n  \"\"\"\"\"\"\n  bottleneck_size = 2**bottleneck_bits\n  x_means_hot, e_loss, _ = vq_body(\n      x,\n      bottleneck_size,\n      beta=beta,\n      decay=decay,\n      epsilon=epsilon,\n      soft_em=soft_em,\n      num_samples=num_samples)\n  return x_means_hot, e_loss", "output": "Simple vector quantized discrete bottleneck.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def percent_cb(name, complete, total):\n    \"\"\"  \"\"\"\n    logger.debug(\n        \"{}: {} transferred out of {}\".format(\n            name, sizeof_fmt(complete), sizeof_fmt(total)\n        )\n    )\n    progress.update_target(name, complete, total)", "output": "Callback for updating target progress", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dumps(self):\n        \"\"\"\n        \"\"\"\n        return json.dumps([self.__class__.__name__.lower(), self._kwargs])", "output": "Saves the Augmenter to string\n\n        Returns\n        -------\n        str\n            JSON formatted string that describes the Augmenter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetch_task_failures(tracking_url):\n    \"\"\"\n    \n    \"\"\"\n    import mechanize\n    timeout = 3.0\n    failures_url = tracking_url.replace('jobdetails.jsp', 'jobfailures.jsp') + '&cause=failed'\n    logger.debug('Fetching data from %s', failures_url)\n    b = mechanize.Browser()\n    b.open(failures_url, timeout=timeout)\n    links = list(b.links(text_regex='Last 4KB'))  # For some reason text_regex='All' doesn't work... no idea why\n    links = random.sample(links, min(10, len(links)))  # Fetch a random subset of all failed tasks, so not to be biased towards the early fails\n    error_text = []\n    for link in links:\n        task_url = link.url.replace('&start=-4097', '&start=-100000')  # Increase the offset\n        logger.debug('Fetching data from %s', task_url)\n        b2 = mechanize.Browser()\n        try:\n            r = b2.open(task_url, timeout=timeout)\n            data = r.read()\n        except Exception as e:\n            logger.debug('Error fetching data from %s: %s', task_url, e)\n            continue\n        # Try to get the hex-encoded traceback back from the output\n        for exc in re.findall(r'luigi-exc-hex=[0-9a-f]+', data):\n            error_text.append('---------- %s:' % task_url)\n            error_text.append(exc.split('=')[-1].decode('hex'))\n\n    return '\\n'.join(error_text)", "output": "Uses mechanize to fetch the actual task logs from the task tracker.\n\n    This is highly opportunistic, and we might not succeed.\n    So we set a low timeout and hope it works.\n    If it does not, it's not the end of the world.\n\n    TODO: Yarn has a REST API that we should probably use instead:\n    http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_to_style(cls, style_dict):\n        \"\"\"\n        \n        \"\"\"\n\n        from openpyxl.style import Style\n        xls_style = Style()\n        for key, value in style_dict.items():\n            for nk, nv in value.items():\n                if key == \"borders\":\n                    (xls_style.borders.__getattribute__(nk)\n                     .__setattr__('border_style', nv))\n                else:\n                    xls_style.__getattribute__(key).__setattr__(nk, nv)\n\n        return xls_style", "output": "converts a style_dict to an openpyxl style object\n        Parameters\n        ----------\n        style_dict : style dictionary to convert", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def most_uncertain_by_mask(self, mask, y):\n        \"\"\" \n        \"\"\"\n        idxs = np.where(mask)[0]\n        # the most uncertain samples will have abs(probs-1/num_classes) close to 0;\n        return idxs[np.argsort(np.abs(self.probs[idxs,y]-(1/self.num_classes)))[:4]]", "output": "Extracts the first 4 most uncertain indexes from the ordered list of probabilities\n\n            Arguments:\n                mask (numpy.ndarray): the mask of probabilities specific to the selected class; a boolean array with shape (num_of_samples,) which contains True where class==selected_class, and False everywhere else\n                y (int): the selected class\n\n            Returns:\n                idxs (ndarray): An array of indexes of length 4", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deeper_conv_block(conv_layer, kernel_size, weighted=True):\n    '''\n    '''\n    n_dim = get_n_dim(conv_layer)\n    filter_shape = (kernel_size,) * 2\n    n_filters = conv_layer.filters\n    weight = np.zeros((n_filters, n_filters) + filter_shape)\n    center = tuple(map(lambda x: int((x - 1) / 2), filter_shape))\n    for i in range(n_filters):\n        filter_weight = np.zeros((n_filters,) + filter_shape)\n        index = (i,) + center\n        filter_weight[index] = 1\n        weight[i, ...] = filter_weight\n    bias = np.zeros(n_filters)\n    new_conv_layer = get_conv_class(n_dim)(\n        conv_layer.filters, n_filters, kernel_size=kernel_size\n    )\n    bn = get_batch_norm_class(n_dim)(n_filters)\n\n    if weighted:\n        new_conv_layer.set_weights(\n            (add_noise(weight, np.array([0, 1])), add_noise(bias, np.array([0, 1])))\n        )\n        new_weights = [\n            add_noise(np.ones(n_filters, dtype=np.float32), np.array([0, 1])),\n            add_noise(np.zeros(n_filters, dtype=np.float32), np.array([0, 1])),\n            add_noise(np.zeros(n_filters, dtype=np.float32), np.array([0, 1])),\n            add_noise(np.ones(n_filters, dtype=np.float32), np.array([0, 1])),\n        ]\n        bn.set_weights(new_weights)\n\n    return [StubReLU(), new_conv_layer, bn]", "output": "deeper conv layer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_converter(dataType):\n    \"\"\" \"\"\"\n    if not _need_converter(dataType):\n        return lambda x: x\n\n    if isinstance(dataType, ArrayType):\n        conv = _create_converter(dataType.elementType)\n        return lambda row: [conv(v) for v in row]\n\n    elif isinstance(dataType, MapType):\n        kconv = _create_converter(dataType.keyType)\n        vconv = _create_converter(dataType.valueType)\n        return lambda row: dict((kconv(k), vconv(v)) for k, v in row.items())\n\n    elif isinstance(dataType, NullType):\n        return lambda x: None\n\n    elif not isinstance(dataType, StructType):\n        return lambda x: x\n\n    # dataType must be StructType\n    names = [f.name for f in dataType.fields]\n    converters = [_create_converter(f.dataType) for f in dataType.fields]\n    convert_fields = any(_need_converter(f.dataType) for f in dataType.fields)\n\n    def convert_struct(obj):\n        if obj is None:\n            return\n\n        if isinstance(obj, (tuple, list)):\n            if convert_fields:\n                return tuple(conv(v) for v, conv in zip(obj, converters))\n            else:\n                return tuple(obj)\n\n        if isinstance(obj, dict):\n            d = obj\n        elif hasattr(obj, \"__dict__\"):  # object\n            d = obj.__dict__\n        else:\n            raise TypeError(\"Unexpected obj type: %s\" % type(obj))\n\n        if convert_fields:\n            return tuple([conv(d.get(name)) for name, conv in zip(names, converters)])\n        else:\n            return tuple([d.get(name) for name in names])\n\n    return convert_struct", "output": "Create a converter to drop the names of fields in obj", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_pipelines(region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    client = _get_client(region, key, keyid, profile)\n    r = {}\n    try:\n        paginator = client.get_paginator('list_pipelines')\n        pipelines = []\n        for page in paginator.paginate():\n            pipelines += page['pipelineIdList']\n        r['result'] = pipelines\n    except (botocore.exceptions.BotoCoreError, botocore.exceptions.ClientError) as e:\n        r['error'] = six.text_type(e)\n    return r", "output": "Get a list of pipeline ids and names for all pipelines.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_datapipeline.list_pipelines profile=myprofile", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_existing_pidfile(pidfile_path):\n    \"\"\" \n\n        \"\"\"\n    try:\n        os.remove(pidfile_path)\n    except OSError as exc:\n        if exc.errno == errno.ENOENT:\n            pass\n        else:\n            raise", "output": "Remove the named PID file if it exists.\n\n        Removing a PID file that doesn't already exist puts us in the\n        desired state, so we ignore the condition if the file does not\n        exist.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Audio(self, run, tag):\n    \"\"\"\n    \"\"\"\n    accumulator = self.GetAccumulator(run)\n    return accumulator.Audio(tag)", "output": "Retrieve the audio events associated with a run and tag.\n\n    Args:\n      run: A string name of the run for which values are retrieved.\n      tag: A string name of the tag for which values are retrieved.\n\n    Raises:\n      KeyError: If the run is not found, or the tag is not available for\n        the given run.\n\n    Returns:\n      An array of `event_accumulator.AudioEvents`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_command_formatting(self, command):\n        \"\"\"\n        \"\"\"\n\n        if command.description:\n            self.paginator.add_line(command.description, empty=True)\n\n        signature = self.get_command_signature(command)\n        if command.aliases:\n            self.paginator.add_line(signature)\n            self.add_aliases_formatting(command.aliases)\n        else:\n            self.paginator.add_line(signature, empty=True)\n\n        if command.help:\n            try:\n                self.paginator.add_line(command.help, empty=True)\n            except RuntimeError:\n                for line in command.help.splitlines():\n                    self.paginator.add_line(line)\n                self.paginator.add_line()", "output": "A utility function to format commands and groups.\n\n        Parameters\n        ------------\n        command: :class:`Command`\n            The command to format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_compact(graph_path):\n    \"\"\" \"\"\"\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n        # Note, we just load the graph and do *not* need to initialize anything.\n        with tf.gfile.GFile(graph_path, \"rb\") as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n            tf.import_graph_def(graph_def)\n\n        input_img = sess.graph.get_tensor_by_name('import/input_img:0')\n        prediction_img = sess.graph.get_tensor_by_name('import/prediction_img:0')\n\n        prediction = sess.run(prediction_img, {input_img: cv2.imread('lena.png')[None, ...]})\n        cv2.imwrite('applied_compact.png', prediction[0])", "output": "Run the pruned and frozen inference graph.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def version_cmp(pkg1, pkg2, ignore_epoch=False, **kwargs):\n    '''\n    \n    '''\n    del ignore_epoch  # Unused parameter\n\n    # Don't worry about ignore_epoch since we're shelling out to pkg.\n    sym = {\n        '<': -1,\n        '>': 1,\n        '=': 0,\n    }\n    try:\n        cmd = ['pkg', 'version', '--test-version', pkg1, pkg2]\n        ret = __salt__['cmd.run_all'](cmd,\n                                      output_loglevel='trace',\n                                      python_shell=False,\n                                      ignore_retcode=True)\n        if ret['stdout'] in sym:\n            return sym[ret['stdout']]\n\n    except Exception as exc:\n        log.error(exc)\n\n    return None", "output": "Do a cmp-style comparison on two packages. Return -1 if pkg1 < pkg2, 0 if\n    pkg1 == pkg2, and 1 if pkg1 > pkg2. Return None if there was a problem\n    making the comparison.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.version_cmp '2.1.11' '2.1.12'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def DELETE_SLICE_0(self, instr):\n        ''\n        value = self.ast_stack.pop()\n\n        kw = dict(lineno=instr.lineno, col_offset=0)\n        slice = _ast.Slice(lower=None, step=None, upper=None, **kw)\n        subscr = _ast.Subscript(value=value, slice=slice, ctx=_ast.Del(), **kw)\n\n        delete = _ast.Delete(targets=[subscr], **kw)\n        self.ast_stack.append(delete)", "output": "obj[:] = expr", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_locations(profile, **libcloud_kwargs):\n    '''\n    \n    '''\n    conn = _get_driver(profile=profile)\n    libcloud_kwargs = salt.utils.args.clean_kwargs(**libcloud_kwargs)\n    locations = conn.list_locations(**libcloud_kwargs)\n\n    ret = []\n    for loc in locations:\n        ret.append(_simple_location(loc))\n    return ret", "output": "Return a list of locations for this cloud\n\n    :param profile: The profile key\n    :type  profile: ``str``\n\n    :param libcloud_kwargs: Extra arguments for the driver's list_locations method\n    :type  libcloud_kwargs: ``dict``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion libcloud_compute.list_locations profile1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def push_error_to_driver_through_redis(redis_client,\n                                       error_type,\n                                       message,\n                                       driver_id=None):\n    \"\"\"\n    \"\"\"\n    if driver_id is None:\n        driver_id = ray.DriverID.nil()\n    # Do everything in Python and through the Python Redis client instead\n    # of through the raylet.\n    error_data = ray.gcs_utils.construct_error_message(driver_id, error_type,\n                                                       message, time.time())\n    redis_client.execute_command(\"RAY.TABLE_APPEND\",\n                                 ray.gcs_utils.TablePrefix.ERROR_INFO,\n                                 ray.gcs_utils.TablePubsub.ERROR_INFO,\n                                 driver_id.binary(), error_data)", "output": "Push an error message to the driver to be printed in the background.\n\n    Normally the push_error_to_driver function should be used. However, in some\n    instances, the raylet client is not available, e.g., because the\n    error happens in Python before the driver or worker has connected to the\n    backend processes.\n\n    Args:\n        redis_client: The redis client to use.\n        error_type (str): The type of the error.\n        message (str): The message that will be printed in the background\n            on the driver.\n        driver_id: The ID of the driver to push the error message to. If this\n            is None, then the message will be pushed to all drivers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_root_item(self, item):\r\n        \"\"\"\"\"\"\r\n        root_item = item\r\n        while isinstance(root_item.parent(), QTreeWidgetItem):\r\n            root_item = root_item.parent()\r\n        return root_item", "output": "Return the root item of the specified item.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def receive_trial_result(self, parameter_id, parameters, value):\n        '''\n        \n        '''\n        logger.debug('acquiring lock for param {}'.format(parameter_id))\n        self.thread_lock.acquire()\n        logger.debug('lock for current acquired')\n        reward = extract_scalar_reward(value)\n        if self.optimize_mode is OptimizeMode.Minimize:\n            reward = -reward\n\n        logger.debug('receive trial result is:\\n')\n        logger.debug(str(parameters))\n        logger.debug(str(reward))\n\n        indiv = Individual(indiv_id=int(os.path.split(parameters['save_dir'])[1]),\n                           graph_cfg=graph_loads(parameters['graph']), result=reward)\n        self.population.append(indiv)\n        logger.debug('releasing lock')\n        self.thread_lock.release()\n        self.events[indiv.indiv_id].set()", "output": "Record an observation of the objective function\n        parameter_id : int\n        parameters : dict of parameters\n        value: final metrics of the trial, including reward", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_comparison_methods(cls):\n        \"\"\"\n        \n        \"\"\"\n        cls.__eq__ = _make_comparison_op(operator.eq, cls)\n        cls.__ne__ = _make_comparison_op(operator.ne, cls)\n        cls.__lt__ = _make_comparison_op(operator.lt, cls)\n        cls.__gt__ = _make_comparison_op(operator.gt, cls)\n        cls.__le__ = _make_comparison_op(operator.le, cls)\n        cls.__ge__ = _make_comparison_op(operator.ge, cls)", "output": "Add in comparison methods.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parseEntity(filename):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlParseEntity(filename)\n    if ret is None:raise parserError('xmlParseEntity() failed')\n    return xmlDoc(_obj=ret)", "output": "parse an XML external entity out of context and build a\n      tree.  [78] extParsedEnt ::= TextDecl? content  This\n       correspond to a \"Well Balanced\" chunk", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_arglist(self, objtxt):\r\n        \"\"\"\"\"\"\r\n        obj, valid = self._eval(objtxt)\r\n        if valid:\r\n            return getargtxt(obj)", "output": "Get func/method argument list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dimensionize(maybe_a_list, nd=2):\n    \"\"\"\n    \"\"\"\n    if not hasattr(maybe_a_list, '__iter__'):\n        # Argument is probably an integer so we map it to a list of size `nd`.\n        now_a_list = [maybe_a_list] * nd\n        return now_a_list\n    else:\n        # Argument is probably an `nd`-sized list.\n        return maybe_a_list", "output": "Convert integers to a list of integers to fit the number of dimensions if\n    the argument is not already a list.\n\n    For example:\n    `dimensionize(3, nd=2)`\n        will produce the following result:\n        `(3, 3)`.\n    `dimensionize([3, 1], nd=2)`\n        will produce the following result:\n        `[3, 1]`.\n\n    For more information, refer to:\n    - https://github.com/guillaume-chevalier/python-conv-lib/blob/master/conv/conv.py\n    - https://github.com/guillaume-chevalier/python-conv-lib\n    - MIT License, Copyright (c) 2018 Guillaume Chevalier", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_activation(self, inputs, activation, **kwargs):\n        \"\"\"\"\"\"\n        if isinstance(activation, string_types):\n            return symbol.Activation(inputs, act_type=activation, **kwargs)\n        else:\n            return activation(inputs, **kwargs)", "output": "Get activation function. Convert if is string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kill_node(config_file, yes, override_cluster_name):\n    \"\"\"\"\"\"\n\n    config = yaml.load(open(config_file).read())\n    if override_cluster_name is not None:\n        config[\"cluster_name\"] = override_cluster_name\n    config = _bootstrap_config(config)\n\n    confirm(\"This will kill a node in your cluster\", yes)\n\n    provider = get_node_provider(config[\"provider\"], config[\"cluster_name\"])\n    try:\n        nodes = provider.non_terminated_nodes({TAG_RAY_NODE_TYPE: \"worker\"})\n        node = random.choice(nodes)\n        logger.info(\"kill_node: Terminating worker {}\".format(node))\n\n        updater = NodeUpdaterThread(\n            node_id=node,\n            provider_config=config[\"provider\"],\n            provider=provider,\n            auth_config=config[\"auth\"],\n            cluster_name=config[\"cluster_name\"],\n            file_mounts=config[\"file_mounts\"],\n            initialization_commands=[],\n            setup_commands=[],\n            runtime_hash=\"\")\n\n        _exec(updater, \"ray stop\", False, False)\n\n        time.sleep(5)\n\n        if config.get(\"provider\", {}).get(\"use_internal_ips\", False) is True:\n            node_ip = provider.internal_ip(node)\n        else:\n            node_ip = provider.external_ip(node)\n    finally:\n        provider.cleanup()\n\n    return node_ip", "output": "Kills a random Raylet worker.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, resource, client):\n        \"\"\"\n        \"\"\"\n        project = cls(project_id=resource[\"projectId\"], client=client)\n        project.set_properties_from_api_repr(resource)\n        return project", "output": "Factory:  construct a project given its API representation.\n\n        :type resource: dict\n        :param resource: project resource representation returned from the API\n\n        :type client: :class:`google.cloud.resource_manager.client.Client`\n        :param client: The Client used with this project.\n\n        :rtype: :class:`google.cloud.resource_manager.project.Project`\n        :returns: The project created.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attn(image_feat, query, hparams, name=\"attn\"):\n  \"\"\"\"\"\"\n  with tf.variable_scope(name, \"attn\", values=[image_feat, query]):\n    attn_dim = hparams.attn_dim\n    num_glimps = hparams.num_glimps\n    num_channels = common_layers.shape_list(image_feat)[-1]\n    if len(common_layers.shape_list(image_feat)) == 4:\n      image_feat = common_layers.flatten4d3d(image_feat)\n    query = tf.expand_dims(query, 1)\n    image_proj = common_attention.compute_attention_component(\n        image_feat, attn_dim, name=\"image_proj\")\n    query_proj = common_attention.compute_attention_component(\n        query, attn_dim, name=\"query_proj\")\n    h = tf.nn.relu(image_proj + query_proj)\n    h_proj = common_attention.compute_attention_component(\n        h, num_glimps, name=\"h_proj\")\n    p = tf.nn.softmax(h_proj, axis=1)\n    image_ave = tf.matmul(image_feat, p, transpose_a=True)\n    image_ave = tf.reshape(image_ave, [-1, num_channels*num_glimps])\n\n    return image_ave", "output": "Attention on image feature with question as query.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _format_comments(ret, comments):\n    '''\n    \n    '''\n    if isinstance(comments, six.string_types):\n        ret['comment'] = comments\n    else:\n        ret['comment'] = '. '.join(comments)\n        if len(comments) > 1:\n            ret['comment'] += '.'\n    return ret", "output": "DRY code for joining comments together and conditionally adding a period at\n    the end, and adding this comment string to the state return dict.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def connect_patch_node_proxy(self, name, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_patch_node_proxy_with_http_info(name, **kwargs)\n        else:\n            (data) = self.connect_patch_node_proxy_with_http_info(name, **kwargs)\n            return data", "output": "connect PATCH requests to proxy of Node\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_patch_node_proxy(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the NodeProxyOptions (required)\n        :param str path: Path is the URL path to use for the current proxy request to node.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_key(key, value, host=None, port=None, db=None, password=None):\n    '''\n    \n    '''\n    server = _connect(host, port, db, password)\n    return server.set(key, value)", "output": "Set redis key value\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.set_key foo bar", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def map_predicate(self, f):\n        \"\"\"\n        \n        \"\"\"\n        # Functions passed to this are of type str -> bool.  Don't ever call\n        # them on None, which is the only non-str value we ever store in\n        # categories.\n        if self.missing_value is None:\n            def f_to_use(x):\n                return False if x is None else f(x)\n        else:\n            f_to_use = f\n\n        # Call f on each unique value in our categories.\n        results = np.vectorize(f_to_use, otypes=[bool_dtype])(self.categories)\n\n        # missing_value should produce False no matter what\n        results[self.reverse_categories[self.missing_value]] = False\n\n        # unpack the results form each unique value into their corresponding\n        # locations in our indices.\n        return results[self.as_int_array()]", "output": "Map a function from str -> bool element-wise over ``self``.\n\n        ``f`` will be applied exactly once to each non-missing unique value in\n        ``self``. Missing values will always return False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def logNormalRDD(sc, mean, std, size, numPartitions=None, seed=None):\n        \"\"\"\n        \n        \"\"\"\n        return callMLlibFunc(\"logNormalRDD\", sc._jsc, float(mean), float(std),\n                             size, numPartitions, seed)", "output": "Generates an RDD comprised of i.i.d. samples from the log normal\n        distribution with the input mean and standard distribution.\n\n        :param sc: SparkContext used to create the RDD.\n        :param mean: mean for the log Normal distribution\n        :param std: std for the log Normal distribution\n        :param size: Size of the RDD.\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`).\n        :param seed: Random seed (default: a random long integer).\n        :return: RDD of float comprised of i.i.d. samples ~ log N(mean, std).\n\n        >>> from math import sqrt, exp\n        >>> mean = 0.0\n        >>> std = 1.0\n        >>> expMean = exp(mean + 0.5 * std * std)\n        >>> expStd = sqrt((exp(std * std) - 1.0) * exp(2.0 * mean + std * std))\n        >>> x = RandomRDDs.logNormalRDD(sc, mean, std, 1000, seed=2)\n        >>> stats = x.stats()\n        >>> stats.count()\n        1000\n        >>> abs(stats.mean() - expMean) < 0.5\n        True\n        >>> from math import sqrt\n        >>> abs(stats.stdev() - expStd) < 0.5\n        True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def assert_shape_match(shape1, shape2):\n  \"\"\"\n  \"\"\"\n  shape1 = tf.TensorShape(shape1)\n  shape2 = tf.TensorShape(shape2)\n  if shape1.ndims is None or shape2.ndims is None:\n    raise ValueError('Shapes must have known rank. Got %s and %s.' %\n                     (shape1.ndims, shape2.ndims))\n  shape1.assert_same_rank(shape2)\n  shape1.assert_is_compatible_with(shape2)", "output": "Ensure the shape1 match the pattern given by shape2.\n\n  Ex:\n    assert_shape_match((64, 64, 3), (None, None, 3))\n\n  Args:\n    shape1 (tuple): Static shape\n    shape2 (tuple): Dynamic shape (can contain None)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def jitter(field_name, width, mean=0, distribution=\"uniform\", range=None):\n    ''' \n\n    '''\n    return field(field_name, Jitter(mean=mean,\n                                    width=width,\n                                    distribution=distribution,\n                                    range=range))", "output": "Create a ``DataSpec`` dict that applies a client-side ``Jitter``\n    transformation to a ``ColumnDataSource`` column.\n\n    Args:\n        field_name (str) : a field name to configure ``DataSpec`` with\n\n        width (float) : the width of the random distribution to apply\n\n        mean (float, optional) : an offset to apply (default: 0)\n\n        distribution (str, optional) : ``\"uniform\"`` or ``\"normal\"``\n            (default: ``\"uniform\"``)\n\n        range (Range, optional) : a range to use for computing synthetic\n            coordinates when necessary, e.g. a ``FactorRange`` when the\n            column data is categorical (default: None)\n\n    Returns:\n        dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expand_path(experiment_config, key):\n    ''''''\n    if experiment_config.get(key):\n        experiment_config[key] = os.path.expanduser(experiment_config[key])", "output": "Change '~' to user home directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flush_synced(self, using=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        return self._get_connection(using).indices.flush_synced(index=self._name, **kwargs)", "output": "Perform a normal flush, then add a generated unique marker (sync_id) to\n        all shards.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.flush_synced`` unchanged.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_remote_map(self):\n        '''\n        \n        '''\n        remote_map = salt.utils.path.join(self.cache_root, 'remote_map.txt')\n        try:\n            with salt.utils.files.fopen(remote_map, 'w+') as fp_:\n                timestamp = \\\n                    datetime.now().strftime('%d %b %Y %H:%M:%S.%f')\n                fp_.write(\n                    '# {0}_remote map as of {1}\\n'.format(\n                        self.role,\n                        timestamp\n                    )\n                )\n                for repo in self.remotes:\n                    fp_.write(\n                        salt.utils.stringutils.to_str(\n                            '{0} = {1}\\n'.format(\n                                repo.cachedir_basename,\n                                repo.id\n                            )\n                        )\n                    )\n        except OSError:\n            pass\n        else:\n            log.info('Wrote new %s remote map to %s', self.role, remote_map)", "output": "Write the remote_map.txt", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_value_estimate(self, brain_info, idx):\n        \"\"\"\n        \n        \"\"\"\n        feed_dict = {self.model.batch_size: 1, self.model.sequence_length: 1}\n        for i in range(len(brain_info.visual_observations)):\n            feed_dict[self.model.visual_in[i]] = [brain_info.visual_observations[i][idx]]\n        if self.use_vec_obs:\n            feed_dict[self.model.vector_in] = [brain_info.vector_observations[idx]]\n        if self.use_recurrent:\n            if brain_info.memories.shape[1] == 0:\n                brain_info.memories = self.make_empty_memory(len(brain_info.agents))\n            feed_dict[self.model.memory_in] = [brain_info.memories[idx]]\n        if not self.use_continuous_act and self.use_recurrent:\n            feed_dict[self.model.prev_action] = brain_info.previous_vector_actions[idx].reshape(\n                [-1, len(self.model.act_size)])\n        value_estimate = self.sess.run(self.model.value, feed_dict)\n        return value_estimate", "output": "Generates value estimates for bootstrapping.\n        :param brain_info: BrainInfo to be used for bootstrapping.\n        :param idx: Index in BrainInfo of agent.\n        :return: Value estimate.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_entry(user, identifier=None, cmd=None):\n    '''\n    \n    '''\n    cron_entries = list_tab(user).get('crons', False)\n    for cron_entry in cron_entries:\n        if identifier and cron_entry.get('identifier') == identifier:\n            return cron_entry\n        elif cmd and cron_entry.get('cmd') == cmd:\n            return cron_entry\n    return False", "output": "Return the specified entry from user's crontab.\n    identifier will be used if specified, otherwise will lookup cmd\n    Either identifier or cmd should be specified.\n\n    user:\n        User's crontab to query\n\n    identifier:\n        Search for line with identifier\n\n    cmd:\n        Search for cron line with cmd\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cron.identifier_exists root identifier=task1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def invalid_comparison(left, right, op):\n    \"\"\"\n    \n    \"\"\"\n    if op is operator.eq:\n        res_values = np.zeros(left.shape, dtype=bool)\n    elif op is operator.ne:\n        res_values = np.ones(left.shape, dtype=bool)\n    else:\n        raise TypeError(\"Invalid comparison between dtype={dtype} and {typ}\"\n                        .format(dtype=left.dtype, typ=type(right).__name__))\n    return res_values", "output": "If a comparison has mismatched types and is not necessarily meaningful,\n    follow python3 conventions by:\n\n        - returning all-False for equality\n        - returning all-True for inequality\n        - raising TypeError otherwise\n\n    Parameters\n    ----------\n    left : array-like\n    right : scalar, array-like\n    op : operator.{eq, ne, lt, le, gt}\n\n    Raises\n    ------\n    TypeError : on inequality comparisons", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def freqItems(self, cols, support=None):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(cols, tuple):\n            cols = list(cols)\n        if not isinstance(cols, list):\n            raise ValueError(\"cols must be a list or tuple of column names as strings.\")\n        if not support:\n            support = 0.01\n        return DataFrame(self._jdf.stat().freqItems(_to_seq(self._sc, cols), support), self.sql_ctx)", "output": "Finding frequent items for columns, possibly with false positives. Using the\n        frequent element count algorithm described in\n        \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n        :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n\n        .. note:: This function is meant for exploratory data analysis, as we make no\n            guarantee about the backward compatibility of the schema of the resulting DataFrame.\n\n        :param cols: Names of the columns to calculate frequent items for as a list or tuple of\n            strings.\n        :param support: The frequency with which to consider an item 'frequent'. Default is 1%.\n            The support must be greater than 1e-4.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def text_to_char_array(original, alphabet):\n    \n    \"\"\"\n    return np.asarray([alphabet.label_from_string(c) for c in original])", "output": "r\"\"\"\n    Given a Python string ``original``, remove unsupported characters, map characters\n    to integers and return a numpy array representing the processed string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _activate_outbound(self):\n        \"\"\"\"\"\"\n        m = Message()\n        m.add_byte(cMSG_NEWKEYS)\n        self._send_message(m)\n        block_size = self._cipher_info[self.local_cipher][\"block-size\"]\n        if self.server_mode:\n            IV_out = self._compute_key(\"B\", block_size)\n            key_out = self._compute_key(\n                \"D\", self._cipher_info[self.local_cipher][\"key-size\"]\n            )\n        else:\n            IV_out = self._compute_key(\"A\", block_size)\n            key_out = self._compute_key(\n                \"C\", self._cipher_info[self.local_cipher][\"key-size\"]\n            )\n        engine = self._get_cipher(\n            self.local_cipher, key_out, IV_out, self._ENCRYPT\n        )\n        mac_size = self._mac_info[self.local_mac][\"size\"]\n        mac_engine = self._mac_info[self.local_mac][\"class\"]\n        # initial mac keys are done in the hash's natural size (not the\n        # potentially truncated transmission size)\n        if self.server_mode:\n            mac_key = self._compute_key(\"F\", mac_engine().digest_size)\n        else:\n            mac_key = self._compute_key(\"E\", mac_engine().digest_size)\n        sdctr = self.local_cipher.endswith(\"-ctr\")\n        self.packetizer.set_outbound_cipher(\n            engine, block_size, mac_engine, mac_size, mac_key, sdctr\n        )\n        compress_out = self._compression_info[self.local_compression][0]\n        if compress_out is not None and (\n            self.local_compression != \"zlib@openssh.com\" or self.authenticated\n        ):\n            self._log(DEBUG, \"Switching on outbound compression ...\")\n            self.packetizer.set_outbound_compressor(compress_out())\n        if not self.packetizer.need_rekey():\n            self.in_kex = False\n        # we always expect to receive NEWKEYS now\n        self._expect_packet(MSG_NEWKEYS)", "output": "switch on newly negotiated encryption parameters for\n         outbound traffic", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sample(self, batch_size, batch_idxs=None):\n        \"\"\"\n        \"\"\"\n        if batch_idxs is None:\n            batch_idxs = sample_batch_indexes(0, self.nb_entries, size=batch_size)\n        assert len(batch_idxs) == batch_size\n\n        batch_params = []\n        batch_total_rewards = []\n        for idx in batch_idxs:\n            batch_params.append(self.params[idx])\n            batch_total_rewards.append(self.total_rewards[idx])\n        return batch_params, batch_total_rewards", "output": "Return a randomized batch of params and rewards\n\n        # Argument\n            batch_size (int): Size of the all batch\n            batch_idxs (int): Indexes to extract\n        # Returns\n            A list of params randomly selected and a list of associated rewards", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def processor(ctx, processor_cls, process_time_limit, enable_stdout_capture=True, get_object=False):\n    \"\"\"\n    \n    \"\"\"\n    g = ctx.obj\n    Processor = load_cls(None, None, processor_cls)\n\n    processor = Processor(projectdb=g.projectdb,\n                          inqueue=g.fetcher2processor, status_queue=g.status_queue,\n                          newtask_queue=g.newtask_queue, result_queue=g.processor2result,\n                          enable_stdout_capture=enable_stdout_capture,\n                          process_time_limit=process_time_limit)\n\n    g.instances.append(processor)\n    if g.get('testing_mode') or get_object:\n        return processor\n\n    processor.run()", "output": "Run Processor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def policy_definition_absent(name, connection_auth=None):\n    '''\n    \n    '''\n    ret = {\n        'name': name,\n        'result': False,\n        'comment': '',\n        'changes': {}\n    }\n\n    if not isinstance(connection_auth, dict):\n        ret['comment'] = 'Connection information must be specified via connection_auth dictionary!'\n        return ret\n\n    policy = __salt__['azurearm_resource.policy_definition_get'](name, azurearm_log_level='info', **connection_auth)\n\n    if 'error' in policy:\n        ret['result'] = True\n        ret['comment'] = 'Policy definition {0} is already absent.'.format(name)\n        return ret\n\n    elif __opts__['test']:\n        ret['comment'] = 'Policy definition {0} would be deleted.'.format(name)\n        ret['result'] = None\n        ret['changes'] = {\n            'old': policy,\n            'new': {},\n        }\n        return ret\n\n    deleted = __salt__['azurearm_resource.policy_definition_delete'](name, **connection_auth)\n\n    if deleted:\n        ret['result'] = True\n        ret['comment'] = 'Policy definition {0} has been deleted.'.format(name)\n        ret['changes'] = {\n            'old': policy,\n            'new': {}\n        }\n        return ret\n\n    ret['comment'] = 'Failed to delete policy definition {0}!'.format(name)\n    return ret", "output": ".. versionadded:: 2019.2.0\n\n    Ensure a policy definition does not exist in the current subscription.\n\n    :param name:\n        Name of the policy definition.\n\n    :param connection_auth:\n        A dict with subscription and authentication parameters to be used in connecting to the\n        Azure Resource Manager API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_replication_group(name, primary_cluster_id, replication_group_description,\n                             wait=None, region=None, key=None,\n                             keyid=None, profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    if not conn:\n        return None\n    try:\n        cc = conn.create_replication_group(name, primary_cluster_id,\n                                           replication_group_description)\n        if not wait:\n            log.info('Created cache cluster %s.', name)\n            return True\n        while True:\n            time.sleep(3)\n            config = describe_replication_group(name, region, key, keyid, profile)\n            if not config:\n                return True\n            if config['status'] == 'available':\n                return True\n    except boto.exception.BotoServerError as e:\n        msg = 'Failed to create replication group {0}.'.format(name)\n        log.error(msg)\n        log.debug(e)\n        return {}", "output": "Create replication group.\n\n    CLI example::\n\n        salt myminion boto_elasticache.create_replication_group myelasticache myprimarycluster description", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pay_dividends(self, next_trading_day):\n        \"\"\"\n        \n        \"\"\"\n        net_cash_payment = 0.0\n\n        try:\n            payments = self._unpaid_dividends[next_trading_day]\n            # Mark these dividends as paid by dropping them from our unpaid\n            del self._unpaid_dividends[next_trading_day]\n        except KeyError:\n            payments = []\n\n        # representing the fact that we're required to reimburse the owner of\n        # the stock for any dividends paid while borrowing.\n        for payment in payments:\n            net_cash_payment += payment['amount']\n\n        # Add stock for any stock dividends paid.  Again, the values here may\n        # be negative in the case of short positions.\n        try:\n            stock_payments = self._unpaid_stock_dividends[next_trading_day]\n        except KeyError:\n            stock_payments = []\n\n        for stock_payment in stock_payments:\n            payment_asset = stock_payment['payment_asset']\n            share_count = stock_payment['share_count']\n            # note we create a Position for stock dividend if we don't\n            # already own the asset\n            if payment_asset in self.positions:\n                position = self.positions[payment_asset]\n            else:\n                position = self.positions[payment_asset] = Position(\n                    payment_asset,\n                )\n\n            position.amount += share_count\n\n        return net_cash_payment", "output": "Returns a cash payment based on the dividends that should be paid out\n        according to the accumulated bookkeeping of earned, unpaid, and stock\n        dividends.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_char_in_pairs(pos_char, pairs):\r\n        \"\"\"\"\"\"\r\n        for pos_left, pos_right in pairs.items():\r\n            if pos_left < pos_char < pos_right:\r\n                return True\r\n\r\n        return False", "output": "Return True if the charactor is in pairs of brackets or quotes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_base_anchors(base_size, scales, ratios):\n        \"\"\"\n        \n        \"\"\"\n        base_anchor = np.array([1, 1, base_size, base_size]) - 1\n        ratio_anchors = AnchorGenerator._ratio_enum(base_anchor, ratios)\n        anchors = np.vstack([AnchorGenerator._scale_enum(ratio_anchors[i, :], scales)\n                             for i in range(ratio_anchors.shape[0])])\n        return anchors", "output": "Generate anchor (reference) windows by enumerating aspect ratios X\n        scales wrt a reference (0, 0, 15, 15) window.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stepping(start, stop, steps):\n    \"\"\"\n    \"\"\"\n\n    def clip(value):\n        return max(value, stop) if (start > stop) else min(value, stop)\n\n    curr = float(start)\n    while True:\n        yield clip(curr)\n        curr += (stop - start) / steps", "output": "Yield an infinite series of values that step from a start value to a\n    final value over some number of steps. Each step is (stop-start)/steps.\n\n    After the final value is reached, the generator continues yielding that\n    value.\n\n    EXAMPLE:\n      >>> sizes = stepping(1., 200., 100)\n      >>> assert next(sizes) == 1.\n      >>> assert next(sizes) == 1 * (200.-1.) / 100\n      >>> assert next(sizes) == 1 + (200.-1.) / 100 + (200.-1.) / 100", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _decode_pax_field(self, value, encoding, fallback_encoding, fallback_errors):\n        \"\"\"\n        \"\"\"\n        try:\n            return value.decode(encoding, \"strict\")\n        except UnicodeDecodeError:\n            return value.decode(fallback_encoding, fallback_errors)", "output": "Decode a single field from a pax record.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_movielens_iter(filename, batch_size):\n    \"\"\"\n    \"\"\"\n    logging.info(\"Preparing data iterators for \" + filename + \" ... \")\n    user = []\n    item = []\n    score = []\n    with open(filename, 'r') as f:\n        num_samples = 0\n        for line in f:\n            tks = line.strip().split('::')\n            if len(tks) != 4:\n                continue\n            num_samples += 1\n            user.append((tks[0]))\n            item.append((tks[1]))\n            score.append((tks[2]))\n    # convert to ndarrays\n    user = mx.nd.array(user, dtype='int32')\n    item = mx.nd.array(item)\n    score = mx.nd.array(score)\n    # prepare data iters\n    data_train = {'user': user, 'item': item}\n    label_train = {'score': score}\n    iter_train = mx.io.NDArrayIter(data=data_train,label=label_train,\n                                   batch_size=batch_size, shuffle=True)\n    return mx.io.PrefetchingIter(iter_train)", "output": "Not particularly fast code to parse the text file and load into NDArrays.\n    return two data iters, one for train, the other for validation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def service_update(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.update_service(**kwargs)", "output": "Update a service\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystoneng.service_update name=cinder type=volumev2\n        salt '*' keystoneng.service_update name=cinder description='new description'\n        salt '*' keystoneng.service_update name=ab4d35e269f147b3ae2d849f77f5c88f enabled=False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe_topic(name, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    topics = list_topics(region=region, key=key, keyid=keyid, profile=profile)\n    ret = {}\n    for topic, arn in topics.items():\n        if name in (topic, arn):\n            ret = {'TopicArn': arn}\n            ret['Attributes'] = get_topic_attributes(arn, region=region, key=key, keyid=keyid,\n                    profile=profile)\n            ret['Subscriptions'] = list_subscriptions_by_topic(arn, region=region, key=key,\n                    keyid=keyid, profile=profile)\n            # Grab extended attributes for the above subscriptions\n            for sub in range(len(ret['Subscriptions'])):\n                sub_arn = ret['Subscriptions'][sub]['SubscriptionArn']\n                if not sub_arn.startswith('arn:aws:sns:'):\n                    # Sometimes a sub is in e.g. PendingAccept or other\n                    # wierd states and doesn't have an ARN yet\n                    log.debug('Subscription with invalid ARN %s skipped...', sub_arn)\n                    continue\n                deets = get_subscription_attributes(SubscriptionArn=sub_arn, region=region,\n                        key=key, keyid=keyid, profile=profile)\n                ret['Subscriptions'][sub].update(deets)\n    return ret", "output": "Returns details about a specific SNS topic, specified by name or ARN.\n\n    CLI example::\n\n        salt my_favorite_client boto3_sns.describe_topic a_sns_topic_of_my_choice", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(self, watch_key, tensor_value):\n    \"\"\"\n    \"\"\"\n    if watch_key not in self._tensor_data:\n      self._tensor_data[watch_key] = _WatchStore(\n          watch_key,\n          mem_bytes_limit=self._watch_mem_bytes_limit)\n    self._tensor_data[watch_key].add(tensor_value)", "output": "Add a tensor value.\n\n    Args:\n      watch_key: A string representing the debugger tensor watch, e.g.,\n        'Dense_1/BiasAdd:0:DebugIdentity'.\n      tensor_value: The value of the tensor as a numpy.ndarray.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def status_webapp(app, url='http://localhost:8080/manager', timeout=180):\n    '''\n    \n    '''\n\n    webapps = ls(url, timeout=timeout)\n    for i in webapps:\n        if i == app:\n            return webapps[i]['mode']\n\n    return 'missing'", "output": "return the status of the webapp (stopped | running | missing)\n\n    app\n        the webapp context path\n    url : http://localhost:8080/manager\n        the URL of the server manager webapp\n    timeout : 180\n        timeout for HTTP request\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' tomcat.status_webapp /jenkins\n        salt '*' tomcat.status_webapp /jenkins http://localhost:8080/manager", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xincludeProcessFlags(self, flags):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlXIncludeProcessFlags(self._o, flags)\n        return ret", "output": "Implement the XInclude substitution on the XML document @doc", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def textMerge(self, second):\n        \"\"\" \"\"\"\n        if second is None: second__o = None\n        else: second__o = second._o\n        ret = libxml2mod.xmlTextMerge(self._o, second__o)\n        if ret is None:raise treeError('xmlTextMerge() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Merge two text nodes into one", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self, batch_size, ignore_stale_grad=False):\n        \"\"\"\n        \"\"\"\n        if not self._kv_initialized:\n            self._init_kvstore()\n        if self._params_to_init:\n            self._init_params()\n        assert not (self._kvstore and self._update_on_kvstore), \\\n                'update() when parameters are updated on kvstore ' \\\n                'is not supported. Try setting `update_on_kvstore` ' \\\n                'to False when creating trainer.'\n\n        self._check_and_rescale_grad(self._scale / batch_size)\n        self._update(ignore_stale_grad)", "output": "Makes one step of parameter update.\n\n        Should be called after `autograd.backward()` and outside of `record()` scope,\n        and after `trainer.update()`.\n\n\n        For normal parameter updates, `step()` should be used, which internally calls\n        `allreduce_grads()` and then `update()`. However, if you need to get the reduced\n        gradients to perform certain transformation, such as in gradient clipping, then\n        you may want to manually call `allreduce_grads()` and `update()` separately.\n\n        Parameters\n        ----------\n        batch_size : int\n            Batch size of data processed. Gradient will be normalized by `1/batch_size`.\n            Set this to 1 if you normalized loss manually with `loss = mean(loss)`.\n        ignore_stale_grad : bool, optional, default=False\n            If true, ignores Parameters with stale gradient (gradient that has not\n            been updated by `backward` after last step) and skip update.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def maybe_convert_platform(values):\n    \"\"\"  \"\"\"\n\n    if isinstance(values, (list, tuple)):\n        values = construct_1d_object_array_from_listlike(list(values))\n    if getattr(values, 'dtype', None) == np.object_:\n        if hasattr(values, '_values'):\n            values = values._values\n        values = lib.maybe_convert_objects(values)\n\n    return values", "output": "try to do platform conversion, allow ndarray or list here", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_bday(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.n >= 0:\n            nb_offset = 1\n        else:\n            nb_offset = -1\n        if self._prefix.startswith('C'):\n            # CustomBusinessHour\n            return CustomBusinessDay(n=nb_offset,\n                                     weekmask=self.weekmask,\n                                     holidays=self.holidays,\n                                     calendar=self.calendar)\n        else:\n            return BusinessDay(n=nb_offset)", "output": "Used for moving to next business day.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wheelEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        # This feature is disabled on MacOS, see Issue 1510\r\n        if sys.platform != 'darwin':\r\n            if event.modifiers() & Qt.ControlModifier:\r\n                if hasattr(event, 'angleDelta'):\r\n                    if event.angleDelta().y() < 0:\r\n                        self.zoom_out.emit()\r\n                    elif event.angleDelta().y() > 0:\r\n                        self.zoom_in.emit()\r\n                elif hasattr(event, 'delta'):\r\n                    if event.delta() < 0:\r\n                        self.zoom_out.emit()\r\n                    elif event.delta() > 0:\r\n                        self.zoom_in.emit()\r\n                return\r\n        QPlainTextEdit.wheelEvent(self, event)\r\n        self.highlight_current_cell()", "output": "Reimplemented to emit zoom in/out signals when Ctrl is pressed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def peek(self, size: int) -> memoryview:\n        \"\"\"\n        \n        \"\"\"\n        assert size > 0\n        try:\n            is_memview, b = self._buffers[0]\n        except IndexError:\n            return memoryview(b\"\")\n\n        pos = self._first_pos\n        if is_memview:\n            return typing.cast(memoryview, b[pos : pos + size])\n        else:\n            return memoryview(b)[pos : pos + size]", "output": "Get a view over at most ``size`` bytes (possibly fewer) at the\n        current buffer position.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lock_retention_policy(self, client=None):\n        \"\"\"\n        \"\"\"\n        if \"metageneration\" not in self._properties:\n            raise ValueError(\"Bucket has no retention policy assigned: try 'reload'?\")\n\n        policy = self._properties.get(\"retentionPolicy\")\n\n        if policy is None:\n            raise ValueError(\"Bucket has no retention policy assigned: try 'reload'?\")\n\n        if policy.get(\"isLocked\"):\n            raise ValueError(\"Bucket's retention policy is already locked.\")\n\n        client = self._require_client(client)\n\n        query_params = {\"ifMetagenerationMatch\": self.metageneration}\n\n        if self.user_project is not None:\n            query_params[\"userProject\"] = self.user_project\n\n        path = \"/b/{}/lockRetentionPolicy\".format(self.name)\n        api_response = client._connection.api_request(\n            method=\"POST\", path=path, query_params=query_params, _target_object=self\n        )\n        self._set_properties(api_response)", "output": "Lock the bucket's retention policy.\n\n        :raises ValueError:\n            if the bucket has no metageneration (i.e., new or never reloaded);\n            if the bucket has no retention policy assigned;\n            if the bucket's retention policy is already locked.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chgroups(name, groups, append=False, root=None):\n    '''\n    \n    '''\n    if isinstance(groups, six.string_types):\n        groups = groups.split(',')\n    ugrps = set(list_groups(name))\n    if ugrps == set(groups):\n        return True\n    cmd = ['usermod']\n\n    if __grains__['kernel'] != 'OpenBSD':\n        if append and __grains__['kernel'] != 'AIX':\n            cmd.append('-a')\n        cmd.append('-G')\n    else:\n        if append:\n            cmd.append('-G')\n        else:\n            cmd.append('-S')\n\n    if append and __grains__['kernel'] == 'AIX':\n        cmd.extend([','.join(ugrps) + ',' + ','.join(groups), name])\n    else:\n        cmd.extend([','.join(groups), name])\n\n    if root is not None and __grains__['kernel'] != 'AIX':\n        cmd.extend(('-R', root))\n\n    result = __salt__['cmd.run_all'](cmd, python_shell=False)\n    # try to fallback on gpasswd to add user to localgroups\n    # for old lib-pamldap support\n    if __grains__['kernel'] != 'OpenBSD' and __grains__['kernel'] != 'AIX':\n        if result['retcode'] != 0 and 'not found in' in result['stderr']:\n            ret = True\n            for group in groups:\n                cmd = ['gpasswd', '-a', name, group]\n                if __salt__['cmd.retcode'](cmd, python_shell=False) != 0:\n                    ret = False\n            return ret\n    return result['retcode'] == 0", "output": "Change the groups to which this user belongs\n\n    name\n        User to modify\n\n    groups\n        Groups to set for the user\n\n    append : False\n        If ``True``, append the specified group(s). Otherwise, this function\n        will replace the user's groups with the specified group(s).\n\n    root\n        Directory to chroot into\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' user.chgroups foo wheel,root\n        salt '*' user.chgroups foo wheel,root append=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_experiment(self, id):\n        ''''''\n        if id in self.experiments:\n            self.experiments.pop(id)\n        self.write_file()", "output": "remove an experiment by id", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_children(self):\n        \"\"\"\n        \"\"\"\n        handle = SymbolHandle()\n        check_call(_LIB.MXSymbolGetChildren(\n            self.handle, ctypes.byref(handle)))\n        ret = Symbol(handle=handle)\n        if len(ret.list_outputs()) == 0:\n            return None\n        return ret", "output": "Gets a new grouped symbol whose output contains\n        inputs to output nodes of the original symbol.\n\n        Example\n        -------\n        >>> x = mx.sym.Variable('x')\n        >>> y = mx.sym.Variable('y')\n        >>> z = mx.sym.Variable('z')\n        >>> a = y+z\n        >>> b = x+a\n        >>> b.get_children()\n        <Symbol Grouped>\n        >>> b.get_children().list_outputs()\n        ['x', '_plus10_output']\n        >>> b.get_children().get_children().list_outputs()\n        ['y', 'z']\n\n        Returns\n        -------\n        sgroup : Symbol or None\n            The children of the head node. If the symbol has no\n            inputs then ``None`` will be returned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def adafactor_decay_rate_adam(beta2):\n  \"\"\"\n  \"\"\"\n  t = tf.to_float(tf.train.get_or_create_global_step()) + 1.0\n  decay = beta2 * (1.0 - tf.pow(beta2, t - 1.0)) / (1.0 - tf.pow(beta2, t))\n  # decay = tf.cond(tf.equal(t, 1.0), lambda: beta2, lambda: decay)\n  return decay", "output": "Second-moment decay rate like Adam, subsuming the correction factor.\n\n  Args:\n    beta2: a float between 0 and 1\n  Returns:\n    a scalar", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def boston(display=False):\n    \"\"\"  \"\"\"\n\n    d = sklearn.datasets.load_boston()\n    df = pd.DataFrame(data=d.data, columns=d.feature_names) # pylint: disable=E1101\n    return df, d.target", "output": "Return the boston housing data in a nice package.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_assets(self, assets):\n        \"\"\"\n        \"\"\"\n        missing_sids = np.setdiff1d(assets, self.sids)\n\n        if len(missing_sids):\n            raise NoDataForSid(\n                'Assets not contained in daily pricing file: {}'.format(\n                    missing_sids\n                )\n            )", "output": "Validate that asset identifiers are contained in the daily bars.\n\n        Parameters\n        ----------\n        assets : array-like[int]\n           The asset identifiers to validate.\n\n        Raises\n        ------\n        NoDataForSid\n            If one or more of the provided asset identifiers are not\n            contained in the daily bars.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_tf_weights_in_gpt2(model, gpt2_checkpoint_path):\n    \"\"\" \n    \"\"\"\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        print(\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"\n            \"https://www.tensorflow.org/install/ for installation instructions.\")\n        raise\n    tf_path = os.path.abspath(gpt2_checkpoint_path)\n    print(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        print(\"Loading TF weight {} with shape {}\".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array.squeeze())\n\n    for name, array in zip(names, arrays):\n        name = name[6:]  # skip \"model/\"\n        name = name.split('/')\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r'[A-Za-z]+\\d+', m_name):\n                l = re.split(r'(\\d+)', m_name)\n            else:\n                l = [m_name]\n            if l[0] == 'w' or l[0] == 'g':\n                pointer = getattr(pointer, 'weight')\n            elif l[0] == 'b':\n                pointer = getattr(pointer, 'bias')\n            elif l[0] == 'wpe' or l[0] == 'wte':\n                pointer = getattr(pointer, l[0])\n                pointer = getattr(pointer, 'weight')\n            else:\n                pointer = getattr(pointer, l[0])\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        print(\"Initialize PyTorch weight {}\".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model", "output": "Load tf checkpoints in a pytorch model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_indicator_DMA(DataFrame, M1=10, M2=50, M3=10):\n    \"\"\"\n    \n    \"\"\"\n    CLOSE = DataFrame.close\n    DDD = MA(CLOSE, M1) - MA(CLOSE, M2)\n    AMA = MA(DDD, M3)\n    return pd.DataFrame({\n        'DDD': DDD, 'AMA': AMA\n    })", "output": "\u5e73\u5747\u7ebf\u5dee DMA", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def api(f):\n    \"\"\"\n    \n    \"\"\"\n    def wraps(self, *args, **kwargs):\n        try:\n            return f(self, *args, **kwargs)\n        except Exception as e:\n            logging.exception(e)\n            return json_error_response(get_error_msg())\n\n    return functools.update_wrapper(wraps, f)", "output": "A decorator to label an endpoint as an API. Catches uncaught exceptions and\n    return the response in the JSON format", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_values(self):\n        \"\"\"\n        \"\"\"\n        def stringify(value):\n            if self.encoding is not None:\n                encoder = partial(pprint_thing_encoded,\n                                  encoding=self.encoding)\n            else:\n                encoder = pprint_thing\n            return encoder(value)\n\n        lhs, rhs = self.lhs, self.rhs\n\n        if is_term(lhs) and lhs.is_datetime and is_term(rhs) and rhs.is_scalar:\n            v = rhs.value\n            if isinstance(v, (int, float)):\n                v = stringify(v)\n            v = Timestamp(_ensure_decoded(v))\n            if v.tz is not None:\n                v = v.tz_convert('UTC')\n            self.rhs.update(v)\n\n        if is_term(rhs) and rhs.is_datetime and is_term(lhs) and lhs.is_scalar:\n            v = lhs.value\n            if isinstance(v, (int, float)):\n                v = stringify(v)\n            v = Timestamp(_ensure_decoded(v))\n            if v.tz is not None:\n                v = v.tz_convert('UTC')\n            self.lhs.update(v)", "output": "Convert datetimes to a comparable value in an expression.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _handle_display_data(self, msg):\n        \"\"\"\n        \n        \"\"\"\n        img = None\n        data = msg['content']['data']\n        if 'image/svg+xml' in data:\n            fmt = 'image/svg+xml'\n            img = data['image/svg+xml']\n        elif 'image/png' in data:\n            # PNG data is base64 encoded as it passes over the network\n            # in a JSON structure so we decode it.\n            fmt = 'image/png'\n            img = decodestring(data['image/png'].encode('ascii'))\n        elif 'image/jpeg' in data and self._jpg_supported:\n            fmt = 'image/jpeg'\n            img = decodestring(data['image/jpeg'].encode('ascii'))\n        if img is not None:\n            self.sig_new_inline_figure.emit(img, fmt)\n            if (self.figurebrowser is not None and\n                    self.figurebrowser.mute_inline_plotting):\n                if not self.sended_render_message:\n                    msg['content']['data']['text/plain'] = ''\n                    self._append_html(\n                        _('<br><hr>'\n                          '\\nFigures now render in the Plots pane by default. '\n                          'To make them also appear inline in the Console, '\n                          'uncheck \"Mute Inline Plotting\" under the Plots '\n                          'pane options menu. \\n'\n                          '<hr><br>'), before_prompt=True)\n                    self.sended_render_message = True\n                else:\n                    msg['content']['data']['text/plain'] = ''\n                del msg['content']['data'][fmt]\n\n        return super(FigureBrowserWidget, self)._handle_display_data(msg)", "output": "Reimplemented to handle communications between the figure explorer\n        and the kernel.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_run(**kwargs):\n    '''\n    \n    '''\n    command = 'show running-config'\n    info = ''\n    info = show(command, **kwargs)\n    if isinstance(info, list):\n        info = info[0]\n    return info", "output": "Shortcut to run `show running-config` on the NX-OS device.\n\n    .. code-block:: bash\n\n        salt '*' nxos.cmd show_run", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_cname(name=None, canonical=None, return_fields=None, **api_opts):\n    '''\n    \n    '''\n    infoblox = _get_infoblox(**api_opts)\n    o = infoblox.get_cname(name=name, canonical=canonical, return_fields=return_fields)\n    return o", "output": "Get CNAME information.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-call infoblox.get_cname name=example.example.com\n        salt-call infoblox.get_cname canonical=example-ha-0.example.com", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_button_box(self, stdbtns):\r\n        \"\"\"\"\"\"\r\n        bbox = QDialogButtonBox(stdbtns)\r\n        run_btn = bbox.addButton(_(\"Run\"), QDialogButtonBox.AcceptRole)\r\n        run_btn.clicked.connect(self.run_btn_clicked)\r\n        bbox.accepted.connect(self.accept)\r\n        bbox.rejected.connect(self.reject)\r\n        btnlayout = QHBoxLayout()\r\n        btnlayout.addStretch(1)\r\n        btnlayout.addWidget(bbox)\r\n        self.layout().addLayout(btnlayout)", "output": "Create dialog button box and add it to the dialog layout", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generic_service_exception(*args):\n        \"\"\"\n        \n        \"\"\"\n        exception_tuple = LambdaErrorResponses.ServiceException\n\n        return BaseLocalService.service_response(\n            LambdaErrorResponses._construct_error_response_body(LambdaErrorResponses.SERVICE_ERROR, \"ServiceException\"),\n            LambdaErrorResponses._construct_headers(exception_tuple[0]),\n            exception_tuple[1]\n        )", "output": "Creates a Lambda Service Generic ServiceException Response\n\n        Parameters\n        ----------\n        args list\n            List of arguments Flask passes to the method\n\n        Returns\n        -------\n        Flask.Response\n            A response object representing the GenericServiceException Error", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_order_threading(self):\n        \"\"\"\n        \"\"\"\n\n        self.if_start_orderthreading = True\n\n        self.order_handler.if_start_orderquery = True\n        self.trade_engine.create_kernel('ORDER', daemon=True)\n        self.trade_engine.start_kernel('ORDER')\n        self.sync_order_and_deal()", "output": "\u5f00\u542f\u67e5\u8be2\u5b50\u7ebf\u7a0b(\u5b9e\u76d8\u4e2d\u7528)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dequantize(q, params):\n  \"\"\"\"\"\"\n  if not params.quantize:\n    return q\n  return tf.to_float(tf.bitcast(q, tf.int16)) * params.quantization_scale", "output": "Dequantize q according to params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _run_query(client, query, job_config=None):\n    \"\"\"\n    \"\"\"\n    start_time = time.time()\n    query_job = client.query(query, job_config=job_config)\n    print(\"Executing query with job ID: {}\".format(query_job.job_id))\n\n    while True:\n        print(\"\\rQuery executing: {:0.2f}s\".format(time.time() - start_time), end=\"\")\n        try:\n            query_job.result(timeout=0.5)\n            break\n        except futures.TimeoutError:\n            continue\n    print(\"\\nQuery complete after {:0.2f}s\".format(time.time() - start_time))\n    return query_job", "output": "Runs a query while printing status updates\n\n    Args:\n        client (google.cloud.bigquery.client.Client):\n            Client to bundle configuration needed for API requests.\n        query (str):\n            SQL query to be executed. Defaults to the standard SQL dialect.\n            Use the ``job_config`` parameter to change dialects.\n        job_config (google.cloud.bigquery.job.QueryJobConfig, optional):\n            Extra configuration options for the job.\n\n    Returns:\n        google.cloud.bigquery.job.QueryJob: the query job created\n\n    Example:\n        >>> client = bigquery.Client()\n        >>> _run_query(client, \"SELECT 17\")\n        Executing query with job ID: bf633912-af2c-4780-b568-5d868058632b\n        Query executing: 1.66s\n        Query complete after 2.07s\n        'bf633912-af2c-4780-b568-5d868058632b'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def TowerContext(tower_name, is_training, vs_name=''):\n    \"\"\"\n    \n    \"\"\"\n    if is_training:\n        return TrainTowerContext(tower_name, vs_name=vs_name)\n    else:\n        return PredictTowerContext(tower_name, vs_name=vs_name)", "output": "The context for a tower function, containing metadata about the current tower.\n    Tensorpack trainers use :class:`TowerContext` to manage tower function.\n    Many tensorpack layers have to be called under a :class:`TowerContext`.\n\n    Example:\n\n    .. code-block:: python\n\n        with TowerContext('', is_training=True):\n            # call a tensorpack layer or a tower function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _MaybePurgeOrphanedData(self, event):\n    \"\"\"\n    \"\"\"\n    if not self.purge_orphaned_data:\n      return\n    ## Check if the event happened after a crash, and purge expired tags.\n    if self.file_version and self.file_version >= 2:\n      ## If the file_version is recent enough, use the SessionLog enum\n      ## to check for restarts.\n      self._CheckForRestartAndMaybePurge(event)\n    else:\n      ## If there is no file version, default to old logic of checking for\n      ## out of order steps.\n      self._CheckForOutOfOrderStepAndMaybePurge(event)", "output": "Maybe purge orphaned data due to a TensorFlow crash.\n\n    When TensorFlow crashes at step T+O and restarts at step T, any events\n    written after step T are now \"orphaned\" and will be at best misleading if\n    they are included in TensorBoard.\n\n    This logic attempts to determine if there is orphaned data, and purge it\n    if it is found.\n\n    Args:\n      event: The event to use as a reference, to determine if a purge is needed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sort(self, ascending=True):\n        \"\"\"\n        \n        \"\"\"\n        from .sframe import SFrame as _SFrame\n\n        if self.dtype not in (int, float, str, datetime.datetime):\n            raise TypeError(\"Only sarray with type (int, float, str, datetime.datetime) can be sorted\")\n        sf = _SFrame()\n        sf['a'] = self\n        return sf.sort('a', ascending)['a']", "output": "Sort all values in this SArray.\n\n        Sort only works for sarray of type str, int and float, otherwise TypeError\n        will be raised. Creates a new, sorted SArray.\n\n        Parameters\n        ----------\n        ascending: boolean, optional\n           If true, the sarray values are sorted in ascending order, otherwise,\n           descending order.\n\n        Returns\n        -------\n        out: SArray\n\n        Examples\n        --------\n        >>> sa = SArray([3,2,1])\n        >>> sa.sort()\n        dtype: int\n        Rows: 3\n        [1, 2, 3]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deepcopy_bound(name):\n    '''\n    \n\n    '''\n    def _deepcopy_method(x, memo):\n        return type(x)(x.im_func, copy.deepcopy(x.im_self, memo), x.im_class)  # pylint: disable=incompatible-py3-code\n    try:\n        pre_dispatch = copy._deepcopy_dispatch\n        copy._deepcopy_dispatch[types.MethodType] = _deepcopy_method\n        ret = copy.deepcopy(name)\n    finally:\n        copy._deepcopy_dispatch = pre_dispatch\n    return ret", "output": "Compatibility helper function to allow copy.deepcopy copy bound methods\n    which is broken on Python 2.6, due to the following bug:\n    https://bugs.python.org/issue1515\n\n    Warnings:\n        - This method will mutate the global deepcopy dispatcher, which means that\n        this function is NOT threadsafe!\n\n        - Not Py3 compatible. The intended use case is deepcopy compat for Py2.6", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_SU_save_financial_files():\n    \"\"\"\n    \"\"\"\n    download_financialzip()\n    coll = DATABASE.financial\n    coll.create_index(\n        [(\"code\", ASCENDING), (\"report_date\", ASCENDING)], unique=True)\n    for item in os.listdir(download_path):\n        if item[0:4] != 'gpcw':\n            print(\n                \"file \", item, \" is not start with gpcw , seems not a financial file , ignore!\")\n            continue\n\n        date = int(item.split('.')[0][-8:])\n        print('QUANTAXIS NOW SAVING {}'.format(date))\n        if coll.find({'report_date': date}).count() < 3600:\n\n            print(coll.find({'report_date': date}).count())\n            data = QA_util_to_json_from_pandas(parse_filelist([item]).reset_index(\n            ).drop_duplicates(subset=['code', 'report_date']).sort_index())\n            # data[\"crawl_date\"] = str(datetime.date.today())\n            try:\n                coll.insert_many(data, ordered=False)\n\n            except Exception as e:\n                if isinstance(e, MemoryError):\n                    coll.insert_many(data, ordered=True)\n                elif isinstance(e, pymongo.bulk.BulkWriteError):\n                    pass\n        else:\n            print('ALL READY IN DATABASE')\n\n    print('SUCCESSFULLY SAVE/UPDATE FINANCIAL DATA')", "output": "\u672c\u5730\u5b58\u50a8financialdata", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def str2ints(self, str_value):\n    \"\"\"\"\"\"\n    if not self._encoder:\n      raise ValueError(\n          \"Text.str2ints is not available because encoder hasn't been defined.\")\n    return self._encoder.encode(str_value)", "output": "Conversion string => encoded list[int].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def namespace(self, value):\n        \"\"\"\n        \"\"\"\n        if not isinstance(value, str):\n            raise ValueError(\"Namespace must be a string\")\n        self._namespace = value", "output": "Update the query's namespace.\n\n        :type value: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_inheritance(path, objectType, user=None):\n    '''\n    \n    '''\n\n    ret = {'result': False,\n           'Inheritance': False,\n           'comment': ''}\n\n    sidRet = _getUserSid(user)\n\n    dc = daclConstants()\n    objectType = dc.getObjectTypeBit(objectType)\n    path = dc.processPath(path, objectType)\n\n    try:\n        sd = win32security.GetNamedSecurityInfo(path, objectType, win32security.DACL_SECURITY_INFORMATION)\n        dacls = sd.GetSecurityDescriptorDacl()\n    except Exception as e:\n        ret['result'] = False\n        ret['comment'] = 'Error obtaining the Security Descriptor or DACL of the path: {0}.'.format(e)\n        return ret\n\n    for counter in range(0, dacls.GetAceCount()):\n        ace = dacls.GetAce(counter)\n        if (ace[0][1] & win32security.INHERITED_ACE) == win32security.INHERITED_ACE:\n            if not sidRet['sid'] or ace[2] == sidRet['sid']:\n                ret['Inheritance'] = True\n                break\n\n    ret['result'] = True\n    return ret", "output": "Check a specified path to verify if inheritance is enabled\n\n    Args:\n        path: path of the registry key or file system object to check\n        objectType: The type of object (FILE, DIRECTORY, REGISTRY)\n        user: if provided, will consider only the ACEs for that user\n\n    Returns (bool): 'Inheritance' of True/False\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'minion-id' win_dacl.check_inheritance c:\\temp directory <username>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_namespaced_role(self, name, namespace, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_role_with_http_info(name, namespace, **kwargs)\n        else:\n            (data) = self.read_namespaced_role_with_http_info(name, namespace, **kwargs)\n            return data", "output": "read the specified Role\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_role(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Role (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1Role\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_properties(self, api_response):\n        \"\"\"\n        \"\"\"\n        cleaned = api_response.copy()\n        self._scrub_local_properties(cleaned)\n\n        statistics = cleaned.get(\"statistics\", {})\n        if \"creationTime\" in statistics:\n            statistics[\"creationTime\"] = float(statistics[\"creationTime\"])\n        if \"startTime\" in statistics:\n            statistics[\"startTime\"] = float(statistics[\"startTime\"])\n        if \"endTime\" in statistics:\n            statistics[\"endTime\"] = float(statistics[\"endTime\"])\n\n        self._properties.clear()\n        self._properties.update(cleaned)\n        self._copy_configuration_properties(cleaned.get(\"configuration\", {}))\n\n        # For Future interface\n        self._set_future_result()", "output": "Update properties from resource in body of ``api_response``\n\n        :type api_response: dict\n        :param api_response: response returned from an API call", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_variable(name, temp_s):\n    '''\n    \n    '''\n    return tf.Variable(tf.zeros(temp_s), name=name)", "output": "Get variable by name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_command(self, ctx, cmd_name):\n        \"\"\"\n        \n        \"\"\"\n        if cmd_name not in self._commands:\n            logger.error(\"Command %s not available\", cmd_name)\n            return\n\n        pkg_name = self._commands[cmd_name]\n\n        try:\n            mod = importlib.import_module(pkg_name)\n        except ImportError:\n            logger.exception(\"Command '%s' is not configured correctly. Unable to import '%s'\", cmd_name, pkg_name)\n            return\n\n        if not hasattr(mod, \"cli\"):\n            logger.error(\"Command %s is not configured correctly. It must expose an function called 'cli'\", cmd_name)\n            return\n\n        return mod.cli", "output": "Overrides method from ``click.MultiCommand`` that returns Click CLI object for given command name, if found.\n\n        :param ctx: Click context\n        :param cmd_name: Top-level command name\n        :return: Click object representing the command", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def maximize(self,\n                 init_points=5,\n                 n_iter=25,\n                 acq='ucb',\n                 kappa=2.576,\n                 xi=0.0,\n                 **gp_params):\n        \"\"\"\"\"\"\n        self._prime_subscriptions()\n        self.dispatch(Events.OPTMIZATION_START)\n        self._prime_queue(init_points)\n        self.set_gp_params(**gp_params)\n\n        util = UtilityFunction(kind=acq, kappa=kappa, xi=xi)\n        iteration = 0\n        while not self._queue.empty or iteration < n_iter:\n            try:\n                x_probe = next(self._queue)\n            except StopIteration:\n                x_probe = self.suggest(util)\n                iteration += 1\n\n            self.probe(x_probe, lazy=False)\n\n        self.dispatch(Events.OPTMIZATION_END)", "output": "Mazimize your function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mcmc_sampling(self):\n        \"\"\"\n        \"\"\"\n        init_weight = np.ones((self.effective_model_num), dtype=np.float) / self.effective_model_num\n        self.weight_samples = np.broadcast_to(init_weight, (NUM_OF_INSTANCE, self.effective_model_num))\n        for i in range(NUM_OF_SIMULATION_TIME):\n            # sample new value from Q(i, j)\n            new_values = np.random.randn(NUM_OF_INSTANCE, self.effective_model_num) * STEP_SIZE + self.weight_samples\n            new_values = self.normalize_weight(new_values)\n            # compute alpha(i, j) = min{1, P(j)Q(j, i)/P(i)Q(i, j)}\n            alpha = np.minimum(1, self.target_distribution(new_values) / self.target_distribution(self.weight_samples))\n            # sample u\n            u = np.random.rand(NUM_OF_INSTANCE)\n            # new value\n            change_value_flag = (u < alpha).astype(np.int)\n            for j in range(NUM_OF_INSTANCE):\n                new_values[j] = self.weight_samples[j] * (1 - change_value_flag[j]) + new_values[j] * change_value_flag[j]\n            self.weight_samples = new_values", "output": "Adjust the weight of each function using mcmc sampling.\n        The initial value of each weight is evenly distribute.\n        Brief introduction:\n        (1)Definition of sample:\n            Sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n        (2)Definition of samples:\n            Samples is a collection of sample, it's a (NUM_OF_INSTANCE * NUM_OF_FUNCTIONS) matrix,\n            representing{{w11, w12, ..., w1k}, {w21, w22, ... w2k}, ...{wk1, wk2,..., wkk}}\n        (3)Definition of model:\n            Model is the function we chose right now. Such as: 'wap', 'weibull'.\n        (4)Definition of pos:\n            Pos is the position we want to predict, corresponds to the value of epoch.\n\n        Returns\n        -------\n        None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absent(name):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    # remove if needed\n    if name in __salt__['pdbedit.list'](False):\n        res = __salt__['pdbedit.delete'](name)\n        if res[name] in ['deleted']:  # check if we need to update changes\n            ret['changes'].update(res)\n        elif res[name] not in ['absent']:  # oops something went wrong\n            ret['result'] = False\n    else:\n        ret['comment'] = 'account {login} is absent'.format(login=name)\n\n    return ret", "output": "Ensure user account is absent\n\n    name : string\n        username", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _decrypt_object(obj):\n    '''\n    \n    '''\n    if isinstance(obj, six.string_types):\n        return _fetch_secret(obj)\n    elif isinstance(obj, dict):\n        for pass_key, pass_path in six.iteritems(obj):\n            obj[pass_key] = _decrypt_object(pass_path)\n    elif isinstance(obj, list):\n        for pass_key, pass_path in enumerate(obj):\n            obj[pass_key] = _decrypt_object(pass_path)\n    return obj", "output": "Recursively try to find a pass path (string) that can be handed off to pass", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _load_version(cls, state, version):\n        \"\"\"\n        \n        \"\"\"\n        assert(version == cls._PYTHON_NN_CLASSIFIER_MODEL_VERSION)\n        knn_model = _tc.nearest_neighbors.NearestNeighborsModel(state['knn_model'])\n        del state['knn_model']\n        state['_target_type'] = eval(state['_target_type'])\n        return cls(knn_model, state)", "output": "A function to load a previously saved NearestNeighborClassifier model.\n\n        Parameters\n        ----------\n        unpickler : GLUnpickler\n            A GLUnpickler file handler.\n\n        version : int\n            Version number maintained by the class writer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dtype_to_stata_type(dtype, column):\n    \"\"\"\n    \n    \"\"\"\n    # TODO: expand to handle datetime to integer conversion\n    if dtype.type == np.object_:  # try to coerce it to the biggest string\n        # not memory efficient, what else could we\n        # do?\n        itemsize = max_len_string_array(ensure_object(column.values))\n        return max(itemsize, 1)\n    elif dtype == np.float64:\n        return 255\n    elif dtype == np.float32:\n        return 254\n    elif dtype == np.int32:\n        return 253\n    elif dtype == np.int16:\n        return 252\n    elif dtype == np.int8:\n        return 251\n    else:  # pragma : no cover\n        raise NotImplementedError(\n            \"Data type {dtype} not supported.\".format(dtype=dtype))", "output": "Convert dtype types to stata types. Returns the byte of the given ordinal.\n    See TYPE_MAP and comments for an explanation. This is also explained in\n    the dta spec.\n    1 - 244 are strings of this length\n                         Pandas    Stata\n    251 - for int8      byte\n    252 - for int16     int\n    253 - for int32     long\n    254 - for float32   float\n    255 - for double    double\n\n    If there are dates to convert, then dtype will already have the correct\n    type inserted.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_bot_commands_formatting(self, commands, heading):\n        \"\"\"\n        \"\"\"\n        if commands:\n            # U+2002 Middle Dot\n            joined = '\\u2002'.join(c.name for c in commands)\n            self.paginator.add_line('__**%s**__' % heading)\n            self.paginator.add_line(joined)", "output": "Adds the minified bot heading with commands to the output.\n\n        The formatting should be added to the :attr:`paginator`.\n\n        The default implementation is a bold underline heading followed\n        by commands separated by an EN SPACE (U+2002) in the next line.\n\n        Parameters\n        -----------\n        commands: Sequence[:class:`Command`]\n            A list of commands that belong to the heading.\n        heading: :class:`str`\n            The heading to add to the line.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def industry_code(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            return self.__dict__[\"industry_code\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'industry_code' \".format(self.order_book_id)\n            )", "output": "[str] \u56fd\u6c11\u7ecf\u6d4e\u884c\u4e1a\u5206\u7c7b\u4ee3\u7801\uff0c\u5177\u4f53\u53ef\u53c2\u8003\u201cIndustry\u5217\u8868\u201d \uff08\u80a1\u7968\u4e13\u7528\uff09", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_shared_locations(self, paths, dry_run=False):\n        \"\"\"\n        \n        \"\"\"\n        shared_path = os.path.join(self.path, 'SHARED')\n        logger.info('creating %s', shared_path)\n        if dry_run:\n            return None\n        lines = []\n        for key in ('prefix', 'lib', 'headers', 'scripts', 'data'):\n            path = paths[key]\n            if os.path.isdir(paths[key]):\n                lines.append('%s=%s' % (key,  path))\n        for ns in paths.get('namespace', ()):\n            lines.append('namespace=%s' % ns)\n\n        with codecs.open(shared_path, 'w', encoding='utf-8') as f:\n            f.write('\\n'.join(lines))\n        return shared_path", "output": "Write shared location information to the SHARED file in .dist-info.\n        :param paths: A dictionary as described in the documentation for\n        :meth:`shared_locations`.\n        :param dry_run: If True, the action is logged but no file is actually\n                        written.\n        :return: The path of the file written to.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def address_info(self):\n        \"\"\"\"\"\"\n        return {\n            \"node_ip_address\": self._node_ip_address,\n            \"redis_address\": self._redis_address,\n            \"object_store_address\": self._plasma_store_socket_name,\n            \"raylet_socket_name\": self._raylet_socket_name,\n            \"webui_url\": self._webui_url,\n        }", "output": "Get a dictionary of addresses.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_policy(self):\n        \"\"\"\n        \n        \"\"\"\n        self.trainer_metrics.start_policy_update_timer(\n            number_experiences=len(self.training_buffer.update_buffer['actions']),\n            mean_return=float(np.mean(self.cumulative_returns_since_policy_update)))\n        n_sequences = max(int(self.trainer_parameters['batch_size'] / self.policy.sequence_length), 1)\n        value_total, policy_total, forward_total, inverse_total = [], [], [], []\n        advantages = self.training_buffer.update_buffer['advantages'].get_batch()\n        self.training_buffer.update_buffer['advantages'].set(\n            (advantages - advantages.mean()) / (advantages.std() + 1e-10))\n        num_epoch = self.trainer_parameters['num_epoch']\n        for _ in range(num_epoch):\n            self.training_buffer.update_buffer.shuffle()\n            buffer = self.training_buffer.update_buffer\n            for l in range(len(self.training_buffer.update_buffer['actions']) // n_sequences):\n                start = l * n_sequences\n                end = (l + 1) * n_sequences\n                run_out = self.policy.update(buffer.make_mini_batch(start, end), n_sequences)\n                value_total.append(run_out['value_loss'])\n                policy_total.append(np.abs(run_out['policy_loss']))\n                if self.use_curiosity:\n                    inverse_total.append(run_out['inverse_loss'])\n                    forward_total.append(run_out['forward_loss'])\n        self.stats['Losses/Value Loss'].append(np.mean(value_total))\n        self.stats['Losses/Policy Loss'].append(np.mean(policy_total))\n        if self.use_curiosity:\n            self.stats['Losses/Forward Loss'].append(np.mean(forward_total))\n            self.stats['Losses/Inverse Loss'].append(np.mean(inverse_total))\n        self.training_buffer.reset_update_buffer()\n        self.trainer_metrics.end_policy_update()", "output": "Uses demonstration_buffer to update the policy.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_config(**kwargs):\n    '''\n    \n    '''\n    config = {\n        'filter_id_regex': ['.*!doc_skip'],\n        'filter_function_regex': [],\n        'replace_text_regex': {},\n        'proccesser': 'highstate_doc.proccesser_markdown',\n        'max_render_file_size': 10000,\n        'note': None\n    }\n    if '__salt__' in globals():\n        config_key = '{0}.config'.format(__virtualname__)\n        config.update(__salt__['config.get'](config_key, {}))\n    # pylint: disable=C0201\n    for k in set(config.keys()) & set(kwargs.keys()):\n        config[k] = kwargs[k]\n    return config", "output": "Return configuration", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def network_size(value, options=None, version=None):\n    '''\n    \n    '''\n    ipaddr_filter_out = _filter_ipaddr(value, options=options, version=version)\n    if not ipaddr_filter_out:\n        return\n    if not isinstance(value, (list, tuple, types.GeneratorType)):\n        return _network_size(ipaddr_filter_out[0])\n    return [\n        _network_size(ip_a)\n        for ip_a in ipaddr_filter_out\n    ]", "output": "Get the size of a network.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_util_datetime_to_strdatetime(dt):\n    \"\"\"\n    \n    \"\"\"\n    strdatetime = \"%04d-%02d-%02d %02d:%02d:%02d\" % (\n        dt.year,\n        dt.month,\n        dt.day,\n        dt.hour,\n        dt.minute,\n        dt.second\n    )\n    return strdatetime", "output": ":param dt:  pythone datetime.datetime\n    :return:  1999-02-01 09:30:91 string type", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def translate_device_rates(val, numeric_rate=True):\n    '''\n    \n    '''\n    val = map_vals(val, 'Path', 'Rate')\n    for idx in range(len(val)):\n        try:\n            is_abs = os.path.isabs(val[idx]['Path'])\n        except AttributeError:\n            is_abs = False\n        if not is_abs:\n            raise SaltInvocationError(\n                'Path \\'{Path}\\' is not absolute'.format(**val[idx])\n            )\n\n        # Attempt to convert to an integer. Will fail if rate was specified as\n        # a shorthand (e.g. 1mb), this is OK as we will check to make sure the\n        # value is an integer below if that is what is required.\n        try:\n            val[idx]['Rate'] = int(val[idx]['Rate'])\n        except (TypeError, ValueError):\n            pass\n\n        if numeric_rate:\n            try:\n                val[idx]['Rate'] = int(val[idx]['Rate'])\n            except ValueError:\n                raise SaltInvocationError(\n                    'Rate \\'{Rate}\\' for path \\'{Path}\\' is '\n                    'non-numeric'.format(**val[idx])\n                )\n    return val", "output": "CLI input is a list of PATH:RATE pairs, but the API expects a list of\n    dictionaries in the format [{'Path': path, 'Rate': rate}]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait_for_exit(self, raise_error: bool = True) -> \"Future[int]\":\n        \"\"\"\n        \"\"\"\n        future = Future()  # type: Future[int]\n\n        def callback(ret: int) -> None:\n            if ret != 0 and raise_error:\n                # Unfortunately we don't have the original args any more.\n                future_set_exception_unless_cancelled(\n                    future, CalledProcessError(ret, \"unknown\")\n                )\n            else:\n                future_set_result_unless_cancelled(future, ret)\n\n        self.set_exit_callback(callback)\n        return future", "output": "Returns a `.Future` which resolves when the process exits.\n\n        Usage::\n\n            ret = yield proc.wait_for_exit()\n\n        This is a coroutine-friendly alternative to `set_exit_callback`\n        (and a replacement for the blocking `subprocess.Popen.wait`).\n\n        By default, raises `subprocess.CalledProcessError` if the process\n        has a non-zero exit status. Use ``wait_for_exit(raise_error=False)``\n        to suppress this behavior and return the exit status without raising.\n\n        .. versionadded:: 4.2\n\n        Availability: Unix", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def itn(n, digits=8, format=DEFAULT_FORMAT):\n    \"\"\"\n    \"\"\"\n    # POSIX 1003.1-1988 requires numbers to be encoded as a string of\n    # octal digits followed by a null-byte, this allows values up to\n    # (8**(digits-1))-1. GNU tar allows storing numbers greater than\n    # that if necessary. A leading 0o200 byte indicates this particular\n    # encoding, the following digits-1 bytes are a big-endian\n    # representation. This allows values up to (256**(digits-1))-1.\n    if 0 <= n < 8 ** (digits - 1):\n        s = (\"%0*o\" % (digits - 1, n)).encode(\"ascii\") + NUL\n    else:\n        if format != GNU_FORMAT or n >= 256 ** (digits - 1):\n            raise ValueError(\"overflow in number field\")\n\n        if n < 0:\n            # XXX We mimic GNU tar's behaviour with negative numbers,\n            # this could raise OverflowError.\n            n = struct.unpack(\"L\", struct.pack(\"l\", n))[0]\n\n        s = bytearray()\n        for i in range(digits - 1):\n            s.insert(0, n & 0o377)\n            n >>= 8\n        s.insert(0, 0o200)\n    return s", "output": "Convert a python number to a number field.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def swaplevel(self, i=-2, j=-1, axis=0):\n        \"\"\"\n        \n        \"\"\"\n        result = self.copy()\n\n        axis = self._get_axis_number(axis)\n        if axis == 0:\n            result.index = result.index.swaplevel(i, j)\n        else:\n            result.columns = result.columns.swaplevel(i, j)\n        return result", "output": "Swap levels i and j in a MultiIndex on a particular axis.\n\n        Parameters\n        ----------\n        i, j : int, string (can be mixed)\n            Level of index to be swapped. Can pass level name as string.\n\n        Returns\n        -------\n        DataFrame\n\n        .. versionchanged:: 0.18.1\n\n           The indexes ``i`` and ``j`` are now optional, and default to\n           the two innermost levels of the index.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_to_color(cls, color_spec):\n        \"\"\"\n        \n        \"\"\"\n\n        from openpyxl.styles import Color\n\n        if isinstance(color_spec, str):\n            return Color(color_spec)\n        else:\n            return Color(**color_spec)", "output": "Convert ``color_spec`` to an openpyxl v2 Color object\n        Parameters\n        ----------\n        color_spec : str, dict\n            A 32-bit ARGB hex string, or a dict with zero or more of the\n            following keys.\n                'rgb'\n                'indexed'\n                'auto'\n                'theme'\n                'tint'\n                'index'\n                'type'\n        Returns\n        -------\n        color : openpyxl.styles.Color", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def status(name=None, user=None, conf_file=None, bin_env=None):\n    '''\n    \n    '''\n    all_process = {}\n    for line in status_raw(name, user, conf_file, bin_env).splitlines():\n        if len(line.split()) > 2:\n            process, state, reason = line.split(None, 2)\n        else:\n            process, state, reason = line.split() + ['']\n        all_process[process] = {'state': state, 'reason': reason}\n    return all_process", "output": "List programs and its state\n\n    user\n        user to run supervisorctl as\n    conf_file\n        path to supervisord config file\n    bin_env\n        path to supervisorctl bin or path to virtualenv with supervisor\n        installed\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' supervisord.status", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_slider_range(self, cursor_pos):\n        \"\"\"\"\"\"\n        # The slider range indicator position follows the mouse vertical\n        # position while its height corresponds to the part of the file that\n        # is currently visible on screen.\n\n        vsb = self.editor.verticalScrollBar()\n        groove_height = self.get_scrollbar_position_height()\n        slider_height = self.value_to_position(vsb.pageStep())-self.offset\n\n        # Calcul the minimum and maximum y-value to constraint the slider\n        # range indicator position to the height span of the scrollbar area\n        # where the slider may move.\n        min_ypos = self.offset\n        max_ypos = groove_height + self.offset - slider_height\n\n        # Determine the bounded y-position of the slider rect.\n        slider_y = max(min_ypos, min(max_ypos, cursor_pos.y()-slider_height/2))\n\n        return QRect(1, slider_y, self.WIDTH-2, slider_height)", "output": "Make slider range QRect", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absent(name, zone, data=None, rdtype=None, **kwargs):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': False,\n           'comment': ''}\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = '{0} record \"{1}\" will be deleted'.format(rdtype, name)\n        return ret\n\n    status = __salt__['ddns.delete'](zone, name, rdtype, data, **kwargs)\n\n    if status is None:\n        ret['result'] = True\n        ret['comment'] = 'No matching DNS record(s) present'\n    elif status:\n        ret['result'] = True\n        ret['comment'] = 'Deleted DNS record(s)'\n        ret['changes'] = {'Deleted': {'name': name,\n                                      'zone': zone\n                                     }\n                         }\n    else:\n        ret['result'] = False\n        ret['comment'] = 'Failed to delete DNS record(s)'\n    return ret", "output": "Ensures that the named DNS record is absent.\n\n    name\n        The host portion of the DNS record, e.g., 'webserver'. Name and zone\n        are concatenated when the entry is created unless name includes a\n        trailing dot, so make sure that information is not duplicated in these\n        two arguments.\n\n    zone\n        The zone to check\n\n    data\n        Data for the DNS record. E.g., the IP address for an A record. If omitted,\n        all records matching name (and rdtype, if provided) will be purged.\n\n    rdtype\n        DNS resource type. If omitted, all types will be purged.\n\n    ``**kwargs``\n        Additional arguments the ddns.update function may need (e.g.\n        nameserver, keyfile, keyname).  Note that the nsupdate key file can\u2019t\n        be reused by this function, the keyfile and other arguments must\n        follow the `dnspython <http://www.dnspython.org/>`_ spec.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_as_spell_check(self, color=Qt.blue):\n        \"\"\"\n        \n        \"\"\"\n        self.format.setUnderlineStyle(\n            QTextCharFormat.SpellCheckUnderline)\n        self.format.setUnderlineColor(color)", "output": "Underlines text as a spellcheck error.\n\n        :param color: Underline color\n        :type color: QtGui.QColor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dir_list(load):\n    '''\n    \n    '''\n    if 'env' in load:\n        # \"env\" is not supported; Use \"saltenv\".\n        load.pop('env')\n\n    ret = []\n\n    if 'saltenv' not in load:\n        return ret\n\n    saltenv = load['saltenv']\n    metadata = _init()\n\n    if not metadata or saltenv not in metadata:\n        return ret\n\n    # grab all the dirs from the buckets cache file\n    for bucket in _find_dirs(metadata[saltenv]):\n        for dirs in six.itervalues(bucket):\n            # trim env and trailing slash\n            dirs = _trim_env_off_path(dirs, saltenv, trim_slash=True)\n            # remove empty string left by the base env dir in single bucket mode\n            ret += [_f for _f in dirs if _f]\n\n    return ret", "output": "Return a list of all directories on the master", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zeros(stype, shape, ctx=None, dtype=None, **kwargs):\n    \"\"\"\n    \"\"\"\n    # pylint: disable= no-member, protected-access\n    if stype == 'default':\n        return _zeros_ndarray(shape, ctx=ctx, dtype=dtype, **kwargs)\n    if ctx is None:\n        ctx = current_context()\n    dtype = mx_real_t if dtype is None else dtype\n    if stype in ('row_sparse', 'csr'):\n        aux_types = _STORAGE_AUX_TYPES[stype]\n    else:\n        raise ValueError(\"unknown storage type\" + stype)\n    out = _ndarray_cls(_new_alloc_handle(stype, shape, ctx, True, dtype, aux_types))\n    return _internal._zeros(shape=shape, ctx=ctx, dtype=dtype, out=out, **kwargs)", "output": "Return a new array of given shape and type, filled with zeros.\n\n    Parameters\n    ----------\n    stype: string\n        The storage type of the empty array, such as 'row_sparse', 'csr', etc\n    shape : int or tuple of int\n        The shape of the empty array\n    ctx : Context, optional\n        An optional device context (default is the current default context)\n    dtype : str or numpy.dtype, optional\n        An optional value type (default is `float32`)\n\n    Returns\n    -------\n    RowSparseNDArray or CSRNDArray\n        A created array\n    Examples\n    --------\n    >>> mx.nd.sparse.zeros('csr', (1,2))\n    <CSRNDArray 1x2 @cpu(0)>\n    >>> mx.nd.sparse.zeros('row_sparse', (1,2), ctx=mx.cpu(), dtype='float16').asnumpy()\n    array([[ 0.,  0.]], dtype=float16)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        \"\"\"\n        \"\"\"\n        # Initialize the subscription channel.\n        self.subscribe(ray.gcs_utils.XRAY_HEARTBEAT_BATCH_CHANNEL)\n        self.subscribe(ray.gcs_utils.XRAY_DRIVER_CHANNEL)\n\n        # TODO(rkn): If there were any dead clients at startup, we should clean\n        # up the associated state in the state tables.\n\n        # Handle messages from the subscription channels.\n        while True:\n            # Update the mapping from raylet client ID to IP address.\n            # This is only used to update the load metrics for the autoscaler.\n            self.update_raylet_map()\n\n            # Process autoscaling actions\n            if self.autoscaler:\n                self.autoscaler.update()\n\n            self._maybe_flush_gcs()\n\n            # Process a round of messages.\n            self.process_messages()\n\n            # Wait for a heartbeat interval before processing the next round of\n            # messages.\n            time.sleep(ray._config.heartbeat_timeout_milliseconds() * 1e-3)", "output": "Run the monitor.\n\n        This function loops forever, checking for messages about dead database\n        clients and cleaning up state accordingly.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def was_installed_by_pip(pkg):\n    # type: (str) -> bool\n    \"\"\"\n    \"\"\"\n    try:\n        dist = pkg_resources.get_distribution(pkg)\n        return (dist.has_metadata('INSTALLER') and\n                'pip' in dist.get_metadata_lines('INSTALLER'))\n    except pkg_resources.DistributionNotFound:\n        return False", "output": "Checks whether pkg was installed by pip\n\n    This is used not to display the upgrade message when pip is in fact\n    installed by system package manager, such as dnf on Fedora.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keyPressEvent(self, event):\n        \"\"\"\"\"\"\n        if event.key() == Qt.Key_Alt:\n            self._alt_key_is_down = True\n            self.update()", "output": "Override Qt method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decompress(self, value: bytes, max_length: int = 0) -> bytes:\n        \"\"\"\n        \"\"\"\n        return self.decompressobj.decompress(value, max_length)", "output": "Decompress a chunk, returning newly-available data.\n\n        Some data may be buffered for later processing; `flush` must\n        be called when there is no more input data to ensure that\n        all data was processed.\n\n        If ``max_length`` is given, some input data may be left over\n        in ``unconsumed_tail``; you must retrieve this value and pass\n        it back to a future call to `decompress` if it is not empty.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bokeh_commit(name, rawtext, text, lineno, inliner, options=None, content=None):\n    ''' \n\n    '''\n    app = inliner.document.settings.env.app\n    node = _make_gh_link_node(app, rawtext, 'commit', 'commit ', 'commit', text, options)\n    return [node], []", "output": "Link to a Bokeh Github issue.\n\n    Returns 2 part tuple containing list of nodes to insert into the\n    document and a list of system messages.  Both are allowed to be\n    empty.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_namespaced_daemon_set(self, name, namespace, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_daemon_set_with_http_info(name, namespace, **kwargs)\n        else:\n            (data) = self.read_namespaced_daemon_set_with_http_info(name, namespace, **kwargs)\n            return data", "output": "read the specified DaemonSet\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_daemon_set(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the DaemonSet (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'. Deprecated. Planned for removal in 1.18.\n        :param bool export: Should this value be exported.  Export strips fields that a user can not specify. Deprecated. Planned for removal in 1.18.\n        :return: V1DaemonSet\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def calc(self, x:Image, *args:Any, **kwargs:Any)->Image:\n        \"\"\n        if self._wrap: return getattr(x, self._wrap)(self.func, *args, **kwargs)\n        else:          return self.func(x, *args, **kwargs)", "output": "Apply to image `x`, wrapping it if necessary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(self, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        Statement = self.get_model('statement')\n        Tag = self.get_model('tag')\n\n        tags = kwargs.pop('tags', [])\n\n        if 'search_text' not in kwargs:\n            kwargs['search_text'] = self.tagger.get_bigram_pair_string(kwargs['text'])\n\n        if 'search_in_response_to' not in kwargs:\n            if kwargs.get('in_response_to'):\n                kwargs['search_in_response_to'] = self.tagger.get_bigram_pair_string(kwargs['in_response_to'])\n\n        statement = Statement(**kwargs)\n\n        statement.save()\n\n        tags_to_add = []\n\n        for _tag in tags:\n            tag, _ = Tag.objects.get_or_create(name=_tag)\n            tags_to_add.append(tag)\n\n        statement.tags.add(*tags_to_add)\n\n        return statement", "output": "Creates a new statement matching the keyword arguments specified.\n        Returns the created statement.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _tool_from_string(name):\n    \"\"\"  \"\"\"\n    known_tools = sorted(_known_tools.keys())\n\n    if name in known_tools:\n        tool_fn = _known_tools[name]\n\n        if isinstance(tool_fn, string_types):\n            tool_fn = _known_tools[tool_fn]\n\n        return tool_fn()\n    else:\n        matches, text = difflib.get_close_matches(name.lower(), known_tools), \"similar\"\n\n        if not matches:\n            matches, text = known_tools, \"possible\"\n\n        raise ValueError(\"unexpected tool name '%s', %s tools are %s\" % (name, text, nice_join(matches)))", "output": "Takes a string and returns a corresponding `Tool` instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attention_image_summary(attn, image_shapes=None):\n  \"\"\"\n  \"\"\"\n  attn = tf.cast(attn, tf.float32)\n  num_heads = common_layers.shape_list(attn)[1]\n  # [batch, query_length, memory_length, num_heads]\n  image = tf.transpose(attn, [0, 2, 3, 1])\n  image = tf.pow(image, 0.2)  # for high-dynamic-range\n  # Each head will correspond to one of RGB.\n  # pad the heads to be a multiple of 3\n  image = tf.pad(image, [[0, 0], [0, 0], [0, 0], [0, tf.mod(-num_heads, 3)]])\n  image = split_last_dimension(image, 3)\n  image = tf.reduce_max(image, 4)\n  if image_shapes is not None:\n    if len(image_shapes) == 4:\n      q_rows, q_cols, m_rows, m_cols = list(image_shapes)\n      image = tf.reshape(image, [-1, q_rows, q_cols, m_rows, m_cols, 3])\n      image = tf.transpose(image, [0, 1, 3, 2, 4, 5])\n      image = tf.reshape(image, [-1, q_rows * m_rows, q_cols * m_cols, 3])\n    else:\n      assert len(image_shapes) == 6\n      q_rows, q_cols, q_channnels, m_rows, m_cols, m_channels = list(\n          image_shapes)\n      image = tf.reshape(\n          image,\n          [-1, q_rows, q_cols, q_channnels, m_rows, m_cols, m_channels, 3])\n      image = tf.transpose(image, [0, 1, 4, 3, 2, 5, 6, 7])\n      image = tf.reshape(\n          image,\n          [-1, q_rows * m_rows * q_channnels, q_cols * m_cols * m_channels, 3])\n  tf.summary.image(\"attention\", image, max_outputs=1)", "output": "Compute color image summary.\n\n  Args:\n    attn: a Tensor with shape [batch, num_heads, query_length, memory_length]\n    image_shapes: optional tuple of integer scalars.\n      If the query positions and memory positions represent the\n      pixels of flattened images, then pass in their dimensions:\n        (query_rows, query_cols, memory_rows, memory_cols).\n      If the query positions and memory positions represent the\n      pixels x channels of flattened images, then pass in their dimensions:\n        (query_rows, query_cols, query_channels,\n         memory_rows, memory_cols, memory_channels).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def table(self, tableName):\n        \"\"\"\n        \"\"\"\n        return DataFrame(self._jsparkSession.table(tableName), self._wrapped)", "output": "Returns the specified table as a :class:`DataFrame`.\n\n        :return: :class:`DataFrame`\n\n        >>> df.createOrReplaceTempView(\"table1\")\n        >>> df2 = spark.table(\"table1\")\n        >>> sorted(df.collect()) == sorted(df2.collect())\n        True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_filter(self, root_path, path_list):\r\n        \"\"\"\"\"\"\r\n        self.root_path = osp.normpath(to_text_string(root_path))\r\n        self.path_list = [osp.normpath(to_text_string(p)) for p in path_list]\r\n        self.invalidateFilter()", "output": "Setup proxy model filter parameters", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_resource_result(f, futmap):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            result = f.result()\n            for resource, configs in result.items():\n                fut = futmap.get(resource, None)\n                if fut is None:\n                    raise RuntimeError(\"Resource {} not found in future-map: {}\".format(resource, futmap))\n                if resource.error is not None:\n                    # Resource-level exception\n                    fut.set_exception(KafkaException(resource.error))\n                else:\n                    # Resource-level success\n                    # configs will be a dict for describe_configs()\n                    # and None for alter_configs()\n                    fut.set_result(configs)\n        except Exception as e:\n            # Request-level exception, raise the same for all resources\n            for resource, fut in futmap.items():\n                fut.set_exception(e)", "output": "Map per-resource results to per-resource futures in futmap.\n        The result value of each (successful) future is a ConfigResource.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe(table_name, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    table = Table(table_name, connection=conn)\n    return table.describe()", "output": "Describe a DynamoDB table.\n\n    CLI example::\n\n        salt myminion boto_dynamodb.describe table_name region=us-east-1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def step(self, exploration):\n        \"\"\"\n        \n        \"\"\"\n        old_s = self._current_ob\n        if self.rng.rand() <= exploration:\n            act = self.rng.choice(range(self.num_actions))\n        else:\n            history = self.recent_state()\n            history.append(old_s)\n            history = np.stack(history, axis=-1)  # state_shape + (Hist,)\n\n            # assume batched network\n            history = np.expand_dims(history, axis=0)\n            q_values = self.predictor(history)[0][0]  # this is the bottleneck\n            act = np.argmax(q_values)\n\n        self._current_ob, reward, isOver, info = self.player.step(act)\n        self._current_game_score.feed(reward)\n        self._current_episode.append(Experience(old_s, act, reward, isOver))\n\n        if isOver:\n            flush_experience = True\n            if 'ale.lives' in info:  # if running Atari, do something special\n                if info['ale.lives'] != 0:\n                    # only record score and flush experience\n                    # when a whole game is over (not when an episode is over)\n                    flush_experience = False\n            self.player.reset()\n\n            if flush_experience:\n                self.total_scores.append(self._current_game_score.sum)\n                self._current_game_score.reset()\n\n                # Ensure that the whole episode of experience is continuous in the replay buffer\n                with self.memory.writer_lock:\n                    for exp in self._current_episode:\n                        self.memory.append(exp)\n                self._current_episode.clear()", "output": "Run the environment for one step.\n        If the episode ends, store the entire episode to the replay memory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_statistics_information(info):\n  \"\"\"\"\"\"\n  if not info.splits.total_num_examples:\n    # That means that we have yet to calculate the statistics for this.\n    return \"None computed\"\n\n  stats = [(info.splits.total_num_examples, \"ALL\")]\n  for split_name, split_info in info.splits.items():\n    stats.append((split_info.num_examples, split_name.upper()))\n  # Sort reverse on number of examples.\n  stats.sort(reverse=True)\n  stats = \"\\n\".join([\n      \"{0:10} | {1:>10,}\".format(name, num_exs) for (num_exs, name) in stats\n  ])\n  return STATISTICS_TABLE.format(split_statistics=stats)", "output": "Make statistics information table.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def eventFilter(self, widget, event):\r\n        \"\"\"\"\"\"\r\n        if event.type() == QEvent.KeyPress and event.key() == Qt.Key_Delete:\r\n            index = self.view().currentIndex().row()\r\n            if index >= EXTERNAL_PATHS:\r\n                # Remove item and update the view.\r\n                self.removeItem(index)\r\n                self.showPopup()\r\n                # Set the view selection so that it doesn't bounce around.\r\n                new_index = min(self.count() - 1, index)\r\n                new_index = 0 if new_index < EXTERNAL_PATHS else new_index\r\n                self.view().setCurrentIndex(self.model().index(new_index, 0))\r\n                self.setCurrentIndex(new_index)\r\n            return True\r\n        return QComboBox.eventFilter(self, widget, event)", "output": "Used to handle key events on the QListView of the combobox.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def random_rollout_subsequences(rollouts, num_subsequences, subsequence_length):\n  \"\"\"\"\"\"\n  def choose_subsequence():\n    # TODO(koz4k): Weigh rollouts by their lengths so sampling is uniform over\n    # frames and not rollouts.\n    rollout = random.choice(rollouts)\n    try:\n      from_index = random.randrange(len(rollout) - subsequence_length + 1)\n    except ValueError:\n      # Rollout too short; repeat.\n      return choose_subsequence()\n    return rollout[from_index:(from_index + subsequence_length)]\n\n  return [choose_subsequence() for _ in range(num_subsequences)]", "output": "Chooses a random frame sequence of given length from a set of rollouts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def treeReduce(self, f, depth=2):\n        \"\"\"\n        \n        \"\"\"\n        if depth < 1:\n            raise ValueError(\"Depth cannot be smaller than 1 but got %d.\" % depth)\n\n        zeroValue = None, True  # Use the second entry to indicate whether this is a dummy value.\n\n        def op(x, y):\n            if x[1]:\n                return y\n            elif y[1]:\n                return x\n            else:\n                return f(x[0], y[0]), False\n\n        reduced = self.map(lambda x: (x, False)).treeAggregate(zeroValue, op, op, depth)\n        if reduced[1]:\n            raise ValueError(\"Cannot reduce empty RDD.\")\n        return reduced[0]", "output": "Reduces the elements of this RDD in a multi-level tree pattern.\n\n        :param depth: suggested depth of the tree (default: 2)\n\n        >>> add = lambda x, y: x + y\n        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n        >>> rdd.treeReduce(add)\n        -5\n        >>> rdd.treeReduce(add, 1)\n        -5\n        >>> rdd.treeReduce(add, 2)\n        -5\n        >>> rdd.treeReduce(add, 5)\n        -5\n        >>> rdd.treeReduce(add, 10)\n        -5", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_deps_list(self, pkg_name, installed_distros=None):\n        \"\"\"\n        \n        \"\"\"\n        # https://github.com/Miserlou/Zappa/issues/1478.  Using `pkg_resources`\n        # instead of `pip` is the recommended approach.  The usage is nearly\n        # identical.\n        import pkg_resources\n        deps = []\n        if not installed_distros:\n            installed_distros = pkg_resources.WorkingSet()\n        for package in installed_distros:\n            if package.project_name.lower() == pkg_name.lower():\n                deps = [(package.project_name, package.version)]\n                for req in package.requires():\n                    deps += self.get_deps_list(pkg_name=req.project_name, installed_distros=installed_distros)\n        return list(set(deps))", "output": "For a given package, returns a list of required packages. Recursive.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _stop_trial(self, trial, error=False, error_msg=None,\n                    stop_logger=True):\n        \"\"\"\n        \"\"\"\n\n        if stop_logger:\n            trial.close_logger()\n\n        if error:\n            self.set_status(trial, Trial.ERROR)\n        else:\n            self.set_status(trial, Trial.TERMINATED)\n\n        try:\n            trial.write_error_log(error_msg)\n            if hasattr(trial, \"runner\") and trial.runner:\n                if (not error and self._reuse_actors\n                        and self._cached_actor is None):\n                    logger.debug(\"Reusing actor for {}\".format(trial.runner))\n                    self._cached_actor = trial.runner\n                else:\n                    logger.info(\n                        \"Destroying actor for trial {}. If your trainable is \"\n                        \"slow to initialize, consider setting \"\n                        \"reuse_actors=True to reduce actor creation \"\n                        \"overheads.\".format(trial))\n                    trial.runner.stop.remote()\n                    trial.runner.__ray_terminate__.remote()\n        except Exception:\n            logger.exception(\"Error stopping runner for Trial %s\", str(trial))\n            self.set_status(trial, Trial.ERROR)\n        finally:\n            trial.runner = None", "output": "Stops this trial.\n\n        Stops this trial, releasing all allocating resources. If stopping the\n        trial fails, the run will be marked as terminated in error, but no\n        exception will be thrown.\n\n        Args:\n            error (bool): Whether to mark this trial as terminated in error.\n            error_msg (str): Optional error message.\n            stop_logger (bool): Whether to shut down the trial logger.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_items(self, row_lookup, col_lookup, item):\n        \"\"\"\n        \"\"\"\n        self.qc.write_items(row_lookup, col_lookup, item)", "output": "Perform remote write and replace blocks.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pyeapi_call(method, *args, **kwargs):\n    '''\n    \n   '''\n    pyeapi_kwargs = pyeapi_nxos_api_args(**kwargs)\n    return __salt__['pyeapi.call'](method, *args, **pyeapi_kwargs)", "output": ".. versionadded:: 2019.2.0\n\n    Invoke an arbitrary method from the ``pyeapi`` library.\n    This function forwards the existing connection details to the\n    :mod:`pyeapi.run_commands <salt.module.arista_pyeapi.run_commands>`\n    execution function.\n\n    method\n        The name of the ``pyeapi`` method to invoke.\n\n    kwargs\n        Key-value arguments to send to the ``pyeapi`` method.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm.pyeapi_call run_commands 'show version' encoding=text\n        salt '*' napalm.pyeapi_call get_config as_string=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def save(self, fp, *, seek_begin=True):\n        \"\"\"\n        \"\"\"\n\n        data = await self.read()\n        if isinstance(fp, io.IOBase) and fp.writable():\n            written = fp.write(data)\n            if seek_begin:\n                fp.seek(0)\n            return written\n        else:\n            with open(fp, 'wb') as f:\n                return f.write(data)", "output": "|coro|\n\n        Saves this asset into a file-like object.\n\n        Parameters\n        ----------\n        fp: Union[BinaryIO, :class:`os.PathLike`]\n            Same as in :meth:`Attachment.save`.\n        seek_begin: :class:`bool`\n            Same as in :meth:`Attachment.save`.\n\n        Raises\n        ------\n        Same as :meth:`read`.\n\n        Returns\n        --------\n        :class:`int`\n            The number of bytes written.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pool_to_HW(shape, data_frmt):\n    \"\"\" \n    \"\"\"\n    if len(shape) != 4:\n        return shape # Not NHWC|NCHW, return as is\n    if data_frmt == 'NCHW':\n        return [shape[2], shape[3]]\n    return [shape[1], shape[2]]", "output": "Convert from NHWC|NCHW => HW", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def my_protocol_parser(out, buf):\n    \"\"\"\n    \"\"\"\n    while True:\n        tp = yield from buf.read(5)\n        if tp in (MSG_PING, MSG_PONG):\n            # skip line\n            yield from buf.skipuntil(b'\\r\\n')\n            out.feed_data(Message(tp, None))\n        elif tp == MSG_STOP:\n            out.feed_data(Message(tp, None))\n        elif tp == MSG_TEXT:\n            # read text\n            text = yield from buf.readuntil(b'\\r\\n')\n            out.feed_data(Message(tp, text.strip().decode('utf-8')))\n        else:\n            raise ValueError('Unknown protocol prefix.')", "output": "Parser is used with StreamParser for incremental protocol parsing.\n    Parser is a generator function, but it is not a coroutine. Usually\n    parsers are implemented as a state machine.\n\n    more details in asyncio/parsers.py\n    existing parsers:\n      * HTTP protocol parsers asyncio/http/protocol.py\n      * websocket parser asyncio/http/websocket.py", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_secure_stub(credentials, user_agent, stub_class, host, extra_options=()):\n    \"\"\"\n    \"\"\"\n    channel = make_secure_channel(\n        credentials, user_agent, host, extra_options=extra_options\n    )\n    return stub_class(channel)", "output": "Makes a secure stub for an RPC service.\n\n    Uses / depends on gRPC.\n\n    :type credentials: :class:`google.auth.credentials.Credentials`\n    :param credentials: The OAuth2 Credentials to use for creating\n                        access tokens.\n\n    :type user_agent: str\n    :param user_agent: The user agent to be used with API requests.\n\n    :type stub_class: type\n    :param stub_class: A gRPC stub type for a given service.\n\n    :type host: str\n    :param host: The host for the service.\n\n    :type extra_options: tuple\n    :param extra_options: (Optional) Extra gRPC options passed when creating\n                          the channel.\n\n    :rtype: object, instance of ``stub_class``\n    :returns: The stub object used to make gRPC requests to a given API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare_auth(self, auth, url=''):\n        \"\"\"\"\"\"\n\n        # If no Auth is explicitly provided, extract it from the URL first.\n        if auth is None:\n            url_auth = get_auth_from_url(self.url)\n            auth = url_auth if any(url_auth) else None\n\n        if auth:\n            if isinstance(auth, tuple) and len(auth) == 2:\n                # special-case basic HTTP auth\n                auth = HTTPBasicAuth(*auth)\n\n            # Allow auth to make its changes.\n            r = auth(self)\n\n            # Update self to reflect the auth changes.\n            self.__dict__.update(r.__dict__)\n\n            # Recompute Content-Length\n            self.prepare_content_length(self.body)", "output": "Prepares the given HTTP auth data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe_replication_group(name, region=None, key=None, keyid=None,\n                               profile=None, parameter=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    if not conn:\n        return None\n    try:\n        cc = conn.describe_replication_groups(name)\n    except boto.exception.BotoServerError as e:\n        msg = 'Failed to get config for cache cluster {0}.'.format(name)\n        log.error(msg)\n        log.debug(e)\n        return {}\n    ret = odict.OrderedDict()\n    cc = cc['DescribeReplicationGroupsResponse']['DescribeReplicationGroupsResult']\n    cc = cc['ReplicationGroups'][0]\n\n    attrs = ['status', 'description', 'primary_endpoint',\n             'member_clusters', 'replication_group_id',\n             'pending_modified_values', 'primary_cluster_id',\n             'node_groups']\n    for key, val in six.iteritems(cc):\n        _key = boto.utils.pythonize_name(key)\n        if _key == 'status':\n            if val:\n                ret[_key] = val\n            else:\n                ret[_key] = None\n        if _key == 'description':\n            if val:\n                ret[_key] = val\n            else:\n                ret[_key] = None\n        if _key == 'replication_group_id':\n            if val:\n                ret[_key] = val\n            else:\n                ret[_key] = None\n        if _key == 'member_clusters':\n            if val:\n                ret[_key] = val\n            else:\n                ret[_key] = None\n        if _key == 'node_groups':\n            if val:\n                ret[_key] = val\n            else:\n                ret[_key] = None\n        if _key == 'pending_modified_values':\n            if val:\n                ret[_key] = val\n            else:\n                ret[_key] = None\n    return ret", "output": "Get replication group information.\n\n    CLI example::\n\n        salt myminion boto_elasticache.describe_replication_group mygroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_error(self,\n                     request: BaseRequest,\n                     status: int=500,\n                     exc: Optional[BaseException]=None,\n                     message: Optional[str]=None) -> StreamResponse:\n        \"\"\"\"\"\"\n        self.log_exception(\"Error handling request\", exc_info=exc)\n\n        ct = 'text/plain'\n        if status == HTTPStatus.INTERNAL_SERVER_ERROR:\n            title = '{0.value} {0.phrase}'.format(\n                HTTPStatus.INTERNAL_SERVER_ERROR\n            )\n            msg = HTTPStatus.INTERNAL_SERVER_ERROR.description\n            tb = None\n            if self.debug:\n                with suppress(Exception):\n                    tb = traceback.format_exc()\n\n            if 'text/html' in request.headers.get('Accept', ''):\n                if tb:\n                    tb = html_escape(tb)\n                    msg = '<h2>Traceback:</h2>\\n<pre>{}</pre>'.format(tb)\n                message = (\n                    \"<html><head>\"\n                    \"<title>{title}</title>\"\n                    \"</head><body>\\n<h1>{title}</h1>\"\n                    \"\\n{msg}\\n</body></html>\\n\"\n                ).format(title=title, msg=msg)\n                ct = 'text/html'\n            else:\n                if tb:\n                    msg = tb\n                message = title + '\\n\\n' + msg\n\n        resp = Response(status=status, text=message, content_type=ct)\n        resp.force_close()\n\n        # some data already got sent, connection is broken\n        if request.writer.output_size > 0 or self.transport is None:\n            self.force_close()\n\n        return resp", "output": "Handle errors.\n\n        Returns HTTP response with specific status code. Logs additional\n        information. It always closes current connection.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _load_from_file(self, filename):\n    \"\"\"\"\"\"\n    if not tf.gfile.Exists(filename):\n      raise ValueError(\"File %s not found\" % filename)\n    with tf.gfile.Open(filename) as f:\n      self._load_from_file_object(f)", "output": "Load from a vocab file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def popitem(self, fromall=False, last=True):\n        \"\"\"\n        \n        \"\"\"\n        if not self._items:\n            raise KeyError('popitem(): %s is empty' % self.__class__.__name__)\n\n        if fromall:\n            node = self._items[-1 if last else 0]\n            key = node.key\n            return key, self.popvalue(key, last=last)\n        else:\n            key = list(self._map.keys())[-1 if last else 0]\n            return key, self.pop(key)", "output": "Pop and return a key:value item.\n\n        If <fromall> is False, items()[0] is popped if <last> is False or\n        items()[-1] is popped if <last> is True. All remaining items with the\n        same key are removed.\n\n        If <fromall> is True, allitems()[0] is popped if <last> is False or\n        allitems()[-1] is popped if <last> is True. Any remaining items with\n        the same key remain.\n\n        Example:\n          omd = omdict([(1,1), (1,11), (1,111), (2,2), (3,3)])\n          omd.popitem() == (3,3)\n          omd.popitem(fromall=False, last=False) == (1,1)\n          omd.popitem(fromall=False, last=False) == (2,2)\n\n          omd = omdict([(1,1), (1,11), (1,111), (2,2), (3,3)])\n          omd.popitem(fromall=True, last=False) == (1,1)\n          omd.popitem(fromall=True, last=False) == (1,11)\n          omd.popitem(fromall=True, last=True) == (3,3)\n          omd.popitem(fromall=True, last=False) == (1,111)\n\n        Params:\n          fromall: Whether to pop an item from items() (<fromall> is True) or\n            allitems() (<fromall> is False).\n          last: Boolean whether to pop the first item or last item of items()\n            or allitems().\n        Raises: KeyError if the dictionary is empty.\n        Returns: The first or last item from item() or allitem().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def which_pip(allow_global=False):\n    \"\"\"\"\"\"\n\n    location = None\n    if \"VIRTUAL_ENV\" in os.environ:\n        location = os.environ[\"VIRTUAL_ENV\"]\n    if allow_global:\n        if location:\n            pip = which(\"pip\", location=location)\n            if pip:\n                return pip\n\n        for p in (\"pip\", \"pip3\", \"pip2\"):\n            where = system_which(p)\n            if where:\n                return where\n\n    pip = which(\"pip\")\n    if not pip:\n        pip = fallback_which(\"pip\", allow_global=allow_global, location=location)\n    return pip", "output": "Returns the location of virtualenv-installed pip.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _syndic_return(self, load):\n        '''\n        \n        '''\n        # Verify the load\n        if any(key not in load for key in ('return', 'jid', 'id')):\n            return None\n        # if we have a load, save it\n        if 'load' in load:\n            fstr = '{0}.save_load'.format(self.opts['master_job_cache'])\n            self.mminion.returners[fstr](load['jid'], load['load'])\n\n        # Format individual return loads\n        for key, item in six.iteritems(load['return']):\n            ret = {'jid': load['jid'],\n                   'id': key,\n                   'return': item}\n            if 'out' in load:\n                ret['out'] = load['out']\n            self._return(ret)", "output": "Receive a syndic minion return and format it to look like returns from\n        individual minions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def current_hold_price(self):\n        \"\"\"\n        \"\"\"\n        \n        def weights(x):\n            n=len(x)\n            res=1\n            while res>0 or res<0:\n                res=sum(x[:n]['amount'])\n                n=n-1\n            \n            x=x[n+1:]     \n            \n            if sum(x['amount']) != 0:\n                return np.average(\n                    x['price'],\n                    weights=x['amount'],\n                    returned=True\n                )\n            else:\n                return np.nan\n        return self.history_table.set_index(\n            'datetime',\n            drop=False\n        ).sort_index().groupby('code').apply(weights).dropna()", "output": "\u8ba1\u7b97\u76ee\u524d\u6301\u4ed3\u7684\u6210\u672c  \u7528\u4e8e\u6a21\u62df\u76d8\u548c\u5b9e\u76d8\u67e5\u8be2\n\n        Returns:\n            [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_ipsec_site_connection(self, name, ipsecpolicy,\n                                     ikepolicy,\n                                     vpnservice,\n                                     peer_cidrs,\n                                     peer_address,\n                                     peer_id,\n                                     psk,\n                                     admin_state_up=True,\n                                     **kwargs):\n        '''\n        \n        '''\n        ipsecpolicy_id = self._find_ipsecpolicy_id(ipsecpolicy)\n        ikepolicy_id = self._find_ikepolicy_id(ikepolicy)\n        vpnservice_id = self._find_vpnservice_id(vpnservice)\n        body = {'psk': psk,\n                'ipsecpolicy_id': ipsecpolicy_id,\n                'admin_state_up': admin_state_up,\n                'peer_cidrs': [peer_cidrs],\n                'ikepolicy_id': ikepolicy_id,\n                'vpnservice_id': vpnservice_id,\n                'peer_address': peer_address,\n                'peer_id': peer_id,\n                'name': name}\n        if 'initiator' in kwargs:\n            body['initiator'] = kwargs['initiator']\n        if 'mtu' in kwargs:\n            body['mtu'] = kwargs['mtu']\n        if 'dpd_action' in kwargs:\n            body['dpd'] = {'action': kwargs['dpd_action']}\n        if 'dpd_interval' in kwargs:\n            if 'dpd' not in body:\n                body['dpd'] = {}\n            body['dpd']['interval'] = kwargs['dpd_interval']\n        if 'dpd_timeout' in kwargs:\n            if 'dpd' not in body:\n                body['dpd'] = {}\n            body['dpd']['timeout'] = kwargs['dpd_timeout']\n        return self.network_conn.create_ipsec_site_connection(\n            body={'ipsec_site_connection': body})", "output": "Creates a new IPsecSiteConnection", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tf_idf(text):\n    \"\"\"\n    \n    \"\"\"\n    _raise_error_if_not_sarray(text, \"text\")\n\n    if len(text) == 0:\n        return _turicreate.SArray()\n\n    dataset = _turicreate.SFrame({'docs': text})\n    scores = _feature_engineering.TFIDF('docs').fit_transform(dataset)\n\n    return scores['docs']", "output": "Compute the TF-IDF scores for each word in each document. The collection\n    of documents must be in bag-of-words format.\n\n    .. math::\n        \\mbox{TF-IDF}(w, d) = tf(w, d) * log(N / f(w))\n\n    where :math:`tf(w, d)` is the number of times word :math:`w` appeared in\n    document :math:`d`, :math:`f(w)` is the number of documents word :math:`w`\n    appeared in, :math:`N` is the number of documents, and we use the\n    natural logarithm.\n\n    Parameters\n    ----------\n    text : SArray[str | dict | list]\n        Input text data. \n\n    Returns\n    -------\n    out : SArray[dict]\n        The same document corpus where each score has been replaced by the\n        TF-IDF transformation.\n\n    See Also\n    --------\n    count_words, count_ngrams, tokenize,\n\n    References\n    ----------\n    - `Wikipedia - TF-IDF <https://en.wikipedia.org/wiki/TFIDF>`_\n\n    Examples\n    --------\n    .. sourcecode:: python\n\n        >>> import turicreate\n\n        >>> docs = turicreate.SArray('https://static.turi.com/datasets/nips-text')\n        >>> docs_tfidf = turicreate.text_analytics.tf_idf(docs)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_vq_codebook(codebook_size, hidden_size):\n  \"\"\"\"\"\"\n  with tf.variable_scope(\"vq\", reuse=tf.AUTO_REUSE):\n    means = tf.get_variable(\n        name=\"means\",\n        shape=[codebook_size, hidden_size],\n        initializer=tf.uniform_unit_scaling_initializer())\n\n    ema_count = tf.get_variable(\n        name=\"ema_count\",\n        shape=[codebook_size],\n        initializer=tf.constant_initializer(0),\n        trainable=False)\n\n    with tf.colocate_with(means):\n      ema_means = tf.get_variable(\n          name=\"ema_means\",\n          initializer=means.initialized_value(),\n          trainable=False)\n\n  return means, ema_means, ema_count", "output": "Get lookup table for VQ bottleneck.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_model_metadata(mlmodel, model_class, metadata, version=None):\n    \"\"\"\n    \n    \"\"\"\n    info = _get_model_metadata(model_class, metadata, version)\n    mlmodel.user_defined_metadata.update(info)", "output": "Sets user-defined metadata, making sure information all models should have\n    is also available", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_message(\n        self, message: Union[str, bytes], binary: bool = False\n    ) -> \"Future[None]\":\n        \"\"\"\n        \"\"\"\n        return self.protocol.write_message(message, binary=binary)", "output": "Sends a message to the WebSocket server.\n\n        If the stream is closed, raises `WebSocketClosedError`.\n        Returns a `.Future` which can be used for flow control.\n\n        .. versionchanged:: 5.0\n           Exception raised on a closed stream changed from `.StreamClosedError`\n           to `WebSocketClosedError`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_store(name, store, saltenv='base'):\n    '''\n    \n\n    '''\n    ret = {'name': name,\n           'result': True,\n           'comment': '',\n           'changes': {}}\n\n    cert_file = __salt__['cp.cache_file'](name, saltenv)\n    if cert_file is False:\n        ret['result'] = False\n        ret['comment'] += 'Certificate file not found.'\n    else:\n        cert_serial = __salt__['certutil.get_cert_serial'](cert_file)\n        serials = __salt__['certutil.get_stored_cert_serials'](store)\n\n        if cert_serial not in serials:\n            out = __salt__['certutil.add_store'](name, store)\n            if \"successfully\" in out:\n                ret['changes']['added'] = name\n            else:\n                ret['result'] = False\n                ret['comment'] += \"Failed to store certificate {0}\".format(name)\n        else:\n            ret['comment'] += \"{0} already stored.\".format(name)\n\n    return ret", "output": "Store a certificate to the given store\n\n    name\n        The certificate to store, this can use local paths\n        or salt:// paths\n\n    store\n        The store to add the certificate to\n\n    saltenv\n        The salt environment to use, this is ignored if a local\n        path is specified", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expect_list(self, pattern_list, timeout=-1, searchwindowsize=-1,\n                    async_=False, **kw):\n        '''\n        '''\n        if timeout == -1:\n            timeout = self.timeout\n        if 'async' in kw:\n            async_ = kw.pop('async')\n        if kw:\n            raise TypeError(\"Unknown keyword arguments: {}\".format(kw))\n\n        exp = Expecter(self, searcher_re(pattern_list), searchwindowsize)\n        if async_:\n            from ._async import expect_async\n            return expect_async(exp, timeout)\n        else:\n            return exp.expect_loop(timeout)", "output": "This takes a list of compiled regular expressions and returns the\n        index into the pattern_list that matched the child output. The list may\n        also contain EOF or TIMEOUT(which are not compiled regular\n        expressions). This method is similar to the expect() method except that\n        expect_list() does not recompile the pattern list on every call. This\n        may help if you are trying to optimize for speed, otherwise just use\n        the expect() method.  This is called by expect().\n\n\n        Like :meth:`expect`, passing ``async_=True`` will make this return an\n        asyncio coroutine.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_execution_plan(self,\n                          domain,\n                          default_screen,\n                          start_date,\n                          end_date):\n        \"\"\"\n        \n        \"\"\"\n        if self._domain is not GENERIC and self._domain is not domain:\n            raise AssertionError(\n                \"Attempted to compile Pipeline with domain {} to execution \"\n                \"plan with different domain {}.\".format(self._domain, domain)\n            )\n\n        return ExecutionPlan(\n            domain=domain,\n            terms=self._prepare_graph_terms(default_screen),\n            start_date=start_date,\n            end_date=end_date,\n        )", "output": "Compile into an ExecutionPlan.\n\n        Parameters\n        ----------\n        domain : zipline.pipeline.domain.Domain\n            Domain on which the pipeline will be executed.\n        default_screen : zipline.pipeline.term.Term\n            Term to use as a screen if self.screen is None.\n        all_dates : pd.DatetimeIndex\n            A calendar of dates to use to calculate starts and ends for each\n            term.\n        start_date : pd.Timestamp\n            The first date of requested output.\n        end_date : pd.Timestamp\n            The last date of requested output.\n\n        Returns\n        -------\n        graph : zipline.pipeline.graph.ExecutionPlan\n            Graph encoding term dependencies, including metadata about extra\n            row requirements.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_datacenter_id():\n    '''\n    \n    '''\n    datacenter_id = config.get_cloud_config_value(\n        'datacenter_id',\n        get_configured_provider(),\n        __opts__,\n        search_global=False\n    )\n\n    conn = get_conn()\n\n    try:\n        conn.get_datacenter(datacenter_id=datacenter_id)\n    except PBNotFoundError:\n        log.error('Failed to get datacenter: %s', datacenter_id)\n        raise\n\n    return datacenter_id", "output": "Return datacenter ID from provider configuration", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _run_all(cmd, log_lvl=None, log_msg=None, exitcode=0):\n    '''\n    \n    '''\n    res = __salt__['cmd.run_all'](cmd)\n    if res['retcode'] == exitcode:\n        if res['stdout']:\n            return res['stdout']\n        else:\n            return True\n\n    if log_lvl is not None:\n        log.log(LOG[log_lvl], log_msg, res['stderr'])\n    return False", "output": "Simple wrapper around cmd.run_all\n    log_msg can contain {0} for stderr\n    :return: True or stdout, False if retcode wasn't exitcode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shapes(x):\n  \"\"\"\"\"\"\n  def shape(x):\n    try:\n      return x.shape\n    except Exception:  # pylint: disable=broad-except\n      return []\n  return nested_map(x, shape)", "output": "Get a structure of shapes for a structure of nested arrays.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(name, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    topics = get_all_topics(region=region, key=key, keyid=keyid,\n                            profile=profile)\n    if name.startswith('arn:aws:sns:'):\n        return name in list(topics.values())\n    else:\n        return name in list(topics.keys())", "output": "Check to see if an SNS topic exists.\n\n    CLI example::\n\n        salt myminion boto_sns.exists mytopic region=us-east-1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _query_iterator(result, chunksize, columns, index_col=None,\n                        coerce_float=True, parse_dates=None):\n        \"\"\"\"\"\"\n\n        while True:\n            data = result.fetchmany(chunksize)\n            if not data:\n                break\n            else:\n                yield _wrap_result(data, columns, index_col=index_col,\n                                   coerce_float=coerce_float,\n                                   parse_dates=parse_dates)", "output": "Return generator through chunked result set", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Modified(self):\n    \"\"\"\"\"\"\n    try:\n      self._parent_message_weakref._UpdateOneofState(self._field)\n      super(_OneofListener, self).Modified()\n    except ReferenceError:\n      pass", "output": "Also updates the state of the containing oneof in the parent message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def import_key(key_name, public_key_material, region=None, key=None,\n               keyid=None, profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    try:\n        key = conn.import_key_pair(key_name, public_key_material)\n        log.debug(\"the key to return is : %s\", key)\n        return key.fingerprint\n    except boto.exception.BotoServerError as e:\n        log.debug(e)\n        return False", "output": "Imports the public key from an RSA key pair that you created with a third-party tool.\n    Supported formats:\n    - OpenSSH public key format (e.g., the format in ~/.ssh/authorized_keys)\n    - Base64 encoded DER format\n    - SSH public key file format as specified in RFC4716\n    - DSA keys are not supported. Make sure your key generator is set up to create RSA keys.\n    Supported lengths: 1024, 2048, and 4096.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_ec2.import mykey publickey", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_datetimelike_v_numeric(a, b):\n    \"\"\"\n    \n    \"\"\"\n\n    if not hasattr(a, 'dtype'):\n        a = np.asarray(a)\n    if not hasattr(b, 'dtype'):\n        b = np.asarray(b)\n\n    def is_numeric(x):\n        \"\"\"\n        Check if an object has a numeric dtype (i.e. integer or float).\n        \"\"\"\n        return is_integer_dtype(x) or is_float_dtype(x)\n\n    is_datetimelike = needs_i8_conversion\n    return ((is_datetimelike(a) and is_numeric(b)) or\n            (is_datetimelike(b) and is_numeric(a)))", "output": "Check if we are comparing a datetime-like object to a numeric object.\n\n    By \"numeric,\" we mean an object that is either of an int or float dtype.\n\n    Parameters\n    ----------\n    a : array-like, scalar\n        The first object to check.\n    b : array-like, scalar\n        The second object to check.\n\n    Returns\n    -------\n    boolean\n        Whether we return a comparing a datetime-like to a numeric object.\n\n    Examples\n    --------\n    >>> dt = np.datetime64(pd.datetime(2017, 1, 1))\n    >>>\n    >>> is_datetimelike_v_numeric(1, 1)\n    False\n    >>> is_datetimelike_v_numeric(dt, dt)\n    False\n    >>> is_datetimelike_v_numeric(1, dt)\n    True\n    >>> is_datetimelike_v_numeric(dt, 1)  # symmetric check\n    True\n    >>> is_datetimelike_v_numeric(np.array([dt]), 1)\n    True\n    >>> is_datetimelike_v_numeric(np.array([1]), dt)\n    True\n    >>> is_datetimelike_v_numeric(np.array([dt]), np.array([1]))\n    True\n    >>> is_datetimelike_v_numeric(np.array([1]), np.array([2]))\n    False\n    >>> is_datetimelike_v_numeric(np.array([dt]), np.array([dt]))\n    False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(self, type, orig, replace):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlACatalogAdd(self._o, type, orig, replace)\n        return ret", "output": "Add an entry in the catalog, it may overwrite existing but\n           different entries.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def datetime_to_str(self,format=\"%Y-%m-%dT%H:%M:%S%ZP\"):\n        \"\"\"\n        \n\n        \"\"\"\n        if(self.dtype != datetime.datetime):\n            raise TypeError(\"datetime_to_str expects SArray of datetime as input SArray\")\n\n        with cython_context():\n            return SArray(_proxy=self.__proxy__.datetime_to_str(format))", "output": "Create a new SArray with all the values cast to str. The string format is\n        specified by the 'format' parameter.\n\n        Parameters\n        ----------\n        format : str\n            The format to output the string. Default format is \"%Y-%m-%dT%H:%M:%S%ZP\".\n\n        Returns\n        -------\n        out : SArray[str]\n            The SArray converted to the type 'str'.\n\n        Examples\n        --------\n        >>> dt = datetime.datetime(2011, 10, 20, 9, 30, 10, tzinfo=GMT(-5))\n        >>> sa = turicreate.SArray([dt])\n        >>> sa.datetime_to_str(\"%e %b %Y %T %ZP\")\n        dtype: str\n        Rows: 1\n        [20 Oct 2011 09:30:10 GMT-05:00]\n\n        See Also\n        ----------\n        str_to_datetime\n\n        References\n        ----------\n        [1] Boost date time from string conversion guide (http://www.boost.org/doc/libs/1_48_0/doc/html/date_time/date_time_io.html)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def constraint_present(name, constraint_id, constraint_type, constraint_options=None, cibname=None):\n    '''\n    \n    '''\n    return _item_present(name=name,\n                         item='constraint',\n                         item_id=constraint_id,\n                         item_type=constraint_type,\n                         create=None,\n                         extra_args=constraint_options,\n                         cibname=cibname)", "output": "Ensure that a constraint is created\n\n    Should be run on one cluster node only\n    (there may be races)\n    Can only be run on a node with a functional pacemaker/corosync\n\n    name\n        Irrelevant, not used (recommended: {{formulaname}}__constraint_present_{{constraint_id}})\n    constraint_id\n        name for the constraint (try first to create manually to find out the autocreated name)\n    constraint_type\n        constraint type (location, colocation, order)\n    constraint_options\n        options for creating the constraint\n    cibname\n        use a cached CIB-file named like cibname instead of the live CIB\n\n    Example:\n\n    .. code-block:: yaml\n\n        haproxy_pcs__constraint_present_colocation-vip_galera-haproxy-clone-INFINITY:\n            pcs.constraint_present:\n                - constraint_id: colocation-vip_galera-haproxy-clone-INFINITY\n                - constraint_type: colocation\n                - constraint_options:\n                    - 'add'\n                    - 'vip_galera'\n                    - 'with'\n                    - 'haproxy-clone'\n                - cibname: cib_for_haproxy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_dir(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            return S_ISDIR(self.stat().st_mode)\n        except OSError as e:\n            if e.errno not in (ENOENT, ENOTDIR):\n                raise\n            # Path doesn't exist or is a broken symlink\n            # (see https://bitbucket.org/pitrou/pathlib/issue/12/)\n            return False", "output": "Whether this path is a directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _publish_actor_class_to_key(self, key, actor_class_info):\n        \"\"\"\n        \"\"\"\n        # We set the driver ID here because it may not have been available when\n        # the actor class was defined.\n        self._worker.redis_client.hmset(key, actor_class_info)\n        self._worker.redis_client.rpush(\"Exports\", key)", "output": "Push an actor class definition to Redis.\n\n        The is factored out as a separate function because it is also called\n        on cached actor class definitions when a worker connects for the first\n        time.\n\n        Args:\n            key: The key to store the actor class info at.\n            actor_class_info: Information about the actor class.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def checkout(self):\n        '''\n        \n        '''\n        self.winrepo_dirs = {}\n        for repo in self.remotes:\n            cachedir = self.do_checkout(repo)\n            if cachedir is not None:\n                self.winrepo_dirs[repo.id] = cachedir", "output": "Checkout the targeted branches/tags from the winrepo remotes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download(self, layer, force=False):\n        \"\"\"\n        \n        \"\"\"\n        if layer.is_defined_within_template:\n            LOG.info(\"%s is a local Layer in the template\", layer.name)\n            layer.codeuri = resolve_code_path(self.cwd, layer.codeuri)\n            return layer\n\n        # disabling no-member due to https://github.com/PyCQA/pylint/issues/1660\n        layer_path = Path(self.layer_cache).joinpath(layer.name).resolve()  # pylint: disable=no-member\n        is_layer_downloaded = self._is_layer_cached(layer_path)\n        layer.codeuri = str(layer_path)\n\n        if is_layer_downloaded and not force:\n            LOG.info(\"%s is already cached. Skipping download\", layer.arn)\n            return layer\n\n        layer_zip_path = layer.codeuri + '.zip'\n        layer_zip_uri = self._fetch_layer_uri(layer)\n        unzip_from_uri(layer_zip_uri,\n                       layer_zip_path,\n                       unzip_output_dir=layer.codeuri,\n                       progressbar_label='Downloading {}'.format(layer.layer_arn))\n\n        return layer", "output": "Download a given layer to the local cache.\n\n        Parameters\n        ----------\n        layer samcli.commands.local.lib.provider.Layer\n            Layer representing the layer to be downloaded.\n        force bool\n            True to download the layer even if it exists already on the system\n\n        Returns\n        -------\n        Path\n            Path object that represents where the layer is download to", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def instance(cls, *args, **kwargs):\n        \"\"\"  \"\"\"\n        if cls._instance is None:\n            cls._instance = cls(*args, **kwargs)\n            loaded = cls._instance.reload()\n            logging.getLogger('luigi-interface').info('Loaded %r', loaded)\n\n        return cls._instance", "output": "Singleton getter", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def change_option(self, option_name, new_value):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        if option_name == 'dataframe_format':\r\n            assert new_value.startswith('%')\r\n            new_value = new_value[1:]\r\n        self.sig_option_changed.emit(option_name, new_value)", "output": "Change a config option.\r\n\r\n        This function is called if sig_option_changed is received. If the\r\n        option changed is the dataframe format, then the leading '%' character\r\n        is stripped (because it can't be stored in the user config). Then,\r\n        the signal is emitted again, so that the new value is saved in the\r\n        user config.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def human__decision_tree():\n    \"\"\" \n    \"\"\"\n\n    # build data\n    N = 1000000\n    M = 3\n    X = np.zeros((N,M))\n    X.shape\n    y = np.zeros(N)\n    X[0, 0] = 1\n    y[0] = 8\n    X[1, 1] = 1\n    y[1] = 8\n    X[2, 0:2] = 1\n    y[2] = 4\n\n    # fit model\n    xor_model = sklearn.tree.DecisionTreeRegressor(max_depth=2)\n    xor_model.fit(X, y)\n\n    return xor_model", "output": "Decision Tree", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _align_bags(predicted: List[Set[str]], gold: List[Set[str]]) -> List[float]:\n    \"\"\"\n    \n    \"\"\"\n    f1_scores = []\n    for gold_index, gold_item in enumerate(gold):\n        max_f1 = 0.0\n        max_index = None\n        best_alignment: Tuple[Set[str], Set[str]] = (set(), set())\n        if predicted:\n            for pred_index, pred_item in enumerate(predicted):\n                current_f1 = _compute_f1(pred_item, gold_item)\n                if current_f1 >= max_f1:\n                    best_alignment = (gold_item, pred_item)\n                    max_f1 = current_f1\n                    max_index = pred_index\n            match_flag = _match_numbers_if_present(*best_alignment)\n            gold[gold_index] = set()\n            predicted[max_index] = set()\n        else:\n            match_flag = False\n        if match_flag:\n            f1_scores.append(max_f1)\n        else:\n            f1_scores.append(0.0)\n    return f1_scores", "output": "Takes gold and predicted answer sets and first finds a greedy 1-1 alignment\n    between them and gets maximum metric values over all the answers", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sources_add(name, ruby=None, user=None):\n    '''\n    \n    '''\n    ret = {'name': name, 'result': None, 'comment': '', 'changes': {}}\n\n    if name in __salt__['gem.sources_list'](ruby, runas=user):\n        ret['result'] = True\n        ret['comment'] = 'Gem source is already added.'\n        return ret\n    if __opts__['test']:\n        ret['comment'] = 'The gem source {0} would have been added.'.format(name)\n        return ret\n    if __salt__['gem.sources_add'](source_uri=name, ruby=ruby, runas=user):\n        ret['result'] = True\n        ret['changes'][name] = 'Installed'\n        ret['comment'] = 'Gem source was successfully added.'\n    else:\n        ret['result'] = False\n        ret['comment'] = 'Could not add gem source.'\n    return ret", "output": "Make sure that a gem source is added.\n\n    name\n        The URL of the gem source to be added\n\n    ruby: None\n        For RVM or rbenv installations: the ruby version and gemset to target.\n\n    user: None\n        The user under which to run the ``gem`` command\n\n        .. versionadded:: 0.17.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def partition_dataset():\n    \"\"\"  \"\"\"\n    dataset = datasets.MNIST(\n        './data',\n        train=True,\n        download=True,\n        transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307, ), (0.3081, ))\n        ]))\n    size = dist.get_world_size()\n    bsz = 128 / float(size)\n    partition_sizes = [1.0 / size for _ in range(size)]\n    partition = DataPartitioner(dataset, partition_sizes)\n    partition = partition.use(dist.get_rank())\n    train_set = torch.utils.data.DataLoader(\n        partition, batch_size=int(bsz), shuffle=True)\n    return train_set, bsz", "output": "Partitioning MNIST", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_order(self, order_id):\n        \"\"\"\n        \"\"\"\n        if order_id in self.blotter.orders:\n            return self.blotter.orders[order_id].to_api_obj()", "output": "Lookup an order based on the order id returned from one of the\n        order functions.\n\n        Parameters\n        ----------\n        order_id : str\n            The unique identifier for the order.\n\n        Returns\n        -------\n        order : Order\n            The order object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_cfg_pkgs(self):\n        '''\n        \n        '''\n        if self.grains_core.os_data().get('os_family') == 'Debian':\n            return self.__get_cfg_pkgs_dpkg()\n        elif self.grains_core.os_data().get('os_family') in ['Suse', 'redhat']:\n            return self.__get_cfg_pkgs_rpm()\n        else:\n            return dict()", "output": "Package scanner switcher between the platforms.\n\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def discriminator(self, x, is_training, reuse=False):\n    \"\"\"\n    \"\"\"\n    hparams = self.hparams\n    with tf.variable_scope(\n        \"discriminator\", reuse=reuse,\n        initializer=tf.random_normal_initializer(stddev=0.02)):\n      batch_size, height, width = common_layers.shape_list(x)[:3]\n      # Mapping x from [bs, h, w, c] to [bs, 1]\n      net = tf.layers.conv2d(x, 64, (4, 4), strides=(2, 2),\n                             padding=\"SAME\", name=\"d_conv1\")\n      # [bs, h/2, w/2, 64]\n      net = lrelu(net)\n      net = tf.layers.conv2d(net, 128, (4, 4), strides=(2, 2),\n                             padding=\"SAME\", name=\"d_conv2\")\n      # [bs, h/4, w/4, 128]\n      if hparams.discriminator_batchnorm:\n        net = tf.layers.batch_normalization(net, training=is_training,\n                                            momentum=0.999, name=\"d_bn2\")\n      net = lrelu(net)\n      size = height * width\n      net = tf.reshape(net, [batch_size, size * 8])  # [bs, h * w * 8]\n      net = tf.layers.dense(net, 1024, name=\"d_fc3\")  # [bs, 1024]\n      if hparams.discriminator_batchnorm:\n        net = tf.layers.batch_normalization(net, training=is_training,\n                                            momentum=0.999, name=\"d_bn3\")\n      net = lrelu(net)\n      return net", "output": "Discriminator architecture based on InfoGAN.\n\n    Args:\n      x: input images, shape [bs, h, w, channels]\n      is_training: boolean, are we in train or eval model.\n      reuse: boolean, should params be re-used.\n\n    Returns:\n      out_logit: the output logits (before sigmoid).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hours(self):\n        ''' '''\n        for date in self.dates():\n            for hour in xrange(24):\n                yield datetime.datetime.combine(date, datetime.time(hour))", "output": "Same as dates() but returns 24 times more info: one for each hour.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def version():\n    '''\n    \n    '''\n    ret = {}\n    cmd = 'imgadm --version'\n    res = __salt__['cmd.run'](cmd).splitlines()\n    ret = res[0].split()\n    return ret[-1]", "output": "Return imgadm version\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' imgadm.version", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_is_training(is_train):\n    \"\"\"\n    \"\"\"\n    prev = ctypes.c_int()\n    check_call(_LIB.MXAutogradSetIsTraining(\n        ctypes.c_int(is_train), ctypes.byref(prev)))\n    check_call(_LIB.MXAutogradSetIsRecording(\n        ctypes.c_int(is_train), ctypes.byref(prev)))\n    return bool(prev.value)", "output": "Set status to training/not training. When training, graph will be constructed\n    for gradient computation. Operators will also run with ctx.is_train=True. For example,\n    Dropout will drop inputs randomly when is_train=True while simply passing through\n    if is_train=False.\n\n    Parameters\n    ----------\n    is_train: bool\n\n    Returns\n    -------\n    previous state before this set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_distance(F, x):\n    \"\"\"\"\"\"\n    n = x.shape[0]\n\n    square = F.sum(x ** 2.0, axis=1, keepdims=True)\n    distance_square = square + square.transpose() - (2.0 * F.dot(x, x.transpose()))\n\n    # Adding identity to make sqrt work.\n    return F.sqrt(distance_square + F.array(np.identity(n)))", "output": "Helper function for margin-based loss. Return a distance matrix given a matrix.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_dense(net, node, module, builder):\n    \"\"\"\n    \"\"\"\n    input_name, output_name = _get_input_output_name(net, node)\n    has_bias = True\n    name = node['name']\n\n    inputs = node['inputs']\n    args, _ = module.get_params()\n    W = args[_get_node_name(net, inputs[1][0])].asnumpy()\n    if has_bias:\n        Wb = args[_get_node_name(net, inputs[2][0])].asnumpy()\n    else:\n        Wb = None\n    nC, nB = W.shape\n\n    builder.add_inner_product(\n        name=name,\n        W=W,\n        b=Wb,\n        input_channels=nB,\n        output_channels=nC,\n        has_bias=has_bias,\n        input_name=input_name,\n        output_name=output_name\n    )", "output": "Convert a dense layer from mxnet to coreml.\n\n    Parameters\n    ----------\n    net: network\n        A mxnet network object.\n\n    node: layer\n        Node to convert.\n\n    module: module\n        An module for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _decorate_block(self, start, end):\n        \"\"\"\n        \n        \"\"\"\n        color = self._get_scope_highlight_color()\n        draw_order = DRAW_ORDERS.get('codefolding')\n        d = TextDecoration(self.editor.document(), start_line=start,\n                           end_line=end+1, draw_order=draw_order)\n        d.set_background(color)\n        d.set_full_width(True, clear=False)\n        self.editor.decorations.add(d)\n        self._scope_decos.append(d)", "output": "Create a decoration and add it to the editor.\n\n        Args:\n            start (int) start line of the decoration\n            end (int) end line of the decoration", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dump (self):\n        ''''''\n\n        return u''.join ([ u''.join(c) for c in self.w ])", "output": "This returns a copy of the screen as a unicode string. This is similar to\n        __str__/__unicode__ except that lines are not terminated with line\n        feeds.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unschedule(self):\n        \"\"\"\n        \n\n        \"\"\"\n\n        # Run even if events are not defined to remove previously existing ones (thus default to []).\n        events = self.stage_config.get('events', [])\n\n        if not isinstance(events, list): # pragma: no cover\n            print(\"Events must be supplied as a list.\")\n            return\n\n        function_arn = None\n        try:\n            function_response = self.zappa.lambda_client.get_function(FunctionName=self.lambda_name)\n            function_arn = function_response['Configuration']['FunctionArn']\n        except botocore.exceptions.ClientError as e: # pragma: no cover\n            raise ClickException(\"Function does not exist, you should deploy first. Ex: zappa deploy {}. \"\n                  \"Proceeding to unschedule CloudWatch based events.\".format(self.api_stage))\n\n        print(\"Unscheduling..\")\n        self.zappa.unschedule_events(\n            lambda_name=self.lambda_name,\n            lambda_arn=function_arn,\n            events=events,\n            )\n\n        # Remove async task SNS\n        if self.stage_config.get('async_source', None) == 'sns' \\\n           and self.stage_config.get('async_resources', True):\n            removed_arns = self.zappa.remove_async_sns_topic(self.lambda_name)\n            click.echo('SNS Topic removed: %s' % ', '.join(removed_arns))", "output": "Given a a list of scheduled functions,\n        tear down their regular execution.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def facts_refresh():\n    '''\n    \n    '''\n    conn = __proxy__['junos.conn']()\n    ret = {}\n    ret['out'] = True\n    try:\n        conn.facts_refresh()\n    except Exception as exception:\n        ret['message'] = 'Execution failed due to \"{0}\"'.format(exception)\n        ret['out'] = False\n        return ret\n\n    ret['facts'] = __proxy__['junos.get_serialized_facts']()\n\n    try:\n        __salt__['saltutil.sync_grains']()\n    except Exception as exception:\n        log.error('Grains could not be updated due to \"%s\"', exception)\n    return ret", "output": "Reload the facts dictionary from the device. Usually only needed if,\n    the device configuration is changed by some other actor.\n    This function will also refresh the facts stored in the salt grains.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'device_name' junos.facts_refresh", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def combine_comments(comments):\n    '''\n    \n    '''\n    if isinstance(comments, list):\n        for idx in range(len(comments)):\n            if not isinstance(comments[idx], six.string_types):\n                comments[idx] = six.text_type(comments[idx])\n    else:\n        if not isinstance(comments, six.string_types):\n            comments = [six.text_type(comments)]\n        else:\n            comments = [comments]\n    return ' '.join(comments).strip()", "output": "Given a list of comments, or a comment submitted as a string, return a\n    single line of text containing all of the comments.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_argmax(node, **kwargs):\n    \"\"\"\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    axis = int(attrs.get(\"axis\"))\n    keepdims = get_boolean_attribute_value(attrs, \"keepdims\")\n\n    node = onnx.helper.make_node(\n        'ArgMax',\n        inputs=input_nodes,\n        axis=axis,\n        keepdims=keepdims,\n        outputs=[name],\n        name=name\n    )\n    return [node]", "output": "Map MXNet's argmax operator attributes to onnx's ArgMax operator\n    and return the created node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_clean_conf_dir():\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    if sys.platform.startswith(\"win\"):\r\n        current_user = ''\r\n    else:\r\n        current_user = '-' + str(getpass.getuser())\r\n\r\n    conf_dir = osp.join(str(tempfile.gettempdir()),\r\n                        'pytest-spyder{0!s}'.format(current_user),\r\n                        SUBFOLDER)\r\n    return conf_dir", "output": "Return the path to a temp clean configuration dir, for tests and safe mode.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_python_block_output(src, global_dict, local_dict):\n    \"\"\"\n    \"\"\"\n    src = '\\n'.join([l for l in src.split('\\n')\n                     if not l.startswith('%') and not 'plt.show()' in l])\n    ret_status = True\n    err = ''\n    with _string_io() as s:\n        try:\n            exec(src, global_dict, global_dict)\n        except Exception as e:\n            err = str(e)\n            ret_status = False\n    return (ret_status, s.getvalue()+err)", "output": "Evaluate python source codes\n\n    Returns\n    (bool, str):\n      - True if success\n      - output", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def marker_index_document_id(self):\n        \"\"\"\n        \n        \"\"\"\n        params = '%s:%s:%s' % (self.index, self.doc_type, self.update_id)\n        return hashlib.sha1(params.encode('utf-8')).hexdigest()", "output": "Generate an id for the indicator document.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_toolbar(self):\n        \"\"\"\"\"\"\n        self.savefig_btn = create_toolbutton(\n            self, icon=ima.icon('filesave'),\n            tip=_(\"Save Image As...\"),\n            triggered=self.emit_save_figure)\n        self.delfig_btn = create_toolbutton(\n            self, icon=ima.icon('editclear'),\n            tip=_(\"Delete image\"),\n            triggered=self.emit_remove_figure)\n\n        toolbar = QVBoxLayout()\n        toolbar.setContentsMargins(0, 0, 0, 0)\n        toolbar.setSpacing(1)\n        toolbar.addWidget(self.savefig_btn)\n        toolbar.addWidget(self.delfig_btn)\n        toolbar.addStretch(2)\n\n        return toolbar", "output": "Setup the toolbar.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fit(self, data):\n        \"\"\"\n        \n        \"\"\"\n        jmodel = callMLlibFunc(\"fitChiSqSelector\", self.selectorType, self.numTopFeatures,\n                               self.percentile, self.fpr, self.fdr, self.fwe, data)\n        return ChiSqSelectorModel(jmodel)", "output": "Returns a ChiSquared feature selector.\n\n        :param data: an `RDD[LabeledPoint]` containing the labeled dataset\n                     with categorical features. Real-valued features will be\n                     treated as categorical for each distinct value.\n                     Apply feature discretizer before using this function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_parallel_map_providers_query(data, queue=None):\n    '''\n    \n    '''\n    salt.utils.crypt.reinit_crypto()\n\n    cloud = Cloud(data['opts'])\n    try:\n        with salt.utils.context.func_globals_inject(\n            cloud.clouds[data['fun']],\n            __active_provider_name__=':'.join([\n                data['alias'],\n                data['driver']\n            ])\n        ):\n            return (\n                data['alias'],\n                data['driver'],\n                salt.utils.data.simple_types_filter(\n                    cloud.clouds[data['fun']]()\n                )\n            )\n    except Exception as err:\n        log.debug(\n            'Failed to execute \\'%s()\\' while querying for running nodes: %s',\n            data['fun'], err, exc_info_on_loglevel=logging.DEBUG\n        )\n        # Failed to communicate with the provider, don't list any nodes\n        return data['alias'], data['driver'], ()", "output": "This function will be called from another process when building the\n    providers map.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_clip_with_axis(axis, args, kwargs):\n    \"\"\"\n    \n    \"\"\"\n\n    if isinstance(axis, ndarray):\n        args = (axis,) + args\n        axis = None\n\n    validate_clip(args, kwargs)\n    return axis", "output": "If 'NDFrame.clip' is called via the numpy library, the third\n    parameter in its signature is 'out', which can takes an ndarray,\n    so check if the 'axis' parameter is an instance of ndarray, since\n    'axis' itself should either be an integer or None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_record(table, sys_id):\n    '''\n    \n    '''\n    client = _get_client()\n    client.table = table\n    response = client.delete(sys_id)\n    return response", "output": "Delete an existing record\n\n    :param table: The table name, e.g. sys_user\n    :type  table: ``str``\n\n    :param sys_id: The unique ID of the record\n    :type  sys_id: ``str``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion servicenow.delete_record sys_computer 2134566", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_data_shape(data_shape_str):\n    \"\"\"\"\"\"\n    ds = data_shape_str.strip().split(',')\n    if len(ds) == 1:\n        data_shape = (int(ds[0]), int(ds[0]))\n    elif len(ds) == 2:\n        data_shape = (int(ds[0]), int(ds[1]))\n    else:\n        raise ValueError(\"Unexpected data_shape: %s\", data_shape_str)\n    return data_shape", "output": "Parse string to tuple or int", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def new_service(name, restart=True):\n    '''\n    \n    '''\n    out = __mgmt(name, 'service', 'new')\n\n    if restart:\n\n        if out == 'success':\n            return __firewall_cmd('--reload')\n\n    return out", "output": "Add a new service\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' firewalld.new_service my_service\n\n    By default firewalld will be reloaded. However, to avoid reloading\n    you need to specify the restart as False\n\n    .. code-block:: bash\n\n        salt '*' firewalld.new_service my_service False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def index(self, key):\n        \"\"\"\n        \n        \"\"\"\n        if is_iterable(key):\n            return [self.index(subkey) for subkey in key]\n        return self.map[key]", "output": "Get the index of a given entry, raising an IndexError if it's not\n        present.\n\n        `key` can be an iterable of entries that is not a string, in which case\n        this returns a list of indices.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_arguments_for_execution(self, function_name, serialized_args):\n        \"\"\"\n        \"\"\"\n        arguments = []\n        for (i, arg) in enumerate(serialized_args):\n            if isinstance(arg, ObjectID):\n                # get the object from the local object store\n                argument = self.get_object([arg])[0]\n                if isinstance(argument, RayError):\n                    raise argument\n            else:\n                # pass the argument by value\n                argument = arg\n\n            arguments.append(argument)\n        return arguments", "output": "Retrieve the arguments for the remote function.\n\n        This retrieves the values for the arguments to the remote function that\n        were passed in as object IDs. Arguments that were passed by value are\n        not changed. This is called by the worker that is executing the remote\n        function.\n\n        Args:\n            function_name (str): The name of the remote function whose\n                arguments are being retrieved.\n            serialized_args (List): The arguments to the function. These are\n                either strings representing serialized objects passed by value\n                or they are ray.ObjectIDs.\n\n        Returns:\n            The retrieved arguments in addition to the arguments that were\n                passed by value.\n\n        Raises:\n            RayError: This exception is raised if a task that\n                created one of the arguments failed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(self, where=None, start=None, stop=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        if com._all_none(where, start, stop):\n            self._handle.remove_node(self.group, recursive=True)\n            return None\n\n        raise TypeError(\"cannot delete on an abstract storer\")", "output": "support fully deleting the node in its entirety (only) - where\n        specification must be None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def AddRunsFromDirectory(self, path, name=None):\n    \"\"\"\n    \"\"\"\n    logger.info('Starting AddRunsFromDirectory: %s', path)\n    for subdir in io_wrapper.GetLogdirSubdirectories(path):\n      logger.info('Adding run from directory %s', subdir)\n      rpath = os.path.relpath(subdir, path)\n      subname = os.path.join(name, rpath) if name else rpath\n      self.AddRun(subdir, name=subname)\n    logger.info('Done with AddRunsFromDirectory: %s', path)\n    return self", "output": "Load runs from a directory; recursively walks subdirectories.\n\n    If path doesn't exist, no-op. This ensures that it is safe to call\n      `AddRunsFromDirectory` multiple times, even before the directory is made.\n\n    If path is a directory, load event files in the directory (if any exist) and\n      recursively call AddRunsFromDirectory on any subdirectories. This mean you\n      can call AddRunsFromDirectory at the root of a tree of event logs and\n      TensorBoard will load them all.\n\n    If the `EventMultiplexer` is already loaded this will cause\n    the newly created accumulators to `Reload()`.\n    Args:\n      path: A string path to a directory to load runs from.\n      name: Optionally, what name to apply to the runs. If name is provided\n        and the directory contains run subdirectories, the name of each subrun\n        is the concatenation of the parent name and the subdirectory name. If\n        name is provided and the directory contains event files, then a run\n        is added called \"name\" and with the events from the path.\n\n    Raises:\n      ValueError: If the path exists and isn't a directory.\n\n    Returns:\n      The `EventMultiplexer`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def routes(family=None):\n    '''\n    \n    '''\n    if family != 'inet' and family != 'inet6' and family is not None:\n        raise CommandExecutionError('Invalid address family {0}'.format(family))\n\n    if __grains__['kernel'] == 'Linux':\n        if not salt.utils.path.which('netstat'):\n            routes_ = _ip_route_linux()\n        else:\n            routes_ = _netstat_route_linux()\n    elif __grains__['kernel'] == 'SunOS':\n        routes_ = _netstat_route_sunos()\n    elif __grains__['os'] in ['FreeBSD', 'MacOS', 'Darwin']:\n        routes_ = _netstat_route_freebsd()\n    elif __grains__['os'] in ['NetBSD']:\n        routes_ = _netstat_route_netbsd()\n    elif __grains__['os'] in ['OpenBSD']:\n        routes_ = _netstat_route_openbsd()\n    elif __grains__['os'] in ['AIX']:\n        routes_ = _netstat_route_aix()\n    else:\n        raise CommandExecutionError('Not yet supported on this platform')\n\n    if not family:\n        return routes_\n    else:\n        ret = [route for route in routes_ if route['addr_family'] == family]\n        return ret", "output": "Return currently configured routes from routing table\n\n    .. versionchanged:: 2015.8.0\n        Added support for SunOS (Solaris 10, Illumos, SmartOS)\n\n    .. versionchanged:: 2016.11.4\n        Added support for AIX\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' network.routes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def random_size_crop(src, size, min_area=0.25, ratio=(3.0/4.0, 4.0/3.0)):\n    \"\"\"\"\"\"\n    h, w, _ = src.shape\n    area = w*h\n    for _ in range(10):\n        new_area = random.uniform(min_area, 1.0) * area\n        new_ratio = random.uniform(*ratio)\n        new_w = int(new_area*new_ratio)\n        new_h = int(new_area/new_ratio)\n\n        if random.uniform(0., 1.) < 0.5:\n            new_w, new_h = new_h, new_w\n\n        if new_w > w or new_h > h:\n            continue\n\n        x0 = random.randint(0, w - new_w)\n        y0 = random.randint(0, h - new_h)\n\n        out = fixed_crop(src, x0, y0, new_w, new_h, size)\n        return out, (x0, y0, new_w, new_h)\n\n    return random_crop(src, size)", "output": "Randomly crop src with size. Randomize area and aspect ratio", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write(self, filepath, skip_unknown=False):\n        \"\"\"\"\"\"\n        fp = codecs.open(filepath, 'w', encoding='utf-8')\n        try:\n            self.write_file(fp, skip_unknown)\n        finally:\n            fp.close()", "output": "Write the metadata fields to filepath.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _compute(self, arrays, dates, assets, mask):\n        \"\"\"\n        \n        \"\"\"\n        return masked_rankdata_2d(\n            arrays[0],\n            mask,\n            self.inputs[0].missing_value,\n            self._method,\n            self._ascending,\n        )", "output": "For each row in the input, compute a like-shaped array of per-row\n        ranks.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def factory(opts, **kwargs):\n        '''\n        \n        '''\n        import salt.transport.ipc\n        return salt.transport.ipc.IPCMessageServer(opts, **kwargs)", "output": "If we have additional IPC transports other than UXD and TCP, add them here", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hasDefault(self, param):\n        \"\"\"\n        \n        \"\"\"\n        param = self._resolveParam(param)\n        return param in self._defaultParamMap", "output": "Checks whether a param has a default value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def islink(path):\n    '''\n    \n    '''\n    if six.PY3 or not salt.utils.platform.is_windows():\n        return os.path.islink(path)\n\n    if not HAS_WIN32FILE:\n        log.error('Cannot check if %s is a link, missing required modules', path)\n\n    if not _is_reparse_point(path):\n        return False\n\n    # check that it is a symlink reparse point (in case it is something else,\n    # like a mount point)\n    reparse_data = _get_reparse_data(path)\n\n    # sanity check - this should not happen\n    if not reparse_data:\n        # not a reparse point\n        return False\n\n    # REPARSE_DATA_BUFFER structure - see\n    # http://msdn.microsoft.com/en-us/library/ff552012.aspx\n\n    # parse the structure header to work out which type of reparse point this is\n    header_parser = struct.Struct('L')\n    ReparseTag, = header_parser.unpack(reparse_data[:header_parser.size])\n    # http://msdn.microsoft.com/en-us/library/windows/desktop/aa365511.aspx\n    if not ReparseTag & 0xA000FFFF == 0xA000000C:\n        return False\n    else:\n        return True", "output": "Equivalent to os.path.islink()", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_alive(pidfile):\n    '''\n    \n    '''\n    try:\n        with salt.utils.files.fopen(pidfile) as fp_:\n            os.kill(int(fp_.read().strip()), 0)\n        return True\n    except Exception as ex:\n        if os.access(pidfile, os.W_OK) and os.path.isfile(pidfile):\n            os.unlink(pidfile)\n        return False", "output": "Check if PID is still alive.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def IsValidForDescriptor(self, message_descriptor):\n    \"\"\"\"\"\"\n    for path in self.paths:\n      if not _IsValidPath(message_descriptor, path):\n        return False\n    return True", "output": "Checks whether the FieldMask is valid for Message Descriptor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_sequence(cls, *args):\n        \"\"\"\n        \n\n        \"\"\"\n        start = None\n        stop = None\n        # fill with args. This checks for from_sequence(100), from_sequence(10,100)\n        if len(args) == 1:\n            stop = args[0]\n        elif len(args) == 2:\n            start = args[0]\n            stop = args[1]\n\n        if stop is None and start is None:\n            raise TypeError(\"from_sequence expects at least 1 argument. got 0\")\n        elif start is None:\n            return _create_sequential_sarray(stop)\n        else:\n            size = stop - start\n            # this matches the behavior of range\n            # i.e. range(100,10) just returns an empty array\n            if (size < 0):\n                size = 0\n            return _create_sequential_sarray(size, start)", "output": "from_sequence(start=0, stop)\n\n        Create an SArray from sequence\n\n        .. sourcecode:: python\n\n            Construct an SArray of integer values from 0 to 999\n\n            >>> tc.SArray.from_sequence(1000)\n\n            This is equivalent, but more efficient than:\n\n            >>> tc.SArray(range(1000))\n\n            Construct an SArray of integer values from 10 to 999\n\n            >>> tc.SArray.from_sequence(10, 1000)\n\n            This is equivalent, but more efficient than:\n\n            >>> tc.SArray(range(10, 1000))\n\n        Parameters\n        ----------\n        start : int, optional\n            The start of the sequence. The sequence will contain this value.\n\n        stop : int\n          The end of the sequence. The sequence will not contain this value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def end_episode(self):\n        \"\"\"\n        \n        \"\"\"\n        self.training_buffer.reset_local_buffers()\n        for agent_id in self.cumulative_rewards:\n            self.cumulative_rewards[agent_id] = 0\n        for agent_id in self.episode_steps:\n            self.episode_steps[agent_id] = 0\n        if self.use_curiosity:\n            for agent_id in self.intrinsic_rewards:\n                self.intrinsic_rewards[agent_id] = 0", "output": "A signal that the Episode has ended. The buffer must be reset.\n        Get only called when the academy resets.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _process_batch(self, param, dataframe):\n        \"\"\"\n        \"\"\"\n        now = time.time()\n        if param.eval_metric is not None:\n            metrics = dict(param.eval_metric.get_name_value())\n            param.eval_metric.reset()\n        else:\n            metrics = {}\n        # #11504\n        try:\n            speed = self.frequent / (now - self.last_time)\n        except ZeroDivisionError:\n            speed = float('inf')\n        metrics['batches_per_sec'] = speed * self.batch_size\n        metrics['records_per_sec'] = speed\n        metrics['elapsed'] = self.elapsed()\n        metrics['minibatch_count'] = param.nbatch\n        metrics['epoch'] = param.epoch\n        self.append_metrics(metrics, dataframe)\n        self.last_time = now", "output": "Update parameters for selected dataframe after a completed batch\n        Parameters\n        ----------\n        dataframe : pandas.DataFrame\n            Selected dataframe needs to be modified.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_tgt(self, valid, expr, tgt_type, minions=None, expr_form=None):\n        '''\n        \n        '''\n\n        v_minions = set(self.check_minions(valid, 'compound').get('minions', []))\n        if minions is None:\n            _res = self.check_minions(expr, tgt_type)\n            minions = set(_res['minions'])\n        else:\n            minions = set(minions)\n        d_bool = not bool(minions.difference(v_minions))\n        if len(v_minions) == len(minions) and d_bool:\n            return True\n        return d_bool", "output": "Return a Bool. This function returns if the expression sent in is\n        within the scope of the valid expression", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def external_incompatibilities(self):  # type: () -> Generator[Incompatibility]\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(self._cause, ConflictCause):\n            cause = self._cause  # type: ConflictCause\n            for incompatibility in cause.conflict.external_incompatibilities:\n                yield incompatibility\n\n            for incompatibility in cause.other.external_incompatibilities:\n                yield incompatibility\n        else:\n            yield self", "output": "Returns all external incompatibilities in this incompatibility's\n        derivation graph.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def runner(self, fun, **kwargs):\n        '''\n        \n        '''\n        return self.pool.fire_async(self.client_cache['runner'].low, args=(fun, kwargs))", "output": "Wrap RunnerClient for executing :ref:`runner modules <all-salt.runners>`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reduce(self, reduce_fn):\n        \"\"\"\n        \"\"\"\n        op = Operator(\n            _generate_uuid(),\n            OpType.Reduce,\n            \"Sum\",\n            reduce_fn,\n            num_instances=self.env.config.parallelism)\n        return self.__register(op)", "output": "Applies a rolling sum operator to the stream.\n\n        Attributes:\n             sum_attribute_index (int): The index of the attribute to sum\n             (assuming tuple records).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_fp32(learn:Learner):\n    \"\"\n    learn.data.remove_tfm(batch_to_half)\n    for cb in learn.callbacks: \n        if isinstance(cb, MixedPrecision): learn.callbacks.remove(cb)\n    learn.model = learn.model.float()\n    return learn", "output": "Put `learn` back to FP32 precision mode.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(cls, filename):\n        \"\"\"\n        \n        \"\"\"\n        path = os.path.join(SparkFiles.getRootDirectory(), filename)\n        return os.path.abspath(path)", "output": "Get the absolute path of a file added through C{SparkContext.addFile()}.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lstm(name, input, state_c, state_h, kernel_i, kernel_j, kernel_f, kernel_o, bias_i, bias_j, bias_f, bias_o, new_state_c, new_state_h):\n    ''' \n    '''\n\n    ''' No peephole:\n    - it = f(Xt*Wi + Ht_1*Ri + Wbi + Rbi)\n    - ft = f(Xt*Wf + Ht_1*Rf + Wbf + Rbf)\n    - ct = g(Xt*Wc + Ht_1*Rc + Wbc + Rbc)\n    - Ct =   ft . Ct_  + it . ct\n    - ot = f(Xt*Wo + Ht_1*Ro + Wbo + Rbo)\n    - Ht =   ot . h(Ct)\n    '''\n\n    nn = Build(name)\n    inputs = nn.concat(input, state_h)\n\n    i = nn.sigmoid(nn.mad(x=inputs, kernel=kernel_i, bias=bias_i))\n    j =    nn.tanh(nn.mad(inputs, kernel_j, bias_j))\n    f = nn.sigmoid(nn.mad(inputs, kernel_f, bias_f))\n    o = nn.sigmoid(nn.mad(inputs, kernel_o, bias_o))\n\n    # new_c = state_c * f' + i' * j'\n    nn.add(\n        nn.mul(state_c, f), nn.mul(i, j),\n        out=new_state_c)\n\n    # new_h = \n    nn.mul(o, nn.tanh(new_state_c),\n        out=new_state_h)\n\n    return nn.layers", "output": "Full:\n    - it = f(Xt*Wi + Ht_1*Ri + Pi . Ct_1 + Wbi + Rbi)\n    - ft = f(Xt*Wf + Ht_1*Rf + Pf . Ct_1 + Wbf + Rbf)\n    - ct = g(Xt*Wc + Ht_1*Rc + Wbc + Rbc)\n    - Ct =  ft . Ct_1  + it . ct\n    - ot = f(Xt*Wo + Ht_1*Ro + Po . Ct + Wbo + Rbo)\n    - Ht =  ot . h(Ct)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def match_check(self, regex, fun):\n        '''\n        \n        '''\n        vals = []\n        if isinstance(fun, six.string_types):\n            fun = [fun]\n        for func in fun:\n            try:\n                if re.match(regex, func):\n                    vals.append(True)\n                else:\n                    vals.append(False)\n            except Exception:\n                log.error('Invalid regular expression: %s', regex)\n        return vals and all(vals)", "output": "Validate a single regex to function comparison, the function argument\n        can be a list of functions. It is all or nothing for a list of\n        functions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sigmoid_recall_one_hot(logits, labels, weights_fn=None):\n  \"\"\"\n  \"\"\"\n  with tf.variable_scope(\"sigmoid_recall_one_hot\", values=[logits, labels]):\n    del weights_fn\n    num_classes = logits.shape[-1]\n    predictions = tf.nn.sigmoid(logits)\n    predictions = tf.argmax(predictions, -1)\n    predictions = tf.one_hot(predictions, num_classes)\n    _, recall = tf.metrics.recall(labels=labels, predictions=predictions)\n    return recall, tf.constant(1.0)", "output": "Calculate recall for a set, given one-hot labels and logits.\n\n  Predictions are converted to one-hot,\n  as predictions[example][arg-max(example)] = 1\n\n  Args:\n    logits: Tensor of size [batch-size, o=1, p=1, num-classes]\n    labels: Tensor of size [batch-size, o=1, p=1, num-classes]\n    weights_fn: Function that takes in labels and weighs examples (unused)\n  Returns:\n    recall (scalar), weights", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tensorrt_bind(symbol, ctx, all_params, type_dict=None, stype_dict=None, group2ctx=None,\n                  **kwargs):\n    \"\"\"\n    \"\"\"\n    kwargs['shared_buffer'] = all_params\n    return symbol.simple_bind(ctx, type_dict=type_dict, stype_dict=stype_dict,\n                              group2ctx=group2ctx, **kwargs)", "output": "Bind current symbol to get an optimized trt executor.\n\n    Parameters\n    ----------\n    symbol : Symbol\n        The symbol you wish to bind, and optimize with TensorRT.\n\n    ctx : Context\n        The device context the generated executor to run on.\n\n    all_params : Dict of str->ndarray\n        A dictionary of mappings from parameter names to parameter NDArrays.\n\n    type_dict  : Dict of str->numpy.dtype\n        Input type dictionary, name->dtype\n\n    stype_dict  : Dict of str->str\n        Input storage type dictionary, name->storage_type\n\n    group2ctx : Dict of string to mx.Context\n        The dict mapping the `ctx_group` attribute to the context assignment.\n\n    kwargs : Dict of str->shape\n        Input shape dictionary, name->shape\n\n    Returns\n    -------\n    executor : mxnet.Executor\n        An optimized TensorRT executor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_absolute_impute__roc_auc(X, y, model_generator, method_name, num_fcounts=11):\n    \"\"\" \n    \"\"\"\n    return __run_measure(measures.remove_mask, X, y, model_generator, method_name, 0, num_fcounts, sklearn.metrics.roc_auc_score)", "output": "Remove Absolute (impute)\n    xlabel = \"Max fraction of features removed\"\n    ylabel = \"1 - ROC AUC\"\n    transform = \"one_minus\"\n    sort_order = 9", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mean(self, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        nv.validate_window_func('mean', args, kwargs)\n        return self._apply('ewma', **kwargs)", "output": "Exponential weighted moving average.\n\n        Parameters\n        ----------\n        *args, **kwargs\n            Arguments and keyword arguments to be passed into func.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_routes(iface):\n    '''\n    \n    '''\n    path = os.path.join(_RH_NETWORK_SCRIPT_DIR, 'route-{0}'.format(iface))\n    path6 = os.path.join(_RH_NETWORK_SCRIPT_DIR, 'route6-{0}'.format(iface))\n    routes = _read_file(path)\n    routes.extend(_read_file(path6))\n    return routes", "output": "Return the contents of the interface routes script.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ip.get_routes eth0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deterministic_dict(normal_dict):\n  \"\"\"\n  \n  \"\"\"\n  out = OrderedDict()\n  for key in sorted(normal_dict.keys()):\n    out[key] = normal_dict[key]\n  return out", "output": "Returns a version of `normal_dict` whose iteration order is always the same", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compress_histogram(buckets, bps=NORMAL_HISTOGRAM_BPS):\n  \"\"\"\n  \"\"\"\n  # See also: Histogram::Percentile() in core/lib/histogram/histogram.cc\n  buckets = np.array(buckets)\n  if not buckets.size:\n    return [CompressedHistogramValue(b, 0.0) for b in bps]\n  (minmin, maxmax) = (buckets[0][0], buckets[-1][1])\n  counts = buckets[:, 2]\n  right_edges = list(buckets[:, 1])\n  weights = (counts * bps[-1] / (counts.sum() or 1.0)).cumsum()\n\n  result = []\n  bp_index = 0\n  while bp_index < len(bps):\n    i = np.searchsorted(weights, bps[bp_index], side='right')\n    while i < len(weights):\n      cumsum = weights[i]\n      cumsum_prev = weights[i - 1] if i > 0 else 0.0\n      if cumsum == cumsum_prev:  # prevent division-by-zero in `_lerp`\n        i += 1\n        continue\n      if not i or not cumsum_prev:\n        lhs = minmin\n      else:\n        lhs = max(right_edges[i - 1], minmin)\n      rhs = min(right_edges[i], maxmax)\n      weight = _lerp(bps[bp_index], cumsum_prev, cumsum, lhs, rhs)\n      result.append(CompressedHistogramValue(bps[bp_index], weight))\n      bp_index += 1\n      break\n    else:\n      break\n  while bp_index < len(bps):\n    result.append(CompressedHistogramValue(bps[bp_index], maxmax))\n    bp_index += 1\n  return result", "output": "Creates fixed size histogram by adding compression to accumulated state.\n\n  This routine transforms a histogram at a particular step by linearly\n  interpolating its variable number of buckets to represent their cumulative\n  weight at a constant number of compression points. This significantly reduces\n  the size of the histogram and makes it suitable for a two-dimensional area\n  plot where the output of this routine constitutes the ranges for a single x\n  coordinate.\n\n  Args:\n    buckets: A list of buckets, each of which is a 3-tuple of the form\n      `(min, max, count)`.\n    bps: Compression points represented in basis points, 1/100ths of a percent.\n        Defaults to normal distribution.\n\n  Returns:\n    List of values for each basis point.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tabbed_parsing_character_generator(tmp_dir, train):\n  \"\"\"\"\"\"\n  character_vocab = text_encoder.ByteTextEncoder()\n  filename = \"parsing_{0}.pairs\".format(\"train\" if train else \"dev\")\n  pair_filepath = os.path.join(tmp_dir, filename)\n  return text_problems.text2text_generate_encoded(\n      text_problems.text2text_txt_tab_iterator(pair_filepath), character_vocab)", "output": "Generate source and target data from a single file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_path(self):\n        \"\"\"\n        \n        \"\"\"\n        md5_hash = hashlib.md5(self.task_id.encode()).hexdigest()\n        logger.debug('Hash %s corresponds to task %s', md5_hash, self.task_id)\n\n        return os.path.join(self.temp_dir, str(self.unique.value), md5_hash)", "output": "Returns a temporary file path based on a MD5 hash generated with the task's name and its arguments", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(self, doc):\n        \"\"\"\"\"\"\n        array = doc.to_array(self.attrs)\n        if len(array.shape) == 1:\n            array = array.reshape((array.shape[0], 1))\n        self.tokens.append(array)\n        spaces = doc.to_array(SPACY)\n        assert array.shape[0] == spaces.shape[0]\n        spaces = spaces.reshape((spaces.shape[0], 1))\n        self.spaces.append(numpy.asarray(spaces, dtype=bool))\n        self.strings.update(w.text for w in doc)", "output": "Add a doc's annotations to the binder for serialization.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def netmiko_multi_call(*methods, **kwargs):\n    '''\n    \n    '''\n    netmiko_kwargs = netmiko_args()\n    kwargs.update(netmiko_kwargs)\n    return __salt__['netmiko.multi_call'](*methods, **kwargs)", "output": ".. versionadded:: 2019.2.0\n\n    Execute a list of arbitrary Netmiko methods, passing the authentication\n    details from the existing NAPALM connection.\n\n    methods\n        List of dictionaries with the following keys:\n\n        - ``name``: the name of the Netmiko function to invoke.\n        - ``args``: list of arguments to send to the ``name`` method.\n        - ``kwargs``: key-value arguments to send to the ``name`` method.\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm.netmiko_multi_call \"{'name': 'send_command', 'args': ['show version']}\" \"{'name': 'send_command', 'args': ['show interfaces']}\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __within2(value, within=None, errmsg=None, dtype=None):\n    ''''''\n    valid, _value = False, value\n    if dtype:\n        try:\n            _value = dtype(value)  # TODO: this is a bit loose when dtype is a class\n            valid = _value in within\n        except ValueError:\n            pass\n    else:\n        valid = _value in within\n    if errmsg is None:\n        if dtype:\n            typename = getattr(dtype, '__name__',\n                               hasattr(dtype, '__class__')\n                               and getattr(dtype.__class__, 'name', dtype))\n            errmsg = '{0} within \\'{1}\\''.format(typename, within)\n        else:\n            errmsg = 'within \\'{0}\\''.format(within)\n    return (valid, _value, errmsg)", "output": "validate that a value is in ``within`` and optionally a ``dtype``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_cpu(b:ItemsList):\n    \"\"\n    if is_listy(b): return [to_cpu(o) for o in b]\n    return b.cpu() if isinstance(b,Tensor) else b", "output": "Recursively map lists of tensors in `b ` to the cpu.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _merge_dict(self, global_dict, local_dict):\n        \"\"\"\n        \n        \"\"\"\n\n        # Local has higher priority than global. So iterate over local dict and merge into global if keys are overridden\n        global_dict = global_dict.copy()\n\n        for key in local_dict.keys():\n\n            if key in global_dict:\n                # Both local & global contains the same key. Let's do a merge.\n                global_dict[key] = self._do_merge(global_dict[key], local_dict[key])\n\n            else:\n                # Key is not in globals, just in local. Copy it over\n                global_dict[key] = local_dict[key]\n\n        return global_dict", "output": "Merges the two dictionaries together\n\n        :param global_dict: Global dictionary to be merged\n        :param local_dict: Local dictionary to be merged\n        :return: New merged dictionary with values shallow copied", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save(self):\n        \"\"\"\n        \n\n        \"\"\"\n        if self.wechat_id is not None:\n            self.client.update(\n                {'wechat_id': self.wechat_id},\n                {'$set': self.message},\n                upsert=True\n            )\n        else:\n            self.client.update(\n                {\n                    'username': self.username,\n                    'password': self.password\n                },\n                {'$set': self.message},\n                upsert=True\n            )\n\n        # user ==> portfolio \u7684\u5b58\u50a8\n        # account\u7684\u5b58\u50a8\u5728  portfolio.save ==> account.save \u4e2d\n        for portfolio in list(self.portfolio_list.values()):\n            portfolio.save()", "output": "\u5c06QA_USER\u7684\u4fe1\u606f\u5b58\u5165\u6570\u636e\u5e93\n\n        ATTENTION:\n\n        \u5728save user\u7684\u65f6\u5019, \u9700\u8981\u540c\u65f6\u8c03\u7528  user/portfolio/account\u94fe\u6761\u4e0a\u6240\u6709\u7684\u5b9e\u4f8b\u5316\u7c7b \u540c\u65f6save", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_or_edit_conditional_breakpoint(self):\r\n        \"\"\"\"\"\"\r\n        editorstack = self.get_current_editorstack()\r\n        if editorstack is not None:\r\n            self.switch_to_plugin()\r\n            editorstack.set_or_edit_conditional_breakpoint()", "output": "Set/Edit conditional breakpoint", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _resolve_api_id(self):\n        '''\n        \n        '''\n        apis = __salt__['boto_apigateway.describe_apis'](name=self.rest_api_name,\n                                                         description=_Swagger.AWS_API_DESCRIPTION,\n                                                         **self._common_aws_args).get('restapi')\n        if apis:\n            if len(apis) == 1:\n                self.restApiId = apis[0].get('id')\n            else:\n                raise ValueError('Multiple APIs matching given name {0} and '\n                                 'description {1}'.format(self.rest_api_name, self.info_json))", "output": "returns an Api Id that matches the given api_name and the hardcoded _Swagger.AWS_API_DESCRIPTION\n        as the api description", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode(self, data: bytes) -> bytes:\n        \"\"\"\n        \"\"\"\n        if CONTENT_TRANSFER_ENCODING in self.headers:\n            data = self._decode_content_transfer(data)\n        if CONTENT_ENCODING in self.headers:\n            return self._decode_content(data)\n        return data", "output": "Decodes data according the specified Content-Encoding\n        or Content-Transfer-Encoding headers value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_frame_tiny():\n  \"\"\"\"\"\"\n  hparams = next_frame_basic_deterministic()\n  hparams.hidden_size = 32\n  hparams.num_hidden_layers = 1\n  hparams.num_compress_steps = 2\n  hparams.filter_double_steps = 1\n  return hparams", "output": "Tiny for testing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_regularization_penalty(self) -> Union[float, torch.Tensor]:\n        \"\"\"\n        \n        \"\"\"\n        if self._regularizer is None:\n            return 0.0\n        else:\n            return self._regularizer(self)", "output": "Computes the regularization penalty for the model.\n        Returns 0 if the model was not configured to use regularization.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_categorical_option_type(option_name, option_value, possible_values):\n    \"\"\"\n    \n    \"\"\"\n    err_msg = '{0} is not a valid option for {1}. '.format(option_value, option_name)\n    err_msg += ' Expected one of: '.format(possible_values)\n\n    err_msg += ', '.join(map(str, possible_values))\n    if option_value not in possible_values:\n        raise ToolkitError(err_msg)", "output": "Check whether or not the requested option is one of the allowed values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def metric_get(self, project, metric_name):\n        \"\"\"\n        \"\"\"\n        target = \"/projects/%s/metrics/%s\" % (project, metric_name)\n        return self.api_request(method=\"GET\", path=target)", "output": "API call:  retrieve a metric resource.\n\n        See\n        https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.metrics/get\n\n        :type project: str\n        :param project: ID of the project containing the metric.\n\n        :type metric_name: str\n        :param metric_name: the name of the metric\n\n        :rtype: dict\n        :returns: The JSON metric object returned from the API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def special_type(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            return self.__dict__[\"special_type\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'special_type' \".format(self.order_book_id)\n            )", "output": "[str] \u7279\u522b\u5904\u7406\u72b6\u6001\u3002\u2019Normal\u2019 - \u6b63\u5e38\u4e0a\u5e02, \u2018ST\u2019 - ST\u5904\u7406, \u2018StarST\u2019 - *ST\u4ee3\u8868\u8be5\u80a1\u7968\u6b63\u5728\u63a5\u53d7\u9000\u5e02\u8b66\u544a,\n        \u2018PT\u2019 - \u4ee3\u8868\u8be5\u80a1\u7968\u8fde\u7eed3\u5e74\u6536\u5165\u4e3a\u8d1f\uff0c\u5c06\u88ab\u6682\u505c\u4ea4\u6613, \u2018Other\u2019 - \u5176\u4ed6\uff08\u80a1\u7968\u4e13\u7528\uff09", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _raise_error_if_not_sframe(dataset, variable_name=\"SFrame\"):\n    \"\"\"\n    \n    \"\"\"\n    err_msg = \"Input %s is not an SFrame. If it is a Pandas DataFrame,\"\n    err_msg += \" you may use the to_sframe() function to convert it to an SFrame.\"\n\n    if not isinstance(dataset, _SFrame):\n      raise ToolkitError(err_msg % variable_name)", "output": "Check if the input is an SFrame. Provide a proper error\n    message otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_column(self, column):\n        \"\"\" \n        \"\"\"\n        processed_column = DatasetColumn(\n            name=self.get_or_fail(column, 'name'),\n            description=self.get_or_default(column, 'description', ''))\n        if 'type' in column:\n            original_type = column['type'].lower()\n            processed_column.original_type = original_type\n            if (original_type == 'string' or original_type == 'date'\n                    or original_type == 'time' or original_type == 'yearmonth'\n                    or original_type == 'duration'\n                    or original_type == 'geopoint'\n                    or original_type == 'geojson'):\n                processed_column.type = 'string'\n            elif (original_type == 'numeric' or original_type == 'number'\n                  or original_type == 'year'):\n                processed_column.type = 'numeric'\n            elif original_type == 'boolean':\n                processed_column.type = 'boolean'\n            elif original_type == 'datetime':\n                processed_column.type = 'datetime'\n            else:\n                # Possibly extended data type - not going to try to track those\n                # here. Will set the type and let the server handle it.\n                processed_column.type = original_type\n        return processed_column", "output": "process a column, check for the type, and return the processed\n            column\n             Parameters\n            ==========\n            column: a list of values in a column to be processed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _load_and_verify_metadata(self, submission_type):\n    \"\"\"\n    \"\"\"\n    metadata_filename = os.path.join(self._extracted_submission_dir,\n                                     'metadata.json')\n    if not os.path.isfile(metadata_filename):\n      logging.error('metadata.json not found')\n      return None\n    try:\n      with open(metadata_filename, 'r') as f:\n        metadata = json.load(f)\n    except IOError as e:\n      logging.error('Failed to load metadata: %s', e)\n      return None\n    for field_name in REQUIRED_METADATA_JSON_FIELDS:\n      if field_name not in metadata:\n        logging.error('Field %s not found in metadata', field_name)\n        return None\n    # Verify submission type\n    if submission_type != metadata['type']:\n      logging.error('Invalid submission type in metadata, expected \"%s\", '\n                    'actual \"%s\"', submission_type, metadata['type'])\n      return None\n    # Check submission entry point\n    entry_point = metadata['entry_point']\n    if not os.path.isfile(os.path.join(self._extracted_submission_dir,\n                                       entry_point)):\n      logging.error('Entry point not found: %s', entry_point)\n      return None\n    if not entry_point.endswith('.sh'):\n      logging.warning('Entry point is not an .sh script. '\n                      'This is not necessarily a problem, but if submission '\n                      'won''t run double check entry point first: %s',\n                      entry_point)\n    # Metadata verified\n    return metadata", "output": "Loads and verifies metadata.\n\n    Args:\n      submission_type: type of the submission\n\n    Returns:\n      dictionaty with metadata or None if metadata not found or invalid", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_messages(module):\n    \"\"\"\n    \"\"\"\n    answer = collections.OrderedDict()\n    for name in dir(module):\n        candidate = getattr(module, name)\n        if inspect.isclass(candidate) and issubclass(candidate, message.Message):\n            answer[name] = candidate\n    return answer", "output": "Discovers all protobuf Message classes in a given import module.\n\n    Args:\n        module (module): A Python module; :func:`dir` will be run against this\n            module to find Message subclasses.\n\n    Returns:\n        dict[str, google.protobuf.message.Message]: A dictionary with the\n            Message class names as keys, and the Message subclasses themselves\n            as values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_permission(self, authorizer_name, authorizer_lambda_function_arn):\n        \"\"\"\n        \"\"\"\n        rest_api = ApiGatewayRestApi(self.logical_id, depends_on=self.depends_on, attributes=self.resource_attributes)\n        api_id = rest_api.get_runtime_attr('rest_api_id')\n\n        partition = ArnGenerator.get_partition_name()\n        resource = '${__ApiId__}/authorizers/*'\n        source_arn = fnSub(ArnGenerator.generate_arn(partition=partition, service='execute-api', resource=resource),\n                           {\"__ApiId__\": api_id})\n\n        lambda_permission = LambdaPermission(self.logical_id + authorizer_name + 'AuthorizerPermission',\n                                             attributes=self.passthrough_resource_attributes)\n        lambda_permission.Action = 'lambda:invokeFunction'\n        lambda_permission.FunctionName = authorizer_lambda_function_arn\n        lambda_permission.Principal = 'apigateway.amazonaws.com'\n        lambda_permission.SourceArn = source_arn\n\n        return lambda_permission", "output": "Constructs and returns the Lambda Permission resource allowing the Authorizer to invoke the function.\n\n        :returns: the permission resource\n        :rtype: model.lambda_.LambdaPermission", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def export_for_training(self, file_path='./export.json'):\n        \"\"\"\n        \n        \"\"\"\n        import json\n        export = {'conversations': self._generate_export_data()}\n        with open(file_path, 'w+') as jsonfile:\n            json.dump(export, jsonfile, ensure_ascii=False)", "output": "Create a file from the database that can be used to\n        train other chat bots.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_https_port(port=443):\n    '''\n    \n    '''\n    _current = global_settings()\n\n    if _current['Global Settings']['HTTP_PORT']['VALUE'] == port:\n        return True\n\n    _xml = \"\"\"<RIBCL VERSION=\"2.0\">\n                <LOGIN USER_LOGIN=\"adminname\" PASSWORD=\"password\">\n                  <RIB_INFO MODE=\"write\">\n                    <MOD_GLOBAL_SETTINGS>\n                      <HTTPS_PORT value=\"{0}\"/>\n                    </MOD_GLOBAL_SETTINGS>\n                  </RIB_INFO>\n                </LOGIN>\n              </RIBCL>\"\"\".format(port)\n\n    return __execute_cmd('Set_HTTPS_Port', _xml)", "output": "Configure the port HTTPS should listen on\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ilo.set_https_port 4334", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_api_events(self, function):\n        \"\"\"\n        \n        \"\"\"\n\n        if not (function.valid() and\n                isinstance(function.properties, dict) and\n                isinstance(function.properties.get(\"Events\"), dict)\n                ):\n            # Function resource structure is invalid.\n            return {}\n\n        api_events = {}\n        for event_id, event in function.properties[\"Events\"].items():\n\n            if event and isinstance(event, dict) and event.get(\"Type\") == \"Api\":\n                api_events[event_id] = event\n\n        return api_events", "output": "Method to return a dictionary of API Events on the function\n\n        :param SamResource function: Function Resource object\n        :return dict: Dictionary of API events along with any other configuration passed to it.\n            Example: {\n                FooEvent: {Path: \"/foo\", Method: \"post\", RestApiId: blah, MethodSettings: {<something>},\n                            Cors: {<something>}, Auth: {<something>}},\n                BarEvent: {Path: \"/bar\", Method: \"any\", MethodSettings: {<something>}, Cors: {<something>},\n                            Auth: {<something>}}\"\n            }", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compute_nats_and_bits_per_dim(data_dim,\n                                  latent_dim,\n                                  average_reconstruction,\n                                  average_prior):\n  \"\"\"\n  \"\"\"\n  with tf.name_scope(None, default_name=\"compute_nats_per_dim\"):\n    data_dim = tf.cast(data_dim, average_reconstruction.dtype)\n    latent_dim = tf.cast(latent_dim, average_prior.dtype)\n    negative_log_likelihood = data_dim * average_reconstruction\n    negative_log_prior = latent_dim * average_prior\n    negative_elbo = negative_log_likelihood + negative_log_prior\n    nats_per_dim = tf.divide(negative_elbo, data_dim, name=\"nats_per_dim\")\n    bits_per_dim = tf.divide(nats_per_dim, tf.log(2.), name=\"bits_per_dim\")\n    return nats_per_dim, bits_per_dim", "output": "Computes negative ELBO, which is an upper bound on the negative likelihood.\n\n  Args:\n    data_dim: int-like indicating data dimensionality.\n    latent_dim: int-like indicating latent dimensionality.\n    average_reconstruction: Scalar Tensor indicating the reconstruction cost\n      averaged over all data dimensions and any data batches.\n    average_prior: Scalar Tensor indicating the negative log-prior probability\n      averaged over all latent dimensions and any data batches.\n\n  Returns:\n    Tuple of scalar Tensors, representing the nats and bits per data dimension\n    (e.g., subpixels) respectively.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save(self, filename, format='auto'):\n        \"\"\"\n        \n        \"\"\"\n\n        if format is 'auto':\n            if filename.endswith(('.json', '.json.gz')):\n                format = 'json'\n            else:\n                format = 'binary'\n\n        if format not in ['binary', 'json', 'csv']:\n            raise ValueError('Invalid format: %s. Supported formats are: %s'\n                             % (format, ['binary', 'json', 'csv']))\n        with cython_context():\n            self.__proxy__.save_graph(_make_internal_url(filename), format)", "output": "Save the SGraph to disk. If the graph is saved in binary format, the\n        graph can be re-loaded using the :py:func:`load_sgraph` method.\n        Alternatively, the SGraph can be saved in JSON format for a\n        human-readable and portable representation.\n\n        Parameters\n        ----------\n        filename : string\n            Filename to use when saving the file. It can be either a local or\n            remote url.\n\n        format : {'auto', 'binary', 'json'}, optional\n            File format. If not specified, the format is detected automatically\n            based on the filename. Note that JSON format graphs cannot be\n            re-loaded with :py:func:`load_sgraph`.\n\n        See Also\n        --------\n        load_sgraph\n\n        Examples\n        --------\n        >>> g = turicreate.SGraph()\n        >>> g = g.add_vertices([turicreate.Vertex(i) for i in range(5)])\n\n        Save and load in binary format.\n\n        >>> g.save('mygraph')\n        >>> g2 = turicreate.load_sgraph('mygraph')\n\n        Save in JSON format.\n\n        >>> g.save('mygraph.json', format='json')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _glob_to_re(self, pattern):\n        \"\"\"\n        \"\"\"\n        pattern_re = fnmatch.translate(pattern)\n\n        # '?' and '*' in the glob pattern become '.' and '.*' in the RE, which\n        # IMHO is wrong -- '?' and '*' aren't supposed to match slash in Unix,\n        # and by extension they shouldn't match such \"special characters\" under\n        # any OS.  So change all non-escaped dots in the RE to match any\n        # character except the special characters (currently: just os.sep).\n        sep = os.sep\n        if os.sep == '\\\\':\n            # we're using a regex to manipulate a regex, so we need\n            # to escape the backslash twice\n            sep = r'\\\\\\\\'\n        escaped = r'\\1[^%s]' % sep\n        pattern_re = re.sub(r'((?<!\\\\)(\\\\\\\\)*)\\.', escaped, pattern_re)\n        return pattern_re", "output": "Translate a shell-like glob pattern to a regular expression.\n\n        Return a string containing the regex.  Differs from\n        'fnmatch.translate()' in that '*' does not match \"special characters\"\n        (which are platform-specific).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_url(url):\n    \"\"\"\"\"\"\n    logging.debug('reading {url} ...'.format(url=url))\n    token = os.environ.get(\"BOKEH_GITHUB_API_TOKEN\")\n    headers = {}\n    if token:\n        headers['Authorization'] = 'token %s' % token\n    request = Request(url, headers=headers)\n    response = urlopen(request).read()\n    return json.loads(response.decode(\"UTF-8\"))", "output": "Reads given URL as JSON and returns data as loaded python object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _request_one_trial_job(self):\n        \"\"\"\n        \"\"\"\n        if not self.generated_hyper_configs:\n            ret = {\n                'parameter_id': '-1_0_0',\n                'parameter_source': 'algorithm',\n                'parameters': ''\n            }\n            send(CommandType.NoMoreTrialJobs, json_tricks.dumps(ret))\n            return\n        assert self.generated_hyper_configs\n        params = self.generated_hyper_configs.pop()\n        ret = {\n            'parameter_id': params[0],\n            'parameter_source': 'algorithm',\n            'parameters': params[1]\n        }\n        self.parameters[params[0]] = params[1]\n        send(CommandType.NewTrialJob, json_tricks.dumps(ret))\n        self.credit -= 1", "output": "get one trial job, i.e., one hyperparameter configuration.\n\n        If this function is called, Command will be sent by BOHB:\n        a. If there is a parameter need to run, will return \"NewTrialJob\" with a dict:\n        { \n            'parameter_id': id of new hyperparameter\n            'parameter_source': 'algorithm'\n            'parameters': value of new hyperparameter\n        }\n        b. If BOHB don't have parameter waiting, will return \"NoMoreTrialJobs\" with\n        {\n            'parameter_id': '-1_0_0',\n            'parameter_source': 'algorithm',\n            'parameters': ''\n        }", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _incr_counter(self, *args):\n        \"\"\"\n        \n        \"\"\"\n        if len(args) == 2:\n            # backwards compatibility with existing hadoop jobs\n            group_name, count = args\n            print('reporter:counter:%s,%s' % (group_name, count), file=sys.stderr)\n        else:\n            group, name, count = args\n            print('reporter:counter:%s,%s,%s' % (group, name, count), file=sys.stderr)", "output": "Increments a Hadoop counter.\n\n        Note that this seems to be a bit slow, ~1 ms\n\n        Don't overuse this function by updating very frequently.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def connect_container_to_network(container, net_id, **kwargs):\n    '''\n    \n    '''\n    kwargs = __utils__['args.clean_kwargs'](**kwargs)\n    log.debug(\n        'Connecting container \\'%s\\' to network \\'%s\\' with the following '\n        'configuration: %s', container, net_id, kwargs\n    )\n    response = _client_wrapper('connect_container_to_network',\n                               container,\n                               net_id,\n                               **kwargs)\n    log.debug(\n        'Successfully connected container \\'%s\\' to network \\'%s\\'',\n        container, net_id\n    )\n    _clear_context()\n    return True", "output": ".. versionadded:: 2015.8.3\n    .. versionchanged:: 2017.7.0\n        Support for ``ipv4_address`` argument added\n    .. versionchanged:: 2018.3.0\n        All arguments are now passed through to\n        `connect_container_to_network()`_, allowing for any new arguments added\n        to this function to be supported automagically.\n\n    Connect container to network. See the `connect_container_to_network()`_\n    docs for information on supported arguments.\n\n    container\n        Container name or ID\n\n    net_id\n        Network name or ID\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt myminion docker.connect_container_to_network web-1 mynet\n        salt myminion docker.connect_container_to_network web-1 mynet ipv4_address=10.20.0.10\n        salt myminion docker.connect_container_to_network web-1 1f9d2454d0872b68dd9e8744c6e7a4c66b86f10abaccc21e14f7f014f729b2bc", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _gotitem(self, key, ndim, subset=None):\n        \"\"\"\n        \n        \"\"\"\n\n        if ndim == 2:\n            if subset is None:\n                subset = self.obj\n            return DataFrameGroupBy(subset, self.grouper, selection=key,\n                                    grouper=self.grouper,\n                                    exclusions=self.exclusions,\n                                    as_index=self.as_index,\n                                    observed=self.observed)\n        elif ndim == 1:\n            if subset is None:\n                subset = self.obj[key]\n            return SeriesGroupBy(subset, selection=key,\n                                 grouper=self.grouper)\n\n        raise AssertionError(\"invalid ndim for _gotitem\")", "output": "sub-classes to define\n        return a sliced object\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : 1,2\n            requested ndim of result\n        subset : object, default None\n            subset to act on", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _AnyMessageToJsonObject(self, message):\n    \"\"\"\"\"\"\n    if not message.ListFields():\n      return {}\n    # Must print @type first, use OrderedDict instead of {}\n    js = OrderedDict()\n    type_url = message.type_url\n    js['@type'] = type_url\n    sub_message = _CreateMessageFromTypeUrl(type_url)\n    sub_message.ParseFromString(message.value)\n    message_descriptor = sub_message.DESCRIPTOR\n    full_name = message_descriptor.full_name\n    if _IsWrapperMessage(message_descriptor):\n      js['value'] = self._WrapperMessageToJsonObject(sub_message)\n      return js\n    if full_name in _WKTJSONMETHODS:\n      js['value'] = methodcaller(_WKTJSONMETHODS[full_name][0],\n                                 sub_message)(self)\n      return js\n    return self._RegularMessageToJsonObject(sub_message, js)", "output": "Converts Any message according to Proto3 JSON Specification.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate(self,\n                 asset,\n                 amount,\n                 portfolio,\n                 algo_datetime,\n                 algo_current_data):\n        \"\"\"\n        \n        \"\"\"\n        if portfolio.positions[asset].amount + amount < 0:\n            self.handle_violation(asset, amount, algo_datetime)", "output": "Fail if we would hold negative shares of asset after completing this\n        order.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shift(self, periods, fill_value=None):\n        \"\"\"\n        \n        \"\"\"\n        # since categoricals always have ndim == 1, an axis parameter\n        # doesn't make any sense here.\n        codes = self.codes\n        if codes.ndim > 1:\n            raise NotImplementedError(\"Categorical with ndim > 1.\")\n        if np.prod(codes.shape) and (periods != 0):\n            codes = np.roll(codes, ensure_platform_int(periods), axis=0)\n            if isna(fill_value):\n                fill_value = -1\n            elif fill_value in self.categories:\n                fill_value = self.categories.get_loc(fill_value)\n            else:\n                raise ValueError(\"'fill_value={}' is not present \"\n                                 \"in this Categorical's \"\n                                 \"categories\".format(fill_value))\n            if periods > 0:\n                codes[:periods] = fill_value\n            else:\n                codes[periods:] = fill_value\n\n        return self.from_codes(codes, dtype=self.dtype)", "output": "Shift Categorical by desired number of periods.\n\n        Parameters\n        ----------\n        periods : int\n            Number of periods to move, can be positive or negative\n        fill_value : object, optional\n            The scalar value to use for newly introduced missing values.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        shifted : Categorical", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_docstring(self):\r\n        \"\"\"\"\"\"\r\n        line_to_cursor = self.code_editor.get_text('sol', 'cursor')\r\n        if self.is_beginning_triple_quotes(line_to_cursor):\r\n            cursor = self.code_editor.textCursor()\r\n            prev_pos = cursor.position()\r\n\r\n            quote = line_to_cursor[-1]\r\n            docstring_type = CONF.get('editor', 'docstring_type')\r\n            docstring = self._generate_docstring(docstring_type, quote)\r\n\r\n            if docstring:\r\n                self.code_editor.insert_text(docstring)\r\n\r\n                cursor = self.code_editor.textCursor()\r\n                cursor.setPosition(prev_pos, QTextCursor.KeepAnchor)\r\n                cursor.movePosition(QTextCursor.NextBlock)\r\n                cursor.movePosition(QTextCursor.EndOfLine,\r\n                                    QTextCursor.KeepAnchor)\r\n                cursor.clearSelection()\r\n                self.code_editor.setTextCursor(cursor)\r\n                return True\r\n\r\n        return False", "output": "Write docstring to editor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_env(self):\n        '''\n        \n        '''\n        from salt.config import minion_config\n        from salt.grains import core as g_core\n        g_core.__opts__ = minion_config(self.DEFAULT_MINION_CONFIG_PATH)\n        self.grains_core = g_core", "output": "Initialize some Salt environment.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update(self):\n        \"\"\"  \"\"\"\n        aps = []\n        for k, v in self.records.items():\n            recall, prec = self._recall_prec(v, self.counts[k])\n            ap = self._average_precision(recall, prec)\n            aps.append(ap)\n            if self.num is not None and k < (self.num - 1):\n                self.sum_metric[k] = ap\n                self.num_inst[k] = 1\n        if self.num is None:\n            self.num_inst = 1\n            self.sum_metric = np.mean(aps)\n        else:\n            self.num_inst[-1] = 1\n            self.sum_metric[-1] = np.mean(aps)", "output": "update num_inst and sum_metric", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _SkipGroup(buffer, pos, end):\n  \"\"\"\"\"\"\n\n  while 1:\n    (tag_bytes, pos) = ReadTag(buffer, pos)\n    new_pos = SkipField(buffer, pos, end, tag_bytes)\n    if new_pos == -1:\n      return pos\n    pos = new_pos", "output": "Skip sub-group.  Returns the new position.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(cls, path:PathOrStr, cache_name:PathOrStr='tmp', processor:PreProcessor=None, **kwargs):\n        \"\"\n        warn(\"\"\"This method is deprecated and only kept to load data serialized in v1.0.43 or earlier.\n                Use `load_data` for data saved with v1.0.44 or later.\"\"\", DeprecationWarning)\n        cache_path = Path(path)/cache_name\n        vocab = Vocab(pickle.load(open(cache_path/'itos.pkl','rb')))\n        train_ids,train_lbls = np.load(cache_path/f'train_ids.npy'), np.load(cache_path/f'train_lbl.npy')\n        valid_ids,valid_lbls = np.load(cache_path/f'valid_ids.npy'), np.load(cache_path/f'valid_lbl.npy')\n        test_ids = np.load(cache_path/f'test_ids.npy') if os.path.isfile(cache_path/f'test_ids.npy') else None\n        classes = loadtxt_str(cache_path/'classes.txt') if os.path.isfile(cache_path/'classes.txt') else None\n        return cls.from_ids(path, vocab, train_ids, valid_ids, test_ids, train_lbls, valid_lbls, classes, processor, **kwargs)", "output": "Load a `TextDataBunch` from `path/cache_name`. `kwargs` are passed to the dataloader creation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_continuous_query(database, name, query, resample_time=None, coverage_period=None, **client_args):\n    '''\n     '''\n    client = _client(**client_args)\n    full_query = 'CREATE CONTINUOUS QUERY {name} ON {database}'\n    if resample_time:\n        full_query += ' RESAMPLE EVERY {resample_time}'\n    if coverage_period:\n        full_query += ' FOR {coverage_period}'\n    full_query += ' BEGIN {query} END'\n    query = full_query.format(\n        name=name,\n        database=database,\n        query=query,\n        resample_time=resample_time,\n        coverage_period=coverage_period\n    )\n    client.query(query)\n    return True", "output": "Create a continuous query.\n\n    database\n        Name of the database for which the continuous query will be\n        created on.\n\n    name\n        Name of the continuous query to create.\n\n    query\n        The continuous query string.\n\n    resample_time : None\n        Duration between continuous query resampling.\n\n    coverage_period : None\n        Duration specifying time period per sample.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb.create_continuous_query mydb cq_month 'SELECT mean(*) INTO mydb.a_month.:MEASUREMENT FROM mydb.a_week./.*/ GROUP BY time(5m), *'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_zones(self, max_results=None, page_token=None):\n        \"\"\"\n        \"\"\"\n        path = \"/projects/%s/managedZones\" % (self.project,)\n        return page_iterator.HTTPIterator(\n            client=self,\n            api_request=self._connection.api_request,\n            path=path,\n            item_to_value=_item_to_zone,\n            items_key=\"managedZones\",\n            page_token=page_token,\n            max_results=max_results,\n        )", "output": "List zones for the project associated with this client.\n\n        See\n        https://cloud.google.com/dns/api/v1/managedZones/list\n\n        :type max_results: int\n        :param max_results: maximum number of zones to return, If not\n                            passed, defaults to a value set by the API.\n\n        :type page_token: str\n        :param page_token: Optional. If present, return the next batch of\n            zones, using the value, which must correspond to the\n            ``nextPageToken`` value returned in the previous response.\n            Deprecated: use the ``pages`` property of the returned iterator\n            instead of manually passing the token.\n\n        :rtype: :class:`~google.api_core.page_iterator.Iterator`\n        :returns: Iterator of :class:`~google.cloud.dns.zone.ManagedZone`\n                  belonging to this project.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_type_compatible(a, b):\n    \"\"\"\"\"\"\n    is_ts_compat = lambda x: isinstance(x, (Timestamp, DateOffset))\n    is_td_compat = lambda x: isinstance(x, (Timedelta, DateOffset))\n    return ((is_number(a) and is_number(b)) or\n            (is_ts_compat(a) and is_ts_compat(b)) or\n            (is_td_compat(a) and is_td_compat(b)) or\n            com._any_none(a, b))", "output": "helper for interval_range to check type compat of start/end/freq", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_secgroup_id(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The get_secgroup_id function must be called with -f or --function.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    name = kwargs.get('name', None)\n    if name is None:\n        raise SaltCloudSystemExit(\n            'The get_secgroup_id function requires a \\'name\\'.'\n        )\n\n    try:\n        ret = list_security_groups()[name]['id']\n    except KeyError:\n        raise SaltCloudSystemExit(\n            'The security group \\'{0}\\' could not be found.'.format(name)\n        )\n\n    return ret", "output": "Returns a security group's ID from the given security group name.\n\n    .. versionadded:: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f get_secgroup_id opennebula name=my-secgroup-name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_median(temp_list):\n    \"\"\"\n    \"\"\"\n    num = len(temp_list)\n    temp_list.sort()\n    print(temp_list)\n    if num % 2 == 0:\n        median = (temp_list[int(num/2)] + temp_list[int(num/2) - 1]) / 2\n    else:\n        median = temp_list[int(num/2)]\n    return median", "output": "Return median", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def switch(self, gen_mode:bool=None):\n        \"\"\n        self.gen_mode = (not self.gen_mode) if gen_mode is None else gen_mode\n        self.opt.opt = self.opt_gen.opt if self.gen_mode else self.opt_critic.opt\n        self._set_trainable()\n        self.model.switch(gen_mode)\n        self.loss_func.switch(gen_mode)", "output": "Switch the model, if `gen_mode` is provided, in the desired mode.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _valid_choices(cla55: type) -> Dict[str, str]:\n    \"\"\"\n    \n    \"\"\"\n    valid_choices: Dict[str, str] = {}\n\n    if cla55 not in Registrable._registry:\n        raise ValueError(f\"{cla55} is not a known Registrable class\")\n\n    for name, subclass in Registrable._registry[cla55].items():\n        # These wrapper classes need special treatment\n        if isinstance(subclass, (_Seq2SeqWrapper, _Seq2VecWrapper)):\n            subclass = subclass._module_class\n\n        valid_choices[name] = full_name(subclass)\n\n    return valid_choices", "output": "Return a mapping {registered_name -> subclass_name}\n    for the registered subclasses of `cla55`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def begin_training(self, _=tuple(), pipeline=None, sgd=None, **kwargs):\n        \"\"\"\n        \"\"\"\n        if self.model is True:\n            self.model = self.Model(pipeline[0].model.nO)\n            link_vectors_to_models(self.vocab)\n        if sgd is None:\n            sgd = self.create_optimizer()\n        return sgd", "output": "Allocate model, using width from tensorizer in pipeline.\n\n        gold_tuples (iterable): Gold-standard training data.\n        pipeline (list): The pipeline the model is part of.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def putmask(self, mask, value):\n        \"\"\"\n        \n        \"\"\"\n        values = self.values.copy()\n        try:\n            np.putmask(values, mask, self._convert_for_op(value))\n            return self._shallow_copy(values)\n        except (ValueError, TypeError) as err:\n            if is_object_dtype(self):\n                raise err\n\n            # coerces to object\n            return self.astype(object).putmask(mask, value)", "output": "Return a new Index of the values set with the mask.\n\n        See Also\n        --------\n        numpy.ndarray.putmask", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cmdify(self):\n        \"\"\"\n        \"\"\"\n        return \" \".join(\n            itertools.chain(\n                [_quote_if_contains(self.command, r\"[\\s^()]\")],\n                (_quote_if_contains(arg, r\"[\\s^]\") for arg in self.args),\n            )\n        )", "output": "Encode into a cmd-executable string.\n\n        This re-implements CreateProcess's quoting logic to turn a list of\n        arguments into one single string for the shell to interpret.\n\n        * All double quotes are escaped with a backslash.\n        * Existing backslashes before a quote are doubled, so they are all\n          escaped properly.\n        * Backslashes elsewhere are left as-is; cmd will interpret them\n          literally.\n\n        The result is then quoted into a pair of double quotes to be grouped.\n\n        An argument is intentionally not quoted if it does not contain\n        whitespaces. This is done to be compatible with Windows built-in\n        commands that don't work well with quotes, e.g. everything with `echo`,\n        and DOS-style (forward slash) switches.\n\n        The intended use of this function is to pre-process an argument list\n        before passing it into ``subprocess.Popen(..., shell=True)``.\n\n        See also: https://docs.python.org/3/library/subprocess.html#converting-argument-sequence", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def display(port=None, height=None):\n  \"\"\"\n  \"\"\"\n  _display(port=port, height=height, print_message=True, display_handle=None)", "output": "Display a TensorBoard instance already running on this machine.\n\n  Args:\n    port: The port on which the TensorBoard server is listening, as an\n      `int`, or `None` to automatically select the most recently\n      launched TensorBoard.\n    height: The height of the frame into which to render the TensorBoard\n      UI, as an `int` number of pixels, or `None` to use a default value\n      (currently 800).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(self, qname):\n        '''\n        \n        '''\n        try:\n            q = self.exists(qname)\n            if not q:\n                return False\n            queue = self.show(qname)\n            if queue:\n                queue.delete()\n        except pyrax.exceptions as err_msg:\n            log.error('RackSpace API got some problems during deletion: %s',\n                      err_msg)\n            return False\n\n        return True", "output": "Delete an existings RackSpace Queue.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_value_string (f, value_string):\n    \"\"\" \n    \"\"\"\n    assert isinstance(f, Feature)\n    assert isinstance(value_string, basestring)\n    if f.free or value_string in f.values:\n        return\n\n    values = [value_string]\n\n    if f.subfeatures:\n        if not value_string in f.values and \\\n               not value_string in f.subfeatures:\n            values = value_string.split('-')\n\n    # An empty value is allowed for optional features\n    if not values[0] in f.values and \\\n           (values[0] or not f.optional):\n        raise InvalidValue (\"'%s' is not a known value of feature '%s'\\nlegal values: '%s'\" % (values [0], f.name, f.values))\n\n    for v in values [1:]:\n        # this will validate any subfeature values in value-string\n        implied_subfeature(f, v, values[0])", "output": "Checks that value-string is a valid value-string for the given feature.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_dict(self):\n    \"\"\"\n    \"\"\"\n    return {\n        \"node\": [v.to_dict() for v in self.vertices],\n        \"edge\": [e.to_dict() for e in self.edges]\n    }", "output": "Returns a simplified dictionary representing the Graph.\n\n    Returns:\n      A dictionary that can easily be serialized to JSON.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self, back=None):\n        '''\n        \n        '''\n        back = self.backends(back)\n        for fsb in back:\n            fstr = '{0}.update'.format(fsb)\n            if fstr in self.servers:\n                log.debug('Updating %s fileserver cache', fsb)\n                self.servers[fstr]()", "output": "Update all of the enabled fileserver backends which support the update\n        function, or", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def coerce_indexer_dtype(indexer, categories):\n    \"\"\"  \"\"\"\n    length = len(categories)\n    if length < _int8_max:\n        return ensure_int8(indexer)\n    elif length < _int16_max:\n        return ensure_int16(indexer)\n    elif length < _int32_max:\n        return ensure_int32(indexer)\n    return ensure_int64(indexer)", "output": "coerce the indexer input array to the smallest dtype possible", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def layer_width(layer):\n    '''\n    '''\n\n    if is_layer(layer, \"Dense\"):\n        return layer.units\n    if is_layer(layer, \"Conv\"):\n        return layer.filters\n    raise TypeError(\"The layer should be either Dense or Conv layer.\")", "output": "get layer width.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _metric_alarm_to_dict(alarm):\n    '''\n    \n    '''\n    d = odict.OrderedDict()\n    fields = ['name', 'metric', 'namespace', 'statistic', 'comparison',\n              'threshold', 'period', 'evaluation_periods', 'unit',\n              'description', 'dimensions', 'alarm_actions',\n              'insufficient_data_actions', 'ok_actions']\n    for f in fields:\n        if hasattr(alarm, f):\n            d[f] = getattr(alarm, f)\n    return d", "output": "Convert a boto.ec2.cloudwatch.alarm.MetricAlarm into a dict. Convenience\n    for pretty printing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _from_selection(self):\n        \"\"\"\n        \n        \"\"\"\n        # upsampling and PeriodIndex resampling do not work\n        # with selection, this state used to catch and raise an error\n        return (self.groupby is not None and\n                (self.groupby.key is not None or\n                 self.groupby.level is not None))", "output": "Is the resampling from a DataFrame column or MultiIndex level.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathCompareValues(self, inf, strict):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlXPathCompareValues(self._o, inf, strict)\n        return ret", "output": "Implement the compare operation on XPath objects: @arg1 <\n          @arg2    (1, 1, ... @arg1 <= @arg2   (1, 0, ... @arg1 >\n          @arg2    (0, 1, ... @arg1 >= @arg2   (0, 0, ...  When\n          neither object to be compared is a node-set and the\n          operator is <=, <, >=, >, then the objects are compared by\n          converted both objects to numbers and comparing the numbers\n          according to IEEE 754. The < comparison will be true if and\n          only if the first number is less than the second number.\n          The <= comparison will be true if and only if the first\n          number is less than or equal to the second number. The >\n          comparison will be true if and only if the first number is\n          greater than the second number. The >= comparison will be\n          true if and only if the first number is greater than or\n           equal to the second number.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _trim_files(files, trim_output):\n    '''\n    \n    '''\n    count = 100\n    if not isinstance(trim_output, bool):\n        count = trim_output\n\n    if not(isinstance(trim_output, bool) and trim_output is False) and len(files) > count:\n        files = files[:count]\n        files.append(\"List trimmed after {0} files.\".format(count))\n\n    return files", "output": "Trim the file list for output.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_custom_getter_compose(custom_getter):\n  \"\"\"\n  \"\"\"\n  tf.get_variable_scope().set_custom_getter(\n      _compose_custom_getters(tf.get_variable_scope().custom_getter,\n                              custom_getter))", "output": "Set a custom getter in the current variable scope.\n\n  Do not overwrite the existing custom getter - rather compose with it.\n\n  Args:\n    custom_getter: a custom getter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def revnet_164_cifar():\n  \"\"\"\"\"\"\n  hparams = revnet_cifar_base()\n  hparams.bottleneck = True\n  hparams.num_channels = [16, 32, 64]\n  hparams.num_layers_per_block = [8, 8, 8]\n  return hparams", "output": "Tiny hparams suitable for CIFAR/etc.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def detect_and_visualize(self, im_list, root_dir=None, extension=None,\n                             classes=[], thresh=0.6, show_timer=False):\n        \"\"\"\n        \n\n        \"\"\"\n        dets = self.im_detect(im_list, root_dir, extension, show_timer=show_timer)\n        if not isinstance(im_list, list):\n            im_list = [im_list]\n        assert len(dets) == len(im_list)\n        for k, det in enumerate(dets):\n            img = cv2.imread(im_list[k])\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            self.visualize_detection(img, det, classes, thresh)", "output": "wrapper for im_detect and visualize_detection\n\n        Parameters:\n        ----------\n        im_list : list of str or str\n            image path or list of image paths\n        root_dir : str or None\n            directory of input images, optional if image path already\n            has full directory information\n        extension : str or None\n            image extension, eg. \".jpg\", optional\n\n        Returns:\n        ----------", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_col(self, itemsize=None):\n        \"\"\"  \"\"\"\n\n        # validate this column for string truncation (or reset to the max size)\n        if _ensure_decoded(self.kind) == 'string':\n            c = self.col\n            if c is not None:\n                if itemsize is None:\n                    itemsize = self.itemsize\n                if c.itemsize < itemsize:\n                    raise ValueError(\n                        \"Trying to store a string with len [{itemsize}] in \"\n                        \"[{cname}] column but\\nthis column has a limit of \"\n                        \"[{c_itemsize}]!\\nConsider using min_itemsize to \"\n                        \"preset the sizes on these columns\".format(\n                            itemsize=itemsize, cname=self.cname,\n                            c_itemsize=c.itemsize))\n                return c.itemsize\n\n        return None", "output": "validate this column: return the compared against itemsize", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def map_fn_switch(fn, elems, use_map_fn=True, **kwargs):\n  \"\"\"\n  \"\"\"\n  if use_map_fn:\n    return tf.map_fn(fn, elems, **kwargs)\n  elems_unpacked = (tf.unstack(e) for e in elems)\n  out_unpacked = [fn(e) for e in zip(*elems_unpacked)]\n  out = tf.stack(out_unpacked)\n  return out", "output": "Construct the graph with either tf.map_fn or a python for loop.\n\n  This function is mainly for for benchmarking purpose.\n\n  tf.map_fn is dynamic but is much slower than creating a static graph with\n  for loop. However, having a for loop make the graph much longer to build\n  and can consume too much RAM on distributed setting.\n\n  Args:\n    fn (fct): same that tf.map_fn but for now can only return a single tensor\n      value (instead of a tuple of tensor for the general case)\n    elems (tuple): same that tf.map_fn\n    use_map_fn (bool): If True, tf.map_fn is used, if False, for _ in _: is used\n      instead\n    **kwargs: Additional tf.map_fn arguments (ignored if use_map_fn is False)\n\n  Returns:\n    tf.Tensor: the output of tf.map_fn", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _maybe_download_corpus(tmp_dir, vocab_type):\n  \"\"\"\n  \"\"\"\n  filename = os.path.basename(PTB_URL)\n  compressed_filepath = generator_utils.maybe_download(\n      tmp_dir, filename, PTB_URL)\n  ptb_files = []\n  ptb_char_files = []\n\n  with tarfile.open(compressed_filepath, \"r:gz\") as tgz:\n    files = []\n    # Selecting only relevant files.\n    for m in tgz.getmembers():\n      if \"ptb\" in m.name and \".txt\" in m.name:\n        if \"char\" in m.name:\n          ptb_char_files += [m.name]\n        else:\n          ptb_files += [m.name]\n        files += [m]\n\n    tgz.extractall(tmp_dir, members=files)\n\n  if vocab_type == text_problems.VocabType.CHARACTER:\n    return ptb_char_files\n  else:\n    return ptb_files", "output": "Download and unpack the corpus.\n\n  Args:\n    tmp_dir: directory containing dataset.\n    vocab_type: which vocabulary are we using.\n\n  Returns:\n    The list of names of files.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_modules_map(self, path=None):\n        '''\n        \n        '''\n        paths = {}\n        root = ansible.modules.__path__[0]\n        if not path:\n            path = root\n        for p_el in os.listdir(path):\n            p_el_path = os.path.join(path, p_el)\n            if os.path.islink(p_el_path):\n                continue\n            if os.path.isdir(p_el_path):\n                paths.update(self._get_modules_map(p_el_path))\n            else:\n                if (any(p_el.startswith(elm) for elm in ['__', '.']) or\n                        not p_el.endswith('.py') or\n                        p_el in ansible.constants.IGNORE_FILES):\n                    continue\n                p_el_path = p_el_path.replace(root, '').split('.')[0]\n                als_name = p_el_path.replace('.', '').replace('/', '', 1).replace('/', '.')\n                paths[als_name] = p_el_path\n\n        return paths", "output": "Get installed Ansible modules\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rnn(bptt, vocab_size, num_embed, nhid, num_layers, dropout, num_proj, batch_size):\n    \"\"\"  \"\"\"\n    state_names = []\n    data = S.var('data')\n    weight = S.var(\"encoder_weight\", stype='row_sparse')\n    embed = S.sparse.Embedding(data=data, weight=weight, input_dim=vocab_size,\n                               output_dim=num_embed, name='embed', sparse_grad=True)\n    states = []\n    outputs = S.Dropout(embed, p=dropout)\n    for i in range(num_layers):\n        prefix = 'lstmp%d_' % i\n        init_h = S.var(prefix + 'init_h', shape=(batch_size, num_proj), init=mx.init.Zero())\n        init_c = S.var(prefix + 'init_c', shape=(batch_size, nhid), init=mx.init.Zero())\n        state_names += [prefix + 'init_h', prefix + 'init_c']\n        lstmp = mx.gluon.contrib.rnn.LSTMPCell(nhid, num_proj, prefix=prefix)\n        outputs, next_states = lstmp.unroll(bptt, outputs, begin_state=[init_h, init_c], \\\n                                            layout='NTC', merge_outputs=True)\n        outputs = S.Dropout(outputs, p=dropout)\n        states += [S.stop_gradient(s) for s in next_states]\n    outputs = S.reshape(outputs, shape=(-1, num_proj))\n\n    trainable_lstm_args = []\n    for arg in outputs.list_arguments():\n        if 'lstmp' in arg and 'init' not in arg:\n            trainable_lstm_args.append(arg)\n    return outputs, states, trainable_lstm_args, state_names", "output": "word embedding + LSTM Projected", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def newDoc(version):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlNewDoc(version)\n    if ret is None:raise treeError('xmlNewDoc() failed')\n    return xmlDoc(_obj=ret)", "output": "Creates a new XML document", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def IsTensorFlowEventsFile(path):\n  \"\"\"\n  \"\"\"\n  if not path:\n    raise ValueError('Path must be a nonempty string')\n  return 'tfevents' in tf.compat.as_str_any(os.path.basename(path))", "output": "Check the path name to see if it is probably a TF Events file.\n\n  Args:\n    path: A file path to check if it is an event file.\n\n  Raises:\n    ValueError: If the path is an empty string.\n\n  Returns:\n    If path is formatted like a TensorFlowEventsFile.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def notify_owner(func):\n    ''' \n\n    '''\n    def wrapper(self, *args, **kwargs):\n        old = self._saved_copy()\n        result = func(self, *args, **kwargs)\n        self._notify_owners(old)\n        return result\n    wrapper.__doc__ = \"Container method ``%s`` instrumented to notify property owners\" % func.__name__\n    return wrapper", "output": "A decorator for mutating methods of property container classes\n    that notifies owners of the property container about mutating changes.\n\n    Args:\n        func (callable) : the container method to wrap in a notification\n\n    Returns:\n        wrapped method\n\n    Examples:\n\n        A ``__setitem__`` could be wrapped like this:\n\n        .. code-block:: python\n\n            # x[i] = y\n            @notify_owner\n            def __setitem__(self, i, y):\n                return super(PropertyValueDict, self).__setitem__(i, y)\n\n    The returned wrapped method will have a docstring indicating what\n    original method it is wrapping.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cl_int_plot_top_losses(self, k, largest=True, figsize=(12,12), heatmap:bool=True, heatmap_thresh:int=16,\n                            return_fig:bool=None)->Optional[plt.Figure]:\n    \"\"\n    tl_val,tl_idx = self.top_losses(k, largest)\n    classes = self.data.classes\n    cols = math.ceil(math.sqrt(k))\n    rows = math.ceil(k/cols)\n    fig,axes = plt.subplots(rows, cols, figsize=figsize)\n    fig.suptitle('prediction/actual/loss/probability', weight='bold', size=14)\n    for i,idx in enumerate(tl_idx):\n        im,cl = self.data.dl(self.ds_type).dataset[idx]\n        cl = int(cl)\n        im.show(ax=axes.flat[i], title=\n            f'{classes[self.pred_class[idx]]}/{classes[cl]} / {self.losses[idx]:.2f} / {self.probs[idx][cl]:.2f}')\n        if heatmap:\n            xb,_ = self.data.one_item(im, detach=False, denorm=False)\n            m = self.learn.model.eval()\n            with hook_output(m[0]) as hook_a:\n                with hook_output(m[0], grad= True) as hook_g:\n                    preds = m(xb)\n                    preds[0,cl].backward()\n            acts = hook_a.stored[0].cpu()\n            if (acts.shape[-1]*acts.shape[-2]) >= heatmap_thresh:\n                grad = hook_g.stored[0][0].cpu()\n                grad_chan = grad.mean(1).mean(1)\n                mult = F.relu(((acts*grad_chan[...,None,None])).sum(0))\n                sz = list(im.shape[-2:])\n                axes.flat[i].imshow(mult, alpha=0.6, extent=(0,*sz[::-1],0), interpolation='bilinear', cmap='magma')                \n    if ifnone(return_fig, defaults.return_fig): return fig", "output": "Show images in `top_losses` along with their prediction, actual, loss, and probability of actual class.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_parse_dates_arg(parse_dates):\n    \"\"\"\n    \n    \"\"\"\n    msg = (\"Only booleans, lists, and \"\n           \"dictionaries are accepted \"\n           \"for the 'parse_dates' parameter\")\n\n    if parse_dates is not None:\n        if is_scalar(parse_dates):\n            if not lib.is_bool(parse_dates):\n                raise TypeError(msg)\n\n        elif not isinstance(parse_dates, (list, dict)):\n            raise TypeError(msg)\n\n    return parse_dates", "output": "Check whether or not the 'parse_dates' parameter\n    is a non-boolean scalar. Raises a ValueError if\n    that is the case.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fix_input_files_for_variadic_seq(headerDir, sourceDir, timestamp):\n    \"\"\"\"\"\"\n    # Fix files in include/source-directories.\n    files  = glob.glob( os.path.join( headerDir, \"*.hpp\" ) )\n    files += glob.glob( os.path.join( headerDir, \"aux_\", \"*.hpp\" ) )\n    files += glob.glob( os.path.join( sourceDir, \"src\", \"*\" ) )\n    for currentFile in sorted( files ):\n        fix_header_comment( currentFile, timestamp )", "output": "Fixes files used as input when pre-processing MPL-containers in their variadic form.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_cluster_id(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The get_cluster_id function must be called with -f or --function.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    name = kwargs.get('name', None)\n    if name is None:\n        raise SaltCloudSystemExit(\n            'The get_cluster_id function requires a name.'\n        )\n\n    try:\n        ret = list_clusters()[name]['id']\n    except KeyError:\n        raise SaltCloudSystemExit(\n            'The cluster \\'{0}\\' could not be found'.format(name)\n        )\n\n    return ret", "output": "Returns a cluster's ID from the given cluster name.\n\n    .. versionadded:: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f get_cluster_id opennebula name=my-cluster-name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keys_to_typing(value):\n    \"\"\"\"\"\"\n    typing = []\n    for val in value:\n        if isinstance(val, Keys):\n            typing.append(val)\n        elif isinstance(val, int):\n            val = str(val)\n            for i in range(len(val)):\n                typing.append(val[i])\n        else:\n            for i in range(len(val)):\n                typing.append(val[i])\n    return typing", "output": "Processes the values that will be typed in the element.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def universal_transformer_base_range(rhp):\n  \"\"\"\"\"\"\n  # After starting from base, set intervals for some parameters.\n  rhp.set_discrete(\"num_rec_steps\", [6, 8, 10])\n  rhp.set_discrete(\"hidden_size\", [1024, 2048, 4096])\n  rhp.set_discrete(\"filter_size\", [2048, 4096, 8192])\n  rhp.set_discrete(\"num_heads\", [8, 16, 32])\n  rhp.set_discrete(\"transformer_ffn_type\", [\"sepconv\", \"fc\"])\n  rhp.set_float(\"learning_rate\", 0.3, 3.0, scale=rhp.LOG_SCALE)\n  rhp.set_float(\"weight_decay\", 0.0, 2.0)", "output": "Range of hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_error_hint(self, ctx):\n        \"\"\"\n        \"\"\"\n        hint_list = self.opts or [self.human_readable_name]\n        return ' / '.join('\"%s\"' % x for x in hint_list)", "output": "Get a stringified version of the param for use in error messages to\n        indicate which param caused the error.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sliced_shape(self, shapes, i, major_axis):\n        \"\"\"\n        \"\"\"\n        sliced_shapes = []\n        for desc, axis in zip(shapes, major_axis):\n            shape = list(desc.shape)\n            if axis >= 0:\n                shape[axis] = self.slices[i].stop - self.slices[i].start\n            sliced_shapes.append(DataDesc(desc.name, tuple(shape), desc.dtype, desc.layout))\n        return sliced_shapes", "output": "Get the sliced shapes for the i-th executor.\n\n        Parameters\n        ----------\n        shapes : list of (str, tuple)\n            The original (name, shape) pairs.\n        i : int\n            Which executor we are dealing with.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reconnect(force=False, **kwargs):  # pylint: disable=unused-argument\n    '''\n    \n    '''\n    default_ret = {\n        'out': None,\n        'result': True,\n        'comment': 'Already alive.'\n    }\n    if not salt.utils.napalm.is_proxy(__opts__):\n        # regular minion is always alive\n        # otherwise, the user would not be able to execute this command\n        return default_ret\n    is_alive = alive()\n    log.debug('Is alive fetch:')\n    log.debug(is_alive)\n    if not is_alive.get('result', False) or\\\n       not is_alive.get('out', False) or\\\n       not is_alive.get('out', {}).get('is_alive', False) or\\\n       force:  # even if alive, but the user wants to force a restart\n        proxyid = __opts__.get('proxyid') or __opts__.get('id')\n        # close the connection\n        log.info('Closing the NAPALM proxy connection with %s', proxyid)\n        salt.utils.napalm.call(\n            napalm_device,  # pylint: disable=undefined-variable\n            'close',\n            **{}\n        )\n        # and re-open\n        log.info('Re-opening the NAPALM proxy connection with %s', proxyid)\n        salt.utils.napalm.call(\n            napalm_device,  # pylint: disable=undefined-variable\n            'open',\n            **{}\n        )\n        default_ret.update({\n            'comment': 'Connection restarted!'\n        })\n        return default_ret\n    # otherwise, I have nothing to do here:\n    return default_ret", "output": "Reconnect the NAPALM proxy when the connection\n    is dropped by the network device.\n    The connection can be forced to be restarted\n    using the ``force`` argument.\n\n    .. note::\n\n        This function can be used only when running proxy minions.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm.reconnect\n        salt '*' napalm.reconnect force=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def accept(self, timeout=None):\n        \"\"\"\n        \n        \"\"\"\n        self.lock.acquire()\n        try:\n            if len(self.server_accepts) > 0:\n                chan = self.server_accepts.pop(0)\n            else:\n                self.server_accept_cv.wait(timeout)\n                if len(self.server_accepts) > 0:\n                    chan = self.server_accepts.pop(0)\n                else:\n                    # timeout\n                    chan = None\n        finally:\n            self.lock.release()\n        return chan", "output": "Return the next channel opened by the client over this transport, in\n        server mode.  If no channel is opened before the given timeout,\n        ``None`` is returned.\n\n        :param int timeout:\n            seconds to wait for a channel, or ``None`` to wait forever\n        :return: a new `.Channel` opened by the client", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_multiline_pattern(self, regexp, cursor, findflag):\r\n        \"\"\"\"\"\"\r\n        pattern = to_text_string(regexp.pattern())\r\n        text = to_text_string(self.toPlainText())\r\n        try:\r\n            regobj = re.compile(pattern)\r\n        except sre_constants.error:\r\n            return\r\n        if findflag & QTextDocument.FindBackward:\r\n            # Find backward\r\n            offset = min([cursor.selectionEnd(), cursor.selectionStart()])\r\n            text = text[:offset]\r\n            matches = [_m for _m in regobj.finditer(text, 0, offset)]\r\n            if matches:\r\n                match = matches[-1]\r\n            else:\r\n                return\r\n        else:\r\n            # Find forward\r\n            offset = max([cursor.selectionEnd(), cursor.selectionStart()])\r\n            match = regobj.search(text, offset)\r\n        if match:\r\n            pos1, pos2 = match.span()\r\n            fcursor = self.textCursor()\r\n            fcursor.setPosition(pos1)\r\n            fcursor.setPosition(pos2, QTextCursor.KeepAnchor)\r\n            return fcursor", "output": "Reimplement QTextDocument's find method\r\n\r\n        Add support for *multiline* regular expressions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _api_key_patch_add(conn, apiKey, pvlist):\n    '''\n    \n    '''\n    response = conn.update_api_key(apiKey=apiKey,\n                                   patchOperations=_api_key_patchops('add', pvlist))\n    return response", "output": "the add patch operation for a list of (path, value) tuples on an ApiKey resource list path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read_stepfiles_list(path_prefix, path_suffix=\".index\", min_steps=0):\n  \"\"\"\"\"\"\n  stepfiles = []\n  for filename in _try_twice_tf_glob(path_prefix + \"*-[0-9]*\" + path_suffix):\n    basename = filename[:-len(path_suffix)] if path_suffix else filename\n    try:\n      steps = int(basename.rsplit(\"-\")[-1])\n    except ValueError:  # The -[0-9]* part is not an integer.\n      continue\n    if steps < min_steps:\n      continue\n    if not os.path.exists(filename):\n      tf.logging.info(filename + \" was deleted, so skipping it\")\n      continue\n    stepfiles.append(StepFile(basename, os.path.getmtime(filename),\n                              os.path.getctime(filename), steps))\n  return sorted(stepfiles, key=lambda x: -x.steps)", "output": "Return list of StepFiles sorted by step from files at path_prefix.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _prepare_lambada_data(tmp_dir, data_dir, vocab_size, vocab_filename):\n  \"\"\"\n\n  \"\"\"\n\n  if not tf.gfile.Exists(data_dir):\n    tf.gfile.MakeDirs(data_dir)\n\n  file_path = generator_utils.maybe_download(tmp_dir, _TAR, _URL)\n  tar_all = tarfile.open(file_path)\n  tar_all.extractall(tmp_dir)\n  tar_all.close()\n  tar_train = tarfile.open(os.path.join(tmp_dir, \"train-novels.tar\"))\n  tar_train.extractall(tmp_dir)\n  tar_train.close()\n\n  vocab_path = os.path.join(data_dir, vocab_filename)\n  if not tf.gfile.Exists(vocab_path):\n    with tf.gfile.GFile(os.path.join(tmp_dir, _VOCAB), \"r\") as infile:\n      reader = csv.reader(infile, delimiter=\"\\t\")\n      words = [row[0] for row in reader]\n      words = [_UNK] + words[:vocab_size]\n    with tf.gfile.GFile(vocab_path, \"w\") as outfile:\n      outfile.write(\"\\n\".join(words))", "output": "Downloading and preparing the dataset.\n\n  Args:\n    tmp_dir: tem directory\n    data_dir: data directory\n    vocab_size: size of vocabulary\n    vocab_filename: name of vocab file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def as_list(self):\n        \"\"\"\n        \"\"\"\n        if self._dims is None:\n            raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\n        return [dim.value for dim in self._dims]", "output": "Returns a list of integers or `None` for each dimension.\n\n        Returns:\n          A list of integers or `None` for each dimension.\n\n        Raises:\n          ValueError: If `self` is an unknown shape with an unknown rank.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def random_shift(image, wsr=0.1, hsr=0.1):\n  \"\"\"\n  \"\"\"\n  height, width, _ = common_layers.shape_list(image)\n  width_range, height_range = wsr*width, hsr*height\n  height_translations = tf.random_uniform((1,), -height_range, height_range)\n  width_translations = tf.random_uniform((1,), -width_range, width_range)\n  translations = tf.concat((height_translations, width_translations), axis=0)\n  return tf.contrib.image.translate(image, translations=translations)", "output": "Apply random horizontal and vertical shift to images.\n\n  This is the default data-augmentation strategy used on CIFAR in Glow.\n\n  Args:\n    image: a 3-D Tensor\n    wsr: Width shift range, as a float fraction of the width.\n    hsr: Height shift range, as a float fraction of the width.\n  Returns:\n    images: images translated by the provided wsr and hsr.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reduce(self, start=0, end=None):\n        \"\"\"\n        \"\"\"\n        if end is None:\n            end = self._capacity - 1\n        if end < 0:\n            end += self._capacity\n        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)", "output": "Returns result of applying `self.operation`\n        to a contiguous subsequence of the array.\n\n          self.operation(\n              arr[start], operation(arr[start+1], operation(... arr[end])))\n\n        Parameters\n        ----------\n        start: int\n          beginning of the subsequence\n        end: int\n          end of the subsequences\n\n        Returns\n        -------\n        reduced: obj\n          result of reducing self.operation over the specified range of array\n          elements.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_max_gradient(self)->None:\n        \"\"\n        max_gradient = max(x.data.max() for x in self.gradients)\n        self._add_gradient_scalar('max_gradient', scalar_value=max_gradient)", "output": "Writes the maximum of the gradients to Tensorboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_ax_freq(ax):\n    \"\"\"\n    \n    \"\"\"\n    ax_freq = getattr(ax, 'freq', None)\n    if ax_freq is None:\n        # check for left/right ax in case of secondary yaxis\n        if hasattr(ax, 'left_ax'):\n            ax_freq = getattr(ax.left_ax, 'freq', None)\n        elif hasattr(ax, 'right_ax'):\n            ax_freq = getattr(ax.right_ax, 'freq', None)\n    if ax_freq is None:\n        # check if a shared ax (sharex/twinx) has already freq set\n        shared_axes = ax.get_shared_x_axes().get_siblings(ax)\n        if len(shared_axes) > 1:\n            for shared_ax in shared_axes:\n                ax_freq = getattr(shared_ax, 'freq', None)\n                if ax_freq is not None:\n                    break\n    return ax_freq", "output": "Get the freq attribute of the ax object if set.\n    Also checks shared axes (eg when using secondary yaxis, sharex=True\n    or twinx)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_libcloud_version(reqver=LIBCLOUD_MINIMAL_VERSION, why=None):\n    '''\n    \n    '''\n    if not HAS_LIBCLOUD:\n        return False\n\n    if not isinstance(reqver, (list, tuple)):\n        raise RuntimeError(\n            '\\'reqver\\' needs to passed as a tuple or list, i.e., (0, 14, 0)'\n        )\n    try:\n        import libcloud  # pylint: disable=redefined-outer-name\n    except ImportError:\n        raise ImportError(\n            'salt-cloud requires >= libcloud {0} which is not installed'.format(\n                '.'.join([six.text_type(num) for num in reqver])\n            )\n        )\n\n    if LIBCLOUD_VERSION_INFO >= reqver:\n        return libcloud.__version__\n\n    errormsg = 'Your version of libcloud is {0}. '.format(libcloud.__version__)\n    errormsg += 'salt-cloud requires >= libcloud {0}'.format(\n        '.'.join([six.text_type(num) for num in reqver])\n    )\n    if why:\n        errormsg += ' for {0}'.format(why)\n    errormsg += '. Please upgrade.'\n    raise ImportError(errormsg)", "output": "Compare different libcloud versions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_frequency(cls, index, freq, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        if is_period_dtype(cls):\n            # Frequency validation is not meaningful for Period Array/Index\n            return None\n\n        inferred = index.inferred_freq\n        if index.size == 0 or inferred == freq.freqstr:\n            return None\n\n        try:\n            on_freq = cls._generate_range(start=index[0], end=None,\n                                          periods=len(index), freq=freq,\n                                          **kwargs)\n            if not np.array_equal(index.asi8, on_freq.asi8):\n                raise ValueError\n        except ValueError as e:\n            if \"non-fixed\" in str(e):\n                # non-fixed frequencies are not meaningful for timedelta64;\n                #  we retain that error message\n                raise e\n            # GH#11587 the main way this is reached is if the `np.array_equal`\n            #  check above is False.  This can also be reached if index[0]\n            #  is `NaT`, in which case the call to `cls._generate_range` will\n            #  raise a ValueError, which we re-raise with a more targeted\n            #  message.\n            raise ValueError('Inferred frequency {infer} from passed values '\n                             'does not conform to passed frequency {passed}'\n                             .format(infer=inferred, passed=freq.freqstr))", "output": "Validate that a frequency is compatible with the values of a given\n        Datetime Array/Index or Timedelta Array/Index\n\n        Parameters\n        ----------\n        index : DatetimeIndex or TimedeltaIndex\n            The index on which to determine if the given frequency is valid\n        freq : DateOffset\n            The frequency to validate", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self, job):\n        \"\"\"\n        \"\"\"\n        self.cur.execute('''UPDATE jobs\n            SET last_run=?,next_run=?,last_run_result=? WHERE hash=?''', (\n            job[\"last-run\"], job[\"next-run\"], job[\"last-run-result\"], job[\"id\"]))", "output": "Update last_run, next_run, and last_run_result for an existing job.\n        :param dict job: The job dictionary\n        :returns: True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _crossover(candidate):\n        \"\"\"\n        \"\"\"\n        sample_index1 = np.random.choice(len(candidate))\n        sample_index2 = np.random.choice(len(candidate))\n        sample_1 = candidate[sample_index1]\n        sample_2 = candidate[sample_index2]\n        cross_index = int(len(sample_1) * np.random.uniform(low=0.3, high=0.7))\n        logger.info(\n            LOGGING_PREFIX +\n            \"Perform crossover between %sth and %sth at index=%s\",\n            sample_index1, sample_index2, cross_index)\n\n        next_gen = []\n        for i in range(len(sample_1)):\n            if i > cross_index:\n                next_gen.append(sample_2[i])\n            else:\n                next_gen.append(sample_1[i])\n        return next_gen", "output": "Perform crossover action to candidates.\n\n        For example, new gene = 60% sample_1 + 40% sample_2.\n\n        Args:\n            candidate: List of candidate genes (encodings).\n\n        Examples:\n            >>> # Genes that represent 3 parameters\n            >>> gene1 = np.array([[0, 0, 1], [0, 1], [1, 0]])\n            >>> gene2 = np.array([[0, 1, 0], [1, 0], [0, 1]])\n            >>> new_gene = _crossover([gene1, gene2])\n            >>> # new_gene could be the first [n=1] parameters of\n            >>> # gene1 + the rest of gene2\n            >>> # in which case:\n            >>> #   new_gene[0] = gene1[0]\n            >>> #   new_gene[1] = gene2[1]\n            >>> #   new_gene[2] = gene1[1]\n\n        Returns:\n            New gene (encoding)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zoom_blur(x, severity=1):\n  \"\"\"\n  \"\"\"\n  c = [\n      np.arange(1, 1.11, 0.01),\n      np.arange(1, 1.16, 0.01),\n      np.arange(1, 1.21, 0.02),\n      np.arange(1, 1.26, 0.02),\n      np.arange(1, 1.31, 0.03)\n  ][severity - 1]\n  x = (np.array(x) / 255.).astype(np.float32)\n  out = np.zeros_like(x)\n  for zoom_factor in c:\n    out += clipped_zoom(x, zoom_factor)\n  x = (x + out) / (len(c) + 1)\n  x_clip = np.clip(x, 0, 1) * 255\n  return around_and_astype(x_clip)", "output": "Zoom blurring to images.\n\n  Applying zoom blurring to images by zooming the central part of the images.\n\n  Args:\n    x: numpy array, uncorrupted image, assumed to have uint8 pixel in [0,255].\n    severity: integer, severity of corruption.\n\n  Returns:\n    numpy array, image with uint8 pixels in [0,255]. Applied zoom blur.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_stoplimit_prices(price, label):\n    \"\"\"\n    \n    \"\"\"\n    try:\n        if not isfinite(price):\n            raise BadOrderParameters(\n                msg=\"Attempted to place an order with a {} price \"\n                    \"of {}.\".format(label, price)\n            )\n    # This catches arbitrary objects\n    except TypeError:\n        raise BadOrderParameters(\n            msg=\"Attempted to place an order with a {} price \"\n                \"of {}.\".format(label, type(price))\n        )\n\n    if price < 0:\n        raise BadOrderParameters(\n            msg=\"Can't place a {} order with a negative price.\".format(label)\n        )", "output": "Check to make sure the stop/limit prices are reasonable and raise\n    a BadOrderParameters exception if not.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_gettext_patterns():\r\n    \"\"\r\n    kwstr = 'msgid msgstr'\r\n    kw = r\"\\b\" + any(\"keyword\", kwstr.split()) + r\"\\b\"\r\n    fuzzy = any(\"builtin\", [r\"#,[^\\n]*\"])\r\n    links = any(\"normal\", [r\"#:[^\\n]*\"])\r\n    comment = any(\"comment\", [r\"#[^\\n]*\"])\r\n    number = any(\"number\",\r\n                 [r\"\\b[+-]?[0-9]+[lL]?\\b\",\r\n                  r\"\\b[+-]?0[xX][0-9A-Fa-f]+[lL]?\\b\",\r\n                  r\"\\b[+-]?[0-9]+(?:\\.[0-9]+)?(?:[eE][+-]?[0-9]+)?\\b\"])\r\n    sqstring = r\"(\\b[rRuU])?'[^'\\\\\\n]*(\\\\.[^'\\\\\\n]*)*'?\"\r\n    dqstring = r'(\\b[rRuU])?\"[^\"\\\\\\n]*(\\\\.[^\"\\\\\\n]*)*\"?'\r\n    string = any(\"string\", [sqstring, dqstring])\r\n    return \"|\".join([kw, string, number, fuzzy, links, comment,\r\n                     any(\"SYNC\", [r\"\\n\"])])", "output": "Strongly inspired from idlelib.ColorDelegator.make_pat", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def random_deinterleave(text, separator_symbol=\"X\"):\n  \"\"\"\n  \"\"\"\n  words = text.strip().split(\" \")\n  n = len(words)\n  if n <= 1:\n    return text, \"\"\n  cut = [False] * n\n  cut[0] = True\n  num_cuts = int(math.exp(random.uniform(0, math.log(n))))\n  for _ in range(num_cuts):\n    cut[random.randint(1, n -1)] = True\n  out = [[], []]\n  part = random.randint(0, 1)\n  for i in range(n):\n    if cut[i]:\n      out[part].append(separator_symbol)\n      part = 1 - part\n    out[part].append(words[i])\n  return \" \".join(out[0]), \" \".join(out[1])", "output": "Create a fill-in-the-blanks training example from text.\n\n  Split on spaces, then cut into segments at random points.  Alternate segments\n  are assigned to the two output strings. separator_symbol separates segments\n  within each of the outputs.\n\n  example:\n    text=\"The quick brown fox jumps over the lazy dog.\"\n    returns: (\"X quick brown X the lazy X\", \"The X fox jumps over X dog.\")\n\n  The two outputs can also be reversed to yield an instance of the same problem.\n\n  Args:\n    text: a string\n    separator_symbol: a string\n  Returns:\n    a pair of strings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_line(self, line: str) -> None:\n        \"\"\"\n        \"\"\"\n        if line[0].isspace():\n            # continuation of a multi-line header\n            if self._last_key is None:\n                raise HTTPInputError(\"first header line cannot start with whitespace\")\n            new_part = \" \" + line.lstrip()\n            self._as_list[self._last_key][-1] += new_part\n            self._dict[self._last_key] += new_part\n        else:\n            try:\n                name, value = line.split(\":\", 1)\n            except ValueError:\n                raise HTTPInputError(\"no colon in header line\")\n            self.add(name, value.strip())", "output": "Updates the dictionary with a single header line.\n\n        >>> h = HTTPHeaders()\n        >>> h.parse_line(\"Content-Type: text/html\")\n        >>> h.get('content-type')\n        'text/html'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __restore_selection(self, start_pos, end_pos):\r\n        \"\"\"\"\"\"\r\n        cursor = self.textCursor()\r\n        cursor.setPosition(start_pos)\r\n        cursor.setPosition(end_pos, QTextCursor.KeepAnchor)\r\n        self.setTextCursor(cursor)", "output": "Restore cursor selection from position bounds", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        \"\"\"\"\"\"\n        random.seed(self.seed)\n        np.random.seed(self.np_seed)\n        if not isinstance(self, multiprocessing.Process):\n            # Calling mxnet methods in a subprocess will raise an exception if\n            # mxnet is built with GPU support\n            # https://github.com/apache/incubator-mxnet/issues/4659\n            mx.random.seed(self.mx_seed)\n\n        # Startup - Master waits for this\n        try:\n            stream_iter = iter(self.stream)\n            self._errorq.put(None)\n        except Exception as e:  # pylint: disable=broad-except\n            tb = traceback.format_exc()\n            self._errorq.put((e, tb))\n\n        # Async work\n        while True:\n            try:  # Check control queue\n                c = self._controlq.get(False)\n                if c is None:\n                    break\n                else:\n                    raise RuntimeError('Got unexpected control code {}'.format(repr(c)))\n            except queue.Empty:\n                pass\n            except RuntimeError as e:\n                tb = traceback.format_exc()\n                self._errorq.put((e, tb))\n                self._dataq.put(None)\n\n            try:\n                data = next(stream_iter)\n                error = None\n            except Exception as e:  # pylint: disable=broad-except\n                tb = traceback.format_exc()\n                error = (e, tb)\n                data = None\n            finally:\n                self._errorq.put(error)\n                self._dataq.put(data)", "output": "Method representing the process\u2019s activity.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reshape(self, data_shapes, label_shapes=None):\n        \"\"\"\n        \"\"\"\n        assert self.binded\n        self._data_shapes, self._label_shapes = _parse_data_desc(\n            self.data_names, self.label_names, data_shapes, label_shapes)\n\n        self._exec_group.reshape(self._data_shapes, self._label_shapes)", "output": "Reshapes the module for new input shapes.\n\n        Parameters\n        ----------\n        data_shapes : list of (str, tuple)\n            Typically is ``data_iter.provide_data``.\n        label_shapes : list of (str, tuple)\n            Typically is ``data_iter.provide_label``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dates(self):\n        ''' '''\n        dates = []\n        d = self.date_a\n        while d < self.date_b:\n            dates.append(d)\n            d += datetime.timedelta(1)\n\n        return dates", "output": "Returns a list of dates in this date interval.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_dataset_metadata(self):\n    \"\"\"\"\"\"\n    if self.dataset_meta:\n      return\n    shell_call(['gsutil', 'cp',\n                'gs://' + self.storage_client.bucket_name + '/'\n                + 'dataset/' + self.dataset_name + '_dataset.csv',\n                LOCAL_DATASET_METADATA_FILE])\n    with open(LOCAL_DATASET_METADATA_FILE, 'r') as f:\n      self.dataset_meta = eval_lib.DatasetMetadata(f)", "output": "Read `dataset_meta` field from bucket", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def basic_range1(ranged_hparams):\n  \"\"\"\"\"\"\n  rhp = ranged_hparams\n  rhp.set_discrete(\"batch_size\", [1024, 2048, 4096])\n  rhp.set_discrete(\"num_hidden_layers\", [1, 2, 3, 4, 5, 6])\n  rhp.set_discrete(\"hidden_size\", [32, 64, 128, 256, 512], scale=rhp.LOG_SCALE)\n  rhp.set_discrete(\"kernel_height\", [1, 3, 5, 7])\n  rhp.set_discrete(\"kernel_width\", [1, 3, 5, 7])\n  rhp.set_discrete(\"compress_steps\", [0, 1, 2])\n  rhp.set_float(\"dropout\", 0.0, 0.5)\n  rhp.set_float(\"weight_decay\", 1e-4, 10.0, scale=rhp.LOG_SCALE)\n  rhp.set_float(\"label_smoothing\", 0.0, 0.2)\n  rhp.set_float(\"clip_grad_norm\", 0.01, 50.0, scale=rhp.LOG_SCALE)\n  rhp.set_float(\"learning_rate\", 0.005, 2.0, scale=rhp.LOG_SCALE)\n  rhp.set_categorical(\"initializer\",\n                      [\"uniform\", \"orthogonal\", \"uniform_unit_scaling\"])\n  rhp.set_float(\"initializer_gain\", 0.5, 3.5)\n  rhp.set_categorical(\"learning_rate_decay_scheme\",\n                      [\"none\", \"sqrt\", \"noam\", \"exp\"])\n  rhp.set_float(\"optimizer_adam_epsilon\", 1e-7, 1e-2, scale=rhp.LOG_SCALE)\n  rhp.set_float(\"optimizer_adam_beta1\", 0.8, 0.9)\n  rhp.set_float(\"optimizer_adam_beta2\", 0.995, 0.999)\n  rhp.set_categorical(\n      \"optimizer\",\n      [\"adam\", \"adagrad\", \"momentum\", \"rms_prop\", \"sgd\", \"yellow_fin\"])", "output": "A basic range of hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_dvs_infrastructure_traffic_resources(infra_traffic_resources,\n                                                resource_dicts):\n    '''\n    \n    '''\n    for res_dict in resource_dicts:\n        filtered_traffic_resources = \\\n                [r for r in infra_traffic_resources if r.key == res_dict['key']]\n        if filtered_traffic_resources:\n            traffic_res = filtered_traffic_resources[0]\n        else:\n            traffic_res = vim.DvsHostInfrastructureTrafficResource()\n            traffic_res.key = res_dict['key']\n            traffic_res.allocationInfo = \\\n                    vim.DvsHostInfrastructureTrafficResourceAllocation()\n            infra_traffic_resources.append(traffic_res)\n        if res_dict.get('limit'):\n            traffic_res.allocationInfo.limit = res_dict['limit']\n        if res_dict.get('reservation'):\n            traffic_res.allocationInfo.reservation = res_dict['reservation']\n        if res_dict.get('num_shares') or res_dict.get('share_level'):\n            if not traffic_res.allocationInfo.shares:\n                traffic_res.allocationInfo.shares = vim.SharesInfo()\n        if res_dict.get('share_level'):\n            traffic_res.allocationInfo.shares.level = \\\n                    vim.SharesLevel(res_dict['share_level'])\n        if res_dict.get('num_shares'):\n            #XXX Even though we always set the number of shares if provided,\n            #the vCenter will ignore it unless the share level is 'custom'.\n            traffic_res.allocationInfo.shares.shares = res_dict['num_shares']", "output": "Applies the values of the resource dictionaries to infra traffic resources,\n    creating the infra traffic resource if required\n    (vim.DistributedVirtualSwitchProductSpec)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_empty_ids(self):\n        \"\"\"\n        \n        \"\"\"\n        cursor = self.get_collection().find(\n            {\n                '_id': {'$in': self._document_ids},\n                self._field: {'$exists': True}\n            },\n            {'_id': True}\n        )\n\n        return set(self._document_ids) - {doc['_id'] for doc in cursor}", "output": "Get documents id with missing targeted field", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wrap_objective(f, *args, **kwds):\n  \"\"\"\n  \"\"\"\n  objective_func = f(*args, **kwds)\n  objective_name = f.__name__\n  args_str = \" [\" + \", \".join([_make_arg_str(arg) for arg in args]) + \"]\"\n  description = objective_name.title() + args_str\n  return Objective(objective_func, objective_name, description)", "output": "Decorator for creating Objective factories.\n\n  Changes f from the closure: (args) => () => TF Tensor\n  into an Obejective factory: (args) => Objective\n\n  while perserving function name, arg info, docs... for interactive python.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def storage_class(self, value):\n        \"\"\"\n        \"\"\"\n        if value not in self._STORAGE_CLASSES:\n            raise ValueError(\"Invalid storage class: %s\" % (value,))\n        self._patch_property(\"storageClass\", value)", "output": "Set the storage class for the bucket.\n\n        See https://cloud.google.com/storage/docs/storage-classes\n\n        :type value: str\n        :param value: one of \"MULTI_REGIONAL\", \"REGIONAL\", \"NEARLINE\",\n                      \"COLDLINE\", \"STANDARD\", or \"DURABLE_REDUCED_AVAILABILITY\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mean_absolute_error(pred:Tensor, targ:Tensor)->Rank0Tensor:\n    \"\"\n    pred,targ = flatten_check(pred,targ)\n    return torch.abs(targ - pred).mean()", "output": "Mean absolute error between `pred` and `targ`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def full(shape, val, dtype=None, **kwargs):\n    \"\"\"\n    \"\"\"\n    if dtype is None:\n        dtype = _numpy.float32\n    return _internal._full(shape=shape, dtype=dtype, value=float(val), **kwargs)", "output": "Returns a new array of given shape and type, filled with the given value `val`.\n\n    Parameters\n    ----------\n    shape :  int or sequence of ints\n        Shape of the new array.\n    val : scalar\n        Fill value.\n    dtype : str or numpy.dtype, optional\n        The value type of the inner value, default to ``np.float32``.\n\n    Returns\n    -------\n    out : Symbol\n        The created Symbol", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_uint_info(self, field, data):\n        \"\"\"\n        \"\"\"\n        if getattr(data, 'base', None) is not None and \\\n           data.base is not None and isinstance(data, np.ndarray) \\\n           and isinstance(data.base, np.ndarray) and (not data.flags.c_contiguous):\n            warnings.warn(\"Use subset (sliced data) of np.ndarray is not recommended \" +\n                          \"because it will generate extra copies and increase memory consumption\")\n            data = np.array(data, copy=True, dtype=ctypes.c_uint)\n        else:\n            data = np.array(data, copy=False, dtype=ctypes.c_uint)\n        _check_call(_LIB.XGDMatrixSetUIntInfo(self.handle,\n                                              c_str(field),\n                                              c_array(ctypes.c_uint, data),\n                                              c_bst_ulong(len(data))))", "output": "Set uint type property into the DMatrix.\n\n        Parameters\n        ----------\n        field: str\n            The field name of the information\n\n        data: numpy array\n            The array of data to be set", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_remove(name, user=None, password=None, host=None, port=None,\n                database='admin', authdb=None):\n    '''\n    \n    '''\n    conn = _connect(user, password, host, port)\n    if not conn:\n        return 'Failed to connect to mongo database'\n\n    try:\n        log.info('Removing user %s', name)\n        mdb = pymongo.database.Database(conn, database)\n        mdb.remove_user(name)\n    except pymongo.errors.PyMongoError as err:\n        log.error('Creating database %s failed with error: %s', name, err)\n        return six.text_type(err)\n\n    return True", "output": "Remove a MongoDB user\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mongodb.user_remove <name> <user> <password> <host> <port> <database>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_command(self, ctx, cmd_name):\n        \"\"\"\n        \n        \"\"\"\n\n        if cmd_name not in self.all_cmds:\n            return None\n        return EventTypeSubCommand(self.events_lib, cmd_name, self.all_cmds[cmd_name])", "output": "gets the subcommands under the service name\n\n        Parameters\n        ----------\n        ctx : Context\n            the context object passed into the method\n        cmd_name : str\n            the service name\n        Returns\n        -------\n        EventTypeSubCommand:\n            returns subcommand if successful, None if not.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def destroy_multiprocessing(parallel_data, queue=None):\n    '''\n    \n    '''\n    salt.utils.crypt.reinit_crypto()\n\n    parallel_data['opts']['output'] = 'json'\n    clouds = salt.loader.clouds(parallel_data['opts'])\n\n    try:\n        fun = clouds['{0}.destroy'.format(parallel_data['driver'])]\n        with salt.utils.context.func_globals_inject(\n            fun,\n            __active_provider_name__=':'.join([\n                parallel_data['alias'],\n                parallel_data['driver']\n            ])\n        ):\n            output = fun(parallel_data['name'])\n\n    except SaltCloudException as exc:\n        log.error(\n            'Failed to destroy %s. Error: %s',\n            parallel_data['name'], exc, exc_info_on_loglevel=logging.DEBUG\n        )\n        return {parallel_data['name']: {'Error': str(exc)}}\n\n    return {\n        parallel_data['name']: salt.utils.data.simple_types_filter(output)\n    }", "output": "This function will be called from another process when running a map in\n    parallel mode. The result from the destroy is always a json object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def after_loop(self, coro):\n        \"\"\"\n        \"\"\"\n\n        if not (inspect.iscoroutinefunction(coro) or inspect.isawaitable(coro)):\n            raise TypeError('Expected coroutine or awaitable, received {0.__name__!r}.'.format(type(coro)))\n\n        self._after_loop = coro", "output": "A function that also acts as a decorator to register a coroutine to be\n        called after the loop finished running.\n\n        Parameters\n        ------------\n        coro: :term:`py:awaitable`\n            The coroutine to register after the loop finishes.\n\n        Raises\n        -------\n        TypeError\n            The function was not a coroutine.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def crop_bbox_by_coords(bbox, crop_coords, crop_height, crop_width, rows, cols):\n    \"\"\"\n    \"\"\"\n    bbox = denormalize_bbox(bbox, rows, cols)\n    x_min, y_min, x_max, y_max = bbox\n    x1, y1, x2, y2 = crop_coords\n    cropped_bbox = [x_min - x1, y_min - y1, x_max - x1, y_max - y1]\n    return normalize_bbox(cropped_bbox, crop_height, crop_width)", "output": "Crop a bounding box using the provided coordinates of bottom-left and top-right corners in pixels and the\n    required height and width of the crop.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def field_mask(original, modified):\n    \"\"\"\n    \"\"\"\n    if original is None and modified is None:\n        return field_mask_pb2.FieldMask()\n\n    if original is None and modified is not None:\n        original = copy.deepcopy(modified)\n        original.Clear()\n\n    if modified is None and original is not None:\n        modified = copy.deepcopy(original)\n        modified.Clear()\n\n    if type(original) != type(modified):\n        raise ValueError(\n            \"expected that both original and modified should be of the \"\n            'same type, received \"{!r}\" and \"{!r}\".'.format(\n                type(original), type(modified)\n            )\n        )\n\n    return field_mask_pb2.FieldMask(paths=_field_mask_helper(original, modified))", "output": "Create a field mask by comparing two messages.\n\n    Args:\n        original (~google.protobuf.message.Message): the original message.\n            If set to None, this field will be interpretted as an empty\n            message.\n        modified (~google.protobuf.message.Message): the modified message.\n            If set to None, this field will be interpretted as an empty\n            message.\n\n    Returns:\n        google.protobuf.field_mask_pb2.FieldMask: field mask that contains\n        the list of field names that have different values between the two\n        messages. If the messages are equivalent, then the field mask is empty.\n\n    Raises:\n        ValueError: If the ``original`` or ``modified`` are not the same type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _commit(self):\n        \"\"\"\n        \"\"\"\n        if self._id is None:\n            mode = _datastore_pb2.CommitRequest.NON_TRANSACTIONAL\n        else:\n            mode = _datastore_pb2.CommitRequest.TRANSACTIONAL\n\n        commit_response_pb = self._client._datastore_api.commit(\n            self.project, mode, self._mutations, transaction=self._id\n        )\n        _, updated_keys = _parse_commit_response(commit_response_pb)\n        # If the back-end returns without error, we are guaranteed that\n        # ``commit`` will return keys that match (length and\n        # order) directly ``_partial_key_entities``.\n        for new_key_pb, entity in zip(updated_keys, self._partial_key_entities):\n            new_id = new_key_pb.path[-1].id\n            entity.key = entity.key.completed_key(new_id)", "output": "Commits the batch.\n\n        This is called by :meth:`commit`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(self, s):\n        \"\"\"\n        \n        \"\"\"\n        # TODO: can we use xml.utils.iso8601 or something similar?\n\n        from luigi import date_interval as d\n\n        for cls in [d.Year, d.Month, d.Week, d.Date, d.Custom]:\n            i = cls.parse(s)\n            if i:\n                return i\n\n        raise ValueError('Invalid date interval - could not be parsed')", "output": "Parses a :py:class:`~luigi.date_interval.DateInterval` from the input.\n\n        see :py:mod:`luigi.date_interval`\n          for details on the parsing of DateIntervals.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lstm_seq2seq_internal(inputs, targets, hparams, train):\n  \"\"\"\"\"\"\n  with tf.variable_scope(\"lstm_seq2seq\"):\n    if inputs is not None:\n      inputs_length = common_layers.length_from_embedding(inputs)\n      # Flatten inputs.\n      inputs = common_layers.flatten4d3d(inputs)\n\n      # LSTM encoder.\n      inputs = tf.reverse_sequence(inputs, inputs_length, seq_axis=1)\n      _, final_encoder_state = lstm(inputs, inputs_length, hparams, train,\n                                    \"encoder\")\n    else:\n      final_encoder_state = None\n\n    # LSTM decoder.\n    shifted_targets = common_layers.shift_right(targets)\n    # Add 1 to account for the padding added to the left from shift_right\n    targets_length = common_layers.length_from_embedding(shifted_targets) + 1\n    decoder_outputs, _ = lstm(\n        common_layers.flatten4d3d(shifted_targets),\n        targets_length,\n        hparams,\n        train,\n        \"decoder\",\n        initial_state=final_encoder_state)\n    return tf.expand_dims(decoder_outputs, axis=2)", "output": "The basic LSTM seq2seq model, main step used for training.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _stringify_na_values(na_values):\n    \"\"\"  \"\"\"\n    result = []\n    for x in na_values:\n        result.append(str(x))\n        result.append(x)\n        try:\n            v = float(x)\n\n            # we are like 999 here\n            if v == int(v):\n                v = int(v)\n                result.append(\"{value}.0\".format(value=v))\n                result.append(str(v))\n\n            result.append(v)\n        except (TypeError, ValueError, OverflowError):\n            pass\n        try:\n            result.append(int(x))\n        except (TypeError, ValueError, OverflowError):\n            pass\n    return set(result)", "output": "return a stringified and numeric for these values", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_cert_serial(cert_file):\n    '''\n    \n    '''\n    cmd = \"certutil.exe -silent -verify {0}\".format(cert_file)\n    out = __salt__['cmd.run'](cmd)\n    # match serial number by paragraph to work with multiple languages\n    matches = re.search(r\":\\s*(\\w*)\\r\\n\\r\\n\", out)\n    if matches is not None:\n        return matches.groups()[0].strip()\n    else:\n        return None", "output": "Get the serial number of a certificate file\n\n    cert_file\n        The certificate file to find the serial for\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' certutil.get_cert_serial <certificate name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def network_create(provider, names, **kwargs):\n    '''\n    \n\n    '''\n    client = _get_client()\n    return client.extra_action(provider=provider, names=names, action='network_create', **kwargs)", "output": "Create private network\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minionname cloud.network_create my-nova names=['salt'] cidr='192.168.100.0/24'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transform(graph):\n    '''\n    '''\n\n    graphs = []\n    for _ in range(Constant.N_NEIGHBOURS * 2):\n        random_num = randrange(3)\n        temp_graph = None\n        if random_num == 0:\n            temp_graph = to_deeper_graph(deepcopy(graph))\n        elif random_num == 1:\n            temp_graph = to_wider_graph(deepcopy(graph))\n        elif random_num == 2:\n            temp_graph = to_skip_connection_graph(deepcopy(graph))\n\n        if temp_graph is not None and temp_graph.size() <= Constant.MAX_MODEL_SIZE:\n            graphs.append(temp_graph)\n\n        if len(graphs) >= Constant.N_NEIGHBOURS:\n            break\n\n    return graphs", "output": "core transform function for graph.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _reverse_lookup(dictionary, value):\n    '''\n    \n    '''\n    value_index = -1\n    for idx, dict_value in enumerate(dictionary.values()):\n        if type(dict_value) == list:\n            if value in dict_value:\n                value_index = idx\n                break\n        elif value == dict_value:\n            value_index = idx\n            break\n\n    return list(dictionary)[value_index]", "output": "Lookup the key in a dictionary by it's value. Will return the first match.\n\n    :param dict dictionary: The dictionary to search\n\n    :param str value: The value to search for.\n\n    :return: Returns the first key to match the value\n    :rtype: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_name_and_version(name, version, for_filename=False):\n    \"\"\"\"\"\"\n    if for_filename:\n        # For both name and version any runs of non-alphanumeric or '.'\n        # characters are replaced with a single '-'.  Additionally any\n        # spaces in the version string become '.'\n        name = _FILESAFE.sub('-', name)\n        version = _FILESAFE.sub('-', version.replace(' ', '.'))\n    return '%s-%s' % (name, version)", "output": "Return the distribution name with version.\n\n    If for_filename is true, return a filename-escaped form.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def first(self, offset):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(self.index, DatetimeIndex):\n            raise TypeError(\"'first' only supports a DatetimeIndex index\")\n\n        if len(self.index) == 0:\n            return self\n\n        offset = to_offset(offset)\n        end_date = end = self.index[0] + offset\n\n        # Tick-like, e.g. 3 weeks\n        if not offset.isAnchored() and hasattr(offset, '_inc'):\n            if end_date in self.index:\n                end = self.index.searchsorted(end_date, side='left')\n                return self.iloc[:end]\n\n        return self.loc[:end]", "output": "Convenience method for subsetting initial periods of time series data\n        based on a date offset.\n\n        Parameters\n        ----------\n        offset : string, DateOffset, dateutil.relativedelta\n\n        Returns\n        -------\n        subset : same type as caller\n\n        Raises\n        ------\n        TypeError\n            If the index is not  a :class:`DatetimeIndex`\n\n        See Also\n        --------\n        last : Select final periods of time series based on a date offset.\n        at_time : Select values at a particular time of the day.\n        between_time : Select values between particular times of the day.\n\n        Examples\n        --------\n        >>> i = pd.date_range('2018-04-09', periods=4, freq='2D')\n        >>> ts = pd.DataFrame({'A': [1,2,3,4]}, index=i)\n        >>> ts\n                    A\n        2018-04-09  1\n        2018-04-11  2\n        2018-04-13  3\n        2018-04-15  4\n\n        Get the rows for the first 3 days:\n\n        >>> ts.first('3D')\n                    A\n        2018-04-09  1\n        2018-04-11  2\n\n        Notice the data for 3 first calender days were returned, not the first\n        3 days observed in the dataset, and therefore data for 2018-04-13 was\n        not returned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _handle_message(self, opcode: int, data: bytes) -> \"Optional[Future[None]]\":\n        \"\"\"\"\"\"\n        if self.client_terminated:\n            return None\n\n        if self._frame_compressed:\n            assert self._decompressor is not None\n            try:\n                data = self._decompressor.decompress(data)\n            except _DecompressTooLargeError:\n                self.close(1009, \"message too big after decompression\")\n                self._abort()\n                return None\n\n        if opcode == 0x1:\n            # UTF-8 data\n            self._message_bytes_in += len(data)\n            try:\n                decoded = data.decode(\"utf-8\")\n            except UnicodeDecodeError:\n                self._abort()\n                return None\n            return self._run_callback(self.handler.on_message, decoded)\n        elif opcode == 0x2:\n            # Binary data\n            self._message_bytes_in += len(data)\n            return self._run_callback(self.handler.on_message, data)\n        elif opcode == 0x8:\n            # Close\n            self.client_terminated = True\n            if len(data) >= 2:\n                self.close_code = struct.unpack(\">H\", data[:2])[0]\n            if len(data) > 2:\n                self.close_reason = to_unicode(data[2:])\n            # Echo the received close code, if any (RFC 6455 section 5.5.1).\n            self.close(self.close_code)\n        elif opcode == 0x9:\n            # Ping\n            try:\n                self._write_frame(True, 0xA, data)\n            except StreamClosedError:\n                self._abort()\n            self._run_callback(self.handler.on_ping, data)\n        elif opcode == 0xA:\n            # Pong\n            self.last_pong = IOLoop.current().time()\n            return self._run_callback(self.handler.on_pong, data)\n        else:\n            self._abort()\n        return None", "output": "Execute on_message, returning its Future if it is a coroutine.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pb(name, data, display_name=None, description=None):\n  \"\"\"\n  \"\"\"\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n\n  data = np.array(data)\n  if data.shape != ():\n    raise ValueError('Expected scalar shape for data, saw shape: %s.'\n                     % data.shape)\n  if data.dtype.kind not in ('b', 'i', 'u', 'f'):  # bool, int, uint, float\n    raise ValueError('Cast %s to float is not supported' % data.dtype.name)\n  tensor = tf.make_tensor_proto(data.astype(np.float32))\n\n  if display_name is None:\n    display_name = name\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=display_name, description=description)\n  tf_summary_metadata = tf.SummaryMetadata.FromString(\n      summary_metadata.SerializeToString())\n  summary = tf.Summary()\n  summary.value.add(tag='%s/scalar_summary' % name,\n                    metadata=tf_summary_metadata,\n                    tensor=tensor)\n  return summary", "output": "Create a legacy scalar summary protobuf.\n\n  Arguments:\n    name: A unique name for the generated summary, including any desired\n      name scopes.\n    data: A rank-0 `np.array` or array-like form (so raw `int`s and\n      `float`s are fine, too).\n    display_name: Optional name for this summary in TensorBoard, as a\n      `str`. Defaults to `name`.\n    description: Optional long-form description for this summary, as a\n      `str`. Markdown is supported. Defaults to empty.\n\n  Returns:\n    A `tf.Summary` protobuf object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def competitions_submissions_list(self, id, **kwargs):  # noqa: E501\n        \"\"\"\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.competitions_submissions_list_with_http_info(id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.competitions_submissions_list_with_http_info(id, **kwargs)  # noqa: E501\n            return data", "output": "List competition submissions  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.competitions_submissions_list(id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str id: Competition name (required)\n        :param int page: Page number\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def submit(self):\n        \"\"\"\"\"\"\n        if self._w3c:\n            form = self.find_element(By.XPATH, \"./ancestor-or-self::form\")\n            self._parent.execute_script(\n                \"var e = arguments[0].ownerDocument.createEvent('Event');\"\n                \"e.initEvent('submit', true, true);\"\n                \"if (arguments[0].dispatchEvent(e)) { arguments[0].submit() }\", form)\n        else:\n            self._execute(Command.SUBMIT_ELEMENT)", "output": "Submits a form.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _path_is_executable_others(path):\n    '''\n    \n    '''\n    prevpath = None\n    while path and path != prevpath:\n        try:\n            if not os.stat(path).st_mode & stat.S_IXOTH:\n                return False\n        except OSError:\n            return False\n        prevpath = path\n        path, _ = os.path.split(path)\n    return True", "output": "Check every part of path for executable permission", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_role(self, name=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_security\", \"role\", name), params=params\n        )", "output": "`<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-role.html>`_\n\n        :arg name: Role name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_ipaddress(hostname):\n    \"\"\"\n    \"\"\"\n    if six.PY3 and isinstance(hostname, bytes):\n        # IDN A-label bytes are ASCII compatible.\n        hostname = hostname.decode('ascii')\n\n    families = [socket.AF_INET]\n    if hasattr(socket, 'AF_INET6'):\n        families.append(socket.AF_INET6)\n\n    for af in families:\n        try:\n            inet_pton(af, hostname)\n        except (socket.error, ValueError, OSError):\n            pass\n        else:\n            return True\n    return False", "output": "Detects whether the hostname given is an IP address.\n\n    :param str hostname: Hostname to examine.\n    :return: True if the hostname is an IP address, False otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chown(self, tarinfo, targetpath):\n        \"\"\"\n        \"\"\"\n        if pwd and hasattr(os, \"geteuid\") and os.geteuid() == 0:\n            # We have to be root to do so.\n            try:\n                g = grp.getgrnam(tarinfo.gname)[2]\n            except KeyError:\n                g = tarinfo.gid\n            try:\n                u = pwd.getpwnam(tarinfo.uname)[2]\n            except KeyError:\n                u = tarinfo.uid\n            try:\n                if tarinfo.issym() and hasattr(os, \"lchown\"):\n                    os.lchown(targetpath, u, g)\n                else:\n                    if sys.platform != \"os2emx\":\n                        os.chown(targetpath, u, g)\n            except EnvironmentError as e:\n                raise ExtractError(\"could not change owner\")", "output": "Set owner of targetpath according to tarinfo.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_states(queue=False, **kwargs):\n    '''\n    \n\n    '''\n    conflict = _check_queue(queue, kwargs)\n    if conflict is not None:\n        assert False\n        return conflict\n\n    opts = salt.utils.state.get_sls_opts(__opts__, **kwargs)\n    try:\n        st_ = salt.state.HighState(opts,\n                                   proxy=__proxy__,\n                                   initial_pillar=_get_initial_pillar(opts))\n    except NameError:\n        st_ = salt.state.HighState(opts,\n                                   initial_pillar=_get_initial_pillar(opts))\n\n    errors = _get_pillar_errors(kwargs, pillar=st_.opts['pillar'])\n    if errors:\n        __context__['retcode'] = salt.defaults.exitcodes.EX_PILLAR_FAILURE\n        raise CommandExecutionError('Pillar failed to render', info=errors)\n\n    st_.push_active()\n    states = OrderedDict()\n    try:\n        result = st_.compile_low_chunks()\n\n        if not isinstance(result, list):\n            raise Exception(result)\n\n        for s in result:\n            states[s['__sls__']] = True\n    finally:\n        st_.pop_active()\n\n    return list(states.keys())", "output": "Returns the list of states that will be applied on highstate.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.show_states\n\n    .. versionadded:: 2019.2.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def backward(self, out_grads=None):\n        \"\"\"\"\"\"\n        assert self.binded and self.params_initialized\n        self._curr_module.backward(out_grads=out_grads)", "output": "Backward computation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def network_security_group_get(name, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        secgroup = netconn.network_security_groups.get(\n            resource_group_name=resource_group,\n            network_security_group_name=name\n        )\n        result = secgroup.as_dict()\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Get details about a network security group within a resource group.\n\n    :param name: The name of the network security group to query.\n\n    :param resource_group: The resource group name assigned to the\n        network security group.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.network_security_group_get testnsg testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def plasma_prefetch(object_id):\n    \"\"\"\"\"\"\n    local_sched_client = ray.worker.global_worker.raylet_client\n    ray_obj_id = ray.ObjectID(object_id)\n    local_sched_client.fetch_or_reconstruct([ray_obj_id], True)", "output": "Tells plasma to prefetch the given object_id.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cleanup_db(config_path=_DEFAULT_CONFIG_PATH, dry_run=False):\n    '''\n    \n    '''\n    _validate_config(config_path)\n    dry_run = six.text_type(bool(dry_run)).lower()\n\n    ret = {'deleted_keys': list(),\n           'deleted_files': list()}\n\n    cmd = ['db', 'cleanup', '-config={}'.format(config_path),\n           '-dry-run={}'.format(dry_run), '-verbose=true']\n\n    cmd_ret = _cmd_run(cmd)\n\n    type_pattern = r'^List\\s+[\\w\\s]+(?P<package_type>(file|key)s)[\\w\\s]+:$'\n    list_pattern = r'^\\s+-\\s+(?P<package>.*)$'\n    current_block = None\n\n    for line in cmd_ret.splitlines():\n        if current_block:\n            match = re.search(list_pattern, line)\n            if match:\n                package_type = 'deleted_{}'.format(current_block)\n                ret[package_type].append(match.group('package'))\n            else:\n                current_block = None\n        # Intentionally not using an else here, in case of a situation where\n        # the next list header might be bordered by the previous list.\n        if not current_block:\n            match = re.search(type_pattern, line)\n            if match:\n                current_block = match.group('package_type')\n\n    log.debug('Package keys identified for deletion: %s', len(ret['deleted_keys']))\n    log.debug('Package files identified for deletion: %s', len(ret['deleted_files']))\n    return ret", "output": "Remove data regarding unreferenced packages and delete files in the package pool that\n        are no longer being used by packages.\n\n    :param bool dry_run: Report potential changes without making any changes.\n\n    :return: A dictionary of the package keys and files that were removed.\n    :rtype: dict\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' aptly.cleanup_db", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_dirs(path, folder):\n    '''\n     \n    '''\n    lbls, fnames, all_lbls = [], [], []\n    full_path = os.path.join(path, folder)\n    for lbl in sorted(os.listdir(full_path)):\n        if lbl not in ('.ipynb_checkpoints','.DS_Store'):\n            all_lbls.append(lbl)\n            for fname in os.listdir(os.path.join(full_path, lbl)):\n                if fname not in ('.DS_Store'):\n                    fnames.append(os.path.join(folder, lbl, fname))\n                    lbls.append(lbl)\n    return fnames, lbls, all_lbls", "output": "Fetches name of all files in path in long form, and labels associated by extrapolation of directory names.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def device_path(cls, project, location, registry, device):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/locations/{location}/registries/{registry}/devices/{device}\",\n            project=project,\n            location=location,\n            registry=registry,\n            device=device,\n        )", "output": "Return a fully-qualified device string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_signature(self, run_args):\n    \"\"\"\"\"\"\n    return (id(run_args.fct), run_args.input.dtype, run_args.input.shape)", "output": "Create a unique signature for each fct/inputs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _shouldOwn(self, param):\n        \"\"\"\n        \n        \"\"\"\n        if not (self.uid == param.parent and self.hasParam(param.name)):\n            raise ValueError(\"Param %r does not belong to %r.\" % (param, self))", "output": "Validates that the input param belongs to this Params instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_public_ip(name, resource_group):\n    '''\n    \n    '''\n    netconn = get_conn(client_type='network')\n    try:\n        pubip_query = netconn.public_ip_addresses.get(\n            resource_group_name=resource_group,\n            public_ip_address_name=name\n        )\n        pubip = pubip_query.as_dict()\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', exc.message)\n        pubip = {'error': exc.message}\n\n    return pubip", "output": "Get the public ip address details by name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_strings_from_utterance(tokenized_utterance: List[Token]) -> Dict[str, List[int]]:\n    \"\"\"\n    \n    \"\"\"\n    string_linking_scores: Dict[str, List[int]] = defaultdict(list)\n\n    for index, token in enumerate(tokenized_utterance):\n        for string in ATIS_TRIGGER_DICT.get(token.text.lower(), []):\n            string_linking_scores[string].append(index)\n\n    token_bigrams = bigrams([token.text for token in tokenized_utterance])\n    for index, token_bigram in enumerate(token_bigrams):\n        for string in ATIS_TRIGGER_DICT.get(' '.join(token_bigram).lower(), []):\n            string_linking_scores[string].extend([index,\n                                                  index + 1])\n\n    trigrams = ngrams([token.text for token in tokenized_utterance], 3)\n    for index, trigram in enumerate(trigrams):\n        if trigram[0] == 'st':\n            natural_language_key = f'st. {trigram[2]}'.lower()\n        else:\n            natural_language_key = ' '.join(trigram).lower()\n        for string in ATIS_TRIGGER_DICT.get(natural_language_key, []):\n            string_linking_scores[string].extend([index,\n                                                  index + 1,\n                                                  index + 2])\n    return string_linking_scores", "output": "Based on the current utterance, return a dictionary where the keys are the strings in\n    the database that map to lists of the token indices that they are linked to.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sell_avg_holding_price(self):\n        \"\"\"\n        \n        \"\"\"\n        return 0 if self.sell_quantity == 0 else self._sell_holding_cost / self.sell_quantity / self.contract_multiplier", "output": "[float] \u5356\u65b9\u5411\u6301\u4ed3\u5747\u4ef7", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _key(self):\n        \"\"\"\n        \"\"\"\n        return (\n            self._name,\n            self._field_type.upper(),\n            self._mode.upper(),\n            self._description,\n            self._fields,\n        )", "output": "A tuple key that uniquely describes this field.\n\n        Used to compute this instance's hashcode and evaluate equality.\n\n        Returns:\n            tuple: The contents of this\n                   :class:`~google.cloud.bigquery.schema.SchemaField`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compute_chunksize(df, num_splits, default_block_size=32, axis=None):\n    \"\"\"\n    \"\"\"\n    if axis == 0 or axis is None:\n        row_chunksize = get_default_chunksize(len(df.index), num_splits)\n        # Take the min of the default and the memory-usage chunksize first to avoid a\n        # large amount of small partitions.\n        row_chunksize = max(1, row_chunksize, default_block_size)\n        if axis == 0:\n            return row_chunksize\n    # We always execute this because we can only get here if axis is 1 or None.\n    col_chunksize = get_default_chunksize(len(df.columns), num_splits)\n    # Take the min of the default and the memory-usage chunksize first to avoid a\n    # large amount of small partitions.\n    col_chunksize = max(1, col_chunksize, default_block_size)\n    if axis == 1:\n        return col_chunksize\n\n    return row_chunksize, col_chunksize", "output": "Computes the number of rows and/or columns to include in each partition.\n\n    Args:\n        df: The DataFrame to split.\n        num_splits: The maximum number of splits to separate the DataFrame into.\n        default_block_size: Minimum number of rows/columns (default set to 32x32).\n        axis: The axis to split. (0: Index, 1: Columns, None: Both)\n\n    Returns:\n         If axis is 1 or 0, returns an integer number of rows/columns to split the\n         DataFrame. If axis is None, return a tuple containing both.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_relative_timing_fn():\n  \"\"\"\"\"\"\n  start_time = time.time()\n\n  def format_relative_time():\n    time_delta = time.time() - start_time\n    return str(datetime.timedelta(seconds=time_delta))\n\n  def log_relative_time():\n    tf.logging.info(\"Timing: %s\", format_relative_time())\n\n  return log_relative_time", "output": "Make a function that logs the duration since it was made.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_util_get_order_datetime(dt):\n    \"\"\"\n    \"\"\"\n\n    #dt= datetime.datetime.now()\n    dt = datetime.datetime.strptime(str(dt)[0:19], '%Y-%m-%d %H:%M:%S')\n\n    if QA_util_if_trade(str(dt.date())) and dt.time() < datetime.time(15, 0, 0):\n        return str(dt)\n    else:\n        # print('before')\n        # print(QA_util_date_gap(str(dt.date()),1,'lt'))\n        return '{} {}'.format(\n            QA_util_date_gap(str(dt.date()),\n                             1,\n                             'lt'),\n            dt.time()\n        )", "output": "\u59d4\u6258\u7684\u771f\u5b9e\u65e5\u671f\n\n    Returns:\n        [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_new_project(self):\r\n        \"\"\"\"\"\"\r\n        self.switch_to_plugin()\r\n        active_project = self.current_active_project\r\n        dlg = ProjectDialog(self)\r\n        dlg.sig_project_creation_requested.connect(self._create_project)\r\n        dlg.sig_project_creation_requested.connect(self.sig_project_created)\r\n        if dlg.exec_():\r\n            if (active_project is None\r\n                    and self.get_option('visible_if_project_open')):\r\n                self.show_explorer()\r\n            self.sig_pythonpath_changed.emit()\r\n            self.restart_consoles()", "output": "Create new project", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_text(self, t:str, tok:BaseTokenizer) -> List[str]:\n        \"\"\n        for rule in self.pre_rules: t = rule(t)\n        toks = tok.tokenizer(t)\n        for rule in self.post_rules: toks = rule(toks)\n        return toks", "output": "Process one text `t` with tokenizer `tok`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_bqtable(cls, table, client=None):\n        \"\"\"\n        \"\"\"\n        return cls(table.project_id, table.dataset_id, table.table_id, client=client)", "output": "A constructor that takes a :py:class:`BQTable`.\n\n           :param table:\n           :type table: BQTable", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uninstall(name, version=None, uninstall_args=None, override_args=False):\n    '''\n    \n    '''\n    choc_path = _find_chocolatey(__context__, __salt__)\n    # chocolatey helpfully only supports a single package argument\n    cmd = [choc_path, 'uninstall', name]\n    if version:\n        cmd.extend(['--version', version])\n    if uninstall_args:\n        cmd.extend(['--uninstallarguments', uninstall_args])\n    if override_args:\n        cmd.extend(['--overridearguments'])\n    cmd.extend(_yes(__context__))\n    result = __salt__['cmd.run_all'](cmd, python_shell=False)\n\n    if result['retcode'] not in [0, 1605, 1614, 1641]:\n        raise CommandExecutionError(\n            'Running chocolatey failed: {0}'.format(result['stdout'])\n        )\n\n    return result['stdout']", "output": "Instructs Chocolatey to uninstall a package.\n\n    name\n        The name of the package to be uninstalled. Only accepts a single\n        argument.\n\n    version\n        Uninstalls a specific version of the package. Defaults to latest version\n        installed.\n\n    uninstall_args\n        A list of uninstall arguments you want to pass to the uninstallation\n        process i.e product key or feature list\n\n    override_args\n        Set to true if you want to override the original uninstall arguments\n        (for the native uninstaller) in the package and use your own. When this\n        is set to False uninstall_args will be appended to the end of the\n        default arguments\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' chocolatey.uninstall <package name>\n        salt '*' chocolatey.uninstall <package name> version=<package version>\n        salt '*' chocolatey.uninstall <package name> version=<package version> uninstall_args=<args> override_args=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def all_referenced_targets(self, result):\n        \"\"\"\"\"\"\n        if __debug__:\n            from .property import Property\n            assert is_iterable_typed(result, (VirtualTarget, Property))\n        # Find directly referenced targets.\n        deps = self.build_properties().dependency()\n        all_targets = self.sources_ + deps\n\n        # Find other subvariants.\n        r = []\n        for e in all_targets:\n            if not e in result:\n                result.add(e)\n                if isinstance(e, property.Property):\n                    t = e.value\n                else:\n                    t = e\n\n                # FIXME: how can this be?\n                cs = t.creating_subvariant()\n                if cs:\n                    r.append(cs)\n        r = unique(r)\n        for s in r:\n            if s != self:\n                s.all_referenced_targets(result)", "output": "Returns all targets referenced by this subvariant,\n        either directly or indirectly, and either as sources,\n        or as dependency properties. Targets referred with\n        dependency property are returned a properties, not targets.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_map(self):\n        \"\"\"\"\"\"\n        if self._map is None:\n            self._map = OrderedDict((('stata_data', 0),\n                                     ('map', self._file.tell()),\n                                     ('variable_types', 0),\n                                     ('varnames', 0),\n                                     ('sortlist', 0),\n                                     ('formats', 0),\n                                     ('value_label_names', 0),\n                                     ('variable_labels', 0),\n                                     ('characteristics', 0),\n                                     ('data', 0),\n                                     ('strls', 0),\n                                     ('value_labels', 0),\n                                     ('stata_data_close', 0),\n                                     ('end-of-file', 0)))\n        # Move to start of map\n        self._file.seek(self._map['map'])\n        bio = BytesIO()\n        for val in self._map.values():\n            bio.write(struct.pack(self._byteorder + 'Q', val))\n        bio.seek(0)\n        self._file.write(self._tag(bio.read(), 'map'))", "output": "Called twice during file write. The first populates the values in\n        the map with 0s.  The second call writes the final map locations when\n        all blocks have been written.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_att_mats(translate_model):\n  \"\"\"\n  \"\"\"\n  enc_atts = []\n  dec_atts = []\n  encdec_atts = []\n\n  prefix = \"transformer/body/\"\n  postfix_self_attention = \"/multihead_attention/dot_product_attention\"\n  if translate_model.hparams.self_attention_type == \"dot_product_relative\":\n    postfix_self_attention = (\"/multihead_attention/\"\n                              \"dot_product_attention_relative\")\n  postfix_encdec = \"/multihead_attention/dot_product_attention\"\n\n  for i in range(translate_model.hparams.num_hidden_layers):\n    enc_att = translate_model.attention_weights[\n        \"%sencoder/layer_%i/self_attention%s\"\n        % (prefix, i, postfix_self_attention)]\n    dec_att = translate_model.attention_weights[\n        \"%sdecoder/layer_%i/self_attention%s\"\n        % (prefix, i, postfix_self_attention)]\n    encdec_att = translate_model.attention_weights[\n        \"%sdecoder/layer_%i/encdec_attention%s\" % (prefix, i, postfix_encdec)]\n    enc_atts.append(enc_att)\n    dec_atts.append(dec_att)\n    encdec_atts.append(encdec_att)\n\n  return enc_atts, dec_atts, encdec_atts", "output": "Get's the tensors representing the attentions from a build model.\n\n  The attentions are stored in a dict on the Transformer object while building\n  the graph.\n\n  Args:\n    translate_model: Transformer object to fetch the attention weights from.\n\n  Returns:\n  Tuple of attention matrices; (\n      enc_atts: Encoder self attention weights.\n        A list of `num_layers` numpy arrays of size\n        (batch_size, num_heads, inp_len, inp_len)\n      dec_atts: Decoder self attetnion weights.\n        A list of `num_layers` numpy arrays of size\n        (batch_size, num_heads, out_len, out_len)\n      encdec_atts: Encoder-Decoder attention weights.\n        A list of `num_layers` numpy arrays of size\n        (batch_size, num_heads, out_len, inp_len)\n  )", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_term_pillar(filter_name,\n                    term_name,\n                    pillar_key='acl',\n                    pillarenv=None,\n                    saltenv=None):\n    '''\n    \n    '''\n    filter_pillar_cfg = get_filter_pillar(filter_name,\n                                          pillar_key=pillar_key,\n                                          pillarenv=pillarenv,\n                                          saltenv=saltenv)\n    term_pillar_cfg = filter_pillar_cfg.get('terms', [])\n    term_opts = _lookup_element(term_pillar_cfg, term_name)\n    return term_opts", "output": "Helper that can be used inside a state SLS,\n    in order to get the term configuration given its name,\n    under a certain filter uniquely identified by its name.\n\n    filter_name\n        The name of the filter.\n\n    term_name\n        The name of the term.\n\n    pillar_key: ``acl``\n        The root key of the whole policy config. Default: ``acl``.\n\n    pillarenv\n        Query the master to generate fresh pillar data on the fly,\n        specifically from the requested pillar environment.\n\n    saltenv\n        Included only for compatibility with\n        :conf_minion:`pillarenv_from_saltenv`, and is otherwise ignored.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def migrate_paths(opts):\n    '''\n    \n    '''\n    oldpki_dir = os.path.join(syspaths.CONFIG_DIR, 'pki')\n\n    if not os.path.exists(oldpki_dir):\n        # There's not even a pki directory, don't bother migrating\n        return\n\n    newpki_dir = opts['pki_dir']\n\n    if opts['default_include'].startswith('master'):\n        keepers = ['master.pem',\n                   'master.pub',\n                   'syndic_master.pub',\n                   'minions',\n                   'minions_pre',\n                   'minions_rejected',\n                   ]\n        if not os.path.exists(newpki_dir):\n            os.makedirs(newpki_dir)\n        for item in keepers:\n            oi_path = os.path.join(oldpki_dir, item)\n            ni_path = os.path.join(newpki_dir, item)\n            if os.path.exists(oi_path) and not os.path.exists(ni_path):\n                shutil.move(oi_path, ni_path)\n\n    if opts['default_include'].startswith('minion'):\n        keepers = ['minion_master.pub',\n                   'minion.pem',\n                   'minion.pub',\n                   ]\n        if not os.path.exists(newpki_dir):\n            os.makedirs(newpki_dir)\n        for item in keepers:\n            oi_path = os.path.join(oldpki_dir, item)\n            ni_path = os.path.join(newpki_dir, item)\n            if os.path.exists(oi_path) and not os.path.exists(ni_path):\n                shutil.move(oi_path, ni_path)", "output": "Migrate old minion and master pki file paths to new ones.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_index_days(self, i, roll):\n        \"\"\"\n        \n        \"\"\"\n        nanos = (roll % 2) * Timedelta(days=self.day_of_month - 1).value\n        return i + nanos.astype('timedelta64[ns]')", "output": "Add days portion of offset to DatetimeIndex i.\n\n        Parameters\n        ----------\n        i : DatetimeIndex\n        roll : ndarray[int64_t]\n\n        Returns\n        -------\n        result : DatetimeIndex", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cleanup_datastore(self):\n    \"\"\"\"\"\"\n    print_header('CLEANING UP ENTIRE DATASTORE')\n    kinds_to_delete = [u'Submission', u'SubmissionType',\n                       u'DatasetImage', u'DatasetBatch',\n                       u'AdversarialImage', u'AdversarialBatch',\n                       u'Work', u'WorkType',\n                       u'ClassificationBatch']\n    keys_to_delete = [e.key for k in kinds_to_delete\n                      for e in self.datastore_client.query_fetch(kind=k)]\n    self._cleanup_keys_with_confirmation(keys_to_delete)", "output": "Cleans up datastore and deletes all information about current round.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __set_workdir(self):\r\n        \"\"\"\"\"\"\r\n        fname = self.get_current_filename()\r\n        if fname is not None:\r\n            directory = osp.dirname(osp.abspath(fname))\r\n            self.open_dir.emit(directory)", "output": "Set current script directory as working directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_trial_complete(self,\n                          trial_id,\n                          result=None,\n                          error=False,\n                          early_terminated=False):\n        \"\"\"\n        \"\"\"\n        ng_trial_info = self._live_trial_mapping.pop(trial_id)\n        if result:\n            self._nevergrad_opt.tell(ng_trial_info, -result[self._reward_attr])", "output": "Passes the result to Nevergrad unless early terminated or errored.\n\n        The result is internally negated when interacting with Nevergrad\n        so that Nevergrad Optimizers can \"maximize\" this value,\n        as it minimizes on default.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ssh_get_mic(self, session_id, gss_kex=False):\n        \"\"\"\n        \n        \"\"\"\n        self._session_id = session_id\n        if not gss_kex:\n            mic_field = self._ssh_build_mic(\n                self._session_id,\n                self._username,\n                self._service,\n                self._auth_method,\n            )\n            mic_token = self._gss_ctxt.sign(mic_field)\n        else:\n            # for key exchange with gssapi-keyex\n            mic_token = self._gss_srv_ctxt.sign(self._session_id)\n        return mic_token", "output": "Create the MIC token for a SSH2 message.\n\n        :param str session_id: The SSH session ID\n        :param bool gss_kex: Generate the MIC for Key Exchange with SSPI or not\n        :return: gssapi-with-mic:\n                 Returns the MIC token from SSPI for the message we created\n                 with ``_ssh_build_mic``.\n                 gssapi-keyex:\n                 Returns the MIC token from SSPI with the SSH session ID as\n                 message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_conn(client_type):\n    '''\n    \n    '''\n    conn_kwargs = {}\n\n    conn_kwargs['subscription_id'] = salt.utils.stringutils.to_str(\n        config.get_cloud_config_value(\n            'subscription_id',\n            get_configured_provider(), __opts__, search_global=False\n        )\n    )\n\n    cloud_env = config.get_cloud_config_value(\n        'cloud_environment',\n        get_configured_provider(), __opts__, search_global=False\n    )\n\n    if cloud_env is not None:\n        conn_kwargs['cloud_environment'] = cloud_env\n\n    tenant = config.get_cloud_config_value(\n        'tenant',\n        get_configured_provider(), __opts__, search_global=False\n    )\n\n    if tenant is not None:\n        client_id = config.get_cloud_config_value(\n            'client_id',\n            get_configured_provider(), __opts__, search_global=False\n        )\n        secret = config.get_cloud_config_value(\n            'secret',\n            get_configured_provider(), __opts__, search_global=False\n        )\n        conn_kwargs.update({'client_id': client_id, 'secret': secret,\n                            'tenant': tenant})\n\n    username = config.get_cloud_config_value(\n        'username',\n        get_configured_provider(), __opts__, search_global=False\n    )\n\n    if username is not None:\n        password = config.get_cloud_config_value(\n            'password',\n            get_configured_provider(), __opts__, search_global=False\n        )\n        conn_kwargs.update({'username': username, 'password': password})\n\n    client = __utils__['azurearm.get_client'](\n        client_type=client_type, **conn_kwargs\n    )\n\n    return client", "output": "Return a connection object for a client type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fcontext_add_or_delete_policy(action, name, filetype=None, sel_type=None, sel_user=None, sel_level=None):\n    '''\n    \n    '''\n    salt.utils.versions.warn_until(\n        'Sodium',\n        'The \\'selinux.fcontext_add_or_delete_policy\\' module has been deprecated. Please use the '\n        '\\'selinux.fcontext_add_policy\\' and \\'selinux.fcontext_delete_policy\\' modules instead. '\n        'Support for the \\'selinux.fcontext_add_or_delete_policy\\' module will be removed in Salt '\n        '{version}.'\n    )\n    return _fcontext_add_or_delete_policy(action, name, filetype, sel_type, sel_user, sel_level)", "output": ".. versionadded:: 2017.7.0\n\n    Adds or deletes the SELinux policy for a given filespec and other optional parameters.\n\n    Returns the result of the call to semanage.\n\n    Note that you don't have to remove an entry before setting a new\n    one for a given filespec and filetype, as adding one with semanage\n    automatically overwrites a previously configured SELinux context.\n\n    .. warning::\n\n        Use :mod:`selinux.fcontext_add_policy()<salt.modules.selinux.fcontext_add_policy>`,\n        or :mod:`selinux.fcontext_delete_policy()<salt.modules.selinux.fcontext_delete_policy>`.\n\n    .. deprecated:: 2019.2.0\n\n    action\n        The action to perform. Either ``add`` or ``delete``.\n\n    name\n        filespec of the file or directory. Regex syntax is allowed.\n\n    file_type\n        The SELinux filetype specification. Use one of [a, f, d, c, b,\n        s, l, p]. See also ``man semanage-fcontext``. Defaults to 'a'\n        (all files).\n\n    sel_type\n        SELinux context type. There are many.\n\n    sel_user\n        SELinux user. Use ``semanage login -l`` to determine which ones\n        are available to you.\n\n    sel_level\n        The MLS range of the SELinux context.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' selinux.fcontext_add_or_delete_policy add my-policy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hgetall(key, host=None, port=None, db=None, password=None):\n    '''\n    \n    '''\n    server = _connect(host, port, db, password)\n    return server.hgetall(key)", "output": "Get all fields and values from a redis hash, returns dict\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.hgetall foo_hash", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bgsave(host=None, port=None, db=None, password=None):\n    '''\n    \n    '''\n    server = _connect(host, port, db, password)\n    return server.bgsave()", "output": "Asynchronously save the dataset to disk\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.bgsave", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def groupByKey(self, numPartitions=None):\n        \"\"\"\n        \n        \"\"\"\n        if numPartitions is None:\n            numPartitions = self._sc.defaultParallelism\n        return self.transform(lambda rdd: rdd.groupByKey(numPartitions))", "output": "Return a new DStream by applying groupByKey on each RDD.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write(self, value):\n        \"\"\"\n        \n        \"\"\"\n        self.get_collection().update_one(\n            {'_id': self._document_id},\n            {'$set': {self._path: value}},\n            upsert=True\n        )", "output": "Write value to the target", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stringify(data):\n    '''\n    \n    '''\n    ret = []\n    for item in data:\n        if six.PY2 and isinstance(item, str):\n            item = salt.utils.stringutils.to_unicode(item)\n        elif not isinstance(item, six.string_types):\n            item = six.text_type(item)\n        ret.append(item)\n    return ret", "output": "Given an iterable, returns its items as a list, with any non-string items\n    converted to unicode strings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _regex_to_static(src, regex):\n    '''\n    \n    '''\n    if not src or not regex:\n        return None\n\n    try:\n        compiled = re.compile(regex, re.DOTALL)\n        src = [line for line in src if compiled.search(line) or line.count(regex)]\n    except Exception as ex:\n        raise CommandExecutionError(\"{0}: '{1}'\".format(_get_error_message(ex), regex))\n\n    return src and src or []", "output": "Expand regular expression to static match.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _render(template, render, renderer, template_dict, opts):\n    '''\n    \n    '''\n    if render:\n        if template_dict is None:\n            template_dict = {}\n        if not renderer:\n            renderer = opts.get('renderer', 'jinja|yaml')\n        rend = salt.loader.render(opts, {})\n        blacklist = opts.get('renderer_blacklist')\n        whitelist = opts.get('renderer_whitelist')\n        ret = compile_template(template, rend, renderer, blacklist, whitelist, **template_dict)\n        if salt.utils.stringio.is_readable(ret):\n            ret = ret.read()\n        if six.text_type(ret).startswith('#!') and not six.text_type(ret).startswith('#!/'):\n            ret = six.text_type(ret).split('\\n', 1)[1]\n        return ret\n    with salt.utils.files.fopen(template, 'r') as fh_:\n        return fh_.read()", "output": "Render a template", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_spyderplugins_mods(io=False):\r\n    \"\"\"\"\"\"\r\n    # Create user directory\r\n    user_plugin_path = osp.join(get_conf_path(), USER_PLUGIN_DIR)\r\n    if not osp.isdir(user_plugin_path):\r\n        os.makedirs(user_plugin_path)\r\n\r\n    modlist, modnames = [], []\r\n\r\n    # The user plugins directory is given the priority when looking for modules\r\n    for plugin_path in [user_plugin_path] + sys.path:\r\n        _get_spyderplugins(plugin_path, io, modnames, modlist)\r\n    return modlist", "output": "Import modules from plugins package and return the list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _combine_individual_stats(self, operator_count, cv_score, individual_stats):\n        \"\"\"\n        \"\"\"\n        stats = deepcopy(individual_stats)  # Deepcopy, since the string reference to predecessor should be cloned\n        stats['operator_count'] = operator_count\n        stats['internal_cv_score'] = cv_score\n        return stats", "output": "Combine the stats with operator count and cv score and preprare to be written to _evaluated_individuals\n\n        Parameters\n        ----------\n        operator_count: int\n            number of components in the pipeline\n        cv_score: float\n            internal cross validation score\n        individual_stats: dictionary\n            dict containing statistics about the individual. currently:\n            'generation': generation in which the individual was evaluated\n            'mutation_count': number of mutation operations applied to the individual and its predecessor cumulatively\n            'crossover_count': number of crossover operations applied to the individual and its predecessor cumulatively\n            'predecessor': string representation of the individual\n\n        Returns\n        -------\n        stats: dictionary\n            dict containing the combined statistics:\n            'operator_count': number of operators in the pipeline\n            'internal_cv_score': internal cross validation score\n            and all the statistics contained in the 'individual_stats' parameter", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _next_page(self):\n        \"\"\"\n        \"\"\"\n        try:\n            items = six.next(self._gax_page_iter)\n            page = Page(self, items, self.item_to_value)\n            self.next_page_token = self._gax_page_iter.page_token or None\n            return page\n        except StopIteration:\n            return None", "output": "Get the next page in the iterator.\n\n        Wraps the response from the :class:`~google.gax.PageIterator` in a\n        :class:`Page` instance and captures some state at each page.\n\n        Returns:\n            Optional[Page]: The next page in the iterator or :data:`None` if\n                  there are no pages left.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(*args, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    luigi_run_result = _run(*args, **kwargs)\n    return luigi_run_result if kwargs.get('detailed_summary') else luigi_run_result.scheduling_succeeded", "output": "Please dont use. Instead use `luigi` binary.\n\n    Run from cmdline using argparse.\n\n    :param use_dynamic_argparse: Deprecated and ignored", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_scheme(self, scheme_name):\n        \"\"\"\"\"\"\n        self.stack.setCurrentIndex(self.order.index(scheme_name))\n        self.last_used_scheme = scheme_name", "output": "Set the current stack by 'scheme_name'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_redis_client(redis_address, password=None):\n    \"\"\"\n    \"\"\"\n    redis_ip_address, redis_port = redis_address.split(\":\")\n    # For this command to work, some other client (on the same machine\n    # as Redis) must have run \"CONFIG SET protected-mode no\".\n    return redis.StrictRedis(\n        host=redis_ip_address, port=int(redis_port), password=password)", "output": "Create a Redis client.\n\n    Args:\n        The IP address, port, and password of the Redis server.\n\n    Returns:\n        A Redis client.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build(self, grad_list, get_opt_fn):\n        \"\"\"\n        \n        \"\"\"\n        assert len(grad_list) == len(self.towers)\n        DataParallelBuilder._check_grad_list(grad_list)\n\n        if self._scale_gradient and len(self.towers) > 1:\n            # pretend to average the grads, in order to make async and\n            # sync have consistent effective learning rate\n            gradproc = ScaleGradient(('.*', 1.0 / len(self.towers)), verbose=False)\n            grad_list = [gradproc.process(gv) for gv in grad_list]\n        # Ngpu x Nvar x 2\n\n        train_ops = []\n        opt = get_opt_fn()\n        with tf.name_scope('async_apply_gradients'):\n            for i, grad_and_vars in enumerate(zip(*grad_list)):\n                # Ngpu x 2\n                v = grad_and_vars[0][1]\n                with tf.device(v.device):\n                    # will call apply_gradients (therefore gradproc) multiple times\n                    train_ops.append(opt.apply_gradients(\n                        grad_and_vars, name='apply_grad_{}'.format(i)))\n        return tf.group(*train_ops, name='train_op')", "output": "Args:\n            grad_list ([[(grad, var), ...], ...]): #GPU lists to be reduced. Each is the gradients computed on each GPU.\n            get_opt_fn (-> tf.train.Optimizer): callable which returns an optimizer\n\n        Returns:\n            tf.Operation: the training op", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_dfu_devices(*args, **kwargs):\n    \"\"\"\"\"\"\n    devices = get_dfu_devices(*args, **kwargs)\n    if not devices:\n        print(\"No DFU capable devices found\")\n        return\n    for device in devices:\n        print(\"Bus {} Device {:03d}: ID {:04x}:{:04x}\"\n              .format(device.bus, device.address,\n                      device.idVendor, device.idProduct))\n        layout = get_memory_layout(device)\n        print(\"Memory Layout\")\n        for entry in layout:\n            print(\"    0x{:x} {:2d} pages of {:3d}K bytes\"\n                  .format(entry['addr'], entry['num_pages'],\n                          entry['page_size'] // 1024))", "output": "Prints a lits of devices detected in DFU mode.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_package(package, image=None, restart=False):\n    '''\n    \n    '''\n    cmd = ['DISM',\n           '/Quiet',\n           '/Image:{0}'.format(image) if image else '/Online',\n           '/Remove-Package']\n\n    if not restart:\n        cmd.append('/NoRestart')\n\n    if '~' in package:\n        cmd.append('/PackageName:{0}'.format(package))\n    else:\n        cmd.append('/PackagePath:{0}'.format(package))\n\n    return __salt__['cmd.run_all'](cmd)", "output": "Uninstall a package\n\n    Args:\n        package (str): The full path to the package. Can be either a .cab file\n            or a folder. Should point to the original source of the package, not\n            to where the file is installed. This can also be the name of a package as listed in\n            ``dism.installed_packages``\n        image (Optional[str]): The path to the root directory of an offline\n            Windows image. If `None` is passed, the running operating system is\n            targeted. Default is None.\n        restart (Optional[bool]): Reboot the machine if required by the install\n\n    Returns:\n        dict: A dictionary containing the results of the command\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        # Remove the Calc Package\n        salt '*' dism.remove_package Microsoft.Windows.Calc.Demo~6595b6144ccf1df~x86~en~1.0.0.0\n\n        # Remove the package.cab (does not remove C:\\\\packages\\\\package.cab)\n        salt '*' dism.remove_package C:\\\\packages\\\\package.cab", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _addsub_offset_array(self, other, op):\n        \"\"\"\n        \n        \"\"\"\n        assert op in [operator.add, operator.sub]\n        if len(other) == 1:\n            return op(self, other[0])\n\n        warnings.warn(\"Adding/subtracting array of DateOffsets to \"\n                      \"{cls} not vectorized\"\n                      .format(cls=type(self).__name__), PerformanceWarning)\n\n        # For EA self.astype('O') returns a numpy array, not an Index\n        left = lib.values_from_object(self.astype('O'))\n\n        res_values = op(left, np.array(other))\n        kwargs = {}\n        if not is_period_dtype(self):\n            kwargs['freq'] = 'infer'\n        return self._from_sequence(res_values, **kwargs)", "output": "Add or subtract array-like of DateOffset objects\n\n        Parameters\n        ----------\n        other : Index, np.ndarray\n            object-dtype containing pd.DateOffset objects\n        op : {operator.add, operator.sub}\n\n        Returns\n        -------\n        result : same class as self", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve_path(path, config_file):\n        \"\"\"\n\n        \"\"\"\n        if os.path.isabs(path):\n            return path\n        return os.path.relpath(path, os.path.dirname(config_file))", "output": "Resolve path relative to config file location.\n\n        Args:\n            path: Path to be resolved.\n            config_file: Path to config file, which `path` is specified\n                relative to.\n\n        Returns:\n            Path relative to the `config_file` location. If `path` is an\n            absolute path then it will be returned without change.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transform(self, document):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(document, RDD):\n            return document.map(self.transform)\n\n        freq = {}\n        for term in document:\n            i = self.indexOf(term)\n            freq[i] = 1.0 if self.binary else freq.get(i, 0) + 1.0\n        return Vectors.sparse(self.numFeatures, freq.items())", "output": "Transforms the input document (list of terms) to term frequency\n        vectors, or transform the RDD of document to RDD of term\n        frequency vectors.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_attr_tuple_class(cls_name, attr_names):\n    \"\"\"\n    \n    \"\"\"\n    attr_class_name = \"{}Attributes\".format(cls_name)\n    attr_class_template = [\n        \"class {}(tuple):\".format(attr_class_name),\n        \"    __slots__ = ()\",\n    ]\n    if attr_names:\n        for i, attr_name in enumerate(attr_names):\n            attr_class_template.append(\n                _tuple_property_pat.format(index=i, attr_name=attr_name)\n            )\n    else:\n        attr_class_template.append(\"    pass\")\n    globs = {\"_attrs_itemgetter\": itemgetter, \"_attrs_property\": property}\n    eval(compile(\"\\n\".join(attr_class_template), \"\", \"exec\"), globs)\n\n    return globs[attr_class_name]", "output": "Create a tuple subclass to hold `Attribute`s for an `attrs` class.\n\n    The subclass is a bare tuple with properties for names.\n\n    class MyClassAttributes(tuple):\n        __slots__ = ()\n        x = property(itemgetter(0))", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_sizes(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_sizes function must be called with '\n            '-f or --function, or with the --list-sizes option'\n        )\n\n    zone = _get_specified_zone(kwargs, get_configured_provider())\n\n    result = {}\n    for size_key in QINGCLOUD_SIZES[zone]:\n        result[size_key] = {}\n        for attribute_key in QINGCLOUD_SIZES[zone][size_key]:\n            result[size_key][attribute_key] = QINGCLOUD_SIZES[zone][size_key][attribute_key]\n\n    return result", "output": "Return a list of the instance sizes that are on the provider.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-cloud --list-sizes my-qingcloud\n        salt-cloud -f avail_sizes my-qingcloud zone=pek2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cycle_sort(arr):\n    \"\"\"\n    \n    \"\"\"\n    len_arr = len(arr)\n    # Finding cycle to rotate.\n    for cur in range(len_arr - 1):\n        item = arr[cur]\n\n        # Finding an indx to put items in.\n        index = cur\n        for i in range(cur + 1, len_arr):\n            if arr[i] < item:\n                index += 1\n\n        # Case of there is not a cycle\n        if index == cur:\n            continue\n\n        # Putting the item immediately right after the duplicate item or on the right.\n        while item == arr[index]:\n            index += 1\n        arr[index], item = item, arr[index]\n\n        # Rotating the remaining cycle.\n        while index != cur:\n\n            # Finding where to put the item.\n            index = cur\n            for i in range(cur + 1, len_arr):\n                if arr[i] < item:\n                    index += 1\n\n            # After item is duplicated, put it in place or put it there.\n            while item == arr[index]:\n                index += 1\n            arr[index], item = item, arr[index]\n    return arr", "output": "cycle_sort\n    This is based on the idea that the permutations to be sorted\n    can be decomposed into cycles,\n    and the results can be individually sorted by cycling.\n    \n    reference: https://en.wikipedia.org/wiki/Cycle_sort\n    \n    Average time complexity : O(N^2)\n    Worst case time complexity : O(N^2)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_dict(self):\n        \"\"\"\n        \n        \"\"\"\n        identity_dict = {}\n        if self.identity:\n            identity_dict = self.identity.to_dict()\n\n        json_dict = {\"resourceId\": self.resource_id,\n                     \"apiId\": self.api_id,\n                     \"resourcePath\": self.resource_path,\n                     \"httpMethod\": self.http_method,\n                     \"requestId\": self.request_id,\n                     \"accountId\": self.account_id,\n                     \"stage\": self.stage,\n                     \"identity\": identity_dict,\n                     \"extendedRequestId\": self.extended_request_id,\n                     \"path\": self.path\n                     }\n\n        return json_dict", "output": "Constructs an dictionary representation of the RequestContext Object to be used in serializing to JSON\n\n        :return: dict representing the object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do(cmdline, runas=None, env=None):\n    '''\n    \n    '''\n    if not cmdline:\n        # This is a positional argument so this should never happen, but this\n        # will handle cases where someone explicitly passes a false value for\n        # cmdline.\n        raise SaltInvocationError('Command must be specified')\n\n    path = _rbenv_path(runas)\n    if not env:\n        env = {}\n\n    # NOTE: Env vars (and their values) need to be str type on both Python 2\n    # and 3. The code below first normalizes all path components to unicode to\n    # stitch them together, and then converts the result back to a str type.\n    env[str('PATH')] = salt.utils.stringutils.to_str(   # future lint: disable=blacklisted-function\n        os.pathsep.join((\n            salt.utils.path.join(path, 'shims'),\n            salt.utils.stringutils.to_unicode(os.environ['PATH'])\n        ))\n    )\n\n    try:\n        cmdline = salt.utils.args.shlex_split(cmdline)\n    except AttributeError:\n        cmdauth = salt.utils.args.shlex_split(six.text_type(cmdline))\n\n    result = __salt__['cmd.run_all'](\n        cmdline,\n        runas=runas,\n        env=env,\n        python_shell=False\n    )\n\n    if result['retcode'] == 0:\n        rehash(runas=runas)\n        return result['stdout']\n    else:\n        return False", "output": "Execute a ruby command with rbenv's shims from the user or the system\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rbenv.do 'gem list bundler'\n        salt '*' rbenv.do 'gem list bundler' deploy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _all_nsot_devices(api_url, email, secret_key):\n    '''\n    \n    '''\n    token = _get_token(api_url, email, secret_key)\n    all_devices = {}\n    if token:\n        headers = {'Authorization': 'AuthToken {}:{}'.format(email, token)}\n        all_devices = _query_nsot(api_url, headers)\n\n    return all_devices", "output": "retrieve a list of all devices that exist in nsot\n\n    :param api_url: str\n    :param email: str\n    :param secret_key: str\n    :return: dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toggle_auto_import(self, checked):\r\n        \"\"\"\"\"\"\r\n        self.combo.validate_current_text()\r\n        self.set_option('automatic_import', checked)\r\n        self.force_refresh()", "output": "Toggle automatic import feature", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transform(self, dstreams, transformFunc):\n        \"\"\"\n        \n        \"\"\"\n        jdstreams = [d._jdstream for d in dstreams]\n        # change the final serializer to sc.serializer\n        func = TransformFunction(self._sc,\n                                 lambda t, *rdds: transformFunc(rdds),\n                                 *[d._jrdd_deserializer for d in dstreams])\n        jfunc = self._jvm.TransformFunction(func)\n        jdstream = self._jssc.transform(jdstreams, jfunc)\n        return DStream(jdstream, self, self._sc.serializer)", "output": "Create a new DStream in which each RDD is generated by applying\n        a function on RDDs of the DStreams. The order of the JavaRDDs in\n        the transform function parameter will be the same as the order\n        of corresponding DStreams in the list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def layers(name):\n    '''\n    \n    '''\n    ret = []\n    cmd = ['docker', 'history', '-q', name]\n    for line in reversed(\n            __salt__['cmd.run_stdout'](cmd, python_shell=False).splitlines()):\n        ret.append(line)\n    if not ret:\n        raise CommandExecutionError('Image \\'{0}\\' not found'.format(name))\n    return ret", "output": "Returns a list of the IDs of layers belonging to the specified image, with\n    the top-most layer (the one correspnding to the passed name) appearing\n    last.\n\n    name\n        Image name or ID\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion docker.layers centos:7", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cpu_times(per_cpu=False):\n    '''\n    \n    '''\n    if per_cpu:\n        result = [dict(times._asdict()) for times in psutil.cpu_times(True)]\n    else:\n        result = dict(psutil.cpu_times(per_cpu)._asdict())\n    return result", "output": "Return the percent of time the CPU spends in each state,\n    e.g. user, system, idle, nice, iowait, irq, softirq.\n\n    per_cpu\n        if True return an array of percents for each CPU, otherwise aggregate\n        all percents into one number\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.cpu_times", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self, bin, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        bin = self._bin(bin)\n\n        cmd = [bin] + list(args)\n        shell = kwargs.get(\"shell\", False)\n        call = kwargs.pop(\"call\", False)\n        input_ = kwargs.pop(\"input_\", None)\n\n        if shell:\n            cmd = list_to_shell_command(cmd)\n\n        try:\n            if self._is_windows:\n                kwargs[\"shell\"] = True\n\n            if input_:\n                p = subprocess.Popen(\n                    cmd,\n                    stdin=subprocess.PIPE,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.STDOUT,\n                    **kwargs\n                )\n                output = p.communicate(encode(input_))[0]\n            elif call:\n                return subprocess.call(cmd, stderr=subprocess.STDOUT, **kwargs)\n            else:\n                output = subprocess.check_output(\n                    cmd, stderr=subprocess.STDOUT, **kwargs\n                )\n        except CalledProcessError as e:\n            raise EnvCommandError(e)\n\n        return decode(output)", "output": "Run a command inside the Python environment.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _grid_sample(x:TensorImage, coords:FlowField, mode:str='bilinear', padding_mode:str='reflection', remove_out:bool=True)->TensorImage:\n    \"\"\n    coords = coords.flow.permute(0, 3, 1, 2).contiguous().permute(0, 2, 3, 1) # optimize layout for grid_sample\n    if mode=='bilinear': # hack to get smoother downwards resampling\n        mn,mx = coords.min(),coords.max()\n        # max amount we're affine zooming by (>1 means zooming in)\n        z = 1/(mx-mn).item()*2\n        # amount we're resizing by, with 100% extra margin\n        d = min(x.shape[1]/coords.shape[1], x.shape[2]/coords.shape[2])/2\n        # If we're resizing up by >200%, and we're zooming less than that, interpolate first\n        if d>1 and d>z: x = F.interpolate(x[None], scale_factor=1/d, mode='area')[0]\n    return F.grid_sample(x[None], coords, mode=mode, padding_mode=padding_mode)[0]", "output": "Resample pixels in `coords` from `x` by `mode`, with `padding_mode` in ('reflection','border','zeros').", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log_mel_spectrogram(data,\n                        audio_sample_rate=8000,\n                        log_offset=0.0,\n                        window_length_secs=0.025,\n                        hop_length_secs=0.010,\n                        **kwargs):\n  \"\"\"\n  \"\"\"\n  window_length_samples = int(round(audio_sample_rate * window_length_secs))\n  hop_length_samples = int(round(audio_sample_rate * hop_length_secs))\n  fft_length = 2 ** int(np.ceil(np.log(window_length_samples) / np.log(2.0)))\n  spectrogram = stft_magnitude(\n      data,\n      fft_length=fft_length,\n      hop_length=hop_length_samples,\n      window_length=window_length_samples)\n  mel_spectrogram = np.dot(spectrogram, spectrogram_to_mel_matrix(\n      num_spectrogram_bins=spectrogram.shape[1],\n      audio_sample_rate=audio_sample_rate, **kwargs))\n  return np.log(mel_spectrogram + log_offset)", "output": "Convert waveform to a log magnitude mel-frequency spectrogram.\n\n  Args:\n    data: 1D np.array of waveform data.\n    audio_sample_rate: The sampling rate of data.\n    log_offset: Add this to values when taking log to avoid -Infs.\n    window_length_secs: Duration of each window to analyze.\n    hop_length_secs: Advance between successive analysis windows.\n    **kwargs: Additional arguments to pass to spectrogram_to_mel_matrix.\n\n  Returns:\n    2D np.array of (num_frames, num_mel_bins) consisting of log mel filterbank\n    magnitudes for successive frames.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_block(self, block_name):\n        \"\"\"\n        \"\"\"\n        # block_name = [block_name] if isinstance(\n        #     block_name, str) else block_name\n        # return QA_DataStruct_Stock_block(self.data[self.data.blockname.apply(lambda x: x in block_name)])\n\n        return self.new(self.data.loc[(block_name, slice(None)), :])", "output": "getblock \u83b7\u53d6\u677f\u5757, block_name\u662flist\u6216\u8005\u662f\u5355\u4e2astr\n\n        Arguments:\n            block_name {[type]} -- [description]\n\n        Returns:\n            [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_element_by_name(self, name):\n        \"\"\"\n        \n        \"\"\"\n        return self.find_element(by=By.NAME, value=name)", "output": "Finds an element by name.\n\n        :Args:\n         - name: The name of the element to find.\n\n        :Returns:\n         - WebElement - the element if it was found\n\n        :Raises:\n         - NoSuchElementException - if the element wasn't found\n\n        :Usage:\n            ::\n\n                element = driver.find_element_by_name('foo')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mdadm():\n    '''\n    \n    '''\n    devices = set()\n    try:\n        with salt.utils.files.fopen('/proc/mdstat', 'r') as mdstat:\n            for line in mdstat:\n                line = salt.utils.stringutils.to_unicode(line)\n                if line.startswith('Personalities : '):\n                    continue\n                if line.startswith('unused devices:'):\n                    continue\n                if ' : ' in line:\n                    devices.add(line.split(' : ')[0])\n    except IOError:\n        return {}\n\n    devices = sorted(devices)\n    if devices:\n        log.trace('mdadm devices detected: %s', ', '.join(devices))\n\n    return {'mdadm': devices}", "output": "Return list of mdadm devices", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ping(self, data: Union[str, bytes] = b\"\") -> None:\n        \"\"\"\n\n        \"\"\"\n        data = utf8(data)\n        if self.ws_connection is None or self.ws_connection.is_closing():\n            raise WebSocketClosedError()\n        self.ws_connection.write_ping(data)", "output": "Send ping frame to the remote end.\n\n        The data argument allows a small amount of data (up to 125\n        bytes) to be sent as a part of the ping message. Note that not\n        all websocket implementations expose this data to\n        applications.\n\n        Consider using the ``websocket_ping_interval`` application\n        setting instead of sending pings manually.\n\n        .. versionchanged:: 5.1\n\n           The data argument is now optional.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_path_from_index(self, index):\n        \"\"\"\n        \n        \"\"\"\n        assert self.image_set_index is not None, \"Dataset not initialized\"\n        pos = self.image_set_index[index]\n        n_db, n_index = self._locate_index(index)\n        return self.imdbs[n_db].image_path_from_index(n_index)", "output": "given image index, find out full path\n\n        Parameters\n        ----------\n        index: int\n            index of a specific image\n\n        Returns\n        ----------\n        full path of this image", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_graph(self, format='svg'):\n        \"\"\"\n        \n        \"\"\"\n        g = self.to_simple_graph(AssetExists())\n        if format == 'svg':\n            return g.svg\n        elif format == 'png':\n            return g.png\n        elif format == 'jpeg':\n            return g.jpeg\n        else:\n            # We should never get here because of the expect_element decorator\n            # above.\n            raise AssertionError(\"Unknown graph format %r.\" % format)", "output": "Render this Pipeline as a DAG.\n\n        Parameters\n        ----------\n        format : {'svg', 'png', 'jpeg'}\n            Image format to render with.  Default is 'svg'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def finding_path(cls, project, scan_config, scan_run, finding):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/scanConfigs/{scan_config}/scanRuns/{scan_run}/findings/{finding}\",\n            project=project,\n            scan_config=scan_config,\n            scan_run=scan_run,\n            finding=finding,\n        )", "output": "Return a fully-qualified finding string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_step(self, step):\r\n        \"\"\"\"\"\"\r\n        new_tab = self.tab_widget.currentIndex() + step\r\n        assert new_tab < self.tab_widget.count() and new_tab >= 0\r\n        if new_tab == self.tab_widget.count()-1:\r\n            try:\r\n                self.table_widget.open_data(self._get_plain_text(),\r\n                                        self.text_widget.get_col_sep(),\r\n                                        self.text_widget.get_row_sep(),\r\n                                        self.text_widget.trnsp_box.isChecked(),\r\n                                        self.text_widget.get_skiprows(),\r\n                                        self.text_widget.get_comments())\r\n                self.done_btn.setEnabled(True)\r\n                self.done_btn.setDefault(True)\r\n                self.fwd_btn.setEnabled(False)\r\n                self.back_btn.setEnabled(True)\r\n            except (SyntaxError, AssertionError) as error:\r\n                QMessageBox.critical(self, _(\"Import wizard\"),\r\n                            _(\"<b>Unable to proceed to next step</b>\"\r\n                              \"<br><br>Please check your entries.\"\r\n                              \"<br><br>Error message:<br>%s\") % str(error))\r\n                return\r\n        elif new_tab == 0:\r\n            self.done_btn.setEnabled(False)\r\n            self.fwd_btn.setEnabled(True)\r\n            self.back_btn.setEnabled(False)\r\n        self._focus_tab(new_tab)", "output": "Proceed to a given step", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_minions(self,\n                      expr,\n                      tgt_type='glob',\n                      delimiter=DEFAULT_TARGET_DELIM,\n                      greedy=True):\n        '''\n        \n        '''\n\n        try:\n            if expr is None:\n                expr = ''\n            check_func = getattr(self, '_check_{0}_minions'.format(tgt_type), None)\n            if tgt_type in ('grain',\n                             'grain_pcre',\n                             'pillar',\n                             'pillar_pcre',\n                             'pillar_exact',\n                             'compound',\n                             'compound_pillar_exact'):\n                _res = check_func(expr, delimiter, greedy)\n            else:\n                _res = check_func(expr, greedy)\n            _res['ssh_minions'] = False\n            if self.opts.get('enable_ssh_minions', False) is True and isinstance('tgt', six.string_types):\n                roster = salt.roster.Roster(self.opts, self.opts.get('roster', 'flat'))\n                ssh_minions = roster.targets(expr, tgt_type)\n                if ssh_minions:\n                    _res['minions'].extend(ssh_minions)\n                    _res['ssh_minions'] = True\n        except Exception:\n            log.exception(\n                    'Failed matching available minions with %s pattern: %s',\n                    tgt_type, expr)\n            _res = {'minions': [], 'missing': []}\n        return _res", "output": "Check the passed regex against the available minions' public keys\n        stored for authentication. This should return a set of ids which\n        match the regex, this will then be used to parse the returns to\n        make sure everyone has checked back in.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bookmarks_changed(self):\r\n        \"\"\"\"\"\"\r\n        bookmarks = self.editor.get_bookmarks()\r\n        if self.editor.bookmarks != bookmarks:\r\n            self.editor.bookmarks = bookmarks\r\n            self.sig_save_bookmarks.emit(self.filename, repr(bookmarks))", "output": "Bookmarks list has changed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _merge_beam_dim(tensor):\n  \"\"\"\n  \"\"\"\n  shape = common_layers.shape_list(tensor)\n  shape[0] *= shape[1]  # batch -> batch * beam_size\n  shape.pop(1)  # Remove beam dim\n  return tf.reshape(tensor, shape)", "output": "Reshapes first two dimensions in to single dimension.\n\n  Args:\n    tensor: Tensor to reshape of shape [A, B, ...]\n\n  Returns:\n    Reshaped tensor of shape [A*B, ...]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lp_pooling(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    p_value = attrs.get('p', 2)\n    new_attrs = translation_utils._fix_attribute_names(attrs,\n                                                       {'kernel_shape': 'kernel',\n                                                        'strides': 'stride',\n                                                        'pads': 'pad'\n                                                       })\n    new_attrs = translation_utils._remove_attributes(new_attrs, ['p'])\n    new_attrs = translation_utils._add_extra_attributes(new_attrs,\n                                                        {'pooling_convention': 'valid',\n                                                         'p_value': p_value\n                                                        })\n    new_op = translation_utils._fix_pooling('lp', inputs, new_attrs)\n    return new_op, new_attrs, inputs", "output": "LP Pooling", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def call(f):\n    \"\"\"\n    \n    \"\"\"\n    @wraps(f)\n    def processor(func, argname, arg):\n        return f(arg)\n    return processor", "output": "Wrap a function in a processor that calls `f` on the argument before\n    passing it along.\n\n    Useful for creating simple arguments to the `@preprocess` decorator.\n\n    Parameters\n    ----------\n    f : function\n        Function accepting a single argument and returning a replacement.\n\n    Examples\n    --------\n    >>> @preprocess(x=call(lambda x: x + 1))\n    ... def foo(x):\n    ...     return x\n    ...\n    >>> foo(1)\n    2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def serialize(obj, **options):\n    '''\n    \n    '''\n\n    try:\n        if 'fp' in options:\n            return salt.utils.json.dump(obj, _json_module=_json, **options)\n        else:\n            return salt.utils.json.dumps(obj, _json_module=_json, **options)\n    except Exception as error:\n        raise SerializationError(error)", "output": "Serialize Python data to JSON.\n\n    :param obj: the data structure to serialize\n    :param options: options given to lower json/simplejson module.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge_class(base, other):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            other = other.rules\n        except AttributeError:\n            pass\n\n        if not isinstance(other, list):\n            other = [other]\n        other_holidays = {holiday.name: holiday for holiday in other}\n\n        try:\n            base = base.rules\n        except AttributeError:\n            pass\n\n        if not isinstance(base, list):\n            base = [base]\n        base_holidays = {holiday.name: holiday for holiday in base}\n\n        other_holidays.update(base_holidays)\n        return list(other_holidays.values())", "output": "Merge holiday calendars together. The base calendar\n        will take precedence to other. The merge will be done\n        based on each holiday's name.\n\n        Parameters\n        ----------\n        base : AbstractHolidayCalendar\n          instance/subclass or array of Holiday objects\n        other : AbstractHolidayCalendar\n          instance/subclass or array of Holiday objects", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def invoke_lambda_function( self,\n                                function_name,\n                                payload,\n                                invocation_type='Event',\n                                log_type='Tail',\n                                client_context=None,\n                                qualifier=None\n                            ):\n        \"\"\"\n        \n        \"\"\"\n        return self.lambda_client.invoke(\n            FunctionName=function_name,\n            InvocationType=invocation_type,\n            LogType=log_type,\n            Payload=payload\n        )", "output": "Directly invoke a named Lambda function with a payload.\n        Returns the response.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parameter_net(self, theta, kernel_shape=9):\n        \"\"\"\n        \"\"\"\n        with argscope(FullyConnected, nl=tf.nn.leaky_relu):\n            net = FullyConnected('fc1', theta, 64)\n            net = FullyConnected('fc2', net, 128)\n\n        pred_filter = FullyConnected('fc3', net, kernel_shape ** 2, nl=tf.identity)\n        pred_filter = tf.reshape(pred_filter, [BATCH, kernel_shape, kernel_shape, 1], name=\"pred_filter\")\n        logger.info('Parameter net output: {}'.format(pred_filter.get_shape().as_list()))\n        return pred_filter", "output": "Estimate filters for convolution layers\n\n        Args:\n            theta: angle of filter\n            kernel_shape: size of each filter\n\n        Returns:\n            learned filter as [B, k, k, 1]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _run_varnishadm(cmd, params=(), **kwargs):\n    '''\n    \n    '''\n    cmd = ['varnishadm', cmd]\n    cmd.extend([param for param in params if param is not None])\n    log.debug('Executing: %s', ' '.join(cmd))\n    return __salt__['cmd.run_all'](cmd, python_shell=False, **kwargs)", "output": "Execute varnishadm command\n    return the output of the command\n\n    cmd\n        The command to run in varnishadm\n\n    params\n        Any additional args to add to the command line\n\n    kwargs\n        Additional options to pass to the salt cmd.run_all function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _serialize(proto):  # type: (Union[bytes, google.protobuf.message.Message]) -> bytes\n    '''\n    \n    '''\n    if isinstance(proto, bytes):\n        return proto\n    elif hasattr(proto, 'SerializeToString') and callable(proto.SerializeToString):\n        result = proto.SerializeToString()\n        return result\n    else:\n        raise ValueError('No SerializeToString method is detected. '\n                         'neither proto is a str.\\ntype is {}'.format(type(proto)))", "output": "Serialize a in-memory proto to bytes\n\n    @params\n    proto is a in-memory proto, such as a ModelProto, TensorProto, etc\n\n    @return\n    Serialized proto in bytes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toggle_view(self, checked):\r\n        \"\"\"\"\"\"\r\n        if checked:\r\n            self.dockwidget.show()\r\n            self.dockwidget.raise_()\r\n            # Start a client in case there are none shown\r\n            if not self.clients:\r\n                if self.main.is_setting_up:\r\n                    self.create_new_client(give_focus=False)\r\n                else:\r\n                    self.create_new_client(give_focus=True)\r\n        else:\r\n            self.dockwidget.hide()", "output": "Toggle view", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render(self, parsed, page=False, minify=False):\n        \"\"\"\n        \"\"\"\n        rendered = []\n        for i, p in enumerate(parsed):\n            if i == 0:\n                settings = p.get(\"settings\", {})\n                self.direction = settings.get(\"direction\", DEFAULT_DIR)\n                self.lang = settings.get(\"lang\", DEFAULT_LANG)\n            rendered.append(self.render_ents(p[\"text\"], p[\"ents\"], p.get(\"title\")))\n        if page:\n            docs = \"\".join([TPL_FIGURE.format(content=doc) for doc in rendered])\n            markup = TPL_PAGE.format(content=docs, lang=self.lang, dir=self.direction)\n        else:\n            markup = \"\".join(rendered)\n        if minify:\n            return minify_html(markup)\n        return markup", "output": "Render complete markup.\n\n        parsed (list): Dependency parses to render.\n        page (bool): Render parses wrapped as full HTML page.\n        minify (bool): Minify HTML markup.\n        RETURNS (unicode): Rendered HTML markup.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def streams(self):\n        \"\"\"\n        \"\"\"\n        from pyspark.sql.streaming import StreamingQueryManager\n        return StreamingQueryManager(self._ssql_ctx.streams())", "output": "Returns a :class:`StreamingQueryManager` that allows managing all the\n        :class:`StreamingQuery` StreamingQueries active on `this` context.\n\n        .. note:: Evolving.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bind(self, database):\n        \"\"\"\n        \"\"\"\n        self._database = database\n\n        for _ in xrange(self.size):\n            session = self._new_session()\n            session.create()\n            self.put(session)", "output": "Associate the pool with a database.\n\n        :type database: :class:`~google.cloud.spanner_v1.database.Database`\n        :param database: database used by the pool:  used to create sessions\n                         when needed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def RelaxNGSetSchema(self, schema):\n        \"\"\" \"\"\"\n        if schema is None: schema__o = None\n        else: schema__o = schema._o\n        ret = libxml2mod.xmlTextReaderRelaxNGSetSchema(self._o, schema__o)\n        return ret", "output": "Use RelaxNG to validate the document as it is processed.\n          Activation is only possible before the first Read(). if\n          @schema is None, then RelaxNG validation is desactivated. @\n          The @schema should not be freed until the reader is\n           deallocated or its use has been deactivated.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_jdbc_resource(name, server=None, **kwargs):\n    '''\n    \n    '''\n    defaults = {\n        'description': '',\n        'enabled': True,\n        'id': name,\n        'poolName': '',\n        'target': 'server'\n    }\n\n    # Data = defaults + merge kwargs + poolname\n    data = defaults\n    data.update(kwargs)\n\n    if not data['poolName']:\n        raise CommandExecutionError('No pool name!')\n\n    return _create_element(name, 'resources/jdbc-resource', data, server)", "output": "Create a JDBC resource", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_value_from_config(self, section, name):\n        \"\"\"\"\"\"\n\n        conf = configuration.get_config()\n\n        try:\n            value = conf.get(section, name)\n        except (NoSectionError, NoOptionError, KeyError):\n            return _no_value\n\n        return self.parse(value)", "output": "Loads the default from the config. Returns _no_value if it doesn't exist", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bind(self, data_shapes, label_shapes=None, for_training=True,\n             inputs_need_grad=False, force_rebind=False, shared_module=None, grad_req='write'):\n        \"\"\"\n        \"\"\"\n        # force rebinding is typically used when one want to switch from\n        # training to prediction phase.\n        super(SVRGModule, self).bind(data_shapes, label_shapes, for_training, inputs_need_grad, force_rebind,\n                                     shared_module, grad_req)\n\n        if for_training:\n            self._mod_aux.bind(data_shapes, label_shapes, for_training, inputs_need_grad, force_rebind, shared_module,\n                               grad_req)", "output": "Binds the symbols to construct executors for both two modules. This is necessary before one\n        can perform computation with the SVRGModule.\n\n        Parameters\n        ----------\n        data_shapes : list of (str, tuple)\n            Typically is ``data_iter.provide_data``.\n        label_shapes : list of (str, tuple)\n            Typically is ``data_iter.provide_label``.\n        for_training : bool\n            Default is ``True``. Whether the executors should be bound for training.\n        inputs_need_grad : bool\n            Default is ``False``. Whether the gradients to the input data need to be computed.\n            Typically this is not needed. But this might be needed when implementing composition\n            of modules.\n        force_rebind : bool\n            Default is ``False``. This function does nothing if the executors are already\n            bound. But with this ``True``, the executors will be forced to rebind.\n        shared_module : Module\n            Default is ``None``. This is used in bucketing. When not ``None``, the shared module\n            essentially corresponds to a different bucket -- a module with different symbol\n            but with the same sets of parameters (e.g. unrolled RNNs with different lengths).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def netstat():\n    '''\n    \n    '''\n    if __grains__['kernel'] == 'Linux':\n        if not salt.utils.path.which('netstat'):\n            return _ss_linux()\n        else:\n            return _netstat_linux()\n    elif __grains__['kernel'] in ('OpenBSD', 'FreeBSD', 'NetBSD'):\n        return _netstat_bsd()\n    elif __grains__['kernel'] == 'SunOS':\n        return _netstat_sunos()\n    elif __grains__['kernel'] == 'AIX':\n        return _netstat_aix()\n    raise CommandExecutionError('Not yet supported on this platform')", "output": "Return information on open ports and states\n\n    .. note::\n        On BSD minions, the output contains PID info (where available) for each\n        netstat entry, fetched from sockstat/fstat output.\n\n    .. versionchanged:: 2014.1.4\n        Added support for OpenBSD, FreeBSD, and NetBSD\n\n    .. versionchanged:: 2015.8.0\n        Added support for SunOS\n\n    .. versionchanged:: 2016.11.4\n        Added support for AIX\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' network.netstat", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tanh_discrete_unbottleneck(x, hidden_size):\n  \"\"\"\"\"\"\n  x = tf.layers.dense(x, hidden_size, name=\"tanh_discrete_unbottleneck\")\n  return x", "output": "Simple un-discretization from tanh.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def request_instance(vm_, conn=None, call=None):\n    '''\n    \n    '''\n    if call == 'function':\n        # Technically this function may be called other ways too, but it\n        # definitely cannot be called with --function.\n        raise SaltCloudSystemExit(\n            'The request_instance action must be called with -a or --action.'\n        )\n    kwargs = copy.deepcopy(vm_)\n    log.info('Creating Cloud VM %s', vm_['name'])\n    __utils__['cloud.check_name'](vm_['name'], 'a-zA-Z0-9._-')\n    conn = get_conn()\n    userdata = config.get_cloud_config_value(\n        'userdata', vm_, __opts__, search_global=False, default=None\n    )\n    if userdata is not None and os.path.isfile(userdata):\n        try:\n            with __utils__['files.fopen'](userdata, 'r') as fp_:\n                kwargs['userdata'] = __utils__['cloud.userdata_template'](\n                    __opts__, vm_, fp_.read()\n                )\n        except Exception as exc:\n            log.exception(\n                'Failed to read userdata from %s: %s', userdata, exc)\n    if 'size' in kwargs:\n        kwargs['flavor'] = kwargs.pop('size')\n    kwargs['key_name'] = config.get_cloud_config_value(\n        'ssh_key_name', vm_, __opts__, search_global=False, default=None\n    )\n    kwargs['wait'] = True\n    try:\n        conn.create_server(**_clean_create_kwargs(**kwargs))\n    except shade.exc.OpenStackCloudException as exc:\n        log.error('Error creating server %s: %s', vm_['name'], exc)\n        destroy(vm_['name'], conn=conn, call='action')\n        raise SaltCloudSystemExit(six.text_type(exc))\n\n    return show_instance(vm_['name'], conn=conn, call='action')", "output": "Request an instance to be built", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_md5(name, path):\n    '''\n    \n    '''\n    output = run_stdout(name, 'md5sum \"{0}\"'.format(path),\n                        chroot_fallback=True,\n                        ignore_retcode=True)\n    try:\n        return output.split()[0]\n    except IndexError:\n        # Destination file does not exist or could not be accessed\n        return None", "output": "Get the MD5 checksum of a file from a container", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _new_session(self):\n        \"\"\"\n        \"\"\"\n        if self.labels:\n            return self._database.session(labels=self.labels)\n        return self._database.session()", "output": "Helper for concrete methods creating session instances.\n\n        :rtype: :class:`~google.cloud.spanner_v1.session.Session`\n        :returns: new session instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def scale_image(self):\n        \"\"\"\"\"\"\n        new_width = int(self.figcanvas.fwidth *\n                        self._scalestep ** self._scalefactor)\n        new_height = int(self.figcanvas.fheight *\n                         self._scalestep ** self._scalefactor)\n        self.figcanvas.setFixedSize(new_width, new_height)", "output": "Scale the image size.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put(self, key, value, format=None, append=False, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        if format is None:\n            format = get_option(\"io.hdf.default_format\") or 'fixed'\n        kwargs = self._validate_format(format, kwargs)\n        self._write_to_group(key, value, append=append, **kwargs)", "output": "Store object in HDFStore\n\n        Parameters\n        ----------\n        key      : object\n        value    : {Series, DataFrame}\n        format   : 'fixed(f)|table(t)', default is 'fixed'\n            fixed(f) : Fixed format\n                       Fast writing/reading. Not-appendable, nor searchable\n            table(t) : Table format\n                       Write as a PyTables Table structure which may perform\n                       worse but allow more flexible operations like searching\n                       / selecting subsets of the data\n        append   : boolean, default False\n            This will force Table format, append the input data to the\n            existing.\n        data_columns : list of columns to create as data columns, or True to\n            use all columns. See\n            `here <http://pandas.pydata.org/pandas-docs/stable/io.html#query-via-data-columns>`__ # noqa\n        encoding : default None, provide an encoding for strings\n        dropna   : boolean, default False, do not write an ALL nan row to\n            the store settable by the option 'io.hdf.dropna_table'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sharpe(self):\n        \"\"\"\n        \n\n        \"\"\"\n        return round(\n            float(\n                self.calc_sharpe(self.annualize_return,\n                                 self.volatility,\n                                 0.05)\n            ),\n            2\n        )", "output": "\u590f\u666e\u6bd4\u7387", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_timedelta_type(obj):\n    ''' \n\n    '''\n    if isinstance(obj, dt.timedelta):\n        return obj.total_seconds() * 1000.\n    elif isinstance(obj, np.timedelta64):\n        return (obj / NP_MS_DELTA)", "output": "Convert any recognized timedelta value to floating point absolute\n    milliseconds.\n\n    Arg:\n        obj (object) : the object to convert\n\n    Returns:\n        float : milliseconds", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def softmax_average_pooling_class_label_top(body_output,\n                                            targets,\n                                            model_hparams,\n                                            vocab_size):\n  \"\"\"\"\"\"\n  del targets  # unused arg\n  with tf.variable_scope(\n      \"softmax_average_pooling_onehot_class_label_modality_%d_%d\" % (\n          vocab_size, model_hparams.hidden_size)):\n    x = body_output\n    x = tf.reduce_mean(x, axis=1, keepdims=True)\n    return tf.layers.dense(x, vocab_size)", "output": "Loss for class label.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_upsample(builder, layer, input_names, output_names, keras_layer):\n    \"\"\"\n    \"\"\"\n    # Get input and output names\n    input_name, output_name = (input_names[0], output_names[0])\n\n    if isinstance(keras_layer, keras.layers.convolutional.UpSampling1D):\n        fh, fw = 1, keras_layer.length\n    else: # 2D\n        fh, fw = keras_layer.size\n\n    builder.add_upsample(name = layer,\n             scaling_factor_h = fh,\n             scaling_factor_w = fw,\n             input_name = input_name,\n             output_name = output_name)", "output": "Convert convolution layer from keras to coreml.\n\n    Parameters\n    ----------\n    keras_layer: layer\n        A keras layer object.\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tap(self, on_element):\n        \"\"\"\n        \n        \"\"\"\n        self._actions.append(lambda: self._driver.execute(\n            Command.SINGLE_TAP, {'element': on_element.id}))\n        return self", "output": "Taps on a given element.\n\n        :Args:\n         - on_element: The element to tap.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode_request(self, fields, files):\n        \"\"\"\n        \n        \"\"\"\n        # Adapted from packaging, which in turn was adapted from\n        # http://code.activestate.com/recipes/146306\n\n        parts = []\n        boundary = self.boundary\n        for k, values in fields:\n            if not isinstance(values, (list, tuple)):\n                values = [values]\n\n            for v in values:\n                parts.extend((\n                    b'--' + boundary,\n                    ('Content-Disposition: form-data; name=\"%s\"' %\n                     k).encode('utf-8'),\n                    b'',\n                    v.encode('utf-8')))\n        for key, filename, value in files:\n            parts.extend((\n                b'--' + boundary,\n                ('Content-Disposition: form-data; name=\"%s\"; filename=\"%s\"' %\n                 (key, filename)).encode('utf-8'),\n                b'',\n                value))\n\n        parts.extend((b'--' + boundary + b'--', b''))\n\n        body = b'\\r\\n'.join(parts)\n        ct = b'multipart/form-data; boundary=' + boundary\n        headers = {\n            'Content-type': ct,\n            'Content-length': str(len(body))\n        }\n        return Request(self.url, body, headers)", "output": "Encode fields and files for posting to an HTTP server.\n\n        :param fields: The fields to send as a list of (fieldname, value)\n                       tuples.\n        :param files: The files to send as a list of (fieldname, filename,\n                      file_bytes) tuple.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unit_ball_L2(shape):\n  \"\"\"\n  \"\"\"\n  x = tf.Variable(tf.zeros(shape))\n  return constrain_L2(x)", "output": "A tensorflow variable tranfomed to be constrained in a L2 unit ball.\n\n  EXPERIMENTAL: Do not use for adverserial examples if you need to be confident\n  they are strong attacks. We are not yet confident in this code.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_reset_and_type_change(self, name, orig_ctr):\n    \"\"\"\"\"\"\n    # Resetting a hyperparameter\n    if name in orig_ctr:\n      tf.logging.warning(\"Overwriting hparam %s\", name)\n\n    ctr_names = [\n        (self._categorical_params, \"categorical\"),\n        (self._discrete_params, \"discrete\"),\n        (self._float_params, \"float\"),\n        (self._int_params, \"int\"),\n    ]\n    ctrs, names = list(zip(*ctr_names))\n    orig_name = names[ctrs.index(orig_ctr)]\n\n    for ctr, ctr_name in ctr_names:\n      if ctr is orig_ctr:\n        continue\n\n      # Using a different type for the same hyperparameter name\n      if name in ctr:\n        raise ValueError(\"Setting hyperparameter %s as type %s, but a \"\n                         \"hyperparemeter of the same name was originally \"\n                         \"registered as type %s\" % (name, ctr_name, orig_name))", "output": "Check if name is in orig_ctr or in one of the other type containers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _vgg16_data_prep(batch):\n    \"\"\"\n    \n    \"\"\"\n    from mxnet import nd\n    mean = nd.array([123.68, 116.779, 103.939], ctx=batch.context)\n    return nd.broadcast_sub(255 * batch, mean.reshape((-1, 1, 1)))", "output": "Takes images scaled to [0, 1] and returns them appropriately scaled and\n    mean-subtracted for VGG-16", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def worker(workers):\n    \"\"\"\"\"\"\n    logging.info(\n        \"The 'superset worker' command is deprecated. Please use the 'celery \"\n        \"worker' command instead.\")\n    if workers:\n        celery_app.conf.update(CELERYD_CONCURRENCY=workers)\n    elif config.get('SUPERSET_CELERY_WORKERS'):\n        celery_app.conf.update(\n            CELERYD_CONCURRENCY=config.get('SUPERSET_CELERY_WORKERS'))\n\n    worker = celery_app.Worker(optimization='fair')\n    worker.start()", "output": "Starts a Superset worker for async SQL query execution.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cache_file(self, template):\n        '''\n        \n        '''\n        saltpath = salt.utils.url.create(template)\n        self.file_client().get_file(saltpath, '', True, self.saltenv)", "output": "Cache a file from the salt master", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ch_dev(arg_params, aux_params, ctx):\n    \"\"\"\n    \n    \"\"\"\n    new_args = dict()\n    new_auxs = dict()\n    for k, v in arg_params.items():\n        new_args[k] = v.as_in_context(ctx)\n    for k, v in aux_params.items():\n        new_auxs[k] = v.as_in_context(ctx)\n    return new_args, new_auxs", "output": "Changes device of given mxnet arguments\n    :param arg_params: arguments\n    :param aux_params: auxiliary parameters\n    :param ctx: new device context\n    :return: arguments and auxiliary parameters on new device", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_fashion_mnist(directory):\n  \"\"\"\"\"\"\n  # Fashion mnist files have the same names as MNIST.\n  # We must choose a separate name (by adding 'fashion-' prefix) in the tmp_dir.\n  for filename in [\n      _MNIST_TRAIN_DATA_FILENAME, _MNIST_TRAIN_LABELS_FILENAME,\n      _MNIST_TEST_DATA_FILENAME, _MNIST_TEST_LABELS_FILENAME\n  ]:\n    generator_utils.maybe_download(directory,\n                                   _FASHION_MNIST_LOCAL_FILE_PREFIX + filename,\n                                   _FASHION_MNIST_URL + filename)", "output": "Download all FashionMNIST files to directory unless they are there.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_login_failed_count(name):\n    '''\n    \n    '''\n    ret = _get_account_policy_data_value(name, 'failedLoginCount')\n\n    return salt.utils.mac_utils.parse_return(ret)", "output": "Get the the number of failed login attempts\n\n    :param str name: The username of the account\n\n    :return: The number of failed login attempts\n    :rtype: int\n\n    :raises: CommandExecutionError on user not found or any other unknown error\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' shadow.get_login_failed_count admin", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Dropout(x, *args, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    if 'is_training' in kwargs:\n        kwargs['training'] = kwargs.pop('is_training')\n    if len(args) > 0:\n        if args[0] != 0.5:\n            logger.warn(\n                \"The first positional argument to tensorpack.Dropout is the probability to keep, rather than to drop. \"\n                \"This is different from the rate argument in tf.layers.Dropout due to historical reasons. \"\n                \"To mimic tf.layers.Dropout, explicitly use keyword argument 'rate' instead\")\n        rate = 1 - args[0]\n    elif 'keep_prob' in kwargs:\n        assert 'rate' not in kwargs, \"Cannot set both keep_prob and rate!\"\n        rate = 1 - kwargs.pop('keep_prob')\n    elif 'rate' in kwargs:\n        rate = kwargs.pop('rate')\n    else:\n        rate = 0.5\n\n    if kwargs.get('training', None) is None:\n        kwargs['training'] = get_current_tower_context().is_training\n\n    if get_tf_version_tuple() <= (1, 12):\n        return tf.layers.dropout(x, rate=rate, **kwargs)\n    else:\n        return tf.nn.dropout(x, rate=rate if kwargs['training'] else 0.)", "output": "Same as `tf.layers.dropout`.\n    However, for historical reasons, the first positional argument is\n    interpreted as keep_prob rather than drop_prob.\n    Explicitly use `rate=` keyword arguments to ensure things are consistent.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def change_bgcolor_enable(self, state):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        self.dataModel.bgcolor(state)\r\n        self.bgcolor_global.setEnabled(not self.is_series and state > 0)", "output": "This is implementet so column min/max is only active when bgcolor is", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def previous_row(self):\n        \"\"\"\"\"\"\n        row = self.currentIndex().row()\n        rows = self.source_model.rowCount()\n        if row == 0:\n            row = rows\n        self.selectRow(row - 1)", "output": "Move to previous row from currently selected row.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_path_conditional(self, path, condition):\n        \"\"\"\n        \n        \"\"\"\n        self.paths[path] = make_conditional(condition, self.paths[path])", "output": "Wrap entire API path definition in a CloudFormation if condition.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def grab_idx(x,i,batch_first:bool=True):\n    \"\"\n    if batch_first: return ([o[i].cpu() for o in x]   if is_listy(x) else x[i].cpu())\n    else:           return ([o[:,i].cpu() for o in x] if is_listy(x) else x[:,i].cpu())", "output": "Grab the `i`-th batch in `x`, `batch_first` stating the batch dimension.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_listlike_indexer(self, keyarr, kind=None):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(keyarr, Index):\n            keyarr = self._convert_index_indexer(keyarr)\n        else:\n            keyarr = self._convert_arr_indexer(keyarr)\n\n        indexer = self._convert_list_indexer(keyarr, kind=kind)\n        return indexer, keyarr", "output": "Parameters\n        ----------\n        keyarr : list-like\n            Indexer to convert.\n\n        Returns\n        -------\n        indexer : numpy.ndarray or None\n            Return an ndarray or None if cannot convert.\n        keyarr : numpy.ndarray\n            Return tuple-safe keys.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def map(self, fn, *iterables, timeout=None, chunksize=1, prefetch=None):\n        \"\"\"\n        \n        \"\"\"\n        if timeout is not None: end_time = timeout + time.time()\n        if prefetch is None: prefetch = self._max_workers\n        if prefetch < 0: raise ValueError(\"prefetch count may not be negative\")\n        argsiter = zip(*iterables)\n        fs = collections.deque(self.submit(fn, *args) for args in itertools.islice(argsiter, self._max_workers+prefetch))\n        # Yield must be hidden in closure so that the futures are submitted before the first iterator value is required.\n        def result_iterator():\n            nonlocal argsiter\n            try:\n                while fs:\n                    res = fs[0].result() if timeout is None else fs[0].result(end_time-time.time())\n                    # Got a result, future needn't be cancelled\n                    del fs[0]\n                    # Dispatch next task before yielding to keep pipeline full\n                    if argsiter:\n                        try:\n                            args = next(argsiter)\n                        except StopIteration:\n                            argsiter = None\n                        else:\n                            fs.append(self.submit(fn, *args))\n                    yield res\n            finally:\n                for future in fs: future.cancel()\n        return result_iterator()", "output": "Collects iterables lazily, rather than immediately.\n        Docstring same as parent: https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.Executor\n        Implmentation taken from this PR: https://github.com/python/cpython/pull/707", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self):\n        ''''''\n        if self.bucket.get() < 1:\n            return None\n        now = time.time()\n        self.mutex.acquire()\n        try:\n            task = self.priority_queue.get_nowait()\n            self.bucket.desc()\n        except Queue.Empty:\n            self.mutex.release()\n            return None\n        task.exetime = now + self.processing_timeout\n        self.processing.put(task)\n        self.mutex.release()\n        return task.taskid", "output": "Get a task from queue when bucket available", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resource_op_defaults_to(name, op_default, value, extra_args=None, cibname=None):\n    '''\n    \n    '''\n    return _item_present(name=name,\n                         item='resource',\n                         item_id='{0}={1}'.format(op_default, value),\n                         item_type=None,\n                         show=['op', 'defaults'],\n                         create=['op', 'defaults'],\n                         extra_args=extra_args,\n                         cibname=cibname)", "output": "Ensure a resource operation default in the cluster is set to a given value\n\n    Should be run on one cluster node only\n    (there may be races)\n    Can only be run on a node with a functional pacemaker/corosync\n\n    name\n        Irrelevant, not used (recommended: pcs_properties__resource_op_defaults_to_{{op_default}})\n    op_default\n        name of the operation default resource property\n    value\n        value of the operation default resource property\n    extra_args\n        additional options for the pcs command\n    cibname\n        use a cached CIB-file named like cibname instead of the live CIB\n\n    Example:\n\n    .. code-block:: yaml\n\n        pcs_properties__resource_op_defaults_to_monitor-interval:\n            pcs.resource_op_defaults_to:\n                - op_default: monitor-interval\n                - value: 60s\n                - cibname: cib_for_cluster_settings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _constrain_L2_grad(op, grad):\n  \"\"\"\n  \"\"\"\n  inp = op.inputs[0]\n  inp_norm = tf.norm(inp)\n  unit_inp = inp / inp_norm\n\n  grad_projection = dot(unit_inp, grad)\n  parallel_grad = unit_inp * grad_projection\n\n  is_in_ball = tf.less_equal(inp_norm, 1)\n  is_pointed_inward = tf.less(grad_projection, 0)\n  allow_grad = tf.logical_or(is_in_ball, is_pointed_inward)\n  clip_grad = tf.logical_not(allow_grad)\n\n  clipped_grad = tf.cond(clip_grad, lambda: grad - parallel_grad, lambda: grad)\n\n  return clipped_grad", "output": "Gradient for constrained optimization on an L2 unit ball.\n\n  This function projects the gradient onto the ball if you are on the boundary\n  (or outside!), but leaves it untouched if you are inside the ball.\n\n  Args:\n    op: the tensorflow op we're computing the gradient for.\n    grad: gradient we need to backprop\n\n  Returns:\n    (projected if necessary) gradient.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe_thing_type(thingTypeName,\n    region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        res = conn.describe_thing_type(thingTypeName=thingTypeName)\n        if res:\n            res.pop('ResponseMetadata', None)\n            thingTypeMetadata = res.get('thingTypeMetadata')\n            if thingTypeMetadata:\n                for dtype in ('creationDate', 'deprecationDate'):\n                    dval = thingTypeMetadata.get(dtype)\n                    if dval and isinstance(dval, datetime.date):\n                        thingTypeMetadata[dtype] = '{0}'.format(dval)\n            return {'thing_type': res}\n        else:\n            return {'thing_type': None}\n    except ClientError as e:\n        err = __utils__['boto3.get_error'](e)\n        if e.response.get('Error', {}).get('Code') == 'ResourceNotFoundException':\n            return {'thing_type': None}\n        return {'error': err}", "output": "Given a thing type name describe its properties.\n\n    Returns a dictionary of interesting properties.\n\n    .. versionadded:: 2016.11.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iot.describe_thing_type mythingtype", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enable(name, **kwargs):\n    '''\n    \n    '''\n    osmajor = _osrel()[0]\n    if osmajor < '6':\n        cmd = 'update-rc.d -f {0} defaults 99'.format(_cmd_quote(name))\n    else:\n        cmd = 'update-rc.d {0} enable'.format(_cmd_quote(name))\n    try:\n        if int(osmajor) >= 6:\n            cmd = 'insserv {0} && '.format(_cmd_quote(name)) + cmd\n    except ValueError:\n        osrel = _osrel()\n        if osrel == 'testing/unstable' or osrel == 'unstable' or osrel.endswith(\"/sid\"):\n            cmd = 'insserv {0} && '.format(_cmd_quote(name)) + cmd\n    return not __salt__['cmd.retcode'](cmd, python_shell=True)", "output": "Enable the named service to start at boot\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.enable <service name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(check_time: int = 500) -> None:\n    \"\"\"\n    \"\"\"\n    io_loop = ioloop.IOLoop.current()\n    if io_loop in _io_loops:\n        return\n    _io_loops[io_loop] = True\n    if len(_io_loops) > 1:\n        gen_log.warning(\"tornado.autoreload started more than once in the same process\")\n    modify_times = {}  # type: Dict[str, float]\n    callback = functools.partial(_reload_on_update, modify_times)\n    scheduler = ioloop.PeriodicCallback(callback, check_time)\n    scheduler.start()", "output": "Begins watching source files for changes.\n\n    .. versionchanged:: 5.0\n       The ``io_loop`` argument (deprecated since version 4.1) has been removed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def selection(x_bounds,\n              x_types,\n              clusteringmodel_gmm_good,\n              clusteringmodel_gmm_bad,\n              minimize_starting_points,\n              minimize_constraints_fun=None):\n    '''\n    \n    '''\n    results = lib_acquisition_function.next_hyperparameter_lowest_mu(\\\n                    _ratio_scores, [clusteringmodel_gmm_good, clusteringmodel_gmm_bad],\\\n                    x_bounds, x_types, minimize_starting_points, \\\n                    minimize_constraints_fun=minimize_constraints_fun)\n\n    return results", "output": "Select the lowest mu value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def textrank(self, sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'), withFlag=False):\n        \"\"\"\n        \n        \"\"\"\n        self.pos_filt = frozenset(allowPOS)\n        g = UndirectWeightedGraph()\n        cm = defaultdict(int)\n        words = tuple(self.tokenizer.cut(sentence))\n        for i, wp in enumerate(words):\n            if self.pairfilter(wp):\n                for j in xrange(i + 1, i + self.span):\n                    if j >= len(words):\n                        break\n                    if not self.pairfilter(words[j]):\n                        continue\n                    if allowPOS and withFlag:\n                        cm[(wp, words[j])] += 1\n                    else:\n                        cm[(wp.word, words[j].word)] += 1\n\n        for terms, w in cm.items():\n            g.addEdge(terms[0], terms[1], w)\n        nodes_rank = g.rank()\n        if withWeight:\n            tags = sorted(nodes_rank.items(), key=itemgetter(1), reverse=True)\n        else:\n            tags = sorted(nodes_rank, key=nodes_rank.__getitem__, reverse=True)\n\n        if topK:\n            return tags[:topK]\n        else:\n            return tags", "output": "Extract keywords from sentence using TextRank algorithm.\n        Parameter:\n            - topK: return how many top keywords. `None` for all possible words.\n            - withWeight: if True, return a list of (word, weight);\n                          if False, return a list of words.\n            - allowPOS: the allowed POS list eg. ['ns', 'n', 'vn', 'v'].\n                        if the POS of w is not in this list, it will be filtered.\n            - withFlag: if True, return a list of pair(word, weight) like posseg.cut\n                        if False, return a list of words", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resume_experiment(args):\n    ''''''\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    experiment_id = None\n    experiment_endTime = None\n    #find the latest stopped experiment\n    if not args.id:\n        print_error('Please set experiment id! \\nYou could use \\'nnictl resume {id}\\' to resume a stopped experiment!\\n' \\\n        'You could use \\'nnictl experiment list all\\' to show all of stopped experiments!')\n        exit(1)\n    else:\n        if experiment_dict.get(args.id) is None:\n            print_error('Id %s not exist!' % args.id)\n            exit(1)\n        if experiment_dict[args.id]['status'] != 'STOPPED':\n            print_error('Experiment %s is running!' % args.id)\n            exit(1)\n        experiment_id = args.id\n    print_normal('Resuming experiment %s...' % experiment_id)\n    nni_config = Config(experiment_dict[experiment_id]['fileName'])\n    experiment_config = nni_config.get_config('experimentConfig')\n    experiment_id = nni_config.get_config('experimentId')\n    new_config_file_name = ''.join(random.sample(string.ascii_letters + string.digits, 8))\n    new_nni_config = Config(new_config_file_name)\n    new_nni_config.set_config('experimentConfig', experiment_config)\n    launch_experiment(args, experiment_config, 'resume', new_config_file_name, experiment_id)\n    new_nni_config.set_config('restServerPort', args.port)", "output": "resume an experiment", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def verify_pubkey_sig(self, message, sig):\n        '''\n        \n        '''\n        if self.opts['master_sign_key_name']:\n            path = os.path.join(self.opts['pki_dir'],\n                                self.opts['master_sign_key_name'] + '.pub')\n\n            if os.path.isfile(path):\n                res = verify_signature(path,\n                                       message,\n                                       binascii.a2b_base64(sig))\n            else:\n                log.error(\n                    'Verification public key %s does not exist. You need to '\n                    'copy it from the master to the minions pki directory',\n                    os.path.basename(path)\n                )\n                return False\n            if res:\n                log.debug(\n                    'Successfully verified signature of master public key '\n                    'with verification public key %s',\n                    self.opts['master_sign_key_name'] + '.pub'\n                )\n                return True\n            else:\n                log.debug('Failed to verify signature of public key')\n                return False\n        else:\n            log.error(\n                'Failed to verify the signature of the message because the '\n                'verification key-pairs name is not defined. Please make '\n                'sure that master_sign_key_name is defined.'\n            )\n            return False", "output": "Wraps the verify_signature method so we have\n        additional checks.\n\n        :rtype: bool\n        :return: Success or failure of public key verification", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_start_of_function(text):\r\n    \"\"\"\"\"\"\r\n    if isinstance(text, str) or isinstance(text, unicode):\r\n        function_prefix = ['def', 'async def']\r\n        text = text.lstrip()\r\n\r\n        for prefix in function_prefix:\r\n            if text.startswith(prefix):\r\n                return True\r\n\r\n    return False", "output": "Return True if text is the beginning of the function definition.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_(uri, value):\n    '''\n    \n    '''\n    return salt.utils.sdb.sdb_set(uri, value, __opts__, __utils__)", "output": "Set a value in a db, using a uri in the form of ``sdb://<profile>/<key>``.\n    If the uri provided does not start with ``sdb://`` or the value is not\n    successfully set, return ``False``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sdb.set sdb://mymemcached/foo bar", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_empty(cls, path:PathOrStr, fn:PathOrStr):\n        \"\"\n        return cls.load_state(path, pickle.load(open(Path(path)/fn, 'rb')))", "output": "Load the state in `fn` to create an empty `LabelList` for inference.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dedup(l, suffix='__', case_sensitive=True):\n    \"\"\"\n    \"\"\"\n    new_l = []\n    seen = {}\n    for s in l:\n        s_fixed_case = s if case_sensitive else s.lower()\n        if s_fixed_case in seen:\n            seen[s_fixed_case] += 1\n            s += suffix + str(seen[s_fixed_case])\n        else:\n            seen[s_fixed_case] = 0\n        new_l.append(s)\n    return new_l", "output": "De-duplicates a list of string by suffixing a counter\n\n    Always returns the same number of entries as provided, and always returns\n    unique values. Case sensitive comparison by default.\n\n    >>> print(','.join(dedup(['foo', 'bar', 'bar', 'bar', 'Bar'])))\n    foo,bar,bar__1,bar__2,Bar\n    >>> print(','.join(dedup(['foo', 'bar', 'bar', 'bar', 'Bar'], case_sensitive=False)))\n    foo,bar,bar__1,bar__2,Bar__3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def duplicated(self, subset=None, keep='first'):\n        \"\"\"\n        \n        \"\"\"\n        from pandas.core.sorting import get_group_index\n        from pandas._libs.hashtable import duplicated_int64, _SIZE_HINT_LIMIT\n\n        if self.empty:\n            return Series(dtype=bool)\n\n        def f(vals):\n            labels, shape = algorithms.factorize(\n                vals, size_hint=min(len(self), _SIZE_HINT_LIMIT))\n            return labels.astype('i8', copy=False), len(shape)\n\n        if subset is None:\n            subset = self.columns\n        elif (not np.iterable(subset) or\n              isinstance(subset, str) or\n              isinstance(subset, tuple) and subset in self.columns):\n            subset = subset,\n\n        # Verify all columns in subset exist in the queried dataframe\n        # Otherwise, raise a KeyError, same as if you try to __getitem__ with a\n        # key that doesn't exist.\n        diff = Index(subset).difference(self.columns)\n        if not diff.empty:\n            raise KeyError(diff)\n\n        vals = (col.values for name, col in self.iteritems()\n                if name in subset)\n        labels, shape = map(list, zip(*map(f, vals)))\n\n        ids = get_group_index(labels, shape, sort=False, xnull=False)\n        return Series(duplicated_int64(ids, keep), index=self.index)", "output": "Return boolean Series denoting duplicate rows, optionally only\n        considering certain columns.\n\n        Parameters\n        ----------\n        subset : column label or sequence of labels, optional\n            Only consider certain columns for identifying duplicates, by\n            default use all of the columns\n        keep : {'first', 'last', False}, default 'first'\n            - ``first`` : Mark duplicates as ``True`` except for the\n              first occurrence.\n            - ``last`` : Mark duplicates as ``True`` except for the\n              last occurrence.\n            - False : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        Series", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_flat_size(self):\n        \"\"\"\n        \"\"\"\n        return sum(\n            np.prod(v.get_shape().as_list()) for v in self.variables.values())", "output": "Returns the total length of all of the flattened variables.\n\n        Returns:\n            The length of all flattened variables concatenated.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def url2path(url, data=True, ext:str='.tgz'):\n    \"\"\n    name = url2name(url)\n    return datapath4file(name, ext=ext, archive=False) if data else modelpath4file(name, ext=ext)", "output": "Change `url` to a path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _load_init(self, data, ctx):\n        \"\"\"\"\"\"\n        if self.shape:\n            for self_dim, data_dim in zip(self.shape, data.shape):\n                assert self_dim in (0, data_dim), \\\n                    \"Failed loading Parameter '%s' from saved params: \" \\\n                    \"shape incompatible expected %s vs saved %s\"%(\n                        self.name, str(self.shape), str(data.shape))\n            self.shape = tuple(i if i != 0 else j for i, j in zip(self.shape, data.shape))\n        if self.dtype:\n            assert np.dtype(self.dtype).type == data.dtype, \\\n                \"Failed loading Parameter '%s' from saved params: \" \\\n                \"dtype incompatible expected %s vs saved %s\"%(\n                    self.name, str(self.dtype), str(data.dtype))\n        if self._stype != data.stype:\n            data = data.tostype(self._stype)\n        if isinstance(ctx, Context):\n            ctx = [ctx]\n        if self._data is None:\n            if self._deferred_init:\n                assert ctx is None or set(ctx) == set(self._deferred_init[1]), \\\n                    \"Failed to load Parameter '%s' on %s because it was \" \\\n                    \"previous initialized on %s.\"%(\n                        self.name, str(ctx), str(self.list_ctx()))\n                ctx = self._deferred_init[1]\n            elif ctx is None:\n                ctx = [cpu()]\n            self._init_impl(data, ctx)\n        else:\n            assert ctx is None or set(ctx) == set(self.list_ctx()), \\\n                \"Failed to load Parameter '%s' on %s because it was \" \\\n                \"previous initialized on %s.\"%(\n                    self.name, str(ctx), str(self.list_ctx()))\n            self.set_data(data)\n        self._deferred_init = ()", "output": "(Re)initializes by loading from data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dragEnterEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        mimeData = event.mimeData()\r\n        formats = list(mimeData.formats())\r\n\r\n        if \"parent-id\" in formats and \\\r\n          int(mimeData.data(\"parent-id\")) == id(self.ancestor):\r\n            event.acceptProposedAction()\r\n\r\n        QTabBar.dragEnterEvent(self, event)", "output": "Override Qt method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_tokens(self, indices):\n        \"\"\"\n        \"\"\"\n\n        to_reduce = False\n        if not isinstance(indices, (list, tuple)):\n            indices = [indices]\n            to_reduce = True\n\n        max_idx = len(self._idx_to_token) - 1\n\n        tokens = []\n        for idx in indices:\n            if not isinstance(idx, int) or idx > max_idx:\n                raise ValueError('Token index {} in the provided `indices` is invalid.'.format(idx))\n            else:\n                tokens.append(self._idx_to_token[idx])\n\n        return tokens[0] if to_reduce else tokens", "output": "Converts token indices to tokens according to the vocabulary.\n\n\n        Parameters\n        ----------\n        indices : int or list of ints\n            A source token index or token indices to be converted.\n\n\n        Returns\n        -------\n        str or list of strs\n            A token or a list of tokens according to the vocabulary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_matched_host_line_numbers(lines, enc):\n    '''\n    \n    '''\n    enc = enc if enc else \"rsa\"\n    for i, line in enumerate(lines):\n        if i % 2 == 0:\n            line_no = int(line.strip().split()[-1])\n            line_enc = lines[i + 1].strip().split()[-2]\n            if line_enc != enc:\n                continue\n            yield line_no", "output": "Helper function which parses ssh-keygen -F function output and yield line\n    number of known_hosts entries with encryption key type matching enc,\n    one by one.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def end_input(self, cmd):\r\n        \"\"\"\"\"\"\r\n        self.input_mode = False\r\n        self.input_loop.exit()\r\n        self.interpreter.widget_proxy.end_input(cmd)", "output": "End of wait_input mode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main():\n    \"\"\"\"\"\"\n    parser = argparse.ArgumentParser(description='.caffemodel to MXNet .params converter.')\n    parser.add_argument('caffemodel', help='Path to the .caffemodel file to convert.')\n    parser.add_argument('output_file_name', help='Name of the output .params file.')\n\n    args = parser.parse_args()\n\n    converter = CaffeModelConverter()\n    converter.convert(args.caffemodel, args.output_file_name)", "output": "Read .caffemodel path and .params path as input from command line\n    and use CaffeModelConverter to do the conversion", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def loadNumpyAnnotations(self, data):\n        \"\"\"\n        \n        \"\"\"\n        print('Converting ndarray to lists...')\n        assert(type(data) == np.ndarray)\n        print(data.shape)\n        assert(data.shape[1] == 7)\n        N = data.shape[0]\n        ann = []\n        for i in range(N):\n            if i % 1000000 == 0:\n                print('{}/{}'.format(i,N))\n            ann += [{\n                'image_id'  : int(data[i, 0]),\n                'bbox'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\n                'score' : data[i, 5],\n                'category_id': int(data[i, 6]),\n                }]\n        return ann", "output": "Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\n        :param  data (numpy.ndarray)\n        :return: annotations (python nested list)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _log_config_on_step(self, trial_state, new_state, trial,\n                            trial_to_clone, new_config):\n        \"\"\"\n        \"\"\"\n        trial_name, trial_to_clone_name = (trial_state.orig_tag,\n                                           new_state.orig_tag)\n        trial_id = \"\".join(itertools.takewhile(str.isdigit, trial_name))\n        trial_to_clone_id = \"\".join(\n            itertools.takewhile(str.isdigit, trial_to_clone_name))\n        trial_path = os.path.join(trial.local_dir,\n                                  \"pbt_policy_\" + trial_id + \".txt\")\n        trial_to_clone_path = os.path.join(\n            trial_to_clone.local_dir,\n            \"pbt_policy_\" + trial_to_clone_id + \".txt\")\n        policy = [\n            trial_name, trial_to_clone_name,\n            trial.last_result[TRAINING_ITERATION],\n            trial_to_clone.last_result[TRAINING_ITERATION],\n            trial_to_clone.config, new_config\n        ]\n        # Log to global file.\n        with open(os.path.join(trial.local_dir, \"pbt_global.txt\"), \"a+\") as f:\n            f.write(json.dumps(policy) + \"\\n\")\n        # Overwrite state in target trial from trial_to_clone.\n        if os.path.exists(trial_to_clone_path):\n            shutil.copyfile(trial_to_clone_path, trial_path)\n        # Log new exploit in target trial log.\n        with open(trial_path, \"a+\") as f:\n            f.write(json.dumps(policy) + \"\\n\")", "output": "Logs transition during exploit/exploit step.\n\n        For each step, logs: [target trial tag, clone trial tag, target trial\n        iteration, clone trial iteration, old config, new config].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _do_download(\n        self, transport, file_obj, download_url, headers, start=None, end=None\n    ):\n        \"\"\"\n        \"\"\"\n        if self.chunk_size is None:\n            download = Download(\n                download_url, stream=file_obj, headers=headers, start=start, end=end\n            )\n            download.consume(transport)\n        else:\n            download = ChunkedDownload(\n                download_url,\n                self.chunk_size,\n                file_obj,\n                headers=headers,\n                start=start if start else 0,\n                end=end,\n            )\n\n            while not download.finished:\n                download.consume_next_chunk(transport)", "output": "Perform a download without any error handling.\n\n        This is intended to be called by :meth:`download_to_file` so it can\n        be wrapped with error handling / remapping.\n\n        :type transport:\n            :class:`~google.auth.transport.requests.AuthorizedSession`\n        :param transport: The transport (with credentials) that will\n                          make authenticated requests.\n\n        :type file_obj: file\n        :param file_obj: A file handle to which to write the blob's data.\n\n        :type download_url: str\n        :param download_url: The URL where the media can be accessed.\n\n        :type headers: dict\n        :param headers: Optional headers to be sent with the request(s).\n\n        :type start: int\n        :param start: Optional, the first byte in a range to be downloaded.\n\n        :type end: int\n        :param end: Optional, The last byte in a range to be downloaded.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_grpc_status(status_code, message, **kwargs):\n    \"\"\"\n    \"\"\"\n    error_class = exception_class_for_grpc_status(status_code)\n    error = error_class(message, **kwargs)\n\n    if error.grpc_status_code is None:\n        error.grpc_status_code = status_code\n\n    return error", "output": "Create a :class:`GoogleAPICallError` from a :class:`grpc.StatusCode`.\n\n    Args:\n        status_code (grpc.StatusCode): The gRPC status code.\n        message (str): The exception message.\n        kwargs: Additional arguments passed to the :class:`GoogleAPICallError`\n            constructor.\n\n    Returns:\n        GoogleAPICallError: An instance of the appropriate subclass of\n            :class:`GoogleAPICallError`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self):\n        ''''''\n        now = time.time()\n        if self.bucket >= self.burst:\n            self.last_update = now\n            return self.bucket\n        bucket = self.rate * (now - self.last_update)\n        self.mutex.acquire()\n        if bucket > 1:\n            self.bucket += bucket\n            if self.bucket > self.burst:\n                self.bucket = self.burst\n            self.last_update = now\n        self.mutex.release()\n        return self.bucket", "output": "Get the number of tokens in bucket", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render(template, saltenv='base', sls='', tmplpath=None, **kws):\n    '''\n    \n    '''\n    template = tmplpath\n    if not os.path.isfile(template):\n        raise SaltRenderError('Template {0} is not a file!'.format(template))\n\n    tmp_data = salt.utils.templates.py(\n            template,\n            True,\n            __salt__=__salt__,\n            salt=__salt__,\n            __grains__=__grains__,\n            grains=__grains__,\n            __opts__=__opts__,\n            opts=__opts__,\n            __pillar__=__pillar__,\n            pillar=__pillar__,\n            __env__=saltenv,\n            saltenv=saltenv,\n            __sls__=sls,\n            sls=sls,\n            **kws)\n    if not tmp_data.get('result', False):\n        raise SaltRenderError(tmp_data.get('data',\n            'Unknown render error in py renderer'))\n\n    return tmp_data['data']", "output": "Render the python module's components\n\n    :rtype: string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_experiments(experiments,\n                    search_alg=None,\n                    scheduler=None,\n                    with_server=False,\n                    server_port=TuneServer.DEFAULT_PORT,\n                    verbose=2,\n                    resume=False,\n                    queue_trials=False,\n                    reuse_actors=False,\n                    trial_executor=None,\n                    raise_on_failed_trial=True):\n    \"\"\"\n\n    \"\"\"\n    # This is important to do this here\n    # because it schematize the experiments\n    # and it conducts the implicit registration.\n    experiments = convert_to_experiment_list(experiments)\n\n    trials = []\n    for exp in experiments:\n        trials += run(\n            exp,\n            search_alg=search_alg,\n            scheduler=scheduler,\n            with_server=with_server,\n            server_port=server_port,\n            verbose=verbose,\n            resume=resume,\n            queue_trials=queue_trials,\n            reuse_actors=reuse_actors,\n            trial_executor=trial_executor,\n            raise_on_failed_trial=raise_on_failed_trial)\n    return trials", "output": "Runs and blocks until all trials finish.\n\n    Examples:\n        >>> experiment_spec = Experiment(\"experiment\", my_func)\n        >>> run_experiments(experiments=experiment_spec)\n\n        >>> experiment_spec = {\"experiment\": {\"run\": my_func}}\n        >>> run_experiments(experiments=experiment_spec)\n\n        >>> run_experiments(\n        >>>     experiments=experiment_spec,\n        >>>     scheduler=MedianStoppingRule(...))\n\n        >>> run_experiments(\n        >>>     experiments=experiment_spec,\n        >>>     search_alg=SearchAlgorithm(),\n        >>>     scheduler=MedianStoppingRule(...))\n\n    Returns:\n        List of Trial objects, holding data for each executed trial.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dataset_list_files(self, dataset):\n        \"\"\" \n        \"\"\"\n        if dataset is None:\n            raise ValueError('A dataset must be specified')\n        if '/' in dataset:\n            self.validate_dataset_string(dataset)\n            dataset_urls = dataset.split('/')\n            owner_slug = dataset_urls[0]\n            dataset_slug = dataset_urls[1]\n        else:\n            owner_slug = self.get_config_value(self.CONFIG_NAME_USER)\n            dataset_slug = dataset\n        dataset_list_files_result = self.process_response(\n            self.datasets_list_files_with_http_info(\n                owner_slug=owner_slug, dataset_slug=dataset_slug))\n        return ListFilesResult(dataset_list_files_result)", "output": "list files for a dataset\n             Parameters\n            ==========\n            dataset: the string identified of the dataset\n                     should be in format [owner]/[dataset-name]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_img(path):\n    \"\"\" \"\"\"\n    img = cv2.resize(cv2.imread(path, 0), (80, 30)).astype(np.float32) / 255\n    img = np.expand_dims(img.transpose(1, 0), 0)\n    return img", "output": "Reads image specified by path into numpy.ndarray", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_adaptive_int(self, n):\n        \"\"\"\n        \n        \"\"\"\n        if n >= Message.big_int:\n            self.packet.write(max_byte)\n            self.add_string(util.deflate_long(n))\n        else:\n            self.packet.write(struct.pack(\">I\", n))\n        return self", "output": "Add an integer to the stream.\n\n        :param int n: integer to add", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_unmask(name, unmask, unmask_runtime, root=None):\n    '''\n    \n    '''\n    if unmask:\n        unmask_(name, runtime=False, root=root)\n    if unmask_runtime:\n        unmask_(name, runtime=True, root=root)", "output": "Common code for conditionally removing masks before making changes to a\n    service's state.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _run_callback(self, callback: Callable[[], Any]) -> None:\n        \"\"\"\n        \"\"\"\n        try:\n            ret = callback()\n            if ret is not None:\n                from tornado import gen\n\n                # Functions that return Futures typically swallow all\n                # exceptions and store them in the Future.  If a Future\n                # makes it out to the IOLoop, ensure its exception (if any)\n                # gets logged too.\n                try:\n                    ret = gen.convert_yielded(ret)\n                except gen.BadYieldError:\n                    # It's not unusual for add_callback to be used with\n                    # methods returning a non-None and non-yieldable\n                    # result, which should just be ignored.\n                    pass\n                else:\n                    self.add_future(ret, self._discard_future_result)\n        except asyncio.CancelledError:\n            pass\n        except Exception:\n            app_log.error(\"Exception in callback %r\", callback, exc_info=True)", "output": "Runs a callback with error handling.\n\n        .. versionchanged:: 6.0\n\n           CancelledErrors are no longer logged.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _decode_batch_input_fn(num_decode_batches, sorted_inputs, vocabulary,\n                           batch_size, max_input_size,\n                           task_id=-1, has_input=True):\n  \"\"\"\"\"\"\n  tf.logging.info(\" batch %d\" % num_decode_batches)\n  for b in range(num_decode_batches):\n    tf.logging.info(\"Decoding batch %d\" % b)\n    batch_length = 0\n    batch_inputs = []\n    for inputs in sorted_inputs[b * batch_size:(b + 1) * batch_size]:\n      input_ids = vocabulary.encode(inputs)\n      if max_input_size > 0:\n        # Subtract 1 for the EOS_ID.\n        input_ids = input_ids[:max_input_size - 1]\n      if has_input or task_id > -1:  # Do not append EOS for pure LM tasks.\n        final_id = text_encoder.EOS_ID if task_id < 0 else task_id\n        input_ids.append(final_id)\n      batch_inputs.append(input_ids)\n      if len(input_ids) > batch_length:\n        batch_length = len(input_ids)\n    final_batch_inputs = []\n    for input_ids in batch_inputs:\n      assert len(input_ids) <= batch_length\n      x = input_ids + [0] * (batch_length - len(input_ids))\n      final_batch_inputs.append(x)\n\n    yield {\n        \"inputs\": np.array(final_batch_inputs).astype(np.int32),\n    }", "output": "Generator to produce batches of inputs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _split_tensor_name(tensor_name):\n  \"\"\"\"\"\"\n  result = re.match(r\"(.*):(\\d+)$\", tensor_name)\n  if not result:\n    raise ValueError(\n        \"Unexpected format for tensor name. Expected node_name:output_number. \"\n        \"Got %r\" % tensor_name)\n  return result.group(1), int(result.group(2))", "output": "Given a tensor name as node_name:output_number, returns both parts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transform(self, func):\n        \"\"\"\n        \"\"\"\n        result = func(self)\n        assert isinstance(result, DataFrame), \"Func returned an instance of type [%s], \" \\\n                                              \"should have been DataFrame.\" % type(result)\n        return result", "output": "Returns a new class:`DataFrame`. Concise syntax for chaining custom transformations.\n\n        :param func: a function that takes and returns a class:`DataFrame`.\n\n        >>> from pyspark.sql.functions import col\n        >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n        >>> def cast_all_to_int(input_df):\n        ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n        >>> def sort_columns_asc(input_df):\n        ...     return input_df.select(*sorted(input_df.columns))\n        >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n        +-----+---+\n        |float|int|\n        +-----+---+\n        |    1|  1|\n        |    2|  2|\n        +-----+---+", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xception(c, k=8, n_middle=8):\n    \"\"\n    layers = [\n        conv(3, k*4, 3, 2),\n        conv(k*4, k*8, 3),\n        ConvSkip(k*8, k*16, act=False),\n        ConvSkip(k*16, k*32),\n        ConvSkip(k*32, k*91),\n    ]\n    for i in range(n_middle): layers.append(middle_flow(k*91))\n    layers += [\n        ConvSkip(k*91,k*128),\n        sep_conv(k*128,k*192,act=False),\n        sep_conv(k*192,k*256),\n        nn.ReLU(),\n        nn.AdaptiveAvgPool2d(1),\n        Flatten(),\n        nn.Linear(k*256,c)\n    ]\n    return nn.Sequential(*layers)", "output": "Preview version of Xception network. Not tested yet - use at own risk. No pretrained model yet.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def supports_proxies(*proxy_types):\n    '''\n    \n    '''\n    def _supports_proxies(fn):\n        @wraps(fn)\n        def __supports_proxies(*args, **kwargs):\n            proxy_type = get_proxy_type()\n            if proxy_type not in proxy_types:\n                raise CommandExecutionError(\n                    '\\'{0}\\' proxy is not supported by function {1}'\n                    ''.format(proxy_type, fn.__name__))\n            return fn(*args, **salt.utils.args.clean_kwargs(**kwargs))\n        return __supports_proxies\n    return _supports_proxies", "output": "Decorator to specify which proxy types are supported by a function\n\n    proxy_types:\n        Arbitrary list of strings with the supported types of proxies", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_info_file():\n  \"\"\"\n  \"\"\"\n  try:\n    os.unlink(_get_info_file_path())\n  except OSError as e:\n    if e.errno == errno.ENOENT:\n      # The user may have wiped their temporary directory or something.\n      # Not a problem: we're already in the state that we want to be in.\n      pass\n    else:\n      raise", "output": "Remove the current process's TensorBoardInfo file, if it exists.\n\n  If the file does not exist, no action is taken and no error is raised.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _communicate(self):\n        \"\"\"\"\"\"\n        if (not self._communicate_first and\n                self._process.state() == QProcess.NotRunning):\n            self.communicate()\n        elif self._fired:\n            self._timer.stop()", "output": "Callback for communicate.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def profile_(profile, names, vm_overrides=None, opts=None, **kwargs):\n    '''\n    \n    '''\n    client = _get_client()\n    if isinstance(opts, dict):\n        client.opts.update(opts)\n    info = client.profile(profile, names, vm_overrides=vm_overrides, **kwargs)\n    return info", "output": "Spin up an instance using Salt Cloud\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minionname cloud.profile my-gce-config myinstance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def repository_has_cookiecutter_json(repo_directory):\n    \"\"\"\n    \"\"\"\n    repo_directory_exists = os.path.isdir(repo_directory)\n\n    repo_config_exists = os.path.isfile(\n        os.path.join(repo_directory, 'cookiecutter.json')\n    )\n    return repo_directory_exists and repo_config_exists", "output": "Determine if `repo_directory` contains a `cookiecutter.json` file.\n\n    :param repo_directory: The candidate repository directory.\n    :return: True if the `repo_directory` is valid, else False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def renamed_tree(self, source, dest):\r\n        \"\"\"\"\"\"\r\n        dirname = osp.abspath(to_text_string(source))\r\n        tofile = to_text_string(dest)\r\n        for fname in self.get_filenames():\r\n            if osp.abspath(fname).startswith(dirname):\r\n                new_filename = fname.replace(dirname, tofile)\r\n                self.renamed(source=fname, dest=new_filename)", "output": "Directory was renamed in file explorer or in project explorer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_args(self, args):\n        \"\"\"\n        \"\"\"\n        state = ParsingState(args)\n        try:\n            self._process_args_for_options(state)\n            self._process_args_for_args(state)\n        except UsageError:\n            if self.ctx is None or not self.ctx.resilient_parsing:\n                raise\n        return state.opts, state.largs, state.order", "output": "Parses positional arguments and returns ``(values, args, order)``\n        for the parsed options and arguments as well as the leftover\n        arguments if there are any.  The order is a list of objects as they\n        appear on the command line.  If arguments appear multiple times they\n        will be memorized multiple times as well.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _flush_decoder(self):\n        \"\"\"\n        \n        \"\"\"\n        if self._decoder:\n            buf = self._decoder.decompress(b'')\n            return buf + self._decoder.flush()\n\n        return b''", "output": "Flushes the decoder. Should only be called if the decoder is actually\n        being used.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_cursor(self, body, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"POST\", \"/_sql/close\", params=params, body=body\n        )", "output": "`<Clear SQL cursor>`_\n\n        :arg body: Specify the cursor value in the `cursor` element to clean the\n            cursor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read(self):\n        \"\"\"\n        \n        \"\"\"\n        result = self.get_collection().aggregate([\n            {'$match': {'_id': self._document_id}},\n            {'$project': {'_value': '$' + self._path, '_id': False}}\n        ])\n\n        for doc in result:\n            if '_value' not in doc:\n                break\n\n            return doc['_value']", "output": "Read the target value\n        Use $project aggregate operator in order to support nested objects", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Read(self, timeout=None):\n        '''\n        \n        '''\n        if not self.Shown:\n            self.Shown = True\n            self.TrayIcon.show()\n        if timeout is None:\n            self.App.exec_()\n        elif timeout == 0:\n            self.App.processEvents()\n        else:\n            self.timer = start_systray_read_timer(self, timeout)\n            self.App.exec_()\n\n            if self.timer:\n                stop_timer(self.timer)\n\n        item = self.MenuItemChosen\n        self.MenuItemChosen = TIMEOUT_KEY\n        return item", "output": "Reads the context menu\n        :param timeout: Optional.  Any value other than None indicates a non-blocking read\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_create(name, password, email, tenant_id=None,\n                enabled=True, profile=None, project_id=None, description=None, **connection_args):\n    '''\n    \n    '''\n    kstone = auth(profile, **connection_args)\n\n    if _OS_IDENTITY_API_VERSION > 2:\n        if tenant_id and not project_id:\n            project_id = tenant_id\n        item = kstone.users.create(name=name,\n                                   password=password,\n                                   email=email,\n                                   project_id=project_id,\n                                   enabled=enabled,\n                                   description=description)\n    else:\n        item = kstone.users.create(name=name,\n                                   password=password,\n                                   email=email,\n                                   tenant_id=tenant_id,\n                                   enabled=enabled)\n    return user_get(item.id, profile=profile, **connection_args)", "output": "Create a user (keystone user-create)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.user_create name=jack password=zero email=jack@halloweentown.org \\\n        tenant_id=a28a7b5a999a455f84b1f5210264375e enabled=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_top_level_images(imagedata, subset=None):\n    '''\n    \n    '''\n    try:\n        parents = [imagedata[x]['ParentId'] for x in imagedata]\n        filter_ = subset if subset is not None else imagedata\n        return [x for x in filter_ if x not in parents]\n    except (KeyError, TypeError):\n        raise CommandExecutionError(\n            'Invalid image data passed to _get_top_level_images(). Please '\n            'report this issue. Full image data: {0}'.format(imagedata)\n        )", "output": "Returns a list of the top-level images (those which are not parents). If\n    ``subset`` (an iterable) is passed, the top-level images in the subset will\n    be returned, otherwise all top-level images will be returned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absent(name, auth=None, **kwargs):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    __salt__['keystoneng.setup_clouds'](auth)\n\n    kwargs['name'] = name\n    role = __salt__['keystoneng.role_get'](**kwargs)\n\n    if role:\n        if __opts__['test'] is True:\n            ret['result'] = None\n            ret['changes'] = {'id': role.id}\n            ret['comment'] = 'Role will be deleted.'\n            return ret\n\n        __salt__['keystoneng.role_delete'](name=role)\n        ret['changes']['id'] = role.id\n        ret['comment'] = 'Deleted role'\n\n    return ret", "output": "Ensure role does not exist\n\n    name\n        Name of the role", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def GetTokenInformation(token, information_class):\n    \"\"\"\n    \n    \"\"\"\n    data_size = ctypes.wintypes.DWORD()\n    ctypes.windll.advapi32.GetTokenInformation(\n        token, information_class.num, 0, 0, ctypes.byref(data_size)\n    )\n    data = ctypes.create_string_buffer(data_size.value)\n    handle_nonzero_success(\n        ctypes.windll.advapi32.GetTokenInformation(\n            token,\n            information_class.num,\n            ctypes.byref(data),\n            ctypes.sizeof(data),\n            ctypes.byref(data_size),\n        )\n    )\n    return ctypes.cast(data, ctypes.POINTER(TOKEN_USER)).contents", "output": "Given a token, get the token information for it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def AddFileDescriptor(self, file_desc):\n    \"\"\"\n    \"\"\"\n\n    self._AddFileDescriptor(file_desc)\n    # TODO(jieluo): This is a temporary solution for FieldDescriptor.file.\n    # Remove it when FieldDescriptor.file is added in code gen.\n    for extension in file_desc.extensions_by_name.values():\n      self._file_desc_by_toplevel_extension[\n          extension.full_name] = file_desc", "output": "Adds a FileDescriptor to the pool, non-recursively.\n\n    If the FileDescriptor contains messages or enums, the caller must explicitly\n    register them.\n\n    Args:\n      file_desc: A FileDescriptor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close(self):\n        \"\"\"\"\"\"\n        if not self.is_open:\n            return\n        if self.writable:\n            check_call(_LIB.MXRecordIOWriterFree(self.handle))\n        else:\n            check_call(_LIB.MXRecordIOReaderFree(self.handle))\n        self.is_open = False\n        self.pid = None", "output": "Closes the record file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge_sort(arr):\n    \"\"\" \n    \"\"\"\n    # Our recursive base case\n    if len(arr) <= 1:\n        return arr\n    mid = len(arr) // 2\n    # Perform merge_sort recursively on both halves\n    left, right = merge_sort(arr[:mid]), merge_sort(arr[mid:])\n\n    # Merge each side together\n    return merge(left, right, arr.copy())", "output": "Merge Sort\n        Complexity: O(n log(n))", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_text_line(self, line_nb):\r\n        \"\"\"\"\"\"\r\n        # Taking into account the case when a file ends in an empty line,\r\n        # since splitlines doesn't return that line as the last element\r\n        # TODO: Make this function more efficient\r\n        try:\r\n            return to_text_string(self.toPlainText()).splitlines()[line_nb]\r\n        except IndexError:\r\n            return self.get_line_separator()", "output": "Return text line at line number *line_nb*", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _insert_layer_between(self, src, snk, new_layer, new_keras_layer):\n        \"\"\"\n        \n        \"\"\"\n        if snk is None:\n            insert_pos = self.layer_list.index(src) + 1\n        else:\n            insert_pos = self.layer_list.index(snk) # insert position\n        self.layer_list.insert(insert_pos, new_layer)\n        self.keras_layer_map[new_layer] = new_keras_layer\n        if src is None: # snk is an input layer\n            self._add_edge(new_layer, snk)\n        elif snk is None:  # src is an output layer\n            self._add_edge(src, new_layer)\n        else:\n            self._add_edge(src, new_layer)\n            self._add_edge(new_layer, snk)\n            self._remove_edge(src, snk)", "output": "Insert the new_layer before layer, whose position is layer_idx. The new layer's\n        parameter is stored in a Keras layer called new_keras_layer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tab_complete(self):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        opts = self._complete_options()\r\n        if len(opts) == 1:\r\n            self.set_current_text(opts[0] + os.sep)\r\n            self.hide_completer()", "output": "If there is a single option available one tab completes the option.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, path_info):\n        \"\"\"\n        \"\"\"\n        assert path_info[\"scheme\"] == \"local\"\n        path = path_info[\"path\"]\n\n        if not os.path.exists(path):\n            return None\n\n        actual_mtime, actual_size = get_mtime_and_size(path)\n        actual_inode = get_inode(path)\n\n        existing_record = self.get_state_record_for_inode(actual_inode)\n        if not existing_record:\n            return None\n\n        mtime, size, checksum, _ = existing_record\n        if self._file_metadata_changed(actual_mtime, mtime, actual_size, size):\n            return None\n\n        self._update_state_record_timestamp_for_inode(actual_inode)\n        return checksum", "output": "Gets the checksum for the specified path info. Checksum will be\n        retrieved from the state database if available.\n\n        Args:\n            path_info (dict): path info to get the checksum for.\n\n        Returns:\n            str or None: checksum for the specified path info or None if it\n            doesn't exist in the state database.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_gen(normalizer, denorm, sz, tfms=None, max_zoom=None, pad=0, crop_type=None,\n              tfm_y=None, sz_y=None, pad_mode=cv2.BORDER_REFLECT, scale=None):\n    \"\"\"\n    \n    \"\"\"\n    if tfm_y is None: tfm_y=TfmType.NO\n    if tfms is None: tfms=[]\n    elif not isinstance(tfms, collections.Iterable): tfms=[tfms]\n    if sz_y is None: sz_y = sz\n    if scale is None:\n        scale = [RandomScale(sz, max_zoom, tfm_y=tfm_y, sz_y=sz_y) if max_zoom is not None\n                 else Scale(sz, tfm_y, sz_y=sz_y)]\n    elif not is_listy(scale): scale = [scale]\n    if pad: scale.append(AddPadding(pad, mode=pad_mode))\n    if crop_type!=CropType.GOOGLENET: tfms=scale+tfms\n    return Transforms(sz, tfms, normalizer, denorm, crop_type,\n                      tfm_y=tfm_y, sz_y=sz_y)", "output": "Generate a standard set of transformations\n\n    Arguments\n    ---------\n     normalizer :\n         image normalizing function\n     denorm :\n         image denormalizing function\n     sz :\n         size, sz_y = sz if not specified.\n     tfms :\n         iterable collection of transformation functions\n     max_zoom : float,\n         maximum zoom\n     pad : int,\n         padding on top, left, right and bottom\n     crop_type :\n         crop type\n     tfm_y :\n         y axis specific transformations\n     sz_y :\n         y size, height\n     pad_mode :\n         cv2 padding style: repeat, reflect, etc.\n\n    Returns\n    -------\n     type : ``Transforms``\n         transformer for specified image operations.\n\n    See Also\n    --------\n     Transforms: the transformer object returned by this function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def group_delete(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.delete_group(**kwargs)", "output": "Delete a group\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystoneng.group_delete name=group1\n        salt '*' keystoneng.group_delete name=group2 domain_id=b62e76fbeeff4e8fb77073f591cf211e\n        salt '*' keystoneng.group_delete name=0e4febc2a5ab4f2c8f374b054162506d", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _pooling_output_shape(input_shape, pool_size=(2, 2),\n                          strides=None, padding='VALID'):\n  \"\"\"\"\"\"\n  dims = (1,) + pool_size + (1,)  # NHWC\n  spatial_strides = strides or (1,) * len(pool_size)\n  strides = (1,) + spatial_strides + (1,)\n  pads = padtype_to_pads(input_shape, dims, strides, padding)\n  operand_padded = onp.add(input_shape, onp.add(*zip(*pads)))\n  t = onp.floor_divide(onp.subtract(operand_padded, dims), strides) + 1\n  return tuple(t)", "output": "Helper: compute the output shape for the pooling layer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_librispeech_v1():\n  \"\"\"\"\"\"\n  hparams = transformer_base()\n\n  hparams.num_heads = 4\n  hparams.filter_size = 1024\n  hparams.hidden_size = 256\n  hparams.num_encoder_layers = 5\n  hparams.num_decoder_layers = 3\n  hparams.learning_rate = 0.15\n  hparams.batch_size = 6000000\n\n  librispeech.set_librispeech_length_hparams(hparams)\n  return hparams", "output": "HParams for training ASR model on LibriSpeech V1.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def analyze_script(self, index=None):\r\n        \"\"\"\"\"\"\r\n        if self.is_analysis_done:\r\n            return\r\n        if index is None:\r\n            index = self.get_stack_index()\r\n        if self.data:\r\n            finfo = self.data[index]\r\n            if self.todolist_enabled:\r\n                finfo.run_todo_finder()\r\n        self.is_analysis_done = True", "output": "Analyze current script with todos", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def michalewicz_function(config, reporter):\n    \"\"\"\"\"\"\n    import numpy as np\n    x = np.array(\n        [config[\"x1\"], config[\"x2\"], config[\"x3\"], config[\"x4\"], config[\"x5\"]])\n    sin_x = np.sin(x)\n    z = (np.arange(1, 6) / np.pi * (x * x))\n    sin_z = np.power(np.sin(z), 20)  # let m = 20\n    y = np.dot(sin_x, sin_z)\n\n    # Negate y since we want to minimize y value\n    reporter(timesteps_total=1, neg_mean_loss=-y)", "output": "f(x) = -sum{sin(xi) * [sin(i*xi^2 / pi)]^(2m)}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _memory_sized_lists(self,\n                            instances: Iterable[Instance]) -> Iterable[List[Instance]]:\n        \"\"\"\n        \n        \"\"\"\n        lazy = is_lazy(instances)\n\n        # Get an iterator over the next epoch worth of instances.\n        iterator = self._take_instances(instances, self._instances_per_epoch)\n\n        # We have four different cases to deal with:\n\n        # With lazy instances and no guidance about how many to load into memory,\n        # we just load ``batch_size`` instances at a time:\n        if lazy and self._max_instances_in_memory is None:\n            yield from lazy_groups_of(iterator, self._batch_size)\n        # If we specified max instances in memory, lazy or not, we just\n        # load ``max_instances_in_memory`` instances at a time:\n        elif self._max_instances_in_memory is not None:\n            yield from lazy_groups_of(iterator, self._max_instances_in_memory)\n        # If we have non-lazy instances, and we want all instances each epoch,\n        # then we just yield back the list of instances:\n        elif self._instances_per_epoch is None:\n            yield ensure_list(instances)\n        # In the final case we have non-lazy instances, we want a specific number\n        # of instances each epoch, and we didn't specify how to many instances to load\n        # into memory. So we convert the whole iterator to a list:\n        else:\n            yield list(iterator)", "output": "Breaks the dataset into \"memory-sized\" lists of instances,\n        which it yields up one at a time until it gets through a full epoch.\n\n        For example, if the dataset is already an in-memory list, and each epoch\n        represents one pass through the dataset, it just yields back the dataset.\n        Whereas if the dataset is lazily read from disk and we've specified to\n        load 1000 instances at a time, then it yields lists of 1000 instances each.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_estimator(model_name,\n                     hparams,\n                     run_config,\n                     schedule=\"train_and_evaluate\",\n                     decode_hparams=None,\n                     use_tpu=False,\n                     use_tpu_estimator=False,\n                     use_xla=False):\n  \"\"\"\"\"\"\n  model_fn = t2t_model.T2TModel.make_estimator_model_fn(\n      model_name, hparams, decode_hparams=decode_hparams, use_tpu=use_tpu)\n\n\n  del use_xla\n  if use_tpu or use_tpu_estimator:\n    problem = hparams.problem\n    batch_size = (\n        problem.tpu_batch_size_per_shard(hparams) *\n        run_config.tpu_config.num_shards)\n    mlperf_log.transformer_print(\n        key=mlperf_log.INPUT_BATCH_SIZE, value=batch_size)\n    if getattr(hparams, \"mtf_mode\", False):\n      batch_size = problem.tpu_batch_size_per_shard(hparams)\n    predict_batch_size = batch_size\n    if decode_hparams and decode_hparams.batch_size:\n      predict_batch_size = decode_hparams.batch_size\n    if decode_hparams and run_config.tpu_config:\n      decode_hparams.add_hparam(\"iterations_per_loop\",\n                                run_config.tpu_config.iterations_per_loop)\n    estimator = tf.contrib.tpu.TPUEstimator(\n        model_fn=model_fn,\n        model_dir=run_config.model_dir,\n        config=run_config,\n        use_tpu=use_tpu,\n        train_batch_size=batch_size,\n        eval_batch_size=batch_size if \"eval\" in schedule else None,\n        predict_batch_size=predict_batch_size,\n        experimental_export_device_assignment=True)\n  else:\n    estimator = tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=run_config.model_dir,\n        config=run_config,\n    )\n  return estimator", "output": "Create a T2T Estimator.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def safe_version(version):\n    \"\"\"\n    \n    \"\"\"\n    try:\n        # normalize the version\n        return str(packaging.version.Version(version))\n    except packaging.version.InvalidVersion:\n        version = version.replace(' ', '.')\n        return re.sub('[^A-Za-z0-9.]+', '-', version)", "output": "Convert an arbitrary string to a standard version string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _spawn(shell, master_read):\n    \"\"\"\n\n    \"\"\"\n    pid, master_fd = pty.fork()\n\n    if pid == pty.CHILD:\n        os.execlp(shell, shell)\n\n    try:\n        mode = tty.tcgetattr(pty.STDIN_FILENO)\n        tty.setraw(pty.STDIN_FILENO)\n        restore = True\n    except tty.error:    # This is the same as termios.error\n        restore = False\n\n    _set_pty_size(master_fd)\n    signal.signal(signal.SIGWINCH, lambda *_: _set_pty_size(master_fd))\n\n    try:\n        pty._copy(master_fd, master_read, pty._read)\n    except OSError:\n        if restore:\n            tty.tcsetattr(pty.STDIN_FILENO, tty.TCSAFLUSH, mode)\n\n    os.close(master_fd)\n    return os.waitpid(pid, 0)[1]", "output": "Create a spawned process.\n\n    Modified version of pty.spawn with terminal size support.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def symlink(self, source, dest):\n        \"\"\"\n        \n        \"\"\"\n        dest = self._adjust_cwd(dest)\n        self._log(DEBUG, \"symlink({!r}, {!r})\".format(source, dest))\n        source = b(source)\n        self._request(CMD_SYMLINK, source, dest)", "output": "Create a symbolic link to the ``source`` path at ``destination``.\n\n        :param str source: path of the original file\n        :param str dest: path of the newly created symlink", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def runner(self, fun, timeout=None, full_return=False, **kwargs):\n        '''\n        \n        '''\n        kwargs['fun'] = fun\n        runner = salt.runner.RunnerClient(self.opts)\n        return runner.cmd_sync(kwargs, timeout=timeout, full_return=full_return)", "output": "Run `runner modules <all-salt.runners>` synchronously\n\n        Wraps :py:meth:`salt.runner.RunnerClient.cmd_sync`.\n\n        Note that runner functions must be called using keyword arguments.\n        Positional arguments are not supported.\n\n        :return: Returns the result from the runner module", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def subseparable_conv(inputs, filters, kernel_size, **kwargs):\n  \"\"\"\"\"\"\n\n  def conv_fn(inputs, filters, kernel_size, **kwargs):\n    \"\"\"Sub-separable convolution, splits into separability-many blocks.\"\"\"\n    separability = None\n    if \"separability\" in kwargs:\n      separability = kwargs.pop(\"separability\")\n    if separability:\n      parts = []\n      abs_sep = separability if separability > 0 else -1 * separability\n      for split_idx, split in enumerate(tf.split(inputs, abs_sep, axis=3)):\n        with tf.variable_scope(\"part_%d\" % split_idx):\n          if separability > 0:\n            parts.append(\n                layers().Conv2D(filters // separability, kernel_size,\n                                **kwargs)(split))\n          else:\n            parts.append(\n                layers().SeparableConv2D(filters // abs_sep,\n                                         kernel_size, **kwargs)(split))\n      if separability > 1:\n        result = layers().Conv2D(filters, (1, 1))(tf.concat(parts, axis=3))\n      elif abs_sep == 1:  # If we have just one block, return it.\n        assert len(parts) == 1\n        result = parts[0]\n      else:\n        result = tf.concat(parts, axis=3)\n    else:\n      result = layers().SeparableConv2D(filters, kernel_size,\n                                        **kwargs)(inputs)\n    if separability is not None:\n      kwargs[\"separability\"] = separability\n    return result\n\n  return conv_internal(conv_fn, inputs, filters, kernel_size, **kwargs)", "output": "Sub-separable convolution. If separability == 0 it's a separable_conv.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _handle_sub_value(self, sub_value, handler_method):\n        \"\"\"\n        \n        \"\"\"\n\n        # Just handle known references within the string to be substituted and return the whole dictionary\n        # because that's the best we can do here.\n        if isinstance(sub_value, string_types):\n            # Ex: {Fn::Sub: \"some string\"}\n            sub_value = self._sub_all_refs(sub_value, handler_method)\n\n        elif isinstance(sub_value, list) and len(sub_value) > 0 and isinstance(sub_value[0], string_types):\n            # Ex: {Fn::Sub: [\"some string\", {a:b}] }\n            sub_value[0] = self._sub_all_refs(sub_value[0], handler_method)\n\n        return sub_value", "output": "Generic method to handle value to Fn::Sub key. We are interested in parsing the ${} syntaxes inside\n        the string portion of the value.\n\n        :param sub_value: Value of the Sub function\n        :param handler_method: Method to be called on every occurrence of `${LogicalId}` structure within the string.\n            Implementation could resolve and replace this structure with whatever they seem fit\n        :return: Resolved value of the Sub dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_instance(name, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The show_instance action must be called with -a or --action.'\n        )\n\n    items = query(action='ve', command=name)\n\n    ret = {}\n    for item in items:\n        if 'text' in item.__dict__:\n            ret[item.tag] = item.text\n        else:\n            ret[item.tag] = item.attrib\n\n        if item._children:\n            ret[item.tag] = {}\n            children = item._children\n            for child in children:\n                ret[item.tag][child.tag] = child.attrib\n\n    __utils__['cloud.cache_node'](ret, __active_provider_name__, __opts__)\n    return ret", "output": "Show the details from Parallels concerning an instance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_comment_trail(self):  # type: () -> Tuple[str, str, str]\n        \"\"\"\n        \n        \"\"\"\n        if self.end():\n            return \"\", \"\", \"\"\n\n        comment = \"\"\n        comment_ws = \"\"\n        self.mark()\n\n        while True:\n            c = self._current\n\n            if c == \"\\n\":\n                break\n            elif c == \"#\":\n                comment_ws = self.extract()\n\n                self.mark()\n                self.inc()  # Skip #\n\n                # The comment itself\n                while not self.end() and not self._current.is_nl() and self.inc():\n                    pass\n\n                comment = self.extract()\n                self.mark()\n\n                break\n            elif c in \" \\t\\r\":\n                self.inc()\n            else:\n                raise self.parse_error(UnexpectedCharError, c)\n\n            if self.end():\n                break\n\n        while self._current.is_spaces() and self.inc():\n            pass\n\n        if self._current == \"\\r\":\n            self.inc()\n\n        if self._current == \"\\n\":\n            self.inc()\n\n        trail = \"\"\n        if self._idx != self._marker or self._current.is_ws():\n            trail = self.extract()\n\n        return comment_ws, comment, trail", "output": "Returns (comment_ws, comment, trail)\n        If there is no comment, comment_ws and comment will\n        simply be empty.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def up(tgt='*', tgt_type='glob', timeout=None, gather_job_timeout=None):  # pylint: disable=C0103\n    '''\n    \n    '''\n    ret = status(\n        output=False,\n        tgt=tgt,\n        tgt_type=tgt_type,\n        timeout=timeout,\n        gather_job_timeout=gather_job_timeout\n    ).get('up', [])\n    return ret", "output": ".. versionchanged:: 2017.7.0\n        The ``expr_form`` argument has been renamed to ``tgt_type``, earlier\n        releases must use ``expr_form``.\n\n    Print a list of all of the minions that are up\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run manage.up\n        salt-run manage.up tgt=\"webservers\" tgt_type=\"nodegroup\"\n        salt-run manage.up timeout=5 gather_job_timeout=10", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_softmax(node, **kwargs):\n    \"\"\"\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    axis = int(attrs.get(\"axis\", -1))\n\n    softmax_node = onnx.helper.make_node(\n        \"Softmax\",\n        input_nodes,\n        [name],\n        axis=axis,\n        name=name\n    )\n\n    return [softmax_node]", "output": "Map MXNet's softmax operator attributes to onnx's Softmax operator\n    and return the created node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tree(path, load_path=None):\n    '''\n    \n    '''\n    load_path = _check_load_paths(load_path)\n\n    aug = _Augeas(loadpath=load_path)\n\n    path = path.rstrip('/') + '/'\n    match_path = path\n    return dict([i for i in _recurmatch(match_path, aug)])", "output": "Returns recursively the complete tree of a node\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' augeas.tree /files/etc/\n\n    path\n        The base of the recursive listing\n\n    .. versionadded:: 2016.3.0\n\n    load_path\n        A colon-spearated list of directories that modules should be searched\n        in. This is in addition to the standard load path and the directories\n        in AUGEAS_LENS_LIB.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_distribution(\n    name,\n    config,\n    tags=None,\n    region=None,\n    key=None,\n    keyid=None,\n    profile=None,\n):\n    '''\n    \n    '''\n    if tags is None:\n        tags = {}\n    if 'Name' in tags:\n        # Be lenient and silently accept if names match, else error\n        if tags['Name'] != name:\n            return {'error': 'Must not pass `Name` in `tags` but as `name`'}\n    tags['Name'] = name\n    tags = {\n        'Items': [{'Key': k, 'Value': v} for k, v in six.iteritems(tags)]\n    }\n\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        conn.create_distribution_with_tags(\n            DistributionConfigWithTags={\n                'DistributionConfig': config,\n                'Tags': tags,\n            },\n        )\n        _cache_id(\n            'cloudfront',\n            sub_resource=name,\n            invalidate=True,\n            region=region,\n            key=key,\n            keyid=keyid,\n            profile=profile,\n        )\n    except botocore.exceptions.ClientError as err:\n        return {'error': __utils__['boto3.get_error'](err)}\n\n    return {'result': True}", "output": "Create a CloudFront distribution with the given name, config, and (optionally) tags.\n\n    name\n        Name for the CloudFront distribution\n\n    config\n        Configuration for the distribution\n\n    tags\n        Tags to associate with the distribution\n\n    region\n        Region to connect to\n\n    key\n        Secret key to use\n\n    keyid\n        Access key to use\n\n    profile\n        A dict with region, key, and keyid,\n        or a pillar key (string) that contains such a dict.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_cloudfront.create_distribution name=mydistribution profile=awsprofile \\\n            config='{\"Comment\":\"partial configuration\",\"Enabled\":true}'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_undeclared_variables(ast):\n    \"\"\"\n    \"\"\"\n    codegen = TrackingCodeGenerator(ast.environment)\n    codegen.visit(ast)\n    return codegen.undeclared_identifiers", "output": "Returns a set of all variables in the AST that will be looked up from\n    the context at runtime.  Because at compile time it's not known which\n    variables will be used depending on the path the execution takes at\n    runtime, all variables are returned.\n\n    >>> from jinja2 import Environment, meta\n    >>> env = Environment()\n    >>> ast = env.parse('{% set foo = 42 %}{{ bar + foo }}')\n    >>> meta.find_undeclared_variables(ast) == set(['bar'])\n    True\n\n    .. admonition:: Implementation\n\n       Internally the code generator is used for finding undeclared variables.\n       This is good to know because the code generator might raise a\n       :exc:`TemplateAssertionError` during compilation and as a matter of\n       fact this function can currently raise that exception as well.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _log_changes(ret, changekey, changevalue):\n    '''\n    \n    '''\n    cl = ret['changes'].get('new', [])\n    cl.append({changekey: _object_reducer(changevalue)})\n    ret['changes']['new'] = cl\n    return ret", "output": "For logging create/update/delete operations to AWS ApiGateway", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_result(self, task, result):\n        ''''''\n        if not result:\n            return\n        if 'taskid' in task and 'project' in task and 'url' in task:\n            logger.info('result %s:%s %s -> %.30r' % (\n                task['project'], task['taskid'], task['url'], result))\n            print(json.dumps({\n                'taskid': task['taskid'],\n                'project': task['project'],\n                'url': task['url'],\n                'result': result,\n                'updatetime': time.time()\n            }))\n        else:\n            logger.warning('result UNKNOW -> %.30r' % result)\n            return", "output": "Called every result", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_today_all(output='pd'):\n    \"\"\"\n    \"\"\"\n\n    data = []\n    today = str(datetime.date.today())\n    codes = QA_fetch_get_stock_list('stock').code.tolist()\n    bestip = select_best_ip()['stock']\n    for code in codes:\n        try:\n            l = QA_fetch_get_stock_day(\n                code, today, today, '00', ip=bestip)\n        except:\n            bestip = select_best_ip()['stock']\n            l = QA_fetch_get_stock_day(\n                code, today, today, '00', ip=bestip)\n        if l is not None:\n            data.append(l)\n\n    res = pd.concat(data)\n    if output in ['pd']:\n        return res\n    elif output in ['QAD']:\n        return QA_DataStruct_Stock_day(res.set_index(['date', 'code'], drop=False))", "output": "today all\n\n    Returns:\n        [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attribute_difference(att_diff):\n    ''' \n    '''\n\n    ret = 0\n    for a_value, b_value in att_diff:\n        if max(a_value, b_value) == 0:\n            ret += 0\n        else:\n            ret += abs(a_value - b_value) * 1.0 / max(a_value, b_value)\n    return ret * 1.0 / len(att_diff)", "output": "The attribute distance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list(region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        trails = conn.describe_trails()\n        if not bool(trails.get('trailList')):\n            log.warning('No trails found')\n        return {'trails': trails.get('trailList', [])}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "List all trails\n\n    Returns list of trails\n\n    CLI Example:\n\n    .. code-block:: yaml\n\n        policies:\n          - {...}\n          - {...}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def fetch_webhook(self, webhook_id):\n        \"\"\"\n        \"\"\"\n        data = await self.http.get_webhook(webhook_id)\n        return Webhook.from_state(data, state=self._connection)", "output": "|coro|\n\n        Retrieves a :class:`.Webhook` with the specified ID.\n\n        Raises\n        --------\n        HTTPException\n            Retrieving the webhook failed.\n        NotFound\n            Invalid webhook ID.\n        Forbidden\n            You do not have permission to fetch this webhook.\n\n        Returns\n        ---------\n        :class:`.Webhook`\n            The webhook you requested.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_pos(self, pos):\n        \"\"\"  \"\"\"\n        self.pos = pos\n        if pos is not None and self.typ is not None:\n            self.typ._v_pos = pos\n        return self", "output": "set the position of this column in the Table", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_operation(op_stack, out_stack):\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    out_stack.append(calc(out_stack.pop(), out_stack.pop(), op_stack.pop()))", "output": "Apply operation to the first 2 items of the output queue\r\n\r\n    op_stack Deque (reference)\r\n    out_stack Deque (reference)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def move(self, path, raise_if_exists=False):\n        \"\"\"\n        \n        \"\"\"\n        self.rename(path, raise_if_exists=raise_if_exists)", "output": "Alias for ``rename()``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, resource):\n        \"\"\"\n        \"\"\"\n        return cls(kind=resource.get(\"kind\"), substeps=resource.get(\"substeps\", ()))", "output": "Factory: construct instance from the JSON repr.\n\n        :type resource: dict\n        :param resource: JSON representation of the entry\n\n        :rtype: :class:`QueryPlanEntryStep`\n        :return: new instance built from the resource", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_one(myStats, destIP, hostname, timeout, mySeqNumber, packet_size, quiet=False):\n    \"\"\"\n    \n    \"\"\"\n    delay = None\n\n    try:  # One could use UDP here, but it's obscure\n        mySocket = socket.socket(socket.AF_INET, socket.SOCK_RAW, socket.getprotobyname(\"icmp\"))\n    except socket.error as e:\n        print(\"failed. (socket error: '%s')\" % e.args[1])\n        raise  # raise the original error\n\n    my_ID = os.getpid() & 0xFFFF\n\n    sentTime = send_one_ping(mySocket, destIP, my_ID, mySeqNumber, packet_size)\n    if sentTime == None:\n        mySocket.close()\n        return delay\n\n    myStats.pktsSent += 1\n\n    recvTime, dataSize, iphSrcIP, icmpSeqNumber, iphTTL = receive_one_ping(mySocket, my_ID, timeout)\n\n    mySocket.close()\n\n    if recvTime:\n        delay = (recvTime - sentTime) * 1000\n        if not quiet:\n            print(\"%d bytes from %s: icmp_seq=%d ttl=%d time=%d ms\" % (\n                dataSize, socket.inet_ntoa(struct.pack(\"!I\", iphSrcIP)), icmpSeqNumber, iphTTL, delay)\n                  )\n        myStats.pktsRcvd += 1\n        myStats.totTime += delay\n        if myStats.minTime > delay:\n            myStats.minTime = delay\n        if myStats.maxTime < delay:\n            myStats.maxTime = delay\n    else:\n        delay = None\n        print(\"Request timed out.\")\n\n    return delay", "output": "Returns either the delay (in ms) or None on timeout.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_kernel_manager_and_kernel_client(self, connection_file,\r\n                                                stderr_handle,\r\n                                                is_cython=False,\r\n                                                is_pylab=False,\r\n                                                is_sympy=False):\r\n        \"\"\"\"\"\"\r\n        # Kernel spec\r\n        kernel_spec = self.create_kernel_spec(is_cython=is_cython,\r\n                                              is_pylab=is_pylab,\r\n                                              is_sympy=is_sympy)\r\n\r\n        # Kernel manager\r\n        try:\r\n            kernel_manager = QtKernelManager(connection_file=connection_file,\r\n                                             config=None, autorestart=True)\r\n        except Exception:\r\n            error_msg = _(\"The error is:<br><br>\"\r\n                          \"<tt>{}</tt>\").format(traceback.format_exc())\r\n            return (error_msg, None)\r\n        kernel_manager._kernel_spec = kernel_spec\r\n\r\n        kwargs = {}\r\n        if os.name == 'nt':\r\n            # avoid closing fds on win+Python 3.7\r\n            # which prevents interrupts\r\n            # jupyter_client > 5.2.3 will do this by default\r\n            kwargs['close_fds'] = False\r\n        # Catch any error generated when trying to start the kernel\r\n        # See issue 7302\r\n        try:\r\n            kernel_manager.start_kernel(stderr=stderr_handle, **kwargs)\r\n        except Exception:\r\n            error_msg = _(\"The error is:<br><br>\"\r\n                          \"<tt>{}</tt>\").format(traceback.format_exc())\r\n            return (error_msg, None)\r\n\r\n        # Kernel client\r\n        kernel_client = kernel_manager.client()\r\n\r\n        # Increase time to detect if a kernel is alive\r\n        # See Issue 3444\r\n        kernel_client.hb_channel.time_to_dead = 18.0\r\n\r\n        return kernel_manager, kernel_client", "output": "Create kernel manager and client.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_nodes_full(call=None, for_output=True):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The list_nodes_full function must be called with -f or --function.'\n        )\n    return _list_nodes(full=True, for_output=for_output)", "output": "Return a list of the VMs that are on the provider", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_frame(self, name=None):\n        \"\"\"\n        \n        \"\"\"\n        if name is None:\n            df = self._constructor_expanddim(self)\n        else:\n            df = self._constructor_expanddim({name: self})\n\n        return df", "output": "Convert Series to DataFrame.\n\n        Parameters\n        ----------\n        name : object, default None\n            The passed name should substitute for the series name (if it has\n            one).\n\n        Returns\n        -------\n        DataFrame\n            DataFrame representation of Series.\n\n        Examples\n        --------\n        >>> s = pd.Series([\"a\", \"b\", \"c\"],\n        ...               name=\"vals\")\n        >>> s.to_frame()\n          vals\n        0    a\n        1    b\n        2    c", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def local_subset(self, *args, **kwargs):\n        '''\n        \n        '''\n        local = salt.client.get_local_client(mopts=self.opts)\n        return local.cmd_subset(*args, **kwargs)", "output": "Run :ref:`execution modules <all-salt.modules>` against subsets of minions\n\n        .. versionadded:: 2016.3.0\n\n        Wraps :py:meth:`salt.client.LocalClient.cmd_subset`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_capability_definitions(service_instance=None):\n    '''\n    \n    '''\n    profile_manager = salt.utils.pbm.get_profile_manager(service_instance)\n    ret_list = [_get_capability_definition_dict(c) for c in\n                salt.utils.pbm.get_capability_definitions(profile_manager)]\n    return ret_list", "output": "Returns a list of the metadata of all capabilities in the vCenter.\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.list_capabilities", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_cookie(name, value, **kwargs):\n    \"\"\"\n    \"\"\"\n    result = {\n        'version': 0,\n        'name': name,\n        'value': value,\n        'port': None,\n        'domain': '',\n        'path': '/',\n        'secure': False,\n        'expires': None,\n        'discard': True,\n        'comment': None,\n        'comment_url': None,\n        'rest': {'HttpOnly': None},\n        'rfc2109': False,\n    }\n\n    badargs = set(kwargs) - set(result)\n    if badargs:\n        err = 'create_cookie() got unexpected keyword arguments: %s'\n        raise TypeError(err % list(badargs))\n\n    result.update(kwargs)\n    result['port_specified'] = bool(result['port'])\n    result['domain_specified'] = bool(result['domain'])\n    result['domain_initial_dot'] = result['domain'].startswith('.')\n    result['path_specified'] = bool(result['path'])\n\n    return cookielib.Cookie(**result)", "output": "Make a cookie from underspecified parameters.\n\n    By default, the pair of `name` and `value` will be set for the domain ''\n    and sent on every request (this is sometimes called a \"supercookie\").", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_minion_cache(self, preserve_minions=None):\n        '''\n        \n        '''\n        if preserve_minions is None:\n            preserve_minions = []\n        keys = self.list_keys()\n        minions = []\n        for key, val in six.iteritems(keys):\n            minions.extend(val)\n        if not self.opts.get('preserve_minion_cache', False):\n            m_cache = os.path.join(self.opts['cachedir'], self.ACC)\n            if os.path.isdir(m_cache):\n                for minion in os.listdir(m_cache):\n                    if minion not in minions and minion not in preserve_minions:\n                        try:\n                            shutil.rmtree(os.path.join(m_cache, minion))\n                        except (OSError, IOError) as ex:\n                            log.warning('Key: Delete cache for %s got OSError/IOError: %s \\n',\n                                        minion,\n                                        ex)\n                            continue\n            cache = salt.cache.factory(self.opts)\n            clist = cache.list(self.ACC)\n            if clist:\n                for minion in clist:\n                    if minion not in minions and minion not in preserve_minions:\n                        cache.flush('{0}/{1}'.format(self.ACC, minion))", "output": "Check the minion cache to make sure that old minion data is cleared\n\n        Optionally, pass in a list of minions which should have their caches\n        preserved. To preserve all caches, set __opts__['preserve_minion_cache']", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pixels_from_softmax(frame_logits, pure_sampling=False,\n                        temperature=1.0, gumbel_noise_factor=0.2):\n  \"\"\"\"\"\"\n  # If we're purely sampling, just sample each pixel.\n  if pure_sampling or temperature == 0.0:\n    return common_layers.sample_with_temperature(frame_logits, temperature)\n\n  # Gumbel-sample from the pixel sofmax and average by pixel values.\n  pixel_range = tf.to_float(tf.range(256))\n  for _ in range(len(frame_logits.get_shape().as_list()) - 1):\n    pixel_range = tf.expand_dims(pixel_range, axis=0)\n\n  frame_logits = tf.nn.log_softmax(frame_logits)\n  gumbel_samples = discretization.gumbel_sample(\n      common_layers.shape_list(frame_logits)) * gumbel_noise_factor\n\n  frame = tf.nn.softmax((frame_logits + gumbel_samples) / temperature, axis=-1)\n  result = tf.reduce_sum(frame * pixel_range, axis=-1)\n  # Round on the forward pass, not on the backward one.\n  return result + tf.stop_gradient(tf.round(result) - result)", "output": "Given frame_logits from a per-pixel softmax, generate colors.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def histogram(self, tag, values, bins, step=None):\n    \"\"\"\n    \"\"\"\n    if step is None:\n      step = self._step\n    else:\n      self._step = step\n    values = onp.array(values)\n    bins = onp.array(bins)\n    values = onp.reshape(values, -1)\n    counts, limits = onp.histogram(values, bins=bins)\n    # boundary logic\n    cum_counts = onp.cumsum(onp.greater(counts, 0, dtype=onp.int32))\n    start, end = onp.searchsorted(\n        cum_counts, [0, cum_counts[-1] - 1], side='right')\n    start, end = int(start), int(end) + 1\n    counts = (\n        counts[start -\n               1:end] if start > 0 else onp.concatenate([[0], counts[:end]]))\n    limits = limits[start:end + 1]\n    sum_sq = values.dot(values)\n    histo = HistogramProto(\n        min=values.min(),\n        max=values.max(),\n        num=len(values),\n        sum=values.sum(),\n        sum_squares=sum_sq,\n        bucket_limit=limits.tolist(),\n        bucket=counts.tolist())\n    summary = Summary(value=[Summary.Value(tag=tag, histo=histo)])\n    self.add_summary(summary, step)", "output": "Saves histogram of values.\n\n    Args:\n      tag: str: label for this data\n      values: ndarray: will be flattened by this routine\n      bins: number of bins in histogram, or array of bins for onp.histogram\n      step: int: training step", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def traverse(self):\n        \"\"\"\n        \"\"\"\n        current = self.head\n        while current is not None:\n            yield current\n            current = current.next", "output": "Traverse this linked list.\n\n        Yields:\n            PlasmaObjectFuture: PlasmaObjectFuture instances.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def help(self):\r\n        \"\"\"\"\"\"\r\n        QMessageBox.about(self, _(\"Help\"),\r\n                          \"\"\"<b>%s</b>\r\n                          <p><i>%s</i><br>    edit foobar.py\r\n                          <p><i>%s</i><br>    xedit foobar.py\r\n                          <p><i>%s</i><br>    run foobar.py\r\n                          <p><i>%s</i><br>    clear x, y\r\n                          <p><i>%s</i><br>    !ls\r\n                          <p><i>%s</i><br>    object?\r\n                          <p><i>%s</i><br>    result = oedit(object)\r\n                          \"\"\" % (_('Shell special commands:'),\r\n                                 _('Internal editor:'),\r\n                                 _('External editor:'),\r\n                                 _('Run script:'),\r\n                                 _('Remove references:'),\r\n                                 _('System commands:'),\r\n                                 _('Python help:'),\r\n                                 _('GUI-based editor:')))", "output": "Help on Spyder console", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fields(cls):\n    \"\"\"\n    \n    \"\"\"\n    if not isclass(cls):\n        raise TypeError(\"Passed object must be a class.\")\n    attrs = getattr(cls, \"__attrs_attrs__\", None)\n    if attrs is None:\n        raise NotAnAttrsClassError(\n            \"{cls!r} is not an attrs-decorated class.\".format(cls=cls)\n        )\n    return attrs", "output": "Return the tuple of ``attrs`` attributes for a class.\n\n    The tuple also allows accessing the fields by their names (see below for\n    examples).\n\n    :param type cls: Class to introspect.\n\n    :raise TypeError: If *cls* is not a class.\n    :raise attr.exceptions.NotAnAttrsClassError: If *cls* is not an ``attrs``\n        class.\n\n    :rtype: tuple (with name accessors) of :class:`attr.Attribute`\n\n    ..  versionchanged:: 16.2.0 Returned tuple allows accessing the fields\n        by name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log_stats(self):\n    \"\"\"\"\"\"\n    logging.info('Validation statistics: ')\n    for k, v in iteritems(self.stats):\n      logging.info('%s - %d valid out of %d total submissions',\n                   k, v[0], v[0] + v[1])", "output": "Print statistics into log.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_connection(self, alias='default', **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs.setdefault('serializer', serializer)\n        conn = self._conns[alias] = Elasticsearch(**kwargs)\n        return conn", "output": "Construct an instance of ``elasticsearch.Elasticsearch`` and register\n        it under given alias.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_params(self, filename):\n        \"\"\"\n        \"\"\"\n        warnings.warn(\"save_params is deprecated. Please use save_parameters. \"\n                      \"Note that if you want load from SymbolBlock later, please \"\n                      \"use export instead. For details, see \"\n                      \"https://mxnet.incubator.apache.org/tutorials/gluon/save_lo\"\n                      \"ad_params.html\")\n        try:\n            self.collect_params().save(filename, strip_prefix=self.prefix)\n        except ValueError as e:\n            raise ValueError('%s\\nsave_params is deprecated. Using ' \\\n                              'save_parameters may resolve this error.'%e.message)", "output": "[Deprecated] Please use save_parameters. Note that if you want load\n        from SymbolBlock later, please use export instead.\n\n        Save parameters to file.\n\n        filename : str\n            Path to file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _gen_success_message(publish_output):\n    \"\"\"\n    \n    \"\"\"\n    application_id = publish_output.get('application_id')\n    details = json.dumps(publish_output.get('details'), indent=2)\n\n    if CREATE_APPLICATION in publish_output.get('actions'):\n        return \"Created new application with the following metadata:\\n{}\".format(details)\n\n    return 'The following metadata of application \"{}\" has been updated:\\n{}'.format(application_id, details)", "output": "Generate detailed success message for published applications.\n\n    Parameters\n    ----------\n    publish_output : dict\n        Output from serverlessrepo publish_application\n\n    Returns\n    -------\n    str\n        Detailed success message", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_custom_resource_definition_status(self, name, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_custom_resource_definition_status_with_http_info(name, **kwargs)\n        else:\n            (data) = self.read_custom_resource_definition_status_with_http_info(name, **kwargs)\n            return data", "output": "read status of the specified CustomResourceDefinition\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_custom_resource_definition_status(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the CustomResourceDefinition (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1beta1CustomResourceDefinition\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def store(self, bank, key, data):\n        '''\n        \n        '''\n        fun = '{0}.store'.format(self.driver)\n        return self.modules[fun](bank, key, data, **self._kwargs)", "output": "Store data using the specified module\n\n        :param bank:\n            The name of the location inside the cache which will hold the key\n            and its associated data.\n\n        :param key:\n            The name of the key (or file inside a directory) which will hold\n            the data. File extensions should not be provided, as they will be\n            added by the driver itself.\n\n        :param data:\n            The data which will be stored in the cache. This data should be\n            in a format which can be serialized by msgpack/json/yaml/etc.\n\n        :raises SaltCacheError:\n            Raises an exception if cache driver detected an error accessing data\n            in the cache backend (auth, permissions, etc).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __load_uri(self, sock_dir, node):\n        '''\n        \n        '''\n        if node == 'master':\n            if self.opts['ipc_mode'] == 'tcp':\n                puburi = int(self.opts['tcp_master_pub_port'])\n                pulluri = int(self.opts['tcp_master_pull_port'])\n            else:\n                puburi = os.path.join(\n                    sock_dir,\n                    'master_event_pub.ipc'\n                )\n                pulluri = os.path.join(\n                    sock_dir,\n                    'master_event_pull.ipc'\n                )\n        else:\n            if self.opts['ipc_mode'] == 'tcp':\n                puburi = int(self.opts['tcp_pub_port'])\n                pulluri = int(self.opts['tcp_pull_port'])\n            else:\n                hash_type = getattr(hashlib, self.opts['hash_type'])\n                # Only use the first 10 chars to keep longer hashes from exceeding the\n                # max socket path length.\n                id_hash = hash_type(salt.utils.stringutils.to_bytes(self.opts['id'])).hexdigest()[:10]\n                puburi = os.path.join(\n                    sock_dir,\n                    'minion_event_{0}_pub.ipc'.format(id_hash)\n                )\n                pulluri = os.path.join(\n                    sock_dir,\n                    'minion_event_{0}_pull.ipc'.format(id_hash)\n                )\n        log.debug('%s PUB socket URI: %s', self.__class__.__name__, puburi)\n        log.debug('%s PULL socket URI: %s', self.__class__.__name__, pulluri)\n        return puburi, pulluri", "output": "Return the string URI for the location of the pull and pub sockets to\n        use for firing and listening to events", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_bio_tags_to_conll_format(labels: List[str]):\n    \"\"\"\n    \n    \"\"\"\n    sentence_length = len(labels)\n    conll_labels = []\n    for i, label in enumerate(labels):\n        if label == \"O\":\n            conll_labels.append(\"*\")\n            continue\n        new_label = \"*\"\n        # Are we at the beginning of a new span, at the first word in the sentence,\n        # or is the label different from the previous one? If so, we are seeing a new label.\n        if label[0] == \"B\" or i == 0 or label[1:] != labels[i - 1][1:]:\n            new_label = \"(\" + label[2:] + new_label\n        # Are we at the end of the sentence, is the next word a new span, or is the next\n        # word not in a span? If so, we need to close the label span.\n        if i == sentence_length - 1 or labels[i + 1][0] == \"B\" or label[1:] != labels[i + 1][1:]:\n            new_label = new_label + \")\"\n        conll_labels.append(new_label)\n    return conll_labels", "output": "Converts BIO formatted SRL tags to the format required for evaluation with the\n    official CONLL 2005 perl script. Spans are represented by bracketed labels,\n    with the labels of words inside spans being the same as those outside spans.\n    Beginning spans always have a opening bracket and a closing asterisk (e.g. \"(ARG-1*\" )\n    and closing spans always have a closing bracket (e.g. \"*)\" ). This applies even for\n    length 1 spans, (e.g \"(ARG-0*)\").\n\n    A full example of the conversion performed:\n\n    [B-ARG-1, I-ARG-1, I-ARG-1, I-ARG-1, I-ARG-1, O]\n    [ \"(ARG-1*\", \"*\", \"*\", \"*\", \"*)\", \"*\"]\n\n    Parameters\n    ----------\n    labels : List[str], required.\n        A list of BIO tags to convert to the CONLL span based format.\n\n    Returns\n    -------\n    A list of labels in the CONLL span based format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge_level_and_latent_dist(level_dist, latent_dist,\n                                merge_std=\"prev_level\"):\n  \"\"\"\n  \"\"\"\n  level_mean, level_std = level_dist.loc, level_dist.scale\n  latent_mean, latent_std = latent_dist.loc, latent_dist.scale\n  new_mean = level_mean + latent_mean\n  if merge_std == \"normal\":\n    z_shape = common_layers.shape_list(latent_mean)\n    log_scale = tf.get_variable(\n        \"merge_std\", shape=z_shape, dtype=tf.float32,\n        initializer=tf.zeros_initializer(), trainable=False)\n    scale = tf.exp(log_scale * 3.0)\n  elif merge_std == \"prev_level\":\n    scale = level_std\n  elif merge_std == \"prev_step\":\n    scale = latent_std\n  return tfp.distributions.Normal(loc=new_mean, scale=scale)", "output": "Merge level_dist and latent_dist.\n\n  new_dist ~ N(level_dist.mean + latent_dis.mean, std) where std is determined\n  according to merge_std.\n\n  Args:\n    level_dist: instance of tfp.distributions.Normal\n    latent_dist: instance of tfp.distributions.Normal\n    merge_std: can be \"prev_level\", \"prev_step\" or \"normal\".\n  Returns:\n    merged_dist: instance of tfp.distributions.Normal", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def GateBranches(x, **unused_kwargs):\n  \"\"\"\n  \"\"\"\n  assert len(x) == 3, x\n  state, gate, candidate = x\n  return gate * state + (1.0 - gate) * candidate", "output": "Implements a gating function on a (memory, gate, candidate) tuple.\n\n  Final update is memory * gate + (1-gate) * candidate\n\n  This gating equation may also be referred to as Highway Network.\n  Highway Networks: https://arxiv.org/abs/1505.00387\n\n  Args:\n    x: A tuple of (memory, gate, candidate)\n\n  Returns:\n    The result of applying gating.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extract_zipped_paths(path):\n    \"\"\"\n    \"\"\"\n    if os.path.exists(path):\n        # this is already a valid path, no need to do anything further\n        return path\n\n    # find the first valid part of the provided path and treat that as a zip archive\n    # assume the rest of the path is the name of a member in the archive\n    archive, member = os.path.split(path)\n    while archive and not os.path.exists(archive):\n        archive, prefix = os.path.split(archive)\n        member = '/'.join([prefix, member])\n\n    if not zipfile.is_zipfile(archive):\n        return path\n\n    zip_file = zipfile.ZipFile(archive)\n    if member not in zip_file.namelist():\n        return path\n\n    # we have a valid zip archive and a valid member of that archive\n    tmp = tempfile.gettempdir()\n    extracted_path = os.path.join(tmp, *member.split('/'))\n    if not os.path.exists(extracted_path):\n        extracted_path = zip_file.extract(member, path=tmp)\n\n    return extracted_path", "output": "Replace nonexistent paths that look like they refer to a member of a zip\n    archive with the location of an extracted copy of the target, or else\n    just return the provided path unchanged.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ls(url='http://localhost:8080/manager', timeout=180):\n    '''\n    \n    '''\n\n    ret = {}\n    data = _wget('list', '', url, timeout=timeout)\n    if data['res'] is False:\n        return {}\n    data['msg'].pop(0)\n    for line in data['msg']:\n        tmp = line.split(':')\n        ret[tmp[0]] = {\n            'mode': tmp[1],\n            'sessions': tmp[2],\n            'fullname': tmp[3],\n            'version': '',\n        }\n        sliced = tmp[3].split('##')\n        if len(sliced) > 1:\n            ret[tmp[0]]['version'] = sliced[1]\n\n    return ret", "output": "list all the deployed webapps\n\n    url : http://localhost:8080/manager\n        the URL of the server manager webapp\n    timeout : 180\n        timeout for HTTP request\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' tomcat.ls\n        salt '*' tomcat.ls http://localhost:8080/manager", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_git_pillar(opts):\n    '''\n    \n    '''\n    ret = []\n    for opts_dict in [x for x in opts.get('ext_pillar', [])]:\n        if 'git' in opts_dict:\n            try:\n                pillar = salt.utils.gitfs.GitPillar(\n                    opts,\n                    opts_dict['git'],\n                    per_remote_overrides=git_pillar.PER_REMOTE_OVERRIDES,\n                    per_remote_only=git_pillar.PER_REMOTE_ONLY,\n                    global_only=git_pillar.GLOBAL_ONLY)\n                ret.append(pillar)\n            except salt.exceptions.FileserverConfigError:\n                if opts.get('git_pillar_verify_config', True):\n                    raise\n                else:\n                    log.critical('Could not initialize git_pillar')\n    return ret", "output": "Clear out the ext pillar caches, used when the master starts", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(cwd, rev, force=False, user=None):\n    '''\n    \n    '''\n    cmd = ['hg', 'update', '{0}'.format(rev)]\n    if force:\n        cmd.append('-C')\n\n    ret = __salt__['cmd.run_all'](cmd, cwd=cwd, runas=user, python_shell=False)\n    if ret['retcode'] != 0:\n        raise CommandExecutionError(\n            'Hg command failed: {0}'.format(ret.get('stderr', ret['stdout']))\n        )\n\n    return ret['stdout']", "output": "Update to a given revision\n\n    cwd\n        The path to the Mercurial repository\n\n    rev\n        The revision to update to\n\n    force : False\n        Force an update\n\n    user : None\n        Run hg as a user other than what the minion runs as\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt devserver1 hg.update /path/to/repo somebranch", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_command(self, name):\n        \"\"\"\n        \"\"\"\n\n        # fast path, no space in name.\n        if ' ' not in name:\n            return self.all_commands.get(name)\n\n        names = name.split()\n        obj = self.all_commands.get(names[0])\n        if not isinstance(obj, GroupMixin):\n            return obj\n\n        for name in names[1:]:\n            try:\n                obj = obj.all_commands[name]\n            except (AttributeError, KeyError):\n                return None\n\n        return obj", "output": "Get a :class:`.Command` or subclasses from the internal list\n        of commands.\n\n        This could also be used as a way to get aliases.\n\n        The name could be fully qualified (e.g. ``'foo bar'``) will get\n        the subcommand ``bar`` of the group command ``foo``. If a\n        subcommand is not found then ``None`` is returned just as usual.\n\n        Parameters\n        -----------\n        name: :class:`str`\n            The name of the command to get.\n\n        Returns\n        --------\n        :class:`Command` or subclass\n            The command that was requested. If not found, returns ``None``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop(name):\n    '''\n    \n    '''\n    if _service_is_upstart(name):\n        cmd = 'stop {0}'.format(name)\n    else:\n        cmd = '/sbin/service {0} stop'.format(name)\n    return not __salt__['cmd.retcode'](cmd, python_shell=False)", "output": "Stop the specified service\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.stop <service name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fill_price_worse_than_limit_price(fill_price, order):\n    \"\"\"\n    \n    \"\"\"\n    if order.limit:\n        # this is tricky! if an order with a limit price has reached\n        # the limit price, we will try to fill the order. do not fill\n        # these shares if the impacted price is worse than the limit\n        # price. return early to avoid creating the transaction.\n\n        # buy order is worse if the impacted price is greater than\n        # the limit price. sell order is worse if the impacted price\n        # is less than the limit price\n        if (order.direction > 0 and fill_price > order.limit) or \\\n                (order.direction < 0 and fill_price < order.limit):\n            return True\n\n    return False", "output": "Checks whether the fill price is worse than the order's limit price.\n\n    Parameters\n    ----------\n    fill_price: float\n        The price to check.\n\n    order: zipline.finance.order.Order\n        The order whose limit price to check.\n\n    Returns\n    -------\n    bool: Whether the fill price is above the limit price (for a buy) or below\n    the limit price (for a sell).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _consume(self):\n        \"\"\"  \"\"\"\n        self._stream_offset += self._buff_i - self._buf_checkpoint\n        self._buf_checkpoint = self._buff_i", "output": "Gets rid of the used parts of the buffer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _unstack_extension_series(series, level, fill_value):\n    \"\"\"\n    \n    \"\"\"\n    # Implementation note: the basic idea is to\n    # 1. Do a regular unstack on a dummy array of integers\n    # 2. Followup with a columnwise take.\n    # We use the dummy take to discover newly-created missing values\n    # introduced by the reshape.\n    from pandas.core.reshape.concat import concat\n\n    dummy_arr = np.arange(len(series))\n    # fill_value=-1, since we will do a series.values.take later\n    result = _Unstacker(dummy_arr, series.index,\n                        level=level, fill_value=-1).get_result()\n\n    out = []\n    values = extract_array(series, extract_numpy=False)\n\n    for col, indices in result.iteritems():\n        out.append(Series(values.take(indices.values,\n                                      allow_fill=True,\n                                      fill_value=fill_value),\n                          name=col, index=result.index))\n    return concat(out, axis='columns', copy=False, keys=result.columns)", "output": "Unstack an ExtensionArray-backed Series.\n\n    The ExtensionDtype is preserved.\n\n    Parameters\n    ----------\n    series : Series\n        A Series with an ExtensionArray for values\n    level : Any\n        The level name or number.\n    fill_value : Any\n        The user-level (not physical storage) fill value to use for\n        missing values introduced by the reshape. Passed to\n        ``series.values.take``.\n\n    Returns\n    -------\n    DataFrame\n        Each column of the DataFrame will have the same dtype as\n        the input Series.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_col_sep(self):\r\n        \"\"\"\"\"\"\r\n        if self.tab_btn.isChecked():\r\n            return u\"\\t\"\r\n        elif self.ws_btn.isChecked():\r\n            return None\r\n        return to_text_string(self.line_edt.text())", "output": "Return the column separator", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_computer_name(name):\n    '''\n    \n    '''\n    if six.PY2:\n        name = _to_unicode(name)\n\n    if windll.kernel32.SetComputerNameExW(\n            win32con.ComputerNamePhysicalDnsHostname, name):\n        ret = {'Computer Name': {'Current': get_computer_name()}}\n        pending = get_pending_computer_name()\n        if pending not in (None, False):\n            ret['Computer Name']['Pending'] = pending\n        return ret\n\n    return False", "output": "Set the Windows computer name\n\n    Args:\n\n        name (str):\n            The new name to give the computer. Requires a reboot to take effect.\n\n    Returns:\n        dict:\n            Returns a dictionary containing the old and new names if successful.\n            ``False`` if not.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'minion-id' system.set_computer_name 'DavesComputer'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _url_params(size:str='>400*300', format:str='jpg') -> str:\n    \"\"\n    _fmts = {'jpg':'ift:jpg','gif':'ift:gif','png':'ift:png','bmp':'ift:bmp', 'svg':'ift:svg','webp':'webp','ico':'ift:ico'}\n    if size not in _img_sizes: \n        raise RuntimeError(f\"\"\"Unexpected size argument value: {size}.\n                    See `widgets.image_downloader._img_sizes` for supported sizes.\"\"\") \n    if format not in _fmts: \n        raise RuntimeError(f\"Unexpected image file format: {format}. Use jpg, gif, png, bmp, svg, webp, or ico.\")\n    return \"&tbs=\" + _img_sizes[size] + \",\" + _fmts[format]", "output": "Build Google Images Search Url params and return them as a string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_answers(self):\n        \"\"\"\n        \n        \"\"\"\n        if 'inner_hits' in self.meta and 'answer' in self.meta.inner_hits:\n            return self.meta.inner_hits.answer.hits\n        return list(self.search_answers())", "output": "Get answers either from inner_hits already present or by searching\n        elasticsearch.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rounding_accuracy(predictions,\n                      labels,\n                      weights_fn=common_layers.weights_nonzero):\n  \"\"\"\"\"\"\n  outputs = tf.squeeze(tf.to_int32(predictions))\n  labels = tf.squeeze(labels)\n  weights = weights_fn(labels)\n  labels = tf.to_int32(labels)\n  return tf.to_float(tf.equal(outputs, labels)), weights", "output": "Rounding accuracy for L1/L2 losses: round down the predictions to ints.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def num_failures(self):\n        \"\"\"\n        \n        \"\"\"\n        min_time = time.time() - self.window\n\n        while self.failures and self.failures[0] < min_time:\n            self.failures.popleft()\n\n        return len(self.failures)", "output": "Return the number of failures in the window.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sort(self):\n        \"\"\"\n        \n        \"\"\"\n        beams = [v for (_, v) in self.entries.items()]\n        sortedBeams = sorted(beams, reverse=True, key=lambda x: x.prTotal*x.prText)\n        return [x.labeling for x in sortedBeams]", "output": "return beam-labelings, sorted by probability", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reduce_mean(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'axes':'axis'})\n    return 'mean', new_attrs, inputs", "output": "Reduce the array along a given axis by mean value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def move(self, path, raise_if_exists=False):\n        \"\"\"\n        \n        \"\"\"\n        self.fs.move(self.path, path, raise_if_exists)", "output": "Call MockFileSystem's move command", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def DynamicConvFilter(inputs, filters, out_channel,\n                      kernel_shape,\n                      stride=1,\n                      padding='SAME'):\n    \"\"\" \n    \"\"\"\n\n    # tf.unstack only works with known batch_size :-(\n    batch_size, h, w, in_channel = inputs.get_shape().as_list()\n    stride = shape4d(stride)\n\n    inputs = tf.unstack(inputs)\n    filters = tf.reshape(filters, [batch_size] + shape2d(kernel_shape) + [in_channel, out_channel])\n    filters = tf.unstack(filters)\n\n    # this is ok as TF uses the cuda stream context\n    rsl = [tf.nn.conv2d(tf.reshape(d, [1, h, w, in_channel]),\n                        tf.reshape(k, [kernel_shape, kernel_shape, in_channel, out_channel]),\n                        stride, padding=\"SAME\") for d, k in zip(inputs, filters)]\n    rsl = tf.concat(rsl, axis=0, name='output')\n    return rsl", "output": "see \"Dynamic Filter Networks\" (NIPS 2016)\n        by Bert De Brabandere*, Xu Jia*, Tinne Tuytelaars and Luc Van Gool\n\n    Remarks:\n        This is the convolution version of a dynamic filter.\n\n    Args:\n        inputs : unfiltered input [b, h, w, 1] only grayscale images.\n        filters : learned filters of [b, k, k, 1] (dynamically generated by the network).\n        out_channel (int): number of output channel.\n        kernel_shape: (h, w) tuple or a int.\n        stride: (h, w) tuple or a int.\n        padding (str): 'valid' or 'same'. Case insensitive.\n\n    Returns\n        tf.Tensor named ``output``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_for_unit_changes(name):\n    '''\n    \n    '''\n    contextkey = 'systemd._check_for_unit_changes.{0}'.format(name)\n    if contextkey not in __context__:\n        if _untracked_custom_unit_found(name) or _unit_file_changed(name):\n            systemctl_reload()\n        # Set context key to avoid repeating this check\n        __context__[contextkey] = True", "output": "Check for modified/updated unit files, and run a daemon-reload if any are\n    found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pipe(self, command, timeout=None, cwd=None):\n        \"\"\"\n        \"\"\"\n        if not timeout:\n            timeout = self.timeout\n\n        if not self.was_run:\n            self.run(block=False, cwd=cwd)\n\n        data = self.out\n\n        if timeout:\n            c = Command(command, timeout)\n        else:\n            c = Command(command)\n\n        c.run(block=False, cwd=cwd)\n        if data:\n            c.send(data)\n        c.block()\n        return c", "output": "Runs the current command and passes its output to the next\n        given process.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def connect_external_kernel(self, shellwidget):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        sw = shellwidget\r\n        kc = shellwidget.kernel_client\r\n        if self.main.help is not None:\r\n            self.main.help.set_shell(sw)\r\n        if self.main.variableexplorer is not None:\r\n            self.main.variableexplorer.add_shellwidget(sw)\r\n            sw.set_namespace_view_settings()\r\n            sw.refresh_namespacebrowser()\r\n            kc.stopped_channels.connect(lambda :\r\n                self.main.variableexplorer.remove_shellwidget(id(sw)))", "output": "Connect an external kernel to the Variable Explorer and Help, if\r\n        it is a Spyder kernel.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _summary(self, name=None):\n        \"\"\"\n        \n        \"\"\"\n        formatter = self._formatter_func\n        if len(self) > 0:\n            index_summary = ', %s to %s' % (formatter(self[0]),\n                                            formatter(self[-1]))\n        else:\n            index_summary = ''\n\n        if name is None:\n            name = type(self).__name__\n        result = '%s: %s entries%s' % (printing.pprint_thing(name),\n                                       len(self), index_summary)\n        if self.freq:\n            result += '\\nFreq: %s' % self.freqstr\n\n        # display as values, not quoted\n        result = result.replace(\"'\", \"\")\n        return result", "output": "Return a summarized representation.\n\n        Parameters\n        ----------\n        name : str\n            name to use in the summary representation\n\n        Returns\n        -------\n        String with a summarized representation of the index", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def welcome(self):\n        \"\"\"\"\"\"\n        if not g.user or not g.user.get_id():\n            return redirect(appbuilder.get_url_for_login)\n\n        welcome_dashboard_id = (\n            db.session\n            .query(UserAttribute.welcome_dashboard_id)\n            .filter_by(user_id=g.user.get_id())\n            .scalar()\n        )\n        if welcome_dashboard_id:\n            return self.dashboard(str(welcome_dashboard_id))\n\n        payload = {\n            'user': bootstrap_user_data(),\n            'common': self.common_bootsrap_payload(),\n        }\n\n        return self.render_template(\n            'superset/basic.html',\n            entry='welcome',\n            title='Superset',\n            bootstrap_data=json.dumps(payload, default=utils.json_iso_dttm_ser),\n        )", "output": "Personalized welcome page", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _stringify(self, data):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(data, string_types):\n            return data\n\n        # Get the most compact dictionary (separators) and sort the keys recursively to get a stable output\n        return json.dumps(data, separators=(',', ':'), sort_keys=True)", "output": "Stable, platform & language-independent stringification of a data with basic Python type.\n\n        We use JSON to dump a string instead of `str()` method in order to be language independent.\n\n        :param data: Data to be stringified. If this is one of JSON native types like string, dict, array etc, it will\n                     be properly serialized. Otherwise this method will throw a TypeError for non-JSON serializable\n                     objects\n        :return: string representation of the dictionary\n        :rtype string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _hashed_indexing_key(self, key):\n        \"\"\"\n        \n\n        \"\"\"\n        from pandas.core.util.hashing import hash_tuples, hash_tuple\n\n        if not isinstance(key, tuple):\n            return hash_tuples(key)\n\n        if not len(key) == self.nlevels:\n            raise KeyError\n\n        def f(k, stringify):\n            if stringify and not isinstance(k, str):\n                k = str(k)\n            return k\n        key = tuple(f(k, stringify)\n                    for k, stringify in zip(key, self._have_mixed_levels))\n        return hash_tuple(key)", "output": "validate and return the hash for the provided key\n\n        *this is internal for use for the cython routines*\n\n        Parameters\n        ----------\n        key : string or tuple\n\n        Returns\n        -------\n        np.uint64\n\n        Notes\n        -----\n        we need to stringify if we have mixed levels", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_distributions(path_item, only=False):\n    \"\"\"\"\"\"\n    importer = get_importer(path_item)\n    finder = _find_adapter(_distribution_finders, importer)\n    return finder(importer, path_item, only)", "output": "Yield distributions accessible via `path_item`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self, action: torch.Tensor) -> 'ChecklistStatelet':\n        \"\"\"\n        \n        \"\"\"\n        checklist_addition = (self.terminal_actions == action).float()\n        new_checklist = self.checklist + checklist_addition\n        new_checklist_state = ChecklistStatelet(terminal_actions=self.terminal_actions,\n                                                checklist_target=self.checklist_target,\n                                                checklist_mask=self.checklist_mask,\n                                                checklist=new_checklist,\n                                                terminal_indices_dict=self.terminal_indices_dict)\n        return new_checklist_state", "output": "Takes an action index, updates checklist and returns an updated state.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def touch(self, connection=None):\n        \"\"\"\n        \n        \"\"\"\n        self.create_marker_table()\n\n        if connection is None:\n            connection = self.connect()\n            connection.autocommit = True  # if connection created here, we commit it here\n\n        connection.cursor().execute(\n            \"\"\"INSERT INTO {marker_table} (update_id, target_table)\n               VALUES (%s, %s)\n               ON DUPLICATE KEY UPDATE\n               update_id = VALUES(update_id)\n            \"\"\".format(marker_table=self.marker_table),\n            (self.update_id, self.table)\n        )\n        # make sure update is properly marked\n        assert self.exists(connection)", "output": "Mark this update as complete.\n\n        IMPORTANT, If the marker table doesn't exist,\n        the connection transaction will be aborted and the connection reset.\n        Then the marker table will be created.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def proxy_config(commands, **kwargs):\n    '''\n    \n    '''\n    no_save_config = DEVICE_DETAILS['no_save_config']\n    no_save_config = kwargs.get('no_save_config', no_save_config)\n    if not isinstance(commands, list):\n        commands = [commands]\n    try:\n        if CONNECTION == 'ssh':\n            _sendline_ssh('config terminal')\n            single_cmd = ''\n            for cmd in commands:\n                single_cmd += cmd + ' ; '\n            ret = _sendline_ssh(single_cmd + 'end')\n            if no_save_config:\n                pass\n            else:\n                _sendline_ssh(COPY_RS)\n            if ret:\n                log.error(ret)\n        elif CONNECTION == 'nxapi':\n            ret = _nxapi_request(commands)\n            if no_save_config:\n                pass\n            else:\n                _nxapi_request(COPY_RS)\n            for each in ret:\n                if 'Failure' in each:\n                    log.error(each)\n    except CommandExecutionError as e:\n        log.error(e)\n        raise\n    return [commands, ret]", "output": "Send configuration commands over SSH or NX-API\n\n    commands\n        List of configuration commands\n\n    no_save_config\n        If True, don't save configuration commands to startup configuration.\n        If False, save configuration to startup configuration.\n        Default: False\n\n    .. code-block: bash\n\n        salt '*' nxos.cmd proxy_config 'feature bgp' no_save_config=True\n        salt '*' nxos.cmd proxy_config 'feature bgp'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def volume(self, ctx, volume: int):\n        \"\"\"\"\"\"\n\n        if ctx.voice_client is None:\n            return await ctx.send(\"Not connected to a voice channel.\")\n\n        ctx.voice_client.source.volume = volume / 100\n        await ctx.send(\"Changed volume to {}%\".format(volume))", "output": "Changes the player's volume", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_target_variable(self, targets, variable):\n        \"\"\"\n        \"\"\"\n        if isinstance(targets, str):\n            targets = [targets]\n        assert is_iterable(targets)\n        assert isinstance(variable, basestring)\n\n        return bjam_interface.call('get-target-variable', targets, variable)", "output": "Gets the value of `variable` on set on the first target in `targets`.\n\n        Args:\n            targets (str or list): one or more targets to get the variable from.\n            variable (str): the name of the variable\n\n        Returns:\n             the value of `variable` set on `targets` (list)\n\n        Example:\n\n            >>> ENGINE = get_manager().engine()\n            >>> ENGINE.set_target_variable(targets, 'MY-VAR', 'Hello World')\n            >>> ENGINE.get_target_variable(targets, 'MY-VAR')\n            ['Hello World']\n\n        Equivalent Jam code:\n\n            MY-VAR on $(targets) = \"Hello World\" ;\n            echo [ on $(targets) return $(MY-VAR) ] ;\n            \"Hello World\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def delete(self, *, reason=None):\n        \"\"\"\n        \"\"\"\n\n        await self._state.http.delete_invite(self.code, reason=reason)", "output": "|coro|\n\n        Revokes the instant invite.\n\n        You must have the :attr:`~Permissions.manage_channels` permission to do this.\n\n        Parameters\n        -----------\n        reason: Optional[:class:`str`]\n            The reason for deleting this invite. Shows up on the audit log.\n\n        Raises\n        -------\n        Forbidden\n            You do not have permissions to revoke invites.\n        NotFound\n            The invite is invalid or expired.\n        HTTPException\n            Revoking the invite failed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def map_run(self, path=None, **kwargs):\n        '''\n        \n        '''\n        kwarg = {}\n        if path:\n            kwarg['map'] = path\n        kwarg.update(kwargs)\n        mapper = salt.cloud.Map(self._opts_defaults(**kwarg))\n        dmap = mapper.map_data()\n        return salt.utils.data.simple_types_filter(\n            mapper.run_map(dmap)\n        )", "output": "To execute a map", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _mean_image_subtraction(image, means):\n  \"\"\"\n  \"\"\"\n  if image.get_shape().ndims != 3:\n    raise ValueError(\"Input must be of size [height, width, C>0]\")\n  num_channels = image.get_shape().as_list()[-1]\n  if len(means) != num_channels:\n    raise ValueError(\"len(means) must match the number of channels\")\n\n  channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)\n  for i in range(num_channels):\n    channels[i] -= means[i]\n  return tf.concat(axis=2, values=channels)", "output": "Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn't match the\n      number of values in `means`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_daemon_thread(*args, **kwargs):\n    \"\"\"\"\"\"\n    thread = threading.Thread(*args, **kwargs)\n    thread.daemon = True\n    thread.start()\n    return thread", "output": "Starts a thread and marks it as a daemon thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_file(path, conn=None):\n    '''\n    \n    '''\n    if conn is None:\n        conn = init()\n\n    log.debug('Removing package file %s', path)\n    os.remove(path)", "output": "Remove a single file from the file system", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def non_dependency (self):\n        \"\"\" \n        \"\"\"\n        result = [p for p in self.lazy_properties if not p.feature.dependency]\n        result.extend(self.non_dependency_)\n        return result", "output": "Returns properties that are not dependencies.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_image(self, fname):\n        \"\"\"\n        \"\"\"\n        with open(os.path.join(self.path_root, fname), 'rb') as fin:\n            img = fin.read()\n        return img", "output": "Reads an input image `fname` and returns the decoded raw bytes.\n        Examples\n        --------\n        >>> dataIter.read_image('Face.jpg') # returns decoded raw bytes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_entrust(self):\n        \"\"\"\n        \n        \"\"\"\n        xq_entrust_list = self._get_xq_history()\n        entrust_list = []\n        replace_none = lambda s: s or 0\n        for xq_entrusts in xq_entrust_list:\n            status = xq_entrusts[\"status\"]  # \u8c03\u4ed3\u72b6\u6001\n            if status == \"pending\":\n                status = \"\u5df2\u62a5\"\n            elif status in [\"canceled\", \"failed\"]:\n                status = \"\u5e9f\u5355\"\n            else:\n                status = \"\u5df2\u6210\"\n            for entrust in xq_entrusts[\"rebalancing_histories\"]:\n                price = entrust[\"price\"]\n                entrust_list.append(\n                    {\n                        \"entrust_no\": entrust[\"id\"],\n                        \"entrust_bs\": u\"\u4e70\u5165\"\n                        if entrust[\"target_weight\"]\n                        > replace_none(entrust[\"prev_weight\"])\n                        else u\"\u5356\u51fa\",\n                        \"report_time\": self._time_strftime(\n                            entrust[\"updated_at\"]\n                        ),\n                        \"entrust_status\": status,\n                        \"stock_code\": entrust[\"stock_symbol\"],\n                        \"stock_name\": entrust[\"stock_name\"],\n                        \"business_amount\": 100,\n                        \"business_price\": price,\n                        \"entrust_amount\": 100,\n                        \"entrust_price\": price,\n                    }\n                )\n        return entrust_list", "output": "\u83b7\u53d6\u59d4\u6258\u5355(\u76ee\u524d\u8fd4\u56de20\u6b21\u8c03\u4ed3\u7684\u7ed3\u679c)\n        \u64cd\u4f5c\u6570\u91cf\u90fd\u63091\u624b\u6a21\u62df\u6362\u7b97\u7684\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_already_built_wheel(metadata_directory):\n    \"\"\"\n    \"\"\"\n    if not metadata_directory:\n        return None\n    metadata_parent = os.path.dirname(metadata_directory)\n    if not os.path.isfile(pjoin(metadata_parent, WHEEL_BUILT_MARKER)):\n        return None\n\n    whl_files = glob(os.path.join(metadata_parent, '*.whl'))\n    if not whl_files:\n        print('Found wheel built marker, but no .whl files')\n        return None\n    if len(whl_files) > 1:\n        print('Found multiple .whl files; unspecified behaviour. '\n              'Will call build_wheel.')\n        return None\n\n    # Exactly one .whl file\n    return whl_files[0]", "output": "Check for a wheel already built during the get_wheel_metadata hook.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def erase_end_of_line (self): # <ESC>[0K -or- <ESC>[K\n        ''''''\n\n        self.fill_region (self.cur_r, self.cur_c, self.cur_r, self.cols)", "output": "Erases from the current cursor position to the end of the current\n        line.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fill_row(self, forward, items, idx, row, ro, ri, overlap,lengths):\n        \"\"\n        ibuf = n = 0\n        ro  -= 1\n        while ibuf < row.size:\n            ro   += 1\n            ix    = idx[ro]\n            rag   = items[ix]\n            if forward:\n                ri = 0 if ibuf else ri\n                n  = min(lengths[ix] - ri, row.size - ibuf)\n                row[ibuf:ibuf+n] = rag[ri:ri+n]\n            else:\n                ri = lengths[ix] if ibuf else ri\n                n  = min(ri, row.size - ibuf)\n                row[ibuf:ibuf+n] = rag[ri-n:ri][::-1]\n            ibuf += n\n        return ro, ri + ((n-overlap) if forward else -(n-overlap))", "output": "Fill the row with tokens from the ragged array. --OBS-- overlap != 1 has not been implemented", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download_all(recommended=False, restart=True):\n    '''\n    \n    '''\n    to_download = _get_available(recommended, restart)\n\n    for name in to_download:\n        download(name)\n\n    return list_downloads()", "output": "Download all available updates so that they can be installed later with the\n    ``update`` or ``update_all`` functions. It returns a list of updates that\n    are now downloaded.\n\n    :param bool recommended: If set to True, only install the recommended\n        updates. If set to False (default) all updates are installed.\n\n    :param bool restart: Set this to False if you do not want to install updates\n        that require a restart. Default is True\n\n    :return: A list containing all downloaded updates on the system.\n    :rtype: list\n\n    CLI Example:\n\n    .. code-block:: bash\n\n       salt '*' softwareupdate.download_all", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_metadata_path(self, key):\n        \"\"\"  \"\"\"\n        return \"{group}/meta/{key}/meta\".format(group=self.group._v_pathname,\n                                                key=key)", "output": "return the metadata pathname for this key", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_client_for_kernel(self):\r\n        \"\"\"\"\"\"\r\n        connect_output = KernelConnectionDialog.get_connection_parameters(self)\r\n        (connection_file, hostname, sshkey, password, ok) = connect_output\r\n        if not ok:\r\n            return\r\n        else:\r\n            self._create_client_for_kernel(connection_file, hostname, sshkey,\r\n                                           password)", "output": "Create a client connected to an existing kernel", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_page_title(self, page):\n        \"\"\"\n        \n        \"\"\"\n        fname = os.path.join(SOURCE_PATH, '{}.rst'.format(page))\n        option_parser = docutils.frontend.OptionParser(\n            components=(docutils.parsers.rst.Parser,))\n        doc = docutils.utils.new_document(\n            '<doc>',\n            option_parser.get_default_values())\n        with open(fname) as f:\n            data = f.read()\n\n        parser = docutils.parsers.rst.Parser()\n        # do not generate any warning when parsing the rst\n        with open(os.devnull, 'a') as f:\n            doc.reporter.stream = f\n            parser.parse(data, doc)\n\n        section = next(node for node in doc.children\n                       if isinstance(node, docutils.nodes.section))\n        title = next(node for node in section.children\n                     if isinstance(node, docutils.nodes.title))\n\n        return title.astext()", "output": "Open the rst file `page` and extract its title.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_cwd(self, dirname):\n        \"\"\"\"\"\"\n        # Replace single for double backslashes on Windows\n        if os.name == 'nt':\n            dirname = dirname.replace(u\"\\\\\", u\"\\\\\\\\\")\n\n        if not self.external_kernel:\n            code = u\"get_ipython().kernel.set_cwd(u'''{}''')\".format(dirname)\n            if self._reading:\n                self.kernel_client.input(u'!' + code)\n            else:\n                self.silent_execute(code)\n            self._cwd = dirname", "output": "Set shell current working directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def info(name):\n    '''\n    \n    '''\n    try:\n        grinfo = grp.getgrnam(name)\n    except KeyError:\n        return {}\n    else:\n        return {'name': grinfo.gr_name,\n                'passwd': grinfo.gr_passwd,\n                'gid': grinfo.gr_gid,\n                'members': grinfo.gr_mem}", "output": "Return information about a group\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' group.info foo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def import_dashboards(self):\n        \"\"\"\"\"\"\n        f = request.files.get('file')\n        if request.method == 'POST' and f:\n            dashboard_import_export.import_dashboards(db.session, f.stream)\n            return redirect('/dashboard/list/')\n        return self.render_template('superset/import_dashboards.html')", "output": "Overrides the dashboards using json instances from the file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __x_product (property_sets):\n    \"\"\" \n    \"\"\"\n    assert is_iterable_typed(property_sets, property_set.PropertySet)\n    x_product_seen = set()\n    return __x_product_aux (property_sets, x_product_seen)[0]", "output": "Return the cross-product of all elements of property_sets, less any\n        that would contain conflicting values for single-valued features.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_checkpoint(with_local=False):\n    \"\"\"\n    \"\"\"\n    gptr = ctypes.POINTER(ctypes.c_char)()\n    global_len = ctypes.c_ulong()\n    if with_local:\n        lptr = ctypes.POINTER(ctypes.c_char)()\n        local_len = ctypes.c_ulong()\n        version = _LIB.RabitLoadCheckPoint(\n            ctypes.byref(gptr),\n            ctypes.byref(global_len),\n            ctypes.byref(lptr),\n            ctypes.byref(local_len))\n        if version == 0:\n            return (version, None, None)\n        return (version,\n                _load_model(gptr, global_len.value),\n                _load_model(lptr, local_len.value))\n    else:\n        version = _LIB.RabitLoadCheckPoint(\n            ctypes.byref(gptr),\n            ctypes.byref(global_len),\n            None, None)\n        if version == 0:\n            return (version, None)\n        return (version,\n                _load_model(gptr, global_len.value))", "output": "Load latest check point.\n\n    Parameters\n    ----------\n    with_local: bool, optional\n        whether the checkpoint contains local model\n\n    Returns\n    -------\n    tuple : tuple\n        if with_local: return (version, gobal_model, local_model)\n        else return (version, gobal_model)\n        if returned version == 0, this means no model has been CheckPointed\n        and global_model, local_model returned will be None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _setup_conn(**kwargs):\n    '''\n    \n    '''\n    kubeconfig = kwargs.get('kubeconfig') or __salt__['config.option']('kubernetes.kubeconfig')\n    kubeconfig_data = kwargs.get('kubeconfig_data') or __salt__['config.option']('kubernetes.kubeconfig-data')\n    context = kwargs.get('context') or __salt__['config.option']('kubernetes.context')\n\n    if (kubeconfig_data and not kubeconfig) or (kubeconfig_data and kwargs.get('kubeconfig_data')):\n        with tempfile.NamedTemporaryFile(prefix='salt-kubeconfig-', delete=False) as kcfg:\n            kcfg.write(base64.b64decode(kubeconfig_data))\n            kubeconfig = kcfg.name\n\n    if not (kubeconfig and context):\n        if kwargs.get('api_url') or __salt__['config.option']('kubernetes.api_url'):\n            salt.utils.versions.warn_until('Sodium',\n                    'Kubernetes configuration via url, certificate, username and password will be removed in Sodiom. '\n                    'Use \\'kubeconfig\\' and \\'context\\' instead.')\n            try:\n                return _setup_conn_old(**kwargs)\n            except Exception:\n                raise CommandExecutionError('Old style kubernetes configuration is only supported up to python-kubernetes 2.0.0')\n        else:\n            raise CommandExecutionError('Invalid kubernetes configuration. Parameter \\'kubeconfig\\' and \\'context\\' are required.')\n    kubernetes.config.load_kube_config(config_file=kubeconfig, context=context)\n\n    # The return makes unit testing easier\n    return {'kubeconfig': kubeconfig, 'context': context}", "output": "Setup kubernetes API connection singleton", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def primes(n):\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    if n==2:\r\n        return [2]\r\n    elif n<2:\r\n        return []\r\n    s=list(range(3,n+1,2))\r\n    mroot = n ** 0.5\r\n    half=(n+1)//2-1\r\n    i=0\r\n    m=3\r\n    while m <= mroot:\r\n        if s[i]:\r\n            j=(m*m-3)//2\r\n            s[j]=0\r\n            while j<half:\r\n                s[j]=0\r\n                j+=m\r\n        i=i+1\r\n        m=2*i+3\r\n    return [2]+[x for x in s if x]", "output": "Simple test function\r\n    Taken from http://www.huyng.com/posts/python-performance-analysis/", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mk_token(opts, tdata):\n    '''\n    \n    '''\n    hash_type = getattr(hashlib, opts.get('hash_type', 'md5'))\n    tok = six.text_type(hash_type(os.urandom(512)).hexdigest())\n    t_path = os.path.join(opts['token_dir'], tok)\n    while os.path.isfile(t_path):\n        tok = six.text_type(hash_type(os.urandom(512)).hexdigest())\n        t_path = os.path.join(opts['token_dir'], tok)\n    tdata['token'] = tok\n    serial = salt.payload.Serial(opts)\n    try:\n        with salt.utils.files.set_umask(0o177):\n            with salt.utils.files.fopen(t_path, 'w+b') as fp_:\n                fp_.write(serial.dumps(tdata))\n    except (IOError, OSError):\n        log.warning(\n            'Authentication failure: can not write token file \"%s\".', t_path)\n        return {}\n    return tdata", "output": "Mint a new token using the config option hash_type and store tdata with 'token' attribute set\n    to the token.\n    This module uses the hash of random 512 bytes as a token.\n\n    :param opts: Salt master config options\n    :param tdata: Token data to be stored with 'token' attirbute of this dict set to the token.\n    :returns: tdata with token if successful. Empty dict if failed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_string(self, source, globals=None, template_class=None):\n        \"\"\"\n        \"\"\"\n        globals = self.make_globals(globals)\n        cls = template_class or self.template_class\n        return cls.from_code(self, self.compile(source), globals, None)", "output": "Load a template from a string.  This parses the source given and\n        returns a :class:`Template` object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_simple_multi_country_equity_info(countries_to_sids,\n                                          countries_to_exchanges,\n                                          start_date,\n                                          end_date):\n    \"\"\"\n    \"\"\"\n    sids = []\n    symbols = []\n    exchanges = []\n\n    for country, country_sids in countries_to_sids.items():\n        exchange = countries_to_exchanges[country]\n        for i, sid in enumerate(country_sids):\n            sids.append(sid)\n            symbols.append('-'.join([country, str(i)]))\n            exchanges.append(exchange)\n\n    return pd.DataFrame(\n        {\n            'symbol': symbols,\n            'start_date': start_date,\n            'end_date': end_date,\n            'asset_name': symbols,\n            'exchange': exchanges,\n        },\n        index=sids,\n        columns=(\n            'start_date',\n            'end_date',\n            'symbol',\n            'exchange',\n            'asset_name',\n        ),\n    )", "output": "Create a DataFrame representing assets that exist for the full duration\n    between `start_date` and `end_date`, from multiple countries.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_flushing_policy(flushing_policy):\n    \"\"\"\"\"\"\n    if \"RAY_USE_NEW_GCS\" not in os.environ:\n        raise Exception(\n            \"set_flushing_policy() is only available when environment \"\n            \"variable RAY_USE_NEW_GCS is present at both compile and run time.\"\n        )\n    ray.worker.global_worker.check_connected()\n    redis_client = ray.worker.global_worker.redis_client\n\n    serialized = pickle.dumps(flushing_policy)\n    redis_client.set(\"gcs_flushing_policy\", serialized)", "output": "Serialize this policy for Monitor to pick up.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_cache_dir(appname):\n    # type: (str) -> str\n    \n    \"\"\"\n    if WINDOWS:\n        # Get the base path\n        path = os.path.normpath(_get_win_folder(\"CSIDL_LOCAL_APPDATA\"))\n\n        # When using Python 2, return paths as bytes on Windows like we do on\n        # other operating systems. See helper function docs for more details.\n        if PY2 and isinstance(path, text_type):\n            path = _win_path_to_bytes(path)\n\n        # Add our app name and Cache directory to it\n        path = os.path.join(path, appname, \"Cache\")\n    elif sys.platform == \"darwin\":\n        # Get the base path\n        path = expanduser(\"~/Library/Caches\")\n\n        # Add our app name to it\n        path = os.path.join(path, appname)\n    else:\n        # Get the base path\n        path = os.getenv(\"XDG_CACHE_HOME\", expanduser(\"~/.cache\"))\n\n        # Add our app name to it\n        path = os.path.join(path, appname)\n\n    return path", "output": "r\"\"\"\n    Return full path to the user-specific cache dir for this application.\n\n        \"appname\" is the name of application.\n\n    Typical user cache directories are:\n        macOS:      ~/Library/Caches/<AppName>\n        Unix:       ~/.cache/<AppName> (XDG default)\n        Windows:    C:\\Users\\<username>\\AppData\\Local\\<AppName>\\Cache\n\n    On Windows the only suggestion in the MSDN docs is that local settings go\n    in the `CSIDL_LOCAL_APPDATA` directory. This is identical to the\n    non-roaming app data dir (the default returned by `user_data_dir`). Apps\n    typically put cache data somewhere *under* the given dir here. Some\n    examples:\n        ...\\Mozilla\\Firefox\\Profiles\\<ProfileName>\\Cache\n        ...\\Acme\\SuperApp\\Cache\\1.0\n\n    OPINION: This function appends \"Cache\" to the `CSIDL_LOCAL_APPDATA` value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rlmb_base_stochastic_discrete():\n  \"\"\"\"\"\"\n  hparams = rlmb_base()\n  hparams.learning_rate_bump = 1.0\n  hparams.grayscale = False\n  hparams.generative_model = \"next_frame_basic_stochastic_discrete\"\n  hparams.generative_model_params = \"next_frame_basic_stochastic_discrete\"\n  # The parameters below are the same as base, but repeated for easier reading.\n  hparams.ppo_epoch_length = 50\n  hparams.simulated_rollout_length = 50\n  hparams.simulated_batch_size = 16\n  return hparams", "output": "Base setting with stochastic discrete model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def editors(self, value):\n        \"\"\"\"\"\"\n        warnings.warn(\n            _ASSIGNMENT_DEPRECATED_MSG.format(\"editors\", EDITOR_ROLE),\n            DeprecationWarning,\n        )\n        self[EDITOR_ROLE] = value", "output": "Update editors.\n\n        DEPRECATED:  use ``policy[\"roles/editors\"] = value`` instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_supplementary_field(self, sid, field_name, as_of_date):\n        \"\"\"\n        \"\"\"\n        try:\n            periods = self.equity_supplementary_map_by_sid[\n                field_name,\n                sid,\n            ]\n            assert periods, 'empty periods list for %r' % (field_name, sid)\n        except KeyError:\n            raise NoValueForSid(field=field_name, sid=sid)\n\n        if not as_of_date:\n            if len(periods) > 1:\n                # This equity has held more than one value, this is ambigious\n                # without the date\n                raise MultipleValuesFoundForSid(\n                    field=field_name,\n                    sid=sid,\n                    options={p.value for p in periods},\n                )\n            # this equity has only ever held this value, we may resolve\n            # without the date\n            return periods[0].value\n\n        for start, end, _, value in periods:\n            if start <= as_of_date < end:\n                return value\n\n        # Could not find a value for this sid on the as_of_date.\n        raise NoValueForSid(field=field_name, sid=sid)", "output": "Get the value of a supplementary field for an asset.\n\n        Parameters\n        ----------\n        sid : int\n            The sid of the asset to query.\n        field_name : str\n            Name of the supplementary field.\n        as_of_date : pd.Timestamp, None\n            The last known value on this date is returned. If None, a\n            value is returned only if we've only ever had one value for\n            this sid. If None and we've had multiple values,\n            MultipleValuesFoundForSid is raised.\n\n        Raises\n        ------\n        NoValueForSid\n            If we have no values for this asset, or no values was known\n            on this as_of_date.\n        MultipleValuesFoundForSid\n            If we have had multiple values for this asset over time, and\n            None was passed for as_of_date.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def request_offline_members(self, *guilds):\n        \n        \"\"\"\n        if any(not g.large or g.unavailable for g in guilds):\n            raise InvalidArgument('An unavailable or non-large guild was passed.')\n\n        _guilds = sorted(guilds, key=lambda g: g.shard_id)\n        for shard_id, sub_guilds in itertools.groupby(_guilds, key=lambda g: g.shard_id):\n            sub_guilds = list(sub_guilds)\n            await self._connection.request_offline_members(sub_guilds, shard_id=shard_id)", "output": "r\"\"\"|coro|\n\n        Requests previously offline members from the guild to be filled up\n        into the :attr:`Guild.members` cache. This function is usually not\n        called. It should only be used if you have the ``fetch_offline_members``\n        parameter set to ``False``.\n\n        When the client logs on and connects to the websocket, Discord does\n        not provide the library with offline members if the number of members\n        in the guild is larger than 250. You can check if a guild is large\n        if :attr:`Guild.large` is ``True``.\n\n        Parameters\n        -----------\n        \\*guilds: :class:`Guild`\n            An argument list of guilds to request offline members for.\n\n        Raises\n        -------\n        InvalidArgument\n            If any guild is unavailable or not large in the collection.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_attached_message(self, key, message_type, required=False):\n    \"\"\"\"\"\"\n    return self._spec.get_attached_message(key, message_type,\n                                           tags=self._tags, required=required)", "output": "Calls ModuleSpec.get_attached_message(); see there for more.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def max(self):\n        \"\"\"\"\"\"\n        try:\n            res = {\n                'target': self.target.max(),\n                'params': dict(\n                    zip(self.keys, self.params[self.target.argmax()])\n                )\n            }\n        except ValueError:\n            res = {}\n        return res", "output": "Get maximum target value found and corresponding parametes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __software_to_pkg_id(self, publisher, name, is_component, is_32bit):\n        '''\n        \n        '''\n        if publisher:\n            # remove , and lowercase as , are used as list separators\n            pub_lc = publisher.replace(',', '').lower()\n\n        else:\n            # remove , and lowercase\n            pub_lc = 'NoValue'  # Capitals/Special Value\n\n        if name:\n            name_lc = name.replace(',', '').lower()\n            # remove ,   OR we do the URL Encode on chars we do not want e.g. \\\\ and ,\n        else:\n            name_lc = 'NoValue'  # Capitals/Special Value\n\n        if is_component:\n            soft_type = 'comp'\n        else:\n            soft_type = 'soft'\n\n        if is_32bit:\n            soft_type += '32'  # Tag only the 32bit only\n\n        default_pkg_id = pub_lc+'\\\\\\\\'+name_lc+'\\\\\\\\'+soft_type\n\n        # Check to see if class was initialise with pkg_obj with a method called\n        # to_pkg_id, and if so use it for the naming standard instead of the default\n        if self.__pkg_obj and hasattr(self.__pkg_obj, 'to_pkg_id'):\n            pkg_id = self.__pkg_obj.to_pkg_id(publisher, name, is_component, is_32bit)\n            if pkg_id:\n                return pkg_id\n\n        return default_pkg_id", "output": "Determine the Package ID of a software/component using the\n        software/component ``publisher``, ``name``, whether its a software or a\n        component, and if its 32bit or 64bit archiecture.\n\n        Args:\n            publisher (str): Publisher of the software/component.\n            name (str): Name of the software.\n            is_component (bool): True if package is a component.\n            is_32bit (bool): True if the software/component is 32bit architecture.\n\n        Returns:\n            str: Package Id", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_raw(self):\n        \"\"\"\n        \n        \"\"\"\n        length = ctypes.c_ulong()\n        cptr = ctypes.POINTER(ctypes.c_char)()\n        _check_call(_LIB.XGBoosterGetModelRaw(self.handle,\n                                              ctypes.byref(length),\n                                              ctypes.byref(cptr)))\n        return ctypes2buffer(cptr, length.value)", "output": "Save the model to a in memory buffer represetation\n\n        Returns\n        -------\n        a in memory buffer represetation of the model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sort_values(self, return_indexer=False, ascending=True):\n        \"\"\"\n        \n        \"\"\"\n        if return_indexer:\n            _as = self.argsort()\n            if not ascending:\n                _as = _as[::-1]\n            sorted_index = self.take(_as)\n            return sorted_index, _as\n        else:\n            sorted_values = np.sort(self._ndarray_values)\n            attribs = self._get_attributes_dict()\n            freq = attribs['freq']\n\n            if freq is not None and not is_period_dtype(self):\n                if freq.n > 0 and not ascending:\n                    freq = freq * -1\n                elif freq.n < 0 and ascending:\n                    freq = freq * -1\n            attribs['freq'] = freq\n\n            if not ascending:\n                sorted_values = sorted_values[::-1]\n\n            return self._simple_new(sorted_values, **attribs)", "output": "Return sorted copy of Index.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def orc(self, path):\n        \"\"\"\n        \"\"\"\n        if isinstance(path, basestring):\n            return self._df(self._jreader.orc(path))\n        else:\n            raise TypeError(\"path can be only a single string\")", "output": "Loads a ORC file stream, returning the result as a :class:`DataFrame`.\n\n        .. note:: Evolving.\n\n        >>> orc_sdf = spark.readStream.schema(sdf_schema).orc(tempfile.mkdtemp())\n        >>> orc_sdf.isStreaming\n        True\n        >>> orc_sdf.schema == sdf_schema\n        True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def first_cyclic_node(head):\n    \"\"\"\n    \n    \"\"\"\n    runner = walker = head\n    while runner and runner.next:\n        runner = runner.next.next\n        walker = walker.next\n        if runner is walker:\n            break\n\n    if runner is None or runner.next is None:\n        return None\n\n    walker = head\n    while runner is not walker:\n        runner, walker = runner.next, walker.next\n    return runner", "output": ":type head: Node\n    :rtype: Node", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_config_file(path: str, final: bool = True) -> None:\n    \"\"\"\n    \"\"\"\n    return options.parse_config_file(path, final=final)", "output": "Parses global options from a config file.\n\n    See `OptionParser.parse_config_file`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_module(sym_filepath, params_filepath):\n    \"\"\"\n    \"\"\"\n    if not (os.path.isfile(sym_filepath) and os.path.isfile(params_filepath)):\n        raise ValueError(\"Symbol and params files provided are invalid\")\n    else:\n        try:\n            # reads symbol.json file from given path and\n            # retrieves model prefix and number of epochs\n            model_name = sym_filepath.rsplit('.', 1)[0].rsplit('-', 1)[0]\n            params_file_list = params_filepath.rsplit('.', 1)[0].rsplit('-', 1)\n            # Setting num_epochs to 0 if not present in filename\n            num_epochs = 0 if len(params_file_list) == 1 else int(params_file_list[1])\n        except IndexError:\n            logging.info(\"Model and params name should be in format: \"\n                         \"prefix-symbol.json, prefix-epoch.params\")\n            raise\n\n        sym, arg_params, aux_params = mx.model.load_checkpoint(model_name, num_epochs)\n\n        # Merging arg and aux parameters\n        params = {}\n        params.update(arg_params)\n        params.update(aux_params)\n\n        return sym, params", "output": "Loads the MXNet model file and\n    returns MXNet symbol and params (weights).\n\n    Parameters\n    ----------\n    json_path : str\n        Path to the json file\n    params_path : str\n        Path to the params file\n\n    Returns\n    -------\n    sym : MXNet symbol\n        Model symbol object\n\n    params : params object\n        Model weights including both arg and aux params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kernels_pull_cli(self,\n                         kernel,\n                         kernel_opt=None,\n                         path=None,\n                         metadata=False):\n        \"\"\" \n        \"\"\"\n        kernel = kernel or kernel_opt\n        effective_path = self.kernels_pull(\n            kernel, path=path, metadata=metadata, quiet=False)\n        if metadata:\n            print('Source code and metadata downloaded to ' + effective_path)\n        else:\n            print('Source code downloaded to ' + effective_path)", "output": "client wrapper for kernels_pull", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def score(self, X, eval_metric='acc', num_batch=None, batch_end_callback=None, reset=True):\n        \"\"\"\n        \"\"\"\n        # setup metric\n        if not isinstance(eval_metric, metric.EvalMetric):\n            eval_metric = metric.create(eval_metric)\n\n        X = self._init_iter(X, None, is_train=False)\n        if reset:\n            X.reset()\n\n        data_shapes = X.provide_data\n        data_names = [x[0] for x in data_shapes]\n        type_dict = dict((key, value.dtype) for (key, value) in self.arg_params.items())\n        for x in X.provide_data:\n            if isinstance(x, DataDesc):\n                type_dict[x.name] = x.dtype\n            else:\n                type_dict[x[0]] = mx_real_t\n\n        self._init_predictor(data_shapes, type_dict)\n        data_arrays = [self._pred_exec.arg_dict[name] for name in data_names]\n\n        for i, batch in enumerate(X):\n            if num_batch is not None and i == num_batch:\n                break\n            _load_data(batch, data_arrays)\n            self._pred_exec.forward(is_train=False)\n            eval_metric.update(batch.label, self._pred_exec.outputs)\n\n            if batch_end_callback is not None:\n                batch_end_params = BatchEndParam(epoch=0,\n                                                 nbatch=i,\n                                                 eval_metric=eval_metric,\n                                                 locals=locals())\n                _multiple_callbacks(batch_end_callback, batch_end_params)\n        return eval_metric.get()[1]", "output": "Run the model given an input and calculate the score\n        as assessed by an evaluation metric.\n\n        Parameters\n        ----------\n        X : mxnet.DataIter\n        eval_metric : metric.metric\n            The metric for calculating score.\n        num_batch : int or None\n            The number of batches to run. Go though all batches if ``None``.\n        Returns\n        -------\n        s : float\n            The final score.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def readlink(path, canonicalize=False):\n    '''\n    \n    '''\n    path = os.path.expanduser(path)\n\n    if not os.path.isabs(path):\n        raise SaltInvocationError('Path to link must be absolute.')\n\n    if not os.path.islink(path):\n        raise SaltInvocationError('A valid link was not specified.')\n\n    if canonicalize:\n        return os.path.realpath(path)\n    else:\n        return os.readlink(path)", "output": ".. versionadded:: 2014.1.0\n\n    Return the path that a symlink points to\n    If canonicalize is set to True, then it return the final target\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.readlink /path/to/link", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _multi_call(function, contentkey, *args, **kwargs):\n    '''\n    \n    '''\n    ret = function(*args, **kwargs)\n    position = ret.get('position')\n\n    while position:\n        more = function(*args, position=position, **kwargs)\n        ret[contentkey].extend(more[contentkey])\n        position = more.get('position')\n    return ret.get(contentkey)", "output": "Retrieve full list of values for the contentkey from a boto3 ApiGateway\n    client function that may be paged via 'position'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def profile_func(filename=None):\n    '''\n    \n    '''\n    def proffunc(fun):\n        def profiled_func(*args, **kwargs):\n            logging.info('Profiling function %s', fun.__name__)\n            try:\n                profiler = cProfile.Profile()\n                retval = profiler.runcall(fun, *args, **kwargs)\n                profiler.dump_stats((filename or '{0}_func.profile'\n                                     .format(fun.__name__)))\n            except IOError:\n                logging.exception('Could not open profile file %s', filename)\n\n            return retval\n        return profiled_func\n    return proffunc", "output": "Decorator for adding profiling to a nested function in Salt", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def virtual_memory():\n    '''\n    \n    '''\n    if psutil.version_info < (0, 6, 0):\n        msg = 'virtual_memory is only available in psutil 0.6.0 or greater'\n        raise CommandExecutionError(msg)\n    return dict(psutil.virtual_memory()._asdict())", "output": ".. versionadded:: 2014.7.0\n\n    Return a dict that describes statistics about system memory usage.\n\n    .. note::\n\n        This function is only available in psutil version 0.6.0 and above.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.virtual_memory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nextElementSibling(self):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlNextElementSibling(self._o)\n        if ret is None:return None\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Finds the first closest next sibling of the node which is\n          an element node. Note the handling of entities references\n          is different than in the W3C DOM element traversal spec\n          since we don't have back reference from entities content to\n           entities references.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_batch_end(self, last_loss, epoch, num_batch, **kwargs:Any)->None:\n        \"\"\n        if self.stop: return True #to skip validation after stopping during training\n        if torch.isnan(last_loss):\n            print (f'Epoch/Batch ({epoch}/{num_batch}): Invalid loss, terminating training.')\n            return {'stop_epoch': True, 'stop_training': True, 'skip_validate': True}", "output": "Test if `last_loss` is NaN and interrupts training.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _fill_mi_header(row, control_row):\n    \"\"\"\n    \"\"\"\n    last = row[0]\n    for i in range(1, len(row)):\n        if not control_row[i]:\n            last = row[i]\n\n        if row[i] == '' or row[i] is None:\n            row[i] = last\n        else:\n            control_row[i] = False\n            last = row[i]\n\n    return row, control_row", "output": "Forward fill blank entries in row but only inside the same parent index.\n\n    Used for creating headers in Multiindex.\n    Parameters\n    ----------\n    row : list\n        List of items in a single row.\n    control_row : list of bool\n        Helps to determine if particular column is in same parent index as the\n        previous value. Used to stop propagation of empty cells between\n        different indexes.\n\n    Returns\n    ----------\n    Returns changed row and control_row", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extract_from_urllib3():\n    \"\"\"\n    \n    \"\"\"\n    util.ssl_.SSLContext = orig_util_SSLContext\n    util.HAS_SNI = orig_util_HAS_SNI\n    util.ssl_.HAS_SNI = orig_util_HAS_SNI\n    util.IS_SECURETRANSPORT = False\n    util.ssl_.IS_SECURETRANSPORT = False", "output": "Undo monkey-patching by :func:`inject_into_urllib3`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install(self, version, upgrade=False):\n        \"\"\"\n        \n        \"\"\"\n        print(\"Installing version: \" + colorize(\"info\", version))\n\n        self.make_lib(version)\n        self.make_bin()\n        self.make_env()\n        self.update_path()\n\n        return 0", "output": "Installs Poetry in $POETRY_HOME.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_mirror(name, config_path=_DEFAULT_CONFIG_PATH, force=False):\n    '''\n    \n    '''\n    _validate_config(config_path)\n    force = six.text_type(bool(force)).lower()\n\n    current_mirror = __salt__['aptly.get_mirror'](name=name, config_path=config_path)\n\n    if not current_mirror:\n        log.debug('Mirror already absent: %s', name)\n        return True\n\n    cmd = ['mirror', 'drop', '-config={}'.format(config_path),\n           '-force={}'.format(force), name]\n\n    _cmd_run(cmd)\n    mirror = __salt__['aptly.get_mirror'](name=name, config_path=config_path)\n\n    if mirror:\n        log.error('Unable to remove mirror: %s', name)\n        return False\n    log.debug('Removed mirror: %s', name)\n    return True", "output": "Remove a mirrored remote repository. By default, Package data is not removed.\n\n    :param str name: The name of the remote repository mirror.\n    :param str config_path: The path to the configuration file for the aptly instance.\n    :param bool force: Whether to remove the mirror even if it is used as the source\n        of an existing snapshot.\n\n    :return: A boolean representing whether all changes succeeded.\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' aptly.delete_mirror name=\"test-mirror\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_unpacked_egg(path):\n    \"\"\"\n    \n    \"\"\"\n    return (\n        _is_egg_path(path) and\n        os.path.isfile(os.path.join(path, 'EGG-INFO', 'PKG-INFO'))\n    )", "output": "Determine if given path appears to be an unpacked egg.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def connected():\n    '''\n    \n    '''\n    opts = salt.config.master_config(__opts__['conf_file'])\n\n    if opts.get('con_cache'):\n        cache_cli = CacheCli(opts)\n        minions = cache_cli.get_cached()\n    else:\n        minions = list(salt.utils.minions.CkMinions(opts).connected_ids())\n    return minions", "output": "List all connected minions on a salt-master", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_moe_base():\n  \"\"\"\"\"\"\n  hparams = common_hparams.basic_params1()\n  hparams.norm_type = \"layer\"\n  hparams.hidden_size = 512\n  hparams.batch_size = 4096\n  hparams.max_length = 2001\n  hparams.max_input_seq_length = 2000\n  hparams.max_target_seq_length = 2000\n  hparams.dropout = 0.0\n  hparams.clip_grad_norm = 0.  # i.e. no gradient clipping\n  hparams.optimizer_adam_epsilon = 1e-9\n  hparams.learning_rate_decay_scheme = \"noam\"\n  hparams.learning_rate = 0.1\n  hparams.learning_rate_warmup_steps = 2000\n  hparams.initializer_gain = 1.0\n  hparams.num_hidden_layers = 5\n  hparams.initializer = \"uniform_unit_scaling\"\n  hparams.weight_decay = 0.0\n  hparams.optimizer_adam_beta1 = 0.9\n  hparams.optimizer_adam_beta2 = 0.98\n  hparams.num_sampled_classes = 0\n  hparams.label_smoothing = 0.0\n  hparams.shared_embedding_and_softmax_weights = True\n  # According to noam, (\"n\", \"da\") seems better for harder-to-learn models\n  hparams.layer_preprocess_sequence = \"n\"\n  hparams.layer_postprocess_sequence = \"da\"\n\n  # Hparams used by transformer_prepare_decoder() function\n  hparams.add_hparam(\"pos\", \"timing\")  # timing, none\n  hparams.add_hparam(\"proximity_bias\", False)\n  hparams.add_hparam(\"causal_decoder_self_attention\", True)\n\n  hparams = common_attention.add_standard_attention_hparams(hparams)\n\n  # Decoder layers type. If set, num_decoder_layers parameter will be ignored\n  # and the number of decoder layer will be deduced from the string\n  # See top file comment for example of usage\n  hparams.add_hparam(\"layer_types\", \"\")\n  # Default attention type (ex: a, loc, red,...) and feed-forward type (ex: fc,\n  # sep, moe,...)\n  hparams.add_hparam(\"default_att\", \"a\")\n  hparams.add_hparam(\"default_ff\", \"fc\")\n\n  return hparams", "output": "Set of hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_browser_controller(browser=None):\n    ''' \n\n    '''\n    browser = settings.browser(browser)\n\n    if browser is not None:\n        if browser == 'none':\n            controller = DummyWebBrowser()\n        else:\n            controller = webbrowser.get(browser)\n    else:\n        controller = webbrowser\n\n    return controller", "output": "Return a browser controller.\n\n    Args:\n        browser (str or None) : browser name, or ``None`` (default: ``None``)\n            If passed the string ``'none'``, a dummy web browser controller\n            is returned\n\n            Otherwise, use the value to select an appropriate controller using\n            the ``webbrowser`` standard library module. In the value is\n            ``None`` then a system default is used.\n\n    .. note::\n        If the environment variable ``BOKEH_BROWSER`` is set, it will take\n        precedence.\n\n    Returns:\n        controller : a web browser controller", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def FromMicroseconds(self, micros):\n    \"\"\"\"\"\"\n    self._NormalizeDuration(\n        micros // _MICROS_PER_SECOND,\n        (micros % _MICROS_PER_SECOND) * _NANOS_PER_MICROSECOND)", "output": "Converts microseconds to Duration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install(path, user, admin_user, admin_password, admin_email, title, url):\n    '''\n    \n    '''\n    retcode = __salt__['cmd.retcode']((\n        'wp --path={0} core install '\n        '--title=\"{1}\" '\n        '--admin_user={2} '\n        \"--admin_password='{3}' \"\n        '--admin_email={4} '\n        '--url={5}'\n    ).format(\n        path,\n        title,\n        admin_user,\n        admin_password,\n        admin_email,\n        url\n    ), runas=user)\n\n    if retcode == 0:\n        return True\n    return False", "output": "Run the initial setup functions for a wordpress install\n\n    path\n        path to wordpress install location\n\n    user\n        user to run the command as\n\n    admin_user\n        Username for the Administrative user for the wordpress install\n\n    admin_password\n        Initial Password for the Administrative user for the wordpress install\n\n    admin_email\n        Email for the Administrative user for the wordpress install\n\n    title\n        Title of the wordpress website for the wordpress install\n\n    url\n        Url for the wordpress install\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' wordpress.install /var/www/html apache dwallace password123 \\\n            dwallace@example.com \"Daniel's Awesome Blog\" https://blog.dwallace.com", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_gapic(operation, operations_client, result_type, **kwargs):\n    \"\"\"\n    \"\"\"\n    refresh = functools.partial(operations_client.get_operation, operation.name)\n    cancel = functools.partial(operations_client.cancel_operation, operation.name)\n    return Operation(operation, refresh, cancel, result_type, **kwargs)", "output": "Create an operation future from a gapic client.\n\n    This interacts with the long-running operations `service`_ (specific\n    to a given API) via a gapic client.\n\n    .. _service: https://github.com/googleapis/googleapis/blob/\\\n                 050400df0fdb16f63b63e9dee53819044bffc857/\\\n                 google/longrunning/operations.proto#L38\n\n    Args:\n        operation (google.longrunning.operations_pb2.Operation): The operation.\n        operations_client (google.api_core.operations_v1.OperationsClient):\n            The operations client.\n        result_type (:func:`type`): The protobuf result type.\n        kwargs: Keyword args passed into the :class:`Operation` constructor.\n\n    Returns:\n        ~.api_core.operation.Operation: The operation future to track the given\n            operation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_endpoint_configuration(self, rest_api, value):\n        \"\"\"\n        \n        \"\"\"\n\n        rest_api.EndpointConfiguration = {\"Types\": [value]}\n        rest_api.Parameters = {\"endpointConfigurationTypes\": value}", "output": "Sets endpoint configuration property of AWS::ApiGateway::RestApi resource\n        :param rest_api: RestApi resource\n        :param string/dict value: Value to be set", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def iscallable(self, objtxt):\r\n        \"\"\"\"\"\"\r\n        obj, valid = self._eval(objtxt)\r\n        if valid:\r\n            return callable(obj)", "output": "Is object callable?", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_session_metric_values(self, session_name):\n    \"\"\"\"\"\"\n\n    # result is a list of api_pb2.MetricValue instances.\n    result = []\n    metric_infos = self._experiment.metric_infos\n    for metric_info in metric_infos:\n      metric_name = metric_info.name\n      try:\n        metric_eval = metrics.last_metric_eval(\n            self._context.multiplexer,\n            session_name,\n            metric_name)\n      except KeyError:\n        # It's ok if we don't find the metric in the session.\n        # We skip it here. For filtering and sorting purposes its value is None.\n        continue\n\n      # metric_eval is a 3-tuple of the form [wall_time, step, value]\n      result.append(api_pb2.MetricValue(name=metric_name,\n                                        wall_time_secs=metric_eval[0],\n                                        training_step=metric_eval[1],\n                                        value=metric_eval[2]))\n    return result", "output": "Builds the session metric values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_header_validity(header):\n    \"\"\"\n    \"\"\"\n    name, value = header\n\n    if isinstance(value, bytes):\n        pat = _CLEAN_HEADER_REGEX_BYTE\n    else:\n        pat = _CLEAN_HEADER_REGEX_STR\n    try:\n        if not pat.match(value):\n            raise InvalidHeader(\"Invalid return character or leading space in header: %s\" % name)\n    except TypeError:\n        raise InvalidHeader(\"Value for header {%s: %s} must be of type str or \"\n                            \"bytes, not %s\" % (name, value, type(value)))", "output": "Verifies that header value is a string which doesn't contain\n    leading whitespace or return characters. This prevents unintended\n    header injection.\n\n    :param header: tuple, in the format (name, value).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def AddAccuracy(model, softmax, label):\n    \"\"\"\"\"\"\n    accuracy = brew.accuracy(model, [softmax, label], \"accuracy\")\n    return accuracy", "output": "Adds an accuracy op to the model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_dvs_config(config_spec, config_dict):\n    '''\n    \n    '''\n    if config_dict.get('name'):\n        config_spec.name = config_dict['name']\n    if config_dict.get('contact_email') or config_dict.get('contact_name'):\n        if not config_spec.contact:\n            config_spec.contact = vim.DVSContactInfo()\n        config_spec.contact.contact = config_dict.get('contact_email')\n        config_spec.contact.name = config_dict.get('contact_name')\n    if config_dict.get('description'):\n        config_spec.description = config_dict.get('description')\n    if config_dict.get('max_mtu'):\n        config_spec.maxMtu = config_dict.get('max_mtu')\n    if config_dict.get('lacp_api_version'):\n        config_spec.lacpApiVersion = config_dict.get('lacp_api_version')\n    if config_dict.get('network_resource_control_version'):\n        config_spec.networkResourceControlVersion = \\\n                config_dict.get('network_resource_control_version')\n    if config_dict.get('uplink_names'):\n        if not config_spec.uplinkPortPolicy or \\\n           not isinstance(config_spec.uplinkPortPolicy,\n                          vim.DVSNameArrayUplinkPortPolicy):\n\n            config_spec.uplinkPortPolicy = \\\n                    vim.DVSNameArrayUplinkPortPolicy()\n        config_spec.uplinkPortPolicy.uplinkPortName = \\\n                config_dict['uplink_names']", "output": "Applies the values of the config dict dictionary to a config spec\n    (vim.VMwareDVSConfigSpec)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_string_array(data, encoding, errors, itemsize=None):\n    \"\"\"\n    \n    \"\"\"\n\n    # encode if needed\n    if encoding is not None and len(data):\n        data = Series(data.ravel()).str.encode(\n            encoding, errors).values.reshape(data.shape)\n\n    # create the sized dtype\n    if itemsize is None:\n        ensured = ensure_object(data.ravel())\n        itemsize = max(1, libwriters.max_len_string_array(ensured))\n\n    data = np.asarray(data, dtype=\"S{size}\".format(size=itemsize))\n    return data", "output": "we take a string-like that is object dtype and coerce to a fixed size\n    string type\n\n    Parameters\n    ----------\n    data : a numpy array of object dtype\n    encoding : None or string-encoding\n    errors : handler for encoding errors\n    itemsize : integer, optional, defaults to the max length of the strings\n\n    Returns\n    -------\n    data in a fixed-length string dtype, encoded to bytes if needed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(cls, tokens:Tokens, max_vocab:int, min_freq:int) -> 'Vocab':\n        \"\"\n        freq = Counter(p for o in tokens for p in o)\n        itos = [o for o,c in freq.most_common(max_vocab) if c >= min_freq]\n        for o in reversed(defaults.text_spec_tok):\n            if o in itos: itos.remove(o)\n            itos.insert(0, o)\n        return cls(itos)", "output": "Create a vocabulary from a set of `tokens`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deliver_slice(schedule):\n    \"\"\"\n    \n    \"\"\"\n    if schedule.email_format == SliceEmailReportFormat.data:\n        email = _get_slice_data(schedule)\n    elif schedule.email_format == SliceEmailReportFormat.visualization:\n        email = _get_slice_visualization(schedule)\n    else:\n        raise RuntimeError('Unknown email report format')\n\n    subject = __(\n        '%(prefix)s %(title)s',\n        prefix=config.get('EMAIL_REPORTS_SUBJECT_PREFIX'),\n        title=schedule.slice.slice_name,\n    )\n\n    _deliver_email(schedule, subject, email)", "output": "Given a schedule, delivery the slice as an email report", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _bytes_to_json(value):\n    \"\"\"\"\"\"\n    if isinstance(value, bytes):\n        value = base64.standard_b64encode(value).decode(\"ascii\")\n    return value", "output": "Coerce 'value' to an JSON-compatible representation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _diff(old_pipeline_definition, new_pipeline_definition):\n    '''\n    \n    '''\n    old_pipeline_definition.pop('ResponseMetadata', None)\n    new_pipeline_definition.pop('ResponseMetadata', None)\n\n    diff = salt.utils.data.decode(difflib.unified_diff(\n        salt.utils.json.dumps(old_pipeline_definition, indent=4).splitlines(True),\n        salt.utils.json.dumps(new_pipeline_definition, indent=4).splitlines(True),\n    ))\n    return ''.join(diff)", "output": "Return string diff of pipeline definitions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_by_name(opname, operators):\n    \"\"\"\n\n    \"\"\"\n    ret_op_classes = [op for op in operators if op.__name__ == opname]\n\n    if len(ret_op_classes) == 0:\n        raise TypeError('Cannot found operator {} in operator dictionary'.format(opname))\n    elif len(ret_op_classes) > 1:\n        raise ValueError(\n            'Found duplicate operators {} in operator dictionary. Please check '\n            'your dictionary file.'.format(opname)\n        )\n    ret_op_class = ret_op_classes[0]\n    return ret_op_class", "output": "Return operator class instance by name.\n\n    Parameters\n    ----------\n    opname: str\n        Name of the sklearn class that belongs to a TPOT operator\n    operators: list\n        List of operator classes from operator library\n\n    Returns\n    -------\n    ret_op_class: class\n        An operator class", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_log_rhos(target_action_log_probs, behaviour_action_log_probs):\n    \"\"\"\"\"\"\n    t = tf.stack(target_action_log_probs)\n    b = tf.stack(behaviour_action_log_probs)\n    log_rhos = tf.reduce_sum(t - b, axis=0)\n    return log_rhos", "output": "With the selected log_probs for multi-discrete actions of behaviour\n    and target policies we compute the log_rhos for calculating the vtrace.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_multiscale(image, resolutions,\n                    resize_method=tf.image.ResizeMethod.BICUBIC,\n                    num_channels=3):\n  \"\"\"\n  \"\"\"\n  scaled_images = []\n  for height in resolutions:\n    scaled_image = tf.image.resize_images(\n        image,\n        size=[height, height],  # assuming that height = width\n        method=resize_method)\n    scaled_image = tf.to_int64(scaled_image)\n    scaled_image.set_shape([height, height, num_channels])\n    scaled_images.append(scaled_image)\n\n  return scaled_images", "output": "Returns list of scaled images, one for each resolution.\n\n  Args:\n    image: Tensor of shape [height, height, num_channels].\n    resolutions: List of heights that image's height is resized to.\n    resize_method: tf.image.ResizeMethod.\n    num_channels: Number of channels in image.\n\n  Returns:\n    List of Tensors, one for each resolution with shape given by\n    [resolutions[i], resolutions[i], num_channels].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cleanup_closed(self) -> None:\n        \"\"\"\n        \"\"\"\n        if self._cleanup_closed_handle:\n            self._cleanup_closed_handle.cancel()\n\n        for transport in self._cleanup_closed_transports:\n            if transport is not None:\n                transport.abort()\n\n        self._cleanup_closed_transports = []\n\n        if not self._cleanup_closed_disabled:\n            self._cleanup_closed_handle = helpers.weakref_handle(\n                self, '_cleanup_closed',\n                self._cleanup_closed_period, self._loop)", "output": "Double confirmation for transport close.\n        Some broken ssl servers may leave socket open without proper close.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_shortcuts(self):\r\n        \"\"\"\"\"\"\r\n        conflicts = []\r\n        for index, sh1 in enumerate(self.source_model.shortcuts):\r\n            if index == len(self.source_model.shortcuts)-1:\r\n                break\r\n            if str(sh1.key) == '':\r\n                continue\r\n            for sh2 in self.source_model.shortcuts[index+1:]:\r\n                if sh2 is sh1:\r\n                    continue\r\n                if str(sh2.key) == str(sh1.key) \\\r\n                   and (sh1.context == sh2.context or sh1.context == '_' or\r\n                        sh2.context == '_'):\r\n                    conflicts.append((sh1, sh2))\r\n        if conflicts:\r\n            self.parent().show_this_page.emit()\r\n            cstr = \"\\n\".join(['%s <---> %s' % (sh1, sh2)\r\n                              for sh1, sh2 in conflicts])\r\n            QMessageBox.warning(self, _(\"Conflicts\"),\r\n                                _(\"The following conflicts have been \"\r\n                                  \"detected:\")+\"\\n\"+cstr, QMessageBox.Ok)", "output": "Check shortcuts for conflicts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_arguments(self, name: str, strip: bool = True) -> List[str]:\n        \"\"\"\n        \"\"\"\n\n        # Make sure `get_arguments` isn't accidentally being called with a\n        # positional argument that's assumed to be a default (like in\n        # `get_argument`.)\n        assert isinstance(strip, bool)\n\n        return self._get_arguments(name, self.request.arguments, strip)", "output": "Returns a list of the arguments with the given name.\n\n        If the argument is not present, returns an empty list.\n\n        This method searches both the query and body arguments.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ensure_scope(level, global_dict=None, local_dict=None, resolvers=(),\n                  target=None, **kwargs):\n    \"\"\"\"\"\"\n    return Scope(level + 1, global_dict=global_dict, local_dict=local_dict,\n                 resolvers=resolvers, target=target)", "output": "Ensure that we are grabbing the correct scope.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _SimpleEncoder(wire_type, encode_value, compute_value_size):\n  \"\"\"\n  \"\"\"\n\n  def SpecificEncoder(field_number, is_repeated, is_packed):\n    if is_packed:\n      tag_bytes = TagBytes(field_number, wire_format.WIRETYPE_LENGTH_DELIMITED)\n      local_EncodeVarint = _EncodeVarint\n      def EncodePackedField(write, value):\n        write(tag_bytes)\n        size = 0\n        for element in value:\n          size += compute_value_size(element)\n        local_EncodeVarint(write, size)\n        for element in value:\n          encode_value(write, element)\n      return EncodePackedField\n    elif is_repeated:\n      tag_bytes = TagBytes(field_number, wire_type)\n      def EncodeRepeatedField(write, value):\n        for element in value:\n          write(tag_bytes)\n          encode_value(write, element)\n      return EncodeRepeatedField\n    else:\n      tag_bytes = TagBytes(field_number, wire_type)\n      def EncodeField(write, value):\n        write(tag_bytes)\n        return encode_value(write, value)\n      return EncodeField\n\n  return SpecificEncoder", "output": "Return a constructor for an encoder for fields of a particular type.\n\n  Args:\n      wire_type:  The field's wire type, for encoding tags.\n      encode_value:  A function which encodes an individual value, e.g.\n        _EncodeVarint().\n      compute_value_size:  A function which computes the size of an individual\n        value, e.g. _VarintSize().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def looks_like_python(name):\n    # type: (str) -> bool\n    \"\"\"\n    \n    \"\"\"\n\n    if not any(name.lower().startswith(py_name) for py_name in PYTHON_IMPLEMENTATIONS):\n        return False\n    match = RE_MATCHER.match(name)\n    if match:\n        return any(fnmatch(name, rule) for rule in MATCH_RULES)\n    return False", "output": "Determine whether the supplied filename looks like a possible name of python.\n\n    :param str name: The name of the provided file.\n    :return: Whether the provided name looks like python.\n    :rtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_api_key_description(apiKey, description, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        response = _api_key_patch_replace(conn, apiKey, '/description', description)\n        return {'updated': True, 'apiKey': _convert_datetime_str(response)}\n    except ClientError as e:\n        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}", "output": "update the given apiKey with the given description.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_apigateway.update_api_key_description api_key description", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_from_pb(self, instance_pb):\n        \"\"\"\n        \"\"\"\n        if not instance_pb.display_name:  # Simple field (string)\n            raise ValueError(\"Instance protobuf does not contain display_name\")\n        self.display_name = instance_pb.display_name\n        self.configuration_name = instance_pb.config\n        self.node_count = instance_pb.node_count", "output": "Refresh self from the server-provided protobuf.\n\n        Helper for :meth:`from_pb` and :meth:`reload`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sanitize_value(x):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(x, _six.string_types + _six.integer_types + (float,)):\n        return x\n    elif _HAS_SKLEARN and _sp.issparse(x):\n        return x.todense()\n    elif isinstance(x, _np.ndarray):\n        return x\n    elif isinstance(x, tuple):\n        return (_sanitize_value(v) for v in x)\n    elif isinstance(x, list):\n        return [_sanitize_value(v) for v in x]\n    elif isinstance(x, dict):\n        return dict( (_sanitize_value(k), _sanitize_value(v)) for k, v in x.items())\n    else:\n        assert False, str(x)", "output": "Performs cleaning steps on the data so various type comparisons can\n    be performed correctly.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read(self, size=1024):\n        \"\"\"\n        \"\"\"\n        b = super(PtyProcessUnicode, self).read(size)\n        return self.decoder.decode(b, final=False)", "output": "Read at most ``size`` bytes from the pty, return them as unicode.\n\n        Can block if there is nothing to read. Raises :exc:`EOFError` if the\n        terminal was closed.\n\n        The size argument still refers to bytes, not unicode code points.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_optional(annotation: type):\n    \"\"\"\n    \n    \"\"\"\n    origin = getattr(annotation, '__origin__', None)\n    args = getattr(annotation, '__args__', ())\n    if origin == Union and len(args) == 2 and args[1] == type(None):\n        return args[0]\n    else:\n        return annotation", "output": "Optional[X] annotations are actually represented as Union[X, NoneType].\n    For our purposes, the \"Optional\" part is not interesting, so here we\n    throw it away.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def isdir(self, path):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            self.remote_context.check_output([\"test\", \"-d\", path])\n        except subprocess.CalledProcessError as e:\n            if e.returncode == 1:\n                return False\n            else:\n                raise\n        return True", "output": "Return `True` if directory at `path` exist, False otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_manager(sdf, columns, index):\n    \"\"\" \n    \"\"\"\n\n    # from BlockManager perspective\n    axes = [ensure_index(columns), ensure_index(index)]\n\n    return create_block_manager_from_arrays(\n        [sdf[c] for c in columns], columns, axes)", "output": "create and return the block manager from a dataframe of series,\n    columns, index", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_datasource(jboss_config, name, profile=None):\n    '''\n    \n       '''\n    log.debug(\"======================== MODULE FUNCTION: jboss7.read_datasource, name=%s\", name)\n    return __read_datasource(jboss_config, name, profile)", "output": "Read datasource properties in the running jboss instance.\n\n    jboss_config\n        Configuration dictionary with properties specified above.\n    name\n        Datasource name\n    profile\n        Profile name (JBoss domain mode only)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' jboss7.read_datasource '{\"cli_path\": \"integration.modules.sysmod.SysModuleTest.test_valid_docs\", \"controller\": \"10.11.12.13:9999\", \"cli_user\": \"jbossadm\", \"cli_password\": \"jbossadm\"}'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def next(self) -> Any:\n        \"\"\"\"\"\"\n        item = await self.stream.next()\n        if self.stream.at_eof():\n            await self.release()\n        return item", "output": "Emits next multipart reader object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_rnn_checkpoint(cells, prefix, epoch):\n    \"\"\"\n    \"\"\"\n    sym, arg, aux = load_checkpoint(prefix, epoch)\n    if isinstance(cells, BaseRNNCell):\n        cells = [cells]\n    for cell in cells:\n        arg = cell.pack_weights(arg)\n\n    return sym, arg, aux", "output": "Load model checkpoint from file.\n    Pack weights after loading.\n\n    Parameters\n    ----------\n    cells : mxnet.rnn.RNNCell or list of RNNCells\n        The RNN cells used by this symbol.\n    prefix : str\n        Prefix of model name.\n    epoch : int\n        Epoch number of model we would like to load.\n\n    Returns\n    -------\n    symbol : Symbol\n        The symbol configuration of computation network.\n    arg_params : dict of str to NDArray\n        Model parameter, dict of name to NDArray of net's weights.\n    aux_params : dict of str to NDArray\n        Model parameter, dict of name to NDArray of net's auxiliary states.\n\n    Notes\n    -----\n    - symbol will be loaded from ``prefix-symbol.json``.\n    - parameters will be loaded from ``prefix-epoch.params``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def optimize_rfc(data, targets):\n    \"\"\"\"\"\"\n    def rfc_crossval(n_estimators, min_samples_split, max_features):\n        \"\"\"Wrapper of RandomForest cross validation.\n\n        Notice how we ensure n_estimators and min_samples_split are casted\n        to integer before we pass them along. Moreover, to avoid max_features\n        taking values outside the (0, 1) range, we also ensure it is capped\n        accordingly.\n        \"\"\"\n        return rfc_cv(\n            n_estimators=int(n_estimators),\n            min_samples_split=int(min_samples_split),\n            max_features=max(min(max_features, 0.999), 1e-3),\n            data=data,\n            targets=targets,\n        )\n\n    optimizer = BayesianOptimization(\n        f=rfc_crossval,\n        pbounds={\n            \"n_estimators\": (10, 250),\n            \"min_samples_split\": (2, 25),\n            \"max_features\": (0.1, 0.999),\n        },\n        random_state=1234,\n        verbose=2\n    )\n    optimizer.maximize(n_iter=10)\n\n    print(\"Final result:\", optimizer.max)", "output": "Apply Bayesian Optimization to Random Forest parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def patch_traces(self, project_id, traces):\n        \"\"\"\n        \n        \"\"\"\n        traces_pb = _traces_mapping_to_pb(traces)\n        self._gapic_api.patch_traces(project_id, traces_pb)", "output": "Sends new traces to Stackdriver Trace or updates existing traces.\n\n        Args:\n            project_id (Optional[str]): ID of the Cloud project where the trace\n                data is stored.\n            traces (dict): Required. The traces to be patched in the API call.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _symmetric_warp(c, magnitude:partial(uniform,size=4)=0, invert=False):\n    \"\"\n    m = listify(magnitude, 4)\n    targ_pts = [[-1-m[3],-1-m[1]], [-1-m[2],1+m[1]], [1+m[3],-1-m[0]], [1+m[2],1+m[0]]]\n    return _do_perspective_warp(c, targ_pts, invert)", "output": "Apply symmetric warp of `magnitude` to `c`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _axis_levels(self, axis):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        ax = self._axis(axis)\r\n        return 1 if not hasattr(ax, 'levels') else len(ax.levels)", "output": "Return the number of levels in the labels taking into account the axis.\r\n\r\n        Get the number of levels for the columns (0) or rows (1).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_ago_to_since(since: str) -> str:\n    \"\"\"\n    \n    \"\"\"\n    since_words = since.split(' ')\n    grains = ['days', 'years', 'hours', 'day', 'year', 'weeks']\n    if (len(since_words) == 2 and since_words[1] in grains):\n        since += ' ago'\n    return since", "output": "Backwards compatibility hack. Without this slices with since: 7 days will\n    be treated as 7 days in the future.\n\n    :param str since:\n    :returns: Since with ago added if necessary\n    :rtype: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_stream(self, stream):\n        \"\"\"\n        \n        \"\"\"\n        batches = super(ArrowStreamPandasSerializer, self).load_stream(stream)\n        import pyarrow as pa\n        for batch in batches:\n            yield [self.arrow_to_pandas(c) for c in pa.Table.from_batches([batch]).itercolumns()]", "output": "Deserialize ArrowRecordBatches to an Arrow table and return as a list of pandas.Series.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reconnect(self):\n        \"\"\"\"\"\"\n        parsed = urlparse.urlparse(self.amqp_url)\n        port = parsed.port or 5672\n        self.connection = amqp.Connection(host=\"%s:%s\" % (parsed.hostname, port),\n                                          userid=parsed.username or 'guest',\n                                          password=parsed.password or 'guest',\n                                          virtual_host=unquote(\n                                              parsed.path.lstrip('/') or '%2F'))\n        self.channel = self.connection.channel()\n        try:\n            self.channel.queue_declare(self.name)\n        except amqp.exceptions.PreconditionFailed:\n            pass", "output": "Reconnect to rabbitmq server", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_learner_stats(grad_info):\n    \"\"\"\n    \"\"\"\n\n    if LEARNER_STATS_KEY in grad_info:\n        return grad_info[LEARNER_STATS_KEY]\n\n    multiagent_stats = {}\n    for k, v in grad_info.items():\n        if type(v) is dict:\n            if LEARNER_STATS_KEY in v:\n                multiagent_stats[k] = v[LEARNER_STATS_KEY]\n\n    return multiagent_stats", "output": "Return optimization stats reported from the policy graph.\n\n    Example:\n        >>> grad_info = evaluator.learn_on_batch(samples)\n        >>> print(get_stats(grad_info))\n        {\"vf_loss\": ..., \"policy_loss\": ...}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_video_frames(self, path):\n        \"\"\"\n        \n        \"\"\"\n        videogen = skvideo.io.vreader(path)\n        frames = np.array([frame for frame in videogen])\n        return frames", "output": "Get video frames", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_for_numeric_binop(self, other, op):\n        \"\"\"\n        \n        \"\"\"\n        opstr = '__{opname}__'.format(opname=op.__name__)\n        # if we are an inheritor of numeric,\n        # but not actually numeric (e.g. DatetimeIndex/PeriodIndex)\n        if not self._is_numeric_dtype:\n            raise TypeError(\"cannot evaluate a numeric op {opstr} \"\n                            \"for type: {typ}\"\n                            .format(opstr=opstr, typ=type(self).__name__))\n\n        if isinstance(other, Index):\n            if not other._is_numeric_dtype:\n                raise TypeError(\"cannot evaluate a numeric op \"\n                                \"{opstr} with type: {typ}\"\n                                .format(opstr=opstr, typ=type(other)))\n        elif isinstance(other, np.ndarray) and not other.ndim:\n            other = other.item()\n\n        if isinstance(other, (Index, ABCSeries, np.ndarray)):\n            if len(self) != len(other):\n                raise ValueError(\"cannot evaluate a numeric op with \"\n                                 \"unequal lengths\")\n            other = com.values_from_object(other)\n            if other.dtype.kind not in ['f', 'i', 'u']:\n                raise TypeError(\"cannot evaluate a numeric op \"\n                                \"with a non-numeric dtype\")\n        elif isinstance(other, (ABCDateOffset, np.timedelta64, timedelta)):\n            # higher up to handle\n            pass\n        elif isinstance(other, (datetime, np.datetime64)):\n            # higher up to handle\n            pass\n        else:\n            if not (is_float(other) or is_integer(other)):\n                raise TypeError(\"can only perform ops with scalar values\")\n\n        return other", "output": "Return valid other; evaluate or raise TypeError if we are not of\n        the appropriate type.\n\n        Notes\n        -----\n        This is an internal method called by ops.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_plugin_data(self, plugin):\n        \"\"\"\"\"\"\n        # The data object is named \"data\" in the editor plugin while it is\n        # named \"clients\" in the notebook plugin.\n        try:\n            data = plugin.get_current_tab_manager().data\n        except AttributeError:\n            data = plugin.get_current_tab_manager().clients\n\n        return data", "output": "Get the data object of the plugin's current tab manager.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def disallowed_in_before_trading_start(exception):\n    \"\"\"\n    \n    \"\"\"\n    def decorator(method):\n        @wraps(method)\n        def wrapped_method(self, *args, **kwargs):\n            if self._in_before_trading_start:\n                raise exception\n            return method(self, *args, **kwargs)\n        return wrapped_method\n    return decorator", "output": "Decorator for API methods that cannot be called from within\n    TradingAlgorithm.before_trading_start.  `exception` will be raised if the\n    method is called inside `before_trading_start`.\n\n    Examples\n    --------\n    @disallowed_in_before_trading_start(SomeException(\"Don't do that!\"))\n    def method(self):\n        # Do stuff that is not allowed inside before_trading_start.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_params():\n    '''  '''\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data_dir\", type=str, default='/tmp/tensorflow/mnist/input_data', help=\"data directory\")\n    parser.add_argument(\"--dropout_rate\", type=float, default=0.5, help=\"dropout rate\")\n    parser.add_argument(\"--channel_1_num\", type=int, default=32)\n    parser.add_argument(\"--channel_2_num\", type=int, default=64)\n    parser.add_argument(\"--conv_size\", type=int, default=5)\n    parser.add_argument(\"--pool_size\", type=int, default=2)\n    parser.add_argument(\"--hidden_size\", type=int, default=1024)\n    parser.add_argument(\"--learning_rate\", type=float, default=1e-4)\n    parser.add_argument(\"--batch_num\", type=int, default=2700)\n    parser.add_argument(\"--batch_size\", type=int, default=32)\n\n    args, _ = parser.parse_known_args()\n    return args", "output": "Get parameters from command line", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def profile_config_get(name, config_key, remote_addr=None,\n                       cert=None, key=None, verify_cert=True):\n    ''' \n    '''\n    profile = profile_get(\n        name,\n        remote_addr,\n        cert,\n        key,\n        verify_cert,\n        _raw=True\n    )\n\n    return _get_property_dict_item(profile, 'config', config_key)", "output": "Get a profile config item.\n\n        name :\n            The name of the profile to get the config item from.\n\n        config_key :\n            The key for the item to retrieve.\n\n        remote_addr :\n            An URL to a remote Server, you also have to give cert and key if\n            you provide remote_addr and its a TCP Address!\n\n            Examples:\n                https://myserver.lan:8443\n                /var/lib/mysocket.sock\n\n        cert :\n            PEM Formatted SSL Certificate.\n\n            Examples:\n                ~/.config/lxc/client.crt\n\n        key :\n            PEM Formatted SSL Key.\n\n            Examples:\n                ~/.config/lxc/client.key\n\n        verify_cert : True\n            Wherever to verify the cert, this is by default True\n            but in the most cases you want to set it off as LXD\n            normaly uses self-signed certificates.\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            $ salt '*' lxd.profile_config_get autostart boot.autostart", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def annealing_cos(start:Number, end:Number, pct:float)->Number:\n    \"\"\n    cos_out = np.cos(np.pi * pct) + 1\n    return end + (start-end)/2 * cos_out", "output": "Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def future_set_exception_unless_cancelled(\n    future: \"Union[futures.Future[_T], Future[_T]]\", exc: BaseException\n) -> None:\n    \"\"\"\n\n    \"\"\"\n    if not future.cancelled():\n        future.set_exception(exc)\n    else:\n        app_log.error(\"Exception after Future was cancelled\", exc_info=exc)", "output": "Set the given ``exc`` as the `Future`'s exception.\n\n    If the Future is already canceled, logs the exception instead. If\n    this logging is not desired, the caller should explicitly check\n    the state of the Future and call ``Future.set_exception`` instead of\n    this wrapper.\n\n    Avoids ``asyncio.InvalidStateError`` when calling ``set_exception()`` on\n    a cancelled `asyncio.Future`.\n\n    .. versionadded:: 6.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imagetransformer_base_10l_8h_big_uncond_dr03_dan_64_2d():\n  \"\"\"\"\"\"\n  hparams = image_transformer2d_base()\n  hparams.unconditional = True\n  hparams.hidden_size = 512\n  hparams.batch_size = 1\n  hparams.img_len = 64\n  hparams.num_heads = 8\n  hparams.filter_size = 2048\n  hparams.batch_size = 1\n  hparams.max_length = 3075\n  hparams.max_length = 14000\n  hparams.layer_preprocess_sequence = \"none\"\n  hparams.layer_postprocess_sequence = \"dan\"\n  hparams.layer_prepostprocess_dropout = 0.1\n  hparams.dec_attention_type = cia.AttentionType.LOCAL_2D\n  hparams.query_shape = (16, 16)\n  hparams.memory_flange = (8, 8)\n  return hparams", "output": "big 1d model for unconditional generation on imagenet.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def debug_video_writer_factory(output_dir):\n  \"\"\"\"\"\"\n  if FLAGS.disable_ffmpeg:\n    return common_video.IndividualFrameWriter(output_dir)\n  else:\n    output_path = os.path.join(output_dir, \"video.avi\")\n    return common_video.WholeVideoWriter(\n        fps=10, output_path=output_path, file_format=\"avi\"\n    )", "output": "Creates a VideoWriter for debug videos.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_fname_label(self):\r\n        \"\"\"\"\"\"\r\n        filename = to_text_string(self.get_current_filename())\r\n        if len(filename) > 100:\r\n            shorten_filename = u'...' + filename[-100:]\r\n        else:\r\n            shorten_filename = filename\r\n        self.fname_label.setText(shorten_filename)", "output": "Upadte file name label.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def constant(self, name, value):\n        \"\"\"\"\"\"\n        assert is_iterable_typed(name, basestring)\n        assert is_iterable_typed(value, basestring)\n        self.registry.current().add_constant(name[0], value)", "output": "Declare and set a project global constant.\n        Project global constants are normal variables but should\n        not be changed. They are applied to every child Jamfile.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def label_from_lists(self, train_labels:Iterator, valid_labels:Iterator, label_cls:Callable=None, **kwargs)->'LabelList':\n        \"\"\n        label_cls = self.train.get_label_cls(train_labels, label_cls)\n        self.train = self.train._label_list(x=self.train, y=label_cls(train_labels, **kwargs))\n        self.valid = self.valid._label_list(x=self.valid, y=self.train.y.new(valid_labels, **kwargs))\n        self.__class__ = LabelLists\n        self.process()\n        return self", "output": "Use the labels in `train_labels` and `valid_labels` to label the data. `label_cls` will overwrite the default.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_services_mapping():\n    '''\n    \n    '''\n    if _SERVICES:\n        return _SERVICES\n    services_txt = ''\n    try:\n        with salt.utils.files.fopen('/etc/services', 'r') as srv_f:\n            services_txt = salt.utils.stringutils.to_unicode(srv_f.read())\n    except IOError as ioe:\n        log.error('Unable to read from /etc/services:')\n        log.error(ioe)\n        return _SERVICES  # no mapping possible, sorry\n        # will return the default mapping\n    service_rgx = re.compile(r'^([a-zA-Z0-9-]+)\\s+(\\d+)\\/(tcp|udp)(.*)$')\n    for line in services_txt.splitlines():\n        service_rgx_s = service_rgx.search(line)\n        if service_rgx_s and len(service_rgx_s.groups()) == 4:\n            srv_name, port, protocol, _ = service_rgx_s.groups()\n            if srv_name not in _SERVICES:\n                _SERVICES[srv_name] = {\n                    'port': [],\n                    'protocol': []\n                }\n            try:\n                _SERVICES[srv_name]['port'].append(int(port))\n            except ValueError as verr:\n                log.error(verr)\n                log.error('Did not read that properly:')\n                log.error(line)\n                log.error('Please report the above error: %s does not seem a valid port value!', port)\n            _SERVICES[srv_name]['protocol'].append(protocol)\n    return _SERVICES", "output": "Build a map of services based on the IANA assignment list:\n    http://www.iana.org/assignments/port-numbers\n\n    It will load the /etc/services file and will build the mapping on the fly,\n    similar to the Capirca's SERVICES file:\n    https://github.com/google/capirca/blob/master/def/SERVICES.svc\n\n    As this module is be available on Unix systems only,\n    we'll read the services from /etc/services.\n    In the worst case, the user will not be able to specify the\n    services shortcut and they will need to specify the protocol / port combination\n    using the source_port / destination_port & protocol fields.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_rabbitmq_plugin():\n    '''\n    \n    '''\n    global RABBITMQ_PLUGINS\n\n    if RABBITMQ_PLUGINS is None:\n        version = __salt__['pkg.version']('rabbitmq-server').split('-')[0]\n        RABBITMQ_PLUGINS = ('/usr/lib/rabbitmq/lib/rabbitmq_server-{0}'\n                            '/sbin/rabbitmq-plugins').format(version)\n\n    return RABBITMQ_PLUGINS", "output": "Returns the rabbitmq-plugin command path if we're running an OS that\n    doesn't put it in the standard /usr/bin or /usr/local/bin\n    This works by taking the rabbitmq-server version and looking for where it\n    seems to be hidden in /usr/lib.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vqa_recurrent_self_attention_base():\n  \"\"\"\"\"\"\n  hparams = universal_transformer.universal_transformer_base()\n  hparams.batch_size = 1024\n  hparams.use_fixed_batch_size = True\n  hparams.weight_decay = 0.\n  hparams.clip_grad_norm = 0.\n  # use default initializer\n  # hparams.initializer = \"xavier\"\n  hparams.learning_rate_schedule = (\n      \"constant*linear_warmup*rsqrt_normalized_decay\")\n  hparams.learning_rate_warmup_steps = 8000\n  hparams.learning_rate_constant = 7e-4\n  hparams.learning_rate_decay_rate = 0.5\n  hparams.learning_rate_decay_steps = 50000\n  # hparams.dropout = 0.5\n  hparams.summarize_grads = True\n  hparams.summarize_vars = True\n\n  # not used hparams\n  hparams.label_smoothing = 0.1\n  hparams.multiply_embedding_mode = \"sqrt_depth\"\n\n  # add new hparams\n  # use raw image as input\n  hparams.add_hparam(\"image_input_type\", \"feature\")\n  hparams.add_hparam(\"image_model_fn\", \"resnet_v1_152\")\n  hparams.add_hparam(\"resize_side\", 512)\n  hparams.add_hparam(\"height\", 448)\n  hparams.add_hparam(\"width\", 448)\n  hparams.add_hparam(\"distort\", True)\n  hparams.add_hparam(\"train_resnet\", False)\n\n  # question hidden size\n  # hparams.hidden_size = 512\n  # hparams.filter_size = 1024\n  # hparams.num_hidden_layers = 4\n\n  # self attention parts\n  # hparams.norm_type = \"layer\"\n  # hparams.layer_preprocess_sequence = \"n\"\n  # hparams.layer_postprocess_sequence = \"da\"\n  # hparams.layer_prepostprocess_dropout = 0.1\n  # hparams.attention_dropout = 0.1\n  # hparams.relu_dropout = 0.1\n  # hparams.add_hparam(\"pos\", \"timing\")\n  # hparams.add_hparam(\"num_encoder_layers\", 0)\n  # hparams.add_hparam(\"num_decoder_layers\", 0)\n  # hparams.add_hparam(\"num_heads\", 8)\n  # hparams.add_hparam(\"attention_key_channels\", 0)\n  # hparams.add_hparam(\"attention_value_channels\", 0)\n  # hparams.add_hparam(\"self_attention_type\", \"dot_product\")\n\n  # iterative part\n  hparams.transformer_ffn_type = \"fc\"\n\n  return hparams", "output": "VQA attention baseline hparams.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_value(self, name):\n        \"\"\"\"\"\"\n        code = u\"get_ipython().kernel.get_value('%s')\" % name\n        if self._reading:\n            method = self.kernel_client.input\n            code = u'!' + code\n        else:\n            method = self.silent_execute\n\n        # Wait until the kernel returns the value\n        wait_loop = QEventLoop()\n        self.sig_got_reply.connect(wait_loop.quit)\n        method(code)\n        wait_loop.exec_()\n\n        # Remove loop connection and loop\n        self.sig_got_reply.disconnect(wait_loop.quit)\n        wait_loop = None\n\n        # Handle exceptions\n        if self._kernel_value is None:\n            if self._kernel_reply:\n                msg = self._kernel_reply[:]\n                self._kernel_reply = None\n                raise ValueError(msg)\n\n        return self._kernel_value", "output": "Ask kernel for a value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fs_decode(path):\n    \"\"\"\n    \n    \"\"\"\n\n    path = _get_path(path)\n    if path is None:\n        raise TypeError(\"expected a valid path to decode\")\n    if isinstance(path, six.binary_type):\n        path = path.decode(_fs_encoding, _fs_decode_errors)\n    return path", "output": "Decode a filesystem path using the proper filesystem encoding\n\n    :param path: The filesystem path to decode from bytes or string\n    :return: [description]\n    :rtype: [type]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_xys(self, xs, ys, imgsize:int=4, figsize:Optional[Tuple[int,int]]=None, **kwargs):\n        \"\"\n        super().show_xys(ys, xs, imgsize=imgsize, figsize=figsize, **kwargs)", "output": "Shows `ys` (target images) on a figure of `figsize`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _clear_block_deco(self):\n        \"\"\"\"\"\"\n        for deco in self._block_decos:\n            self.editor.decorations.remove(deco)\n        self._block_decos[:] = []", "output": "Clear the folded block decorations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def body(self, features):\n    \"\"\"\n    \"\"\"\n    features[\"targets\"] = features[\"inputs\"]\n    is_training = self.hparams.mode == tf.estimator.ModeKeys.TRAIN\n\n    # Input images.\n    inputs = tf.to_float(features[\"targets_raw\"])\n\n    # Noise vector.\n    z = tf.random_uniform([self.hparams.batch_size,\n                           self.hparams.bottleneck_bits],\n                          minval=-1, maxval=1, name=\"z\")\n\n    # Generator output: fake images.\n    out_shape = common_layers.shape_list(inputs)[1:4]\n    g = self.generator(z, is_training, out_shape)\n\n    losses = self.losses(inputs, g)  # pylint: disable=not-callable\n\n    summary_g_image = tf.reshape(\n        g[0, :], [1] + common_layers.shape_list(inputs)[1:])\n    tf.summary.image(\"generated\", summary_g_image, max_outputs=1)\n\n    if is_training:  # Returns an dummy output and the losses dictionary.\n      return tf.zeros_like(inputs), losses\n    return tf.reshape(g, tf.shape(inputs)), losses", "output": "Body of the model.\n\n    Args:\n      features: a dictionary with the tensors.\n\n    Returns:\n      A pair (predictions, losses) where predictions is the generated image\n      and losses is a dictionary of losses (that get added for the final loss).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset_assets(self, init_cash=None):\n        ''\n        self.sell_available = copy.deepcopy(self.init_hold)\n        self.history = []\n        self.init_cash = init_cash\n        self.cash = [self.init_cash]\n        self.cash_available = self.cash[-1]", "output": "reset_history/cash/", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_field_to_observations_map(generator, query_for_tag=''):\n  \"\"\"\n  \"\"\"\n\n  def increment(stat, event, tag=''):\n    assert stat in TRACKED_FIELDS\n    field_to_obs[stat].append(Observation(step=event.step,\n                                          wall_time=event.wall_time,\n                                          tag=tag)._asdict())\n\n  field_to_obs = dict([(t, []) for t in TRACKED_FIELDS])\n\n  for event in generator:\n    ## Process the event\n    if event.HasField('graph_def') and (not query_for_tag):\n      increment('graph', event)\n    if event.HasField('session_log') and (not query_for_tag):\n      status = event.session_log.status\n      if status == event_pb2.SessionLog.START:\n        increment('sessionlog:start', event)\n      elif status == event_pb2.SessionLog.STOP:\n        increment('sessionlog:stop', event)\n      elif status == event_pb2.SessionLog.CHECKPOINT:\n        increment('sessionlog:checkpoint', event)\n    elif event.HasField('summary'):\n      for value in event.summary.value:\n        if query_for_tag and value.tag != query_for_tag:\n          continue\n\n        for proto_name, display_name in SUMMARY_TYPE_TO_FIELD.items():\n          if value.HasField(proto_name):\n            increment(display_name, event, value.tag)\n  return field_to_obs", "output": "Return a field to `Observations` dict for the event generator.\n\n  Args:\n    generator: A generator over event protos.\n    query_for_tag: A string that if specified, only create observations for\n      events with this tag name.\n\n  Returns:\n    A dict mapping keys in `TRACKED_FIELDS` to an `Observation` list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_bool(text):\n    '''\n    \n    '''\n    downcased_text = six.text_type(text).strip().lower()\n\n    if downcased_text == 'false':\n        return False\n    elif downcased_text == 'true':\n        return True\n    return text", "output": "Convert the string name of a boolean to that boolean value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_color_scheme(self, foreground_color, background_color):\n        \"\"\"\"\"\"\n        if dark_color(foreground_color):\n            self.default_foreground_color = 30\n        else:\n            self.default_foreground_color = 37\n\n        if dark_color(background_color):\n            self.default_background_color = 47\n        else:\n            self.default_background_color = 40", "output": "Set color scheme (foreground and background).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_head(heads, head_grads):\n    \"\"\"\"\"\"\n    if isinstance(heads, NDArray):\n        heads = [heads]\n    if isinstance(head_grads, NDArray):\n        head_grads = [head_grads]\n\n    head_handles = c_handle_array(heads)\n\n    if head_grads is None:\n        hgrad_handles = ctypes.c_void_p(0)\n    else:\n        assert len(heads) == len(head_grads), \\\n            \"heads and head_grads must be lists of the same length\"\n        hgrad_handles = c_array(NDArrayHandle,\n                                [i.handle if i is not None else NDArrayHandle(0)\n                                 for i in head_grads])\n    return head_handles, hgrad_handles", "output": "parse head gradient for backward and grad.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log_similarity_result(logfile, result):\n    \"\"\"\"\"\"\n    assert result['task'] == 'similarity'\n\n    if not logfile:\n        return\n\n    with open(logfile, 'a') as f:\n        f.write('\\t'.join([\n            str(result['global_step']),\n            result['task'],\n            result['dataset_name'],\n            json.dumps(result['dataset_kwargs']),\n            result['similarity_function'],\n            str(result['spearmanr']),\n            str(result['num_dropped']),\n        ]))\n\n        f.write('\\n')", "output": "Log a similarity evaluation result dictionary as TSV to logfile.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def track_job(job_id):\n    \"\"\"\n    \n    \"\"\"\n    cmd = \"bjobs -noheader -o stat {}\".format(job_id)\n    track_job_proc = subprocess.Popen(\n        cmd, stdout=subprocess.PIPE, shell=True)\n    status = track_job_proc.communicate()[0].strip('\\n')\n    return status", "output": "Tracking is done by requesting each job and then searching for whether the job\n    has one of the following states:\n    - \"RUN\",\n    - \"PEND\",\n    - \"SSUSP\",\n    - \"EXIT\"\n    based on the LSF documentation", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _unicode_output(cursor, name, default_type, size, precision, scale):\n    '''\n    \n    '''\n    if default_type in (cx_Oracle.STRING, cx_Oracle.LONG_STRING,\n                        cx_Oracle.FIXED_CHAR, cx_Oracle.CLOB):\n        return cursor.var(six.text_type, size, cursor.arraysize)", "output": "Return strings values as python unicode string\n\n    http://www.oracle.com/technetwork/articles/dsl/tuininga-cx-oracle-084866.html", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def partition(a:Collection, sz:int)->List[Collection]:\n    \"\"\n    return [a[i:i+sz] for i in range(0, len(a), sz)]", "output": "Split iterables `a` in equal parts of size `sz`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _load_data(filepath):\n  \"\"\"\"\"\"\n  with h5py.File(filepath, \"r\") as h5dataset:\n    image_array = np.array(h5dataset[\"images\"])\n    # The 'label' data set in the hdf5 file actually contains the float values\n    # and not the class labels.\n    values_array = np.array(h5dataset[\"labels\"])\n  return image_array, values_array", "output": "Loads the images and latent values into Numpy arrays.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_figure(self, fig, fmt):\n        \"\"\"\"\"\"\n        self.figcanvas.load_figure(fig, fmt)\n        self.scale_image()\n        self.figcanvas.repaint()", "output": "Set a new figure in the figure canvas.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def logx_linear(x, a, b):\n    \"\"\"\n    \"\"\"\n    x = np.log(x)\n    return a*x + b", "output": "logx linear\n\n    Parameters\n    ----------\n    x: int\n    a: float\n    b: float\n\n    Returns\n    -------\n    float\n        a * np.log(x) + b", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_figcanvas(self):\n        \"\"\"\"\"\"\n        self.figcanvas = FigureCanvas(background_color=self.background_color)\n        self.figcanvas.installEventFilter(self)\n        self.setWidget(self.figcanvas)", "output": "Setup the FigureCanvas.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process(self, batch):\n        \"\"\"\n        \"\"\"\n        if \"query\" in batch:\n            return self.process_query_batch(batch)\n        if \"read\" in batch:\n            return self.process_read_batch(batch)\n        raise ValueError(\"Invalid batch\")", "output": "Process a single, partitioned query or read.\n\n        :type batch: mapping\n        :param batch:\n            one of the mappings returned from an earlier call to\n            :meth:`generate_query_batches`.\n\n        :rtype: :class:`~google.cloud.spanner_v1.streamed.StreamedResultSet`\n        :returns: a result set instance which can be used to consume rows.\n        :raises ValueError: if batch does not contain either 'read' or 'query'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cast_expected_to_returned_type(expected, returned):\n        '''\n        \n        '''\n        ret_type = type(returned)\n        new_expected = expected\n        if expected == \"False\" and ret_type == bool:\n            expected = False\n        try:\n            new_expected = ret_type(expected)\n        except ValueError:\n            log.info(\"Unable to cast expected into type of returned\")\n            log.info(\"returned = %s\", returned)\n            log.info(\"type of returned = %s\", type(returned))\n            log.info(\"expected = %s\", expected)\n            log.info(\"type of expected = %s\", type(expected))\n        return new_expected", "output": "Determine the type of variable returned\n        Cast the expected to the type of variable returned", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _expand_user(filepath_or_buffer):\n    \"\"\"\n    \"\"\"\n    if isinstance(filepath_or_buffer, str):\n        return os.path.expanduser(filepath_or_buffer)\n    return filepath_or_buffer", "output": "Return the argument with an initial component of ~ or ~user\n       replaced by that user's home directory.\n\n    Parameters\n    ----------\n    filepath_or_buffer : object to be converted if possible\n\n    Returns\n    -------\n    expanded_filepath_or_buffer : an expanded filepath or the\n                                  input if not expandable", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_job_id(job_id, prefix=None):\n    \"\"\"\n    \"\"\"\n    if job_id is not None:\n        return job_id\n    elif prefix is not None:\n        return str(prefix) + str(uuid.uuid4())\n    else:\n        return str(uuid.uuid4())", "output": "Construct an ID for a new job.\n\n    :type job_id: str or ``NoneType``\n    :param job_id: the user-provided job ID\n\n    :type prefix: str or ``NoneType``\n    :param prefix: (Optional) the user-provided prefix for a job ID\n\n    :rtype: str\n    :returns: A job ID", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_sigint(self, signum, frame):\n        \"\"\"\n        \n        \"\"\"\n        self.finish()\n        self.original_handler(signum, frame)", "output": "Call self.finish() before delegating to the original SIGINT handler.\n\n        This handler should only be in place while the progress display is\n        active.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def path(self):\n        \"\"\"\n        \"\"\"\n        return \"/projects/%s/managedZones/%s/changes/%s\" % (\n            self.zone.project,\n            self.zone.name,\n            self.name,\n        )", "output": "URL path for change set APIs.\n\n        :rtype: str\n        :returns: the path based on project, zone, and change set names.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sanitize_index(data, index, copy=False):\n    \"\"\"\n    \n    \"\"\"\n\n    if index is None:\n        return data\n\n    if len(data) != len(index):\n        raise ValueError('Length of values does not match length of index')\n\n    if isinstance(data, ABCIndexClass) and not copy:\n        pass\n    elif isinstance(data, (ABCPeriodIndex, ABCDatetimeIndex)):\n        data = data._values\n        if copy:\n            data = data.copy()\n\n    elif isinstance(data, np.ndarray):\n\n        # coerce datetimelike types\n        if data.dtype.kind in ['M', 'm']:\n            data = sanitize_array(data, index, copy=copy)\n\n    return data", "output": "Sanitize an index type to return an ndarray of the underlying, pass\n    through a non-Index.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def endpoint_create(service, publicurl=None, internalurl=None, adminurl=None,\n                    region=None, profile=None, url=None, interface=None, **connection_args):\n    '''\n    \n    '''\n    kstone = auth(profile, **connection_args)\n    keystone_service = service_get(name=service, profile=profile,\n                                   **connection_args)\n    if not keystone_service or 'Error' in keystone_service:\n        return {'Error': 'Could not find the specified service'}\n\n    if _OS_IDENTITY_API_VERSION > 2:\n        kstone.endpoints.create(service=keystone_service[service]['id'],\n                                region_id=region,\n                                url=url,\n                                interface=interface)\n    else:\n        kstone.endpoints.create(region=region,\n                                service_id=keystone_service[service]['id'],\n                                publicurl=publicurl,\n                                adminurl=adminurl,\n                                internalurl=internalurl)\n    return endpoint_get(service, region, profile, interface, **connection_args)", "output": "Create an endpoint for an Openstack service\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt 'v2' keystone.endpoint_create nova 'http://public/url' 'http://internal/url' 'http://adminurl/url' region\n\n        salt 'v3' keystone.endpoint_create nova url='http://public/url' interface='public' region='RegionOne'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def order_by(self, field_path, **kwargs):\n        \"\"\"\n        \"\"\"\n        query = query_mod.Query(self)\n        return query.order_by(field_path, **kwargs)", "output": "Create an \"order by\" query with this collection as parent.\n\n        See\n        :meth:`~.firestore_v1beta1.query.Query.order_by` for\n        more information on this method.\n\n        Args:\n            field_path (str): A field path (``.``-delimited list of\n                field names) on which to order the query results.\n            kwargs (Dict[str, Any]): The keyword arguments to pass along\n                to the query. The only supported keyword is ``direction``,\n                see :meth:`~.firestore_v1beta1.query.Query.order_by` for\n                more information.\n\n        Returns:\n            ~.firestore_v1beta1.query.Query: An \"order by\" query.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _fix_channels(op_name, attrs, inputs, proto_obj):\n    \"\"\"\n    \"\"\"\n    weight_name = inputs[1].name\n    if not weight_name in proto_obj._params:\n        raise ValueError(\"Unable to get channels/units attr from onnx graph.\")\n    else:\n        wshape = proto_obj._params[weight_name].shape\n        assert len(wshape) >= 2, \"Weights shape is invalid: {}\".format(wshape)\n\n        if op_name == 'FullyConnected':\n            attrs['num_hidden'] = wshape[0]\n        else:\n            if op_name == 'Convolution':\n                # Weight shape for Conv and FC: (M x C x kH x kW) : M is number of\n                # feature maps/hidden  and C is number of channels\n                attrs['num_filter'] = wshape[0]\n            elif op_name == 'Deconvolution':\n                # Weight shape for DeConv : (C x M x kH x kW) : M is number of\n                # feature maps/filters and C is number of channels\n                attrs['num_filter'] = wshape[1]\n    return attrs", "output": "A workaround for getting 'channels' or 'units' since onnx don't provide\n    these attributes. We check the shape of weights provided to get the number.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_binary(self, fname, silent=True):\n        \"\"\"\n        \"\"\"\n        _check_call(_LIB.XGDMatrixSaveBinary(self.handle,\n                                             c_str(fname),\n                                             int(silent)))", "output": "Save DMatrix to an XGBoost buffer.\n\n        Parameters\n        ----------\n        fname : string\n            Name of the output buffer file.\n        silent : bool (optional; default: True)\n            If set, the output is suppressed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flatten_iter(iterable):\n    \"\"\"\n    \n    \"\"\"\n    for element in iterable:\n        if isinstance(element, Iterable):\n            yield from flatten_iter(element)    \n        else:\n            yield element", "output": "Takes as input multi dimensional iterable and\n    returns generator which produces one dimensional output.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def support_index_min(self, tags=None):\n        # type: (Optional[List[Pep425Tag]]) -> Optional[int]\n        \"\"\"\n        \n        \"\"\"\n        if tags is None:  # for mock\n            tags = pep425tags.get_supported()\n        indexes = [tags.index(c) for c in self.file_tags if c in tags]\n        return min(indexes) if indexes else None", "output": "Return the lowest index that one of the wheel's file_tag combinations\n        achieves in the supported_tags list e.g. if there are 8 supported tags,\n        and one of the file tags is first in the list, then return 0.  Returns\n        None is the wheel is not supported.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_all(logdir, verbose=False):\n  \"\"\"\n  \"\"\"\n  data = prepare_data()\n  rng = random.Random(0)\n\n  base_writer = tf.summary.create_file_writer(logdir)\n  with base_writer.as_default():\n    experiment = hp.Experiment(hparams=HPARAMS, metrics=METRICS)\n    experiment_string = experiment.summary_pb().SerializeToString()\n    tf.summary.experimental.write_raw_pb(experiment_string, step=0)\n    base_writer.flush()\n  base_writer.close()\n\n  sessions_per_group = 2\n  num_sessions = flags.FLAGS.num_session_groups * sessions_per_group\n  session_index = 0  # across all session groups\n  for group_index in xrange(flags.FLAGS.num_session_groups):\n    hparams = {h: sample_uniform(h.domain, rng) for h in HPARAMS}\n    hparams_string = str(hparams)\n    group_id = hashlib.sha256(hparams_string.encode(\"utf-8\")).hexdigest()\n    for repeat_index in xrange(sessions_per_group):\n      session_id = str(session_index)\n      session_index += 1\n      if verbose:\n        print(\n            \"--- Running training session %d/%d\"\n            % (session_index, num_sessions)\n        )\n        print(hparams_string)\n        print(\"--- repeat #: %d\" % (repeat_index + 1))\n      run(\n          data=data,\n          base_logdir=logdir,\n          session_id=session_id,\n          group_id=group_id,\n          hparams=hparams,\n      )", "output": "Perform random search over the hyperparameter space.\n\n  Arguments:\n    logdir: The top-level directory into which to write data. This\n      directory should be empty or nonexistent.\n    verbose: If true, print out each run's name as it begins.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_in_subprocess(func, *args, **kwargs):\n    \"\"\"\"\"\"\n    from multiprocessing import Process\n    thread = Process(target=func, args=args, kwargs=kwargs)\n    thread.daemon = True\n    thread.start()\n    return thread", "output": "Run function in subprocess, return a Process object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_flex_doc(op_name, typ):\n    \"\"\"\n    \n    \"\"\"\n    op_name = op_name.replace('__', '')\n    op_desc = _op_descriptions[op_name]\n\n    if op_desc['reversed']:\n        equiv = 'other ' + op_desc['op'] + ' ' + typ\n    else:\n        equiv = typ + ' ' + op_desc['op'] + ' other'\n\n    if typ == 'series':\n        base_doc = _flex_doc_SERIES\n        doc_no_examples = base_doc.format(\n            desc=op_desc['desc'],\n            op_name=op_name,\n            equiv=equiv,\n            reverse=op_desc['reverse']\n        )\n        if op_desc['series_examples']:\n            doc = doc_no_examples + op_desc['series_examples']\n        else:\n            doc = doc_no_examples\n    elif typ == 'dataframe':\n        base_doc = _flex_doc_FRAME\n        doc = base_doc.format(\n            desc=op_desc['desc'],\n            op_name=op_name,\n            equiv=equiv,\n            reverse=op_desc['reverse']\n        )\n    elif typ == 'panel':\n        base_doc = _flex_doc_PANEL\n        doc = base_doc.format(\n            desc=op_desc['desc'],\n            op_name=op_name,\n            equiv=equiv,\n            reverse=op_desc['reverse']\n        )\n    else:\n        raise AssertionError('Invalid typ argument.')\n    return doc", "output": "Make the appropriate substitutions for the given operation and class-typ\n    into either _flex_doc_SERIES or _flex_doc_FRAME to return the docstring\n    to attach to a generated method.\n\n    Parameters\n    ----------\n    op_name : str {'__add__', '__sub__', ... '__eq__', '__ne__', ...}\n    typ : str {series, 'dataframe']}\n\n    Returns\n    -------\n    doc : str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_image_properties(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.update_image_properties(**kwargs)", "output": "Update properties for an image\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' glanceng.update_image_properties name=image1 hw_scsi_model=virtio-scsi hw_disk_bus=scsi\n        salt '*' glanceng.update_image_properties name=0e4febc2a5ab4f2c8f374b054162506d min_ram=1024", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_batch_end(self, last_target, train, **kwargs):\n        \"\"\n        if train: return\n        bs = last_target.size(0)\n        for name in self.names:\n            self.metrics[name] += bs * self.learn.loss_func.metrics[name].detach().cpu()\n        self.nums += bs", "output": "Update the metrics if not `train`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve_parameter_refs(self, input):\n        \"\"\"\n        \n        \"\"\"\n        return self._traverse(input, self.parameters, self._try_resolve_parameter_refs)", "output": "Resolves references to parameters within the given dictionary recursively. Other intrinsic functions such as\n        !GetAtt, !Sub or !Ref to non-parameters will be left untouched.\n\n        Result is a dictionary where parameter values are inlined. Don't pass this dictionary directly into\n        transform's output because it changes the template structure by inlining parameter values.\n\n        :param input: Any primitive type (dict, array, string etc) whose values might contain intrinsic functions\n        :return: A copy of a dictionary with parameter references replaced by actual value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _error_messages(self, driver_id):\n        \"\"\"\n        \"\"\"\n        assert isinstance(driver_id, ray.DriverID)\n        message = self.redis_client.execute_command(\n            \"RAY.TABLE_LOOKUP\", ray.gcs_utils.TablePrefix.ERROR_INFO, \"\",\n            driver_id.binary())\n\n        # If there are no errors, return early.\n        if message is None:\n            return []\n\n        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(\n            message, 0)\n        error_messages = []\n        for i in range(gcs_entries.EntriesLength()):\n            error_data = ray.gcs_utils.ErrorTableData.GetRootAsErrorTableData(\n                gcs_entries.Entries(i), 0)\n            assert driver_id.binary() == error_data.DriverId()\n            error_message = {\n                \"type\": decode(error_data.Type()),\n                \"message\": decode(error_data.ErrorMessage()),\n                \"timestamp\": error_data.Timestamp(),\n            }\n            error_messages.append(error_message)\n        return error_messages", "output": "Get the error messages for a specific driver.\n\n        Args:\n            driver_id: The ID of the driver to get the errors for.\n\n        Returns:\n            A list of the error messages for this driver.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_records(after=None, before=None):\n    '''\n    \n    '''\n    ret = {}\n    fmdump = _check_fmdump()\n    cmd = '{cmd}{after}{before}'.format(\n        cmd=fmdump,\n        after=' -t {0}'.format(after) if after else '',\n        before=' -T {0}'.format(before) if before else ''\n    )\n    res = __salt__['cmd.run_all'](cmd)\n    retcode = res['retcode']\n    result = {}\n    if retcode != 0:\n        result['Error'] = 'error executing fmdump'\n    else:\n        result = _parse_fmdump(res['stdout'])\n\n    return result", "output": "Display fault management logs\n\n    after : string\n        filter events after time, see man fmdump for format\n\n    before : string\n        filter events before time, see man fmdump for format\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' fmadm.list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extract_ipv4(roster_order, ipv4):\n    '''\n    \n    '''\n    for ip_type in roster_order:\n        for ip_ in ipv4:\n            if ':' in ip_:\n                continue\n            if not salt.utils.validate.net.ipv4_addr(ip_):\n                continue\n            if ip_type == 'local' and ip_.startswith('127.'):\n                return ip_\n            elif ip_type == 'private' and not salt.utils.cloud.is_public_ip(ip_):\n                return ip_\n            elif ip_type == 'public' and salt.utils.cloud.is_public_ip(ip_):\n                return ip_\n    return None", "output": "Extract the preferred IP address from the ipv4 grain", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def which(self, fname):\n        \"\"\"\"\"\"\n        for pe in os.environ['PATH'].split(os.pathsep):\n            checkname = os.path.join(pe, fname)\n            if os.access(checkname, os.X_OK) and not os.path.isdir(checkname):\n                return checkname\n        return None", "output": "Returns the fully qualified path by searching Path of the given\n        name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expand_all(self):\n        \"\"\"\"\"\"\n        block = self.editor.document().firstBlock()\n        while block.isValid():\n            TextBlockHelper.set_collapsed(block, False)\n            block.setVisible(True)\n            block = block.next()\n        self._clear_block_deco()\n        self._refresh_editor_and_scrollbars()\n        self.expand_all_triggered.emit()", "output": "Expands all fold triggers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_daily_10yr_treasury_data():\n    \"\"\"\"\"\"\n    url = \"https://www.federalreserve.gov/datadownload/Output.aspx?rel=H15\" \\\n          \"&series=bcb44e57fb57efbe90002369321bfb3f&lastObs=&from=&to=\" \\\n          \"&filetype=csv&label=include&layout=seriescolumn\"\n    return pd.read_csv(url, header=5, index_col=0, names=['DATE', 'BC_10YEAR'],\n                       parse_dates=True, converters={1: dataconverter},\n                       squeeze=True)", "output": "Download daily 10 year treasury rates from the Federal Reserve and\n    return a pandas.Series.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_worktree_support(failhard=True):\n    '''\n    \n    '''\n    git_version = version(versioninfo=False)\n    if _LooseVersion(git_version) < _LooseVersion('2.5.0'):\n        if failhard:\n            raise CommandExecutionError(\n                'Worktrees are only supported in git 2.5.0 and newer '\n                '(detected git version: ' + git_version + ')'\n            )\n        return False\n    return True", "output": "Ensure that we don't try to operate on worktrees in git < 2.5.0.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def upgrade(self, index=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"POST\", _make_path(index, \"_upgrade\"), params=params\n        )", "output": "Upgrade one or more indices to the latest format through an API.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-upgrade.html>`_\n\n        :arg index: A comma-separated list of index names; use `_all` or empty\n            string to perform the operation on all indices\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default 'open', valid\n            choices are: 'open', 'closed', 'none', 'all'\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)\n        :arg only_ancient_segments: If true, only ancient (an older Lucene major\n            release) segments will be upgraded\n        :arg wait_for_completion: Specify whether the request should block until\n            the all segments are upgraded (default: false)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def readline(self, size=-1):\n        ''' '''\n\n        if size == 0:\n            return self.string_type()\n        # delimiter default is EOF\n        index = self.expect([self.crlf, self.delimiter])\n        if index == 0:\n            return self.before + self.crlf\n        else:\n            return self.before", "output": "This reads and returns one entire line. The newline at the end of\n        line is returned as part of the string, unless the file ends without a\n        newline. An empty string is returned if EOF is encountered immediately.\n        This looks for a newline as a CR/LF pair (\\\\r\\\\n) even on UNIX because\n        this is what the pseudotty device returns. So contrary to what you may\n        expect you will receive newlines as \\\\r\\\\n.\n\n        If the size argument is 0 then an empty string is returned. In all\n        other cases the size argument is ignored, which is not standard\n        behavior for a file-like object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _load_dataset_clipping(self, dataset_dir, epsilon):\n    \"\"\"\n    \"\"\"\n    self.dataset_max_clip = {}\n    self.dataset_min_clip = {}\n    self._dataset_image_count = 0\n    for fname in os.listdir(dataset_dir):\n      if not fname.endswith('.png'):\n        continue\n      image_id = fname[:-4]\n      image = np.array(\n          Image.open(os.path.join(dataset_dir, fname)).convert('RGB'))\n      image = image.astype('int32')\n      self._dataset_image_count += 1\n      self.dataset_max_clip[image_id] = np.clip(image + epsilon,\n                                                0,\n                                                255).astype('uint8')\n      self.dataset_min_clip[image_id] = np.clip(image - epsilon,\n                                                0,\n                                                255).astype('uint8')", "output": "Helper method which loads dataset and determines clipping range.\n\n    Args:\n      dataset_dir: location of the dataset.\n      epsilon: maximum allowed size of adversarial perturbation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_wikiheadlines(path):\n  \"\"\"\"\"\"\n  lang_match = re.match(r\".*\\.([a-z][a-z])-([a-z][a-z])$\", path)\n  assert lang_match is not None, \"Invalid Wikiheadlines filename: %s\" % path\n  l1, l2 = lang_match.groups()\n  with tf.io.gfile.GFile(path) as f:\n    for line in f:\n      s1, s2 = line.split(\"|||\")\n      yield {\n          l1: s1.strip(),\n          l2: s2.strip()\n      }", "output": "Generates examples from Wikiheadlines dataset file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def in_order_traverse(self):\n        \"\"\"\n        \n        \"\"\"\n        result = []\n\n        if not self.node:\n            return result\n\n        result.extend(self.node.left.in_order_traverse())\n        result.append(self.node.key)\n        result.extend(self.node.right.in_order_traverse())\n        return result", "output": "In-order traversal of the tree", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transpose(self):\n        \"\"\"\n        \n        \"\"\"\n        java_transposed_matrix = self._java_matrix_wrapper.call(\"transpose\")\n        return BlockMatrix(java_transposed_matrix, self.colsPerBlock, self.rowsPerBlock)", "output": "Transpose this BlockMatrix. Returns a new BlockMatrix\n        instance sharing the same underlying data. Is a lazy operation.\n\n        >>> blocks = sc.parallelize([((0, 0), Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])),\n        ...                          ((1, 0), Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12]))])\n        >>> mat = BlockMatrix(blocks, 3, 2)\n\n        >>> mat_transposed = mat.transpose()\n        >>> mat_transposed.toLocalMatrix()\n        DenseMatrix(2, 6, [1.0, 4.0, 2.0, 5.0, 3.0, 6.0, 7.0, 10.0, 8.0, 11.0, 9.0, 12.0], 0)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write(self, x):\n        \"\"\"\"\"\"\n        if self._new_lines:\n            if not self._first_write:\n                self.stream.write('\\n' * self._new_lines)\n                self.code_lineno += self._new_lines\n                if self._write_debug_info is not None:\n                    self.debug_info.append((self._write_debug_info,\n                                            self.code_lineno))\n                    self._write_debug_info = None\n            self._first_write = False\n            self.stream.write('    ' * self._indentation)\n            self._new_lines = 0\n        self.stream.write(x)", "output": "Write a string into the output stream.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_close_matches(word, possibilities, n=None, cutoff=0.6):\n    \"\"\"\"\"\"\n    if n is None:\n        n = settings.num_close_matches\n    return difflib_get_close_matches(word, possibilities, n, cutoff)", "output": "Overrides `difflib.get_close_match` to controle argument `n`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def min_heapify(arr, start, simulation, iteration):\n    \"\"\" \n    \"\"\"\n    # Offset last_parent by the start (last_parent calculated as if start index was 0)\n    # All array accesses need to be offset by start\n    end = len(arr) - 1\n    last_parent = (end - start - 1) // 2\n\n    # Iterate from last parent to first\n    for parent in range(last_parent, -1, -1):\n        current_parent = parent\n\n        # Iterate from current_parent to last_parent\n        while current_parent <= last_parent:\n            # Find lesser child of current_parent\n            child = 2 * current_parent + 1\n            if child + 1 <= end - start and arr[child + start] > arr[\n                child + 1 + start]:\n                child = child + 1\n            \n            # Swap if child is less than parent\n            if arr[child + start] < arr[current_parent + start]:\n                arr[current_parent + start], arr[child + start] = \\\n                    arr[child + start], arr[current_parent + start]\n                current_parent = child\n                if simulation:\n                    iteration = iteration + 1\n                    print(\"iteration\",iteration,\":\",*arr)\n            # If no swap occured, no need to keep iterating\n            else:\n                break\n    return iteration", "output": "Min heapify helper for min_heap_sort", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def relu(inplace:bool=False, leaky:float=None):\n    \"\"\n    return nn.LeakyReLU(inplace=inplace, negative_slope=leaky) if leaky is not None else nn.ReLU(inplace=inplace)", "output": "Return a relu activation, maybe `leaky` and `inplace`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def move_to(self, channel):\n        \"\"\"\n        \"\"\"\n        guild_id, _ = self.channel._get_voice_state_pair()\n        await self.main_ws.voice_state(guild_id, channel.id)", "output": "|coro|\n\n        Moves you to a different voice channel.\n\n        Parameters\n        -----------\n        channel: :class:`abc.Snowflake`\n            The channel to move to. Must be a voice channel.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mergeCombiners(self, iterator, limit=None):\n        \"\"\"  \"\"\"\n        if limit is None:\n            limit = self.memory_limit\n        # speedup attribute lookup\n        comb, hfun, objsize = self.agg.mergeCombiners, self._partition, self._object_size\n        c, data, pdata, batch = 0, self.data, self.pdata, self.batch\n        for k, v in iterator:\n            d = pdata[hfun(k)] if pdata else data\n            d[k] = comb(d[k], v) if k in d else v\n            if not limit:\n                continue\n\n            c += objsize(v)\n            if c > batch:\n                if get_used_memory() > limit:\n                    self._spill()\n                    limit = self._next_limit()\n                    batch /= 2\n                    c = 0\n                else:\n                    batch *= 1.5\n\n        if limit and get_used_memory() >= limit:\n            self._spill()", "output": "Merge (K,V) pair by mergeCombiner", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_minute_close(self, dt, data_portal):\n        \"\"\"\n        \n        \"\"\"\n        self.sync_last_sale_prices(dt, data_portal)\n\n        packet = {\n            'period_start': self._first_session,\n            'period_end': self._last_session,\n            'capital_base': self._capital_base,\n            'minute_perf': {\n                'period_open': self._market_open,\n                'period_close': dt,\n            },\n            'cumulative_perf': {\n                'period_open': self._first_session,\n                'period_close': self._last_session,\n            },\n            'progress': self._progress(self),\n            'cumulative_risk_metrics': {},\n        }\n        ledger = self._ledger\n        ledger.end_of_bar(self._session_count)\n        self.end_of_bar(\n            packet,\n            ledger,\n            dt,\n            self._session_count,\n            data_portal,\n        )\n        return packet", "output": "Handles the close of the given minute in minute emission.\n\n        Parameters\n        ----------\n        dt : Timestamp\n            The minute that is ending\n\n        Returns\n        -------\n        A minute perf packet.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reader(self, stream, context):\n        \"\"\"\n        \n        \"\"\"\n        progress = self.progress\n        verbose = self.verbose\n        while True:\n            s = stream.readline()\n            if not s:\n                break\n            if progress is not None:\n                progress(s, context)\n            else:\n                if not verbose:\n                    sys.stderr.write('.')\n                else:\n                    sys.stderr.write(s.decode('utf-8'))\n                sys.stderr.flush()\n        stream.close()", "output": "Read lines from a subprocess' output stream and either pass to a progress\n        callable (if specified) or write progress information to sys.stderr.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_module_is_image_embedding(module_spec):\n  \"\"\"\n  \"\"\"\n  issues = []\n\n  # Find issues with \"default\" signature inputs. The common signatures for\n  # image models prescribe a specific name; we trust it if we find it\n  # and if we can do the necessary inference of input shapes from it.\n  input_info_dict = module_spec.get_input_info_dict()\n  if (list(input_info_dict.keys()) != [\"images\"] or\n      input_info_dict[\"images\"].dtype != tf.float32):\n    issues.append(\"Module 'default' signature must require a single input, \"\n                  \"which must have type float32 and name 'images'.\")\n  else:\n    try:\n      image_util.get_expected_image_size(module_spec)\n    except ValueError as e:\n      issues.append(\"Module does not support hub.get_expected_image_size(); \"\n                    \"original error was:\\n\" + str(e))  # Raised again below.\n\n  # Find issues with \"default\" signature outputs. We test that the dtype and\n  # shape is appropriate for use in input_layer().\n  output_info_dict = module_spec.get_output_info_dict()\n  if \"default\" not in output_info_dict:\n    issues.append(\"Module 'default' signature must have a 'default' output.\")\n  else:\n    output_type = output_info_dict[\"default\"].dtype\n    output_shape = output_info_dict[\"default\"].get_shape()\n    if not (output_type == tf.float32 and output_shape.ndims == 2 and\n            output_shape.dims[1].value):\n      issues.append(\"Module 'default' signature must have a 'default' output \"\n                    \"of tf.Tensor(shape=(_,K), dtype=float32).\")\n\n  if issues:\n    raise ValueError(\"Module is not usable as image embedding: %r\" % issues)", "output": "Raises ValueError if `module_spec` is not usable as image embedding.\n\n  Args:\n    module_spec: A `_ModuleSpec` to test.\n\n  Raises:\n    ValueError: if `module_spec` default signature is not compatible with\n        mappingan \"images\" input to a Tensor(float32, shape=(_,K)).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start():\n    '''\n    \n    '''\n    from . import app\n    root, apiopts, conf = app.get_app(__opts__)\n\n    if not apiopts.get('disable_ssl', False):\n        if 'ssl_crt' not in apiopts or 'ssl_key' not in apiopts:\n            logger.error(\"Not starting '%s'. Options 'ssl_crt' and \"\n                    \"'ssl_key' are required if SSL is not disabled.\",\n                    __name__)\n\n            return None\n\n        verify_certs(apiopts['ssl_crt'], apiopts['ssl_key'])\n\n        cherrypy.server.ssl_module = 'builtin'\n        cherrypy.server.ssl_certificate = apiopts['ssl_crt']\n        cherrypy.server.ssl_private_key = apiopts['ssl_key']\n        if 'ssl_chain' in apiopts.keys():\n            cherrypy.server.ssl_certificate_chain = apiopts['ssl_chain']\n\n    cherrypy.quickstart(root, apiopts.get('root_prefix', '/'), conf)", "output": "Start the server loop", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hypot(left, right):\n    \"\"\"\n    \"\"\"\n    if isinstance(left, Symbol) and isinstance(right, Symbol):\n        return _internal._Hypot(left, right)\n    if isinstance(left, Symbol) and isinstance(right, Number):\n        return _internal._HypotScalar(left, scalar=right)\n    if isinstance(left, Number) and isinstance(right, Symbol):\n        return _internal._HypotScalar(right, scalar=left)\n    if isinstance(left, Number) and isinstance(right, Number):\n        return _numpy.hypot(left, right)\n    else:\n        raise TypeError('types (%s, %s) not supported' % (str(type(left)), str(type(right))))", "output": "Given the \"legs\" of a right triangle, returns its hypotenuse.\n\n    Equivalent to :math:`\\\\sqrt(left^2 + right^2)`, element-wise.\n    Both inputs can be Symbol or scalar number. Broadcasting is not supported.\n\n    Parameters\n    ---------\n    left : Symbol or scalar\n        First leg of the triangle(s).\n    right : Symbol or scalar\n        Second leg of the triangle(s).\n\n    Returns\n    -------\n    Symbol or scalar\n        The hypotenuse of the triangle(s)\n\n    Examples\n    --------\n    >>> mx.sym.hypot(3, 4)\n    5.0\n    >>> x = mx.sym.Variable('x')\n    >>> y = mx.sym.Variable('y')\n    >>> z = mx.sym.hypot(x, 4)\n    >>> z.eval(x=mx.nd.array([3,5,2]))[0].asnumpy()\n    array([ 5.,  6.40312433,  4.47213602], dtype=float32)\n    >>> z = mx.sym.hypot(x, y)\n    >>> z.eval(x=mx.nd.array([3,4]), y=mx.nd.array([10,2]))[0].asnumpy()\n    array([ 10.44030666,   4.47213602], dtype=float32)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_ndarray_function(handle, name, func_name):\n    \"\"\"\"\"\"\n    code, doc_str = _generate_ndarray_function_code(handle, name, func_name)\n\n    local = {}\n    exec(code, None, local)  # pylint: disable=exec-used\n    ndarray_function = local[func_name]\n    ndarray_function.__name__ = func_name\n    ndarray_function.__doc__ = doc_str\n    ndarray_function.__module__ = 'mxnet.ndarray'\n    return ndarray_function", "output": "Create a NDArray function from the FunctionHandle.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _bfs(root_node, process_node):\n    \"\"\"\n    \n    \"\"\"\n\n    from collections import deque\n\n    seen_nodes = set()\n    next_nodes = deque()\n\n    seen_nodes.add(root_node)\n    next_nodes.append(root_node)\n\n    while next_nodes:\n        current_node = next_nodes.popleft()\n\n        # process current node\n        process_node(current_node)\n\n        for child_node in current_node.children:\n            if child_node not in seen_nodes:\n                seen_nodes.add(child_node)\n                next_nodes.append(child_node)", "output": "Implementation of Breadth-first search (BFS) on caffe network DAG\n    :param root_node: root node of caffe network DAG\n    :param process_node: function to run on each node", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_vm(name, data, quiet=False):\n    '''\n    \n    '''\n    for hv_ in data:\n        # Check if data is a dict, and not '\"virt.full_info\" is not available.'\n        if not isinstance(data[hv_], dict):\n            continue\n        if name in data[hv_].get('vm_info', {}):\n            ret = {hv_: {name: data[hv_]['vm_info'][name]}}\n            if not quiet:\n                __jid_event__.fire_event({'data': ret, 'outputter': 'nested'}, 'progress')\n            return ret\n    return {}", "output": "Scan the query data for the named VM", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_kexgss_complete(self, m):\n        \"\"\"\n        \n        \"\"\"\n        if self.transport.host_key is None:\n            self.transport.host_key = NullHostKey()\n        self.f = m.get_mpint()\n        mic_token = m.get_string()\n        # This must be TRUE, if there is a GSS-API token in this message.\n        bool = m.get_boolean()\n        srv_token = None\n        if bool:\n            srv_token = m.get_string()\n        if (self.f < 1) or (self.f > self.p - 1):\n            raise SSHException('Server kex \"f\" is out of range')\n        K = pow(self.f, self.x, self.p)\n        # okay, build up the hash H of\n        # (V_C || V_S || I_C || I_S || K_S || min || n || max || p || g || e || f || K)  # noqa\n        hm = Message()\n        hm.add(\n            self.transport.local_version,\n            self.transport.remote_version,\n            self.transport.local_kex_init,\n            self.transport.remote_kex_init,\n            self.transport.host_key.__str__(),\n        )\n        if not self.old_style:\n            hm.add_int(self.min_bits)\n        hm.add_int(self.preferred_bits)\n        if not self.old_style:\n            hm.add_int(self.max_bits)\n        hm.add_mpint(self.p)\n        hm.add_mpint(self.g)\n        hm.add_mpint(self.e)\n        hm.add_mpint(self.f)\n        hm.add_mpint(K)\n        H = sha1(hm.asbytes()).digest()\n        self.transport._set_K_H(K, H)\n        if srv_token is not None:\n            self.kexgss.ssh_init_sec_context(\n                target=self.gss_host, recv_token=srv_token\n            )\n            self.kexgss.ssh_check_mic(mic_token, H)\n        else:\n            self.kexgss.ssh_check_mic(mic_token, H)\n        self.transport.gss_kex_used = True\n        self.transport._activate_outbound()", "output": "Parse the SSH2_MSG_KEXGSS_COMPLETE message (client mode).\n\n        :param `Message` m: The content of the SSH2_MSG_KEXGSS_COMPLETE message", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def focusOutEvent(self, e):\r\n        \"\"\"\"\"\"\r\n        self.source_model.update_active_row()\r\n        super(ShortcutsTable, self).focusOutEvent(e)", "output": "Qt Override.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _internal_set(self, obj, value, hint=None, setter=None):\n        ''' \n\n        '''\n        value = self.property.prepare_value(obj, self.name, value)\n\n        old = self.__get__(obj, obj.__class__)\n        self._real_set(obj, old, value, hint=hint, setter=setter)", "output": "Internal implementation to set property values, that is used\n        by __set__, set_from_json, etc.\n\n        Delegate to the |Property| instance to prepare the value appropriately,\n        then `set.\n\n        Args:\n            obj (HasProps)\n                The object the property is being set on.\n\n            old (obj) :\n                The previous value of the property to compare\n\n            hint (event hint or None, optional)\n                An optional update event hint, e.g. ``ColumnStreamedEvent``\n                (default: None)\n\n                Update event hints are usually used at times when better\n                update performance can be obtained by special-casing in\n                some way (e.g. streaming or patching column data sources)\n\n            setter (ClientSession or ServerSession or None, optional) :\n                This is used to prevent \"boomerang\" updates to Bokeh apps.\n                (default: None)\n\n                In the context of a Bokeh server application, incoming updates\n                to properties will be annotated with the session that is\n                doing the updating. This value is propagated through any\n                subsequent change notifications that the update triggers.\n                The session can compare the event setter to itself, and\n                suppress any updates that originate from itself.\n\n        Returns:\n            None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_accept_keywords(approved, flag):\n    ''''''\n    if flag in approved:\n        return False\n    elif (flag.startswith('~') and flag[1:] in approved) \\\n            or ('~'+flag in approved):\n        return False\n    else:\n        return True", "output": "check compatibility of accept_keywords", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transpose(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    new_attrs = translation_utils._fix_attribute_names(attrs,\n                                                       {'perm' : 'axes'})\n    return 'transpose', new_attrs, inputs", "output": "Transpose the input array.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_pin(name_str):\n    \"\"\"\"\"\"\n    if len(name_str) < 1:\n        raise ValueError(\"Expecting pin name to be at least 4 charcters.\")\n    if name_str[0] != 'P':\n        raise ValueError(\"Expecting pin name to start with P\")\n    pin_str = name_str[1:].split('/')[0]\n    if not pin_str.isdigit():\n        raise ValueError(\"Expecting numeric pin number.\")\n    return int(pin_str)", "output": "Parses a string and returns a pin-num.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fail_eof(self, end_tokens=None, lineno=None):\n        \"\"\"\"\"\"\n        stack = list(self._end_token_stack)\n        if end_tokens is not None:\n            stack.append(end_tokens)\n        return self._fail_ut_eof(None, stack, lineno)", "output": "Like fail_unknown_tag but for end of template situations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_builder_configs():\n  \"\"\"\n  \"\"\"\n  text_encoder_configs = [\n      None,\n      tfds.features.text.TextEncoderConfig(\n          name=\"bytes\", encoder=tfds.features.text.ByteTextEncoder()),\n      tfds.features.text.TextEncoderConfig(\n          name=\"subwords8k\",\n          encoder_cls=tfds.features.text.SubwordTextEncoder,\n          vocab_size=2**13),\n      tfds.features.text.TextEncoderConfig(\n          name=\"subwords32k\",\n          encoder_cls=tfds.features.text.SubwordTextEncoder,\n          vocab_size=2**15),\n  ]\n  version = \"0.1.0\"\n  configs = []\n  for text_encoder_config in text_encoder_configs:\n    for data in _DATA_OPTIONS:\n      config = LibrispeechConfig(\n          version=version, text_encoder_config=text_encoder_config, data=data)\n      configs.append(config)\n  return configs", "output": "Make built-in Librispeech BuilderConfigs.\n\n  Uses 4 text encodings (plain text, bytes, subwords with 8k vocab, subwords\n  with 32k vocab) crossed with the data subsets (clean100, clean360, all).\n\n  Returns:\n    `list<tfds.audio.LibrispeechConfig>`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fcontext_policy_applied(name, recursive=False):\n    '''\n    \n    '''\n    ret = {'name': name, 'result': False, 'changes': {}, 'comment': ''}\n\n    changes_text = __salt__['selinux.fcontext_policy_is_applied'](name, recursive)\n    if changes_text == '':\n        ret.update({'result': True,\n                    'comment': 'SElinux policies are already applied for filespec \"{0}\"'.format(name)})\n        return ret\n    if __opts__['test']:\n        ret.update({'result': None})\n    else:\n        apply_ret = __salt__['selinux.fcontext_apply_policy'](name, recursive)\n        if apply_ret['retcode'] != 0:\n            ret.update({'comment': apply_ret})\n        else:\n            ret.update({'result': True})\n            ret.update({'changes': apply_ret.get('changes')})\n    return ret", "output": ".. versionadded:: 2017.7.0\n\n    Checks and makes sure the SELinux policies for a given filespec are\n    applied.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_vertex(self, key):\n    \"\"\"\n    \"\"\"\n    if key in self.vertex_map:\n      return self.vertex_map[key]\n    vertex = self.new_vertex()\n    self.vertex_map[key] = vertex\n    return vertex", "output": "Returns or Creates a Vertex mapped by key.\n\n    Args:\n      key: A string reference for a vertex.  May refer to a new Vertex in which\n      case it will be created.\n\n    Returns:\n      A the Vertex mapped to by key.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def inference(metric):\n    \"\"\"\"\"\"\n\n    logging.info('Now we are doing BERT classification inference on %s!', ctx)\n    model = BERTClassifier(bert, dropout=0.1, num_classes=len(task.get_labels()))\n    model.hybridize(static_alloc=True)\n    model.load_parameters(model_parameters, ctx=ctx)\n\n    metric.reset()\n    step_loss = 0\n    tic = time.time()\n    for batch_id, seqs in enumerate(dev_data):\n        input_ids, valid_length, type_ids, label = seqs\n        out = model(input_ids.as_in_context(ctx),\n                    type_ids.as_in_context(ctx),\n                    valid_length.astype('float32').as_in_context(ctx))\n\n        ls = loss_function(out, label.as_in_context(ctx)).mean()\n\n        step_loss += ls.asscalar()\n        metric.update([label], [out])\n\n        if (batch_id + 1) % (args.log_interval) == 0:\n            log_inference(batch_id, len(dev_data), metric, step_loss, args.log_interval)\n            step_loss = 0\n\n    mx.nd.waitall()\n    toc = time.time()\n    total_num = dev_batch_size * len(dev_data)\n    logging.info('Time cost=%.2fs, throughput=%.2fsamples/s', toc - tic, \\\n                 total_num / (toc - tic))", "output": "Inference function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_hyper():\n    '''\n    \n    '''\n    try:\n        if __grains__['virtual_subtype'] != 'Xen Dom0':\n            return False\n    except KeyError:\n        # virtual_subtype isn't set everywhere.\n        return False\n    try:\n        with salt.utils.files.fopen('/proc/modules') as fp_:\n            if 'xen_' not in salt.utils.stringutils.to_unicode(fp_.read()):\n                return False\n    except (OSError, IOError):\n        return False\n    # there must be a smarter way...\n    return 'xenstore' in __salt__['cmd.run'](__grains__['ps'])", "output": "Returns a bool whether or not this node is a hypervisor of any kind\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' virt.is_hyper", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init(opts):\n    '''\n    \n    '''\n    proxy_dict = opts.get('proxy', {})\n    conn_args = proxy_dict.copy()\n    conn_args.pop('proxytype', None)\n    opts['multiprocessing'] = conn_args.get('multiprocessing', True)\n    # This is not a SSH-based proxy, so it should be safe to enable\n    # multiprocessing.\n    try:\n        conn = pyeapi.client.connect(**conn_args)\n        node = pyeapi.client.Node(conn, enablepwd=conn_args.get('enablepwd'))\n        pyeapi_device['connection'] = node\n        pyeapi_device['initialized'] = True\n        pyeapi_device['up'] = True\n    except pyeapi.eapilib.ConnectionError as cerr:\n        log.error('Unable to connect to %s', conn_args['host'], exc_info=True)\n        return False\n    return True", "output": "Open the connection to the Arista switch over the eAPI.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_python_tag(wheelname, new_tag):\n    # type: (str, str) -> str\n    \"\"\"\n    \"\"\"\n    parts = wheelname.split('-')\n    parts[-3] = new_tag\n    return '-'.join(parts)", "output": "Replace the Python tag in a wheel file name with a new value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_batch(value, linecount, fill_with=None):\n    \"\"\"\n    \n    \"\"\"\n    tmp = []\n    for item in value:\n        if len(tmp) == linecount:\n            yield tmp\n            tmp = []\n        tmp.append(item)\n    if tmp:\n        if fill_with is not None and len(tmp) < linecount:\n            tmp += [fill_with] * (linecount - len(tmp))\n        yield tmp", "output": "A filter that batches items. It works pretty much like `slice`\n    just the other way round. It returns a list of lists with the\n    given number of items. If you provide a second parameter this\n    is used to fill up missing items. See this example:\n\n    .. sourcecode:: html+jinja\n\n        <table>\n        {%- for row in items|batch(3, '&nbsp;') %}\n          <tr>\n          {%- for column in row %}\n            <td>{{ column }}</td>\n          {%- endfor %}\n          </tr>\n        {%- endfor %}\n        </table>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _pprint_dict(seq, _nest_lvl=0, max_seq_items=None, **kwds):\n    \"\"\"\n    \n    \"\"\"\n    fmt = \"{{{things}}}\"\n    pairs = []\n\n    pfmt = \"{key}: {val}\"\n\n    if max_seq_items is False:\n        nitems = len(seq)\n    else:\n        nitems = max_seq_items or get_option(\"max_seq_items\") or len(seq)\n\n    for k, v in list(seq.items())[:nitems]:\n        pairs.append(\n            pfmt.format(\n                key=pprint_thing(k, _nest_lvl + 1,\n                                 max_seq_items=max_seq_items, **kwds),\n                val=pprint_thing(v, _nest_lvl + 1,\n                                 max_seq_items=max_seq_items, **kwds)))\n\n    if nitems < len(seq):\n        return fmt.format(things=\", \".join(pairs) + \", ...\")\n    else:\n        return fmt.format(things=\", \".join(pairs))", "output": "internal. pprinter for iterables. you should probably use pprint_thing()\n    rather then calling this directly.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dispatch_to_index_op(op, left, right, index_class):\n    \"\"\"\n    \n    \"\"\"\n    left_idx = index_class(left)\n\n    # avoid accidentally allowing integer add/sub.  For datetime64[tz] dtypes,\n    # left_idx may inherit a freq from a cached DatetimeIndex.\n    # See discussion in GH#19147.\n    if getattr(left_idx, 'freq', None) is not None:\n        left_idx = left_idx._shallow_copy(freq=None)\n    try:\n        result = op(left_idx, right)\n    except NullFrequencyError:\n        # DatetimeIndex and TimedeltaIndex with freq == None raise ValueError\n        # on add/sub of integers (or int-like).  We re-raise as a TypeError.\n        raise TypeError('incompatible type for a datetime/timedelta '\n                        'operation [{name}]'.format(name=op.__name__))\n    return result", "output": "Wrap Series left in the given index_class to delegate the operation op\n    to the index implementation.  DatetimeIndex and TimedeltaIndex perform\n    type checking, timezone handling, overflow checks, etc.\n\n    Parameters\n    ----------\n    op : binary operator (operator.add, operator.sub, ...)\n    left : Series\n    right : object\n    index_class : DatetimeIndex or TimedeltaIndex\n\n    Returns\n    -------\n    result : object, usually DatetimeIndex, TimedeltaIndex, or Series", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_edge(graph, node, text, yes_color='#0000FF', no_color='#FF0000'):\n    \"\"\"\"\"\"\n    try:\n        match = _EDGEPAT.match(text)\n        if match is not None:\n            yes, no, missing = match.groups()\n            if yes == missing:\n                graph.edge(node, yes, label='yes, missing', color=yes_color)\n                graph.edge(node, no, label='no', color=no_color)\n            else:\n                graph.edge(node, yes, label='yes', color=yes_color)\n                graph.edge(node, no, label='no, missing', color=no_color)\n            return\n    except ValueError:\n        pass\n    match = _EDGEPAT2.match(text)\n    if match is not None:\n        yes, no = match.groups()\n        graph.edge(node, yes, label='yes', color=yes_color)\n        graph.edge(node, no, label='no', color=no_color)\n        return\n    raise ValueError('Unable to parse edge: {0}'.format(text))", "output": "parse dumped edge", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def renamed_class(cls):\n    \"\"\"\"\"\"\n\n    class DeprecationWrapper(cls):\n        def __init__(self, config=None, env=None, logger_creator=None):\n            old_name = cls.__name__.replace(\"Trainer\", \"Agent\")\n            new_name = cls.__name__\n            logger.warn(\"DeprecationWarning: {} has been renamed to {}. \".\n                        format(old_name, new_name) +\n                        \"This will raise an error in the future.\")\n            cls.__init__(self, config, env, logger_creator)\n\n    DeprecationWrapper.__name__ = cls.__name__\n\n    return DeprecationWrapper", "output": "Helper class for renaming Agent => Trainer with a warning.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_device(device):\n    '''\n    \n    '''\n    if os.path.exists(device):\n        dev = os.stat(device).st_mode\n\n        if stat.S_ISBLK(dev):\n            return\n\n    raise CommandExecutionError(\n        'Invalid device passed to partition module.'\n    )", "output": "Ensure the device name supplied is valid in a manner similar to the\n    `exists` function, but raise errors on invalid input rather than return\n    False.\n\n    This function only validates a block device, it does not check if the block\n    device is a drive or a partition or a filesystem, etc.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _scrub_links(links, name):\n    '''\n    \n    '''\n    if isinstance(links, list):\n        ret = []\n        for l in links:\n            ret.append(l.replace('/{0}/'.format(name), '/', 1))\n    else:\n        ret = links\n\n    return ret", "output": "Remove container name from HostConfig:Links values to enable comparing\n    container configurations correctly.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def name(self, value):\n        \"\"\"\n        \"\"\"\n        if not isinstance(value, six.string_types):\n            raise ValueError(\"Pass a string\")\n        self._properties[\"id\"] = value", "output": "Update name of the change set.\n\n        :type value: str\n        :param value: New name for the changeset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_each_method_maximun_cpu_mem(self):\n        \"\"\"\"\"\"\n        # \u672c\u51fd\u6570\u7528\u4e8e\u4e30\u5bccself.method_exec_info\u7684\u4fe1\u606f:\u5b58\u5165cpu\u3001mem\u6700\u503c\u70b9\n        self.method_exec_info = deepcopy(self.data.get(\"method_exec_info\", []))\n        method_exec_info = deepcopy(self.method_exec_info)  # \u7528\u6765\u8f85\u52a9\u5faa\u73af\n        method_index, cpu_max, cpu_max_time, mem_max, mem_max_time = 0, 0, 0, 0, 0  # \u4e34\u65f6\u53d8\u91cf\n        self.max_mem = 0\n        for index, timestamp in enumerate(self.timestamp_list):\n            # method_exec_info\u662f\u6309\u987a\u5e8f\u7684,\u9010\u4e2a\u904d\u5386\u627e\u51fa\u6bcf\u4e2amethod_exec_info\u4e2d\u7684cpu\u548cmem\u7684\u6700\u503c\u70b9\u548ctimestamp:\n            start, end = method_exec_info[0][\"start_time\"], method_exec_info[0][\"end_time\"]\n            if timestamp < start:\n                # \u65b9\u6cd5\u6b63\u5f0fstart\u4e4b\u524d\u7684\u6570\u636e\uff0c\u4e0d\u80fd\u53c2\u4e0e\u65b9\u6cd5\u5185\u7684cpu\u3001mem\u8ba1\u7b97\uff0c\u76f4\u63a5\u5ffd\u7565\u6b64\u6761\u6570\u636e\n                continue\n            elif timestamp <= end:\n                # \u65b9\u6cd5\u6267\u884c\u671f\u95f4\u7684\u6570\u636e,\u7eb3\u5165\u6700\u503c\u6bd4\u8f83:\n                if self.cpu_axis[index] > cpu_max:\n                    cpu_max, cpu_max_time = self.cpu_axis[index], timestamp\n                if self.mem_axis[index] > mem_max:\n                    mem_max, mem_max_time = self.mem_axis[index], timestamp\n                continue\n            else:\n                # \u672c\u6b21\u65b9\u6cd5\u7b5b\u9009\u5b8c\u6bd5\uff0c\u4fdd\u5b58\u672c\u65b9\u6cd5\u7684\u6700\u503ccpu\u548cmem\n                if cpu_max_time != 0 and mem_max_time != 0:\n                    self.method_exec_info[method_index].update({\"cpu_max\": cpu_max, \"mem_max\": mem_max, \"cpu_max_time\": cpu_max_time, \"mem_max_time\": mem_max_time})\n                # \u4fdd\u5b58\u6700\u5927\u7684\u5185\u5b58\uff0c\u540e\u9762\u7ed8\u56fe\u65f6\u7528\n                if mem_max > self.max_mem:\n                    self.max_mem = mem_max\n                cpu_max, mem_max = 0, 0  # \u4e34\u65f6\u53d8\u91cf\n                # \u51c6\u5907\u8fdb\u884c\u4e0b\u4e00\u4e2a\u65b9\u6cd5\u7684\u68c0\u67e5\uff0c\u53d1\u73b0\u5df2\u7ecf\u68c0\u67e5\u5b8c\u5219\u6b63\u5f0f\u7ed3\u675f\n                del method_exec_info[0]\n                if method_exec_info:\n                    method_index += 1  # \u8fdb\u884c\u4e0b\u4e00\u4e2a\u65b9\u6cd5\u65f6:\u5f53\u524d\u65b9\u6cd5\u7684\u5e8f\u53f7+1\n                    continue\n                else:\n                    break", "output": "\u83b7\u53d6\u6bcf\u4e2a\u65b9\u6cd5\u4e2d\u7684cpu\u548c\u5185\u5b58\u8017\u8d39\u6700\u503c\u70b9.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def scc(graph):\n    '''  '''\n    order = []\n    vis = {vertex: False for vertex in graph}\n\n    graph_transposed = {vertex: [] for vertex in graph}\n\n    for (v, neighbours) in graph.iteritems():\n        for u in neighbours:\n            add_edge(graph_transposed, u, v)\n\n    for v in graph:\n        if not vis[v]:\n            dfs_transposed(v, graph_transposed, order, vis)\n\n    vis = {vertex: False for vertex in graph}\n    vertex_scc = {}\n\n    current_comp = 0\n    for v in reversed(order):\n        if not vis[v]:\n            # Each dfs will visit exactly one component\n            dfs(v, current_comp, vertex_scc, graph, vis)\n            current_comp += 1\n\n    return vertex_scc", "output": "Computes the strongly connected components of a graph", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_redirect_location(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.status in self.REDIRECT_STATUSES:\n            return self.headers.get('location')\n\n        return False", "output": "Should we redirect and where to?\n\n        :returns: Truthy redirect location string if we got a redirect status\n            code and valid location. ``None`` if redirect status and no\n            location. ``False`` if not a redirect status code.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_token(self, token):\n        '''\n        \n        '''\n        load = {}\n        load['token'] = token\n        load['cmd'] = 'get_token'\n        tdata = self._send_token_request(load)\n        return tdata", "output": "Request a token from the master", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cloud(tgt, provider=None):\n    '''\n    \n    '''\n    if not isinstance(tgt, six.string_types):\n        return {}\n\n    opts = salt.config.cloud_config(\n        os.path.join(os.path.dirname(__opts__['conf_file']), 'cloud')\n    )\n    if not opts.get('update_cachedir'):\n        return {}\n\n    cloud_cache = __utils__['cloud.list_cache_nodes_full'](opts=opts, provider=provider)\n    if cloud_cache is None:\n        return {}\n\n    ret = {}\n    for driver, providers in six.iteritems(cloud_cache):\n        for provider, servers in six.iteritems(providers):\n            for name, data in six.iteritems(servers):\n                if fnmatch.fnmatch(name, tgt):\n                    ret[name] = data\n                    ret[name]['provider'] = provider\n    return ret", "output": "Return cloud cache data for target.\n\n    .. note:: Only works with glob matching\n\n    tgt\n      Glob Target to match minion ids\n\n    provider\n      Cloud Provider\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run cache.cloud 'salt*'\n        salt-run cache.cloud glance.example.org provider=openstack", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_bnum(opts, minions, quiet):\n    '''\n    \n    '''\n    partition = lambda x: float(x) / 100.0 * len(minions)\n    try:\n        if '%' in opts['batch']:\n            res = partition(float(opts['batch'].strip('%')))\n            if res < 1:\n                return int(math.ceil(res))\n            else:\n                return int(res)\n        else:\n            return int(opts['batch'])\n    except ValueError:\n        if not quiet:\n            salt.utils.stringutils.print_cli('Invalid batch data sent: {0}\\nData must be in the '\n                      'form of %10, 10% or 3'.format(opts['batch']))", "output": "Return the active number of minions to maintain", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log_path(cls, project, log):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/logs/{log}\", project=project, log=log\n        )", "output": "Return a fully-qualified log string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wavfile_to_examples(wav_file):\n  \"\"\"\n  \"\"\"\n  from scipy.io import wavfile\n  sr, wav_data = wavfile.read(wav_file)\n  assert wav_data.dtype == np.int16, 'Bad sample type: %r' % wav_data.dtype\n  samples = wav_data / 32768.0  # Convert to [-1.0, +1.0]\n  return waveform_to_examples(samples, sr)", "output": "Convenience wrapper around waveform_to_examples() for a common WAV format.\n\n  Args:\n    wav_file: String path to a file, or a file-like object. The file\n    is assumed to contain WAV audio data with signed 16-bit PCM samples.\n\n  Returns:\n    See waveform_to_examples.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(self, item):\n        \"\"\"\n        \n        \"\"\"\n        loc = self.items.get_loc(item)\n        self._block.delete(loc)\n        self.axes[0] = self.axes[0].delete(loc)", "output": "Delete single item from SingleBlockManager.\n\n        Ensures that self.blocks doesn't become empty.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_workday(dt):\n    \"\"\"\n    \n    \"\"\"\n    dt += timedelta(days=1)\n    while dt.weekday() > 4:\n        # Mon-Fri are 0-4\n        dt += timedelta(days=1)\n    return dt", "output": "returns next weekday used for observances", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_input_files_for_numbered_seq(sourceDir, suffix, containers):\n    \"\"\"\"\"\"\n    # Check input files for each MPL-container type.\n    for container in containers:\n        files = glob.glob( os.path.join( sourceDir, container, container + '*' + suffix ) )\n        for currentFile in sorted( files ):\n            if check_header_comment( currentFile ):\n                return True\n    return False", "output": "Check if files, used as input when pre-processing MPL-containers in their numbered form, need fixing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_header_param(name, value):\n    \"\"\"\n    \n    \"\"\"\n    if not any(ch in value for ch in '\"\\\\\\r\\n'):\n        result = '%s=\"%s\"' % (name, value)\n        try:\n            result.encode('ascii')\n        except (UnicodeEncodeError, UnicodeDecodeError):\n            pass\n        else:\n            return result\n    if not six.PY3 and isinstance(value, six.text_type):  # Python 2:\n        value = value.encode('utf-8')\n    value = email.utils.encode_rfc2231(value, 'utf-8')\n    value = '%s*=%s' % (name, value)\n    return value", "output": "Helper function to format and quote a single header parameter.\n\n    Particularly useful for header parameters which might contain\n    non-ASCII values, like file names. This follows RFC 2231, as\n    suggested by RFC 2388 Section 4.4.\n\n    :param name:\n        The name of the parameter, a string expected to be ASCII only.\n    :param value:\n        The value of the parameter, provided as a unicode string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log_cmap(field_name, palette, low, high, low_color=None, high_color=None, nan_color=\"gray\"):\n    ''' \n\n    '''\n    return field(field_name, LogColorMapper(palette=palette,\n                                            low=low,\n                                            high=high,\n                                            nan_color=nan_color,\n                                            low_color=low_color,\n                                            high_color=high_color))", "output": "Create a ``DataSpec`` dict that applies a client-side ``LogColorMapper``\n    transformation to a ``ColumnDataSource`` column.\n\n    Args:\n        field_name (str) : a field name to configure ``DataSpec`` with\n\n        palette (seq[color]) : a list of colors to use for colormapping\n\n        low (float) : a minimum value of the range to map into the palette.\n            Values below this are clamped to ``low``.\n\n        high (float) : a maximum value of the range to map into the palette.\n            Values above this are clamped to ``high``.\n\n        low_color (color, optional) : color to be used if data is lower than\n            ``low`` value. If None, values lower than ``low`` are mapped to the\n            first color in the palette. (default: None)\n\n        high_color (color, optional) : color to be used if data is higher than\n            ``high`` value. If None, values higher than ``high`` are mapped to\n            the last color in the palette. (default: None)\n\n        nan_color (color, optional) : a default color to use when mapping data\n            from a column does not succeed (default: \"gray\")", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lambda_handler(event, context):\n    \"\"\" \n    \"\"\"\n    print(\"event.session.application.applicationId=\" +\n          event['session']['application']['applicationId'])\n\n    \"\"\"\n    Uncomment this if statement and populate with your skill's application ID to\n    prevent someone else from configuring a skill that sends requests to this\n    function.\n    \"\"\"\n    # if (event['session']['application']['applicationId'] !=\n    #         \"amzn1.echo-sdk-ams.app.[unique-value-here]\"):\n    #     raise ValueError(\"Invalid Application ID\")\n\n    if event['session']['new']:\n        on_session_started({'requestId': event['request']['requestId']},\n                           event['session'])\n\n    if event['request']['type'] == \"LaunchRequest\":\n        return on_launch(event['request'], event['session'])\n    elif event['request']['type'] == \"IntentRequest\":\n        return on_intent(event['request'], event['session'])\n    elif event['request']['type'] == \"SessionEndedRequest\":\n        return on_session_ended(event['request'], event['session'])", "output": "Route the incoming request based on type (LaunchRequest, IntentRequest,\n    etc.) The JSON body of the request is provided in the event parameter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def initialize_env_specs(hparams, env_problem_name):\n  \"\"\"\"\"\"\n  if env_problem_name:\n    env = registry.env_problem(env_problem_name, batch_size=hparams.batch_size)\n  else:\n    env = rl_utils.setup_env(hparams, hparams.batch_size,\n                             hparams.eval_max_num_noops,\n                             hparams.rl_env_max_episode_steps,\n                             env_name=hparams.rl_env_name)\n    env.start_new_epoch(0)\n\n  return rl.make_real_env_fn(env)", "output": "Initializes env_specs using the appropriate env.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _configure_iam_role(config):\n    \"\"\"\n    \"\"\"\n    email = SERVICE_ACCOUNT_EMAIL_TEMPLATE.format(\n        account_id=DEFAULT_SERVICE_ACCOUNT_ID,\n        project_id=config[\"provider\"][\"project_id\"])\n    service_account = _get_service_account(email, config)\n\n    if service_account is None:\n        logger.info(\"_configure_iam_role: \"\n                    \"Creating new service account {}\".format(\n                        DEFAULT_SERVICE_ACCOUNT_ID))\n\n        service_account = _create_service_account(\n            DEFAULT_SERVICE_ACCOUNT_ID, DEFAULT_SERVICE_ACCOUNT_CONFIG, config)\n\n    assert service_account is not None, \"Failed to create service account\"\n\n    _add_iam_policy_binding(service_account, DEFAULT_SERVICE_ACCOUNT_ROLES)\n\n    config[\"head_node\"][\"serviceAccounts\"] = [{\n        \"email\": service_account[\"email\"],\n        # NOTE: The amount of access is determined by the scope + IAM\n        # role of the service account. Even if the cloud-platform scope\n        # gives (scope) access to the whole cloud-platform, the service\n        # account is limited by the IAM rights specified below.\n        \"scopes\": [\"https://www.googleapis.com/auth/cloud-platform\"]\n    }]\n\n    return config", "output": "Setup a gcp service account with IAM roles.\n\n    Creates a gcp service acconut and binds IAM roles which allow it to control\n    control storage/compute services. Specifically, the head node needs to have\n    an IAM role that allows it to create further gce instances and store items\n    in google cloud storage.\n\n    TODO: Allow the name/id of the service account to be configured", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def plot_multi(func:Callable[[int,int,plt.Axes],None], r:int=1, c:int=1, figsize:Tuple=(12,6)):\n    \"\"\n    axes = plt.subplots(r, c, figsize=figsize)[1]\n    for i in range(r):\n        for j in range(c): func(i,j,axes[i,j])", "output": "Call `func` for every combination of `r,c` on a subplot", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def auto_complete_paths(current, completion_type):\n    \"\"\"\n    \"\"\"\n    directory, filename = os.path.split(current)\n    current_path = os.path.abspath(directory)\n    # Don't complete paths if they can't be accessed\n    if not os.access(current_path, os.R_OK):\n        return\n    filename = os.path.normcase(filename)\n    # list all files that start with ``filename``\n    file_list = (x for x in os.listdir(current_path)\n                 if os.path.normcase(x).startswith(filename))\n    for f in file_list:\n        opt = os.path.join(current_path, f)\n        comp_file = os.path.normcase(os.path.join(directory, f))\n        # complete regular files when there is not ``<dir>`` after option\n        # complete directories when there is ``<file>``, ``<path>`` or\n        # ``<dir>``after option\n        if completion_type != 'dir' and os.path.isfile(opt):\n            yield comp_file\n        elif os.path.isdir(opt):\n            yield os.path.join(comp_file, '')", "output": "If ``completion_type`` is ``file`` or ``path``, list all regular files\n    and directories starting with ``current``; otherwise only list directories\n    starting with ``current``.\n\n    :param current: The word to be completed\n    :param completion_type: path completion type(`file`, `path` or `dir`)i\n    :return: A generator of regular files and/or directories", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_operator(self, node_name, op_name, attrs, inputs):\n        \"\"\"\n        \"\"\"\n        if op_name in convert_map:\n            op_name, new_attrs, inputs = convert_map[op_name](attrs, inputs, self)\n        else:\n            raise NotImplementedError(\"Operator {} not implemented.\".format(op_name))\n        if isinstance(op_name, string_types):\n            new_op = getattr(symbol, op_name, None)\n            if not new_op:\n                raise RuntimeError(\"Unable to map op_name {} to sym\".format(op_name))\n            if node_name is None:\n                mxnet_sym = new_op(*inputs, **new_attrs)\n            else:\n                mxnet_sym = new_op(name=node_name, *inputs, **new_attrs)\n            return mxnet_sym\n        return op_name", "output": "Convert from onnx operator to mxnet operator.\n        The converter must specify conversions explicitly for incompatible name, and\n        apply handlers to operator attributes.\n\n        Parameters\n        ----------\n        :param node_name : str\n            name of the node to be translated.\n        :param op_name : str\n            Operator name, such as Convolution, FullyConnected\n        :param attrs : dict\n            Dict of operator attributes\n        :param inputs: list\n            list of inputs to the operator\n        Returns\n        -------\n        :return mxnet_sym\n            Converted mxnet symbol", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_cython(obj):\n    \"\"\"\"\"\"\n\n    # TODO(suo): We could split these into two functions, one for Cython\n    # functions and another for Cython methods.\n    # TODO(suo): There doesn't appear to be a Cython function 'type' we can\n    # check against via isinstance. Please correct me if I'm wrong.\n    def check_cython(x):\n        return type(x).__name__ == \"cython_function_or_method\"\n\n    # Check if function or method, respectively\n    return check_cython(obj) or \\\n        (hasattr(obj, \"__func__\") and check_cython(obj.__func__))", "output": "Check if an object is a Cython function or method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def default_kms_key_name(self, value):\n        \"\"\"\n        \"\"\"\n        encryption_config = self._properties.get(\"encryption\", {})\n        encryption_config[\"defaultKmsKeyName\"] = value\n        self._patch_property(\"encryption\", encryption_config)", "output": "Set default KMS encryption key for objects in the bucket.\n\n        :type value: str or None\n        :param value: new KMS key name (None to clear any existing key).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _next_generation(self, sorted_trials):\n        \"\"\"\n        \"\"\"\n\n        candidate = []\n        next_generation = []\n        num_population = self._next_population_size(len(sorted_trials))\n        top_num = int(max(num_population * self._keep_top_ratio, 2))\n\n        for i in range(top_num):\n            candidate.append(sorted_trials[i].extra_arg)\n            next_generation.append(sorted_trials[i].extra_arg)\n\n        for i in range(top_num, num_population):\n            flip_coin = np.random.uniform()\n            if flip_coin < self._selection_bound:\n                next_generation.append(GeneticSearch._selection(candidate))\n            else:\n                if flip_coin < self._selection_bound + self._crossover_bound:\n                    next_generation.append(GeneticSearch._crossover(candidate))\n                else:\n                    next_generation.append(GeneticSearch._mutation(candidate))\n        return next_generation", "output": "Generate genes (encodings) for the next generation.\n\n        Use the top K (_keep_top_ratio) trials of the last generation\n        as candidates to generate the next generation. The action could\n        be selection, crossover and mutation according corresponding\n        ratio (_selection_bound, _crossover_bound).\n\n        Args:\n            sorted_trials: List of finished trials with top\n                performance ones first.\n\n        Returns:\n            A list of new genes (encodings)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_pythonshell_font(self, font=None):\n        \"\"\"\"\"\"\n        if font is None:\n            font = QFont()\n        for style in self.font_styles:\n            style.apply_style(font=font,\n                              is_default=style is self.default_style)\n        self.ansi_handler.set_base_format(self.default_style.format)", "output": "Python Shell only", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def quick_layout_settings(self):\r\n        \"\"\"\"\"\"\r\n        get = CONF.get\r\n        set_ = CONF.set\r\n\r\n        section = 'quick_layouts'\r\n\r\n        names = get(section, 'names')\r\n        order = get(section, 'order')\r\n        active = get(section, 'active')\r\n\r\n        dlg = self.dialog_layout_settings(self, names, order, active)\r\n        if dlg.exec_():\r\n            set_(section, 'names', dlg.names)\r\n            set_(section, 'order', dlg.order)\r\n            set_(section, 'active', dlg.active)\r\n            self.quick_layout_set_menu()", "output": "Layout settings dialog", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_quota_volume(name):\n    '''\n    \n    '''\n    cmd = 'volume quota {0}'.format(name)\n    cmd += ' list'\n\n    root = _gluster_xml(cmd)\n    if not _gluster_ok(root):\n        return None\n\n    ret = {}\n    for limit in _iter(root, 'limit'):\n        path = limit.find('path').text\n        ret[path] = _etree_to_dict(limit)\n\n    return ret", "output": "List quotas of glusterfs volume\n\n    name\n        Name of the gluster volume\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' glusterfs.list_quota_volume <volume>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _heapify_max(x):\n    \"\"\"\"\"\"\n    n = len(x)\n    for i in reversed(range(n//2)):\n        _siftup_max(x, i)", "output": "Transform list into a maxheap, in-place, in O(len(x)) time.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def search(filter,      # pylint: disable=C0103\n           dn=None,     # pylint: disable=C0103\n           scope=None,\n           attrs=None,\n           **kwargs):\n    '''\n    \n    '''\n    if not dn:\n        dn = _config('dn', 'basedn')  # pylint: disable=C0103\n    if not scope:\n        scope = _config('scope')\n    if attrs == '':  # Allow command line 'return all' attr override\n        attrs = None\n    elif attrs is None:\n        attrs = _config('attrs')\n    _ldap = _connect(**kwargs)\n    start = time.time()\n    log.debug(\n        'Running LDAP search with filter:%s, dn:%s, scope:%s, '\n        'attrs:%s', filter, dn, scope, attrs\n    )\n    results = _ldap.search_s(dn, int(scope), filter, attrs)\n    elapsed = (time.time() - start)\n    if elapsed < 0.200:\n        elapsed_h = six.text_type(round(elapsed * 1000, 1)) + 'ms'\n    else:\n        elapsed_h = six.text_type(round(elapsed, 2)) + 's'\n\n    ret = {\n        'results': results,\n        'count': len(results),\n        'time': {'human': elapsed_h, 'raw': six.text_type(round(elapsed, 5))},\n    }\n    return ret", "output": "Run an arbitrary LDAP query and return the results.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'ldaphost' ldap.search \"filter=cn=myhost\"\n\n    Return data:\n\n    .. code-block:: python\n\n        {'myhost': {'count': 1,\n                    'results': [['cn=myhost,ou=hosts,o=acme,c=gb',\n                                 {'saltKeyValue': ['ntpserver=ntp.acme.local',\n                                                   'foo=myfoo'],\n                                  'saltState': ['foo', 'bar']}]],\n                    'time': {'human': '1.2ms', 'raw': '0.00123'}}}\n\n    Search and connection options can be overridden by specifying the relevant\n    option as key=value pairs, for example:\n\n    .. code-block:: bash\n\n        salt 'ldaphost' ldap.search filter=cn=myhost dn=ou=hosts,o=acme,c=gb\n        scope=1 attrs='' server='localhost' port='7393' tls=True bindpw='ssh'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_recurrent_encoder(input_state, memory_in, sequence_length, name='lstm'):\n        \"\"\"\n        \n        \"\"\"\n        s_size = input_state.get_shape().as_list()[1]\n        m_size = memory_in.get_shape().as_list()[1]\n        lstm_input_state = tf.reshape(input_state, shape=[-1, sequence_length, s_size])\n        memory_in = tf.reshape(memory_in[:, :], [-1, m_size])\n        _half_point = int(m_size / 2)\n        with tf.variable_scope(name):\n            rnn_cell = tf.contrib.rnn.BasicLSTMCell(_half_point)\n            lstm_vector_in = tf.contrib.rnn.LSTMStateTuple(memory_in[:, :_half_point],\n                                                           memory_in[:, _half_point:])\n            recurrent_output, lstm_state_out = tf.nn.dynamic_rnn(rnn_cell, lstm_input_state,\n                                                                 initial_state=lstm_vector_in)\n\n        recurrent_output = tf.reshape(recurrent_output, shape=[-1, _half_point])\n        return recurrent_output, tf.concat([lstm_state_out.c, lstm_state_out.h], axis=1)", "output": "Builds a recurrent encoder for either state or observations (LSTM).\n        :param sequence_length: Length of sequence to unroll.\n        :param input_state: The input tensor to the LSTM cell.\n        :param memory_in: The input memory to the LSTM cell.\n        :param name: The scope of the LSTM cell.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sink_create(\n        self, project, sink_name, filter_, destination, unique_writer_identity=False\n    ):\n        \"\"\"\n        \"\"\"\n        parent = \"projects/%s\" % (project,)\n        sink_pb = LogSink(name=sink_name, filter=filter_, destination=destination)\n        created_pb = self._gapic_api.create_sink(\n            parent, sink_pb, unique_writer_identity=unique_writer_identity\n        )\n        return MessageToDict(created_pb)", "output": "API call:  create a sink resource.\n\n        See\n        https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.sinks/create\n\n        :type project: str\n        :param project: ID of the project in which to create the sink.\n\n        :type sink_name: str\n        :param sink_name: the name of the sink\n\n        :type filter_: str\n        :param filter_: the advanced logs filter expression defining the\n                        entries exported by the sink.\n\n        :type destination: str\n        :param destination: destination URI for the entries exported by\n                            the sink.\n\n        :type unique_writer_identity: bool\n        :param unique_writer_identity: (Optional) determines the kind of\n                                       IAM identity returned as\n                                       writer_identity in the new sink.\n\n        :rtype: dict\n        :returns: The sink resource returned from the API (converted from a\n                  protobuf to a dictionary).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get(self):\n        '''\n        \n\n        '''\n        user = self.USER\n        try:\n            uid = pwd.getpwnam(user).pw_uid\n        except KeyError:\n            log.info('User does not exist')\n            return False\n\n        cmd = self.gsetting_command + ['get', str(self.SCHEMA), str(self.KEY)]\n        environ = {}\n        environ['XDG_RUNTIME_DIR'] = '/run/user/{0}'.format(uid)\n        result = __salt__['cmd.run_all'](cmd, runas=user, env=environ, python_shell=False)\n\n        if 'stdout' in result:\n            if 'uint32' in result['stdout']:\n                return re.sub('uint32 ', '', result['stdout'])\n            else:\n                return result['stdout']\n        else:\n            return False", "output": "get the value for user in gsettings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dt64arr_to_periodarr(data, freq, tz=None):\n    \"\"\"\n    \n\n    \"\"\"\n    if data.dtype != np.dtype('M8[ns]'):\n        raise ValueError('Wrong dtype: {dtype}'.format(dtype=data.dtype))\n\n    if freq is None:\n        if isinstance(data, ABCIndexClass):\n            data, freq = data._values, data.freq\n        elif isinstance(data, ABCSeries):\n            data, freq = data._values, data.dt.freq\n\n    freq = Period._maybe_convert_freq(freq)\n\n    if isinstance(data, (ABCIndexClass, ABCSeries)):\n        data = data._values\n\n    base, mult = libfrequencies.get_freq_code(freq)\n    return libperiod.dt64arr_to_periodarr(data.view('i8'), base, tz), freq", "output": "Convert an datetime-like array to values Period ordinals.\n\n    Parameters\n    ----------\n    data : Union[Series[datetime64[ns]], DatetimeIndex, ndarray[datetime64ns]]\n    freq : Optional[Union[str, Tick]]\n        Must match the `freq` on the `data` if `data` is a DatetimeIndex\n        or Series.\n    tz : Optional[tzinfo]\n\n    Returns\n    -------\n    ordinals : ndarray[int]\n    freq : Tick\n        The frequencey extracted from the Series or DatetimeIndex if that's\n        used.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_control(self, char):\n        \"\"\"\"\"\"\n        # These are technically control characters but we count them as whitespace\n        # characters.\n        if char in ['\\t', '\\n', '\\r']:\n            return False\n        cat = unicodedata.category(char)\n        if cat.startswith('C'):\n            return True\n        return False", "output": "Checks whether `chars` is a control character.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fill_binop(left, right, fill_value):\n    \"\"\"\n    \n    \"\"\"\n    # TODO: can we make a no-copy implementation?\n    if fill_value is not None:\n        left_mask = isna(left)\n        right_mask = isna(right)\n        left = left.copy()\n        right = right.copy()\n\n        # one but not both\n        mask = left_mask ^ right_mask\n        left[left_mask & mask] = fill_value\n        right[right_mask & mask] = fill_value\n    return left, right", "output": "If a non-None fill_value is given, replace null entries in left and right\n    with this value, but only in positions where _one_ of left/right is null,\n    not both.\n\n    Parameters\n    ----------\n    left : array-like\n    right : array-like\n    fill_value : object\n\n    Returns\n    -------\n    left : array-like\n    right : array-like\n\n    Notes\n    -----\n    Makes copies if fill_value is not None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sort_cols(self, cols, kwargs):\n        \"\"\" \n        \"\"\"\n        if not cols:\n            raise ValueError(\"should sort by at least one column\")\n        if len(cols) == 1 and isinstance(cols[0], list):\n            cols = cols[0]\n        jcols = [_to_java_column(c) for c in cols]\n        ascending = kwargs.get('ascending', True)\n        if isinstance(ascending, (bool, int)):\n            if not ascending:\n                jcols = [jc.desc() for jc in jcols]\n        elif isinstance(ascending, list):\n            jcols = [jc if asc else jc.desc()\n                     for asc, jc in zip(ascending, jcols)]\n        else:\n            raise TypeError(\"ascending can only be boolean or list, but got %s\" % type(ascending))\n        return self._jseq(jcols)", "output": "Return a JVM Seq of Columns that describes the sort order", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_default_storage_policy_of_datastore(profile_manager, datastore):\n    '''\n    \n    '''\n    # Retrieve all datastores visible\n    hub = pbm.placement.PlacementHub(\n        hubId=datastore._moId, hubType='Datastore')\n    log.trace('placement_hub = %s', hub)\n    try:\n        policy_id = profile_manager.QueryDefaultRequirementProfile(hub)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise VMwareApiError('Not enough permissions. Required privilege: '\n                             '{0}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise VMwareRuntimeError(exc.msg)\n    policy_refs = get_policies_by_id(profile_manager, [policy_id])\n    if not policy_refs:\n        raise VMwareObjectRetrievalError('Storage policy with id \\'{0}\\' was '\n                                         'not found'.format(policy_id))\n    return policy_refs[0]", "output": "Returns the default storage policy reference assigned to a datastore.\n\n    profile_manager\n        Reference to the profile manager.\n\n    datastore\n        Reference to the datastore.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_self_subject_rules_review(self, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_self_subject_rules_review_with_http_info(body, **kwargs)\n        else:\n            (data) = self.create_self_subject_rules_review_with_http_info(body, **kwargs)\n            return data", "output": "create a SelfSubjectRulesReview\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_self_subject_rules_review(body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param V1SelfSubjectRulesReview body: (required)\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1SelfSubjectRulesReview\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(self, fn_):\n        '''\n        \n        '''\n        data = fn_.read()\n        fn_.close()\n        if data:\n            if six.PY3:\n                return self.loads(data, encoding='utf-8')\n            else:\n                return self.loads(data)", "output": "Run the correct serialization to load a file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_json(cls, json_str):\n        \"\"\"\n        \"\"\"\n        vocab_dict = json.loads(json_str)\n\n        unknown_token = vocab_dict.get('unknown_token')\n        vocab = cls(unknown_token=unknown_token)\n        vocab._idx_to_token = vocab_dict.get('idx_to_token')\n        vocab._token_to_idx = vocab_dict.get('token_to_idx')\n        if unknown_token:\n            vocab._token_to_idx = DefaultLookupDict(vocab._token_to_idx[unknown_token],\n                                                    vocab._token_to_idx)\n        vocab._reserved_tokens = vocab_dict.get('reserved_tokens')\n        vocab._padding_token = vocab_dict.get('padding_token')\n        vocab._bos_token = vocab_dict.get('bos_token')\n        vocab._eos_token = vocab_dict.get('eos_token')\n        return vocab", "output": "Deserialize Vocab object from json string.\n\n        Parameters\n        ----------\n        json_str : str\n            Serialized json string of a Vocab object.\n\n\n        Returns\n        -------\n        Vocab", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log_loss(preds, labels):\n    \"\"\"\"\"\"\n    log_likelihood = np.sum(labels * np.log(preds)) / len(preds)\n    return -log_likelihood", "output": "Logarithmic loss with non-necessarily-binary labels.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wd(self, val:float)->None:\n        \"\"\n        if not self.true_wd: self.set_val('weight_decay', listify(val, self._wd), bn_groups=self.bn_wd)\n        self._wd = listify(val, self._wd)", "output": "Set weight decay.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n        \"\"\n        return sep.join([self.itos[i] for i in nums]) if sep is not None else [self.itos[i] for i in nums]", "output": "Convert a list of `nums` to their tokens.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_(conn,\n         key,\n         value,\n         time=DEFAULT_TIME,\n         min_compress_len=DEFAULT_MIN_COMPRESS_LEN):\n    '''\n    \n    '''\n    if not isinstance(time, integer_types):\n        raise SaltInvocationError('\\'time\\' must be an integer')\n    if not isinstance(min_compress_len, integer_types):\n        raise SaltInvocationError('\\'min_compress_len\\' must be an integer')\n    _check_stats(conn)\n    return conn.set(key, value, time, min_compress_len)", "output": "Set a key on the memcached server, overwriting the value if it exists.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _refresh(self):\n        \"\"\"\n        \"\"\"\n        padding = self.height()\n        css_base = \"\"\"QLineEdit {{\n                                 border: none;\n                                 padding-right: {padding}px;\n                                 }}\n                   \"\"\"\n        css_oxygen = \"\"\"QLineEdit {{background: transparent;\n                                   border: none;\n                                   padding-right: {padding}px;\n                                   }}\n                     \"\"\"\n        if self._application_style == 'oxygen':\n            css_template = css_oxygen\n        else:\n            css_template = css_base\n\n        css = css_template.format(padding=padding)\n        self.setStyleSheet(css)\n        self.update()", "output": "After an application style change, the paintEvent updates the\n        custom defined stylesheet.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_retry_request(self):\n        \"\"\"\"\"\"\n        req_manager = _ReadRowsRequestManager(\n            self.request, self.last_scanned_row_key, self._counter\n        )\n        return req_manager.build_updated_request()", "output": "Helper for :meth:`__iter__`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_frame_savp_gan():\n  \"\"\"\"\"\"\n  hparams = next_frame_savp()\n  hparams.use_gan = True\n  hparams.use_vae = False\n  hparams.gan_loss_multiplier = 0.001\n  hparams.optimizer_adam_beta1 = 0.5\n  hparams.learning_rate_constant = 2e-4\n  hparams.gan_loss = \"cross_entropy\"\n  hparams.learning_rate_decay_steps = 100000\n  hparams.learning_rate_schedule = \"constant*linear_decay\"\n  return hparams", "output": "SAVP - GAN only model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_newest_ckpt(ckpt_dir):\n    \"\"\"\"\"\"\n    full_paths = [\n        os.path.join(ckpt_dir, fname) for fname in os.listdir(ckpt_dir)\n        if fname.startswith(\"experiment_state\") and fname.endswith(\".json\")\n    ]\n    return max(full_paths)", "output": "Returns path to most recently modified checkpoint.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_auto_deployable(self, stage, swagger=None):\n        \"\"\"\n        \n        \"\"\"\n        if not swagger:\n            return\n\n        # CloudFormation does NOT redeploy the API unless it has a new deployment resource\n        # that points to latest RestApi resource. Append a hash of Swagger Body location to\n        # redeploy only when the API data changes. First 10 characters of hash is good enough\n        # to prevent redeployment when API has not changed\n\n        # NOTE: `str(swagger)` is for backwards compatibility. Changing it to a JSON or something will break compat\n        generator = logical_id_generator.LogicalIdGenerator(self.logical_id, str(swagger))\n        self.logical_id = generator.gen()\n        hash = generator.get_hash(length=40)  # Get the full hash\n        self.Description = \"RestApi deployment id: {}\".format(hash)\n        stage.update_deployment_ref(self.logical_id)", "output": "Sets up the resource such that it will triggers a re-deployment when Swagger changes\n\n        :param swagger: Dictionary containing the Swagger definition of the API", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_event(self, rule, callback):\n        \"\"\"\n        \"\"\"\n        self.event_manager.add_event(\n            zipline.utils.events.Event(rule, callback),\n        )", "output": "Adds an event to the algorithm's EventManager.\n\n        Parameters\n        ----------\n        rule : EventRule\n            The rule for when the callback should be triggered.\n        callback : callable[(context, data) -> None]\n            The function to execute when the rule is triggered.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _extract_lambda_function_code(resource_properties, code_property_key):\n        \"\"\"\n        \n        \"\"\"\n\n        codeuri = resource_properties.get(code_property_key, SamFunctionProvider._DEFAULT_CODEURI)\n\n        if isinstance(codeuri, dict):\n            codeuri = SamFunctionProvider._DEFAULT_CODEURI\n\n        return codeuri", "output": "Extracts the Lambda Function Code from the Resource Properties\n\n        Parameters\n        ----------\n        resource_properties dict\n            Dictionary representing the Properties of the Resource\n        code_property_key str\n            Property Key of the code on the Resource\n\n        Returns\n        -------\n        str\n            Representing the local code path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_data_for_env_problem(problem_name):\n  \"\"\"\"\"\"\n  assert FLAGS.env_problem_max_env_steps > 0, (\"--env_problem_max_env_steps \"\n                                               \"should be greater than zero\")\n  assert FLAGS.env_problem_batch_size > 0, (\"--env_problem_batch_size should be\"\n                                            \" greather than zero\")\n  problem = registry.env_problem(problem_name)\n  task_id = None if FLAGS.task_id < 0 else FLAGS.task_id\n  data_dir = os.path.expanduser(FLAGS.data_dir)\n  tmp_dir = os.path.expanduser(FLAGS.tmp_dir)\n  # TODO(msaffar): Handle large values for env_problem_batch_size where we\n  #  cannot create that many environments within the same process.\n  problem.initialize(batch_size=FLAGS.env_problem_batch_size)\n  env_problem_utils.play_env_problem_randomly(\n      problem, num_steps=FLAGS.env_problem_max_env_steps)\n  problem.generate_data(data_dir=data_dir, tmp_dir=tmp_dir, task_id=task_id)", "output": "Generate data for `EnvProblem`s.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _clear_context(context):\n    '''\n    \n    '''\n    for var in (x for x in __context__ if x.startswith('chocolatey.')):\n        context.pop(var)", "output": "Clear variables stored in __context__. Run this function when a new version\n    of chocolatey is installed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def receive_trial_result(self, parameter_id, parameters, value):\n        \"\"\"\n        \n        \"\"\"\n        reward = extract_scalar_reward(value)\n        # restore the paramsters contains '_index'\n        if parameter_id not in self.total_data:\n            raise RuntimeError('Received parameter_id not in total_data.')\n        params = self.total_data[parameter_id]\n\n        if self.optimize_mode is OptimizeMode.Maximize:\n            reward = -reward\n\n        rval = self.rval\n        domain = rval.domain\n        trials = rval.trials\n\n        new_id = len(trials)\n\n        rval_specs = [None]\n        rval_results = [domain.new_result()]\n        rval_miscs = [dict(tid=new_id, cmd=domain.cmd, workdir=domain.workdir)]\n\n        vals = params\n        idxs = dict()\n\n        out_y = dict()\n        json2vals(self.json, vals, out_y)\n        vals = out_y\n        for key in domain.params:\n            if key in [VALUE, INDEX]:\n                continue\n            if key not in vals or vals[key] is None or vals[key] == []:\n                idxs[key] = vals[key] = []\n            else:\n                idxs[key] = [new_id]\n                vals[key] = [vals[key]]\n\n        self.miscs_update_idxs_vals(rval_miscs, idxs, vals,\n                                    idxs_map={new_id: new_id},\n                                    assert_all_vals_used=False)\n\n        trial = trials.new_trial_docs([new_id], rval_specs, rval_results, rval_miscs)[0]\n        trial['result'] = {'loss': reward, 'status': 'ok'}\n        trial['state'] = hp.JOB_STATE_DONE\n        trials.insert_trial_docs([trial])\n        trials.refresh()", "output": "Record an observation of the objective function\n\n        Parameters\n        ----------\n        parameter_id : int\n        parameters : dict\n        value : dict/float\n            if value is dict, it should have \"default\" key.\n            value is final metrics of the trial.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_config(\n            self,\n            section='MONGODB',\n            option='uri',\n            default_value=DEFAULT_DB_URI\n    ):\n        \"\"\"\n        \"\"\"\n\n\n        res = self.client.quantaxis.usersetting.find_one({'section': section})\n        if res:\n            return res.get(option, default_value)\n        else:\n            self.set_config(section, option, default_value)\n            return default_value", "output": "[summary]\n\n        Keyword Arguments:\n            section {str} -- [description] (default: {'MONGODB'})\n            option {str} -- [description] (default: {'uri'})\n            default_value {[type]} -- [description] (default: {DEFAULT_DB_URI})\n\n        Returns:\n            [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check(set=None, entry=None, family='ipv4'):\n    '''\n    \n\n    '''\n    if not set:\n        return 'Error: Set needs to be specified'\n    if not entry:\n        return 'Error: Entry needs to be specified'\n\n    settype = _find_set_type(set)\n    if not settype:\n        return 'Error: Set {0} does not exist'.format(set)\n\n    current_members = _parse_members(settype, _find_set_members(set))\n\n    if not current_members:\n        return False\n\n    if isinstance(entry, list):\n        entries = _parse_members(settype, entry)\n    else:\n        entries = [_parse_member(settype, entry)]\n\n    for current_member in current_members:\n        for entry in entries:\n            if _member_contains(current_member, entry):\n                return True\n\n    return False", "output": "Check that an entry exists in the specified set.\n\n    set\n        The ipset name\n\n    entry\n        An entry in the ipset.  This parameter can be a single IP address, a\n        range of IP addresses, or a subnet block.  Example:\n\n        .. code-block:: cfg\n\n            192.168.0.1\n            192.168.0.2-192.168.0.19\n            192.168.0.0/25\n\n    family\n        IP protocol version: ipv4 or ipv6\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ipset.check setname '192.168.0.1 comment \"Hello\"'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dist_is_in_project(self, dist):\n        \"\"\"\"\"\"\n        from .project import _normalized\n        prefixes = [\n            _normalized(prefix) for prefix in self.base_paths[\"libdirs\"].split(os.pathsep)\n            if _normalized(prefix).startswith(_normalized(self.prefix.as_posix()))\n        ]\n        location = self.locate_dist(dist)\n        if not location:\n            return False\n        location = _normalized(make_posix(location))\n        return any(location.startswith(prefix) for prefix in prefixes)", "output": "Determine whether the supplied distribution is in the environment.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def limit(self, count):\n        \"\"\"\n        \"\"\"\n        query = query_mod.Query(self)\n        return query.limit(count)", "output": "Create a limited query with this collection as parent.\n\n        See\n        :meth:`~.firestore_v1beta1.query.Query.limit` for\n        more information on this method.\n\n        Args:\n            count (int): Maximum number of documents to return that match\n                the query.\n\n        Returns:\n            ~.firestore_v1beta1.query.Query: A limited query.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_defined(self, objtxt, force_import=False):\r\n        \"\"\"\"\"\"\r\n        return self.interpreter.is_defined(objtxt, force_import)", "output": "Return True if object is defined", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_numeric_methods_disabled(cls):\n        \"\"\"\n        \n        \"\"\"\n        cls.__pow__ = make_invalid_op('__pow__')\n        cls.__rpow__ = make_invalid_op('__rpow__')\n        cls.__mul__ = make_invalid_op('__mul__')\n        cls.__rmul__ = make_invalid_op('__rmul__')\n        cls.__floordiv__ = make_invalid_op('__floordiv__')\n        cls.__rfloordiv__ = make_invalid_op('__rfloordiv__')\n        cls.__truediv__ = make_invalid_op('__truediv__')\n        cls.__rtruediv__ = make_invalid_op('__rtruediv__')\n        cls.__mod__ = make_invalid_op('__mod__')\n        cls.__divmod__ = make_invalid_op('__divmod__')\n        cls.__neg__ = make_invalid_op('__neg__')\n        cls.__pos__ = make_invalid_op('__pos__')\n        cls.__abs__ = make_invalid_op('__abs__')\n        cls.__inv__ = make_invalid_op('__inv__')", "output": "Add in numeric methods to disable other than add/sub.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_wider_model(self, pre_layer_id, n_add):\n        \"\"\"\n        \"\"\"\n        self.operation_history.append((\"to_wider_model\", pre_layer_id, n_add))\n        pre_layer = self.layer_list[pre_layer_id]\n        output_id = self.layer_id_to_output_node_ids[pre_layer_id][0]\n        dim = layer_width(pre_layer)\n        self.vis = {}\n        self._search(output_id, dim, dim, n_add)\n        # Update the tensor shapes.\n        for u in self.topological_order:\n            for v, layer_id in self.adj_list[u]:\n                self.node_list[v].shape = self.layer_list[layer_id].output_shape", "output": "Widen the last dimension of the output of the pre_layer.\n        Args:\n            pre_layer_id: The ID of a convolutional layer or dense layer.\n            n_add: The number of dimensions to add.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deprecate_thing_type(thingTypeName, undoDeprecate=False,\n    region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.deprecate_thing_type(\n            thingTypeName=thingTypeName,\n            undoDeprecate=undoDeprecate\n        )\n        deprecated = True if undoDeprecate is False else False\n        return {'deprecated': deprecated}\n    except ClientError as e:\n        return {'deprecated': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Given a thing type name, deprecate it when undoDeprecate is False\n    and undeprecate it when undoDeprecate is True.\n\n    Returns {deprecated: true} if the thing type was deprecated and returns\n    {deprecated: false} if the thing type was not deprecated.\n\n    .. versionadded:: 2016.11.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iot.deprecate_thing_type mythingtype", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def engines(opts, functions, runners, utils, proxy=None):\n    '''\n    \n    '''\n    pack = {'__salt__': functions,\n            '__runners__': runners,\n            '__proxy__': proxy,\n            '__utils__': utils}\n    return LazyLoader(\n        _module_dirs(opts, 'engines'),\n        opts,\n        tag='engines',\n        pack=pack,\n    )", "output": "Return the master services plugins", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exception_occurred(self, text, is_traceback):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        # Skip errors without traceback or dismiss\r\n        if (not is_traceback and self.error_dlg is None) or self.dismiss_error:\r\n            return\r\n\r\n        if CONF.get('main', 'show_internal_errors'):\r\n            if self.error_dlg is None:\r\n                self.error_dlg = SpyderErrorDialog(self)\r\n                self.error_dlg.close_btn.clicked.connect(self.close_error_dlg)\r\n                self.error_dlg.rejected.connect(self.remove_error_dlg)\r\n                self.error_dlg.details.go_to_error.connect(self.go_to_error)\r\n                self.error_dlg.show()\r\n            self.error_dlg.append_traceback(text)\r\n        elif DEV or get_debug_level():\r\n            self.dockwidget.show()\r\n            self.dockwidget.raise_()", "output": "Exception ocurred in the internal console.\r\n\r\n        Show a QDialog or the internal console to warn the user.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def accept_override(self):\r\n        \"\"\"\"\"\"\r\n        conflicts = self.check_conflicts()\r\n        if conflicts:\r\n            for shortcut in conflicts:\r\n                shortcut.key = ''\r\n        self.accept()", "output": "Unbind all conflicted shortcuts, and accept the new one", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cleaned(_pipeline_objects):\n    \"\"\"\n    \"\"\"\n    pipeline_objects = copy.deepcopy(_pipeline_objects)\n    for pipeline_object in pipeline_objects:\n        if pipeline_object['id'] == 'DefaultSchedule':\n            for field_object in pipeline_object['fields']:\n                if field_object['key'] == 'startDateTime':\n                    start_date_time_string = field_object['stringValue']\n                    start_date_time = datetime.datetime.strptime(start_date_time_string,\n                                                                 \"%Y-%m-%dT%H:%M:%S\")\n                    field_object['stringValue'] = start_date_time.strftime(\"%H:%M:%S\")\n    return pipeline_objects", "output": "Return standardized pipeline objects to be used for comparing\n\n    Remove year, month, and day components of the startDateTime so that data\n    pipelines with the same time of day but different days are considered\n    equal.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def value_as_date(self):\n        ''' \n        '''\n        if self.value is None:\n            return None\n        v1, v2 = self.value\n        if isinstance(v1, numbers.Number):\n            dt = datetime.utcfromtimestamp(v1 / 1000)\n            d1 = date(*dt.timetuple()[:3])\n        else:\n            d1 = v1\n        if isinstance(v2, numbers.Number):\n            dt = datetime.utcfromtimestamp(v2 / 1000)\n            d2 = date(*dt.timetuple()[:3])\n        else:\n            d2 = v2\n        return d1, d2", "output": "Convenience property to retrieve the value tuple as a tuple of\n        date objects.\n\n        Added in version 1.1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists_alias(self, index=None, name=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"HEAD\", _make_path(index, \"_alias\", name), params=params\n        )", "output": "Return a boolean indicating whether given alias exists.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html>`_\n\n        :arg index: A comma-separated list of index names to filter aliases\n        :arg name: A comma-separated list of alias names to return\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default 'all', valid choices\n            are: 'open', 'closed', 'none', 'all'\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_gecos(name, root=None):\n    '''\n    \n    '''\n    if root is not None and __grains__['kernel'] != 'AIX':\n        getpwnam = functools.partial(_getpwnam, root=root)\n    else:\n        getpwnam = functools.partial(pwd.getpwnam)\n    gecos_field = salt.utils.stringutils.to_unicode(\n        getpwnam(_quote_username(name)).pw_gecos).split(',', 4)\n\n    if not gecos_field:\n        return {}\n    else:\n        # Assign empty strings for any unspecified trailing GECOS fields\n        while len(gecos_field) < 5:\n            gecos_field.append('')\n        return {'fullname': salt.utils.data.decode(gecos_field[0]),\n                'roomnumber': salt.utils.data.decode(gecos_field[1]),\n                'workphone': salt.utils.data.decode(gecos_field[2]),\n                'homephone': salt.utils.data.decode(gecos_field[3]),\n                'other': salt.utils.data.decode(gecos_field[4])}", "output": "Retrieve GECOS field info and return it in dictionary form", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rel2id(self, xs):\n        \"\"\"\n        \"\"\"\n        if isinstance(xs, list):\n            return [self._rel2id[x] for x in xs]\n        return self._rel2id[xs]", "output": "Map relation(s) to id(s)\n\n        Parameters\n        ----------\n        xs : str or list\n            relation\n\n        Returns\n        -------\n        int or list\n            id(s) of relation", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_to_recent(self, project):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        if project not in self.recent_projects:\r\n            self.recent_projects.insert(0, project)\r\n            self.recent_projects = self.recent_projects[:10]", "output": "Add an entry to recent projetcs\r\n\r\n        We only maintain the list of the 10 most recent projects", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _infer_type(obj):\n    \"\"\"\n    \"\"\"\n    if obj is None:\n        return NullType()\n\n    if hasattr(obj, '__UDT__'):\n        return obj.__UDT__\n\n    dataType = _type_mappings.get(type(obj))\n    if dataType is DecimalType:\n        # the precision and scale of `obj` may be different from row to row.\n        return DecimalType(38, 18)\n    elif dataType is not None:\n        return dataType()\n\n    if isinstance(obj, dict):\n        for key, value in obj.items():\n            if key is not None and value is not None:\n                return MapType(_infer_type(key), _infer_type(value), True)\n        return MapType(NullType(), NullType(), True)\n    elif isinstance(obj, list):\n        for v in obj:\n            if v is not None:\n                return ArrayType(_infer_type(obj[0]), True)\n        return ArrayType(NullType(), True)\n    elif isinstance(obj, array):\n        if obj.typecode in _array_type_mappings:\n            return ArrayType(_array_type_mappings[obj.typecode](), False)\n        else:\n            raise TypeError(\"not supported type: array(%s)\" % obj.typecode)\n    else:\n        try:\n            return _infer_schema(obj)\n        except TypeError:\n            raise TypeError(\"not supported type: %s\" % type(obj))", "output": "Infer the DataType from obj", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def disabled(name):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'result': True,\n           'changes': {},\n           'comment': ''}\n\n    stat = __salt__['rdp.status']()\n\n    if stat:\n        if __opts__['test']:\n            ret['result'] = None\n            ret['comment'] = 'RDP will be disabled'\n            return ret\n\n        ret['result'] = __salt__['rdp.disable']()\n        ret['changes'] = {'RDP was disabled': True}\n        return ret\n\n    ret['comment'] = 'RDP is disabled'\n    return ret", "output": "Disable the RDP service", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def symlink_list(self, load):\n        '''\n        \n        '''\n        if 'env' in load:\n            # \"env\" is not supported; Use \"saltenv\".\n            load.pop('env')\n\n        ret = {}\n        if 'saltenv' not in load:\n            return {}\n        if not isinstance(load['saltenv'], six.string_types):\n            load['saltenv'] = six.text_type(load['saltenv'])\n\n        for fsb in self.backends(load.pop('fsbackend', None)):\n            symlstr = '{0}.symlink_list'.format(fsb)\n            if symlstr in self.servers:\n                ret = self.servers[symlstr](load)\n        # some *fs do not handle prefix. Ensure it is filtered\n        prefix = load.get('prefix', '').strip('/')\n        if prefix != '':\n            ret = dict([\n                (x, y) for x, y in six.iteritems(ret) if x.startswith(prefix)\n            ])\n        return ret", "output": "Return a list of symlinked files and dirs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_api_key(self, body, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"PUT\", \"/_security/api_key\", params=params, body=body\n        )", "output": "`<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-create-api-key.html>`_\n\n        :arg body: The api key request to create an API key\n        :arg refresh: If `true` (the default) then refresh the affected shards\n            to make this operation visible to search, if `wait_for` then wait\n            for a refresh to make this operation visible to search, if `false`\n            then do nothing with refreshes., valid choices are: 'true', 'false',\n            'wait_for'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_last_closed_file(self, fname):\r\n        \"\"\"\"\"\"\r\n        if fname in self.last_closed_files:\r\n            self.last_closed_files.remove(fname)\r\n        self.last_closed_files.insert(0, fname)\r\n        if len(self.last_closed_files) > 10:\r\n            self.last_closed_files.pop(-1)", "output": "Add to last closed file list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rescue(device, start, end):\n    '''\n    \n    '''\n    _validate_device(device)\n    _validate_partition_boundary(start)\n    _validate_partition_boundary(end)\n\n    cmd = 'parted -m -s {0} rescue {1} {2}'.format(device, start, end)\n    out = __salt__['cmd.run'](cmd).splitlines()\n    return out", "output": "Rescue a lost partition that was located somewhere between start and end.\n    If a partition is found, parted will ask if you want to create an\n    entry for it in the partition table.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' partition.rescue /dev/sda 0 8056", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def volume_present(name, provider=None, **kwargs):\n    '''\n    \n    '''\n    ret = _check_name(name)\n    if not ret['result']:\n        return ret\n\n    volumes = __salt__['cloud.volume_list'](provider=provider)\n\n    if name in volumes:\n        ret['comment'] = 'Volume exists: {0}'.format(name)\n        ret['result'] = True\n        return ret\n    elif __opts__['test']:\n        ret['comment'] = 'Volume {0} will be created.'.format(name)\n        ret['result'] = None\n        return ret\n\n    response = __salt__['cloud.volume_create'](\n        names=name,\n        provider=provider,\n        **kwargs\n    )\n    if response:\n        ret['result'] = True\n        ret['comment'] = 'Volume {0} was created'.format(name)\n        ret['changes'] = {'old': None, 'new': response}\n    else:\n        ret['result'] = False\n        ret['comment'] = 'Volume {0} failed to create.'.format(name)\n    return ret", "output": "Check that a block volume exists.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def f1_score(prediction, ground_truth):\n    '''\n    \n    '''\n    prediction_tokens = normalize_answer(prediction).split()\n    ground_truth_tokens = normalize_answer(ground_truth).split()\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1_result = (2 * precision * recall) / (precision + recall)\n    return f1_result", "output": "Calculate the f1 score.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parent_directory(self):\r\n        \"\"\"\"\"\"\r\n        self.chdir(os.path.join(getcwd_or_home(), os.path.pardir))", "output": "Change working directory to parent directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reshape_like_all_dims(a, b):\n  \"\"\"\"\"\"\n  ret = tf.reshape(a, tf.shape(b))\n  if not tf.executing_eagerly():\n    ret.set_shape(b.get_shape())\n  return ret", "output": "Reshapes a to match the shape of b.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pbs_for_set_with_merge(document_path, document_data, merge):\n    \"\"\"\n    \"\"\"\n    extractor = DocumentExtractorForMerge(document_data)\n    extractor.apply_merge(merge)\n\n    merge_empty = not document_data\n\n    write_pbs = []\n\n    if extractor.has_updates or merge_empty:\n        write_pbs.append(\n            extractor.get_update_pb(document_path, allow_empty_mask=merge_empty)\n        )\n\n    if extractor.transform_paths:\n        transform_pb = extractor.get_transform_pb(document_path)\n        write_pbs.append(transform_pb)\n\n    return write_pbs", "output": "Make ``Write`` protobufs for ``set()`` methods.\n\n    Args:\n        document_path (str): A fully-qualified document path.\n        document_data (dict): Property names and values to use for\n            replacing a document.\n        merge (Optional[bool] or Optional[List<apispec>]):\n            If True, merge all fields; else, merge only the named fields.\n\n    Returns:\n        List[google.cloud.firestore_v1beta1.types.Write]: One\n        or two ``Write`` protobuf instances for ``set()``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shutdown(name, **kwargs):\n    '''\n    \n    '''\n    ret = {'name': name, 'changes': {}, 'result': True, 'comment': ''}\n    ret['changes'] = __salt__['junos.shutdown'](**kwargs)\n    return ret", "output": "Shuts down the device.\n\n    .. code-block:: yaml\n\n            shut the device:\n              junos:\n                - shutdown\n                - in_min: 10\n\n    Parameters:\n      Optional\n        * kwargs:\n            * reboot:\n              Whether to reboot instead of shutdown. (default=False)\n            * at:\n              Specify time for reboot. (To be used only if reboot=yes)\n            * in_min:\n              Specify delay in minutes for shutdown", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _wait_output(popen, is_slow):\n    \"\"\"\n\n    \"\"\"\n    proc = Process(popen.pid)\n    try:\n        proc.wait(settings.wait_slow_command if is_slow\n                  else settings.wait_command)\n        return True\n    except TimeoutExpired:\n        for child in proc.children(recursive=True):\n            _kill_process(child)\n        _kill_process(proc)\n        return False", "output": "Returns `True` if we can get output of the command in the\n    `settings.wait_command` time.\n\n    Command will be killed if it wasn't finished in the time.\n\n    :type popen: Popen\n    :rtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flatten(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    #Mxnet does not have axis support. By default uses axis=1\n    if 'axis' in attrs and attrs['axis'] != 1:\n        raise RuntimeError(\"Flatten operator only supports axis=1\")\n    new_attrs = translation_utils._remove_attributes(attrs, ['axis'])\n    return 'Flatten', new_attrs, inputs", "output": "Flattens the input array into a 2-D array by collapsing the higher dimensions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def info(cwd,\n         targets=None,\n         user=None,\n         username=None,\n         password=None,\n         fmt='str'):\n    '''\n    \n    '''\n    opts = list()\n    if fmt == 'xml':\n        opts.append('--xml')\n    if targets:\n        opts += salt.utils.args.shlex_split(targets)\n    infos = _run_svn('info', cwd, user, username, password, opts)\n\n    if fmt in ('str', 'xml'):\n        return infos\n\n    info_list = []\n    for infosplit in infos.split('\\n\\n'):\n        info_list.append(_INI_RE.findall(infosplit))\n\n    if fmt == 'list':\n        return info_list\n    if fmt == 'dict':\n        return [dict(tmp) for tmp in info_list]", "output": "Display the Subversion information from the checkout.\n\n    cwd\n        The path to the Subversion repository\n\n    targets : None\n        files, directories, and URLs to pass to the command as arguments\n        svn uses '.' by default\n\n    user : None\n        Run svn as a user other than what the minion runs as\n\n    username : None\n        Connect to the Subversion server as another user\n\n    password : None\n        Connect to the Subversion server with this password\n\n        .. versionadded:: 0.17.0\n\n    fmt : str\n        How to fmt the output from info.\n        (str, xml, list, dict)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' svn.info /path/to/svn/repo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_tornado_coroutine(func):\n    \"\"\"\n    \n    \"\"\"\n    if 'tornado.gen' not in sys.modules:\n        return False\n    gen = sys.modules['tornado.gen']\n    if not hasattr(gen, \"is_coroutine_function\"):\n        # Tornado version is too old\n        return False\n    return gen.is_coroutine_function(func)", "output": "Return whether *func* is a Tornado coroutine function.\n    Running coroutines are not supported.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _error_msg_network(option, expected):\n    '''\n    \n    '''\n    msg = 'Invalid network setting -- Setting: {0}, Expected: [{1}]'\n    return msg.format(option, '|'.join(str(e) for e in expected))", "output": "Build an appropriate error message from a given option and\n    a list of expected values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def status(name):\n    '''\n    \n    '''\n    # Get volume status\n    root = _gluster_xml('volume status {0}'.format(name))\n    if not _gluster_ok(root):\n        # Most probably non-existing volume, the error output is logged\n        # This return value is easy to test and intuitive\n        return None\n\n    ret = {'bricks': {}, 'nfs': {}, 'healers': {}}\n\n    def etree_legacy_wrap(t):\n        ret = _etree_to_dict(t)\n        ret['online'] = (ret['status'] == '1')\n        ret['host'] = ret['hostname']\n        return ret\n\n    # Build a hash to map hostname to peerid\n    hostref = {}\n    for node in _iter(root, 'node'):\n        peerid = node.find('peerid').text\n        hostname = node.find('hostname').text\n        if hostname not in ('NFS Server', 'Self-heal Daemon'):\n            hostref[peerid] = hostname\n\n    for node in _iter(root, 'node'):\n        hostname = node.find('hostname').text\n        if hostname not in ('NFS Server', 'Self-heal Daemon'):\n            path = node.find('path').text\n            ret['bricks'][\n                '{0}:{1}'.format(hostname, path)] = etree_legacy_wrap(node)\n        elif hostname == 'NFS Server':\n            peerid = node.find('peerid').text\n            true_hostname = hostref[peerid]\n            ret['nfs'][true_hostname] = etree_legacy_wrap(node)\n        else:\n            peerid = node.find('peerid').text\n            true_hostname = hostref[peerid]\n            ret['healers'][true_hostname] = etree_legacy_wrap(node)\n\n    return ret", "output": "Check the status of a gluster volume.\n\n    name\n        Volume name\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' glusterfs.status myvolume", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_locations(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_locations function must be called with '\n            '-f or --function, or with the --list-locations option'\n        )\n\n    ret = {}\n    ret['locations'] = []\n\n    try:\n        resconn = get_conn(client_type='resource')\n        provider_query = resconn.providers.get(\n            resource_provider_namespace='Microsoft.Compute'\n        )\n        locations = []\n        for resource in provider_query.resource_types:\n            if six.text_type(resource.resource_type) == 'virtualMachines':\n                resource_dict = resource.as_dict()\n                locations = resource_dict['locations']\n        for location in locations:\n            lowercase = location.lower().replace(' ', '')\n            ret['locations'].append(lowercase)\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('resource', exc.message)\n        ret = {'Error': exc.message}\n\n    return ret", "output": "Return a dict of all available regions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put_abs (self, r, c, ch):\n        ''''''\n\n        r = constrain (r, 1, self.rows)\n        c = constrain (c, 1, self.cols)\n        if isinstance(ch, bytes):\n            ch = self._decode(ch)[0]\n        else:\n            ch = ch[0]\n        self.w[r-1][c-1] = ch", "output": "Screen array starts at 1 index.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_upsample(net, node, module, builder):\n    \"\"\"\n    \"\"\"\n    input_name, output_name = _get_input_output_name(net, node)\n    name = node['name']\n    param = _get_attr(node)\n    inputs = node['inputs']\n    args, _ = module.get_params()\n\n    scale = literal_eval(param['scale'])\n\n    #method\n    if 'sample_type' in param.keys():\n        method = param['sample_type']\n        if method == 'nearest':\n            mode = 'NN'\n        elif method == '':\n            mode = 'BILINEAR'\n\n    builder.add_upsample(name, scaling_factor_h=scale, scaling_factor_w=scale,\n                         input_name=input_name, output_name=output_name, mode=mode)", "output": "Convert a UpSampling layer from mxnet to coreml.\n\n    Parameters\n    ----------\n    net: network\n        A mxnet network object.\n\n    node: layer\n        Node to convert.\n\n    module: module\n        An module for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def debug_file(self):\r\n        \"\"\"\"\"\"\r\n        self.switch_to_plugin()\r\n        current_editor = self.get_current_editor()\r\n        if current_editor is not None:\r\n            current_editor.sig_debug_start.emit()\r\n        self.run_file(debug=True)", "output": "Debug current script", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_request(name=None):\n    '''\n    \n    '''\n    notify_path = os.path.join(__opts__['cachedir'], 'req_state.p')\n    serial = salt.payload.Serial(__opts__)\n    if not os.path.isfile(notify_path):\n        return True\n    if not name:\n        try:\n            os.remove(notify_path)\n        except (IOError, OSError):\n            pass\n    else:\n        req = check_request()\n        if name in req:\n            req.pop(name)\n        else:\n            return False\n        with salt.utils.files.set_umask(0o077):\n            try:\n                if salt.utils.platform.is_windows():\n                    # Make sure cache file isn't read-only\n                    __salt__['cmd.run']('attrib -R \"{0}\"'.format(notify_path))\n                with salt.utils.files.fopen(notify_path, 'w+b') as fp_:\n                    serial.dump(req, fp_)\n            except (IOError, OSError):\n                log.error(\n                    'Unable to write state request file %s. Check permission.',\n                    notify_path\n                )\n    return True", "output": ".. versionadded:: 2017.7.3\n\n    Clear out the state execution request without executing it\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.clear_request", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _at_least_x_are_true(a, b, x):\n  \"\"\"\"\"\"\n  match = tf.equal(a, b)\n  match = tf.cast(match, tf.int32)\n  return tf.greater_equal(tf.reduce_sum(match), x)", "output": "At least `x` of `a` and `b` `Tensors` are true.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def levenshtein_distance(self, a, b):\n        '''\n        '''\n\n        n, m = len(a), len(b)\n        if n > m:\n            a,b = b,a\n            n,m = m,n\n        current = range(n+1)\n        for i in range(1,m+1):\n            previous, current = current, [i]+[0]*n\n            for j in range(1,n+1):\n                add, delete = previous[j]+1, current[j-1]+1\n                change = previous[j-1]\n                if a[j-1] != b[i-1]:\n                    change = change + 1\n                current[j] = min(add, delete, change)\n        return current[n]", "output": "This calculates the Levenshtein distance between a and b.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_pretty_string(obj):\n    \"\"\"\n    \"\"\"\n    sio = StringIO()\n    pprint.pprint(obj, stream=sio)\n    return sio.getvalue()", "output": "Return a prettier version of obj\n\n    Parameters\n    ----------\n    obj : object\n        Object to pretty print\n\n    Returns\n    -------\n    s : str\n        Pretty print object repr", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def readdir(path):\n    '''\n    \n    '''\n    path = os.path.expanduser(path)\n\n    if not os.path.isabs(path):\n        raise SaltInvocationError('Dir path must be absolute.')\n\n    if not os.path.isdir(path):\n        raise SaltInvocationError('A valid directory was not specified.')\n\n    dirents = ['.', '..']\n    dirents.extend(os.listdir(path))\n    return dirents", "output": ".. versionadded:: 2014.1.0\n\n    Return a list containing the contents of a directory\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.readdir /path/to/dir/", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_cronjob(self):\n        \"\"\"\"\"\"\n        now = time.time()\n        self._last_tick = int(self._last_tick)\n        if now - self._last_tick < 1:\n            return False\n        self._last_tick += 1\n        for project in itervalues(self.projects):\n            if not project.active:\n                continue\n            if project.waiting_get_info:\n                continue\n            if int(project.min_tick) == 0:\n                continue\n            if self._last_tick % int(project.min_tick) != 0:\n                continue\n            self.on_select_task({\n                'taskid': '_on_cronjob',\n                'project': project.name,\n                'url': 'data:,_on_cronjob',\n                'status': self.taskdb.SUCCESS,\n                'fetch': {\n                    'save': {\n                        'tick': self._last_tick,\n                    },\n                },\n                'process': {\n                    'callback': '_on_cronjob',\n                },\n            })\n        return True", "output": "Check projects cronjob tick, return True when a new tick is sended", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _has_ipv6(host):\n    \"\"\"  \"\"\"\n    sock = None\n    has_ipv6 = False\n\n    # App Engine doesn't support IPV6 sockets and actually has a quota on the\n    # number of sockets that can be used, so just early out here instead of\n    # creating a socket needlessly.\n    # See https://github.com/urllib3/urllib3/issues/1446\n    if _appengine_environ.is_appengine_sandbox():\n        return False\n\n    if socket.has_ipv6:\n        # has_ipv6 returns true if cPython was compiled with IPv6 support.\n        # It does not tell us if the system has IPv6 support enabled. To\n        # determine that we must bind to an IPv6 address.\n        # https://github.com/shazow/urllib3/pull/611\n        # https://bugs.python.org/issue658327\n        try:\n            sock = socket.socket(socket.AF_INET6)\n            sock.bind((host, 0))\n            has_ipv6 = True\n        except Exception:\n            pass\n\n    if sock:\n        sock.close()\n    return has_ipv6", "output": "Returns True if the system can bind an IPv6 address.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def json(self, *, loads: JSONDecoder=DEFAULT_JSON_DECODER) -> Any:\n        \"\"\"\"\"\"\n        body = await self.text()\n        return loads(body)", "output": "Return BODY as JSON.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_namespace(namespace, apiserver_url):\n    '''  '''\n    # Prepare URL\n    url = \"{0}/api/v1/namespaces\".format(apiserver_url)\n    # Prepare data\n    data = {\n        \"kind\": \"Namespace\",\n        \"apiVersion\": \"v1\",\n        \"metadata\": {\n            \"name\": namespace,\n        }\n    }\n    log.trace(\"namespace creation requests: %s\", data)\n    # Make request\n    ret = _kpost(url, data)\n    log.trace(\"result is: %s\", ret)\n    # Check requests status\n    return ret", "output": "create namespace on the defined k8s cluster", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_items(self):\r\n        \"\"\"\"\"\"\r\n        itemlist = []\r\n        def add_to_itemlist(item):\r\n            for index in range(item.childCount()):\r\n                citem = item.child(index)\r\n                itemlist.append(citem)\r\n                add_to_itemlist(citem)\r\n        for tlitem in self.get_top_level_items():\r\n            add_to_itemlist(tlitem)\r\n        return itemlist", "output": "Return items (excluding top level items)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_job_details(self, job_id):\n        \"\"\"\n        \n        \"\"\"\n        response = requests.get(self._get_job_details_url(job_id))\n\n        response.raise_for_status()\n\n        return response", "output": "Gets all details for existing job\n\n        :param job_id: job_id as returned by 'create_operation_job(...)'\n        :return: job info as xml", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def release(self) -> None:\n        \"\"\"\"\"\"\n        self._value += 1\n        while self._waiters:\n            waiter = self._waiters.popleft()\n            if not waiter.done():\n                self._value -= 1\n\n                # If the waiter is a coroutine paused at\n                #\n                #     with (yield semaphore.acquire()):\n                #\n                # then the context manager's __exit__ calls release() at the end\n                # of the \"with\" block.\n                waiter.set_result(_ReleasingContextManager(self))\n                break", "output": "Increment the counter and wake one waiter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_to_side(cls, side_spec):\n        \"\"\"\n        \n        \"\"\"\n\n        from openpyxl.styles import Side\n\n        _side_key_map = {\n            'border_style': 'style',\n        }\n\n        if isinstance(side_spec, str):\n            return Side(style=side_spec)\n\n        side_kwargs = {}\n        for k, v in side_spec.items():\n            if k in _side_key_map:\n                k = _side_key_map[k]\n            if k == 'color':\n                v = cls._convert_to_color(v)\n            side_kwargs[k] = v\n\n        return Side(**side_kwargs)", "output": "Convert ``side_spec`` to an openpyxl v2 Side object\n        Parameters\n        ----------\n        side_spec : str, dict\n            A string specifying the border style, or a dict with zero or more\n            of the following keys (or their synonyms).\n                'style' ('border_style')\n                'color'\n        Returns\n        -------\n        side : openpyxl.styles.Side", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show(self, ax:plt.Axes=None, figsize:tuple=(3,3), title:Optional[str]=None, hide_axis:bool=True, **kwargs):\n        \"\"\n        if ax is None: _,ax = plt.subplots(figsize=figsize)\n        pnt = scale_flow(FlowField(self.size, self.data), to_unit=False).flow.flip(1)\n        params = {'s': 10, 'marker': '.', 'c': 'r', **kwargs}\n        ax.scatter(pnt[:, 0], pnt[:, 1], **params)\n        if hide_axis: ax.axis('off')\n        if title: ax.set_title(title)", "output": "Show the `ImagePoints` on `ax`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def event_return(events):\n    '''\n    \n    '''\n    options = _get_options()\n\n    index = options['master_event_index']\n    doc_type = options['master_event_doc_type']\n\n    if options['index_date']:\n        index = '{0}-{1}'.format(index,\n            datetime.date.today().strftime('%Y.%m.%d'))\n\n    _ensure_index(index)\n\n    for event in events:\n        data = {\n            'tag': event.get('tag', ''),\n            'data': event.get('data', '')\n        }\n\n    ret = __salt__['elasticsearch.document_create'](index=index,\n                                                    doc_type=doc_type,\n                                                    id=uuid.uuid4(),\n                                                    body=salt.utils.json.dumps(data))", "output": "Return events to Elasticsearch\n\n    Requires that the `event_return` configuration be set in master config.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clean(config=None, path=None, saltenv='base'):\n    '''\n    \n    '''\n    config_tree = tree(config=config, path=path, saltenv=saltenv)\n    return _print_config_text(config_tree)", "output": "Return a clean version of the config, without any special signs (such as\n    ``!`` as an individual line) or empty lines, but just lines with significant\n    value in the configuration of the network device.\n\n    config\n        The configuration sent as text. This argument is ignored when ``path``\n        is configured.\n\n    path\n        Absolute or remote path from where to load the configuration text. This\n        argument allows any URI supported by\n        :py:func:`cp.get_url <salt.modules.cp.get_url>`), e.g., ``salt://``,\n        ``https://``, ``s3://``, ``ftp:/``, etc.\n\n    saltenv: ``base``\n        Salt fileserver environment from which to retrieve the file.\n        Ignored if ``path`` is not a ``salt://`` URL.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' iosconfig.clean path=salt://path/to/my/config.txt\n        salt '*' iosconfig.clean path=https://bit.ly/2mAdq7z", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def writeline(self, x, node=None, extra=0):\n        \"\"\"\"\"\"\n        self.newline(node, extra)\n        self.write(x)", "output": "Combination of newline and write.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_snapshot(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The show_snapshot function must be called with -f or --function.'\n        )\n    if not kwargs or 'name' not in kwargs:\n        log.error(\n            'Must specify name.'\n        )\n        return False\n\n    conn = get_conn()\n    return _expand_item(conn.ex_get_snapshot(kwargs['name']))", "output": "Show the details of an existing snapshot.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f show_snapshot gce name=mysnapshot", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _python_installed(ret, python, user=None):\n    '''\n    \n    '''\n    default = __salt__['pyenv.default'](runas=user)\n    for version in __salt__['pyenv.versions'](user):\n        if version == python:\n            ret['result'] = True\n            ret['comment'] = 'Requested python exists.'\n            ret['default'] = default == python\n            break\n\n    return ret", "output": "Check to see if given python is installed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _item_to_blob(iterator, item):\n    \"\"\"\n    \"\"\"\n    name = item.get(\"name\")\n    blob = Blob(name, bucket=iterator.bucket)\n    blob._set_properties(item)\n    return blob", "output": "Convert a JSON blob to the native object.\n\n    .. note::\n\n        This assumes that the ``bucket`` attribute has been\n        added to the iterator after being created.\n\n    :type iterator: :class:`~google.api_core.page_iterator.Iterator`\n    :param iterator: The iterator that has retrieved the item.\n\n    :type item: dict\n    :param item: An item to be converted to a blob.\n\n    :rtype: :class:`.Blob`\n    :returns: The next blob in the page.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def info(name):\n    '''\n    \n    '''\n    cert_file = _cert_file(name, 'cert')\n    # Use the salt module if available\n    if 'tls.cert_info' in __salt__:\n        cert_info = __salt__['tls.cert_info'](cert_file)\n        # Strip out the extensions object contents;\n        # these trip over our poor state output\n        # and they serve no real purpose here anyway\n        cert_info['extensions'] = cert_info['extensions'].keys()\n        return cert_info\n    # Cobble it together using the openssl binary\n    openssl_cmd = 'openssl x509 -in {0} -noout -text'.format(cert_file)\n    return __salt__['cmd.run'](openssl_cmd, output_loglevel='quiet')", "output": "Return information about a certificate\n\n    .. note::\n        Will output tls.cert_info if that's available, or OpenSSL text if not\n\n    :param name: CommonName of cert\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt 'gitlab.example.com' acme.info dev.example.com", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dictionary(self):\n        # type: () -> Dict[str, Any]\n        \"\"\"\n        \"\"\"\n        # NOTE: Dictionaries are not populated if not loaded. So, conditionals\n        #       are not needed here.\n        retval = {}\n\n        for variant in self._override_order:\n            retval.update(self._config[variant])\n\n        return retval", "output": "A dictionary representing the loaded configuration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _extract_axes(self, data, axes, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        return [self._extract_axis(self, data, axis=i, **kwargs)\n                for i, a in enumerate(axes)]", "output": "Return a list of the axis indices.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_and_box_cache(arg, cache_array, box, errors, name=None):\n    \"\"\"\n    \n    \"\"\"\n    from pandas import Series, DatetimeIndex, Index\n    result = Series(arg).map(cache_array)\n    if box:\n        if errors == 'ignore':\n            return Index(result, name=name)\n        else:\n            return DatetimeIndex(result, name=name)\n    return result.values", "output": "Convert array of dates with a cache and box the result\n\n    Parameters\n    ----------\n    arg : integer, float, string, datetime, list, tuple, 1-d array, Series\n    cache_array : Series\n        Cache of converted, unique dates\n    box : boolean\n        True boxes result as an Index-like, False returns an ndarray\n    errors : string\n        'ignore' plus box=True will convert result to Index\n    name : string, default None\n        Name for a DatetimeIndex\n\n    Returns\n    -------\n    result : datetime of converted dates\n        Returns:\n\n        - Index-like if box=True\n        - ndarray if box=False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_namespaced_event(self, namespace, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_event_with_http_info(namespace, body, **kwargs)\n        else:\n            (data) = self.create_namespaced_event_with_http_info(namespace, body, **kwargs)\n            return data", "output": "create an Event\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_event(namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1Event body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1Event\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_dividend_data(self, dividends, stock_dividends=None):\n        \"\"\"\n        \n        \"\"\"\n\n        # First write the dividend payouts.\n        self._write_dividends(dividends)\n        self._write_stock_dividends(stock_dividends)\n\n        # Second from the dividend payouts, calculate ratios.\n        dividend_ratios = self.calc_dividend_ratios(dividends)\n        self.write_frame('dividends', dividend_ratios)", "output": "Write both dividend payouts and the derived price adjustment ratios.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(name, gid=None, **kwargs):\n    '''\n    \n    '''\n    ### NOTE: **kwargs isn't used here but needs to be included in this\n    ### function for compatibility with the group.present state\n    if info(name):\n        raise CommandExecutionError(\n            'Group \\'{0}\\' already exists'.format(name)\n        )\n    if salt.utils.stringutils.contains_whitespace(name):\n        raise SaltInvocationError('Group name cannot contain whitespace')\n    if name.startswith('_'):\n        raise SaltInvocationError(\n            'Salt will not create groups beginning with underscores'\n        )\n    if gid is not None and not isinstance(gid, int):\n        raise SaltInvocationError('gid must be an integer')\n    # check if gid is already in use\n    gid_list = _list_gids()\n    if six.text_type(gid) in gid_list:\n        raise CommandExecutionError(\n            'gid \\'{0}\\' already exists'.format(gid)\n        )\n\n    cmd = ['dseditgroup', '-o', 'create']\n    if gid:\n        cmd.extend(['-i', gid])\n    cmd.append(name)\n    return __salt__['cmd.retcode'](cmd, python_shell=False) == 0", "output": "Add the specified group\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' group.add foo 3456", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def job_runner(self):\n        # We recommend that you define a subclass, override this method and set up your own config\n        \"\"\"\n        \n        \"\"\"\n        outputs = luigi.task.flatten(self.output())\n        for output in outputs:\n            if not isinstance(output, luigi.contrib.hdfs.HdfsTarget):\n                warnings.warn(\"Job is using one or more non-HdfsTarget outputs\" +\n                              \" so it will be run in local mode\")\n                return LocalJobRunner()\n        else:\n            return DefaultHadoopJobRunner()", "output": "Get the MapReduce runner for this job.\n\n        If all outputs are HdfsTargets, the DefaultHadoopJobRunner will be used.\n        Otherwise, the LocalJobRunner which streams all data through the local machine\n        will be used (great for testing).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show(config_file=False):\n    '''\n    \n    '''\n    ret = {}\n    if config_file:\n        # If the file doesn't exist, return an empty list\n        if not os.path.exists(config_file):\n            return []\n\n        try:\n            with salt.utils.files.fopen(config_file) as fp_:\n                for line in fp_:\n                    line = salt.utils.stringutils.to_str(line)\n                    if not line.startswith('#') and '=' in line:\n                        # search if we have some '=' instead of ' = ' separators\n                        SPLIT = ' = '\n                        if SPLIT not in line:\n                            SPLIT = SPLIT.strip()\n                        key, value = line.split(SPLIT, 1)\n                        key = key.strip()\n                        value = value.lstrip()\n                        ret[key] = value\n        except (OSError, IOError):\n            log.error('Could not open sysctl file')\n            return None\n    else:\n        cmd = 'sysctl -a'\n        out = __salt__['cmd.run_stdout'](cmd, output_loglevel='trace')\n        for line in out.splitlines():\n            if not line or ' = ' not in line:\n                continue\n            comps = line.split(' = ', 1)\n            ret[comps[0]] = comps[1]\n    return ret", "output": "Return a list of sysctl parameters for this minion\n\n    config: Pull the data from the system configuration file\n        instead of the live data.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sysctl.show", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def execute(self, ignore_cache=False):\n        \"\"\"\n        \n        \"\"\"\n        if ignore_cache or not hasattr(self, '_response'):\n            es = connections.get_connection(self._using)\n\n            self._response = self._response_class(\n                self,\n                es.search(\n                    index=self._index,\n                    body=self.to_dict(),\n                    **self._params\n                )\n            )\n        return self._response", "output": "Execute the search and return an instance of ``Response`` wrapping all\n        the data.\n\n        :arg ignore_cache: if set to ``True``, consecutive calls will hit\n            ES, while cached result will be ignored. Defaults to `False`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _interface_configs():\n    '''\n    \n    '''\n    cmd = ['netsh', 'interface', 'ip', 'show', 'config']\n    lines = __salt__['cmd.run'](cmd, python_shell=False).splitlines()\n    ret = {}\n    current_iface = None\n    current_ip_list = None\n\n    for line in lines:\n\n        line = line.strip()\n        if not line:\n            current_iface = None\n            current_ip_list = None\n            continue\n\n        if 'Configuration for interface' in line:\n            _, iface = line.rstrip('\"').split('\"', 1)  # get iface name\n            current_iface = {}\n            ret[iface] = current_iface\n            continue\n\n        if ':' not in line:\n            if current_ip_list:\n                current_ip_list.append(line)\n            else:\n                log.warning('Cannot parse \"%s\"', line)\n            continue\n\n        key, val = line.split(':', 1)\n        key = key.strip()\n        val = val.strip()\n\n        lkey = key.lower()\n        if ('dns servers' in lkey) or ('wins servers' in lkey):\n            current_ip_list = []\n            current_iface[key] = current_ip_list\n            current_ip_list.append(val)\n\n        elif 'ip address' in lkey:\n            current_iface.setdefault('ip_addrs', []).append({key: val})\n\n        elif 'subnet prefix' in lkey:\n            subnet, _, netmask = val.split(' ', 2)\n            last_ip = current_iface['ip_addrs'][-1]\n            last_ip['Subnet'] = subnet.strip()\n            last_ip['Netmask'] = netmask.lstrip().rstrip(')')\n\n        else:\n            current_iface[key] = val\n\n    return ret", "output": "Return all interface configs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def newest_packages(\n        pypi_server=\"https://pypi.python.org/pypi?%3Aaction=packages_rss\"):\n    \"\"\"\n    \n    \"\"\"\n    items = _get(pypi_server)\n    i = []\n    for item in items:\n        i_dict = {'name': item[0].text.split()[0],\n                  'url': item[1].text,\n                  'description': item[3].text,\n                  'date': item[4].text}\n        i.append(Package(i_dict))\n    return i", "output": "Constructs a request to the PyPI server and returns a list of\n    :class:`yarg.parse.Package`.\n\n    :param pypi_server: (option) URL to the PyPI server.\n\n        >>> import yarg\n        >>> yarg.newest_packages()\n        [<Package yarg>, <Package gray>, <Package ragy>]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_cibfile(cibname):\n    '''\n    \n    '''\n    cibfile = os.path.join(_get_cibpath(), '{0}.{1}'.format(cibname, 'cib'))\n    log.trace('cibfile: %s', cibfile)\n    return cibfile", "output": "Get the full path of a cached CIB-file with the name of the CIB", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def showEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        QWidget.showEvent(self, event)\r\n        self.spinner.start()", "output": "Override show event to start waiting spinner.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_or_guess_labels(self, x, kwargs):\n    \"\"\"\n    \n    \"\"\"\n    if 'y' in kwargs and 'y_target' in kwargs:\n      raise ValueError(\"Can not set both 'y' and 'y_target'.\")\n    elif 'y' in kwargs:\n      labels = kwargs['y']\n    elif 'y_target' in kwargs and kwargs['y_target'] is not None:\n      labels = kwargs['y_target']\n    else:\n      preds = self.model.get_probs(x)\n      preds_max = reduce_max(preds, 1, keepdims=True)\n      original_predictions = tf.to_float(tf.equal(preds, preds_max))\n      labels = tf.stop_gradient(original_predictions)\n      del preds\n    if isinstance(labels, np.ndarray):\n      nb_classes = labels.shape[1]\n    else:\n      nb_classes = labels.get_shape().as_list()[1]\n    return labels, nb_classes", "output": "Get the label to use in generating an adversarial example for x.\n    The kwargs are fed directly from the kwargs of the attack.\n    If 'y' is in kwargs, then assume it's an untargeted attack and\n    use that as the label.\n    If 'y_target' is in kwargs and is not none, then assume it's a\n    targeted attack and use that as the label.\n    Otherwise, use the model's prediction as the label and perform an\n    untargeted attack.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def max_heap_sort(arr, simulation=False):\n    \"\"\" \n    \"\"\"\n    iteration = 0\n    if simulation:\n        print(\"iteration\",iteration,\":\",*arr)\n        \n    for i in range(len(arr) - 1, 0, -1):\n        iteration = max_heapify(arr, i, simulation, iteration)\n\n    if simulation:\n                iteration = iteration + 1\n                print(\"iteration\",iteration,\":\",*arr)\n    return arr", "output": "Heap Sort that uses a max heap to sort an array in ascending order\n        Complexity: O(n log(n))", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def readlines(self, sizehint=None):\n        \"\"\"\n        \n        \"\"\"\n        lines = []\n        byte_count = 0\n        while True:\n            line = self.readline()\n            if len(line) == 0:\n                break\n            lines.append(line)\n            byte_count += len(line)\n            if (sizehint is not None) and (byte_count >= sizehint):\n                break\n        return lines", "output": "Read all remaining lines using `readline` and return them as a list.\n        If the optional ``sizehint`` argument is present, instead of reading up\n        to EOF, whole lines totalling approximately sizehint bytes (possibly\n        after rounding up to an internal buffer size) are read.\n\n        :param int sizehint: desired maximum number of bytes to read.\n        :returns: list of lines read from the file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_host(name=None, ipv4addr=None, mac=None, return_fields=None, **api_opts):\n    '''\n    \n    '''\n    infoblox = _get_infoblox(**api_opts)\n    host = infoblox.get_host(name=name, mac=mac, ipv4addr=ipv4addr, return_fields=return_fields)\n    return host", "output": "Get host information\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-call infoblox.get_host hostname.domain.ca\n        salt-call infoblox.get_host ipv4addr=123.123.122.12\n        salt-call infoblox.get_host mac=00:50:56:84:6e:ae", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list():\n  \"\"\"\n  \"\"\"\n  infos = manager.get_all()\n  if not infos:\n    print(\"No known TensorBoard instances running.\")\n    return\n\n  print(\"Known TensorBoard instances:\")\n  for info in infos:\n    template = \"  - port {port}: {data_source} (started {delta} ago; pid {pid})\"\n    print(template.format(\n        port=info.port,\n        data_source=manager.data_source_from_info(info),\n        delta=_time_delta_from_info(info),\n        pid=info.pid,\n    ))", "output": "Print a listing of known running TensorBoard instances.\n\n  TensorBoard instances that were killed uncleanly (e.g., with SIGKILL\n  or SIGQUIT) may appear in this list even if they are no longer\n  running. Conversely, this list may be missing some entries if your\n  operating system's temporary directory has been cleared since a\n  still-running TensorBoard instance started.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_proc_status(proc):\n    '''\n    \n    '''\n    try:\n        return salt.utils.data.decode(proc.status() if PSUTIL2 else proc.status)\n    except (psutil.NoSuchProcess, psutil.AccessDenied):\n        return None", "output": "Returns the status of a Process instance.\n\n    It's backward compatible with < 2.0 versions of psutil.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy(self):\r\n        \"\"\"\"\"\"\r\n        if not self.selectedIndexes():\r\n            return\r\n        (row_min, row_max,\r\n         col_min, col_max) = get_idx_rect(self.selectedIndexes())\r\n        index = header = False\r\n        df = self.model().df\r\n        obj = df.iloc[slice(row_min, row_max + 1),\r\n                      slice(col_min, col_max + 1)]\r\n        output = io.StringIO()\r\n        obj.to_csv(output, sep='\\t', index=index, header=header)\r\n        if not PY2:\r\n            contents = output.getvalue()\r\n        else:\r\n            contents = output.getvalue().decode('utf-8')\r\n        output.close()\r\n        clipboard = QApplication.clipboard()\r\n        clipboard.setText(contents)", "output": "Copy text to clipboard", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setContextDoc(self, doc):\n        \"\"\" \"\"\"\n        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        libxml2mod.xmlXPathSetContextDoc(self._o, doc__o)", "output": "Set the doc of an xpathContext", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def allowed_transitions(constraint_type: str, labels: Dict[int, str]) -> List[Tuple[int, int]]:\n    \"\"\"\n    \n    \"\"\"\n    num_labels = len(labels)\n    start_tag = num_labels\n    end_tag = num_labels + 1\n    labels_with_boundaries = list(labels.items()) + [(start_tag, \"START\"), (end_tag, \"END\")]\n\n    allowed = []\n    for from_label_index, from_label in labels_with_boundaries:\n        if from_label in (\"START\", \"END\"):\n            from_tag = from_label\n            from_entity = \"\"\n        else:\n            from_tag = from_label[0]\n            from_entity = from_label[1:]\n        for to_label_index, to_label in labels_with_boundaries:\n            if to_label in (\"START\", \"END\"):\n                to_tag = to_label\n                to_entity = \"\"\n            else:\n                to_tag = to_label[0]\n                to_entity = to_label[1:]\n            if is_transition_allowed(constraint_type, from_tag, from_entity,\n                                     to_tag, to_entity):\n                allowed.append((from_label_index, to_label_index))\n    return allowed", "output": "Given labels and a constraint type, returns the allowed transitions. It will\n    additionally include transitions for the start and end states, which are used\n    by the conditional random field.\n\n    Parameters\n    ----------\n    constraint_type : ``str``, required\n        Indicates which constraint to apply. Current choices are\n        \"BIO\", \"IOB1\", \"BIOUL\", and \"BMES\".\n    labels : ``Dict[int, str]``, required\n        A mapping {label_id -> label}. Most commonly this would be the value from\n        Vocabulary.get_index_to_token_vocabulary()\n\n    Returns\n    -------\n    ``List[Tuple[int, int]]``\n        The allowed transitions (from_label_id, to_label_id).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deployment_absent(name, namespace='default', **kwargs):\n    '''\n    \n    '''\n\n    ret = {'name': name,\n           'changes': {},\n           'result': False,\n           'comment': ''}\n\n    deployment = __salt__['kubernetes.show_deployment'](name, namespace, **kwargs)\n\n    if deployment is None:\n        ret['result'] = True if not __opts__['test'] else None\n        ret['comment'] = 'The deployment does not exist'\n        return ret\n\n    if __opts__['test']:\n        ret['comment'] = 'The deployment is going to be deleted'\n        ret['result'] = None\n        return ret\n\n    res = __salt__['kubernetes.delete_deployment'](name, namespace, **kwargs)\n    if res['code'] == 200:\n        ret['result'] = True\n        ret['changes'] = {\n            'kubernetes.deployment': {\n                'new': 'absent', 'old': 'present'}}\n        ret['comment'] = res['message']\n    else:\n        ret['comment'] = 'Something went wrong, response: {0}'.format(res)\n\n    return ret", "output": "Ensures that the named deployment is absent from the given namespace.\n\n    name\n        The name of the deployment\n\n    namespace\n        The name of the namespace", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def route_table_create_or_update(name, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    if 'location' not in kwargs:\n        rg_props = __salt__['azurearm_resource.resource_group_get'](\n            resource_group, **kwargs\n        )\n\n        if 'error' in rg_props:\n            log.error(\n                'Unable to determine location from resource group specified.'\n            )\n            return False\n        kwargs['location'] = rg_props['location']\n\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n\n    try:\n        rt_tbl_model = __utils__['azurearm.create_object_model']('network', 'RouteTable', **kwargs)\n    except TypeError as exc:\n        result = {'error': 'The object model could not be built. ({0})'.format(str(exc))}\n        return result\n\n    try:\n        table = netconn.route_tables.create_or_update(\n            resource_group_name=resource_group,\n            route_table_name=name,\n            parameters=rt_tbl_model\n        )\n        table.wait()\n        tbl_result = table.result()\n        result = tbl_result.as_dict()\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n    except SerializationError as exc:\n        result = {'error': 'The object model could not be parsed. ({0})'.format(str(exc))}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Create or update a route table within a specified resource group.\n\n    :param name: The name of the route table to create.\n\n    :param resource_group: The resource group name assigned to the\n        route table.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.route_table_create_or_update test-rt-table testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_character(char):\n    \"\"\"\"\"\"\n    if \\\n        char in string.ascii_letters \\\n        or char in string.digits \\\n        or char in [\n                '_', '.', ':', ';', ' ', '!', '?', '+', '-', '/', '=', '<',\n                '>', '$', '(', ')', '@', '~', '`', '|', '#', '[', ']', '{',\n                '}', '&', '*', '^', '%']:\n        return char\n    elif char in ['\"', '\\'', '\\\\']:\n        return '\\\\{0}'.format(char)\n    elif char == '\\n':\n        return '\\\\n'\n    elif char == '\\r':\n        return '\\\\r'\n    elif char == '\\t':\n        return '\\\\t'\n    else:\n        return '\\\\x{:02x}'.format(ord(char))", "output": "Returns the C-formatting of the character", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def version_parts(self, best=False):\n        \"\"\"\n        \n        \"\"\"\n        version_str = self.version(best=best)\n        if version_str:\n            version_regex = re.compile(r'(\\d+)\\.?(\\d+)?\\.?(\\d+)?')\n            matches = version_regex.match(version_str)\n            if matches:\n                major, minor, build_number = matches.groups()\n                return major, minor or '', build_number or ''\n        return '', '', ''", "output": "Return the version of the OS distribution, as a tuple of version\n        numbers.\n\n        For details, see :func:`distro.version_parts`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _grains(host, user, password):\n    '''\n    \n    '''\n    r = __salt__['dracr.system_info'](host=host,\n                                      admin_username=user,\n                                      admin_password=password)\n    if r.get('retcode', 0) == 0:\n        GRAINS_CACHE = r\n    else:\n        GRAINS_CACHE = {}\n    return GRAINS_CACHE", "output": "Get the grains from the proxied device", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find(self, name, current_location):\n        \"\"\"\"\"\"\n        assert isinstance(name, basestring)\n        assert isinstance(current_location, basestring)\n\n        project_module = None\n\n        # Try interpreting name as project id.\n        if name[0] == '/':\n            project_module = self.id2module.get(name)\n\n        if not project_module:\n            location = os.path.join(current_location, name)\n            # If no project is registered for the given location, try to\n            # load it. First see if we have Jamfile. If not we might have project\n            # root, willing to act as Jamfile. In that case, project-root\n            # must be placed in the directory referred by id.\n\n            project_module = self.module_name(location)\n            if not project_module in self.jamfile_modules:\n                if b2.util.path.glob([location], self.JAMROOT + self.JAMFILE):\n                    project_module = self.load(location)\n                else:\n                    project_module = None\n\n        return project_module", "output": "Given 'name' which can be project-id or plain directory name,\n        return project module corresponding to that id or directory.\n        Returns nothing of project is not found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_locale():\n    '''\n    \n    '''\n    ret = ''\n    lc_ctl = salt.utils.systemd.booted(__context__)\n    # localectl on SLE12 is installed but the integration is still broken in latest SP3 due to\n    # config is rewritten by by many %post installation hooks in the older packages.\n    # If you use it -- you will break your config. This is not the case in SLE15 anymore.\n    if lc_ctl and not (__grains__['os_family'] in ['Suse'] and __grains__['osmajorrelease'] in [12]):\n        ret = (_parse_dbus_locale() if dbus is not None else _localectl_status()['system_locale']).get('LANG', '')\n    else:\n        if 'Suse' in __grains__['os_family']:\n            cmd = 'grep \"^RC_LANG\" /etc/sysconfig/language'\n        elif 'RedHat' in __grains__['os_family']:\n            cmd = 'grep \"^LANG=\" /etc/sysconfig/i18n'\n        elif 'Debian' in __grains__['os_family']:\n            # this block only applies to Debian without systemd\n            cmd = 'grep \"^LANG=\" /etc/default/locale'\n        elif 'Gentoo' in __grains__['os_family']:\n            cmd = 'eselect --brief locale show'\n            return __salt__['cmd.run'](cmd).strip()\n        elif 'Solaris' in __grains__['os_family']:\n            cmd = 'grep \"^LANG=\" /etc/default/init'\n        else:  # don't waste time on a failing cmd.run\n            raise CommandExecutionError('Error: \"{0}\" is unsupported!'.format(__grains__['oscodename']))\n\n        if cmd:\n            try:\n                ret = __salt__['cmd.run'](cmd).split('=')[1].replace('\"', '')\n            except IndexError as err:\n                log.error('Error occurred while running \"%s\": %s', cmd, err)\n\n    return ret", "output": "Get the current system locale\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' locale.get_locale", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send_sms(profile, body, to, from_):\n    '''\n    \n    '''\n    ret = {}\n    ret['message'] = {}\n    ret['message']['sid'] = None\n    client = _get_twilio(profile)\n    try:\n        if TWILIO_5:\n            message = client.sms.messages.create(body=body, to=to, from_=from_)\n        else:\n            message = client.messages.create(body=body, to=to, from_=from_)\n    except TwilioRestException as exc:\n        ret['_error'] = {}\n        ret['_error']['code'] = exc.code\n        ret['_error']['msg'] = exc.msg\n        ret['_error']['status'] = exc.status\n        log.debug('Could not send sms. Error: %s', ret)\n        return ret\n    ret['message'] = {}\n    ret['message']['sid'] = message.sid\n    ret['message']['price'] = message.price\n    ret['message']['price_unit'] = message.price_unit\n    ret['message']['status'] = message.status\n    ret['message']['num_segments'] = message.num_segments\n    ret['message']['body'] = message.body\n    ret['message']['date_sent'] = six.text_type(message.date_sent)\n    ret['message']['date_created'] = six.text_type(message.date_created)\n    log.info(ret)\n    return ret", "output": "Send an sms\n\n    CLI Example:\n\n        twilio.send_sms twilio-account 'Test sms' '+18019999999' '+18011111111'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _format_native_types(self, na_rep='', quoting=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        mask = isna(self)\n        if not self.is_object() and not quoting:\n            values = np.asarray(self).astype(str)\n        else:\n            values = np.array(self, dtype=object, copy=True)\n\n        values[mask] = na_rep\n        return values", "output": "Actually format specific types of the index.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_function_name(integration_uri):\n        \"\"\"\n        \n        \"\"\"\n\n        arn = LambdaUri._get_function_arn(integration_uri)\n\n        LOG.debug(\"Extracted Function ARN: %s\", arn)\n\n        return LambdaUri._get_function_name_from_arn(arn)", "output": "Gets the name of the function from the Integration URI ARN. This is a best effort service which returns None\n        if function name could not be parsed. This can happen when the ARN is an intrinsic function which is too\n        complex or the ARN is not a Lambda integration.\n\n        Parameters\n        ----------\n        integration_uri : basestring or dict\n            Integration URI data extracted from Swagger dictionary. This could be a string of the ARN or an intrinsic\n            function that will resolve to the ARN\n\n        Returns\n        -------\n        basestring or None\n            If the function name could be parsed out of the Integration URI ARN. None, otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_cell_parts(cls, cell_text: str) -> List[Tuple[str, str]]:\n        \"\"\"\n        \n        \"\"\"\n        parts = []\n        for part_text in cls.cell_part_regex.split(cell_text):\n            part_text = part_text.strip()\n            part_entity = f'fb:part.{cls._normalize_string(part_text)}'\n            parts.append((part_entity, part_text))\n        return parts", "output": "Splits a cell into parts and returns the parts of the cell.  We return a list of\n        ``(entity_name, entity_text)``, where ``entity_name`` is ``fb:part.[something]``, and\n        ``entity_text`` is the text of the cell corresponding to that part.  For many cells, there\n        is only one \"part\", and we return a list of length one.\n\n        Note that you shouldn't call this on every cell in the table; SEMPRE decides to make these\n        splits only when at least one of the cells in a column looks \"splittable\".  Only if you're\n        splitting the cells in a column should you use this function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_states(self, fname):\n        \"\"\"\n        \"\"\"\n        if not self._kv_initialized:\n            self._init_kvstore()\n        if self._params_to_init:\n            self._init_params()\n\n        if self._update_on_kvstore:\n            self._kvstore.load_optimizer_states(fname)\n            self._optimizer = self._kvstore._updater.optimizer\n        else:\n            with open(fname, 'rb') as f:\n                states = f.read()\n            for updater in self._updaters:\n                updater.set_states(states)\n                updater.optimizer = self._updaters[0].optimizer\n            self._optimizer = self._updaters[0].optimizer\n        param_dict = {i: param for i, param in enumerate(self._params)}\n        self._optimizer.param_dict = param_dict", "output": "Loads trainer states (e.g. optimizer, momentum) from a file.\n\n        Parameters\n        ----------\n        fname : str\n            Path to input states file.\n\n        Note\n        ----\n        `optimizer.param_dict`, which contains Parameter information (such as\n        `lr_mult` and `wd_mult`) will not be loaded from the file, but rather set\n        based on current Trainer's parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_cidr_ips_ipv6(cidr):\n    '''\n    \n    '''\n    ips = netaddr.IPNetwork(cidr)\n    return [six.text_type(ip.ipv6()) for ip in list(ips)]", "output": "Get a list of IPv6 addresses from a CIDR.\n\n    CLI example::\n\n        salt myminion netaddress.list_cidr_ips_ipv6 192.168.0.0/20", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _wait_for_consistency(checker):\n    \"\"\"\n    \"\"\"\n    for _ in xrange(EVENTUAL_CONSISTENCY_MAX_SLEEPS):\n        if checker():\n            return\n\n        time.sleep(EVENTUAL_CONSISTENCY_SLEEP_INTERVAL)\n\n    logger.warning('Exceeded wait for eventual GCS consistency - this may be a'\n                   'bug in the library or something is terribly wrong.')", "output": "Eventual consistency: wait until GCS reports something is true.\n\n    This is necessary for e.g. create/delete where the operation might return,\n    but won't be reflected for a bit.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_(name, path):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'path': path,\n           'result': True,\n           'changes': {},\n           'comment': ''}\n\n    current = __salt__['alternatives.show_current'](name)\n    if current == path:\n        ret['comment'] = 'Alternative for {0} already set to {1}'.format(name, path)\n        return ret\n\n    display = __salt__['alternatives.display'](name)\n    isinstalled = False\n    for line in display.splitlines():\n        if line.startswith(path):\n            isinstalled = True\n            break\n\n    if isinstalled:\n        if __opts__['test']:\n            ret['comment'] = (\n                'Alternative for {0} will be set to path {1}'\n            ).format(name, path)\n            ret['result'] = None\n            return ret\n        __salt__['alternatives.set'](name, path)\n        current = __salt__['alternatives.show_current'](name)\n        if current == path:\n            ret['comment'] = (\n                'Alternative for {0} set to path {1}'\n            ).format(name, current)\n            ret['changes'] = {'path': current}\n        else:\n            ret['comment'] = 'Alternative for {0} not updated'.format(name)\n\n        return ret\n\n    else:\n        ret['result'] = False\n        ret['comment'] = (\n            'Alternative {0} for {1} doesn\\'t exist'\n            ).format(path, name)\n\n    return ret", "output": ".. versionadded:: 0.17.0\n\n    Sets alternative for <name> to <path>, if <path> is defined\n    as an alternative for <name>.\n\n    name\n        is the master name for this link group\n        (e.g. pager)\n\n    path\n        is the location of one of the alternative target files.\n        (e.g. /usr/bin/less)\n\n    .. code-block:: yaml\n\n        foo:\n          alternatives.set:\n            - path: /usr/bin/foo-2.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_valid(self, qstr=None):\n        \"\"\"\"\"\"\n        if not self.help.source_is_console():\n            return True\n        if qstr is None:\n            qstr = self.currentText()\n        if not re.search(r'^[a-zA-Z0-9_\\.]*$', str(qstr), 0):\n            return False\n        objtxt = to_text_string(qstr)\n        shell_is_defined = False\n        if self.help.get_option('automatic_import'):\n            shell = self.help.internal_shell\n            if shell is not None:\n                shell_is_defined = shell.is_defined(objtxt, force_import=True)\n        if not shell_is_defined:\n            shell = self.help.get_shell()\n            if shell is not None:\n                try:\n                    shell_is_defined = shell.is_defined(objtxt)\n                except socket.error:\n                    shell = self.help.get_shell()\n                    try:\n                        shell_is_defined = shell.is_defined(objtxt)\n                    except socket.error:\n                        # Well... too bad!\n                        pass\n        return shell_is_defined", "output": "Return True if string is valid", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dimensions_to_values(dimensions):\n        \"\"\"\n        \n        \"\"\"\n        values = []\n        for dimension in dimensions:\n            if isinstance(dimension, dict):\n                if 'extractionFn' in dimension:\n                    values.append(dimension)\n                elif 'dimension' in dimension:\n                    values.append(dimension['dimension'])\n            else:\n                values.append(dimension)\n\n        return values", "output": "Replace dimensions specs with their `dimension`\n        values, and ignore those without", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_host_disks(host_reference):\n    '''\n    \n    '''\n    storage_system = host_reference.configManager.storageSystem\n    disks = storage_system.storageDeviceInfo.scsiLun\n    ssds = []\n    non_ssds = []\n\n    for disk in disks:\n        try:\n            has_ssd_attr = disk.ssd\n        except AttributeError:\n            has_ssd_attr = False\n        if has_ssd_attr:\n            ssds.append(disk)\n        else:\n            non_ssds.append(disk)\n\n    return {'SSDs': ssds, 'Non-SSDs': non_ssds}", "output": "Helper function that returns a dictionary containing a list of SSD and Non-SSD disks.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fn_with_custom_grad(grad_fn, use_global_vars=False):\n  \"\"\"\n  \"\"\"\n\n  def dec(fn):\n\n    @functools.wraps(fn)\n    def wrapped(*args):\n      return _fn_with_custom_grad(\n          fn, args, grad_fn, use_global_vars=use_global_vars)\n\n    return wrapped\n\n  return dec", "output": "Decorator to create a subgraph with a custom gradient function.\n\n  The subgraph created by the decorated function is NOT put in a Defun and so\n  does not suffer from the limitations of the Defun (all subgraph ops on the\n  same device, no summaries).\n\n  Args:\n    grad_fn: function with signature\n      (inputs, variables, outputs, output_grads) -> (grad_inputs, grad_vars),\n      all of which are lists of Tensors.\n    use_global_vars: if True, variables will be the global variables created.\n      If False, will be the trainable variables.\n\n  Returns:\n    Decorator for function such that the gradient is defined by grad_fn.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ConvGRUCell(units, kernel_size=(3, 3)):\n  \"\"\"\n  \"\"\"\n\n  def BuildConv():\n    return core.Conv(filters=units, kernel_size=kernel_size, padding='SAME')\n\n  return GeneralGRUCell(\n      candidate_transform=BuildConv,\n      memory_transform=combinators.Identity,\n      gate_nonlinearity=core.Sigmoid,\n      candidate_nonlinearity=core.Tanh)", "output": "Builds a convolutional GRU.\n\n  Paper: https://arxiv.org/abs/1511.06432.\n\n  Args:\n    units: Number of hidden units\n    kernel_size: Kernel size for convolution\n\n  Returns:\n    A Stax model representing a GRU cell with convolution transforms.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_to_pythonpath(self, path):\r\n        \"\"\"\"\"\"\r\n        pathlist = self.get_pythonpath()\r\n        if path in pathlist:\r\n            return False\r\n        else:\r\n            pathlist.insert(0, path)\r\n            self.set_pythonpath(pathlist)\r\n            return True", "output": "Add path to project's PYTHONPATH\r\n        Return True if path was added, False if it was already there", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _line_shift(x:Tensor, mask:bool=False):\n    \"\"\n    bs,nh,n,p = x.size()\n    x_pad = torch.cat([x.new_zeros(bs,nh,n,1), x], dim=3)\n    x_shift = x_pad.view(bs,nh,p + 1,n)[:,:,1:].view_as(x)\n    if mask: x_shift.mul_(torch.tril(x.new_ones(n,p), p-n)[None,None,])\n    return x_shift", "output": "Shift the line i of `x` by p-i elements to the left, is `mask` puts 0s on the diagonal.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def approximate_split(x, num_splits, axis=0):\n  \"\"\"\n  \"\"\"\n  size = shape_list(x)[axis]\n  size_splits = [tf.div(size + i, num_splits) for i in range(num_splits)]\n  return tf.split(x, size_splits, axis=axis)", "output": "Split approximately equally into num_splits parts.\n\n  Args:\n    x: a Tensor\n    num_splits: an integer\n    axis: an integer.\n\n  Returns:\n    a list of num_splits Tensors.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_result(self, job):\n        \"\"\"\n        \"\"\"\n        self.cur.execute(\n            \"INSERT INTO history VALUES(?,?,?,?)\",\n            (job[\"id\"], job[\"description\"], job[\"last-run\"], job[\"last-run-result\"]))\n\n        return True", "output": "Adds a job run result to the history table.\n        :param dict job: The job dictionary\n        :returns: True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_enlargement(locator, global_index):\n    \"\"\"\n    \"\"\"\n    if (\n        is_list_like(locator)\n        and not is_slice(locator)\n        and len(locator) > 0\n        and not is_boolean_array(locator)\n        and (isinstance(locator, type(global_index[0])) and locator not in global_index)\n    ):\n        n_diff_elems = len(pandas.Index(locator).difference(global_index))\n        is_enlargement_boolean = n_diff_elems > 0\n        return is_enlargement_boolean\n    return False", "output": "Determine if a locator will enlarge the global index.\n\n    Enlargement happens when you trying to locate using labels isn't in the\n    original index. In other words, enlargement == adding NaNs !", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def redact_http_basic_auth(output):\n    '''\n    \n    '''\n    # We can't use re.compile because re.compile(someregex).sub() doesn't\n    # support flags even in Python 2.7.\n    url_re = '(https?)://.*@'\n    redacted = r'\\1://<redacted>@'\n    if sys.version_info >= (2, 7):\n        # re.sub() supports flags as of 2.7, use this to do a case-insensitive\n        # match.\n        return re.sub(url_re, redacted, output, flags=re.IGNORECASE)\n    else:\n        # We're on python 2.6, test if a lowercased version of the output\n        # string matches the regex...\n        if re.search(url_re, output.lower()):\n            # ... and if it does, perform the regex substitution.\n            return re.sub(url_re, redacted, output.lower())\n    # No match, just return the original string\n    return output", "output": "Remove HTTP user and password", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def migrate(name, target=''):\n    '''\n    \n    '''\n    client = salt.client.get_local_client(__opts__['conf_file'])\n    data = query(quiet=True)\n    origin_data = _find_vm(name, data, quiet=True)\n    try:\n        origin_host = list(origin_data.keys())[0]\n    except IndexError:\n        __jid_event__.fire_event({'error': 'Named VM {0} was not found to migrate'.format(name)}, 'progress')\n        return ''\n    disks = origin_data[origin_host][name]['disks']\n    if not origin_data:\n        __jid_event__.fire_event({'error': 'Named VM {0} was not found to migrate'.format(name)}, 'progress')\n        return ''\n    if not target:\n        target = _determine_host(data, origin_host)\n    if target not in data:\n        __jid_event__.fire_event({'error': 'Target host {0} not found'.format(origin_data)}, 'progress')\n        return ''\n    try:\n        client.cmd(target, 'virt.seed_non_shared_migrate', [disks, True])\n        jid = client.cmd_async(origin_host,\n                               'virt.migrate_non_shared',\n                               [name, target])\n    except SaltClientError as client_error:\n        return 'Virtual machine {0} could not be migrated: {1}'.format(name, client_error)\n\n    msg = ('The migration of virtual machine {0} to host {1} has begun, '\n           'and can be tracked via jid {2}. The ``salt-run virt.query`` '\n           'runner can also be used, the target VM will be shown as paused '\n           'until the migration is complete.').format(name, target, jid)\n    __jid_event__.fire_event({'message': msg}, 'progress')", "output": "Migrate a VM from one host to another. This routine will just start\n    the migration and display information on how to look up the progress.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def linear(m=1, b=0):\n    ''' \n\n    '''\n    def f(i):\n        return m * i + b\n    return partial(force, sequence=_advance(f))", "output": "Return a driver function that can advance a sequence of linear values.\n\n    .. code-block:: none\n\n        value = m * i + b\n\n    Args:\n        m (float) : a slope for the linear driver\n        x (float) : an offset for the linear driver", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main():\n    ''' \n\n    '''\n    import sys\n    from bokeh.command.bootstrap import main as _main\n\n   # Main entry point (see setup.py)\n    _main(sys.argv)", "output": "Execute the \"bokeh\" command line program.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_proto():\n    '''\n    \n    '''\n    use_ssl = config.get_cloud_config_value(\n        'use_ssl',\n        get_configured_provider(),\n        __opts__,\n        search_global=False,\n        default=True\n    )\n    if use_ssl is True:\n        return 'https'\n    return 'http'", "output": "Checks configuration to see whether the user has SSL turned on. Default is:\n\n    .. code-block:: yaml\n\n        use_ssl: True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_datepart(df:DataFrame, field_name:str, prefix:str=None, drop:bool=True, time:bool=False):\n    \"\"\n    make_date(df, field_name)\n    field = df[field_name]\n    prefix = ifnone(prefix, re.sub('[Dd]ate$', '', field_name))\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Is_month_end', 'Is_month_start', \n            'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    if time: attr = attr + ['Hour', 'Minute', 'Second']\n    for n in attr: df[prefix + n] = getattr(field.dt, n.lower())\n    df[prefix + 'Elapsed'] = field.astype(np.int64) // 10 ** 9\n    if drop: df.drop(field_name, axis=1, inplace=True)\n    return df", "output": "Helper function that adds columns relevant to a date in the column `field_name` of `df`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cache_data(self, cache_directory: str) -> None:\n        \"\"\"\n        \n        \"\"\"\n        self._cache_directory = pathlib.Path(cache_directory)\n        os.makedirs(self._cache_directory, exist_ok=True)", "output": "When you call this method, we will use this directory to store a cache of already-processed\n        ``Instances`` in every file passed to :func:`read`, serialized as one string-formatted\n        ``Instance`` per line.  If the cache file for a given ``file_path`` exists, we read the\n        ``Instances`` from the cache instead of re-processing the data (using\n        :func:`deserialize_instance`).  If the cache file does `not` exist, we will `create` it on\n        our first pass through the data (using :func:`serialize_instance`).\n\n        IMPORTANT CAVEAT: It is the `caller's` responsibility to make sure that this directory is\n        unique for any combination of code and parameters that you use.  That is, if you call this\n        method, we will use any existing cache files in that directory `regardless of the\n        parameters you set for this DatasetReader!`  If you use our commands, the ``Train`` command\n        is responsible for calling this method and ensuring that unique parameters correspond to\n        unique cache directories.  If you don't use our commands, that is your responsibility.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def autoencoder_ordered_discrete_image64():\n  \"\"\"\"\"\"\n  hparams = autoencoder_ordered_discrete()\n  hparams.batch_size = 32\n  hparams.num_hidden_layers = 6\n  hparams.bottleneck_warmup_steps *= 2\n  hparams.gan_codes_warmup_steps *= 2\n\n  return hparams", "output": "Ordered discrete autoencoder model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def linkcode_resolve(domain, info):\n    \"\"\"\n    \n    \"\"\"\n    if domain != 'py':\n        return None\n\n    modname = info['module']\n    fullname = info['fullname']\n\n    submod = sys.modules.get(modname)\n    if submod is None:\n        return None\n\n    obj = submod\n    for part in fullname.split('.'):\n        try:\n            obj = getattr(obj, part)\n        except AttributeError:\n            return None\n\n    try:\n        # inspect.unwrap() was added in Python version 3.4\n        if sys.version_info >= (3, 5):\n            fn = inspect.getsourcefile(inspect.unwrap(obj))\n        else:\n            fn = inspect.getsourcefile(obj)\n    except TypeError:\n        fn = None\n    if not fn:\n        return None\n\n    try:\n        source, lineno = inspect.getsourcelines(obj)\n    except OSError:\n        lineno = None\n\n    if lineno:\n        linespec = \"#L{:d}-L{:d}\".format(lineno, lineno + len(source) - 1)\n    else:\n        linespec = \"\"\n\n    fn = os.path.relpath(fn, start=os.path.dirname(pandas.__file__))\n\n    if '+' in pandas.__version__:\n        return (\"http://github.com/pandas-dev/pandas/blob/master/pandas/\"\n                \"{}{}\".format(fn, linespec))\n    else:\n        return (\"http://github.com/pandas-dev/pandas/blob/\"\n                \"v{}/pandas/{}{}\".format(pandas.__version__, fn, linespec))", "output": "Determine the URL corresponding to Python object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def json_default_with_numpy(obj):\n    \"\"\"\"\"\"\n    if isinstance(obj, (np.integer, np.floating, np.bool_)):\n        return obj.item()\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    else:\n        return obj", "output": "Convert numpy classes to JSON serializable objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ranging_attributes(attributes, param_class):\n    \"\"\"\n    \n    \"\"\"\n    next_attributes = {param_class.next_in_enumeration(attribute) for attribute in attributes}\n    in_first = attributes.difference(next_attributes)\n    in_second = next_attributes.difference(attributes)\n    if len(in_first) == 1 and len(in_second) == 1:\n        for x in attributes:\n            if {param_class.next_in_enumeration(x)} == in_second:\n                return next(iter(in_first)), x\n    return None, None", "output": "Checks if there is a continuous range", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def column_families(keyspace=None):\n    '''\n    \n    '''\n    sys = _sys_mgr()\n    ksps = sys.list_keyspaces()\n\n    if keyspace:\n        if keyspace in ksps:\n            return list(sys.get_keyspace_column_families(keyspace).keys())\n        else:\n            return None\n    else:\n        ret = {}\n        for kspace in ksps:\n            ret[kspace] = list(sys.get_keyspace_column_families(kspace).keys())\n\n        return ret", "output": "Return existing column families for all keyspaces\n    or just the provided one.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cassandra.column_families\n        salt '*' cassandra.column_families <keyspace>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def stream(\n        self, version=\"1.1\", keep_alive=False, keep_alive_timeout=None\n    ):\n        \"\"\"\n        \"\"\"\n        headers = self.get_headers(\n            version,\n            keep_alive=keep_alive,\n            keep_alive_timeout=keep_alive_timeout,\n        )\n        self.protocol.push_data(headers)\n        await self.protocol.drain()\n        await self.streaming_fn(self)\n        self.protocol.push_data(b\"0\\r\\n\\r\\n\")", "output": "Streams headers, runs the `streaming_fn` callback that writes\n        content to the response body, then finalizes the response body.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _select_input(self):\r\n        \"\"\"\"\"\"\r\n        line, index = self.get_position('eof')\r\n        if self.current_prompt_pos is None:\r\n            pline, pindex = line, index\r\n        else:\r\n            pline, pindex = self.current_prompt_pos\r\n        self.setSelection(pline, pindex, line, index)", "output": "Select current line (without selecting console prompt)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_thread_id(self):\r\n        \"\"\"\"\"\"\r\n        if self._id is None:\r\n            for thread_id, obj in list(threading._active.items()):\r\n                if obj is self:\r\n                    self._id = thread_id\r\n        return self._id", "output": "Return thread id", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_nodes(full=False, call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The list_nodes function must be called with -f or --function.'\n        )\n\n    ret = {}\n    if POLL_ALL_LOCATIONS:\n        for location in JOYENT_LOCATIONS:\n            result = query(command='my/machines', location=location,\n                           method='GET')\n            if result[0] in VALID_RESPONSE_CODES:\n                nodes = result[1]\n                for node in nodes:\n                    if 'name' in node:\n                        node['location'] = location\n                        ret[node['name']] = reformat_node(item=node, full=full)\n            else:\n                log.error('Invalid response when listing Joyent nodes: %s', result[1])\n\n    else:\n        location = get_location()\n        result = query(command='my/machines', location=location,\n                       method='GET')\n        nodes = result[1]\n        for node in nodes:\n            if 'name' in node:\n                node['location'] = location\n                ret[node['name']] = reformat_node(item=node, full=full)\n    return ret", "output": "list of nodes, keeping only a brief listing\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -Q", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pivot(self, pivot_col, values=None):\n        \"\"\"\n        \n        \"\"\"\n        if values is None:\n            jgd = self._jgd.pivot(pivot_col)\n        else:\n            jgd = self._jgd.pivot(pivot_col, values)\n        return GroupedData(jgd, self._df)", "output": "Pivots a column of the current :class:`DataFrame` and perform the specified aggregation.\n        There are two versions of pivot function: one that requires the caller to specify the list\n        of distinct values to pivot on, and one that does not. The latter is more concise but less\n        efficient, because Spark needs to first compute the list of distinct values internally.\n\n        :param pivot_col: Name of the column to pivot.\n        :param values: List of values that will be translated to columns in the output DataFrame.\n\n        # Compute the sum of earnings for each year by course with each course as a separate column\n\n        >>> df4.groupBy(\"year\").pivot(\"course\", [\"dotNET\", \"Java\"]).sum(\"earnings\").collect()\n        [Row(year=2012, dotNET=15000, Java=20000), Row(year=2013, dotNET=48000, Java=30000)]\n\n        # Or without specifying column values (less efficient)\n\n        >>> df4.groupBy(\"year\").pivot(\"course\").sum(\"earnings\").collect()\n        [Row(year=2012, Java=20000, dotNET=15000), Row(year=2013, Java=30000, dotNET=48000)]\n        >>> df5.groupBy(\"sales.year\").pivot(\"sales.course\").sum(\"sales.earnings\").collect()\n        [Row(year=2012, Java=20000, dotNET=15000), Row(year=2013, Java=30000, dotNET=48000)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extract_error_message(cls, e):\n        \"\"\"\"\"\"\n        message = str(e)\n        try:\n            if isinstance(e.args, tuple) and len(e.args) > 1:\n                message = e.args[1]\n        except Exception:\n            pass\n        return message", "output": "Extract error message for queries", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_balancers_list_all(**kwargs):\n    '''\n    \n\n    '''\n    result = {}\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        load_balancers = __utils__['azurearm.paged_object_to_list'](netconn.load_balancers.list_all())\n\n        for load_balancer in load_balancers:\n            result[load_balancer['name']] = load_balancer\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    List all load balancers within a subscription.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.load_balancers_list_all", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def error(self, coro):\n        \"\"\"\n        \"\"\"\n\n        if not asyncio.iscoroutinefunction(coro):\n            raise TypeError('The error handler must be a coroutine.')\n\n        self.on_error = coro\n        return coro", "output": "A decorator that registers a coroutine as a local error handler.\n\n        A local error handler is an :func:`.on_command_error` event limited to\n        a single command. However, the :func:`.on_command_error` is still\n        invoked afterwards as the catch-all.\n\n        Parameters\n        -----------\n        coro: :ref:`coroutine <coroutine>`\n            The coroutine to register as the local error handler.\n\n        Raises\n        -------\n        TypeError\n            The coroutine passed is not actually a coroutine.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def concat(self, axis, other, **kwargs):\n        \"\"\"\n        \"\"\"\n        return self._append_list_of_managers(other, axis, **kwargs)", "output": "Concatenates two objects together.\n\n        Args:\n            axis: The axis index object to join (0 for columns, 1 for index).\n            other: The other_index to concat with.\n\n        Returns:\n            Concatenated objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_api_url(\n        cls, path, query_params=None, api_base_url=None, api_version=None\n    ):\n        \"\"\"\n        \"\"\"\n        url = cls.API_URL_TEMPLATE.format(\n            api_base_url=(api_base_url or cls.API_BASE_URL),\n            api_version=(api_version or cls.API_VERSION),\n            path=path,\n        )\n\n        query_params = query_params or {}\n        if query_params:\n            url += \"?\" + urlencode(query_params, doseq=True)\n\n        return url", "output": "Construct an API url given a few components, some optional.\n\n        Typically, you shouldn't need to use this method.\n\n        :type path: str\n        :param path: The path to the resource (ie, ``'/b/bucket-name'``).\n\n        :type query_params: dict or list\n        :param query_params: A dictionary of keys and values (or list of\n                             key-value pairs) to insert into the query\n                             string of the URL.\n\n        :type api_base_url: str\n        :param api_base_url: The base URL for the API endpoint.\n                             Typically you won't have to provide this.\n\n        :type api_version: str\n        :param api_version: The version of the API to call.\n                            Typically you shouldn't provide this and instead\n                            use the default for the library.\n\n        :rtype: str\n        :returns: The URL assembled from the pieces provided.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_node(self, singular, plural, variables, plural_expr,\n                   vars_referenced, num_called_num):\n        \"\"\"\"\"\"\n        # no variables referenced?  no need to escape for old style\n        # gettext invocations only if there are vars.\n        if not vars_referenced and not self.environment.newstyle_gettext:\n            singular = singular.replace('%%', '%')\n            if plural:\n                plural = plural.replace('%%', '%')\n\n        # singular only:\n        if plural_expr is None:\n            gettext = nodes.Name('gettext', 'load')\n            node = nodes.Call(gettext, [nodes.Const(singular)],\n                              [], None, None)\n\n        # singular and plural\n        else:\n            ngettext = nodes.Name('ngettext', 'load')\n            node = nodes.Call(ngettext, [\n                nodes.Const(singular),\n                nodes.Const(plural),\n                plural_expr\n            ], [], None, None)\n\n        # in case newstyle gettext is used, the method is powerful\n        # enough to handle the variable expansion and autoescape\n        # handling itself\n        if self.environment.newstyle_gettext:\n            for key, value in iteritems(variables):\n                # the function adds that later anyways in case num was\n                # called num, so just skip it.\n                if num_called_num and key == 'num':\n                    continue\n                node.kwargs.append(nodes.Keyword(key, value))\n\n        # otherwise do that here\n        else:\n            # mark the return value as safe if we are in an\n            # environment with autoescaping turned on\n            node = nodes.MarkSafeIfAutoescape(node)\n            if variables:\n                node = nodes.Mod(node, nodes.Dict([\n                    nodes.Pair(nodes.Const(key), value)\n                    for key, value in variables.items()\n                ]))\n        return nodes.Output([node])", "output": "Generates a useful node from the data provided.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close(self, code: int = None, reason: str = None) -> None:\n        \"\"\"\n        \"\"\"\n        if self.ws_connection:\n            self.ws_connection.close(code, reason)\n            self.ws_connection = None", "output": "Closes this Web Socket.\n\n        Once the close handshake is successful the socket will be closed.\n\n        ``code`` may be a numeric status code, taken from the values\n        defined in `RFC 6455 section 7.4.1\n        <https://tools.ietf.org/html/rfc6455#section-7.4.1>`_.\n        ``reason`` may be a textual message about why the connection is\n        closing.  These values are made available to the client, but are\n        not otherwise interpreted by the websocket protocol.\n\n        .. versionchanged:: 4.0\n\n           Added the ``code`` and ``reason`` arguments.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hparams_to_batching_scheme(hparams,\n                               drop_long_sequences=False,\n                               shard_multiplier=1,\n                               length_multiplier=1):\n  \"\"\"\"\"\"\n  return batching_scheme(\n      batch_size=hparams.batch_size,\n      min_length=hparams.min_length,\n      max_length=hparams.max_length,\n      min_length_bucket=hparams.min_length_bucket,\n      length_bucket_step=hparams.length_bucket_step,\n      drop_long_sequences=drop_long_sequences,\n      shard_multiplier=shard_multiplier,\n      length_multiplier=length_multiplier)", "output": "Wrapper around _batching_scheme with hparams.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_alias(infos):\n    \"\"\"\n    \"\"\"\n    pairs = []\n    for x in infos:\n        for y in x:\n            if \"alias\" in y:\n                name = y[\"name\"][0]\n                alias = y[\"alias\"][0].split(',')\n                for name2 in alias:\n                    pairs.append((name2.strip(), name))\n    return pairs", "output": "Get aliases of all parameters.\n\n    Parameters\n    ----------\n    infos : list\n        Content of the config header file.\n\n    Returns\n    -------\n    pairs : list\n        List of tuples (param alias, param name).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_embedding(net, node, model, builder):\n    \"\"\"\n    \"\"\"\n    input_name, output_name = _get_input_output_name(net, node)\n    name = node['name']\n    inputs = node['inputs']\n    outputs = node['outputs']\n    arg_params, aux_params = model.get_params()\n    W = arg_params[_get_node_name(net, inputs[1][0])].asnumpy()\n    if not ONE_HOT_ENCODE_HACK:\n        nC, nB = W.shape\n        W = W.T\n        builder.add_embedding(name = name,\n                              W = W,\n                              b = None,\n                              input_dim = nC,\n                              output_channels = nB,\n                              has_bias = False,\n                              input_name = input_name,\n                              output_name = output_name)\n    else:\n        W = W.T\n        nC, nB = W.shape\n        builder.add_inner_product(name = name,\n                W = W,\n                b = None,\n                input_channels = nB,\n                output_channels = nC,\n                has_bias = False,\n                input_name = input_name,\n                output_name = output_name)", "output": "Convert an embedding layer from mxnet to coreml.\n\n    Parameters\n    ----------\n    net: network\n        A mxnet network object.\n\n    node: layer\n        Node to convert.\n\n    model: model\n        An model for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def alias_exists(FunctionName, Name, region=None, key=None,\n                 keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        alias = _find_alias(FunctionName, Name,\n                            region=region, key=key, keyid=keyid, profile=profile)\n        return {'exists': bool(alias)}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "Given a function name and alias name, check to see if the given alias exists.\n\n    Returns True if the given alias exists and returns False if the given\n    alias does not exist.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_lambda.alias_exists myfunction myalias", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _rebind_variables(self, new_inputs):\n        \"\"\"\n        \n        \"\"\"\n        expr = self._expr\n\n        # If we have 11+ variables, some of our variable names may be\n        # substrings of other variable names. For example, we might have x_1,\n        # x_10, and x_100. By enumerating in reverse order, we ensure that\n        # every variable name which is a substring of another variable name is\n        # processed after the variable of which it is a substring. This\n        # guarantees that the substitution of any given variable index only\n        # ever affects exactly its own index. For example, if we have variables\n        # with indices going up to 100, we will process all of the x_1xx names\n        # before x_1x, which will be before x_1, so the substitution of x_1\n        # will not affect x_1x, which will not affect x_1xx.\n        for idx, input_ in reversed(list(enumerate(self.inputs))):\n            old_varname = \"x_%d\" % idx\n            # Temporarily rebind to x_temp_N so that we don't overwrite the\n            # same value multiple times.\n            temp_new_varname = \"x_temp_%d\" % new_inputs.index(input_)\n            expr = expr.replace(old_varname, temp_new_varname)\n        # Clear out the temp variables now that we've finished iteration.\n        return expr.replace(\"_temp_\", \"_\")", "output": "Return self._expr with all variables rebound to the indices implied by\n        new_inputs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shakeshake2_equal_grad(x1, x2, dy):\n  \"\"\"\"\"\"\n  y = shakeshake2_py(x1, x2, equal=True)\n  dx = tf.gradients(ys=[y], xs=[x1, x2], grad_ys=[dy])\n  return dx", "output": "Overriding gradient for shake-shake of 2 tensors.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zlib_decompress_to_string(blob):\n    \"\"\"\n    \n    \"\"\"\n    if PY3K:\n        if isinstance(blob, bytes):\n            decompressed = zlib.decompress(blob)\n        else:\n            decompressed = zlib.decompress(bytes(blob, 'utf-8'))\n        return decompressed.decode('utf-8')\n    return zlib.decompress(blob)", "output": "Decompress things to a string in a py2/3 safe fashion\n    >>> json_str = '{\"test\": 1}'\n    >>> blob = zlib_compress(json_str)\n    >>> got_str = zlib_decompress_to_string(blob)\n    >>> got_str == json_str\n    True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def request_forward_agent(self, handler):\n        \"\"\"\n        \n        \"\"\"\n        m = Message()\n        m.add_byte(cMSG_CHANNEL_REQUEST)\n        m.add_int(self.remote_chanid)\n        m.add_string(\"auth-agent-req@openssh.com\")\n        m.add_boolean(False)\n        self.transport._send_user_message(m)\n        self.transport._set_forward_agent_handler(handler)\n        return True", "output": "Request for a forward SSH Agent on this channel.\n        This is only valid for an ssh-agent from OpenSSH !!!\n\n        :param handler:\n            a required callable handler to use for incoming SSH Agent\n            connections\n\n        :return: True if we are ok, else False\n            (at that time we always return ok)\n\n        :raises: SSHException in case of channel problem.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def updateByQuery(self, using=None):\n        \"\"\"\n        \n        \"\"\"\n        return UpdateByQuery(\n            using=using or self._using,\n            index=self._name,\n        )", "output": "Return a :class:`~elasticsearch_dsl.UpdateByQuery` object searching over the index\n        (or all the indices belonging to this template) and updating Documents that match\n        the search criteria.\n\n        For more information, see here:\n        https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update-by-query.html", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_configuration(self):\n        \"\"\"\n        \n        \"\"\"\n        # get distutils to do the work\n        c = self._get_pypirc_command()\n        c.repository = self.url\n        cfg = c._read_pypirc()\n        self.username = cfg.get('username')\n        self.password = cfg.get('password')\n        self.realm = cfg.get('realm', 'pypi')\n        self.url = cfg.get('repository', self.url)", "output": "Read the PyPI access configuration as supported by distutils, getting\n        PyPI to do the actual work. This populates ``username``, ``password``,\n        ``realm`` and ``url`` attributes from the configuration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_api_repr(self):\n        \"\"\"\"\"\"\n\n        source_refs = [\n            {\n                \"projectId\": table.project,\n                \"datasetId\": table.dataset_id,\n                \"tableId\": table.table_id,\n            }\n            for table in self.sources\n        ]\n\n        configuration = self._configuration.to_api_repr()\n        _helpers._set_sub_prop(configuration, [\"copy\", \"sourceTables\"], source_refs)\n        _helpers._set_sub_prop(\n            configuration,\n            [\"copy\", \"destinationTable\"],\n            {\n                \"projectId\": self.destination.project,\n                \"datasetId\": self.destination.dataset_id,\n                \"tableId\": self.destination.table_id,\n            },\n        )\n\n        return {\n            \"jobReference\": self._properties[\"jobReference\"],\n            \"configuration\": configuration,\n        }", "output": "Generate a resource for :meth:`_begin`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _copy_params(self):\n        \"\"\"\n        \n        \"\"\"\n        cls = type(self)\n        src_name_attrs = [(x, getattr(cls, x)) for x in dir(cls)]\n        src_params = list(filter(lambda nameAttr: isinstance(nameAttr[1], Param), src_name_attrs))\n        for name, param in src_params:\n            setattr(self, name, param._copy_new_parent(self))", "output": "Copy all params defined on the class to current object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_api_method_response(restApiId, resourcePath, httpMethod, statusCode,\n                               region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    try:\n        resource = describe_api_resource(restApiId, resourcePath, region=region,\n                                         key=key, keyid=keyid, profile=profile).get('resource')\n        if resource:\n            conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n            conn.delete_method_response(restApiId=restApiId, resourceId=resource['id'],\n                                        httpMethod=httpMethod, statusCode=str(statusCode))  # future lint: disable=blacklisted-function\n            return {'deleted': True}\n        return {'deleted': False, 'error': 'no such resource'}\n    except ClientError as e:\n        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Delete API method response for a resource in the given API\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_apigateway.delete_api_method_response restApiId resourcePath httpMethod statusCode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setitem(self, axis, key, value):\n        \"\"\"\n        \"\"\"\n\n        def setitem(df, internal_indices=[]):\n            def _setitem():\n                if len(internal_indices) == 1:\n                    if axis == 0:\n                        df[df.columns[internal_indices[0]]] = value\n                    else:\n                        df.iloc[internal_indices[0]] = value\n                else:\n                    if axis == 0:\n                        df[df.columns[internal_indices]] = value\n                    else:\n                        df.iloc[internal_indices] = value\n\n            try:\n                _setitem()\n            except ValueError:\n                # TODO: This is a workaround for a pyarrow serialization issue\n                df = df.copy()\n                _setitem()\n            return df\n\n        if axis == 0:\n            numeric_indices = list(self.columns.get_indexer_for([key]))\n        else:\n            numeric_indices = list(self.index.get_indexer_for([key]))\n        prepared_func = self._prepare_method(setitem)\n        if is_list_like(value):\n            new_data = self.data.apply_func_to_select_indices_along_full_axis(\n                axis, prepared_func, numeric_indices, keep_remaining=True\n            )\n        else:\n            new_data = self.data.apply_func_to_select_indices(\n                axis, prepared_func, numeric_indices, keep_remaining=True\n            )\n        return self.__constructor__(new_data, self.index, self.columns)", "output": "Set the column defined by `key` to the `value` provided.\n\n        Args:\n            key: The column name to set.\n            value: The value to set the column to.\n\n        Returns:\n             A new QueryCompiler", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def str_get_dummies(arr, sep='|'):\n    \"\"\"\n    \n    \"\"\"\n    arr = arr.fillna('')\n    try:\n        arr = sep + arr + sep\n    except TypeError:\n        arr = sep + arr.astype(str) + sep\n\n    tags = set()\n    for ts in arr.str.split(sep):\n        tags.update(ts)\n    tags = sorted(tags - {\"\"})\n\n    dummies = np.empty((len(arr), len(tags)), dtype=np.int64)\n\n    for i, t in enumerate(tags):\n        pat = sep + t + sep\n        dummies[:, i] = lib.map_infer(arr.values, lambda x: pat in x)\n    return dummies, tags", "output": "Split each string in the Series by sep and return a DataFrame\n    of dummy/indicator variables.\n\n    Parameters\n    ----------\n    sep : str, default \"|\"\n        String to split on.\n\n    Returns\n    -------\n    DataFrame\n        Dummy variables corresponding to values of the Series.\n\n    See Also\n    --------\n    get_dummies : Convert categorical variable into dummy/indicator\n        variables.\n\n    Examples\n    --------\n    >>> pd.Series(['a|b', 'a', 'a|c']).str.get_dummies()\n       a  b  c\n    0  1  1  0\n    1  1  0  0\n    2  1  0  1\n\n    >>> pd.Series(['a|b', np.nan, 'a|c']).str.get_dummies()\n       a  b  c\n    0  1  1  0\n    1  0  0  0\n    2  1  0  1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate(self,\n                 _portfolio,\n                 account,\n                 algo_datetime,\n                 _algo_current_data):\n        \"\"\"\n        \n        \"\"\"\n        if (algo_datetime > self.deadline and\n                account.leverage < self.min_leverage):\n            self.fail()", "output": "Make validation checks if we are after the deadline.\n        Fail if the leverage is less than the min leverage.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_critic_model_stats(self, iteration:int)->None:\n        \"\"\n        critic = self.learn.gan_trainer.critic\n        self.stats_writer.write(model=critic, iteration=iteration, tbwriter=self.tbwriter, name='crit_model_stats')\n        self.crit_stats_updated = True", "output": "Writes gradient statistics for critic to Tensorboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generic_loss(top_out, targets, model_hparams, vocab_size, weights_fn):\n  \"\"\"\"\"\"\n  del vocab_size  # unused arg\n  logits = top_out\n  logits = common_attention.maybe_upcast(logits, hparams=model_hparams)\n  cutoff = getattr(model_hparams, \"video_modality_loss_cutoff\", 0.0)\n  return common_layers.padded_cross_entropy(\n      logits,\n      targets,\n      model_hparams.label_smoothing,\n      cutoff=cutoff,\n      weights_fn=weights_fn)", "output": "Compute loss numerator and denominator for one shard of output.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stream_request_body(cls: Type[RequestHandler]) -> Type[RequestHandler]:\n    \"\"\"\n    \"\"\"  # noqa: E501\n    if not issubclass(cls, RequestHandler):\n        raise TypeError(\"expected subclass of RequestHandler, got %r\", cls)\n    cls._stream_request_body = True\n    return cls", "output": "Apply to `RequestHandler` subclasses to enable streaming body support.\n\n    This decorator implies the following changes:\n\n    * `.HTTPServerRequest.body` is undefined, and body arguments will not\n      be included in `RequestHandler.get_argument`.\n    * `RequestHandler.prepare` is called when the request headers have been\n      read instead of after the entire body has been read.\n    * The subclass must define a method ``data_received(self, data):``, which\n      will be called zero or more times as data is available.  Note that\n      if the request has an empty body, ``data_received`` may not be called.\n    * ``prepare`` and ``data_received`` may return Futures (such as via\n      ``@gen.coroutine``, in which case the next method will not be called\n      until those futures have completed.\n    * The regular HTTP method (``post``, ``put``, etc) will be called after\n      the entire body has been read.\n\n    See the `file receiver demo <https://github.com/tornadoweb/tornado/tree/master/demos/file_upload/>`_\n    for example usage.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_delta_deltas(filterbanks, name=None):\n  \"\"\"\n  \"\"\"\n  delta_filter = np.array([2, 1, 0, -1, -2])\n  delta_delta_filter = scipy.signal.convolve(delta_filter, delta_filter, \"full\")\n\n  delta_filter_stack = np.array(\n      [[0] * 4 + [1] + [0] * 4, [0] * 2 + list(delta_filter) + [0] * 2,\n       list(delta_delta_filter)],\n      dtype=np.float32).T[:, None, None, :]\n\n  delta_filter_stack /= np.sqrt(\n      np.sum(delta_filter_stack**2, axis=0, keepdims=True))\n\n  filterbanks = tf.nn.conv2d(\n      filterbanks, delta_filter_stack, [1, 1, 1, 1], \"SAME\", data_format=\"NHWC\",\n      name=name)\n  return filterbanks", "output": "Compute time first and second-order derivative channels.\n\n  Args:\n    filterbanks: float32 tensor with shape [batch_size, len, num_bins, 1]\n    name: scope name\n\n  Returns:\n    float32 tensor with shape [batch_size, len, num_bins, 3]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_lr(lr, epoch, steps, factor):\n    \"\"\"\"\"\"\n    for s in steps:\n        if epoch >= s:\n            lr *= factor\n    return lr", "output": "Get learning rate based on schedule.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode(self, s):\n    \"\"\"\"\"\"\n    s = tf.compat.as_text(s)\n    tokens = self._tokenizer.tokenize(s)\n    tokens = _prepare_tokens_for_encode(tokens)\n    ids = []\n    for token in tokens:\n      ids.extend(self._token_to_ids(token))\n    return text_encoder.pad_incr(ids)", "output": "Encodes text into a list of integers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def authenticated(\n    method: Callable[..., Optional[Awaitable[None]]]\n) -> Callable[..., Optional[Awaitable[None]]]:\n    \"\"\"\n    \"\"\"\n\n    @functools.wraps(method)\n    def wrapper(  # type: ignore\n        self: RequestHandler, *args, **kwargs\n    ) -> Optional[Awaitable[None]]:\n        if not self.current_user:\n            if self.request.method in (\"GET\", \"HEAD\"):\n                url = self.get_login_url()\n                if \"?\" not in url:\n                    if urllib.parse.urlsplit(url).scheme:\n                        # if login url is absolute, make next absolute too\n                        next_url = self.request.full_url()\n                    else:\n                        assert self.request.uri is not None\n                        next_url = self.request.uri\n                    url += \"?\" + urlencode(dict(next=next_url))\n                self.redirect(url)\n                return None\n            raise HTTPError(403)\n        return method(self, *args, **kwargs)\n\n    return wrapper", "output": "Decorate methods with this to require that the user be logged in.\n\n    If the user is not logged in, they will be redirected to the configured\n    `login url <RequestHandler.get_login_url>`.\n\n    If you configure a login url with a query parameter, Tornado will\n    assume you know what you're doing and use it as-is.  If not, it\n    will add a `next` parameter so the login page knows where to send\n    you once you're logged in.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _pprint_seq(seq, _nest_lvl=0, max_seq_items=None, **kwds):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(seq, set):\n        fmt = \"{{{body}}}\"\n    else:\n        fmt = \"[{body}]\" if hasattr(seq, '__setitem__') else \"({body})\"\n\n    if max_seq_items is False:\n        nitems = len(seq)\n    else:\n        nitems = max_seq_items or get_option(\"max_seq_items\") or len(seq)\n\n    s = iter(seq)\n    # handle sets, no slicing\n    r = [pprint_thing(next(s),\n                      _nest_lvl + 1, max_seq_items=max_seq_items, **kwds)\n         for i in range(min(nitems, len(seq)))]\n    body = \", \".join(r)\n\n    if nitems < len(seq):\n        body += \", ...\"\n    elif isinstance(seq, tuple) and len(seq) == 1:\n        body += ','\n\n    return fmt.format(body=body)", "output": "internal. pprinter for iterables. you should probably use pprint_thing()\n    rather then calling this directly.\n\n    bounds length of printed sequence, depending on options", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _process_emerge_err(stdout, stderr):\n    '''\n    \n    '''\n    ret = {}\n    rexp = re.compile(r'^[<>=][^ ]+/[^ ]+ [^\\n]+', re.M)\n\n    slot_conflicts = re.compile(r'^[^ \\n]+/[^ ]+:[^ ]', re.M).findall(stderr)\n    if slot_conflicts:\n        ret['slot conflicts'] = slot_conflicts\n\n    blocked = re.compile(r'(?m)^\\[blocks .+\\] '\n                         r'([^ ]+/[^ ]+-[0-9]+[^ ]+)'\n                         r'.*$').findall(stdout)\n\n    unsatisfied = re.compile(\n            r'Error: The above package list contains').findall(stderr)\n\n    # If there were blocks and emerge could not resolve it.\n    if blocked and unsatisfied:\n        ret['blocked'] = blocked\n\n    sections = re.split('\\n\\n', stderr)\n    for section in sections:\n        if 'The following keyword changes' in section:\n            ret['keywords'] = rexp.findall(section)\n        elif 'The following license changes' in section:\n            ret['license'] = rexp.findall(section)\n        elif 'The following USE changes' in section:\n            ret['use'] = rexp.findall(section)\n        elif 'The following mask changes' in section:\n            ret['mask'] = rexp.findall(section)\n    return ret", "output": "Used to parse emerge output to provide meaningful output when emerge fails", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _compute(self, windows, dates, assets, mask):\n        \"\"\"\n        \n        \"\"\"\n        format_inputs = self._format_inputs\n        compute = self.compute\n        params = self.params\n        ndim = self.ndim\n\n        shape = (len(mask), 1) if ndim == 1 else mask.shape\n        out = self._allocate_output(windows, shape)\n\n        with self.ctx:\n            for idx, date in enumerate(dates):\n                # Never apply a mask to 1D outputs.\n                out_mask = array([True]) if ndim == 1 else mask[idx]\n\n                # Mask our inputs as usual.\n                inputs_mask = mask[idx]\n\n                masked_assets = assets[inputs_mask]\n                out_row = out[idx][out_mask]\n                inputs = format_inputs(windows, inputs_mask)\n\n                compute(date, masked_assets, out_row, *inputs, **params)\n                out[idx][out_mask] = out_row\n        return out", "output": "Call the user's `compute` function on each window with a pre-built\n        output array.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_search_menu(self):\r\n        \"\"\"\"\"\"\r\n        # Disabling all actions except the last one\r\n        # (which is Find in files) to begin with\r\n        for child in self.search_menu.actions()[:-1]:\r\n            child.setEnabled(False)\r\n\r\n        widget, textedit_properties = self.get_focus_widget_properties()\r\n        if textedit_properties is None: # widget is not an editor/console\r\n            return\r\n\r\n        # !!! Below this line, widget is expected to be a QPlainTextEdit\r\n        #     instance\r\n        console, not_readonly, readwrite_editor = textedit_properties\r\n\r\n        # Find actions only trigger an effect in the Editor\r\n        if not console:\r\n            for action in self.search_menu.actions():\r\n                try:\r\n                    action.setEnabled(True)\r\n                except RuntimeError:\r\n                    pass\r\n\r\n        # Disable the replace action for read-only files\r\n        self.search_menu_actions[3].setEnabled(readwrite_editor)", "output": "Update search menu", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _systemctl_cmd(action, name=None, systemd_scope=False, no_block=False,\n                   root=None):\n    '''\n    \n    '''\n    ret = []\n    if systemd_scope \\\n            and salt.utils.systemd.has_scope(__context__) \\\n            and __salt__['config.get']('systemd.scope', True):\n        ret.extend(['systemd-run', '--scope'])\n    ret.append('systemctl')\n    if no_block:\n        ret.append('--no-block')\n    if root:\n        ret.extend(['--root', root])\n    if isinstance(action, six.string_types):\n        action = shlex.split(action)\n    ret.extend(action)\n    if name is not None:\n        ret.append(_canonical_unit_name(name))\n    if 'status' in ret:\n        ret.extend(['-n', '0'])\n    return ret", "output": "Build a systemctl command line. Treat unit names without one\n    of the valid suffixes as a service.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hybrid_forward(self, F, x, sampled_values, label, weight, bias):\n        \"\"\"\"\"\"\n        sampled_candidates, _, _ = sampled_values\n        # (batch_size,)\n        label = F.reshape(label, shape=(-1,))\n        # (num_sampled+batch_size,)\n        ids = F.concat(sampled_candidates, label, dim=0)\n        # lookup weights and biases\n        # (num_sampled+batch_size, dim)\n        w_all = F.Embedding(data=ids, weight=weight,\n                            input_dim=self._num_classes, output_dim=self._in_unit,\n                            sparse_grad=self._sparse_grad)\n        # (num_sampled+batch_size, 1)\n        b_all = F.take(bias, indices=ids)\n        return self._dense(x, sampled_values, label, w_all, b_all)", "output": "Forward computation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(self):\n        \"\"\"\"\"\"\n        if self._properties.get(\"startMs\") is None:\n            return None\n        return _helpers._datetime_from_microseconds(\n            int(self._properties.get(\"startMs\")) * 1000.0\n        )", "output": "Union[Datetime, None]: Datetime when the stage started.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_input_shape_ngpu(self, new_input_shape):\n    \"\"\"\n    \n    \"\"\"\n    assert self.device_name, \"Device name has not been set.\"\n\n    device_name = self.device_name\n    if self.input_shape is None:\n      # First time setting the input shape\n      self.input_shape = [None] + [int(d) for d in list(new_input_shape)]\n\n    if device_name in self.params_device:\n      # There is a copy of weights on this device\n      self.__dict__.update(self.params_device[device_name])\n      return\n\n    # Stop recursion\n    self.params_device[device_name] = {}\n\n    # Initialize weights on this device\n    with tf.device(device_name):\n      self.set_input_shape(self.input_shape)\n      keys_after = self.__dict__.keys()\n      if self.params_names is None:\n        # Prevent overriding training\n        self.params_names = [k for k in keys_after if isinstance(\n            self.__dict__[k], tf.Variable)]\n      params = {k: self.__dict__[k] for k in self.params_names}\n      self.params_device[device_name] = params", "output": "Create and initialize layer parameters on the device previously set\n    in self.device_name.\n\n    :param new_input_shape: a list or tuple for the shape of the input.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_configwidget(self, parent):\n        \"\"\"\"\"\"\n        if self.CONFIGWIDGET_CLASS is not None:\n            configwidget = self.CONFIGWIDGET_CLASS(self, parent)\n            configwidget.initialize()\n            return configwidget", "output": "Create configuration dialog box page widget", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def selection_r(x_bounds,\n                x_types,\n                clusteringmodel_gmm_good,\n                clusteringmodel_gmm_bad,\n                num_starting_points=100,\n                minimize_constraints_fun=None):\n    '''\n    \n    '''\n    minimize_starting_points = [lib_data.rand(x_bounds, x_types)\\\n                                    for i in range(0, num_starting_points)]\n    outputs = selection(x_bounds, x_types,\n                        clusteringmodel_gmm_good,\n                        clusteringmodel_gmm_bad,\n                        minimize_starting_points,\n                        minimize_constraints_fun)\n    return outputs", "output": "Call selection", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getAceTypeText(self, t):\n        '''\n        \n        '''\n        try:\n            return self.validAceTypes[t]['TEXT']\n        except KeyError:\n            raise CommandExecutionError((\n                'No ACE type \"{0}\".  It should be one of the following:  {1}'\n                ).format(t, ', '.join(self.validAceTypes)))", "output": "returns the textual representation of a acetype bit", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_reference(reference, statement):\n    '''\n    \n    '''\n    type_, value = _expand_one_key_dictionary(reference)\n    opt = Option(type_)\n    param = SimpleParameter(value)\n    opt.add_parameter(param)\n    statement.add_child(opt)", "output": "Adds a reference to statement.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def route_table_delete(name, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    result = False\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        table = netconn.route_tables.delete(\n            route_table_name=name,\n            resource_group_name=resource_group\n        )\n        table.wait()\n        result = True\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Delete a route table.\n\n    :param name: The name of the route table to delete.\n\n    :param resource_group: The resource group name assigned to the\n        route table.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.route_table_delete test-rt-table testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def end_before(self, document_fields):\n        \"\"\"\n        \"\"\"\n        return self._cursor_helper(document_fields, before=True, start=False)", "output": "End query results before a particular document value.\n\n        The result set will **exclude** the document specified by\n        ``document_fields``.\n\n        If the current query already has specified an end cursor -- either\n        via this method or\n        :meth:`~.firestore_v1beta1.query.Query.end_at` -- this will\n        overwrite it.\n\n        When the query is sent to the server, the ``document_fields`` will\n        be used in the order given by fields set by\n        :meth:`~.firestore_v1beta1.query.Query.order_by`.\n\n        Args:\n            document_fields (Union[~.firestore_v1beta1.\\\n                document.DocumentSnapshot, dict, list, tuple]): a document\n                snapshot or a dictionary/list/tuple of fields representing a\n                query results cursor. A cursor is a collection of values that\n                represent a position in a query result set.\n\n        Returns:\n            ~.firestore_v1beta1.query.Query: A query with cursor. Acts as\n            a copy of the current query, modified with the newly added\n            \"end before\" cursor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fit(self, dataset):\n        \"\"\"\n        \n        \"\"\"\n        dataset = dataset.map(_convert_to_vector)\n        jmodel = callMLlibFunc(\"fitStandardScaler\", self.withMean, self.withStd, dataset)\n        return StandardScalerModel(jmodel)", "output": "Computes the mean and variance and stores as a model to be used\n        for later scaling.\n\n        :param dataset: The data used to compute the mean and variance\n                     to build the transformation model.\n        :return: a StandardScalarModel", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_current_text_if_valid(self):\r\n        \"\"\"\"\"\"\r\n        valid = self.is_valid(self.currentText())\r\n        if valid or valid is None:\r\n            self.add_current_text()\r\n            return True\r\n        else:\r\n            self.set_current_text(self.selected_text)", "output": "Add current text to combo box history if valid", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def view_code(self):\n        \"\"\"\n        \"\"\"\n\n        return self.data.groupby(level=1).apply(\n            lambda x:\n            [item for item in x.index.remove_unused_levels().levels[0]]\n        )", "output": "\u6309\u80a1\u7968\u6392\u5217\u7684\u67e5\u770bblockname\u7684\u89c6\u56fe\n\n        Returns:\n            [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_timeout(cls, value, name):\n        \"\"\" \n        \"\"\"\n        if value is _Default:\n            return cls.DEFAULT_TIMEOUT\n\n        if value is None or value is cls.DEFAULT_TIMEOUT:\n            return value\n\n        if isinstance(value, bool):\n            raise ValueError(\"Timeout cannot be a boolean value. It must \"\n                             \"be an int, float or None.\")\n        try:\n            float(value)\n        except (TypeError, ValueError):\n            raise ValueError(\"Timeout value %s was %s, but it must be an \"\n                             \"int, float or None.\" % (name, value))\n\n        try:\n            if value <= 0:\n                raise ValueError(\"Attempted to set %s timeout to %s, but the \"\n                                 \"timeout cannot be set to a value less \"\n                                 \"than or equal to 0.\" % (name, value))\n        except TypeError:  # Python 3\n            raise ValueError(\"Timeout value %s was %s, but it must be an \"\n                             \"int, float or None.\" % (name, value))\n\n        return value", "output": "Check that a timeout attribute is valid.\n\n        :param value: The timeout value to validate\n        :param name: The name of the timeout attribute to validate. This is\n            used to specify in error messages.\n        :return: The validated and casted version of the given value.\n        :raises ValueError: If it is a numeric value less than or equal to\n            zero, or the type is not an integer, float, or None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def symbols(self, *args, **kwargs):\n        \"\"\"\n        \"\"\"\n        return [self.symbol(identifier, **kwargs) for identifier in args]", "output": "Lookup multuple Equities as a list.\n\n        Parameters\n        ----------\n        *args : iterable[str]\n            The ticker symbols to lookup.\n        country_code : str or None, optional\n            A country to limit symbol searches to.\n\n\n\n        Returns\n        -------\n        equities : list[Equity]\n            The equities that held the given ticker symbols on the current\n            symbol lookup date.\n\n        Raises\n        ------\n        SymbolNotFound\n            Raised when one of the symbols was not held on the current\n            lookup date.\n\n        See Also\n        --------\n        :func:`zipline.api.set_symbol_lookup_date`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_elements(self, by=By.ID, value=None):\n        \"\"\"\n        \n        \"\"\"\n        if self._w3c:\n            if by == By.ID:\n                by = By.CSS_SELECTOR\n                value = '[id=\"%s\"]' % value\n            elif by == By.TAG_NAME:\n                by = By.CSS_SELECTOR\n            elif by == By.CLASS_NAME:\n                by = By.CSS_SELECTOR\n                value = \".%s\" % value\n            elif by == By.NAME:\n                by = By.CSS_SELECTOR\n                value = '[name=\"%s\"]' % value\n\n        return self._execute(Command.FIND_CHILD_ELEMENTS,\n                             {\"using\": by, \"value\": value})['value']", "output": "Find elements given a By strategy and locator. Prefer the find_elements_by_* methods when\n        possible.\n\n        :Usage:\n            ::\n\n                element = element.find_elements(By.CLASS_NAME, 'foo')\n\n        :rtype: list of WebElement", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode_tuple(data, encoding=None, errors='strict', keep=False,\n                 normalize=False, preserve_dict_class=False, to_str=False):\n    '''\n    \n    '''\n    return tuple(\n        decode_list(data, encoding, errors, keep, normalize,\n                    preserve_dict_class, True, to_str)\n    )", "output": "Decode all string values to Unicode. Optionally use to_str=True to ensure\n    strings are str types and not unicode on Python 2.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_exists(name, path):\n    '''\n    \n    '''\n    cmd = [_get_cmd(), '--display', name]\n    out = __salt__['cmd.run_all'](cmd, python_shell=False)\n\n    if out['retcode'] > 0 and out['stderr'] != '':\n        return False\n\n    return any((line.startswith(path) for line in out['stdout'].splitlines()))", "output": "Check if the given path is an alternative for a name.\n\n    .. versionadded:: 2015.8.4\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' alternatives.check_exists name path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_predictor(self, input_shapes, type_dict=None):\n        \"\"\"\"\"\"\n        shapes = {name: self.arg_params[name].shape for name in self.arg_params}\n        shapes.update(dict(input_shapes))\n        if self._pred_exec is not None:\n            arg_shapes, _, _ = self.symbol.infer_shape(**shapes)\n            assert arg_shapes is not None, \"Incomplete input shapes\"\n            pred_shapes = [x.shape for x in self._pred_exec.arg_arrays]\n            if arg_shapes == pred_shapes:\n                return\n        # for now only use the first device\n        pred_exec = self.symbol.simple_bind(\n            self.ctx[0], grad_req='null', type_dict=type_dict, **shapes)\n        pred_exec.copy_params_from(self.arg_params, self.aux_params)\n\n        _check_arguments(self.symbol)\n        self._pred_exec = pred_exec", "output": "Initialize the predictor module for running prediction.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_create(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_openstack_cloud(auth)\n    kwargs = _clean_kwargs(keep_name=True, **kwargs)\n    return cloud.create_user(**kwargs)", "output": "Create a user\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystoneng.user_create name=user1\n        salt '*' keystoneng.user_create name=user2 password=1234 enabled=False\n        salt '*' keystoneng.user_create name=user3 domain_id=b62e76fbeeff4e8fb77073f591cf211e", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_metrics(self, project, page_size=None, page_token=None):\n        \"\"\"\n        \"\"\"\n        extra_params = {}\n\n        if page_size is not None:\n            extra_params[\"pageSize\"] = page_size\n\n        path = \"/projects/%s/metrics\" % (project,)\n        return page_iterator.HTTPIterator(\n            client=self._client,\n            api_request=self._client._connection.api_request,\n            path=path,\n            item_to_value=_item_to_metric,\n            items_key=\"metrics\",\n            page_token=page_token,\n            extra_params=extra_params,\n        )", "output": "List metrics for the project associated with this client.\n\n        See\n        https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.metrics/list\n\n        :type project: str\n        :param project: ID of the project whose metrics are to be listed.\n\n        :type page_size: int\n        :param page_size: maximum number of metrics to return, If not passed,\n                          defaults to a value set by the API.\n\n        :type page_token: str\n        :param page_token: opaque marker for the next \"page\" of metrics. If not\n                           passed, the API will return the first page of\n                           metrics.\n\n        :rtype: :class:`~google.api_core.page_iterator.Iterator`\n        :returns: Iterator of\n                  :class:`~google.cloud.logging.metric.Metric`\n                  accessible to the current API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_all_settings(domain, store='local'):\n    '''\n    \n    '''\n    return salt.utils.win_lgpo_netsh.get_all_settings(profile=domain,\n                                                      store=store)", "output": "Gets all the properties for the specified profile in the specified store\n\n    .. versionadded:: 2018.3.4\n    .. versionadded:: 2019.2.0\n\n    Args:\n\n        profile (str):\n            The firewall profile to query. Valid options are:\n\n            - domain\n            - public\n            - private\n\n        store (str):\n            The store to use. This is either the local firewall policy or the\n            policy defined by local group policy. Valid options are:\n\n            - lgpo\n            - local\n\n            Default is ``local``\n\n    Returns:\n        dict: A dictionary containing the specified settings\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        # Get all firewall settings for connections on the domain profile\n        salt * win_firewall.get_all_settings domain\n\n        # Get all firewall settings for connections on the domain profile as\n        # defined by local group policy\n        salt * win_firewall.get_all_settings domain lgpo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def checkpoint(self, eager=True):\n        \"\"\"\n        \"\"\"\n        jdf = self._jdf.checkpoint(eager)\n        return DataFrame(jdf, self.sql_ctx)", "output": "Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the\n        logical plan of this DataFrame, which is especially useful in iterative algorithms where the\n        plan may grow exponentially. It will be saved to files inside the checkpoint\n        directory set with L{SparkContext.setCheckpointDir()}.\n\n        :param eager: Whether to checkpoint this DataFrame immediately\n\n        .. note:: Experimental", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_layer(self, x, layer, **kwargs):\n    \"\"\"\n    \"\"\"\n    return self.fprop(x, **kwargs)[layer]", "output": "Return a layer output.\n    :param x: tensor, the input to the network.\n    :param layer: str, the name of the layer to compute.\n    :param **kwargs: dict, extra optional params to pass to self.fprop.\n    :return: the content of layer `layer`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def relevent_issue(issue, after):\n    \"\"\"\"\"\"\n    return (closed_issue(issue, after) and\n            issue_completed(issue) and\n            issue_section(issue))", "output": "Returns True iff this issue is something we should show in the changelog.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _split_index(params):\n    \"\"\"\n    \"\"\"\n    result = {}\n    for key in params:\n        if isinstance(params[key], dict):\n            value = params[key]['_value']\n        else:\n            value = params[key]\n        result[key] = value\n    return result", "output": "Delete index information from params\n\n    Parameters\n    ----------\n    params : dict\n\n    Returns\n    -------\n    result : dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def createEntityParserCtxt(URL, ID, base):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlCreateEntityParserCtxt(URL, ID, base)\n    if ret is None:raise parserError('xmlCreateEntityParserCtxt() failed')\n    return parserCtxt(_obj=ret)", "output": "Create a parser context for an external entity Automatic\n      support for ZLIB/Compress compressed document is provided\n       by default if found at compile-time.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def local_accuracy(X, y, model_generator, method_name):\n    \"\"\" \n    \"\"\"\n\n    def score_map(true, pred):\n        \"\"\" Converts local accuracy from % of standard deviation to numerical scores for coloring.\n        \"\"\"\n\n        v = min(1.0, np.std(pred - true) / (np.std(true) + 1e-8))\n        if v < 1e-6:\n            return 1.0\n        elif v < 0.01:\n            return 0.9\n        elif v < 0.05:\n            return 0.75\n        elif v < 0.1:\n            return 0.6\n        elif v < 0.2:\n            return 0.4\n        elif v < 0.3:\n            return 0.3\n        elif v < 0.5:\n            return 0.2\n        elif v < 0.7:\n            return 0.1\n        else:\n            return 0.0\n    def score_function(X_train, X_test, y_train, y_test, attr_function, trained_model, random_state):\n        return measures.local_accuracy(\n            X_train, y_train, X_test, y_test, attr_function(X_test),\n            model_generator, score_map, trained_model\n        )\n    return None, __score_method(X, y, None, model_generator, score_function, method_name)", "output": "Local Accuracy\n    transform = \"identity\"\n    sort_order = 2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def disk(radius, alias_blur=0.1, dtype=np.float32):\n  \"\"\"\n  \"\"\"\n  if radius <= 8:\n    length = np.arange(-8, 8 + 1)\n    ksize = (3, 3)\n  else:\n    length = np.arange(-radius, radius + 1)\n    ksize = (5, 5)\n  x_axis, y_axis = np.meshgrid(length, length)\n  aliased_disk = np.array((x_axis**2 + y_axis**2) <= radius**2, dtype=dtype)\n  aliased_disk /= np.sum(aliased_disk)\n  # supersample disk to antialias\n  return tfds.core.lazy_imports.cv2.GaussianBlur(\n      aliased_disk, ksize=ksize, sigmaX=alias_blur)", "output": "Generating a Gaussian blurring kernel with disk shape.\n\n  Generating a Gaussian blurring kernel with disk shape using cv2 API.\n\n  Args:\n    radius: integer, radius of blurring kernel.\n    alias_blur: float, standard deviation of Gaussian blurring.\n    dtype: data type of kernel\n\n  Returns:\n    cv2 object of the Gaussian blurring kernel.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_toolbars(self):\r\n        \"\"\"\"\"\"\r\n        value = not self.toolbars_visible\r\n        CONF.set('main', 'toolbars_visible', value)\r\n        if value:\r\n            self.save_visible_toolbars()\r\n        else:\r\n            self.get_visible_toolbars()\r\n\r\n        for toolbar in self.visible_toolbars:\r\n            toolbar.toggleViewAction().setChecked(value)\r\n            toolbar.setVisible(value)\r\n\r\n        self.toolbars_visible = value\r\n        self._update_show_toolbars_action()", "output": "Show/Hides toolbars.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def license_fallback(vendor_dir, sdist_name):\n    \"\"\"\"\"\"\n    libname = libname_from_dir(sdist_name)\n    if libname not in HARDCODED_LICENSE_URLS:\n        raise ValueError('No hardcoded URL for {} license'.format(libname))\n\n    url = HARDCODED_LICENSE_URLS[libname]\n    _, _, name = url.rpartition('/')\n    dest = license_destination(vendor_dir, libname, name)\n    r = requests.get(url, allow_redirects=True)\n    log('Downloading {}'.format(url))\n    r.raise_for_status()\n    dest.write_bytes(r.content)", "output": "Hardcoded license URLs. Check when updating if those are still needed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zrange(key, start, stop, host=None, port=None, db=None, password=None):\n    '''\n    \n    '''\n    server = _connect(host, port, db, password)\n    return server.zrange(key, start, stop)", "output": "Get a range of values from a sorted set in Redis by index\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.zrange foo_sorted 0 10", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def diff(self, periods=1):\n        \"\"\"\n        \n        \"\"\"\n        result = algorithms.diff(com.values_from_object(self), periods)\n        return self._constructor(result, index=self.index).__finalize__(self)", "output": "First discrete difference of element.\n\n        Calculates the difference of a Series element compared with another\n        element in the Series (default is element in previous row).\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for calculating difference, accepts negative\n            values.\n\n        Returns\n        -------\n        Series\n            First differences of the Series.\n\n        See Also\n        --------\n        Series.pct_change: Percent change over given number of periods.\n        Series.shift: Shift index by desired number of periods with an\n            optional time freq.\n        DataFrame.diff: First discrete difference of object.\n\n        Examples\n        --------\n        Difference with previous row\n\n        >>> s = pd.Series([1, 1, 2, 3, 5, 8])\n        >>> s.diff()\n        0    NaN\n        1    0.0\n        2    1.0\n        3    1.0\n        4    2.0\n        5    3.0\n        dtype: float64\n\n        Difference with 3rd previous row\n\n        >>> s.diff(periods=3)\n        0    NaN\n        1    NaN\n        2    NaN\n        3    2.0\n        4    4.0\n        5    6.0\n        dtype: float64\n\n        Difference with following row\n\n        >>> s.diff(periods=-1)\n        0    0.0\n        1   -1.0\n        2   -1.0\n        3   -2.0\n        4   -3.0\n        5    NaN\n        dtype: float64", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ctc_symbol_loss(top_out, targets, model_hparams, vocab_size, weight_fn):\n  \"\"\"\"\"\"\n  del model_hparams, vocab_size  # unused arg\n  logits = top_out\n  with tf.name_scope(\"ctc_loss\", values=[logits, targets]):\n    # For CTC we assume targets are 1d, [batch, length, 1, 1] here.\n    targets_shape = targets.get_shape().as_list()\n    assert len(targets_shape) == 4\n    assert targets_shape[2] == 1\n    assert targets_shape[3] == 1\n    targets = tf.squeeze(targets, axis=[2, 3])\n    logits = tf.squeeze(logits, axis=[2, 3])\n    targets_mask = 1 - tf.to_int32(tf.equal(targets, 0))\n    targets_lengths = tf.reduce_sum(targets_mask, axis=1)\n    sparse_targets = tf.keras.backend.ctc_label_dense_to_sparse(\n        targets, targets_lengths)\n    xent = tf.nn.ctc_loss(\n        sparse_targets,\n        logits,\n        targets_lengths,\n        time_major=False,\n        preprocess_collapse_repeated=False,\n        ctc_merge_repeated=False)\n    weights = weight_fn(targets)\n    return tf.reduce_sum(xent), tf.reduce_sum(weights)", "output": "Compute the CTC loss.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _reduce(self, op, name, axis=0, skipna=True, numeric_only=None,\n                filter_type=None, **kwds):\n        \"\"\"\n        \n        \"\"\"\n        delegate = self._values\n\n        if axis is not None:\n            self._get_axis_number(axis)\n\n        if isinstance(delegate, Categorical):\n            # TODO deprecate numeric_only argument for Categorical and use\n            # skipna as well, see GH25303\n            return delegate._reduce(name, numeric_only=numeric_only, **kwds)\n        elif isinstance(delegate, ExtensionArray):\n            # dispatch to ExtensionArray interface\n            return delegate._reduce(name, skipna=skipna, **kwds)\n        elif is_datetime64_dtype(delegate):\n            # use DatetimeIndex implementation to handle skipna correctly\n            delegate = DatetimeIndex(delegate)\n\n        # dispatch to numpy arrays\n        elif isinstance(delegate, np.ndarray):\n            if numeric_only:\n                raise NotImplementedError('Series.{0} does not implement '\n                                          'numeric_only.'.format(name))\n            with np.errstate(all='ignore'):\n                return op(delegate, skipna=skipna, **kwds)\n\n        # TODO(EA) dispatch to Index\n        # remove once all internals extension types are\n        # moved to ExtensionArrays\n        return delegate._reduce(op=op, name=name, axis=axis, skipna=skipna,\n                                numeric_only=numeric_only,\n                                filter_type=filter_type, **kwds)", "output": "Perform a reduction operation.\n\n        If we have an ndarray as a value, then simply perform the operation,\n        otherwise delegate to the object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_paths(cls, path, bs=64, tfms=(None,None), trn_name='train', val_name='valid', test_name=None, test_with_labels=False, num_workers=8):\n        \"\"\" \n        \"\"\"\n        assert not(tfms[0] is None or tfms[1] is None), \"please provide transformations for your train and validation sets\"\n        trn,val = [folder_source(path, o) for o in (trn_name, val_name)]\n        if test_name:\n            test = folder_source(path, test_name) if test_with_labels else read_dir(path, test_name)\n        else: test = None\n        datasets = cls.get_ds(FilesIndexArrayDataset, trn, val, tfms, path=path, test=test)\n        return cls(path, datasets, bs, num_workers, classes=trn[2])", "output": "Read in images and their labels given as sub-folder names\n\n        Arguments:\n            path: a root path of the data (used for storing trained models, precomputed values, etc)\n            bs: batch size\n            tfms: transformations (for data augmentations). e.g. output of `tfms_from_model`\n            trn_name: a name of the folder that contains training images.\n            val_name:  a name of the folder that contains validation images.\n            test_name:  a name of the folder that contains test images.\n            num_workers: number of workers\n\n        Returns:\n            ImageClassifierData", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_port_def(port_num, proto='tcp'):\n    '''\n    \n    '''\n    try:\n        port_num, _, port_num_proto = port_num.partition('/')\n    except AttributeError:\n        pass\n    else:\n        if port_num_proto:\n            proto = port_num_proto\n    try:\n        if proto.lower() == 'udp':\n            return int(port_num), 'udp'\n    except AttributeError:\n        pass\n    return int(port_num)", "output": "Given a port number and protocol, returns the port definition expected by\n    docker-py. For TCP ports this is simply an integer, for UDP ports this is\n    (port_num, 'udp').\n\n    port_num can also be a string in the format 'port_num/udp'. If so, the\n    \"proto\" argument will be ignored. The reason we need to be able to pass in\n    the protocol separately is because this function is sometimes invoked on\n    data derived from a port range (e.g. '2222-2223/udp'). In these cases the\n    protocol has already been stripped off and the port range resolved into the\n    start and end of the range, and get_port_def() is invoked once for each\n    port number in that range. So, rather than munge udp ports back into\n    strings before passing them to this function, the function will see if it\n    has a string and use the protocol from it if present.\n\n    This function does not catch the TypeError or ValueError which would be\n    raised if the port number is non-numeric. This function either needs to be\n    run on known good input, or should be run within a try/except that catches\n    these two exceptions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_package(package,\n                ignore_check=False,\n                prevent_pending=False,\n                image=None,\n                restart=False):\n    '''\n    \n    '''\n    cmd = ['DISM',\n           '/Quiet',\n           '/Image:{0}'.format(image) if image else '/Online',\n           '/Add-Package',\n           '/PackagePath:{0}'.format(package)]\n\n    if ignore_check:\n        cmd.append('/IgnoreCheck')\n    if prevent_pending:\n        cmd.append('/PreventPending')\n    if not restart:\n        cmd.append('/NoRestart')\n\n    return __salt__['cmd.run_all'](cmd)", "output": "Install a package using DISM\n\n    Args:\n        package (str):\n            The package to install. Can be a .cab file, a .msu file, or a folder\n\n            .. note::\n                An `.msu` package is supported only when the target image is\n                offline, either mounted or applied.\n\n        ignore_check (Optional[bool]):\n            Skip installation of the package if the applicability checks fail\n\n        prevent_pending (Optional[bool]):\n            Skip the installation of the package if there are pending online\n            actions\n\n        image (Optional[str]):\n            The path to the root directory of an offline Windows image. If\n            ``None`` is passed, the running operating system is targeted.\n            Default is None.\n\n        restart (Optional[bool]):\n            Reboot the machine if required by the install\n\n    Returns:\n        dict: A dictionary containing the results of the command\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' dism.add_package C:\\\\Packages\\\\package.cab", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resize_to_contents(self):\r\n        \"\"\"\"\"\"\r\n        QApplication.setOverrideCursor(QCursor(Qt.WaitCursor))\r\n        self.resizeColumnsToContents()\r\n        self.model().fetch_more(columns=True)\r\n        self.resizeColumnsToContents()\r\n        QApplication.restoreOverrideCursor()", "output": "Resize cells to contents", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_cpv(cp, installed=True):\n    '''\n    \n    '''\n    if installed:\n        return _get_portage().db[portage.root]['vartree'].dep_bestmatch(cp)\n    else:\n        return _porttree().dep_bestmatch(cp)", "output": "add version to category/package\n    @cp - name of package in format category/name\n    @installed - boolean value, if False, function returns cpv\n    for latest available package", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_lifecycle(self, policy=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_ilm\", \"policy\", policy), params=params\n        )", "output": "`<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-get-lifecycle.html>`_\n\n        :arg policy: The name of the index lifecycle policy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_interval(value):\n    '''\n    \n    '''\n    match = _INTERVAL_REGEX.match(six.text_type(value))\n    if match is None:\n        raise ValueError('invalid time interval: \\'{0}\\''.format(value))\n\n    result = 0\n    resolution = None\n    for name, multiplier in [('second', 1),\n                             ('minute', 60),\n                             ('hour', 60 * 60),\n                             ('day', 60 * 60 * 24),\n                             ('week', 60 * 60 * 24 * 7)]:\n        if match.group(name) is not None:\n            result += float(match.group(name)) * multiplier\n            if resolution is None:\n                resolution = multiplier\n\n    return result, resolution, match.group('modifier')", "output": "Convert an interval string like 1w3d6h into the number of seconds, time\n    resolution (1 unit of the smallest specified time unit) and the modifier(\n    '+', '-', or '').\n        w = week\n        d = day\n        h = hour\n        m = minute\n        s = second", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_range(self, ignore_blank_lines=True):\n        \"\"\"\n        \n        \"\"\"\n        ref_lvl = self.trigger_level\n        first_line = self._trigger.blockNumber()\n        block = self._trigger.next()\n        last_line = block.blockNumber()\n        lvl = self.scope_level\n        if ref_lvl == lvl:  # for zone set programmatically such as imports\n                            # in pyqode.python\n            ref_lvl -= 1\n        while (block.isValid() and\n                TextBlockHelper.get_fold_lvl(block) > ref_lvl):\n            last_line = block.blockNumber()\n            block = block.next()\n\n        if ignore_blank_lines and last_line:\n            block = block.document().findBlockByNumber(last_line)\n            while block.blockNumber() and block.text().strip() == '':\n                block = block.previous()\n                last_line = block.blockNumber()\n        return first_line, last_line", "output": "Gets the fold region range (start and end line).\n\n        .. note:: Start line do no encompass the trigger line.\n\n        :param ignore_blank_lines: True to ignore blank lines at the end of the\n            scope (the method will rewind to find that last meaningful block\n            that is part of the fold scope).\n        :returns: tuple(int, int)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def visibility_changed(self, enable):\r\n        \"\"\"\"\"\"\r\n        super(SpyderPluginWidget, self).visibility_changed(enable)\r\n        if enable and not self.pydocbrowser.is_server_running():\r\n            self.pydocbrowser.initialize()", "output": "DockWidget visibility has changed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self, asynchronous=False):\n        '''\n        \n        '''\n        log.debug('Process Manager starting!')\n        appendproctitle(self.name)\n\n        # make sure to kill the subprocesses if the parent is killed\n        if signal.getsignal(signal.SIGTERM) is signal.SIG_DFL:\n            # There are no SIGTERM handlers installed, install ours\n            signal.signal(signal.SIGTERM, self.kill_children)\n        if signal.getsignal(signal.SIGINT) is signal.SIG_DFL:\n            # There are no SIGINT handlers installed, install ours\n            signal.signal(signal.SIGINT, self.kill_children)\n\n        while True:\n            log.trace('Process manager iteration')\n            try:\n                # in case someone died while we were waiting...\n                self.check_children()\n                # The event-based subprocesses management code was removed from here\n                # because os.wait() conflicts with the subprocesses management logic\n                # implemented in `multiprocessing` package. See #35480 for details.\n                if asynchronous:\n                    yield gen.sleep(10)\n                else:\n                    time.sleep(10)\n                if not self._process_map:\n                    break\n            # OSError is raised if a signal handler is called (SIGTERM) during os.wait\n            except OSError:\n                break\n            except IOError as exc:\n                # IOError with errno of EINTR (4) may be raised\n                # when using time.sleep() on Windows.\n                if exc.errno != errno.EINTR:\n                    raise\n                break", "output": "Load and start all available api modules", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_routes(iface, **settings):\n    '''\n    \n    '''\n\n    template = 'rh6_route_eth.jinja'\n    try:\n        if int(__grains__['osrelease'][0]) < 6:\n            template = 'route_eth.jinja'\n    except ValueError:\n        pass\n    log.debug('Template name: %s', template)\n\n    opts = _parse_routes(iface, settings)\n    log.debug('Opts: \\n %s', opts)\n    try:\n        template = JINJA.get_template(template)\n    except jinja2.exceptions.TemplateNotFound:\n        log.error('Could not load template %s', template)\n        return ''\n    opts6 = []\n    opts4 = []\n    for route in opts['routes']:\n        ipaddr = route['ipaddr']\n        if salt.utils.validate.net.ipv6_addr(ipaddr):\n            opts6.append(route)\n        else:\n            opts4.append(route)\n    log.debug(\"IPv4 routes:\\n%s\", opts4)\n    log.debug(\"IPv6 routes:\\n%s\", opts6)\n\n    routecfg = template.render(routes=opts4, iface=iface)\n    routecfg6 = template.render(routes=opts6, iface=iface)\n\n    if settings['test']:\n        routes = _read_temp(routecfg)\n        routes.extend(_read_temp(routecfg6))\n        return routes\n\n    _write_file_iface(iface, routecfg, _RH_NETWORK_SCRIPT_DIR, 'route-{0}')\n    _write_file_iface(iface, routecfg6, _RH_NETWORK_SCRIPT_DIR, 'route6-{0}')\n\n    path = os.path.join(_RH_NETWORK_SCRIPT_DIR, 'route-{0}'.format(iface))\n    path6 = os.path.join(_RH_NETWORK_SCRIPT_DIR, 'route6-{0}'.format(iface))\n\n    routes = _read_file(path)\n    routes.extend(_read_file(path6))\n    return routes", "output": "Build a route script for a network interface.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ip.build_routes eth0 <settings>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_blade():\n    '''\n    \n\n  '''\n\n    try:\n        blade_name = __opts__['pure_tags']['fb'].get('san_ip')\n        api_token = __opts__['pure_tags']['fb'].get('api_token')\n        if blade_name and api:\n            blade = PurityFb(blade_name)\n            blade.disable_verify_ssl()\n    except (KeyError, NameError, TypeError):\n        try:\n            blade_name = os.environ.get('PUREFB_IP')\n            api_token = os.environ.get('PUREFB_API')\n            if blade_name:\n                blade = PurityFb(blade_name)\n                blade.disable_verify_ssl()\n        except (ValueError, KeyError, NameError):\n            try:\n                api_token = __pillar__['PUREFB_API']\n                blade = PurityFb(__pillar__['PUREFB_IP'])\n                blade.disable_verify_ssl()\n            except (KeyError, NameError):\n                raise CommandExecutionError('No Pure Storage FlashBlade credentials found.')\n    try:\n        blade.login(api_token)\n    except Exception:\n        raise CommandExecutionError('Pure Storage FlashBlade authentication failed.')\n    return blade", "output": "Get Pure Storage FlasBlade configuration\n\n    1) From the minion config\n        pure_tags:\n          fb:\n            san_ip: management vip or hostname for the FlashBlade\n            api_token: A valid api token for the FlashBlade being managed\n    2) From environment (PUREFB_IP and PUREFB_API)\n    3) From the pillar (PUREFB_IP and PUREFB_API)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def (self, command):\r\n        \"\"\"\"\"\"\r\n        if self.profile:\r\n            # Simple profiling test\r\n            t0 = time()\r\n            for _ in range(10):\r\n                self.execute_command(command)\r\n            self.insert_text(u\"\\n<\u0394t>=%dms\\n\" % (1e2*(time()-t0)))\r\n            self.new_prompt(self.interpreter.p1)\r\n        else:\r\n            self.execute_command(command)\r\n        self.__flush_eventqueue()", "output": "on_enter", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def default(ruby=None, runas=None):\n    '''\n    \n    '''\n    if ruby:\n        _rbenv_exec(['global', ruby], runas=runas)\n        return True\n    else:\n        ret = _rbenv_exec(['global'], runas=runas)\n        return '' if ret is False else ret.strip()", "output": "Returns or sets the currently defined default ruby\n\n    ruby\n        The version to set as the default. Should match one of the versions\n        listed by :py:func:`rbenv.versions <salt.modules.rbenv.versions>`.\n        Leave blank to return the current default.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rbenv.default\n        salt '*' rbenv.default 2.0.0-p0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def loadSGMLSuperCatalog(filename):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlLoadSGMLSuperCatalog(filename)\n    if ret is None:raise treeError('xmlLoadSGMLSuperCatalog() failed')\n    return catalog(_obj=ret)", "output": "Load an SGML super catalog. It won't expand CATALOG or\n      DELEGATE references. This is only needed for manipulating\n      SGML Super Catalogs like adding and removing CATALOG or\n       DELEGATE entries.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_excludes(self, excludes, field, value):\n        \"\"\"  \"\"\"\n        if isinstance(excludes, Hashable):\n            excludes = [excludes]\n\n        # Save required field to be checked latter\n        if 'required' in self.schema[field] and self.schema[field]['required']:\n            self._unrequired_by_excludes.add(field)\n        for exclude in excludes:\n            if (exclude in self.schema and\n                'required' in self.schema[exclude] and\n                    self.schema[exclude]['required']):\n\n                self._unrequired_by_excludes.add(exclude)\n\n        if [True for key in excludes if key in self.document]:\n            # Wrap each field in `excludes` list between quotes\n            exclusion_str = ', '.join(\"'{0}'\"\n                                      .format(word) for word in excludes)\n            self._error(field, errors.EXCLUDES_FIELD, exclusion_str)", "output": "{'type': ('hashable', 'list'),\n             'schema': {'type': 'hashable'}}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transform_array_to_list(array):\n    ''' \n\n    '''\n    if (array.dtype.kind in ('u', 'i', 'f') and (~np.isfinite(array)).any()):\n        transformed = array.astype('object')\n        transformed[np.isnan(array)] = 'NaN'\n        transformed[np.isposinf(array)] = 'Infinity'\n        transformed[np.isneginf(array)] = '-Infinity'\n        return transformed.tolist()\n    elif (array.dtype.kind == 'O' and pd and pd.isnull(array).any()):\n        transformed = array.astype('object')\n        transformed[pd.isnull(array)] = 'NaN'\n        return transformed.tolist()\n    return array.tolist()", "output": "Transforms a NumPy array into a list of values\n\n    Args:\n        array (np.nadarray) : the NumPy array series to transform\n\n    Returns:\n        list or dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def out_filename(template, n_val, mode):\n    \"\"\"\"\"\"\n    return '{0}_{1}_{2}.cpp'.format(template.name, n_val, mode.identifier)", "output": "Determine the output filename", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _maybe_pandas_data(data, feature_names, feature_types):\n    \"\"\"  \"\"\"\n\n    if not isinstance(data, DataFrame):\n        return data, feature_names, feature_types\n\n    data_dtypes = data.dtypes\n    if not all(dtype.name in PANDAS_DTYPE_MAPPER for dtype in data_dtypes):\n        bad_fields = [data.columns[i] for i, dtype in\n                      enumerate(data_dtypes) if dtype.name not in PANDAS_DTYPE_MAPPER]\n\n        msg = \"\"\"DataFrame.dtypes for data must be int, float or bool.\n                Did not expect the data types in fields \"\"\"\n        raise ValueError(msg + ', '.join(bad_fields))\n\n    if feature_names is None:\n        if isinstance(data.columns, MultiIndex):\n            feature_names = [\n                ' '.join([str(x) for x in i])\n                for i in data.columns\n            ]\n        else:\n            feature_names = data.columns.format()\n\n    if feature_types is None:\n        feature_types = [PANDAS_DTYPE_MAPPER[dtype.name] for dtype in data_dtypes]\n\n    data = data.values.astype('float')\n\n    return data, feature_names, feature_types", "output": "Extract internal data from pd.DataFrame for DMatrix data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, resource):\n        \"\"\"\n        \"\"\"\n        this = cls(None)\n\n        # Convert from millis-from-epoch to timestamp well-known type.\n        # TODO: Remove this hack once CL 238585470 hits prod.\n        resource = copy.deepcopy(resource)\n        for training_run in resource.get(\"trainingRuns\", ()):\n            start_time = training_run.get(\"startTime\")\n            if not start_time or \"-\" in start_time:  # Already right format?\n                continue\n            start_time = datetime_helpers.from_microseconds(1e3 * float(start_time))\n            training_run[\"startTime\"] = datetime_helpers.to_rfc3339(start_time)\n\n        this._proto = json_format.ParseDict(resource, types.Model())\n        for key in six.itervalues(cls._PROPERTY_TO_API_FIELD):\n            # Leave missing keys unset. This allows us to use setdefault in the\n            # getters where we want a default value other than None.\n            if key in resource:\n                this._properties[key] = resource[key]\n        return this", "output": "Factory: construct a model resource given its API representation\n\n        Args:\n            resource (Dict[str, object]):\n                Model resource representation from the API\n\n        Returns:\n            google.cloud.bigquery.model.Model: Model parsed from ``resource``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transpose(self, *args, **kwargs):\n        \"\"\"\n        \"\"\"\n        new_data = self.data.transpose(*args, **kwargs)\n        # Switch the index and columns and transpose the\n        new_manager = self.__constructor__(new_data, self.columns, self.index)\n        # It is possible that this is already transposed\n        new_manager._is_transposed = self._is_transposed ^ 1\n        return new_manager", "output": "Transposes this DataManager.\n\n        Returns:\n            Transposed new DataManager.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_data_value(datastore, path, data):\n    '''\n    \n    '''\n    if isinstance(path, six.string_types):\n        path = '/'.split(path)\n    return _proxy_cmd('set_data_value', datastore, path, data)", "output": "Set a data entry in a datastore\n\n    :param datastore: The datastore, e.g. running, operational.\n        One of the NETCONF store IETF types\n    :type  datastore: :class:`DatastoreType` (``str`` enum).\n\n    :param path: The device path to set the value at,\n        a list of element names in order, / separated\n    :type  path: ``list``, ``str`` OR ``tuple``\n\n    :param data: The new value at the given path\n    :type  data: ``dict``\n\n    :rtype: ``bool``\n    :return: ``True`` if successful, otherwise error.\n\n    .. code-block:: bash\n\n        salt cisco-nso cisconso.set_data_value running 'devices/ex0/routes' 10.0.0.20/24", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(name, gid=None, system=False, root=None):\n    '''\n    \n    '''\n    cmd = 'mkgroup '\n    if system and root is not None:\n        cmd += '-a '\n\n    if gid:\n        cmd += 'id={0} '.format(gid)\n\n    cmd += name\n\n    ret = __salt__['cmd.run_all'](cmd, python_shell=False)\n\n    return not ret['retcode']", "output": "Add the specified group\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' group.add foo 3456", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_empty_result(self):\n        \"\"\"\n        \n        \"\"\"\n\n        # we are not asked to reduce or infer reduction\n        # so just return a copy of the existing object\n        if self.result_type not in ['reduce', None]:\n            return self.obj.copy()\n\n        # we may need to infer\n        reduce = self.result_type == 'reduce'\n\n        from pandas import Series\n        if not reduce:\n\n            EMPTY_SERIES = Series([])\n            try:\n                r = self.f(EMPTY_SERIES, *self.args, **self.kwds)\n                reduce = not isinstance(r, Series)\n            except Exception:\n                pass\n\n        if reduce:\n            return self.obj._constructor_sliced(np.nan, index=self.agg_axis)\n        else:\n            return self.obj.copy()", "output": "we have an empty result; at least 1 axis is 0\n\n        we will try to apply the function to an empty\n        series in order to see if this is a reduction function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def users(self, limit=None, after=None):\n        \"\"\"\n        \"\"\"\n\n        if self.custom_emoji:\n            emoji = '{0.name}:{0.id}'.format(self.emoji)\n        else:\n            emoji = self.emoji\n\n        if limit is None:\n            limit = self.count\n\n        return ReactionIterator(self.message, emoji, limit, after)", "output": "Returns an :class:`AsyncIterator` representing the users that have reacted to the message.\n\n        The ``after`` parameter must represent a member\n        and meet the :class:`abc.Snowflake` abc.\n\n        Examples\n        ---------\n\n        Usage ::\n\n            # I do not actually recommend doing this.\n            async for user in reaction.users():\n                await channel.send('{0} has reacted with {1.emoji}!'.format(user, reaction))\n\n        Flattening into a list: ::\n\n            users = await reaction.users().flatten()\n            # users is now a list...\n            winner = random.choice(users)\n            await channel.send('{} has won the raffle.'.format(winner))\n\n        Parameters\n        ------------\n        limit: :class:`int`\n            The maximum number of results to return.\n            If not provided, returns all the users who\n            reacted to the message.\n        after: :class:`abc.Snowflake`\n            For pagination, reactions are sorted by member.\n\n        Raises\n        --------\n        HTTPException\n            Getting the users for the reaction failed.\n\n        Yields\n        --------\n        Union[:class:`User`, :class:`Member`]\n            The member (if retrievable) or the user that has reacted\n            to this message. The case where it can be a :class:`Member` is\n            in a guild message context. Sometimes it can be a :class:`User`\n            if the member has left the guild.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xmoe2_v1_l4k_global_only():\n  \"\"\"\"\"\"\n  hparams = xmoe2_v1_l4k()\n  hparams.decoder_layers = [\n      \"att\" if l == \"local_att\" else l for l in hparams.decoder_layers]\n  return hparams", "output": "With sequence length 4096.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def role_list():\n    '''\n    \n    '''\n    roles = {}\n\n    ## read user_attr file (user:qualifier:res1:res2:attr)\n    with salt.utils.files.fopen('/etc/user_attr', 'r') as user_attr:\n        for role in user_attr:\n            role = salt.utils.stringutils.to_unicode(role)\n            role = role.split(':')\n\n            # skip comments and non complaint lines\n            if len(role) != 5:\n                continue\n\n            # parse attr\n            attrs = {}\n            for attr in role[4].split(';'):\n                attr_key, attr_val = attr.split('=')\n                if attr_key in ['auths', 'profiles', 'roles']:\n                    attrs[attr_key] = attr_val.split(',')\n                else:\n                    attrs[attr_key] = attr_val\n            role[4] = attrs\n\n            # add role info to dict\n            if 'type' in role[4] and role[4]['type'] == 'role':\n                del role[4]['type']\n                roles[role[0]] = role[4]\n\n    return roles", "output": "List all available roles\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rbac.role_list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_message(\n        self, message: Union[str, bytes], binary: bool = False\n    ) -> \"Future[None]\":\n        \"\"\"\"\"\"\n        if binary:\n            opcode = 0x2\n        else:\n            opcode = 0x1\n        message = tornado.escape.utf8(message)\n        assert isinstance(message, bytes)\n        self._message_bytes_out += len(message)\n        flags = 0\n        if self._compressor:\n            message = self._compressor.compress(message)\n            flags |= self.RSV1\n        # For historical reasons, write methods in Tornado operate in a semi-synchronous\n        # mode in which awaiting the Future they return is optional (But errors can\n        # still be raised). This requires us to go through an awkward dance here\n        # to transform the errors that may be returned while presenting the same\n        # semi-synchronous interface.\n        try:\n            fut = self._write_frame(True, opcode, message, flags=flags)\n        except StreamClosedError:\n            raise WebSocketClosedError()\n\n        async def wrapper() -> None:\n            try:\n                await fut\n            except StreamClosedError:\n                raise WebSocketClosedError()\n\n        return asyncio.ensure_future(wrapper())", "output": "Sends the given message to the client of this Web Socket.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def path_is_known_executable(path):\n    # type: (vistir.compat.Path) -> bool\n    \"\"\"\n    \n    \"\"\"\n\n    return (\n        path_is_executable(path)\n        or os.access(str(path), os.R_OK)\n        and path.suffix in KNOWN_EXTS\n    )", "output": "Returns whether a given path is a known executable from known executable extensions\n    or has the executable bit toggled.\n\n    :param path: The path to the target executable.\n    :type path: :class:`~vistir.compat.Path`\n    :return: True if the path has chmod +x, or is a readable, known executable extension.\n    :rtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def destroy(name):\n    '''\n    \n    '''\n    node = show_instance(name, call='action')\n    params = {'SUBID': node['SUBID']}\n    result = _query('server/destroy', method='POST', decode=False, data=_urlencode(params))\n\n    # The return of a destroy call is empty in the case of a success.\n    # Errors are only indicated via HTTP status code. Status code 200\n    # effetively therefore means \"success\".\n    if result.get('body') == '' and result.get('text') == '':\n        return True\n    return result", "output": "Remove a node from Vultr", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_collection_in_tower(self, key):\n        \"\"\"\n        \n        \"\"\"\n        new = tf.get_collection(key)\n        old = set(self.original.get(key, []))\n        # persist the order in new\n        return [x for x in new if x not in old]", "output": "Get items from this collection that are added in the current tower.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def buffer(self, frame):\n        \"\"\"\"\"\"\n        frame.buffer = self.temporary_identifier()\n        self.writeline('%s = []' % frame.buffer)", "output": "Enable buffering for the frame from that point onwards.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hvals(key, host=None, port=None, db=None, password=None):\n    '''\n    \n    '''\n    server = _connect(host, port, db, password)\n    return server.hvals(key)", "output": "Return all the values in a hash.\n\n    .. versionadded:: 2017.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.hvals foo_hash bar_field1 bar_value1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def versions():\n    '''\n    \n    '''\n    ret = {}\n    client = salt.client.get_local_client(__opts__['conf_file'])\n    try:\n        minions = client.cmd('*', 'test.version', timeout=__opts__['timeout'])\n    except SaltClientError as client_error:\n        print(client_error)\n        return ret\n\n    labels = {\n        -2: 'Minion offline',\n        -1: 'Minion requires update',\n        0: 'Up to date',\n        1: 'Minion newer than master',\n        2: 'Master',\n    }\n\n    version_status = {}\n\n    master_version = salt.version.__saltstack_version__\n\n    for minion in minions:\n        if not minions[minion]:\n            minion_version = False\n            ver_diff = -2\n        else:\n            minion_version = salt.version.SaltStackVersion.parse(minions[minion])\n            ver_diff = salt.utils.compat.cmp(minion_version, master_version)\n\n        if ver_diff not in version_status:\n            version_status[ver_diff] = {}\n        if minion_version:\n            version_status[ver_diff][minion] = minion_version.string\n        else:\n            version_status[ver_diff][minion] = minion_version\n\n    # Add version of Master to output\n    version_status[2] = master_version.string\n\n    for key in version_status:\n        if key == 2:\n            ret[labels[key]] = version_status[2]\n        else:\n            for minion in sorted(version_status[key]):\n                ret.setdefault(labels[key], {})[minion] = version_status[key][minion]\n    return ret", "output": "Check the version of active minions\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run manage.versions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_ftp_proxy(server,\n                  port,\n                  user=None,\n                  password=None,\n                  network_service=\"Ethernet\",\n                  bypass_hosts=None):\n    '''\n    \n    '''\n    if __grains__['os'] == 'Windows':\n        return _set_proxy_windows(server=server,\n                                  port=port,\n                                  types=['ftp'],\n                                  bypass_hosts=bypass_hosts)\n\n    return _set_proxy_osx(cmd_function=\"setftpproxy\",\n                          server=server,\n                          port=port,\n                          user=user,\n                          password=password,\n                          network_service=network_service)", "output": "Sets the ftp proxy settings\n\n    server\n        The proxy server to use\n\n    port\n        The port used by the proxy server\n\n    user\n        The username to use for the proxy server if required\n\n    password\n        The password to use if required by the server\n\n    network_service\n        The network service to apply the changes to, this only necessary on\n        macOS\n\n    bypass_hosts\n        The hosts that are allowed to by pass the proxy. Only used on Windows\n        for other OS's use set_proxy_bypass to edit the bypass hosts.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' proxy.set_ftp_proxy example.com 1080 user=proxy_user password=proxy_pass network_service=Ethernet", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def no_cache_dir_callback(option, opt, value, parser):\n    \"\"\"\n    \n    \"\"\"\n    # The value argument will be None if --no-cache-dir is passed via the\n    # command-line, since the option doesn't accept arguments.  However,\n    # the value can be non-None if the option is triggered e.g. by an\n    # environment variable, like PIP_NO_CACHE_DIR=true.\n    if value is not None:\n        # Then parse the string value to get argument error-checking.\n        try:\n            strtobool(value)\n        except ValueError as exc:\n            raise_option_error(parser, option=option, msg=str(exc))\n\n    # Originally, setting PIP_NO_CACHE_DIR to a value that strtobool()\n    # converted to 0 (like \"false\" or \"no\") caused cache_dir to be disabled\n    # rather than enabled (logic would say the latter).  Thus, we disable\n    # the cache directory not just on values that parse to True, but (for\n    # backwards compatibility reasons) also on values that parse to False.\n    # In other words, always set it to False if the option is provided in\n    # some (valid) form.\n    parser.values.cache_dir = False", "output": "Process a value provided for the --no-cache-dir option.\n\n    This is an optparse.Option callback for the --no-cache-dir option.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_installed(self, pkgname):\n        \"\"\"\n        \"\"\"\n\n        return any(d for d in self.get_distributions() if d.project_name == pkgname)", "output": "Given a package name, returns whether it is installed in the environment\n\n        :param str pkgname: The name of a package\n        :return: Whether the supplied package is installed in the environment\n        :rtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_cookies(self, cookies: Optional[LooseCookies]) -> None:\n        \"\"\"\"\"\"\n        if not cookies:\n            return\n\n        c = SimpleCookie()\n        if hdrs.COOKIE in self.headers:\n            c.load(self.headers.get(hdrs.COOKIE, ''))\n            del self.headers[hdrs.COOKIE]\n\n        if isinstance(cookies, Mapping):\n            iter_cookies = cookies.items()\n        else:\n            iter_cookies = cookies  # type: ignore\n        for name, value in iter_cookies:\n            if isinstance(value, Morsel):\n                # Preserve coded_value\n                mrsl_val = value.get(value.key, Morsel())\n                mrsl_val.set(value.key, value.value, value.coded_value)  # type: ignore  # noqa\n                c[name] = mrsl_val\n            else:\n                c[name] = value  # type: ignore\n\n        self.headers[hdrs.COOKIE] = c.output(header='', sep=';').strip()", "output": "Update request cookies header.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deep_update(original, new_dict, new_keys_allowed, whitelist):\n    \"\"\"\n    \"\"\"\n    for k, value in new_dict.items():\n        if k not in original:\n            if not new_keys_allowed:\n                raise Exception(\"Unknown config parameter `{}` \".format(k))\n        if isinstance(original.get(k), dict):\n            if k in whitelist:\n                deep_update(original[k], value, True, [])\n            else:\n                deep_update(original[k], value, new_keys_allowed, [])\n        else:\n            original[k] = value\n    return original", "output": "Updates original dict with values from new_dict recursively.\n    If new key is introduced in new_dict, then if new_keys_allowed is not\n    True, an error will be thrown. Further, for sub-dicts, if the key is\n    in the whitelist, then new subkeys can be introduced.\n\n    Args:\n        original (dict): Dictionary with default values.\n        new_dict (dict): Dictionary with values to be updated\n        new_keys_allowed (bool): Whether new keys are allowed.\n        whitelist (list): List of keys that correspond to dict values\n            where new subkeys can be introduced. This is only at\n            the top level.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def visual(title, X, activation):\n    '''\n    '''\n    assert len(X.shape) == 4\n\n    X = X.transpose((0, 2, 3, 1))\n    if activation == 'sigmoid':\n        X = np.clip((X)*(255.0), 0, 255).astype(np.uint8)\n    elif activation == 'tanh':\n        X = np.clip((X+1.0)*(255.0/2.0), 0, 255).astype(np.uint8)\n    n = np.ceil(np.sqrt(X.shape[0]))\n    buff = np.zeros((int(n*X.shape[1]), int(n*X.shape[2]), int(X.shape[3])), dtype=np.uint8)\n    for i, img in enumerate(X):\n        fill_buf(buff, i, img, X.shape[1:3])\n    cv2.imwrite('%s.jpg' % (title), buff)", "output": "create a grid of images and save it as a final image\n    title : grid image name\n    X : array of images", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def networks(names=None, ids=None):\n    '''\n    \n    '''\n    if names is not None:\n        names = __utils__['args.split_input'](names)\n    if ids is not None:\n        ids = __utils__['args.split_input'](ids)\n\n    response = _client_wrapper('networks', names=names, ids=ids)\n    # Work around https://github.com/docker/docker-py/issues/1775\n    for idx, netinfo in enumerate(response):\n        try:\n            containers = inspect_network(netinfo['Id'])['Containers']\n        except Exception:\n            continue\n        else:\n            if containers:\n                response[idx]['Containers'] = containers\n\n    return response", "output": ".. versionchanged:: 2017.7.0\n        The ``names`` and ``ids`` can be passed as a comma-separated list now,\n        as well as a Python list.\n    .. versionchanged:: 2018.3.0\n        The ``Containers`` key for each network is no longer always empty.\n\n    List existing networks\n\n    names\n        Filter by name\n\n    ids\n        Filter by id\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion docker.networks names=network-web\n        salt myminion docker.networks ids=1f9d2454d0872b68dd9e8744c6e7a4c66b86f10abaccc21e14f7f014f729b2bc", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_transition_any (self, state, action=None, next_state=None):\n\n        ''' '''\n\n        if next_state is None:\n            next_state = state\n        self.state_transitions_any [state] = (action, next_state)", "output": "This adds a transition that associates:\n\n                (current_state) --> (action, next_state)\n\n        That is, any input symbol will match the current state.\n        The process() method checks the \"any\" state associations after it first\n        checks for an exact match of (input_symbol, current_state).\n\n        The action may be set to None in which case the process() method will\n        ignore the action and only set the next_state. The next_state may be\n        set to None in which case the current state will be unchanged.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def connected(name, verbose=False):\n    '''\n    \n    '''\n    containers = inspect_network(name).get('Containers', {})\n    ret = {}\n    for cid, cinfo in six.iteritems(containers):\n        # The Containers dict is keyed by container ID, but we want the results\n        # to be keyed by container name, so we need to pop off the Name and\n        # then add the Id key to the cinfo dict.\n        try:\n            name = cinfo.pop('Name')\n        except (KeyError, AttributeError):\n            # Should never happen\n            log.warning(\n                '\\'Name\\' key not present in container definition for '\n                'container ID \\'%s\\' within inspect results for Docker '\n                'network \\'%s\\'. Full container definition: %s',\n                cid, name, cinfo\n            )\n            continue\n        else:\n            cinfo['Id'] = cid\n            ret[name] = cinfo\n    if not verbose:\n        return list(ret)\n    return ret", "output": ".. versionadded:: 2018.3.0\n\n    Return a list of running containers attached to the specified network\n\n    name\n        Network name\n\n    verbose : False\n        If ``True``, return extended info about each container (IP\n        configuration, etc.)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion docker.connected net_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def call(self, module, *args, **kwargs):\n        '''\n        \n        '''\n\n        module = self._resolver.load_module(module)\n        if not hasattr(module, 'main'):\n            raise CommandExecutionError('This module is not callable '\n                                        '(see \"ansible.help {0}\")'.format(module.__name__.replace('ansible.modules.',\n                                                                                                  '')))\n        if args:\n            kwargs['_raw_params'] = ' '.join(args)\n        js_args = str('{{\"ANSIBLE_MODULE_ARGS\": {args}}}')  # future lint: disable=blacklisted-function\n        js_args = js_args.format(args=salt.utils.json.dumps(kwargs))\n\n        proc_out = salt.utils.timed_subprocess.TimedProc(\n            [\"echo\", \"{0}\".format(js_args)],\n            stdout=subprocess.PIPE, timeout=self.timeout)\n        proc_out.run()\n        proc_exc = salt.utils.timed_subprocess.TimedProc(\n            ['python', module.__file__],\n            stdin=proc_out.stdout, stdout=subprocess.PIPE, timeout=self.timeout)\n        proc_exc.run()\n\n        try:\n            out = salt.utils.json.loads(proc_exc.stdout)\n        except ValueError as ex:\n            out = {'Error': (proc_exc.stderr and (proc_exc.stderr + '.') or six.text_type(ex))}\n            if proc_exc.stdout:\n                out['Given JSON output'] = proc_exc.stdout\n            return out\n\n        if 'invocation' in out:\n            del out['invocation']\n\n        out['timeout'] = self.timeout\n\n        return out", "output": "Call an Ansible module by invoking it.\n        :param module: the name of the module.\n        :param args: Arguments to the module\n        :param kwargs: keywords to the module\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def serializable_value(self, obj):\n        ''' \n\n        '''\n        value = self.__get__(obj, obj.__class__)\n        return self.property.serialize_value(value)", "output": "Produce the value as it should be serialized.\n\n        Sometimes it is desirable for the serialized value to differ from\n        the ``__get__`` in order for the ``__get__`` value to appear simpler\n        for user or developer convenience.\n\n        Args:\n            obj (HasProps) : the object to get the serialized attribute for\n\n        Returns:\n            JSON-like", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_target(self, name, current, total):\n        \"\"\"\"\"\"\n        self.refresh(self._bar(name, current, total))", "output": "Updates progress bar for a specified target.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tf():\n  \"\"\"\n  \"\"\"\n  try:\n    from tensorboard.compat import notf  # pylint: disable=g-import-not-at-top\n  except ImportError:\n    try:\n      import tensorflow  # pylint: disable=g-import-not-at-top\n      return tensorflow\n    except ImportError:\n      pass\n  from tensorboard.compat import tensorflow_stub  # pylint: disable=g-import-not-at-top\n  return tensorflow_stub", "output": "Provide the root module of a TF-like API for use within TensorBoard.\n\n  By default this is equivalent to `import tensorflow as tf`, but it can be used\n  in combination with //tensorboard/compat:tensorflow (to fall back to a stub TF\n  API implementation if the real one is not available) or with\n  //tensorboard/compat:no_tensorflow (to force unconditional use of the stub).\n\n  Returns:\n    The root module of a TF-like API, if available.\n\n  Raises:\n    ImportError: if a TF-like API is not available.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _query_iterator(self, result, chunksize, columns, coerce_float=True,\n                        parse_dates=None):\n        \"\"\"\"\"\"\n\n        while True:\n            data = result.fetchmany(chunksize)\n            if not data:\n                break\n            else:\n                self.frame = DataFrame.from_records(\n                    data, columns=columns, coerce_float=coerce_float)\n\n                self._harmonize_columns(parse_dates=parse_dates)\n\n                if self.index is not None:\n                    self.frame.set_index(self.index, inplace=True)\n\n                yield self.frame", "output": "Return generator through chunked result set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def name(self) -> Optional[str]:\n        \"\"\"\n        \"\"\"\n\n        _, params = parse_content_disposition(\n            self.headers.get(CONTENT_DISPOSITION))\n        return content_disposition_filename(params, 'name')", "output": "Returns name specified in Content-Disposition header or None\n        if missed or header is malformed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_instance(name, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The show_instance action must be called with '\n            '-a or --action.'\n        )\n\n    vm_properties = [\n        \"config.hardware.device\",\n        \"summary.storage.committed\",\n        \"summary.storage.uncommitted\",\n        \"summary.storage.unshared\",\n        \"layoutEx.file\",\n        \"config.guestFullName\",\n        \"config.guestId\",\n        \"guest.net\",\n        \"config.hardware.memoryMB\",\n        \"name\",\n        \"config.hardware.numCPU\",\n        \"config.files.vmPathName\",\n        \"summary.runtime.powerState\",\n        \"guest.toolsStatus\"\n    ]\n\n    vm_list = salt.utils.vmware.get_mors_with_properties(_get_si(), vim.VirtualMachine, vm_properties)\n\n    for vm in vm_list:\n        if vm['name'] == name:\n            return _format_instance_info(vm)\n\n    return {}", "output": "List all available details of the specified VM\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a show_instance vmname", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def markdownify_operative_config_str(string):\n  \"\"\"\"\"\"\n\n  # TODO(b/37527917): Total hack below. Implement more principled formatting.\n  def process(line):\n    \"\"\"Convert a single line to markdown format.\"\"\"\n    if not line.startswith('#'):\n      return '    ' + line\n\n    line = line[2:]\n    if line.startswith('===='):\n      return ''\n    if line.startswith('None'):\n      return '    # None.'\n    if line.endswith(':'):\n      return '#### ' + line\n    return line\n\n  output_lines = []\n  for line in string.splitlines():\n    procd_line = process(line)\n    if procd_line is not None:\n      output_lines.append(procd_line)\n\n  return '\\n'.join(output_lines)", "output": "Convert an operative config string to markdown format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pad(attrs, inputs, proto_obj):\n    \"\"\" \"\"\"\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'pads'  : 'pad_width',\n                                                               'value' : 'constant_value'\n                                                              })\n    new_attrs['pad_width'] = translation_utils._pad_sequence_fix(new_attrs.get('pad_width'))\n    return 'pad', new_attrs, inputs", "output": "Add padding to input tensor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def asfreq(obj, freq, method=None, how=None, normalize=False, fill_value=None):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(obj.index, PeriodIndex):\n        if method is not None:\n            raise NotImplementedError(\"'method' argument is not supported\")\n\n        if how is None:\n            how = 'E'\n\n        new_obj = obj.copy()\n        new_obj.index = obj.index.asfreq(freq, how=how)\n\n    elif len(obj.index) == 0:\n        new_obj = obj.copy()\n        new_obj.index = obj.index._shallow_copy(freq=to_offset(freq))\n\n    else:\n        dti = date_range(obj.index[0], obj.index[-1], freq=freq)\n        dti.name = obj.index.name\n        new_obj = obj.reindex(dti, method=method, fill_value=fill_value)\n        if normalize:\n            new_obj.index = new_obj.index.normalize()\n\n    return new_obj", "output": "Utility frequency conversion method for Series/DataFrame.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def disk_io_counters(device=None):\n    '''\n    \n    '''\n    if not device:\n        return dict(psutil.disk_io_counters()._asdict())\n    else:\n        stats = psutil.disk_io_counters(perdisk=True)\n        if device in stats:\n            return dict(stats[device]._asdict())\n        else:\n            return False", "output": "Return disk I/O statistics.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.disk_io_counters\n\n        salt '*' ps.disk_io_counters device=sda1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __init_from_csr(self, csr, params_str, ref_dataset):\n        \"\"\"\"\"\"\n        if len(csr.indices) != len(csr.data):\n            raise ValueError('Length mismatch: {} vs {}'.format(len(csr.indices), len(csr.data)))\n        self.handle = ctypes.c_void_p()\n\n        ptr_indptr, type_ptr_indptr, __ = c_int_array(csr.indptr)\n        ptr_data, type_ptr_data, _ = c_float_array(csr.data)\n\n        assert csr.shape[1] <= MAX_INT32\n        csr.indices = csr.indices.astype(np.int32, copy=False)\n\n        _safe_call(_LIB.LGBM_DatasetCreateFromCSR(\n            ptr_indptr,\n            ctypes.c_int(type_ptr_indptr),\n            csr.indices.ctypes.data_as(ctypes.POINTER(ctypes.c_int32)),\n            ptr_data,\n            ctypes.c_int(type_ptr_data),\n            ctypes.c_int64(len(csr.indptr)),\n            ctypes.c_int64(len(csr.data)),\n            ctypes.c_int64(csr.shape[1]),\n            c_str(params_str),\n            ref_dataset,\n            ctypes.byref(self.handle)))\n        return self", "output": "Initialize data from a CSR matrix.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace(self, parameters=_void, return_annotation=_void):\n        \"\"\"\n        \"\"\"\n\n        if parameters is _void:\n            parameters = self.parameters.values()\n\n        if return_annotation is _void:\n            return_annotation = self._return_annotation\n\n        return type(self)(parameters, return_annotation=return_annotation)", "output": "Creates a customized copy of the Signature.\n        Pass 'parameters' and/or 'return_annotation' arguments\n        to override them in the new copy.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_builder_configs():\n  \"\"\"\n  \"\"\"\n  config_list = []\n  for each_corruption in TYPE_LIST:\n    for each_severity in range(1, 6):\n      name_str = each_corruption + '_' + str(each_severity)\n      version_str = '0.0.1'\n      description_str = 'corruption type = ' + each_corruption + ', severity = '\n      description_str += str(each_severity)\n      config_list.append(\n          Imagenet2012CorruptedConfig(\n              name=name_str,\n              version=version_str,\n              description=description_str,\n              corruption_type=each_corruption,\n              severity=each_severity,\n          ))\n  return config_list", "output": "Construct a list of BuilderConfigs.\n\n  Construct a list of 60 Imagenet2012CorruptedConfig objects, corresponding to\n  the 12 corruption types, with each type having 5 severities.\n\n  Returns:\n    A list of 60 Imagenet2012CorruptedConfig objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_val(self, key:str) -> Union[List[float],Tuple[List[float],List[float]]]:\n        \"\"\n        val = [pg[key] for pg in self.opt.param_groups[::2]]\n        if is_tuple(val[0]): val = [o[0] for o in val], [o[1] for o in val]\n        return val", "output": "Read a hyperparameter `key` in the optimizer dictionary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_multinomial(node, **kwargs):\n    \"\"\"\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n    dtype = onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[np.dtype(attrs.get(\"dtype\", 'int32'))]\n    sample_size = convert_string_to_list(attrs.get(\"shape\", '1'))\n    if len(sample_size) < 2:\n        sample_size = sample_size[-1]\n    else:\n        raise AttributeError(\"ONNX currently supports integer sample_size only\")\n    node = onnx.helper.make_node(\n        \"Multinomial\",\n        input_nodes,\n        [name],\n        dtype=dtype,\n        sample_size=sample_size,\n        name=name,\n    )\n    return [node]", "output": "Map MXNet's multinomial operator attributes to onnx's\n    Multinomial operator and return the created node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close(self):\n        \"\"\"\n        \n        \"\"\"\n        if self._socket is not None and self._conn is not None:\n            message_input = UnityMessage()\n            message_input.header.status = 400\n            self._communicator_send(message_input.SerializeToString())\n        if self._socket is not None:\n            self._socket.close()\n            self._socket = None\n        if self._socket is not None:\n            self._conn.close()\n            self._conn = None", "output": "Sends a shutdown signal to the unity environment, and closes the socket connection.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getOrCreate(cls, sc):\n        \"\"\"\n        \n        \"\"\"\n        if cls._instantiatedContext is None:\n            jsqlContext = sc._jvm.SQLContext.getOrCreate(sc._jsc.sc())\n            sparkSession = SparkSession(sc, jsqlContext.sparkSession())\n            cls(sc, sparkSession, jsqlContext)\n        return cls._instantiatedContext", "output": "Get the existing SQLContext or create a new one with given SparkContext.\n\n        :param sc: SparkContext", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def searchsorted(self, value, side=\"left\", sorter=None):\n        \"\"\"\n        \n        \"\"\"\n\n        # We are much more performant if the searched\n        # indexer is the same type as the array.\n        #\n        # This doesn't matter for int64, but DOES\n        # matter for smaller int dtypes.\n        #\n        # xref: https://github.com/numpy/numpy/issues/5370\n        try:\n            value = self.dtype.type(value)\n        except ValueError:\n            pass\n\n        return super().searchsorted(value, side=side, sorter=sorter)", "output": "Find indices to insert `value` so as to maintain order.\n\n        For full documentation, see `numpy.searchsorted`\n\n        See Also\n        --------\n        numpy.searchsorted : Equivalent function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_train(self, df:DataFrame):\n        \"\"\n        self.means,self.stds = {},{}\n        for n in self.cont_names:\n            assert is_numeric_dtype(df[n]), (f\"\"\"Cannot normalize '{n}' column as it isn't numerical.\n                Are you sure it doesn't belong in the categorical set of columns?\"\"\")\n            self.means[n],self.stds[n] = df[n].mean(),df[n].std()\n            df[n] = (df[n]-self.means[n]) / (1e-7 + self.stds[n])", "output": "Compute the means and stds of `self.cont_names` columns to normalize them.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_pod(\n        name,\n        namespace,\n        metadata,\n        spec,\n        source,\n        template,\n        saltenv,\n        **kwargs):\n    '''\n    \n    '''\n    body = __create_object_body(\n        kind='Pod',\n        obj_class=kubernetes.client.V1Pod,\n        spec_creator=__dict_to_pod_spec,\n        name=name,\n        namespace=namespace,\n        metadata=metadata,\n        spec=spec,\n        source=source,\n        template=template,\n        saltenv=saltenv)\n\n    cfg = _setup_conn(**kwargs)\n\n    try:\n        api_instance = kubernetes.client.CoreV1Api()\n        api_response = api_instance.create_namespaced_pod(\n            namespace, body)\n\n        return api_response.to_dict()\n    except (ApiException, HTTPError) as exc:\n        if isinstance(exc, ApiException) and exc.status == 404:\n            return None\n        else:\n            log.exception(\n                'Exception when calling '\n                'CoreV1Api->create_namespaced_pod'\n            )\n            raise CommandExecutionError(exc)\n    finally:\n        _cleanup(**cfg)", "output": "Creates the kubernetes deployment as defined by the user.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_batch(im_tensor, im_info):\n    \"\"\"\"\"\"\n    data = [im_tensor, im_info]\n    data_shapes = [('data', im_tensor.shape), ('im_info', im_info.shape)]\n    data_batch = mx.io.DataBatch(data=data, label=None, provide_data=data_shapes, provide_label=None)\n    return data_batch", "output": "return batch", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset():\n    \"\"\"\n    \n    \"\"\"\n    if hasattr(ray.worker.global_worker, \"signal_counters\"):\n        ray.worker.global_worker.signal_counters = defaultdict(lambda: b\"0\")", "output": "Reset the worker state associated with any signals that this worker\n    has received so far.\n\n    If the worker calls receive() on a source next, it will get all the\n    signals generated by that source starting with index = 1.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def linkcode_resolve(domain, info):\n    \"\"\"\n    \n    \"\"\"\n    if domain != 'py':\n        return None\n\n    modname = info['module']\n    fullname = info['fullname']\n\n    submod = sys.modules.get(modname)\n    if submod is None:\n        return None\n\n    obj = submod\n    for part in fullname.split('.'):\n        try:\n            obj = getattr(obj, part)\n        except:\n            return None\n\n    try:\n        fn = inspect.getsourcefile(obj)\n    except:\n        fn = None\n    if not fn:\n        return None\n\n    try:\n        source, lineno = inspect.getsourcelines(obj)\n    except:\n        lineno = None\n\n    if lineno:\n        linespec = \"#L%d-L%d\" % (lineno, lineno + len(source) - 1)\n    else:\n        linespec = \"\"\n\n    filename = info['module'].replace('.', '/')\n    return \"http://github.com/allenai/allennlp/blob/master/%s.py%s\" % (filename, linespec)", "output": "Determine the URL corresponding to Python object\n    This code is from\n    https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L290\n    and https://github.com/Lasagne/Lasagne/pull/262", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def classify(self, dataset):\n        \"\"\"\n        \n\n        \"\"\"\n        m = self.__proxy__['classifier']\n        target = self.__proxy__['target']\n        f = _BOW_FEATURE_EXTRACTOR\n        return m.classify(f(dataset, target))", "output": "Return a classification, for each example in the ``dataset``, using the\n        trained model. The output SFrame contains predictions as both class\n        labels as well as probabilities that the predicted value is the\n        associated label.\n\n        Parameters\n        ----------\n        dataset : SFrame\n            dataset of new observations. Must include columns with the same\n            names as the features used for model training, but does not require\n            a target column. Additional columns are ignored.\n\n        Returns\n        -------\n        out : SFrame\n            An SFrame with model predictions i.e class labels and probabilities.\n\n        See Also\n        ----------\n        create, evaluate, predict\n\n        Examples\n        --------\n        >>> import turicreate as tc\n        >>> dataset = tc.SFrame({'rating': [1, 5], 'text': ['hate it', 'love it']})\n        >>> m = tc.text_classifier.create(dataset, 'rating', features=['text'])\n        >>> output = m.classify(dataset)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_plane_axes_index(self, axis):\n        \"\"\"\n        \n        \"\"\"\n        axis_name = self._get_axis_name(axis)\n\n        if axis_name == 'major_axis':\n            index = 'minor_axis'\n            columns = 'items'\n        if axis_name == 'minor_axis':\n            index = 'major_axis'\n            columns = 'items'\n        elif axis_name == 'items':\n            index = 'major_axis'\n            columns = 'minor_axis'\n\n        return index, columns", "output": "Get my plane axes indexes: these are already\n        (as compared with higher level planes),\n        as we are returning a DataFrame axes indexes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_osarch():\n    '''\n    \n    '''\n    if salt.utils.path.which('rpm'):\n        ret = subprocess.Popen(\n            'rpm --eval \"%{_host_cpu}\"',\n            shell=True,\n            close_fds=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE).communicate()[0]\n    else:\n        ret = ''.join([x for x in platform.uname()[-2:] if x][-1:])\n\n    return salt.utils.stringutils.to_str(ret).strip() or 'unknown'", "output": "Get the os architecture using rpm --eval", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def supernet(self, prefixlen_diff=1, new_prefix=None):\n        \"\"\"\n\n        \"\"\"\n        if self._prefixlen == 0:\n            return self\n\n        if new_prefix is not None:\n            if new_prefix > self._prefixlen:\n                raise ValueError('new prefix must be shorter')\n            if prefixlen_diff != 1:\n                raise ValueError('cannot set prefixlen_diff and new_prefix')\n            prefixlen_diff = self._prefixlen - new_prefix\n\n        new_prefixlen = self.prefixlen - prefixlen_diff\n        if new_prefixlen < 0:\n            raise ValueError(\n                'current prefixlen is %d, cannot have a prefixlen_diff of %d' %\n                (self.prefixlen, prefixlen_diff))\n        return self.__class__((\n            int(self.network_address) & (int(self.netmask) << prefixlen_diff),\n            new_prefixlen))", "output": "The supernet containing the current network.\n\n        Args:\n            prefixlen_diff: An integer, the amount the prefix length of\n              the network should be decreased by.  For example, given a\n              /24 network and a prefixlen_diff of 3, a supernet with a\n              /21 netmask is returned.\n\n        Returns:\n            An IPv4 network object.\n\n        Raises:\n            ValueError: If self.prefixlen - prefixlen_diff < 0. I.e., you have\n              a negative prefix length.\n                OR\n            If prefixlen_diff and new_prefix are both set or new_prefix is a\n              larger number than the current prefix (larger number means a\n              smaller network)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def disbatch(self):\n        '''\n        \n        '''\n        ret = []\n\n        # check clients before going, we want to throw 400 if one is bad\n        for low in self.lowstate:\n            if not self._verify_client(low):\n                return\n\n            # Make sure we have 'token' or 'username'/'password' in each low chunk.\n            # Salt will verify the credentials are correct.\n            if self.token is not None and 'token' not in low:\n                low['token'] = self.token\n\n            if not (('token' in low)\n                    or ('username' in low and 'password' in low and 'eauth' in low)):\n                ret.append('Failed to authenticate')\n                break\n\n            # disbatch to the correct handler\n            try:\n                chunk_ret = yield getattr(self, '_disbatch_{0}'.format(low['client']))(low)\n                ret.append(chunk_ret)\n            except (AuthenticationError, AuthorizationError, EauthAuthenticationError):\n                ret.append('Failed to authenticate')\n                break\n            except Exception as ex:\n                ret.append('Unexpected exception while handling request: {0}'.format(ex))\n                log.error('Unexpected exception while handling request:', exc_info=True)\n\n        if not self._finished:\n            self.write(self.serialize({'return': ret}))\n            self.finish()", "output": "Disbatch all lowstates to the appropriate clients", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(handle):\n  \"\"\"\n  \"\"\"\n  if hasattr(tf_v1.saved_model, \"load_v2\"):\n    module_handle = resolve(handle)\n    return tf_v1.saved_model.load_v2(module_handle)\n  else:\n    raise NotImplementedError(\"hub.load() is not implemented for TF < 1.14.x, \"\n                              \"Current version: %s\", tf.__version__)", "output": "Loads a module from a handle.\n\n  Currently this method only works with Tensorflow 2.x and can only load modules\n  created by calling tensorflow.saved_model.save(). The method works in both\n  eager and graph modes.\n\n  Depending on the type of handle used, the call may involve downloading a\n  Tensorflow Hub module to a local cache location specified by the\n  TFHUB_CACHE_DIR environment variable. If a copy of the module is already\n  present in the TFHUB_CACHE_DIR, the download step is skipped.\n\n  Currently, three types of module handles are supported:\n    1) Smart URL resolvers such as tfhub.dev, e.g.:\n       https://tfhub.dev/google/nnlm-en-dim128/1.\n    2) A directory on a file system supported by Tensorflow containing module\n       files. This may include a local directory (e.g. /usr/local/mymodule) or a\n       Google Cloud Storage bucket (gs://mymodule).\n    3) A URL pointing to a TGZ archive of a module, e.g.\n       https://example.com/mymodule.tar.gz.\n\n  Args:\n    handle: (string) the Module handle to resolve.\n\n  Returns:\n    A trackable object (see tf.saved_model.load() documentation for details).\n\n  Raises:\n    NotImplementedError: If the code is running against incompatible (1.x)\n                         version of TF.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _systemctl_status(name):\n    '''\n    \n    '''\n    contextkey = 'systemd._systemctl_status.%s' % name\n    if contextkey in __context__:\n        return __context__[contextkey]\n    __context__[contextkey] = __salt__['cmd.run_all'](\n        _systemctl_cmd('status', name),\n        python_shell=False,\n        redirect_stderr=True,\n        ignore_retcode=True\n    )\n    return __context__[contextkey]", "output": "Helper function which leverages __context__ to keep from running 'systemctl\n    status' more than once.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _refresh_grpc(operations_stub, operation_name):\n    \"\"\"\n    \"\"\"\n    request_pb = operations_pb2.GetOperationRequest(name=operation_name)\n    return operations_stub.GetOperation(request_pb)", "output": "Refresh an operation using a gRPC client.\n\n    Args:\n        operations_stub (google.longrunning.operations_pb2.OperationsStub):\n            The gRPC operations stub.\n        operation_name (str): The name of the operation.\n\n    Returns:\n        google.longrunning.operations_pb2.Operation: The operation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def AddRun(self, path, name=None):\n    \"\"\"\n    \"\"\"\n    name = name or path\n    accumulator = None\n    with self._accumulators_mutex:\n      if name not in self._accumulators or self._paths[name] != path:\n        if name in self._paths and self._paths[name] != path:\n          # TODO(@dandelionmane) - Make it impossible to overwrite an old path\n          # with a new path (just give the new path a distinct name)\n          logger.warn('Conflict for name %s: old path %s, new path %s',\n                             name, self._paths[name], path)\n        logger.info('Constructing EventAccumulator for %s', path)\n        accumulator = event_accumulator.EventAccumulator(\n            path,\n            size_guidance=self._size_guidance,\n            tensor_size_guidance=self._tensor_size_guidance,\n            purge_orphaned_data=self.purge_orphaned_data)\n        self._accumulators[name] = accumulator\n        self._paths[name] = path\n    if accumulator:\n      if self._reload_called:\n        accumulator.Reload()\n    return self", "output": "Add a run to the multiplexer.\n\n    If the name is not specified, it is the same as the path.\n\n    If a run by that name exists, and we are already watching the right path,\n      do nothing. If we are watching a different path, replace the event\n      accumulator.\n\n    If `Reload` has been called, it will `Reload` the newly created\n    accumulators.\n\n    Args:\n      path: Path to the event files (or event directory) for given run.\n      name: Name of the run to add. If not provided, is set to path.\n\n    Returns:\n      The `EventMultiplexer`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def plus_one_v1(digits):\n    \"\"\"\n    \n    \"\"\"\n    digits[-1] = digits[-1] + 1\n    res = []\n    ten = 0\n    i = len(digits)-1\n    while i >= 0 or ten == 1:\n        summ = 0\n        if i >= 0:\n            summ += digits[i]\n        if ten:\n            summ += 1\n        res.append(summ % 10)\n        ten = summ // 10\n        i -= 1\n    return res[::-1]", "output": ":type digits: List[int]\n    :rtype: List[int]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def single_from_classes(path:Union[Path, str], classes:Collection[str], ds_tfms:TfmList=None, **kwargs):\n        \"\"\n        warn(\"\"\"This method is deprecated and will be removed in a future version, use `load_learner` after\n             `Learner.export()`\"\"\", DeprecationWarning)\n        sd = ImageList([], path=path, ignore_empty=True).split_none()\n        return sd.label_const(0, label_cls=CategoryList, classes=classes).transform(ds_tfms, **kwargs).databunch()", "output": "Create an empty `ImageDataBunch` in `path` with `classes`. Typically used for inference.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_example(self, data_path, image_id):\n    \"\"\"\"\"\"\n    image_filepath = os.path.join(\n        data_path, \"VOCdevkit/VOC2007/JPEGImages\", \"{}.jpg\".format(image_id))\n    annon_filepath = os.path.join(\n        data_path, \"VOCdevkit/VOC2007/Annotations\", \"{}.xml\".format(image_id))\n\n    def _get_example_objects():\n      \"\"\"Function to get all the objects from the annotation XML file.\"\"\"\n      with tf.io.gfile.GFile(annon_filepath, \"r\") as f:\n        root = xml.etree.ElementTree.parse(f).getroot()\n\n        size = root.find(\"size\")\n        width = float(size.find(\"width\").text)\n        height = float(size.find(\"height\").text)\n\n        for obj in root.findall(\"object\"):\n          # Get object's label name.\n          label = obj.find(\"name\").text.lower()\n          # Get objects' pose name.\n          pose = obj.find(\"pose\").text.lower()\n          is_truncated = (obj.find(\"truncated\").text == \"1\")\n          is_difficult = (obj.find(\"difficult\").text == \"1\")\n          bndbox = obj.find(\"bndbox\")\n          xmax = float(bndbox.find(\"xmax\").text)\n          xmin = float(bndbox.find(\"xmin\").text)\n          ymax = float(bndbox.find(\"ymax\").text)\n          ymin = float(bndbox.find(\"ymin\").text)\n          yield {\n              \"label\": label,\n              \"pose\": pose,\n              \"bbox\": tfds.features.BBox(\n                  ymin / height, xmin / width, ymax / height, xmax / width),\n              \"is_truncated\": is_truncated,\n              \"is_difficult\": is_difficult,\n          }\n\n    objects = list(_get_example_objects())\n    # Use set() to remove duplicates\n    labels = sorted(set(obj[\"label\"] for obj in objects))\n    labels_no_difficult = sorted(set(\n        obj[\"label\"] for obj in objects if obj[\"is_difficult\"] == 0\n    ))\n    return {\n        \"image\": image_filepath,\n        \"image/filename\": image_id + \".jpg\",\n        \"objects\": objects,\n        \"labels\": labels,\n        \"labels_no_difficult\": labels_no_difficult,\n    }", "output": "Yields examples.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ProcessImage(self, tag, wall_time, step, image):\n    \"\"\"\"\"\"\n    event = ImageEvent(wall_time=wall_time,\n                       step=step,\n                       encoded_image_string=image.encoded_image_string,\n                       width=image.width,\n                       height=image.height)\n    self.images.AddItem(tag, event)", "output": "Processes an image by adding it to accumulated state.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_processors(self):\n        \"\"\n        procs_x,procs_y = listify(self.train.x._processor),listify(self.train.y._processor)\n        xp = ifnone(self.train.x.processor, [p(ds=self.train.x) for p in procs_x])\n        yp = ifnone(self.train.y.processor, [p(ds=self.train.y) for p in procs_y])\n        return xp,yp", "output": "Read the default class processors if none have been set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fancy_handler(signum, frame, spinner):\n    \"\"\"\n    \"\"\"\n    spinner.red.fail(\"\u2718\")\n    spinner.stop()\n    sys.exit(0)", "output": "Signal handler, used to gracefully shut down the ``spinner`` instance\n    when specified signal is received by the process running the ``spinner``.\n\n    ``signum`` and ``frame`` are mandatory arguments. Check ``signal.signal``\n    function for more details.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _patch_tf(tf):\n  \"\"\"\"\"\"\n  global TF_PATCH\n  if TF_PATCH:\n    return\n\n  v_1_12 = distutils.version.LooseVersion(\"1.12.0\")\n  v_1_13 = distutils.version.LooseVersion(\"1.13.0\")\n  v_2 = distutils.version.LooseVersion(\"2.0.0\")\n  tf_version = distutils.version.LooseVersion(tf.__version__)\n  if v_1_12 <= tf_version < v_1_13:\n    # TODO(b/123930850): remove when 1.13 is stable.\n    TF_PATCH = \"tf1_12\"\n    _patch_for_tf1_12(tf)\n  elif v_1_13 <= tf_version < v_2:\n    TF_PATCH = \"tf1_13\"\n    _patch_for_tf1_13(tf)\n  else:\n    TF_PATCH = \"tf2\"\n    _patch_for_tf2(tf)", "output": "Patch TF to maintain compatibility across versions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def asList( self ):\n        \"\"\"\n        \n        \"\"\"\n        return [res.asList() if isinstance(res,ParseResults) else res for res in self.__toklist]", "output": "Returns the parse results as a nested list of matching tokens, all converted to strings.\n\n        Example::\n\n            patt = OneOrMore(Word(alphas))\n            result = patt.parseString(\"sldkj lsdkj sldkj\")\n            # even though the result prints in string-like form, it is actually a pyparsing ParseResults\n            print(type(result), result) # -> <class 'pyparsing.ParseResults'> ['sldkj', 'lsdkj', 'sldkj']\n\n            # Use asList() to create an actual list\n            result_list = result.asList()\n            print(type(result_list), result_list) # -> <class 'list'> ['sldkj', 'lsdkj', 'sldkj']", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def index_row(a:Union[Collection,pd.DataFrame,pd.Series], idxs:Collection[int])->Any:\n    \"\"\n    if a is None: return a\n    if isinstance(a,(pd.DataFrame,pd.Series)):\n        res = a.iloc[idxs]\n        if isinstance(res,(pd.DataFrame,pd.Series)): return res.copy()\n        return res\n    return a[idxs]", "output": "Return the slice of `a` corresponding to `idxs`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _subnets(proto='inet', interfaces_=None):\n    '''\n    \n    '''\n    if interfaces_ is None:\n        ifaces = interfaces()\n    elif isinstance(interfaces_, list):\n        ifaces = {}\n        for key, value in six.iteritems(interfaces()):\n            if key in interfaces_:\n                ifaces[key] = value\n    else:\n        ifaces = {interfaces_: interfaces().get(interfaces_, {})}\n\n    ret = set()\n\n    if proto == 'inet':\n        subnet = 'netmask'\n        dflt_cidr = 32\n    elif proto == 'inet6':\n        subnet = 'prefixlen'\n        dflt_cidr = 128\n    else:\n        log.error('Invalid proto %s calling subnets()', proto)\n        return\n\n    for ip_info in six.itervalues(ifaces):\n        addrs = ip_info.get(proto, [])\n        addrs.extend([addr for addr in ip_info.get('secondary', []) if addr.get('type') == proto])\n\n        for intf in addrs:\n            if subnet in intf:\n                intf = ipaddress.ip_interface('{0}/{1}'.format(intf['address'], intf[subnet]))\n            else:\n                intf = ipaddress.ip_interface('{0}/{1}'.format(intf['address'], dflt_cidr))\n            if not intf.is_loopback:\n                ret.add(intf.network)\n    return [six.text_type(net) for net in sorted(ret)]", "output": "Returns a list of subnets to which the host belongs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_input_data_with_word2vec(sentences, labels, word2vec_list):\n    \"\"\"\n    \n    \"\"\"\n    x_vec = []\n    for sent in sentences:\n        vec = []\n        for word in sent:\n            if word in word2vec_list:\n                vec.append(word2vec_list[word])\n            else:\n                vec.append(word2vec_list['</s>'])\n        x_vec.append(vec)\n    x_vec = np.array(x_vec)\n    y_vec = np.array(labels)\n    return [x_vec, y_vec]", "output": "Map sentences and labels to vectors based on a pretrained word2vec", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_storage_policies(profile_manager, policy_names=None,\n                         get_all_policies=False):\n    '''\n    \n    '''\n    res_type = pbm.profile.ResourceType(\n        resourceType=pbm.profile.ResourceTypeEnum.STORAGE)\n    try:\n        policy_ids = profile_manager.QueryProfile(res_type)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise VMwareApiError('Not enough permissions. Required privilege: '\n                             '{0}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise VMwareRuntimeError(exc.msg)\n    log.trace('policy_ids = %s', policy_ids)\n    # More policies are returned so we need to filter again\n    policies = [p for p in get_policies_by_id(profile_manager, policy_ids)\n                if p.resourceType.resourceType ==\n                pbm.profile.ResourceTypeEnum.STORAGE]\n    if get_all_policies:\n        return policies\n    if not policy_names:\n        policy_names = []\n    return [p for p in policies if p.name in policy_names]", "output": "Returns a list of the storage policies, filtered by name.\n\n    profile_manager\n        Reference to the profile manager.\n\n    policy_names\n        List of policy names to filter by.\n        Default is None.\n\n    get_all_policies\n        Flag specifying to return all policies, regardless of the specified\n        filter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def alias(self, alias):\n        \"\"\"\n        \"\"\"\n        assert isinstance(alias, basestring), \"alias should be a string\"\n        return DataFrame(getattr(self._jdf, \"as\")(alias), self.sql_ctx)", "output": "Returns a new :class:`DataFrame` with an alias set.\n\n        :param alias: string, an alias name to be set for the DataFrame.\n\n        >>> from pyspark.sql.functions import *\n        >>> df_as1 = df.alias(\"df_as1\")\n        >>> df_as2 = df.alias(\"df_as2\")\n        >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n        >>> joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\").collect()\n        [Row(name=u'Bob', name=u'Bob', age=5), Row(name=u'Alice', name=u'Alice', age=2)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_class_unpicklable(cls):\n    \"\"\"\"\"\"\n    def _break_on_call_reduce(self, protocol=None):\n        raise TypeError('%r cannot be pickled' % self)\n    cls.__reduce_ex__ = _break_on_call_reduce\n    cls.__module__ = '<unknown>'", "output": "Make the given class un-picklable.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate (self, ps):\n        \"\"\" \n        \"\"\"\n        assert isinstance(ps, property_set.PropertySet)\n        self.manager_.targets().log(\n            \"Building project '%s' with '%s'\" % (self.name (), str(ps)))\n        self.manager_.targets().increase_indent ()\n\n        result = GenerateResult ()\n\n        for t in self.targets_to_build ():\n            g = t.generate (ps)\n            result.extend (g)\n\n        self.manager_.targets().decrease_indent ()\n        return result", "output": "Generates all possible targets contained in this project.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next(self):\n        \"\"\"\"\"\"\n        if self.curr_idx == len(self.idx):\n            raise StopIteration\n        #i = batches index, j = starting record\n        i, j = self.idx[self.curr_idx] \n        self.curr_idx += 1\n\n        indices = self.ndindex[i][j:j + self.batch_size]\n        sentences = self.ndsent[i][j:j + self.batch_size]\n        characters = self.ndchar[i][j:j + self.batch_size]\n        label = self.ndlabel[i][j:j + self.batch_size]\n\n        return DataBatch([sentences, characters], [label], pad=0, index = indices, bucket_key=self.buckets[i],\n                         provide_data=[DataDesc(name=self.data_names[0], shape=sentences.shape, layout=self.layout),\n                                       DataDesc(name=self.data_names[1], shape=characters.shape, layout=self.layout)],\n                         provide_label=[DataDesc(name=self.label_name, shape=label.shape, layout=self.layout)])", "output": "Returns the next batch of data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_batch_end(self, last_input, last_output, **kwargs):\n        \"\"\n        self.G_A.zero_grad(); self.G_B.zero_grad()\n        fake_A, fake_B = last_output[0].detach(), last_output[1].detach()\n        real_A, real_B = last_input\n        self._set_trainable(D_A=True)\n        self.D_A.zero_grad()\n        loss_D_A = 0.5 * (self.crit(self.D_A(real_A), True) + self.crit(self.D_A(fake_A), False))\n        loss_D_A.backward()\n        self.opt_D_A.step()\n        self._set_trainable(D_B=True)\n        self.D_B.zero_grad()\n        loss_D_B = 0.5 * (self.crit(self.D_B(real_B), True) + self.crit(self.D_B(fake_B), False))\n        loss_D_B.backward()\n        self.opt_D_B.step()\n        self._set_trainable()\n        metrics = self.learn.loss_func.metrics + [loss_D_A, loss_D_B]\n        for n,m in zip(self.names,metrics): self.smootheners[n].add_value(m)", "output": "Steps through the generators then each of the critics.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def strftime(self, date_format):\n        \"\"\"\n        \n        \"\"\"\n        from pandas import Index\n        return Index(self._format_native_types(date_format=date_format))", "output": "Convert to Index using specified date_format.\n\n        Return an Index of formatted strings specified by date_format, which\n        supports the same string format as the python standard library. Details\n        of the string format can be found in `python string format\n        doc <%(URL)s>`__.\n\n        Parameters\n        ----------\n        date_format : str\n            Date format string (e.g. \"%%Y-%%m-%%d\").\n\n        Returns\n        -------\n        Index\n            Index of formatted strings.\n\n        See Also\n        --------\n        to_datetime : Convert the given argument to datetime.\n        DatetimeIndex.normalize : Return DatetimeIndex with times to midnight.\n        DatetimeIndex.round : Round the DatetimeIndex to the specified freq.\n        DatetimeIndex.floor : Floor the DatetimeIndex to the specified freq.\n\n        Examples\n        --------\n        >>> rng = pd.date_range(pd.Timestamp(\"2018-03-10 09:00\"),\n        ...                     periods=3, freq='s')\n        >>> rng.strftime('%%B %%d, %%Y, %%r')\n        Index(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM',\n               'March 10, 2018, 09:00:02 AM'],\n              dtype='object')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ns(self):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlNodeGetNs(self._o)\n        if ret is None:return None\n        __tmp = xmlNs(_obj=ret)\n        return __tmp", "output": "Get the namespace of a node", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cast(self, dtype):\n        \"\"\"\n        \"\"\"\n        self.dtype = dtype\n        if self._data is None:\n            return\n        with autograd.pause():\n            self._data = [i.astype(dtype) for i in self._data]\n            if self._grad is None:\n                return\n            self._grad = [i.astype(dtype) for i in self._grad]\n            autograd.mark_variables(self._data, self._grad, self.grad_req)", "output": "Cast data and gradient of this Parameter to a new data type.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype\n            The new data type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_episode_begin(self, episode, logs):\n        \"\"\"  \"\"\"\n        assert episode not in self.metrics\n        assert episode not in self.starts\n        self.metrics[episode] = []\n        self.starts[episode] = timeit.default_timer()", "output": "Initialize metrics at the beginning of each episode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flick(self, xspeed, yspeed):\n        \"\"\"\n        \n        \"\"\"\n        self._actions.append(lambda: self._driver.execute(\n            Command.FLICK, {\n                'xspeed': int(xspeed),\n                'yspeed': int(yspeed)}))\n        return self", "output": "Flicks, starting anywhere on the screen.\n\n        :Args:\n         - xspeed: The X speed in pixels per second.\n         - yspeed: The Y speed in pixels per second.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def min(self, numeric_only=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        self.check_for_ordered('min')\n        if numeric_only:\n            good = self._codes != -1\n            pointer = self._codes[good].min(**kwargs)\n        else:\n            pointer = self._codes.min(**kwargs)\n        if pointer == -1:\n            return np.nan\n        else:\n            return self.categories[pointer]", "output": "The minimum value of the object.\n\n        Only ordered `Categoricals` have a minimum!\n\n        Raises\n        ------\n        TypeError\n            If the `Categorical` is not `ordered`.\n\n        Returns\n        -------\n        min : the minimum of this `Categorical`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def group_list(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.list_groups(**kwargs)", "output": "List groups\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystoneng.group_list\n        salt '*' keystoneng.group_list domain_id=b62e76fbeeff4e8fb77073f591cf211e", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_class(path):\n    \"\"\"\n    \n    \"\"\"\n    class_data = path.split(\".\")\n    if len(class_data) < 2:\n        raise ValueError(\n            \"You need to pass a valid path like mymodule.provider_class\")\n    module_path = \".\".join(class_data[:-1])\n    class_str = class_data[-1]\n    module = importlib.import_module(module_path)\n    return getattr(module, class_str)", "output": "Load a class at runtime given a full path.\n\n    Example of the path: mypkg.mysubpkg.myclass", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse(self, source, name, filename):\n        \"\"\"\"\"\"\n        return Parser(self, source, name, encode_filename(filename)).parse()", "output": "Internal parsing function used by `parse` and `compile`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_norm(self, weights):\n    \"\"\"\"\"\"\n    with tf.variable_scope(\"init_norm\"):\n      flat = tf.reshape(weights, [-1, self.layer_depth])\n      return tf.reshape(tf.norm(flat, axis=0), (self.layer_depth,))", "output": "Set the norm of the weight vector.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def schedule_hourly():\n    \"\"\"  \"\"\"\n\n    if not config.get('ENABLE_SCHEDULED_EMAIL_REPORTS'):\n        logging.info('Scheduled email reports not enabled in config')\n        return\n\n    resolution = config.get('EMAIL_REPORTS_CRON_RESOLUTION', 0) * 60\n\n    # Get the top of the hour\n    start_at = datetime.now(tzlocal()).replace(microsecond=0, second=0, minute=0)\n    stop_at = start_at + timedelta(seconds=3600)\n    schedule_window(ScheduleType.dashboard.value, start_at, stop_at, resolution)\n    schedule_window(ScheduleType.slice.value, start_at, stop_at, resolution)", "output": "Celery beat job meant to be invoked hourly", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expanduser(path):\n    # type: (str) -> str\n    \"\"\"\n    \n    \"\"\"\n    expanded = os.path.expanduser(path)\n    if path.startswith('~/') and expanded.startswith('//'):\n        expanded = expanded[1:]\n    return expanded", "output": "Expand ~ and ~user constructions.\n\n    Includes a workaround for https://bugs.python.org/issue14768", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop(name, lbn, target, profile='default', tgt_type='glob'):\n    '''\n    \n    '''\n    return _talk2modjk(name, lbn, target, 'worker_stop', profile, tgt_type)", "output": ".. versionchanged:: 2017.7.0\n        The ``expr_form`` argument has been renamed to ``tgt_type``, earlier\n        releases must use ``expr_form``.\n\n    Stop the named worker from the lbn load balancers at the targeted minions\n    The worker won't get any traffic from the lbn\n\n    Example:\n\n    .. code-block:: yaml\n\n        disable-before-deploy:\n          modjk_worker.stop:\n            - name: {{ grains['id'] }}\n            - lbn: application\n            - target: 'roles:balancer'\n            - tgt_type: grain", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reload(self, message):\n        \"\"\"\n        \"\"\"\n\n        self.phone = message.get('phone')\n        self.level = message.get('level')\n        self.utype = message.get('utype')\n        self.coins = message.get('coins')\n        self.wechat_id = message.get('wechat_id')\n        self.coins_history = message.get('coins_history')\n        self.money = message.get('money')\n        self._subscribed_strategy = message.get('subuscribed_strategy')\n        self._subscribed_code = message.get('subscribed_code')\n        self.username = message.get('username')\n        self.password = message.get('password')\n        self.user_cookie = message.get('user_cookie')\n        #\n        portfolio_list = [item['portfolio_cookie'] for item in DATABASE.portfolio.find(\n            {'user_cookie': self.user_cookie}, {'portfolio_cookie': 1, '_id': 0})]\n\n        # portfolio_list = message.get('portfolio_list')\n        if len(portfolio_list) > 0:\n            self.portfolio_list = dict(\n                zip(\n                    portfolio_list,\n                    [\n                        QA_Portfolio(\n                            user_cookie=self.user_cookie,\n                            portfolio_cookie=item\n                        ) for item in portfolio_list\n                    ]\n                )\n            )\n        else:\n            self.portfolio_list = {}", "output": "\u6062\u590d\u65b9\u6cd5\n\n        Arguments:\n            message {[type]} -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_executor(sym, ctx, data_inputs, initializer=None):\n    \"\"\"\"\"\"\n    data_shapes = {k: v.shape for k, v in data_inputs.items()}\n    arg_names = sym.list_arguments()\n    aux_names = sym.list_auxiliary_states()\n    param_names = list(set(arg_names) - set(data_inputs.keys()))\n    arg_shapes, output_shapes, aux_shapes = sym.infer_shape(**data_shapes)\n    arg_name_shape = {k: s for k, s in zip(arg_names, arg_shapes)}\n    params = {n: nd.empty(arg_name_shape[n], ctx=ctx) for n in param_names}\n    params_grad = {n: nd.empty(arg_name_shape[n], ctx=ctx) for n in param_names}\n    aux_states = {k: nd.empty(s, ctx=ctx) for k, s in zip(aux_names, aux_shapes)}\n    exe = sym.bind(ctx=ctx, args=dict(params, **data_inputs),\n                   args_grad=params_grad,\n                   aux_states=aux_states)\n    if initializer is not None:\n        for k, v in params.items():\n            initializer(k, v)\n    return exe, params, params_grad, aux_states", "output": "Get executor to Stochastic Gradient Langevin Dynamics and/or Bayesian Dark Knowledge", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _compute_gradients(self, loss_fn, x, unused_optim_state):\n    \"\"\"\n    \"\"\"\n\n    # Assumes `x` is a list,\n    # and contains a tensor representing a batch of images\n    assert len(x) == 1 and isinstance(x, list), \\\n        'x should be a list and contain only one image tensor'\n    x = x[0]\n    loss = reduce_mean(loss_fn(x), axis=0)\n    return tf.gradients(loss, x)", "output": "Compute a new value of `x` to minimize `loss_fn`.\n\n    Args:\n        loss_fn: a callable that takes `x`, a batch of images, and returns\n            a batch of loss values. `x` will be optimized to minimize\n            `loss_fn(x)`.\n        x: A list of Tensors, the values to be updated. This is analogous\n            to the `var_list` argument in standard TF Optimizer.\n        unused_optim_state: A (possibly nested) dict, containing any state\n            info needed for the optimizer.\n\n    Returns:\n        new_x: A list of Tensors, the same length as `x`, which are updated\n        new_optim_state: A dict, with the same structure as `optim_state`,\n            which have been updated.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_alias_add(image,\n                    alias,\n                    description='',\n                    remote_addr=None,\n                    cert=None,\n                    key=None,\n                    verify_cert=True):\n    ''' \n    '''\n    image = _verify_image(image, remote_addr, cert, key, verify_cert)\n\n    for alias_info in image.aliases:\n        if alias_info['name'] == alias:\n            return True\n    image.add_alias(alias, description)\n\n    return True", "output": "Create an alias on the given image\n\n        image :\n            An image alias, a fingerprint or a image object\n\n        alias :\n            The alias to add\n\n        description :\n            Description of the alias\n\n        remote_addr :\n            An URL to a remote Server, you also have to give cert and key if\n            you provide remote_addr and its a TCP Address!\n\n            Examples:\n                https://myserver.lan:8443\n                /var/lib/mysocket.sock\n\n        cert :\n            PEM Formatted SSL Certificate.\n\n            Examples:\n                ~/.config/lxc/client.crt\n\n        key :\n            PEM Formatted SSL Key.\n\n            Examples:\n                ~/.config/lxc/client.key\n\n        verify_cert : True\n            Wherever to verify the cert, this is by default True\n            but in the most cases you want to set it off as LXD\n            normaly uses self-signed certificates.\n\n        CLI Examples:\n\n        .. code-block:: bash\n\n            $ salt '*' lxd.image_alias_add xenial/amd64 x \"Short version of xenial/amd64\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def htmlCreateMemoryParserCtxt(buffer, size):\n    \"\"\" \"\"\"\n    ret = libxml2mod.htmlCreateMemoryParserCtxt(buffer, size)\n    if ret is None:raise parserError('htmlCreateMemoryParserCtxt() failed')\n    return parserCtxt(_obj=ret)", "output": "Create a parser context for an HTML in-memory document.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_states(self, states=None, value=None):\n        \"\"\"\n        \"\"\"\n        if states is not None:\n            assert value is None, \"Only one of states & value can be specified.\"\n            _load_general(states, self.state_arrays, (0,)*len(states))\n        else:\n            assert value is not None, \"At least one of states & value must be specified.\"\n            assert states is None, \"Only one of states & value can be specified.\"\n            for d_dst in self.state_arrays:\n                for dst in d_dst:\n                    dst[:] = value", "output": "Set value for states. Only one of states & value can be specified.\n\n        Parameters\n        ----------\n        states : list of list of NDArrays\n            source states arrays formatted like [[state1_dev1, state1_dev2],\n            [state2_dev1, state2_dev2]].\n        value : number\n            a single scalar value for all state arrays.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def upgrade(refresh=True, pkgs=None, **kwargs):\n    '''\n    \n    '''\n    pkgin = _check_pkgin()\n    if not pkgin:\n        # There is not easy way to upgrade packages with old package system\n        return {}\n\n    if salt.utils.data.is_true(refresh):\n        refresh_db()\n\n    old = list_pkgs()\n\n    cmds = []\n    if not pkgs:\n        cmds.append([pkgin, '-y', 'full-upgrade'])\n    elif salt.utils.data.is_list(pkgs):\n        for pkg in pkgs:\n            cmds.append([pkgin, '-y', 'install', pkg])\n    else:\n        result = {'retcode': 1, 'reason': 'Ignoring the parameter `pkgs` because it is not a list!'}\n        log.error(result['reason'])\n\n    for cmd in cmds:\n        result = __salt__['cmd.run_all'](cmd,\n                                         output_loglevel='trace',\n                                         python_shell=False)\n        if result['retcode'] != 0:\n            break\n\n    __context__.pop('pkg.list_pkgs', None)\n    new = list_pkgs()\n    ret = salt.utils.data.compare_dicts(old, new)\n\n    if result['retcode'] != 0:\n        raise CommandExecutionError(\n            'Problem encountered upgrading packages',\n            info={'changes': ret, 'result': result}\n        )\n\n    return ret", "output": "Run pkg upgrade, if pkgin used. Otherwise do nothing\n\n    refresh\n        Whether or not to refresh the package database before installing.\n\n    Multiple Package Upgrade Options:\n\n    pkgs\n        A list of packages to upgrade from a software repository. Must be\n        passed as a python list.\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.upgrade pkgs='[\"foo\",\"bar\"]'\n\n    Returns a dictionary containing the changes:\n\n    .. code-block:: python\n\n        {'<package>':  {'old': '<old-version>',\n                        'new': '<new-version>'}}\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.upgrade", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_local_zip(self):\n        \"\"\"\n        \n        \"\"\"\n\n        if self.stage_config.get('delete_local_zip', True):\n            try:\n                if os.path.isfile(self.zip_path):\n                    os.remove(self.zip_path)\n                if self.handler_path and os.path.isfile(self.handler_path):\n                    os.remove(self.handler_path)\n            except Exception as e: # pragma: no cover\n                sys.exit(-1)", "output": "Remove our local zip file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_service(name,\n                    metadata,\n                    spec,\n                    source,\n                    template,\n                    old_service,\n                    saltenv,\n                    namespace='default',\n                    **kwargs):\n    '''\n    \n    '''\n    body = __create_object_body(\n        kind='Service',\n        obj_class=kubernetes.client.V1Service,\n        spec_creator=__dict_to_service_spec,\n        name=name,\n        namespace=namespace,\n        metadata=metadata,\n        spec=spec,\n        source=source,\n        template=template,\n        saltenv=saltenv)\n\n    # Some attributes have to be preserved\n    # otherwise exceptions will be thrown\n    body.spec.cluster_ip = old_service['spec']['cluster_ip']\n    body.metadata.resource_version = old_service['metadata']['resource_version']\n\n    cfg = _setup_conn(**kwargs)\n\n    try:\n        api_instance = kubernetes.client.CoreV1Api()\n        api_response = api_instance.replace_namespaced_service(\n            name, namespace, body)\n\n        return api_response.to_dict()\n    except (ApiException, HTTPError) as exc:\n        if isinstance(exc, ApiException) and exc.status == 404:\n            return None\n        else:\n            log.exception(\n                'Exception when calling '\n                'CoreV1Api->replace_namespaced_service'\n            )\n            raise CommandExecutionError(exc)\n    finally:\n        _cleanup(**cfg)", "output": "Replaces an existing service with a new one defined by name and namespace,\n    having the specificed metadata and spec.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def upgrade_tools_all(call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The upgrade_tools_all function must be called with '\n            '-f or --function.'\n        )\n\n    ret = {}\n    vm_properties = [\"name\"]\n\n    vm_list = salt.utils.vmware.get_mors_with_properties(_get_si(), vim.VirtualMachine, vm_properties)\n\n    for vm in vm_list:\n        ret[vm['name']] = _upg_tools_helper(vm['object'])\n\n    return ret", "output": "To upgrade VMware Tools on all virtual machines present in\n    the specified provider\n\n    .. note::\n\n        If the virtual machine is running Windows OS, this function\n        will attempt to suppress the automatic reboot caused by a\n        VMware Tools upgrade.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f upgrade_tools_all my-vmware-config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_monday(dt):\n    \"\"\"\n    \n    \"\"\"\n    if dt.weekday() == 5:\n        return dt + timedelta(2)\n    elif dt.weekday() == 6:\n        return dt + timedelta(1)\n    return dt", "output": "If holiday falls on Saturday, use following Monday instead;\n    if holiday falls on Sunday, use Monday instead", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_secure_cookie(\n        self,\n        name: str,\n        value: str = None,\n        max_age_days: int = 31,\n        min_version: int = None,\n    ) -> Optional[bytes]:\n        \"\"\"\n        \"\"\"\n        self.require_setting(\"cookie_secret\", \"secure cookies\")\n        if value is None:\n            value = self.get_cookie(name)\n        return decode_signed_value(\n            self.application.settings[\"cookie_secret\"],\n            name,\n            value,\n            max_age_days=max_age_days,\n            min_version=min_version,\n        )", "output": "Returns the given signed cookie if it validates, or None.\n\n        The decoded cookie value is returned as a byte string (unlike\n        `get_cookie`).\n\n        Similar to `get_cookie`, this method only returns cookies that\n        were present in the request. It does not see outgoing cookies set by\n        `set_secure_cookie` in this handler.\n\n        .. versionchanged:: 3.2.1\n\n           Added the ``min_version`` argument.  Introduced cookie version 2;\n           both versions 1 and 2 are accepted by default.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def scalars_impl(self, tag, run, experiment, output_format):\n    \"\"\"\"\"\"\n    if self._db_connection_provider:\n      db = self._db_connection_provider()\n      # We select for steps greater than -1 because the writer inserts\n      # placeholder rows en masse. The check for step filters out those rows.\n      cursor = db.execute('''\n        SELECT\n          Tensors.step,\n          Tensors.computed_time,\n          Tensors.data,\n          Tensors.dtype\n        FROM Tensors\n        JOIN Tags\n          ON Tensors.series = Tags.tag_id\n        JOIN Runs\n          ON Tags.run_id = Runs.run_id\n        WHERE\n          /* For backwards compatibility, ignore the experiment id\n             for matching purposes if it is empty. */\n          (:exp == '' OR Runs.experiment_id == CAST(:exp AS INT))\n          AND Runs.run_name = :run\n          AND Tags.tag_name = :tag\n          AND Tags.plugin_name = :plugin\n          AND Tensors.shape = ''\n          AND Tensors.step > -1\n        ORDER BY Tensors.step\n      ''', dict(exp=experiment, run=run, tag=tag, plugin=metadata.PLUGIN_NAME))\n      values = [(wall_time, step, self._get_value(data, dtype_enum))\n                for (step, wall_time, data, dtype_enum) in cursor]\n    else:\n      tensor_events = self._multiplexer.Tensors(run, tag)\n      values = [(tensor_event.wall_time,\n                 tensor_event.step,\n                 tensor_util.make_ndarray(tensor_event.tensor_proto).item())\n                for tensor_event in tensor_events]\n\n    if output_format == OutputFormat.CSV:\n      string_io = StringIO()\n      writer = csv.writer(string_io)\n      writer.writerow(['Wall time', 'Step', 'Value'])\n      writer.writerows(values)\n      return (string_io.getvalue(), 'text/csv')\n    else:\n      return (values, 'application/json')", "output": "Result of the form `(body, mime_type)`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_msgpack(path_or_buf, encoding='utf-8', iterator=False, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    path_or_buf, _, _, should_close = get_filepath_or_buffer(path_or_buf)\n    if iterator:\n        return Iterator(path_or_buf)\n\n    def read(fh):\n        unpacked_obj = list(unpack(fh, encoding=encoding, **kwargs))\n        if len(unpacked_obj) == 1:\n            return unpacked_obj[0]\n\n        if should_close:\n            try:\n                path_or_buf.close()\n            except IOError:\n                pass\n        return unpacked_obj\n\n    # see if we have an actual file\n    if isinstance(path_or_buf, str):\n        try:\n            exists = os.path.exists(path_or_buf)\n        except (TypeError, ValueError):\n            exists = False\n\n        if exists:\n            with open(path_or_buf, 'rb') as fh:\n                return read(fh)\n\n    if isinstance(path_or_buf, bytes):\n        # treat as a binary-like\n        fh = None\n        try:\n            fh = BytesIO(path_or_buf)\n            return read(fh)\n        finally:\n            if fh is not None:\n                fh.close()\n    elif hasattr(path_or_buf, 'read') and callable(path_or_buf.read):\n        # treat as a buffer like\n        return read(path_or_buf)\n\n    raise ValueError('path_or_buf needs to be a string file path or file-like')", "output": "Load msgpack pandas object from the specified\n    file path\n\n    THIS IS AN EXPERIMENTAL LIBRARY and the storage format\n    may not be stable until a future release.\n\n    Parameters\n    ----------\n    path_or_buf : string File path, BytesIO like or string\n    encoding : Encoding for decoding msgpack str type\n    iterator : boolean, if True, return an iterator to the unpacker\n               (default is False)\n\n    Returns\n    -------\n    obj : same type as object stored in file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decompress_step(source, hparams, first_relu, name):\n  \"\"\"\"\"\"\n  with tf.variable_scope(name):\n    shape = common_layers.shape_list(source)\n    multiplier = 2\n    kernel = (1, 1)\n    thicker = common_layers.conv_block(\n        source,\n        hparams.hidden_size * multiplier, [((1, 1), kernel)],\n        first_relu=first_relu,\n        name=\"decompress_conv\")\n    return tf.reshape(thicker, [shape[0], shape[1] * 2, 1, hparams.hidden_size])", "output": "Decompression function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def type(filename):\n    \"\"\" \n    \"\"\"\n    assert isinstance(filename, basestring)\n    while 1:\n        filename, suffix = os.path.splitext (filename)\n        if not suffix: return None\n        suffix = suffix[1:]\n\n        if suffix in __suffixes_to_types:\n            return __suffixes_to_types[suffix]", "output": "Returns file type given it's name. If there are several dots in filename,\n        tries each suffix. E.g. for name of \"file.so.1.2\" suffixes \"2\", \"1\", and\n        \"so\"  will be tried.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_delta(self, delta):\n        \"\"\"\n        \n        \"\"\"\n        new_values = super()._add_delta(delta)\n        return type(self)._from_sequence(new_values, tz=self.tz, freq='infer')", "output": "Add a timedelta-like, Tick, or TimedeltaIndex-like object\n        to self, yielding a new DatetimeArray\n\n        Parameters\n        ----------\n        other : {timedelta, np.timedelta64, Tick,\n                 TimedeltaIndex, ndarray[timedelta64]}\n\n        Returns\n        -------\n        result : DatetimeArray", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_resource_by_id(resource_id, api_version, extract_value=None):\n    '''\n    \n    '''\n    ret = {}\n\n    try:\n        resconn = get_conn(client_type='resource')\n        resource_query = resconn.resources.get_by_id(\n            resource_id=resource_id,\n            api_version=api_version\n        )\n        resource_dict = resource_query.as_dict()\n        if extract_value is not None:\n            ret = resource_dict[extract_value]\n        else:\n            ret = resource_dict\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('resource', exc.message)\n        ret = {'Error': exc.message}\n\n    return ret", "output": "Get an AzureARM resource by id", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reformat_node(item=None, full=False):\n    '''\n    \n    '''\n    desired_keys = [\n        'id', 'name', 'state', 'public_ips', 'private_ips', 'size', 'image',\n        'location'\n    ]\n    item['private_ips'] = []\n    item['public_ips'] = []\n    if 'ips' in item:\n        for ip in item['ips']:\n            if salt.utils.cloud.is_public_ip(ip):\n                item['public_ips'].append(ip)\n            else:\n                item['private_ips'].append(ip)\n\n    # add any undefined desired keys\n    for key in desired_keys:\n        if key not in item:\n            item[key] = None\n\n    # remove all the extra key value pairs to provide a brief listing\n    to_del = []\n    if not full:\n        for key in six.iterkeys(item):  # iterate over a copy of the keys\n            if key not in desired_keys:\n                to_del.append(key)\n\n    for key in to_del:\n        del item[key]\n\n    if 'state' in item:\n        item['state'] = joyent_node_state(item['state'])\n\n    return item", "output": "Reformat the returned data from joyent, determine public/private IPs and\n    strip out fields if necessary to provide either full or brief content.\n\n    :param item: node dictionary\n    :param full: full or brief output\n    :return: dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def shutdown(self, timeout: Optional[float]=15.0) -> None:\n        \"\"\"\"\"\"\n        self._force_close = True\n\n        if self._keepalive_handle is not None:\n            self._keepalive_handle.cancel()\n\n        if self._waiter:\n            self._waiter.cancel()\n\n        # wait for handlers\n        with suppress(asyncio.CancelledError, asyncio.TimeoutError):\n            with CeilTimeout(timeout, loop=self._loop):\n                if (self._error_handler is not None and\n                        not self._error_handler.done()):\n                    await self._error_handler\n\n                if (self._task_handler is not None and\n                        not self._task_handler.done()):\n                    await self._task_handler\n\n        # force-close non-idle handler\n        if self._task_handler is not None:\n            self._task_handler.cancel()\n\n        if self.transport is not None:\n            self.transport.close()\n            self.transport = None", "output": "Worker process is about to exit, we need cleanup everything and\n        stop accepting requests. It is especially important for keep-alive\n        connections.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def import_data(self, data):\n        \"\"\"\n        \"\"\"\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(\"Importing data, current processing progress %s / %s\" %(_completed_num, len(data)))\n            _completed_num += 1\n            if self.algorithm_name == 'random_search':\n                return\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                logger.info(\"Useless trial data, value is %s, skip this trial data.\" %_value)\n                continue\n            self.supplement_data_num += 1\n            _parameter_id = '_'.join([\"ImportData\", str(self.supplement_data_num)])\n            self.total_data[_parameter_id] = _add_index(in_x=self.json, parameter=_params)\n            self.receive_trial_result(parameter_id=_parameter_id, parameters=_params, value=_value)\n        logger.info(\"Successfully import data to TPE/Anneal tuner.\")", "output": "Import additional data for tuning\n\n        Parameters\n        ----------\n        data:\n            a list of dictionarys, each of which has at least two keys, 'parameter' and 'value'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_path(name, scheme=_get_default_scheme(), vars=None, expand=True):\n    \"\"\"\n    \"\"\"\n    return get_paths(scheme, vars, expand)[name]", "output": "Return a path corresponding to the scheme.\n\n    ``scheme`` is the install scheme name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reinitialize_all_clients(self):\n        \"\"\"\n        \n        \"\"\"\n        for language in self.clients:\n            language_client = self.clients[language]\n            if language_client['status'] == self.RUNNING:\n                folder = self.get_root_path(language)\n                instance = language_client['instance']\n                instance.folder = folder\n                instance.initialize()", "output": "Send a new initialize message to each LSP server when the project\n        path has changed so they can update the respective server root paths.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_flag_qrect(self, value):\n        \"\"\"\"\"\"\n        if self.slider:\n            position = self.value_to_position(value+0.5)\n            # The 0.5 offset is used to align the flags with the center of\n            # their corresponding text edit block before scaling.\n\n            return QRect(self.FLAGS_DX/2, position-self.FLAGS_DY/2,\n                         self.WIDTH-self.FLAGS_DX, self.FLAGS_DY)\n        else:\n            # When the vertical scrollbar is not visible, the flags are\n            # vertically aligned with the center of their corresponding\n            # text block with no scaling.\n            block = self.editor.document().findBlockByLineNumber(value)\n            top = self.editor.blockBoundingGeometry(block).translated(\n                      self.editor.contentOffset()).top()\n            bottom = top + self.editor.blockBoundingRect(block).height()\n            middle = (top + bottom)/2\n\n            return QRect(self.FLAGS_DX/2, middle-self.FLAGS_DY/2,\n                         self.WIDTH-self.FLAGS_DX, self.FLAGS_DY)", "output": "Make flag QRect", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_ids_to_tokens(self, ids):\n        \"\"\"\"\"\"\n        tokens = []\n        for i in ids:\n            tokens.append(self.ids_to_tokens[i])\n        return tokens", "output": "Converts a sequence of ids in wordpiece tokens using the vocab.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_empty(self, axes=None):\n        \"\"\"  \"\"\"\n        if axes is None:\n            axes = [ensure_index([])] + [ensure_index(a)\n                                         for a in self.axes[1:]]\n\n        # preserve dtype if possible\n        if self.ndim == 1:\n            blocks = np.array([], dtype=self.array_dtype)\n        else:\n            blocks = []\n        return self.__class__(blocks, axes)", "output": "return an empty BlockManager with the items axis of len 0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _construct_error_handling(self):\n        \"\"\"\n        \n        \"\"\"\n        # Both path and method not present\n        self._app.register_error_handler(404, ServiceErrorResponses.route_not_found)\n        # Path is present, but method not allowed\n        self._app.register_error_handler(405, ServiceErrorResponses.route_not_found)\n        # Something went wrong\n        self._app.register_error_handler(500, ServiceErrorResponses.lambda_failure_response)", "output": "Updates the Flask app with Error Handlers for different Error Codes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cutout(x, n_holes:uniform_int=1, length:uniform_int=40):\n    \"\"\n    h,w = x.shape[1:]\n    for n in range(n_holes):\n        h_y = np.random.randint(0, h)\n        h_x = np.random.randint(0, w)\n        y1 = int(np.clip(h_y - length / 2, 0, h))\n        y2 = int(np.clip(h_y + length / 2, 0, h))\n        x1 = int(np.clip(h_x - length / 2, 0, w))\n        x2 = int(np.clip(h_x + length / 2, 0, w))\n        x[:, y1:y2, x1:x2] = 0\n    return x", "output": "Cut out `n_holes` number of square holes of size `length` in image at random locations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(name, call=None):\n    '''\n    \n    '''\n    datacenter_id = get_datacenter_id()\n    conn = get_conn()\n    node = get_node(conn, name)\n\n    conn.start_server(datacenter_id=datacenter_id, server_id=node['id'])\n\n    return True", "output": "start a machine by name\n    :param name: name given to the machine\n    :param call: call value in this case is 'action'\n    :return: true if successful\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a start vm_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def host(proxy=None):\n    '''\n    \n    '''\n    if proxy and salt.utils.napalm.is_proxy(__opts__):\n        # this grain is set only when running in a proxy minion\n        # otherwise will use the default Salt grains\n        return {'host': _get_device_grain('hostname', proxy=proxy)}", "output": "This grain is set by the NAPALM grain module\n    only when running in a proxy minion.\n    When Salt is installed directly on the network device,\n    thus running a regular minion, the ``host`` grain\n    provides the physical hostname of the network device,\n    as it would be on an ordinary minion server.\n    When running in a proxy minion, ``host`` points to the\n    value configured in the pillar: :mod:`NAPALM proxy module <salt.proxy.napalm>`.\n\n    .. note::\n\n        The diference between ``host`` and ``hostname`` is that\n        ``host`` provides the physical location - either domain name or IP address,\n        while ``hostname`` provides the hostname as configured on the device.\n        They are not necessarily the same.\n\n    .. versionadded:: 2017.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'device*' grains.get host\n\n    Output:\n\n    .. code-block:: yaml\n\n        device1:\n            ip-172-31-13-136.us-east-2.compute.internal\n        device2:\n            ip-172-31-11-193.us-east-2.compute.internal\n        device3:\n            ip-172-31-2-181.us-east-2.compute.internal", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(tgt, fun, tgt_type='glob', roster='flat'):\n    '''\n    \n    '''\n    # Set up opts for the SSH object\n    opts = copy.deepcopy(__context__['master_opts'])\n    minopts = copy.deepcopy(__opts__)\n    opts.update(minopts)\n    if roster:\n        opts['roster'] = roster\n    opts['argv'] = [fun]\n    opts['selected_target_option'] = tgt_type\n    opts['tgt'] = tgt\n    opts['arg'] = []\n\n    # Create the SSH object to handle the actual call\n    ssh = salt.client.ssh.SSH(opts)\n\n    # Run salt-ssh to get the minion returns\n    rets = {}\n    for ret in ssh.run_iter(mine=True):\n        rets.update(ret)\n\n    cret = {}\n    for host in rets:\n        if 'return' in rets[host]:\n            cret[host] = rets[host]['return']\n        else:\n            cret[host] = rets[host]\n    return cret", "output": "Get data from the mine based on the target, function and tgt_type\n\n    This will actually run the function on all targeted minions (like\n    publish.publish), as salt-ssh clients can't update the mine themselves.\n\n    We will look for mine_functions in the roster, pillar, and master config,\n    in that order, looking for a match for the defined function\n\n    Targets can be matched based on any standard matching system that can be\n    matched on the defined roster (in salt-ssh) via these keywords::\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-ssh '*' mine.get '*' network.interfaces\n        salt-ssh '*' mine.get 'myminion' network.interfaces roster=flat\n        salt-ssh '*' mine.get '192.168.5.0' network.ipaddrs roster=scan", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def restore_original_dimensions(obs, obs_space, tensorlib=tf):\n    \"\"\"\n    \"\"\"\n\n    if hasattr(obs_space, \"original_space\"):\n        return _unpack_obs(obs, obs_space.original_space, tensorlib=tensorlib)\n    else:\n        return obs", "output": "Unpacks Dict and Tuple space observations into their original form.\n\n    This is needed since we flatten Dict and Tuple observations in transit.\n    Before sending them to the model though, we should unflatten them into\n    Dicts or Tuples of tensors.\n\n    Arguments:\n        obs: The flattened observation tensor.\n        obs_space: The flattened obs space. If this has the `original_space`\n            attribute, we will unflatten the tensor to that shape.\n        tensorlib: The library used to unflatten (reshape) the array/tensor.\n\n    Returns:\n        single tensor or dict / tuple of tensors matching the original\n        observation space.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_string(cls, dataset_id, default_project=None):\n        \"\"\"\n        \"\"\"\n        output_dataset_id = dataset_id\n        output_project_id = default_project\n        parts = dataset_id.split(\".\")\n\n        if len(parts) == 1 and not default_project:\n            raise ValueError(\n                \"When default_project is not set, dataset_id must be a \"\n                \"fully-qualified dataset ID in standard SQL format. \"\n                'e.g. \"project.dataset_id\", got {}'.format(dataset_id)\n            )\n        elif len(parts) == 2:\n            output_project_id, output_dataset_id = parts\n        elif len(parts) > 2:\n            raise ValueError(\n                \"Too many parts in dataset_id. Expected a fully-qualified \"\n                \"dataset ID in standard SQL format. e.g. \"\n                '\"project.dataset_id\", got {}'.format(dataset_id)\n            )\n\n        return cls(output_project_id, output_dataset_id)", "output": "Construct a dataset reference from dataset ID string.\n\n        Args:\n            dataset_id (str):\n                A dataset ID in standard SQL format. If ``default_project``\n                is not specified, this must included both the project ID and\n                the dataset ID, separated by ``.``.\n            default_project (str):\n                Optional. The project ID to use when ``dataset_id`` does not\n                include a project ID.\n\n        Returns:\n            DatasetReference:\n                Dataset reference parsed from ``dataset_id``.\n\n        Examples:\n            >>> DatasetReference.from_string('my-project-id.some_dataset')\n            DatasetReference('my-project-id', 'some_dataset')\n\n        Raises:\n            ValueError:\n                If ``dataset_id`` is not a fully-qualified dataset ID in\n                standard SQL format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_cloudformation(self, **kwargs):\n        \"\"\"\n        \"\"\"\n        function = kwargs.get('function')\n\n        if not function:\n            raise TypeError(\"Missing required keyword argument: function\")\n\n        source_arn = self.get_source_arn()\n        permission = self._construct_permission(function, source_arn=source_arn)\n        subscription_filter = self.get_subscription_filter(function, permission)\n        resources = [permission, subscription_filter]\n\n        return resources", "output": "Returns the CloudWatch Logs Subscription Filter and Lambda Permission to which this CloudWatch Logs event source\n        corresponds.\n\n        :param dict kwargs: no existing resources need to be modified\n        :returns: a list of vanilla CloudFormation Resources, to which this push event expands\n        :rtype: list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_to_files_np(features, tokenizer, max_seq_length,\n                      max_predictions_per_seq, output_files):\n    # pylint: disable=unused-argument\n    \"\"\"\"\"\"\n    next_sentence_labels = []\n    valid_lengths = []\n\n    assert len(output_files) == 1, 'numpy format only support single output file'\n    output_file = output_files[0]\n    (input_ids, segment_ids, masked_lm_positions, masked_lm_ids,\n     masked_lm_weights, next_sentence_labels, valid_lengths) = features\n    total_written = len(next_sentence_labels)\n\n    # store variable length numpy array object directly.\n    outputs = collections.OrderedDict()\n    outputs['input_ids'] = np.array(input_ids, dtype=object)\n    outputs['segment_ids'] = np.array(segment_ids, dtype=object)\n    outputs['masked_lm_positions'] = np.array(masked_lm_positions, dtype=object)\n    outputs['masked_lm_ids'] = np.array(masked_lm_ids, dtype=object)\n    outputs['masked_lm_weights'] = np.array(masked_lm_weights, dtype=object)\n    outputs['next_sentence_labels'] = np.array(next_sentence_labels, dtype='int32')\n    outputs['valid_lengths'] = np.array(valid_lengths, dtype='int32')\n\n    np.savez_compressed(output_file, **outputs)\n    logging.info('Wrote %d total instances', total_written)", "output": "Write to numpy files from `TrainingInstance`s.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(self, require=True, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        if not require or args or kwargs:\n            warnings.warn(\n                \"Parameters to load are deprecated.  Call .resolve and \"\n                \".require separately.\",\n                PkgResourcesDeprecationWarning,\n                stacklevel=2,\n            )\n        if require:\n            self.require(*args, **kwargs)\n        return self.resolve()", "output": "Require packages for this EntryPoint, then resolve it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_group(self, name, obj=None):\n        \"\"\"\n        \n        \"\"\"\n        if obj is None:\n            obj = self._selected_obj\n\n        inds = self._get_index(name)\n        if not len(inds):\n            raise KeyError(name)\n\n        return obj._take(inds, axis=self.axis)", "output": "Construct NDFrame from group with provided name.\n\n        Parameters\n        ----------\n        name : object\n            the name of the group to get as a DataFrame\n        obj : NDFrame, default None\n            the NDFrame to take the DataFrame out of.  If\n            it is None, the object groupby was called on will\n            be used\n\n        Returns\n        -------\n        group : same type as obj", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_metric(self, eval_metric, labels, pre_sliced=False):\n        \"\"\"\n        \"\"\"\n        self._exec_group.update_metric(eval_metric, labels, pre_sliced)", "output": "Evaluates and accumulates evaluation metric on outputs of the last forward computation.\n\n        See Also\n        ----------\n        :meth:`BaseModule.update_metric`.\n\n        Parameters\n        ----------\n        eval_metric : EvalMetric\n            Evaluation metric to use.\n        labels : list of NDArray if `pre_sliced` parameter is set to `False`,\n            list of lists of NDArray otherwise. Typically `data_batch.label`.\n        pre_sliced: bool\n            Whether the labels are already sliced per device (default: False).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mutex():\n    '''\n    \n    '''\n\n    # Test options and the values they take\n    # --mutex-num = [50,500,1000]\n    # --mutex-locks = [10000,25000,50000]\n    # --mutex-loops = [2500,5000,10000]\n\n    # Test data (Orthogonal test cases)\n    mutex_num = [50, 50, 50, 500, 500, 500, 1000, 1000, 1000]\n    locks = [10000, 25000, 50000, 10000, 25000, 50000, 10000, 25000, 50000]\n    mutex_locks = []\n    mutex_locks.extend(locks)\n    mutex_loops = [2500, 5000, 10000, 10000, 2500, 5000, 5000, 10000, 2500]\n\n    # Initializing the test variables\n    test_command = 'sysbench --num-threads=250 --test=mutex '\n    test_command += '--mutex-num={0} --mutex-locks={1} --mutex-loops={2} run '\n    result = None\n    ret_val = {}\n\n    # Test begins!\n    for num, locks, loops in zip(mutex_num, mutex_locks, mutex_loops):\n        key = 'Mutex: {0} Locks: {1} Loops: {2}'.format(num, locks, loops)\n        run_command = test_command.format(num, locks, loops)\n        result = __salt__['cmd.run'](run_command)\n        ret_val[key] = _parser(result)\n\n    return ret_val", "output": "Tests the implementation of mutex\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' sysbench.mutex", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_longest_non_repeat_v2(string):\n    \"\"\"\n    \n    \"\"\"\n    if string is None:\n        return 0, ''\n    sub_string = ''\n    start, max_len = 0, 0\n    used_char = {}\n    for index, char in enumerate(string):\n        if char in used_char and start <= used_char[char]:\n            start = used_char[char] + 1\n        else:\n            if index - start + 1 > max_len:\n                max_len = index - start + 1\n                sub_string = string[start: index + 1]\n        used_char[char] = index\n    return max_len, sub_string", "output": "Find the length of the longest substring\n    without repeating characters.\n    Uses alternative algorithm.\n    Return max_len and the substring as a tuple", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_params(self):\n        \"\"\"\n        \"\"\"\n        assert self.binded and self.params_initialized\n\n        arg_params = dict()\n        aux_params = dict()\n\n        for module in self._modules:\n            arg, aux = module.get_params()\n            arg_params.update(arg)\n            aux_params.update(aux)\n\n        return (arg_params, aux_params)", "output": "Gets current parameters.\n\n        Returns\n        -------\n        (arg_params, aux_params)\n            A pair of dictionaries each mapping parameter names to NDArray values. This\n            is a merged dictionary of all the parameters in the modules.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _traverse_list(self, input_list, resolution_data, resolver_method):\n        \"\"\"\n        \n        \"\"\"\n        for index, value in enumerate(input_list):\n            input_list[index] = self._traverse(value, resolution_data, resolver_method)\n\n        return input_list", "output": "Traverse a list to resolve intrinsic functions on every element\n\n        :param input_list: List of input\n        :param resolution_data: Data that the `resolver_method` needs to operate\n        :param resolver_method: Method that can actually resolve an intrinsic function, if it detects one\n        :return: Modified list with intrinsic functions resolved", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mmf(x, alpha, beta, kappa, delta):\n    \"\"\"\n    \"\"\"\n    return alpha - (alpha - beta) / (1. + (kappa * x)**delta)", "output": "Morgan-Mercer-Flodin\n    http://www.pisces-conservation.com/growthhelp/index.html?morgan_mercer_floden.htm\n\n    Parameters\n    ----------\n    x: int\n    alpha: float\n    beta: float\n    kappa: float\n    delta: float\n\n    Returns\n    -------\n    float\n        alpha - (alpha - beta) / (1. + (kappa * x)**delta)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def infer(self, handler):\n        \"\"\"\"\"\"\n        table = handler.table\n        new_self = self.copy()\n        new_self.set_table(table)\n        new_self.get_attr()\n        new_self.read_metadata(handler)\n        return new_self", "output": "infer this column from the table: create and return a new object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def grains():\n    '''\n    \n    '''\n    if not DETAILS.get('grains_cache', {}):\n        DETAILS['grains_cache'] = GRAINS_CACHE\n        try:\n            DETAILS['grains_cache'] = _get_grain_information()\n        except salt.exceptions.CommandExecutionError:\n            pass\n        except Exception as err:\n            log.error(err)\n    return DETAILS['grains_cache']", "output": "Get the grains from the proxied device", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_repo(repo, conffile='/usr/share/xbps.d/15-saltstack.conf'):\n    '''\n    \n    '''\n\n    if not _locate_repo_files(repo):\n        try:\n            with salt.utils.files.fopen(conffile, 'a+') as conf_file:\n                conf_file.write(\n                    salt.utils.stringutils.to_str(\n                        'repository={0}\\n'.format(repo)\n                    )\n                )\n        except IOError:\n            return False\n\n    return True", "output": "Add an XBPS repository to the system.\n\n    repo\n        url of repo to add (persistent).\n\n    conffile\n        path to xbps conf file to add this repo\n        default: /usr/share/xbps.d/15-saltstack.conf\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' pkg.add_repo <repo url> [conffile=/path/to/xbps/repo.conf]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _mod_bufsize_linux(iface, *args, **kwargs):\n    '''\n    \n    '''\n    ret = {'result': False,\n           'comment': 'Requires rx=<val> tx==<val> rx-mini=<val> and/or rx-jumbo=<val>'}\n    cmd = '/sbin/ethtool -G ' + iface\n    if not kwargs:\n        return ret\n    if args:\n        ret['comment'] = 'Unknown arguments: ' + ' '.join([six.text_type(item)\n                                                           for item in args])\n        return ret\n    eargs = ''\n    for kw in ['rx', 'tx', 'rx-mini', 'rx-jumbo']:\n        value = kwargs.get(kw)\n        if value is not None:\n            eargs += ' ' + kw + ' ' + six.text_type(value)\n    if not eargs:\n        return ret\n    cmd += eargs\n    out = __salt__['cmd.run'](cmd)\n    if out:\n        ret['comment'] = out\n    else:\n        ret['comment'] = eargs.strip()\n        ret['result'] = True\n    return ret", "output": "Modify network interface buffer sizes using ethtool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_den(self):\n        '''\n        \n        '''\n        keys = self.list_keys()\n        for status, keys in six.iteritems(self.list_keys()):\n            for key in keys[self.DEN]:\n                try:\n                    os.remove(os.path.join(self.opts['pki_dir'], status, key))\n                    eload = {'result': True,\n                             'act': 'delete',\n                             'id': key}\n                    self.event.fire_event(eload,\n                                          salt.utils.event.tagify(prefix='key'))\n                except (OSError, IOError):\n                    pass\n        self.check_minion_cache()\n        return self.list_keys()", "output": "Delete all denied keys", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copyPropList(self, target):\n        \"\"\" \"\"\"\n        if target is None: target__o = None\n        else: target__o = target._o\n        ret = libxml2mod.xmlCopyPropList(target__o, self._o)\n        if ret is None:raise treeError('xmlCopyPropList() failed')\n        __tmp = xmlAttr(_obj=ret)\n        return __tmp", "output": "Do a copy of an attribute list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rel_argmax(rel_probs, length, ensure_tree=True):\n    \"\"\"\n    \"\"\"\n    if ensure_tree:\n        rel_probs[:, ParserVocabulary.PAD] = 0\n        root = ParserVocabulary.ROOT\n        tokens = np.arange(1, length)\n        rel_preds = np.argmax(rel_probs, axis=1)\n        roots = np.where(rel_preds[tokens] == root)[0] + 1\n        if len(roots) < 1:\n            rel_preds[1 + np.argmax(rel_probs[tokens, root])] = root\n        elif len(roots) > 1:\n            root_probs = rel_probs[roots, root]\n            rel_probs[roots, root] = 0\n            new_rel_preds = np.argmax(rel_probs[roots], axis=1)\n            new_rel_probs = rel_probs[roots, new_rel_preds] / root_probs\n            new_root = roots[np.argmin(new_rel_probs)]\n            rel_preds[roots] = new_rel_preds\n            rel_preds[new_root] = root\n        return rel_preds\n    else:\n        rel_probs[:, ParserVocabulary.PAD] = 0\n        rel_preds = np.argmax(rel_probs, axis=1)\n        return rel_preds", "output": "Fix the relation prediction by heuristic rules\n\n    Parameters\n    ----------\n    rel_probs : NDArray\n        seq_len x rel_size\n    length :\n        real sentence length\n    ensure_tree :\n        whether to apply rules\n    Returns\n    -------\n    rel_preds : np.ndarray\n        prediction of relations of size (seq_len,)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _raise_timeout(self, err, url, timeout_value):\n        \"\"\"\"\"\"\n\n        if isinstance(err, SocketTimeout):\n            raise ReadTimeoutError(self, url, \"Read timed out. (read timeout=%s)\" % timeout_value)\n\n        # See the above comment about EAGAIN in Python 3. In Python 2 we have\n        # to specifically catch it and throw the timeout error\n        if hasattr(err, 'errno') and err.errno in _blocking_errnos:\n            raise ReadTimeoutError(self, url, \"Read timed out. (read timeout=%s)\" % timeout_value)\n\n        # Catch possible read timeouts thrown as SSL errors. If not the\n        # case, rethrow the original. We need to do this because of:\n        # http://bugs.python.org/issue10272\n        if 'timed out' in str(err) or 'did not complete (read)' in str(err):  # Python < 2.7.4\n            raise ReadTimeoutError(self, url, \"Read timed out. (read timeout=%s)\" % timeout_value)", "output": "Is the error actually a timeout? Will raise a ReadTimeout or pass", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_savename(self, original_filename):\r\n        \"\"\"\r\n        \"\"\"\r\n        if self.edit_filetypes is None:\r\n            self.edit_filetypes = get_edit_filetypes()\r\n        if self.edit_filters is None:\r\n            self.edit_filters = get_edit_filters()\r\n\r\n        # Don't use filters on KDE to not make the dialog incredible\r\n        # slow\r\n        # Fixes issue 4156\r\n        if is_kde_desktop() and not is_anaconda():\r\n            filters = ''\r\n            selectedfilter = ''\r\n        else:\r\n            filters = self.edit_filters\r\n            selectedfilter = get_filter(self.edit_filetypes,\r\n                                        osp.splitext(original_filename)[1])\r\n\r\n        self.redirect_stdio.emit(False)\r\n        filename, _selfilter = getsavefilename(self, _(\"Save file\"),\r\n                                    original_filename,\r\n                                    filters=filters,\r\n                                    selectedfilter=selectedfilter,\r\n                                    options=QFileDialog.HideNameFilterDetails)\r\n        self.redirect_stdio.emit(True)\r\n        if filename:\r\n            return osp.normpath(filename)\r\n        return None", "output": "Select a name to save a file.\r\n\r\n        Args:\r\n            original_filename: Used in the dialog to display the current file\r\n                    path and name.\r\n\r\n        Returns:\r\n            Normalized path for the selected file name or None if no name was\r\n            selected.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def schema_of_json(json, options={}):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(json, basestring):\n        col = _create_column_from_literal(json)\n    elif isinstance(json, Column):\n        col = _to_java_column(json)\n    else:\n        raise TypeError(\"schema argument should be a column or string\")\n\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.schema_of_json(col, options)\n    return Column(jc)", "output": "Parses a JSON string and infers its schema in DDL format.\n\n    :param json: a JSON string or a string literal containing a JSON string.\n    :param options: options to control parsing. accepts the same options as the JSON datasource\n\n    .. versionchanged:: 3.0\n       It accepts `options` parameter to control schema inferring.\n\n    >>> df = spark.range(1)\n    >>> df.select(schema_of_json(lit('{\"a\": 0}')).alias(\"json\")).collect()\n    [Row(json=u'struct<a:bigint>')]\n    >>> schema = schema_of_json('{a: 1}', {'allowUnquotedFieldNames':'true'})\n    >>> df.select(schema.alias(\"json\")).collect()\n    [Row(json=u'struct<a:bigint>')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_elt(modvars, keyword, match_last=False):\n    \"\"\n    keyword = strip_fastai(keyword)\n    if keyword in modvars: return modvars[keyword]\n    comps = keyword.split('.')\n    comp_elt = modvars.get(comps[0])\n    if hasattr(comp_elt, '__dict__'): return find_elt(comp_elt.__dict__, '.'.join(comps[1:]), match_last=match_last)", "output": "Attempt to resolve keywords such as Learner.lr_find. `match_last` starts matching from last component.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def columnCount(self, qindex=QModelIndex()):\r\n        \"\"\"\"\"\"\r\n        if self.total_cols <= self.cols_loaded:\r\n            return self.total_cols\r\n        else:\r\n            return self.cols_loaded", "output": "Array column number", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init(opts):\n    '''\n    \n    '''\n    NETWORK_DEVICE.update(salt.utils.napalm.get_device(opts))\n    DETAILS['initialized'] = True\n    return True", "output": "Opens the connection with the network device.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset_trial(self, trial, new_config, new_experiment_tag):\n        \"\"\"\n        \"\"\"\n        trial.experiment_tag = new_experiment_tag\n        trial.config = new_config\n        trainable = trial.runner\n        with warn_if_slow(\"reset_config\"):\n            reset_val = ray.get(trainable.reset_config.remote(new_config))\n        return reset_val", "output": "Tries to invoke `Trainable.reset_config()` to reset trial.\n\n        Args:\n            trial (Trial): Trial to be reset.\n            new_config (dict): New configuration for Trial\n                trainable.\n            new_experiment_tag (str): New experiment name\n                for trial.\n\n        Returns:\n            True if `reset_config` is successful else False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_serialized_info(self):\n    \"\"\"\"\"\"\n    # Flatten tf-example features dict\n    # Use NonMutableDict to ensure there is no collision between features keys\n    features_dict = utils.NonMutableDict()\n    for feature_key, feature in self._feature_dict.items():\n      serialized_info = feature.get_serialized_info()\n\n      # Features can be either containers (dict of other features) or plain\n      # features (ex: single tensor). Plain features have a None\n      # feature.features_keys\n      if not feature.serialized_keys:\n        features_dict[feature_key] = serialized_info\n      else:\n        # Sanity check which should always be True, as feature.serialized_keys\n        # is computed using feature.get_serialized_info()\n        _assert_keys_match(serialized_info.keys(), feature.serialized_keys)\n        features_dict.update({\n            posixpath.join(feature_key, k): v\n            for k, v in serialized_info.items()\n        })\n\n    return features_dict", "output": "See base class for details.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __get_region(conn, vm_):\n    '''\n    \n    '''\n    location = __get_location(conn, vm_)\n    region = '-'.join(location.name.split('-')[:2])\n\n    return conn.ex_get_region(region)", "output": "Return a GCE libcloud region object with matching name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def initial_refcounts(self, initial_terms):\n        \"\"\"\n        \n        \"\"\"\n        refcounts = self.graph.out_degree()\n        for t in self.outputs.values():\n            refcounts[t] += 1\n\n        for t in initial_terms:\n            self._decref_dependencies_recursive(t, refcounts, set())\n\n        return refcounts", "output": "Calculate initial refcounts for execution of this graph.\n\n        Parameters\n        ----------\n        initial_terms : iterable[Term]\n            An iterable of terms that were pre-computed before graph execution.\n\n        Each node starts with a refcount equal to its outdegree, and output\n        nodes get one extra reference to ensure that they're still in the graph\n        at the end of execution.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_endpoints(closed):\n    \"\"\"\n    \n    \"\"\"\n    left_closed = False\n    right_closed = False\n\n    if closed is None:\n        left_closed = True\n        right_closed = True\n    elif closed == \"left\":\n        left_closed = True\n    elif closed == \"right\":\n        right_closed = True\n    else:\n        raise ValueError(\"Closed has to be either 'left', 'right' or None\")\n\n    return left_closed, right_closed", "output": "Check that the `closed` argument is among [None, \"left\", \"right\"]\n\n    Parameters\n    ----------\n    closed : {None, \"left\", \"right\"}\n\n    Returns\n    -------\n    left_closed : bool\n    right_closed : bool\n\n    Raises\n    ------\n    ValueError : if argument is not among valid values", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_members(members, anycheck=False):\n    '''\n    \n\n    '''\n    if isinstance(members, list):\n\n        # This check will strip down members to a single any statement\n        if anycheck and 'any' in members:\n            return \"<member>any</member>\"\n        response = \"\"\n        for m in members:\n            response += \"<member>{0}</member>\".format(m)\n        return response\n    else:\n        return \"<member>{0}</member>\".format(members)", "output": "Builds a member formatted string for XML operation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_platforms(server_url):\n    '''\n    \n    '''\n    config = _get_asam_configuration(server_url)\n    if not config:\n        return False\n\n    url = config['platform_config_url']\n\n    data = {\n        'manual': 'false',\n    }\n\n    auth = (\n        config['username'],\n        config['password']\n    )\n\n    try:\n        html_content = _make_post_request(url, data, auth, verify=False)\n    except Exception as exc:\n        err_msg = \"Failed to look up existing platforms\"\n        log.error('%s:\\n%s', err_msg, exc)\n        return {server_url: err_msg}\n\n    parser = _parse_html_content(html_content)\n    platform_list = _get_platforms(parser.data)\n\n    if platform_list:\n        return {server_url: platform_list}\n    else:\n        return {server_url: \"No existing platforms found\"}", "output": "To list all ASAM platforms present on the Novell Fan-Out Driver\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run asam.list_platforms prov1.domain.com", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download_image(image_id, url, x1, y1, x2, y2, output_dir):\n  \"\"\"\"\"\"\n  output_filename = os.path.join(output_dir, image_id + '.png')\n  if os.path.exists(output_filename):\n    # Don't download image if it's already there\n    return True\n  try:\n    # Download image\n    url_file = urlopen(url)\n    if url_file.getcode() != 200:\n      return False\n    image_buffer = url_file.read()\n    # Crop, resize and save image\n    image = Image.open(BytesIO(image_buffer)).convert('RGB')\n    w = image.size[0]\n    h = image.size[1]\n    image = image.crop((int(x1 * w), int(y1 * h), int(x2 * w),\n                        int(y2 * h)))\n    image = image.resize((299, 299), resample=Image.ANTIALIAS)\n    image.save(output_filename)\n  except IOError:\n    return False\n  return True", "output": "Downloads one image, crops it, resizes it and saves it locally.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_horizontal_box(cls, children, layout=Layout()):\n        \"\"\n        return widgets.HBox(children, layout=layout)", "output": "Make a horizontal box with `children` and `layout`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attention_lm_ae_extended():\n  \"\"\"\"\"\"\n  hparams = attention_lm_moe_base_long_seq()\n  hparams.attention_layers = \"eeee\"\n  hparams.attention_local = True\n  # hparams.factored_logits=1  # Necessary when the number of expert grow bigger\n  hparams.attention_moe_k = 2\n  hparams.attention_exp_factor = 4\n  # hparams.attention_exp_inputdim = 128\n\n  hparams.layer_preprocess_sequence = \"n\"\n  hparams.layer_postprocess_sequence = \"da\"\n  return hparams", "output": "Experiment with the exp_factor params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sample(self, features):\n    \"\"\"\n    \"\"\"\n    logits, losses = self(features)  # pylint: disable=not-callable\n    if self._target_modality_is_real:\n      return logits, logits, losses  # Raw numbers returned from real modality.\n    if self.hparams.sampling_method == \"argmax\":\n      samples = tf.argmax(logits, axis=-1)\n    else:\n      assert self.hparams.sampling_method == \"random\"\n\n      def multinomial_squeeze(logits, temperature=1.0):\n        logits_shape = common_layers.shape_list(logits)\n        reshaped_logits = (\n            tf.reshape(logits, [-1, logits_shape[-1]]) / temperature)\n        choices = tf.multinomial(reshaped_logits, 1)\n        choices = tf.reshape(choices, logits_shape[:-1])\n        return choices\n\n      samples = multinomial_squeeze(logits, self.hparams.sampling_temp)\n\n    return samples, logits, losses", "output": "Run the model and extract samples.\n\n    Args:\n      features: an map of string to `Tensor`.\n\n    Returns:\n       samples: an integer `Tensor`.\n       logits: a list of `Tensor`s, one per datashard.\n       losses: a dictionary: {loss-name (string): floating point `Scalar`}.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _header_resized(self, row, old_height, new_height):\r\n        \"\"\"\"\"\"\r\n        self.table_header.setRowHeight(row, new_height)\r\n        self._update_layout()", "output": "Resize the corresponding row of the header section selected.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_common_actions(self):\r\n        \"\"\"\"\"\"\r\n        actions = FilteredDirView.setup_common_actions(self)\r\n\r\n        # Toggle horizontal scrollbar\r\n        hscrollbar_action = create_action(self, _(\"Show horizontal scrollbar\"),\r\n                                          toggled=self.toggle_hscrollbar)\r\n        hscrollbar_action.setChecked(self.show_hscrollbar)\r\n        self.toggle_hscrollbar(self.show_hscrollbar)\r\n\r\n        return actions + [hscrollbar_action]", "output": "Setup context menu common actions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lstat(self):\n        \"\"\"\n        \n        \"\"\"\n        if self._closed:\n            self._raise_closed()\n        return self._accessor.lstat(self)", "output": "Like stat(), except if the path points to a symlink, the symlink's\n        status information is returned, rather than its target's.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ConsumeAnyTypeUrl(self, tokenizer):\n    \"\"\"\"\"\"\n    # Consume \"type.googleapis.com/\".\n    tokenizer.ConsumeIdentifier()\n    tokenizer.Consume('.')\n    tokenizer.ConsumeIdentifier()\n    tokenizer.Consume('.')\n    tokenizer.ConsumeIdentifier()\n    tokenizer.Consume('/')\n    # Consume the fully-qualified type name.\n    name = [tokenizer.ConsumeIdentifier()]\n    while tokenizer.TryConsume('.'):\n      name.append(tokenizer.ConsumeIdentifier())\n    return '.'.join(name)", "output": "Consumes a google.protobuf.Any type URL and returns the type name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dataset_docs_str(datasets=None):\n  \"\"\"\n  \"\"\"\n  module_to_builder = make_module_to_builder_dict(datasets)\n\n  sections = sorted(list(module_to_builder.keys()))\n  section_tocs = []\n  section_docs = []\n  for section in sections:\n    builders = tf.nest.flatten(module_to_builder[section])\n    builders = sorted(builders, key=lambda b: b.name)\n    builder_docs = [document_single_builder(builder) for builder in builders]\n    section_doc = SECTION_DATASETS.format(\n        section_name=section, datasets=\"\\n\".join(builder_docs))\n    section_toc = create_section_toc(section, builders)\n\n\n    section_docs.append(section_doc)\n    section_tocs.append(section_toc)\n\n  full_doc = DOC.format(toc=\"\\n\".join(section_tocs),\n                        datasets=\"\\n\".join(section_docs))\n  return full_doc", "output": "Create dataset documentation string for given datasets.\n\n  Args:\n    datasets: list of datasets for which to create documentation.\n              If None, then all available datasets will be used.\n\n  Returns:\n    string describing the datasets (in the MarkDown format).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def full_name(cla55: Optional[type]) -> str:\n    \"\"\"\n    \n    \"\"\"\n    # Special case to handle None:\n    if cla55 is None:\n        return \"?\"\n\n    if issubclass(cla55, Initializer) and cla55 not in [Initializer, PretrainedModelInitializer]:\n        init_fn = cla55()._init_function\n        return f\"{init_fn.__module__}.{init_fn.__name__}\"\n\n    origin = getattr(cla55, '__origin__', None)\n    args = getattr(cla55, '__args__', ())\n\n    # Special handling for compound types\n    if origin in (Dict, dict):\n        key_type, value_type = args\n        return f\"\"\"Dict[{full_name(key_type)}, {full_name(value_type)}]\"\"\"\n    elif origin in (Tuple, tuple, List, list, Sequence, collections.abc.Sequence):\n        return f\"\"\"{_remove_prefix(str(origin))}[{\", \".join(full_name(arg) for arg in args)}]\"\"\"\n    elif origin == Union:\n        # Special special case to handle optional types:\n        if len(args) == 2 and args[-1] == type(None):\n            return f\"\"\"Optional[{full_name(args[0])}]\"\"\"\n        else:\n            return f\"\"\"Union[{\", \".join(full_name(arg) for arg in args)}]\"\"\"\n    else:\n        return _remove_prefix(f\"{cla55.__module__}.{cla55.__name__}\")", "output": "Return the full name (including module) of the given class.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __django_auth_setup():\n    '''\n    \n    '''\n    if django.VERSION >= (1, 7):\n        django.setup()\n\n    global DJANGO_AUTH_CLASS\n\n    if DJANGO_AUTH_CLASS is not None:\n        return\n\n    # Versions 1.7 and later of Django don't pull models until\n    # they are needed.  When using framework facilities outside the\n    # web application container we need to run django.setup() to\n    # get the model definitions cached.\n    if '^model' in __opts__['external_auth']['django']:\n        django_model_fullname = __opts__['external_auth']['django']['^model']\n        django_model_name = django_model_fullname.split('.')[-1]\n        django_module_name = '.'.join(django_model_fullname.split('.')[0:-1])\n\n        django_auth_module = __import__(django_module_name, globals(), locals(), 'SaltExternalAuthModel')\n        DJANGO_AUTH_CLASS_str = 'django_auth_module.{0}'.format(django_model_name)\n        DJANGO_AUTH_CLASS = eval(DJANGO_AUTH_CLASS_str)", "output": "Prepare the connection to the Django authentication framework", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_missing_indexer(indexer):\n    \"\"\"\n    \n    \"\"\"\n\n    if isinstance(indexer, dict):\n\n        # a missing key (but not a tuple indexer)\n        indexer = indexer['key']\n\n        if isinstance(indexer, bool):\n            raise KeyError(\"cannot use a single bool to index into setitem\")\n        return indexer, True\n\n    return indexer, False", "output": "reverse convert a missing indexer, which is a dict\n    return the scalar indexer and a boolean indicating if we converted", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def py3round(number):\n    \"\"\"\"\"\"\n    if abs(round(number) - number) == 0.5:\n        return int(2.0 * round(number / 2.0))\n\n    return int(round(number))", "output": "Unified rounding in all python versions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_all():\n    '''\n    \n    '''\n    ret = set()\n    lines = glob.glob('/etc/init.d/*')\n    for line in lines:\n        service = line.split('/etc/init.d/')[1]\n        # Remove README.  If it's an enabled service, it will be added back in.\n        if service != 'README':\n            ret.add(service)\n    return sorted(ret | set(get_enabled()))", "output": "Return all available boot services\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.get_all", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sanitize_host(host):\n    '''\n    \n    '''\n    RFC952_characters = ascii_letters + digits + \".-\"\n    return \"\".join([c for c in host[0:255] if c in RFC952_characters])", "output": "Sanitize host string.\n    https://tools.ietf.org/html/rfc1123#section-2.1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_sizes(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_sizes function must be called with '\n            '-f or --function, or with the --list-sizes option'\n        )\n\n    items = query(method='sizes', command='?per_page=100')\n    ret = {}\n    for size in items['sizes']:\n        ret[size['slug']] = {}\n        for item in six.iterkeys(size):\n            ret[size['slug']][item] = six.text_type(size[item])\n\n    return ret", "output": "Return a list of the image sizes that are on the provider", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        '''\n        \n        '''\n        import salt.key\n        self.parse_args()\n\n        self.setup_logfile_logger()\n        verify_log(self.config)\n\n        key = salt.key.KeyCLI(self.config)\n        if check_user(self.config['user']):\n            key.run()", "output": "Execute salt-key", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_get_hkfund_list(ip=None, port=None):\n    \"\"\"\n\n    \"\"\"\n\n    global extension_market_list\n    extension_market_list = QA_fetch_get_extensionmarket_list(\n    ) if extension_market_list is None else extension_market_list\n\n    return extension_market_list.query('market==49')", "output": "[summary]\n\n    Keyword Arguments:\n        ip {[type]} -- [description] (default: {None})\n        port {[type]} -- [description] (default: {None})\n\n    # \u6e2f\u80a1 HKMARKET\n        27         5      \u9999\u6e2f\u6307\u6570         FH\n        31         2      \u9999\u6e2f\u4e3b\u677f         KH\n        48         2     \u9999\u6e2f\u521b\u4e1a\u677f         KG\n        49         2      \u9999\u6e2f\u57fa\u91d1         KT\n        43         1     B\u80a1\u8f6cH\u80a1         HB", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def summary_pb(self):\n    \"\"\"\n    \"\"\"\n    hparam_infos = []\n    for hparam in self._hparams:\n      info = api_pb2.HParamInfo(\n          name=hparam.name,\n          description=hparam.description,\n          display_name=hparam.display_name,\n      )\n      domain = hparam.domain\n      if domain is not None:\n        domain.update_hparam_info(info)\n      hparam_infos.append(info)\n    metric_infos = [metric.as_proto() for metric in self._metrics]\n    return summary.experiment_pb(\n        hparam_infos=hparam_infos,\n        metric_infos=metric_infos,\n        user=self._user,\n        description=self._description,\n        time_created_secs=self._time_created_secs,\n    )", "output": "Create a top-level experiment summary describing this experiment.\n\n    The resulting summary should be written to a log directory that\n    encloses all the individual sessions' log directories.\n\n    Analogous to the low-level `experiment_pb` function in the\n    `hparams.summary` module.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_exists(name, runas=None):\n    '''\n    \n    '''\n    if runas is None and not salt.utils.platform.is_windows():\n        runas = salt.utils.user.get_user()\n    return name in list_users(runas=runas)", "output": "Return whether the user exists based on rabbitmqctl list_users.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rabbitmq.user_exists rabbit_user", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def join_path(fname:PathOrStr, path:PathOrStr='.')->Path:\n    \"\"\n    return Path(path)/Path(fname)", "output": "Return `Path(path)/Path(fname)`, `path` defaults to current dir.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def backward(self, req, out_grad, in_data, out_data, in_grad, aux):\n        \"\"\"\n        \"\"\"\n        label = in_data[1]\n        pred = out_data[0]\n        dx = pred - mx.nd.one_hot(label, 2)\n        pos_cls_weight = self.positive_cls_weight\n        scale_factor = ((1 + label * pos_cls_weight) / pos_cls_weight).reshape((pred.shape[0],1))\n        rescaled_dx = scale_factor * dx\n        self.assign(in_grad[0], req[0], rescaled_dx)", "output": "Implements backward computation\n\n        req : list of {'null', 'write', 'inplace', 'add'}, how to assign to in_grad\n        out_grad : list of NDArray, gradient w.r.t. output data.\n        in_grad : list of NDArray, gradient w.r.t. input data. This is the output buffer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enable_death_signal(_warn=True):\n    \"\"\"\n    \n    \"\"\"\n    if platform.system() != 'Linux':\n        return\n    try:\n        import prctl    # pip install python-prctl\n    except ImportError:\n        if _warn:\n            log_once('\"import prctl\" failed! Install python-prctl so that processes can be cleaned with guarantee.',\n                     'warn')\n        return\n    else:\n        assert hasattr(prctl, 'set_pdeathsig'), \\\n            \"prctl.set_pdeathsig does not exist! Note that you need to install 'python-prctl' instead of 'prctl'.\"\n        # is SIGHUP a good choice?\n        prctl.set_pdeathsig(signal.SIGHUP)", "output": "Set the \"death signal\" of the current process, so that\n    the current process will be cleaned with guarantee\n    in case the parent dies accidentally.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def the_gui(gui_queue):\n    \"\"\"\n    \n    \"\"\"\n    layout = [ [sg.Text('Multithreaded Window Example')],\n               [sg.Text('', size=(15,1), key='_OUTPUT_')],\n               [sg.Output(size=(40,6))],\n               [sg.Button('Exit')],]\n\n    window = sg.Window('Multithreaded Window').Layout(layout)\n    # --------------------- EVENT LOOP ---------------------\n    while True:\n        event, values = window.Read(timeout=100)        # wait for up to 100 ms for a GUI event\n        if event is None or event == 'Exit':\n            break\n        #--------------- Loop through all messages coming in from threads ---------------\n        while True:                 # loop executes until runs out of messages in Queue\n            try:                    # see if something has been posted to Queue\n                message = gui_queue.get_nowait()\n            except queue.Empty:     # get_nowait() will get exception when Queue is empty\n                break               # break from the loop if no more messages are queued up\n            # if message received from queue, display the message in the Window\n            if message:\n                window.Element('_OUTPUT_').Update(message)\n                window.Refresh()    # do a refresh because could be showing multiple messages before next Read\n\n    # if user exits the window, then close the window and exit the GUI func\n    window.Close()", "output": "Starts and executes the GUI\n    Reads data from a Queue and displays the data to the window\n    Returns when the user exits / closes the window\n        (that means it does NOT return until the user exits the window)\n    :param gui_queue: Queue the GUI should read from\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_image(self, name):\r\n        \"\"\"\"\"\"\r\n        command = \"%s.show()\" % name\r\n        sw = self.shellwidget\r\n        if sw._reading:\r\n            sw.kernel_client.input(command)\r\n        else:\r\n            sw.execute(command)", "output": "Show image (item is a PIL image)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ensure_str(s, encoding='utf-8', errors='strict'):\n    \"\"\"\n    \"\"\"\n    if not isinstance(s, (text_type, binary_type)):\n        raise TypeError(\"not expecting type '%s'\" % type(s))\n    if PY2 and isinstance(s, text_type):\n        s = s.encode(encoding, errors)\n    elif PY3 and isinstance(s, binary_type):\n        s = s.decode(encoding, errors)\n    return s", "output": "Coerce *s* to `str`.\n\n    For Python 2:\n      - `unicode` -> encoded to `str`\n      - `str` -> `str`\n\n    For Python 3:\n      - `str` -> `str`\n      - `bytes` -> decoded to `str`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gid_to_group(gid):\n    '''\n    \n    '''\n    try:\n        gid = int(gid)\n    except ValueError:\n        # This is not an integer, maybe it's already the group name?\n        gid = group_to_gid(gid)\n\n    if gid == '':\n        # Don't even bother to feed it to grp\n        return ''\n\n    try:\n        return grp.getgrgid(gid).gr_name\n    except (KeyError, NameError):\n        # If group is not present, fall back to the gid.\n        return gid", "output": "Convert the group id to the group name on this system\n\n    gid\n        gid to convert to a group name\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.gid_to_group 0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare(self, config_file=None, user=None, password=None, **kwargs):\n        \"\"\"\n        \"\"\"\n        if config_file is not None:\n            self.read_config(config_file)\n        else:\n            self._prepare_account(user, password, **kwargs)\n        self.autologin()", "output": "\u767b\u5f55\u7684\u7edf\u4e00\u63a5\u53e3\n        :param config_file \u767b\u5f55\u6570\u636e\u6587\u4ef6\uff0c\u82e5\u65e0\u5219\u9009\u62e9\u53c2\u6570\u767b\u5f55\u6a21\u5f0f\n        :param user: \u5404\u5bb6\u5238\u5546\u7684\u8d26\u53f7\u6216\u8005\u96ea\u7403\u7684\u7528\u6237\u540d\n        :param password: \u5bc6\u7801, \u5238\u5546\u4e3a\u52a0\u5bc6\u540e\u7684\u5bc6\u7801\uff0c\u96ea\u7403\u4e3a\u660e\u6587\u5bc6\u7801\n        :param account: [\u96ea\u7403\u767b\u5f55\u9700\u8981]\u96ea\u7403\u624b\u673a\u53f7(\u90ae\u7bb1\u624b\u673a\u4e8c\u9009\u4e00)\n        :param portfolio_code: [\u96ea\u7403\u767b\u5f55\u9700\u8981]\u7ec4\u5408\u4ee3\u7801\n        :param portfolio_market: [\u96ea\u7403\u767b\u5f55\u9700\u8981]\u4ea4\u6613\u5e02\u573a\uff0c\n            \u53ef\u9009['cn', 'us', 'hk'] \u9ed8\u8ba4 'cn'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def refresh_method_objects(self):\n        \"\"\"\"\"\"\n        self.method_object_dict = {}\n        for key, method in self.MATCHING_METHODS.items():\n            method_object = method(self.im_search, self.im_source, self.threshold, self.rgb)\n            self.method_object_dict.update({key: method_object})", "output": "\u521d\u59cb\u5316\u65b9\u6cd5\u5bf9\u8c61.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def epoch_rates_to_pmf(problems, epoch_rates=None):\n  \"\"\"\n  \"\"\"\n  if epoch_rates is None:\n    epoch_rates = [1.0] * len(problems)\n  example_rates = [epoch_rate * p.num_training_examples\n                   for p, epoch_rate in zip(problems, epoch_rates)]\n  return example_rates_to_pmf(example_rates)", "output": "Create a probability-mass-function based on relative epoch rates.\n\n  if epoch_rates=None, then we use uniform epoch rates [1.0] * len(problems)\n  i.e. it takes each problem the same time to go through one epoch.\n\n  If epoch_rates is given, then these are the relative numbers of epochs\n  of each problem to go through in a given amount of time.\n\n  Each must have problem.num_training_examples implemented.\n\n  Args:\n    problems: a list of Problem instances.\n    epoch_rates: an optional list of float\n\n  Returns:\n    a list of floating point values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_gui(self):\n        \"\"\"\"\"\"\n        layout = QGridLayout(self)\n        layout.setContentsMargins(0, 0, 0, 0)\n        layout.addWidget(self.canvas, 0, 1)\n        layout.addLayout(self.setup_toolbar(), 0, 3, 2, 1)\n\n        layout.setColumnStretch(0, 100)\n        layout.setColumnStretch(2, 100)\n        layout.setRowStretch(1, 100)", "output": "Setup the main layout of the widget.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _try_coerce_args(self, values, other):\n        \"\"\"\n        \n        \"\"\"\n\n        values = values.view('i8')\n\n        if isinstance(other, bool):\n            raise TypeError\n        elif is_null_datetimelike(other):\n            other = tslibs.iNaT\n        elif isinstance(other, (datetime, np.datetime64, date)):\n            other = self._box_func(other)\n            if getattr(other, 'tz') is not None:\n                raise TypeError(\"cannot coerce a Timestamp with a tz on a \"\n                                \"naive Block\")\n            other = other.asm8.view('i8')\n        elif hasattr(other, 'dtype') and is_datetime64_dtype(other):\n            other = other.astype('i8', copy=False).view('i8')\n        else:\n            # coercion issues\n            # let higher levels handle\n            raise TypeError(other)\n\n        return values, other", "output": "Coerce values and other to dtype 'i8'. NaN and NaT convert to\n        the smallest i8, and will correctly round-trip to NaT if converted\n        back in _try_coerce_result. values is always ndarray-like, other\n        may not be\n\n        Parameters\n        ----------\n        values : ndarray-like\n        other : ndarray-like or scalar\n\n        Returns\n        -------\n        base-type values, base-type other", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def analyze(self, filename):\n        \"\"\"\"\"\"\n        if self.dockwidget and not self.ismaximized:\n            self.dockwidget.setVisible(True)\n            self.dockwidget.setFocus()\n            self.dockwidget.raise_()\n        pythonpath = self.main.get_spyder_pythonpath()\n        runconf = get_run_configuration(filename)\n        wdir, args = None, []\n        if runconf is not None:\n            if runconf.wdir_enabled:\n                wdir = runconf.wdir\n            if runconf.args_enabled:\n                args = runconf.args\n        self.profiler.analyze(filename, wdir=wdir, args=args,\n                              pythonpath=pythonpath)", "output": "Reimplement analyze method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _reset_kvstore(self):\n        \"\"\"\"\"\"\n        if self._kvstore and 'dist' in self._kvstore.type:\n            raise RuntimeError(\"Cannot reset distributed KVStore.\")\n        self._kv_initialized = False\n        self._kvstore = None\n        self._distributed = None\n        self._update_on_kvstore = None\n        self._params_to_init = [param for param in self._params]", "output": "Reset kvstore.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_configured_provider():\n    '''\n    \n    '''\n    return config.is_provider_configured(\n        __opts__, __active_provider_name__ or __virtualname__,\n        ('auth', 'region_name'), log_message=False,\n    ) or config.is_provider_configured(\n        __opts__, __active_provider_name__ or __virtualname__,\n        ('cloud', 'region_name')\n    )", "output": "Return the first configured instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def variable_name_from_full_name(full_name):\n    \"\"\"\n    \"\"\"\n    projects, _, configs, _, variables, result = full_name.split(\"/\", 5)\n    if projects != \"projects\" or configs != \"configs\" or variables != \"variables\":\n        raise ValueError(\n            \"Unexpected format of resource\",\n            full_name,\n            'Expected \"projects/{proj}/configs/{cfg}/variables/...\"',\n        )\n    return result", "output": "Extract the variable name from a full resource name.\n\n      >>> variable_name_from_full_name(\n              'projects/my-proj/configs/my-config/variables/var-name')\n      \"var-name\"\n      >>> variable_name_from_full_name(\n              'projects/my-proj/configs/my-config/variables/another/var/name')\n      \"another/var/name\"\n\n    :type full_name: str\n    :param full_name:\n        The full resource name of a variable. The full resource name looks like\n        ``projects/prj-name/configs/cfg-name/variables/var-name`` and is\n        returned as the ``name`` field of a variable resource.  See\n        https://cloud.google.com/deployment-manager/runtime-configurator/reference/rest/v1beta1/projects.configs.variables\n\n    :rtype: str\n    :returns: The variable's short name, given its full resource name.\n    :raises: :class:`ValueError` if ``full_name`` is not the expected format", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stats(self, node_id=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        url = '/_cluster/stats'\n        if node_id:\n            url = _make_path('_cluster/stats/nodes', node_id)\n        return self.transport.perform_request('GET', url, params=params)", "output": "The Cluster Stats API allows to retrieve statistics from a cluster wide\n        perspective. The API returns basic index metrics and information about\n        the current nodes that form the cluster.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-stats.html>`_\n\n        :arg node_id: A comma-separated list of node IDs or names to limit the\n            returned information; use `_local` to return information from the\n            node you're connecting to, leave empty to get information from all\n            nodes\n        :arg flat_settings: Return settings in flat format (default: false)\n        :arg timeout: Explicit operation timeout", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_distribution_names(self):\n        \"\"\"\n        \n        \"\"\"\n        result = set()\n        for locator in self.locators:\n            try:\n                result |= locator.get_distribution_names()\n            except NotImplementedError:\n                pass\n        return result", "output": "Return all the distribution names known to this locator.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cint8_array_to_numpy(cptr, length):\n    \"\"\"\"\"\"\n    if isinstance(cptr, ctypes.POINTER(ctypes.c_int8)):\n        return np.fromiter(cptr, dtype=np.int8, count=length)\n    else:\n        raise RuntimeError('Expected int pointer')", "output": "Convert a ctypes int pointer array to a numpy array.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stdkey_home(self, shift, ctrl, prompt_pos=None):\r\n        \"\"\"\"\"\"\r\n        move_mode = self.__get_move_mode(shift)\r\n        if ctrl:\r\n            self.moveCursor(QTextCursor.Start, move_mode)\r\n        else:\r\n            cursor = self.textCursor()\r\n            if prompt_pos is None:\r\n                start_position = self.get_position('sol')\r\n            else:\r\n                start_position = self.get_position(prompt_pos)\r\n            text = self.get_text(start_position, 'eol')\r\n            indent_pos = start_position+len(text)-len(text.lstrip())\r\n            if cursor.position() != indent_pos:\r\n                cursor.setPosition(indent_pos, move_mode)\r\n            else:\r\n                cursor.setPosition(start_position, move_mode)\r\n            self.setTextCursor(cursor)", "output": "Smart HOME feature: cursor is first moved at\r\n        indentation position, then at the start of the line", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_install(self, editor):\n        \"\"\"\n        \n        \"\"\"\n        EditorExtension.on_install(self, editor)\n        self.setParent(editor)\n        self.setPalette(QApplication.instance().palette())\n        self.setFont(QApplication.instance().font())\n        self.editor.panels.refresh()\n        self._background_brush = QBrush(QColor(\n            self.palette().window().color()))\n        self._foreground_pen = QPen(QColor(\n            self.palette().windowText().color()))\n\n        if self.position == self.Position.FLOATING:\n            self.setAttribute(Qt.WA_TransparentForMouseEvents)", "output": "Extends :meth:`spyder.api.EditorExtension.on_install` method to set the\n        editor instance as the parent widget.\n\n        .. warning:: Don't forget to call **super** if you override this\n            method!\n\n        :param editor: editor instance\n        :type editor: spyder.plugins.editor.widgets.codeeditor.CodeEditor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mksls(src, dst=None):\n    '''\n    \n    '''\n    with salt.utils.files.fopen(src, 'r') as fh_:\n        ps_opts = xml.to_dict(ET.fromstring(fh_.read()))\n\n    if dst is not None:\n        with salt.utils.files.fopen(dst, 'w') as fh_:\n            salt.utils.yaml.safe_dump(ps_opts, fh_, default_flow_style=False)\n    else:\n        return salt.utils.yaml.safe_dump(ps_opts, default_flow_style=False)", "output": "Convert an AutoYAST file to an SLS file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_client_for_file(self, filename):\r\n        \"\"\"\"\"\"\r\n        client = None\r\n        for idx, cl in enumerate(self.get_clients()):\r\n            if self.filenames[idx] == filename:\r\n                self.tabwidget.setCurrentIndex(idx)\r\n                client = cl\r\n                break\r\n        return client", "output": "Get client associated with a given file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_trial(self, trial):\n        \"\"\"\"\"\"\n        assert not self.filled(), \"Cannot add trial to filled bracket!\"\n        self._live_trials[trial] = None\n        self._all_trials.append(trial)", "output": "Add trial to bracket assuming bracket is not filled.\n\n        At a later iteration, a newly added trial will be given equal\n        opportunity to catch up.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def label_absent(name, node=None, apiserver_url=None):\n    '''\n    \n\n    '''\n    ret = {'name': name, 'result': True, 'comment': '', 'changes': {}}\n\n    # Get salt minion ID\n    node = _guess_node_id(node)\n    # Try to get kubernetes master\n    apiserver_url = _guess_apiserver(apiserver_url)\n    if apiserver_url is None:\n        return False\n\n    # Get all labels\n    old_labels = _get_labels(node, apiserver_url)\n    # Prepare a temp labels dict\n    labels = dict([(key, value) for key, value in old_labels.items()\n                   if key != name])\n    # Compare old labels and what we want\n    if labels == old_labels:\n        # Label already absent\n        ret['comment'] = \"Label {0} already absent\".format(name)\n    else:\n        # Label needs to be delete\n        res = _set_labels(node, apiserver_url, labels)\n        if res.get('status') == 409:\n            # there is an update during operation, need to retry\n            log.debug(\"Got 409, will try later\")\n            ret['changes'] = {}\n            ret['comment'] = \"Could not delete label {0}, please retry\".format(name)\n        else:\n            ret['changes'] = {\"deleted\": name}\n            ret['comment'] = \"Label {0} absent\".format(name)\n\n    return ret", "output": ".. versionadded:: 2016.3.0\n\n    Delete label to the current node\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' k8s.label_absent hw/disktype\n        salt '*' k8s.label_absent hw/disktype kube-node.cluster.local http://kube-master.cluster.local", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def data_mnist(datadir=tempfile.gettempdir(), train_start=0,\n               train_end=60000, test_start=0, test_end=10000):\n  \"\"\"\n  \n  \"\"\"\n  assert isinstance(train_start, int)\n  assert isinstance(train_end, int)\n  assert isinstance(test_start, int)\n  assert isinstance(test_end, int)\n\n  X_train = download_and_parse_mnist_file(\n      'train-images-idx3-ubyte.gz', datadir=datadir) / 255.\n  Y_train = download_and_parse_mnist_file(\n      'train-labels-idx1-ubyte.gz', datadir=datadir)\n  X_test = download_and_parse_mnist_file(\n      't10k-images-idx3-ubyte.gz', datadir=datadir) / 255.\n  Y_test = download_and_parse_mnist_file(\n      't10k-labels-idx1-ubyte.gz', datadir=datadir)\n\n  X_train = np.expand_dims(X_train, -1)\n  X_test = np.expand_dims(X_test, -1)\n\n  X_train = X_train[train_start:train_end]\n  Y_train = Y_train[train_start:train_end]\n  X_test = X_test[test_start:test_end]\n  Y_test = Y_test[test_start:test_end]\n\n  Y_train = utils.to_categorical(Y_train, nb_classes=10)\n  Y_test = utils.to_categorical(Y_test, nb_classes=10)\n  return X_train, Y_train, X_test, Y_test", "output": "Load and preprocess MNIST dataset\n  :param datadir: path to folder where data should be stored\n  :param train_start: index of first training set example\n  :param train_end: index of last training set example\n  :param test_start: index of first test set example\n  :param test_end: index of last test set example\n  :return: tuple of four arrays containing training data, training labels,\n           testing data and testing labels.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_namespaced_deployment_status(self, name, namespace, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_deployment_status_with_http_info(name, namespace, body, **kwargs)\n        else:\n            (data) = self.replace_namespaced_deployment_status_with_http_info(name, namespace, body, **kwargs)\n            return data", "output": "replace status of the specified Deployment\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_deployment_status(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Deployment (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1Deployment body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1Deployment\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_entry(entry, status, directives):\n    '''\n    '''\n    for directive, state in six.iteritems(directives):\n        if directive == 'delete_others':\n            status['delete_others'] = state\n            continue\n        for attr, vals in six.iteritems(state):\n            status['mentioned_attributes'].add(attr)\n            vals = _toset(vals)\n            if directive == 'default':\n                if vals and (attr not in entry or not entry[attr]):\n                    entry[attr] = vals\n            elif directive == 'add':\n                vals.update(entry.get(attr, ()))\n                if vals:\n                    entry[attr] = vals\n            elif directive == 'delete':\n                existing_vals = entry.pop(attr, OrderedSet())\n                if vals:\n                    existing_vals -= vals\n                    if existing_vals:\n                        entry[attr] = existing_vals\n            elif directive == 'replace':\n                entry.pop(attr, None)\n                if vals:\n                    entry[attr] = vals\n            else:\n                raise ValueError('unknown directive: ' + directive)", "output": "Update an entry's attributes using the provided directives\n\n    :param entry:\n        A dict mapping each attribute name to a set of its values\n    :param status:\n        A dict holding cross-invocation status (whether delete_others\n        is True or not, and the set of mentioned attributes)\n    :param directives:\n        A dict mapping directive types to directive-specific state", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_autosave_filename(self, filename):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            autosave_filename = self.name_mapping[filename]\n        except KeyError:\n            autosave_dir = get_conf_path('autosave')\n            if not osp.isdir(autosave_dir):\n                try:\n                    os.mkdir(autosave_dir)\n                except EnvironmentError as error:\n                    action = _('Error while creating autosave directory')\n                    msgbox = AutosaveErrorDialog(action, error)\n                    msgbox.exec_if_enabled()\n            autosave_filename = self.create_unique_autosave_filename(\n                    filename, autosave_dir)\n            self.name_mapping[filename] = autosave_filename\n            self.stack.sig_option_changed.emit(\n                    'autosave_mapping', self.name_mapping)\n            logger.debug('New autosave file name')\n        return autosave_filename", "output": "Get name of autosave file for specified file name.\n\n        This function uses the dict in `self.name_mapping`. If `filename` is\n        in the mapping, then return the corresponding autosave file name.\n        Otherwise, construct a unique file name and update the mapping.\n\n        Args:\n            filename (str): original file name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filter_with_english_letters(buf):\n        \"\"\"\n        \n        \"\"\"\n        filtered = bytearray()\n        in_tag = False\n        prev = 0\n\n        for curr in range(len(buf)):\n            # Slice here to get bytes instead of an int with Python 3\n            buf_char = buf[curr:curr + 1]\n            # Check if we're coming out of or entering an HTML tag\n            if buf_char == b'>':\n                in_tag = False\n            elif buf_char == b'<':\n                in_tag = True\n\n            # If current character is not extended-ASCII and not alphabetic...\n            if buf_char < b'\\x80' and not buf_char.isalpha():\n                # ...and we're not in a tag\n                if curr > prev and not in_tag:\n                    # Keep everything after last non-extended-ASCII,\n                    # non-alphabetic character\n                    filtered.extend(buf[prev:curr])\n                    # Output a space to delimit stretch we kept\n                    filtered.extend(b' ')\n                prev = curr + 1\n\n        # If we're not in a tag...\n        if not in_tag:\n            # Keep everything after last non-extended-ASCII, non-alphabetic\n            # character\n            filtered.extend(buf[prev:])\n\n        return filtered", "output": "Returns a copy of ``buf`` that retains only the sequences of English\n        alphabet and high byte characters that are not between <> characters.\n        Also retains English alphabet and high byte characters immediately\n        before occurrences of >.\n\n        This filter can be applied to all scripts which contain both English\n        characters and extended ASCII characters, but is currently only used by\n        ``Latin1Prober``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_groups(name):\n    '''\n    \n    '''\n    groups = [group for group in salt.utils.user.get_group_list(name)]\n    return groups", "output": "Return a list of groups the named user belongs to.\n\n    name\n\n        The name of the user for which to list groups. Starting in Salt 2016.11.0,\n        all groups for the user, including groups beginning with an underscore\n        will be listed.\n\n        .. versionchanged:: 2016.11.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.list_groups foo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_wake_on_network(enabled):\n    '''\n    \n    '''\n    state = salt.utils.mac_utils.validate_enabled(enabled)\n    cmd = 'systemsetup -setwakeonnetworkaccess {0}'.format(state)\n    salt.utils.mac_utils.execute_return_success(cmd)\n\n    return salt.utils.mac_utils.confirm_updated(\n        state,\n        get_wake_on_network,\n    )", "output": "Set whether or not the computer will wake from sleep when network activity\n    is detected.\n\n    :param bool enabled: True to enable, False to disable. \"On\" and \"Off\" are\n        also acceptable values. Additionally you can pass 1 and 0 to represent\n        True and False respectively\n\n    :return: True if successful, False if not\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' power.set_wake_on_network True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_os(self):\n        '''\n        \n        '''\n        return {\n            'name': self._grain('os'),\n            'family': self._grain('os_family'),\n            'arch': self._grain('osarch'),\n            'release': self._grain('osrelease'),\n        }", "output": "Get operating system summary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extract(self, path_or_paths):\n    \"\"\"\n    \"\"\"\n    # Add progress bar to follow the download state\n    with self._extractor.tqdm():\n      return _map_promise(self._extract, path_or_paths)", "output": "Extract given path(s).\n\n    Args:\n      path_or_paths: path or `list`/`dict` of path of file to extract. Each\n        path can be a `str` or `tfds.download.Resource`.\n\n    If not explicitly specified in `Resource`, the extraction method is deduced\n    from downloaded file name.\n\n    Returns:\n      extracted_path(s): `str`, The extracted paths matching the given input\n        path_or_paths.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_subnets(call=None, kwargs=None):\n    '''\n    \n    '''\n    if kwargs is None:\n        kwargs = {}\n\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_sizes function must be called with '\n            '-f or --function'\n        )\n\n    netconn = get_conn(client_type='network')\n\n    resource_group = kwargs.get('resource_group') or config.get_cloud_config_value(\n                         'resource_group',\n                         get_configured_provider(), __opts__, search_global=False\n                     )\n\n    if not resource_group and 'group' in kwargs and 'resource_group' not in kwargs:\n        resource_group = kwargs['group']\n\n    if not resource_group:\n        raise SaltCloudSystemExit(\n            'A resource group must be specified'\n        )\n\n    if kwargs.get('network') is None:\n        kwargs['network'] = config.get_cloud_config_value(\n            'network', get_configured_provider(), __opts__, search_global=False\n        )\n\n    if 'network' not in kwargs or kwargs['network'] is None:\n        raise SaltCloudSystemExit(\n            'A \"network\" must be specified'\n        )\n\n    ret = {}\n    subnets = netconn.subnets.list(resource_group, kwargs['network'])\n    for subnet in subnets:\n        ret[subnet.name] = subnet.as_dict()\n        ret[subnet.name]['ip_configurations'] = {}\n        for ip_ in subnet.ip_configurations:\n            comps = ip_.id.split('/')\n            name = comps[-1]\n            ret[subnet.name]['ip_configurations'][name] = ip_.as_dict()\n            ret[subnet.name]['ip_configurations'][name]['subnet'] = subnet.name\n        ret[subnet.name]['resource_group'] = resource_group\n    return ret", "output": "List subnets in a virtual network.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nunique(self, axis=0, dropna=True):\n        \"\"\"\n        \n        \"\"\"\n        return self.apply(Series.nunique, axis=axis, dropna=dropna)", "output": "Count distinct observations over requested axis.\n\n        Return Series with number of distinct observations. Can ignore NaN\n        values.\n\n        .. versionadded:: 0.20.0\n\n        Parameters\n        ----------\n        axis : {0 or 'index', 1 or 'columns'}, default 0\n            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for\n            column-wise.\n        dropna : bool, default True\n            Don't include NaN in the counts.\n\n        Returns\n        -------\n        Series\n\n        See Also\n        --------\n        Series.nunique: Method nunique for Series.\n        DataFrame.count: Count non-NA cells for each column or row.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [1, 1, 1]})\n        >>> df.nunique()\n        A    3\n        B    1\n        dtype: int64\n\n        >>> df.nunique(axis=1)\n        0    1\n        1    2\n        2    2\n        dtype: int64", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getTableMisnestedNodePosition(self):\n        \"\"\"\"\"\"\n        # The foster parent element is the one which comes before the most\n        # recently opened table element\n        # XXX - this is really inelegant\n        lastTable = None\n        fosterParent = None\n        insertBefore = None\n        for elm in self.openElements[::-1]:\n            if elm.name == \"table\":\n                lastTable = elm\n                break\n        if lastTable:\n            # XXX - we should really check that this parent is actually a\n            # node here\n            if lastTable.parent:\n                fosterParent = lastTable.parent\n                insertBefore = lastTable\n            else:\n                fosterParent = self.openElements[\n                    self.openElements.index(lastTable) - 1]\n        else:\n            fosterParent = self.openElements[0]\n        return fosterParent, insertBefore", "output": "Get the foster parent element, and sibling to insert before\n        (or None) when inserting a misnested table node", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _filter_running(runnings):\n    '''\n    \n    '''\n    ret = dict((tag, value) for tag, value in six.iteritems(runnings)\n               if not value['result'] or value['changes'])\n    return ret", "output": "Filter out the result: True + no changes data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_cron(user,\n                path,\n                mask,\n                cmd):\n    '''\n    \n    '''\n    arg_mask = mask.split(',')\n    arg_mask.sort()\n\n    lst = __salt__['incron.list_tab'](user)\n    for cron in lst['crons']:\n        if path == cron['path'] and cron['cmd'] == cmd:\n            cron_mask = cron['mask'].split(',')\n            cron_mask.sort()\n            if cron_mask == arg_mask:\n                return 'present'\n            if any([x in cron_mask for x in arg_mask]):\n                return 'update'\n    return 'absent'", "output": "Return the changes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _to_seq(sc, cols, converter=None):\n    \"\"\"\n    \n    \"\"\"\n    if converter:\n        cols = [converter(c) for c in cols]\n    return sc._jvm.PythonUtils.toSeq(cols)", "output": "Convert a list of Column (or names) into a JVM Seq of Column.\n\n    An optional `converter` could be used to convert items in `cols`\n    into JVM Column objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def define(\n    name: str,\n    default: Any = None,\n    type: type = None,\n    help: str = None,\n    metavar: str = None,\n    multiple: bool = False,\n    group: str = None,\n    callback: Callable[[Any], None] = None,\n) -> None:\n    \"\"\"\n    \"\"\"\n    return options.define(\n        name,\n        default=default,\n        type=type,\n        help=help,\n        metavar=metavar,\n        multiple=multiple,\n        group=group,\n        callback=callback,\n    )", "output": "Defines an option in the global namespace.\n\n    See `OptionParser.define`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download(self, url, destination_path):\n    \"\"\"\n    \"\"\"\n    self._pbar_url.update_total(1)\n    future = self._executor.submit(self._sync_download, url, destination_path)\n    return promise.Promise.resolve(future)", "output": "Download url to given path.\n\n    Returns Promise -> sha256 of downloaded file.\n\n    Args:\n      url: address of resource to download.\n      destination_path: `str`, path to directory where to download the resource.\n\n    Returns:\n      Promise obj -> (`str`, int): (downloaded object checksum, size in bytes).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_args():\n  \"\"\"\"\"\"\n  parser = argparse.ArgumentParser(\n      description='Tool to download dataset images.')\n  parser.add_argument('--input_file', required=True,\n                      help='Location of dataset.csv')\n  parser.add_argument('--output_dir', required=True,\n                      help='Output path to download images')\n  parser.add_argument('--threads', default=multiprocessing.cpu_count() + 1,\n                      help='Number of threads to use')\n  args = parser.parse_args()\n  return args.input_file, args.output_dir, int(args.threads)", "output": "Parses command line arguments.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uptime_check_config_path(cls, project, uptime_check_config):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/uptimeCheckConfigs/{uptime_check_config}\",\n            project=project,\n            uptime_check_config=uptime_check_config,\n        )", "output": "Return a fully-qualified uptime_check_config string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def disconnect(self, *, force=False):\n        \"\"\"\n        \"\"\"\n        if not force and not self.is_connected():\n            return\n\n        self.stop()\n        self._connected.clear()\n\n        try:\n            if self.ws:\n                await self.ws.close()\n\n            await self.terminate_handshake(remove=True)\n        finally:\n            if self.socket:\n                self.socket.close()", "output": "|coro|\n\n        Disconnects this voice client from voice.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_hyperparameter_lowest_mu(fun_prediction,\n                                  fun_prediction_args,\n                                  x_bounds, x_types,\n                                  minimize_starting_points,\n                                  minimize_constraints_fun=None):\n    '''\n    \n    '''\n    best_x = None\n    best_acquisition_value = None\n    x_bounds_minmax = [[i[0], i[-1]] for i in x_bounds]\n    x_bounds_minmax = numpy.array(x_bounds_minmax)\n\n    for starting_point in numpy.array(minimize_starting_points):\n        res = minimize(fun=_lowest_mu,\n                       x0=starting_point.reshape(1, -1),\n                       bounds=x_bounds_minmax,\n                       method=\"L-BFGS-B\",\n                       args=(fun_prediction, fun_prediction_args, \\\n                             x_bounds, x_types, minimize_constraints_fun))\n\n        if (best_acquisition_value is None) or (res.fun < best_acquisition_value):\n            res.x = numpy.ndarray.tolist(res.x)\n            res.x = lib_data.match_val_type(res.x, x_bounds, x_types)\n            if (minimize_constraints_fun is None) or (minimize_constraints_fun(res.x) is True):\n                best_acquisition_value = res.fun\n                best_x = res.x\n\n    outputs = None\n    if best_x is not None:\n        mu, sigma = fun_prediction(best_x, *fun_prediction_args)\n        outputs = {'hyperparameter': best_x, 'expected_mu': mu,\n                   'expected_sigma': sigma, 'acquisition_func': \"lm\"}\n    return outputs", "output": "\"Lowest Mu\" acquisition function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def BCEWithLogitsFlat(*args, axis:int=-1, floatify:bool=True, **kwargs):\n    \"\"\n    return FlattenedLoss(nn.BCEWithLogitsLoss, *args, axis=axis, floatify=floatify, is_2d=False, **kwargs)", "output": "Same as `nn.BCEWithLogitsLoss`, but flattens input and target.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _load_general(data, targets, major_axis):\n    \"\"\"\"\"\"\n    for d_src, d_targets, axis in zip(data, targets, major_axis): # pylint: disable=too-many-nested-blocks\n        if isinstance(d_targets, nd.NDArray):\n            d_src.copyto(d_targets)\n        elif isinstance(d_src, (list, tuple)):\n            for src, dst in zip(d_src, d_targets):\n                src.copyto(dst)\n        else:\n            for slice_idx, d_dst in d_targets:\n                if axis >= 0:\n                    # copy slice\n                    shape = d_src.shape\n                    do_crop = (slice_idx.start != 0 or shape[axis] != slice_idx.stop)\n                    # pylint: disable=no-member,protected-access\n                    if do_crop:\n                        if axis == 0:\n                            d_src[slice_idx.start:slice_idx.stop].copyto(d_dst)\n                        else:\n                            if d_src.context == d_dst.context:\n                                nd.slice_axis(d_src, axis=axis, begin=slice_idx.start,\n                                              end=slice_idx.stop, out=d_dst)\n                            else:\n                                # on different device, crop and then do cross device copy\n                                d_dst_copy = nd.slice_axis(d_src, axis=axis, begin=slice_idx.start,\n                                                           end=slice_idx.stop)\n                                d_dst_copy.copyto(d_dst)\n                    else:\n                        d_src.copyto(d_dst)\n                    # pylint: enable=no-member,protected-access\n                else:\n                    d_src.copyto(d_dst)", "output": "Load a list of arrays into a list of arrays specified by slices.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bbox_hflip(bbox, rows, cols):\n    \"\"\"\"\"\"\n    x_min, y_min, x_max, y_max = bbox\n    return [1 - x_max, y_min, 1 - x_min, y_max]", "output": "Flip a bounding box horizontally around the y-axis.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_training_roidbs(self, names):\n        \"\"\"\n        \n        \"\"\"\n        return COCODetection.load_many(\n            cfg.DATA.BASEDIR, names, add_gt=True, add_mask=cfg.MODE_MASK)", "output": "Args:\n            names (list[str]): name of the training datasets, e.g.  ['train2014', 'valminusminival2014']\n\n        Returns:\n            roidbs (list[dict]):\n\n        Produce \"roidbs\" as a list of dict, each dict corresponds to one image with k>=0 instances.\n        and the following keys are expected for training:\n\n        file_name: str, full path to the image\n        boxes: numpy array of kx4 floats, each row is [x1, y1, x2, y2]\n        class: numpy array of k integers, in the range of [1, #categories], NOT [0, #categories)\n        is_crowd: k booleans. Use k False if you don't know what it means.\n        segmentation: k lists of numpy arrays (one for each instance).\n            Each list of numpy arrays corresponds to the mask for one instance.\n            Each numpy array in the list is a polygon of shape Nx2,\n            because one mask can be represented by N polygons.\n\n            If your segmentation annotations are originally masks rather than polygons,\n            either convert it, or the augmentation will need to be changed or skipped accordingly.\n\n            Include this field only if training Mask R-CNN.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cast(self, dataType):\n        \"\"\" \n        \"\"\"\n        if isinstance(dataType, basestring):\n            jc = self._jc.cast(dataType)\n        elif isinstance(dataType, DataType):\n            from pyspark.sql import SparkSession\n            spark = SparkSession.builder.getOrCreate()\n            jdt = spark._jsparkSession.parseDataType(dataType.json())\n            jc = self._jc.cast(jdt)\n        else:\n            raise TypeError(\"unexpected type: %s\" % type(dataType))\n        return Column(jc)", "output": "Convert the column into type ``dataType``.\n\n        >>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n        [Row(ages=u'2'), Row(ages=u'5')]\n        >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n        [Row(ages=u'2'), Row(ages=u'5')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render_string(self, template_name: str, **kwargs: Any) -> bytes:\n        \"\"\"\n        \"\"\"\n        # If no template_path is specified, use the path of the calling file\n        template_path = self.get_template_path()\n        if not template_path:\n            frame = sys._getframe(0)\n            web_file = frame.f_code.co_filename\n            while frame.f_code.co_filename == web_file:\n                frame = frame.f_back\n            assert frame.f_code.co_filename is not None\n            template_path = os.path.dirname(frame.f_code.co_filename)\n        with RequestHandler._template_loader_lock:\n            if template_path not in RequestHandler._template_loaders:\n                loader = self.create_template_loader(template_path)\n                RequestHandler._template_loaders[template_path] = loader\n            else:\n                loader = RequestHandler._template_loaders[template_path]\n        t = loader.load(template_name)\n        namespace = self.get_template_namespace()\n        namespace.update(kwargs)\n        return t.generate(**namespace)", "output": "Generate the given template with the given arguments.\n\n        We return the generated byte string (in utf8). To generate and\n        write a template as a response, use render() above.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lock():\n    '''\n    \n    '''\n    conn = __proxy__['junos.conn']()\n    ret = {}\n    ret['out'] = True\n    try:\n        conn.cu.lock()\n        ret['message'] = \"Successfully locked the configuration.\"\n    except jnpr.junos.exception.LockError as exception:\n        ret['message'] = 'Could not gain lock due to : \"{0}\"'.format(exception)\n        ret['out'] = False\n\n    return ret", "output": "Attempts an exclusive lock on the candidate configuration. This\n    is a non-blocking call.\n\n    .. note::\n        When locking, it is important to remember to call\n        :py:func:`junos.unlock <salt.modules.junos.unlock>` once finished. If\n        locking during orchestration, remember to include a step in the\n        orchestration job to unlock.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'device_name' junos.lock", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_get_stock_analysis(code):\n    \"\"\"\n    \n    \"\"\"\n    market = 'sh' if _select_market_code(code) == 1 else 'sz'\n    null = 'none'\n    data = eval(requests.get(BusinessAnalysis_url.format(\n        market, code), headers=headers_em).text)\n    zyfw = pd.DataFrame(data.get('zyfw', None))\n    jyps = pd.DataFrame(data.get('jyps', None))\n    zygcfx = data.get('zygcfx', [])\n    temp = []\n    for item in zygcfx:\n        try:\n            data_ = pd.concat([pd.DataFrame(item['hy']).assign(date=item['rq']).assign(classify='hy'),\n                               pd.DataFrame(item['cp']).assign(\n                                   date=item['rq']).assign(classify='cp'),\n                               pd.DataFrame(item['qy']).assign(date=item['rq']).assign(classify='qy')])\n\n            temp.append(data_)\n        except:\n            pass\n    try:\n        res_zyfcfx = pd.concat(temp).set_index(\n            ['date', 'classify'], drop=False)\n    except:\n        res_zyfcfx = None\n\n    return zyfw, jyps, res_zyfcfx", "output": "'zyfw', \u4e3b\u8425\u8303\u56f4 'jyps'#\u7ecf\u8425\u8bc4\u8ff0 'zygcfx' \u4e3b\u8425\u6784\u6210\u5206\u6790\n\n    date \u4e3b\u8425\u6784\u6210\t\u4e3b\u8425\u6536\u5165(\u5143)\t\u6536\u5165\u6bd4\u4f8bcbbl\t\u4e3b\u8425\u6210\u672c(\u5143)\t\u6210\u672c\u6bd4\u4f8b\t\u4e3b\u8425\u5229\u6da6(\u5143)\t\u5229\u6da6\u6bd4\u4f8b\t\u6bdb\u5229\u7387(%)\n    \u884c\u4e1a /\u4ea7\u54c1/ \u533a\u57df hq cp qy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def script(self, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        ubq = self._clone()\n        if ubq._script:\n            ubq._script = {}\n        ubq._script.update(kwargs)\n        return ubq", "output": "Define update action to take:\n        https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-using.html\n        for more details.\n\n        Note: the API only accepts a single script, so calling the script multiple times will overwrite.\n\n        Example::\n\n            ubq = Search()\n            ubq = ubq.script(source=\"ctx._source.likes++\"\")\n            ubq = ubq.script(source=\"ctx._source.likes += params.f\"\",\n                         lang=\"expression\",\n                         params={'f': 3})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def featurize(self, audio_clip, overwrite=False, save_feature_as_csvfile=False):\n        \"\"\" \n        \"\"\"\n        return spectrogram_from_file(\n            audio_clip, step=self.step, window=self.window,\n            max_freq=self.max_freq, overwrite=overwrite,\n            save_feature_as_csvfile=save_feature_as_csvfile)", "output": "For a given audio clip, calculate the log of its Fourier Transform\n        Params:\n            audio_clip(str): Path to the audio clip", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def values(self):\n    \"\"\"\n    \"\"\"\n    return {n: getattr(self, n) for n in self._hparam_types.keys()}", "output": "Return the hyperparameter values as a Python dictionary.\n\n    Returns:\n      A dictionary with hyperparameter names as keys.  The values are the\n      hyperparameter values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clone(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The clone function must be called with -f or --function.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    linode_id = kwargs.get('linode_id', None)\n    datacenter_id = kwargs.get('datacenter_id', None)\n    plan_id = kwargs.get('plan_id', None)\n    required_params = [linode_id, datacenter_id, plan_id]\n\n    for item in required_params:\n        if item is None:\n            raise SaltCloudSystemExit(\n                'The clone function requires a \\'linode_id\\', \\'datacenter_id\\', '\n                'and \\'plan_id\\' to be provided.'\n            )\n\n    clone_args = {\n        'LinodeID': linode_id,\n        'DatacenterID': datacenter_id,\n        'PlanID': plan_id\n    }\n\n    return _query('linode', 'clone', args=clone_args)", "output": "Clone a Linode.\n\n    linode_id\n        The ID of the Linode to clone. Required.\n\n    datacenter_id\n        The ID of the Datacenter where the Linode will be placed. Required.\n\n    plan_id\n        The ID of the plan (size) of the Linode. Required.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f clone my-linode-config linode_id=1234567 datacenter_id=2 plan_id=5", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_connector_c_pool(name, server=None, **kwargs):\n    '''\n    \n    '''\n    if 'transactionSupport' in kwargs and kwargs['transactionSupport'] not in (\n               'XATransaction',\n               'LocalTransaction',\n               'NoTransaction'\n       ):\n        raise CommandExecutionError('Invalid transaction support')\n    return _update_element(name, 'resources/connector-connection-pool', kwargs, server)", "output": "Update a connection pool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_custom_images(call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The list_vlans function must be called with -f or --function.'\n        )\n\n    ret = {}\n    conn = get_conn('SoftLayer_Account')\n    response = conn.getBlockDeviceTemplateGroups()\n    for image in response:\n        if 'globalIdentifier' not in image:\n            continue\n        ret[image['name']] = {\n            'id': image['id'],\n            'name': image['name'],\n            'globalIdentifier': image['globalIdentifier'],\n        }\n        if 'note' in image:\n            ret[image['name']]['note'] = image['note']\n    return ret", "output": "Return a dict of all custom VM images on the cloud provider.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def file_list(*packages, **kwargs):\n    '''\n    \n    '''\n    errors = []\n    ret = []\n    cmd = ['pacman', '-Ql']\n\n    if packages and os.path.exists(packages[0]):\n        packages = list(packages)\n        cmd.extend(('-r', packages.pop(0)))\n\n    cmd.extend(packages)\n\n    out = __salt__['cmd.run'](cmd, output_loglevel='trace', python_shell=False)\n    for line in salt.utils.itertools.split(out, '\\n'):\n        if line.startswith('error'):\n            errors.append(line)\n        else:\n            comps = line.split()\n            ret.append(' '.join(comps[1:]))\n    return {'errors': errors, 'files': ret}", "output": "List the files that belong to a package. Not specifying any packages will\n    return a list of _every_ file on the system's package database (not\n    generally recommended).\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' pkg.file_list httpd\n        salt '*' pkg.file_list httpd postfix\n        salt '*' pkg.file_list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def swapoff(name):\n    '''\n    \n    '''\n    on_ = swaps()\n    if name in on_:\n        if __grains__['kernel'] == 'SunOS':\n            if __grains__['virtual'] != 'zone':\n                __salt__['cmd.run']('swap -a {0}'.format(name), python_shell=False)\n            else:\n                return False\n        elif __grains__['os'] != 'OpenBSD':\n            __salt__['cmd.run']('swapoff {0}'.format(name), python_shell=False)\n        else:\n            __salt__['cmd.run']('swapctl -d {0}'.format(name),\n                                python_shell=False)\n        on_ = swaps()\n        if name in on_:\n            return False\n        return True\n    return None", "output": "Deactivate a named swap mount\n\n    .. versionchanged:: 2016.3.2\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mount.swapoff /root/swapfile", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _huber_loss(x, delta=1.0):\n    \"\"\"\"\"\"\n    return tf.where(\n        tf.abs(x) < delta,\n        tf.square(x) * 0.5, delta * (tf.abs(x) - 0.5 * delta))", "output": "Reference: https://en.wikipedia.org/wiki/Huber_loss", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def addNextSibling(self, elem):\n        \"\"\" \"\"\"\n        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        ret = libxml2mod.xmlAddNextSibling(self._o, elem__o)\n        if ret is None:raise treeError('xmlAddNextSibling() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Add a new node @elem as the next sibling of @cur If the new\n          node was already inserted in a document it is first\n          unlinked from its existing context. As a result of text\n          merging @elem may be freed. If the new node is ATTRIBUTE,\n          it is added into properties instead of children. If there\n           is an attribute with equal name, it is first destroyed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_locations(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_locations function must be called with '\n            '-f or --function, or with the --list-locations option'\n        )\n\n    params = {\n        'action': 'DescribeZones',\n    }\n    items = query(params=params)\n\n    result = {}\n    for region in items['zone_set']:\n        result[region['zone_id']] = {}\n        for key in region:\n            result[region['zone_id']][key] = six.text_type(region[key])\n\n    return result", "output": "Return a dict of all available locations on the provider with\n    relevant data.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-cloud --list-locations my-qingcloud", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_function_definition_from_first_line(self):\r\n        \"\"\"\"\"\"\r\n        document = self.code_editor.document()\r\n        cursor = QTextCursor(\r\n            document.findBlockByLineNumber(self.line_number_cursor - 1))\r\n\r\n        func_text = ''\r\n        func_indent = ''\r\n\r\n        is_first_line = True\r\n        line_number = cursor.blockNumber() + 1\r\n\r\n        number_of_lines = self.code_editor.blockCount()\r\n        remain_lines = number_of_lines - line_number + 1\r\n        number_of_lines_of_function = 0\r\n\r\n        for __ in range(min(remain_lines, 20)):\r\n            cur_text = to_text_string(cursor.block().text()).rstrip()\r\n\r\n            if is_first_line:\r\n                if not is_start_of_function(cur_text):\r\n                    return None\r\n\r\n                func_indent = get_indent(cur_text)\r\n                is_first_line = False\r\n            else:\r\n                cur_indent = get_indent(cur_text)\r\n                if cur_indent <= func_indent:\r\n                    return None\r\n                if is_start_of_function(cur_text):\r\n                    return None\r\n                if cur_text.strip == '':\r\n                    return None\r\n\r\n            if cur_text[-1] == '\\\\':\r\n                cur_text = cur_text[:-1]\r\n\r\n            func_text += cur_text\r\n            number_of_lines_of_function += 1\r\n\r\n            if cur_text.endswith(':'):\r\n                return func_text, number_of_lines_of_function\r\n\r\n            cursor.movePosition(QTextCursor.NextBlock)\r\n\r\n        return None", "output": "Get func def when the cursor is located on the first def line.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_api_keys(self, api_id, stage_name):\n        \"\"\"\n        \n        \"\"\"\n        response = self.apigateway_client.get_api_keys(limit=500)\n        stage_key = '{}/{}'.format(api_id, stage_name)\n        for api_key in response.get('items'):\n            if stage_key in api_key.get('stageKeys'):\n                yield api_key.get('id')", "output": "Generator that allows to iterate per API keys associated to an api_id and a stage_name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def van(first_enc,\n        first_frame,\n        current_enc,\n        gt_image,\n        reuse=False,\n        scope_prefix='',\n        hparams=None):\n  \"\"\"\n  \"\"\"\n  with tf.variable_scope(scope_prefix + 'van', reuse=reuse):\n    output_shape = first_frame.get_shape().as_list()\n    output_shape[0] = -1\n\n    first_depth = 64\n\n    f_first_enc, _ = van_enc_2d(first_enc, first_depth)\n    f_first_frame, image_enc_history = van_image_enc_2d(\n        first_frame, first_depth, hparams=hparams)\n    f_current_enc, van_higher_level = van_enc_2d(\n        current_enc, first_depth, reuse=True)\n    f_gt_image, _ = van_image_enc_2d(gt_image, first_depth, True,\n                                     hparams=hparams)\n\n    analogy_t = analogy_computation_2d(\n        f_first_enc, f_first_frame, f_current_enc, first_depth)\n    enc_img = f_current_enc + analogy_t\n\n    img = van_dec_2d(\n        enc_img, image_enc_history, output_shape, first_depth, hparams=hparams)\n\n    batch_size = tf.to_float(tf.shape(first_enc)[0])\n    r_loss = tf.nn.l2_loss(f_gt_image - f_current_enc - analogy_t) / batch_size\n\n    return img, r_loss, van_higher_level", "output": "Implements a VAN.\n\n  Args:\n    first_enc: The first encoding.\n    first_frame: The first ground truth frame.\n    current_enc: The encoding of the frame to generate.\n    gt_image: The ground truth image, only used for regularization.\n    reuse: To reuse in variable scope or not.\n    scope_prefix: The prefix before the scope name.\n    hparams: The python hparams.\n\n  Returns:\n    The generated image.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def with_wait_cursor(func):\n    \"\"\"\n    \n    \"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        QApplication.setOverrideCursor(\n            QCursor(Qt.WaitCursor))\n        try:\n            ret_val = func(*args, **kwargs)\n        finally:\n            QApplication.restoreOverrideCursor()\n        return ret_val\n    return wrapper", "output": "Show a wait cursor while the wrapped function is running. The cursor is\n    restored as soon as the function exits.\n\n    :param func: wrapped function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_cpu(self):\n        '''\n        \n        '''\n        # CPU data in grains is OK-ish, but lscpu is still better in this case\n        out = __salt__['cmd.run_all'](\"lscpu\")\n        salt.utils.fsutils._verify_run(out)\n        data = dict()\n        for descr, value in [elm.split(\":\", 1) for elm in out['stdout'].split(os.linesep)]:\n            data[descr.strip()] = value.strip()\n\n        return data", "output": "Get available CPU information.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_route_by_view_name(self, view_name, name=None):\n        \"\"\"\n        \"\"\"\n        if not view_name:\n            return (None, None)\n\n        if view_name == \"static\" or view_name.endswith(\".static\"):\n            return self.routes_static_files.get(name, (None, None))\n\n        return self.routes_names.get(view_name, (None, None))", "output": "Find a route in the router based on the specified view name.\n\n        :param view_name: string of view name to search by\n        :param kwargs: additional params, usually for static files\n        :return: tuple containing (uri, Route)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _reindex(self):\n        \"\"\"\n        \n        \"\"\"\n        self._len2split_idx = {}\n        last_split = -1\n        for split_idx, split in enumerate(self._splits):\n            self._len2split_idx.update(\n                dict(list(zip(list(range(last_split + 1, split)), [split_idx] * (split - (last_split + 1))))))", "output": "Index every sentence into a cluster", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_filters(col_params, extractors):\n  \"\"\"\n  \"\"\"\n  result = []\n  for col_param, extractor in zip(col_params, extractors):\n    a_filter = _create_filter(col_param, extractor)\n    if a_filter:\n      result.append(a_filter)\n  return result", "output": "Creates filters for the given col_params.\n\n  Args:\n    col_params: List of ListSessionGroupsRequest.ColParam protobufs.\n    extractors: list of extractor functions of the same length as col_params.\n      Each element should extract the column described by the corresponding\n      element of col_params.\n  Returns:\n    A list of filter functions. Each corresponding to a single\n    col_params.filter oneof field of _request", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def request_minion_cachedir(\n        minion_id,\n        opts=None,\n        fingerprint='',\n        pubkey=None,\n        provider=None,\n        base=None,\n):\n    '''\n    \n    '''\n    if base is None:\n        base = __opts__['cachedir']\n\n    if not fingerprint and pubkey is not None:\n        fingerprint = salt.utils.crypt.pem_finger(key=pubkey, sum_type=(opts and opts.get('hash_type') or 'sha256'))\n\n    init_cachedir(base)\n\n    data = {\n        'minion_id': minion_id,\n        'fingerprint': fingerprint,\n        'provider': provider,\n    }\n\n    fname = '{0}.p'.format(minion_id)\n    path = os.path.join(base, 'requested', fname)\n    mode = 'wb' if six.PY3 else 'w'\n    with salt.utils.files.fopen(path, mode) as fh_:\n        salt.utils.msgpack.dump(data, fh_, encoding=MSGPACK_ENCODING)", "output": "Creates an entry in the requested/ cachedir. This means that Salt Cloud has\n    made a request to a cloud provider to create an instance, but it has not\n    yet verified that the instance properly exists.\n\n    If the fingerprint is unknown, a raw pubkey can be passed in, and a\n    fingerprint will be calculated. If both are empty, then the fingerprint\n    will be set to None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def permute_iter(elements):\n    \"\"\"\n        \n    \"\"\"\n    if len(elements) <= 1:\n        yield elements\n    else:\n        for perm in permute_iter(elements[1:]):\n            for i in range(len(elements)):\n                yield perm[:i] + elements[0:1] + perm[i:]", "output": "iterator: returns a perumation by each call.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate(self, references, buffers):\n        ''' \n\n\n        '''\n        from ..util.serialization import transform_column_source_data\n\n        data_dict = transform_column_source_data(self.column_source.data, buffers=buffers, cols=self.cols)\n\n        return { 'kind'          : 'ColumnDataChanged',\n                 'column_source' : self.column_source.ref,\n                 'new'           : data_dict,\n                 'cols'          : self.cols}", "output": "Create a JSON representation of this event suitable for sending\n        to clients.\n\n        .. code-block:: python\n\n            {\n                'kind'          : 'ColumnDataChanged'\n                'column_source' : <reference to a CDS>\n                'new'           : <new data to steam to column_source>\n                'cols'          : <specific columns to update>\n            }\n\n        Args:\n            references (dict[str, Model]) :\n                If the event requires references to certain models in order to\n                function, they may be collected here.\n\n                **This is an \"out\" parameter**. The values it contains will be\n                modified in-place.\n\n            buffers (set) :\n                If the event needs to supply any additional Bokeh protocol\n                buffers, they may be added to this set.\n\n                **This is an \"out\" parameter**. The values it contains will be\n                modified in-place.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetch(bank, key, cachedir):\n    '''\n    \n    '''\n    inkey = False\n    key_file = os.path.join(cachedir, os.path.normpath(bank), '{0}.p'.format(key))\n    if not os.path.isfile(key_file):\n        # The bank includes the full filename, and the key is inside the file\n        key_file = os.path.join(cachedir, os.path.normpath(bank) + '.p')\n        inkey = True\n\n    if not os.path.isfile(key_file):\n        log.debug('Cache file \"%s\" does not exist', key_file)\n        return {}\n    try:\n        with salt.utils.files.fopen(key_file, 'rb') as fh_:\n            if inkey:\n                return __context__['serial'].load(fh_)[key]\n            else:\n                return __context__['serial'].load(fh_)\n    except IOError as exc:\n        raise SaltCacheError(\n            'There was an error reading the cache file \"{0}\": {1}'.format(\n                key_file, exc\n            )\n        )", "output": "Fetch information from a file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_docstring(owner_name, docstring, formatters):\n    \"\"\"\n    \n    \"\"\"\n    # Build a dict of parameters to a vanilla format() call by searching for\n    # each entry in **formatters and applying any leading whitespace to each\n    # line in the desired substitution.\n    format_params = {}\n    for target, doc_for_target in iteritems(formatters):\n        # Search for '{name}', with optional leading whitespace.\n        regex = re.compile(r'^(\\s*)' + '({' + target + '})$', re.MULTILINE)\n        matches = regex.findall(docstring)\n        if not matches:\n            raise ValueError(\n                \"Couldn't find template for parameter {!r} in docstring \"\n                \"for {}.\"\n                \"\\nParameter name must be alone on a line surrounded by \"\n                \"braces.\".format(target, owner_name),\n            )\n        elif len(matches) > 1:\n            raise ValueError(\n                \"Couldn't found multiple templates for parameter {!r}\"\n                \"in docstring for {}.\"\n                \"\\nParameter should only appear once.\".format(\n                    target, owner_name\n                )\n            )\n\n        (leading_whitespace, _) = matches[0]\n        format_params[target] = pad_lines_after_first(\n            leading_whitespace,\n            doc_for_target,\n        )\n\n    return docstring.format(**format_params)", "output": "Template ``formatters`` into ``docstring``.\n\n    Parameters\n    ----------\n    owner_name : str\n        The name of the function or class whose docstring is being templated.\n        Only used for error messages.\n    docstring : str\n        The docstring to template.\n    formatters : dict[str -> str]\n        Parameters for a a str.format() call on ``docstring``.\n\n        Multi-line values in ``formatters`` will have leading whitespace padded\n        to match the leading whitespace of the substitution string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def boost(self, dtrain, grad, hess):\n        \"\"\"\n        \n        \"\"\"\n        if len(grad) != len(hess):\n            raise ValueError('grad / hess length mismatch: {} / {}'.format(len(grad), len(hess)))\n        if not isinstance(dtrain, DMatrix):\n            raise TypeError('invalid training matrix: {}'.format(type(dtrain).__name__))\n        self._validate_features(dtrain)\n\n        _check_call(_LIB.XGBoosterBoostOneIter(self.handle, dtrain.handle,\n                                               c_array(ctypes.c_float, grad),\n                                               c_array(ctypes.c_float, hess),\n                                               len(grad)))", "output": "Boost the booster for one iteration, with customized gradient statistics.\n\n        Parameters\n        ----------\n        dtrain : DMatrix\n            The training DMatrix.\n        grad : list\n            The first order of gradient.\n        hess : list\n            The second order of gradient.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_encoding(self, encoding):\n        \"\"\"\"\"\"\n        value = str(encoding).upper()\n        self.set_value(value)", "output": "Update encoding of current file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fillna(self, value):\n        \"\"\"\n        \n        \"\"\"\n\n        with cython_context():\n            return SArray(_proxy = self.__proxy__.fill_missing_values(value))", "output": "Create new SArray with all missing values (None or NaN) filled in\n        with the given value.\n\n        The size of the new SArray will be the same as the original SArray. If\n        the given value is not the same type as the values in the SArray,\n        `fillna` will attempt to convert the value to the original SArray's\n        type. If this fails, an error will be raised.\n\n        Parameters\n        ----------\n        value : type convertible to SArray's type\n            The value used to replace all missing values\n\n        Returns\n        -------\n        out : SArray\n            A new SArray with all missing values filled", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(Name,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.get_trail_status(Name=Name)\n        return {'exists': True}\n    except ClientError as e:\n        err = __utils__['boto3.get_error'](e)\n        if e.response.get('Error', {}).get('Code') == 'TrailNotFoundException':\n            return {'exists': False}\n        return {'error': err}", "output": "Given a trail name, check to see if the given trail exists.\n\n    Returns True if the given trail exists and returns False if the given\n    trail does not exist.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_cloudtrail.exists mytrail", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def profile(self, username):\n        \"\"\"\"\"\"\n        if not username and g.user:\n            username = g.user.username\n\n        payload = {\n            'user': bootstrap_user_data(username, include_perms=True),\n            'common': self.common_bootsrap_payload(),\n        }\n\n        return self.render_template(\n            'superset/basic.html',\n            title=_(\"%(user)s's profile\", user=username),\n            entry='profile',\n            bootstrap_data=json.dumps(payload, default=utils.json_iso_dttm_ser),\n        )", "output": "User profile page", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_numeric_v_string_like(a, b):\n    \"\"\"\n    \n    \"\"\"\n\n    is_a_array = isinstance(a, np.ndarray)\n    is_b_array = isinstance(b, np.ndarray)\n\n    is_a_numeric_array = is_a_array and is_numeric_dtype(a)\n    is_b_numeric_array = is_b_array and is_numeric_dtype(b)\n    is_a_string_array = is_a_array and is_string_like_dtype(a)\n    is_b_string_array = is_b_array and is_string_like_dtype(b)\n\n    is_a_scalar_string_like = not is_a_array and is_string_like(a)\n    is_b_scalar_string_like = not is_b_array and is_string_like(b)\n\n    return ((is_a_numeric_array and is_b_scalar_string_like) or\n            (is_b_numeric_array and is_a_scalar_string_like) or\n            (is_a_numeric_array and is_b_string_array) or\n            (is_b_numeric_array and is_a_string_array))", "output": "Check if we are comparing a string-like object to a numeric ndarray.\n\n    NumPy doesn't like to compare such objects, especially numeric arrays\n    and scalar string-likes.\n\n    Parameters\n    ----------\n    a : array-like, scalar\n        The first object to check.\n    b : array-like, scalar\n        The second object to check.\n\n    Returns\n    -------\n    boolean\n        Whether we return a comparing a string-like object to a numeric array.\n\n    Examples\n    --------\n    >>> is_numeric_v_string_like(1, 1)\n    False\n    >>> is_numeric_v_string_like(\"foo\", \"foo\")\n    False\n    >>> is_numeric_v_string_like(1, \"foo\")  # non-array numeric\n    False\n    >>> is_numeric_v_string_like(np.array([1]), \"foo\")\n    True\n    >>> is_numeric_v_string_like(\"foo\", np.array([1]))  # symmetric check\n    True\n    >>> is_numeric_v_string_like(np.array([1, 2]), np.array([\"foo\"]))\n    True\n    >>> is_numeric_v_string_like(np.array([\"foo\"]), np.array([1, 2]))\n    True\n    >>> is_numeric_v_string_like(np.array([1]), np.array([2]))\n    False\n    >>> is_numeric_v_string_like(np.array([\"foo\"]), np.array([\"foo\"]))\n    False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cast(self, dtype):\n        \"\"\"\n        \"\"\"\n        for child in self._children.values():\n            child.cast(dtype)\n        for _, param in self.params.items():\n            param.cast(dtype)", "output": "Cast this Block to use another data type.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype\n            The new data type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_gid_list(user, include_default=True):\n    '''\n    \n    '''\n    if HAS_GRP is False or HAS_PWD is False:\n        return []\n    gid_list = list(\n        six.itervalues(\n            get_group_dict(user, include_default=include_default)\n        )\n    )\n    return sorted(set(gid_list))", "output": "Returns a list of all of the system group IDs of which the user\n    is a member.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_current_widget(self, fig_browser):\n        \"\"\"\n        \n        \"\"\"\n        self.stack.setCurrentWidget(fig_browser)\n        # We update the actions of the options button (cog menu) and\n        # we move it to the layout of the current widget.\n        self.refresh_actions()\n        fig_browser.setup_options_button()", "output": "Set the currently visible fig_browser in the stack widget, refresh the\n        actions of the cog menu button and move it to the layout of the new\n        fig_browser.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def str_encode(arr, encoding, errors=\"strict\"):\n    \"\"\"\n    \n    \"\"\"\n    if encoding in _cpython_optimized_encoders:\n        # CPython optimized implementation\n        f = lambda x: x.encode(encoding, errors)\n    else:\n        encoder = codecs.getencoder(encoding)\n        f = lambda x: encoder(x, errors)[0]\n    return _na_map(f, arr)", "output": "Encode character string in the Series/Index using indicated encoding.\n    Equivalent to :meth:`str.encode`.\n\n    Parameters\n    ----------\n    encoding : str\n    errors : str, optional\n\n    Returns\n    -------\n    encoded : Series/Index of objects", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_corrupted_example(self, x):\n    \"\"\"\n    \"\"\"\n    corruption_type = self.builder_config.corruption_type\n    severity = self.builder_config.severity\n\n    return {\n        'gaussian_noise': corruptions.gaussian_noise,\n        'shot_noise': corruptions.shot_noise,\n        'impulse_noise': corruptions.impulse_noise,\n        'defocus_blur': corruptions.defocus_blur,\n        'frosted_glass_blur': corruptions.frosted_glass_blur,\n        'zoom_blur': corruptions.zoom_blur,\n        'fog': corruptions.fog,\n        'brightness': corruptions.brightness,\n        'contrast': corruptions.contrast,\n        'elastic': corruptions.elastic,\n        'pixelate': corruptions.pixelate,\n        'jpeg_compression': corruptions.jpeg_compression,\n    }[corruption_type](x, severity)", "output": "Return corrupted images.\n\n    Args:\n      x: numpy array, uncorrupted image.\n\n    Returns:\n      numpy array, corrupted images.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def drop_duplicates(self, keep=\"first\", inplace=False, **kwargs):\r\n        \"\"\"\r\n        \"\"\"\r\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\r\n        if kwargs.get(\"subset\", None) is not None:\r\n            duplicates = self.duplicated(keep=keep, **kwargs)\r\n        else:\r\n            duplicates = self.duplicated(keep=keep, **kwargs)\r\n        indices, = duplicates.values.nonzero()\r\n        return self.drop(index=self.index[indices], inplace=inplace)", "output": "Return DataFrame with duplicate rows removed, optionally only considering certain columns\r\n\r\n            Args:\r\n                subset : column label or sequence of labels, optional\r\n                    Only consider certain columns for identifying duplicates, by\r\n                    default use all of the columns\r\n                keep : {'first', 'last', False}, default 'first'\r\n                    - ``first`` : Drop duplicates except for the first occurrence.\r\n                    - ``last`` : Drop duplicates except for the last occurrence.\r\n                    - False : Drop all duplicates.\r\n                inplace : boolean, default False\r\n                    Whether to drop duplicates in place or to return a copy\r\n\r\n            Returns:\r\n                deduplicated : DataFrame", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download(names):\n    '''\n    \n    '''\n    # Create a Windows Update Agent instance\n    wua = salt.utils.win_update.WindowsUpdateAgent()\n\n    # Search for Update\n    updates = wua.search(names)\n\n    if updates.count() == 0:\n        raise CommandExecutionError('No updates found')\n\n    # Make sure it's a list so count comparison is correct\n    if isinstance(names, six.string_types):\n        names = [names]\n\n    if isinstance(names, six.integer_types):\n        names = [six.text_type(names)]\n\n    if updates.count() > len(names):\n        raise CommandExecutionError('Multiple updates found, names need to be '\n                                    'more specific')\n\n    return wua.download(updates)", "output": ".. versionadded:: 2017.7.0\n\n    Downloads updates that match the list of passed identifiers. It's easier to\n    use this function by using list_updates and setting install=True.\n\n    Args:\n\n        names (str, list):\n            A single update or a list of updates to download. This can be any\n            combination of GUIDs, KB numbers, or names. GUIDs or KBs are\n            preferred.\n\n    .. note::\n        An error will be raised if there are more results than there are items\n        in the names parameter\n\n    Returns:\n\n        dict: A dictionary containing the details about the downloaded updates\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        # Normal Usage\n        salt '*' win_wua.download names=['12345678-abcd-1234-abcd-1234567890ab', 'KB2131233']", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_to_path(p):\n    \"\"\"\"\"\"\n    if p not in os.environ[\"PATH\"]:\n        os.environ[\"PATH\"] = \"{0}{1}{2}\".format(p, os.pathsep, os.environ[\"PATH\"])", "output": "Adds a given path to the PATH.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validatePushElement(self, ctxt, elem, qname):\n        \"\"\" \"\"\"\n        if ctxt is None: ctxt__o = None\n        else: ctxt__o = ctxt._o\n        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        ret = libxml2mod.xmlValidatePushElement(ctxt__o, self._o, elem__o, qname)\n        return ret", "output": "Push a new element start on the validation stack.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def initialize_references_json(references_json, references, setter=None):\n    ''' \n\n    '''\n\n    for obj in references_json:\n        obj_id = obj['id']\n        obj_attrs = obj['attributes']\n\n        instance = references[obj_id]\n\n        # We want to avoid any Model specific initialization that happens with\n        # Slider(...) when reconstituting from JSON, but we do need to perform\n        # general HasProps machinery that sets properties, so call it explicitly\n        HasProps.__init__(instance)\n\n        instance.update_from_json(obj_attrs, models=references, setter=setter)", "output": "Given a JSON representation of the models in a graph, and new model\n    objects, set the properties on the models from the JSON\n\n    Args:\n        references_json (``JSON``)\n            JSON specifying attributes and values to initialize new model\n            objects with.\n\n        references (dict[str, Model])\n            A dictionary mapping model IDs to newly created (but not yet\n            initialized) Bokeh models.\n\n            **This is an \"out\" parameter**. The values it contains will be\n            modified in-place.\n\n        setter (ClientSession or ServerSession or None, optional) :\n                This is used to prevent \"boomerang\" updates to Bokeh apps.\n                (default: None)\n\n                In the context of a Bokeh server application, incoming updates\n                to properties will be annotated with the session that is\n                doing the updating. This value is propagated through any\n                subsequent change notifications that the update triggers.\n                The session can compare the event setter to itself, and\n                suppress any updates that originate from itself.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def service_delete(service_id=None, name=None, profile=None, **connection_args):\n    '''\n    \n    '''\n    kstone = auth(profile, **connection_args)\n    if name:\n        service_id = service_get(name=name, profile=profile,\n                                 **connection_args)[name]['id']\n    kstone.services.delete(service_id)\n    return 'Keystone service ID \"{0}\" deleted'.format(service_id)", "output": "Delete a service from Keystone service catalog\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.service_delete c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.service_delete name=nova", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generalized_negative_binomial(mu=1, alpha=1, shape=_Null, dtype=_Null, **kwargs):\n    \"\"\"\n    \"\"\"\n    return _random_helper(_internal._random_generalized_negative_binomial,\n                          _internal._sample_generalized_negative_binomial,\n                          [mu, alpha], shape, dtype, kwargs)", "output": "Draw random samples from a generalized negative binomial distribution.\n\n    Samples are distributed according to a generalized negative binomial\n    distribution parametrized by *mu* (mean) and *alpha* (dispersion).\n    *alpha* is defined as *1/k* where *k* is the failure limit of the\n    number of unsuccessful experiments (generalized to real numbers).\n    Samples will always be returned as a floating point data type.\n\n    Parameters\n    ----------\n    mu : float or Symbol, optional\n        Mean of the negative binomial distribution.\n    alpha : float or Symbol, optional\n        Alpha (dispersion) parameter of the negative binomial distribution.\n    shape : int or tuple of ints, optional\n        The number of samples to draw. If shape is, e.g., `(m, n)` and `mu` and\n        `alpha` are scalars, output shape will be `(m, n)`. If `mu` and `alpha`\n        are Symbols with shape, e.g., `(x, y)`, then output will have shape\n        `(x, y, m, n)`, where `m*n` samples are drawn for each `[mu, alpha)` pair.\n    dtype : {'float16', 'float32', 'float64'}, optional\n        Data type of output samples. Default is 'float32'\n\n    Returns\n    -------\n    Symbol\n        If input `shape` has dimensions, e.g., `(m, n)`, and `mu` and\n        `alpha` are scalars, returned Symbol will resolve to shape `(m, n)`. If `mu`\n        and `alpha` are Symbols with shape, e.g., `(x, y)`, returned Symbol will resolve\n        to shape `(x, y, m, n)`, where `m*n` samples are drawn for each `[mu, alpha)` pair.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def layer_stack_from_hparams(hparams, prefix):\n  \"\"\"\"\"\"\n  layers = hparams.get(prefix + \"layers\")\n  return transformer.LayerStack(\n      [layers_registry[l](hparams, prefix) for l in layers],\n      dropout_rate=hparams.layer_prepostprocess_dropout,\n      norm_epsilon=hparams.norm_epsilon)", "output": "Create a layer stack based on the hyperparameter values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def masked_mean(vector: torch.Tensor,\n                mask: torch.Tensor,\n                dim: int,\n                keepdim: bool = False,\n                eps: float = 1e-8) -> torch.Tensor:\n    \"\"\"\n    \n    \"\"\"\n    one_minus_mask = (1.0 - mask).byte()\n    replaced_vector = vector.masked_fill(one_minus_mask, 0.0)\n\n    value_sum = torch.sum(replaced_vector, dim=dim, keepdim=keepdim)\n    value_count = torch.sum(mask.float(), dim=dim, keepdim=keepdim)\n    return value_sum / value_count.clamp(min=eps)", "output": "To calculate mean along certain dimensions on masked values\n\n    Parameters\n    ----------\n    vector : ``torch.Tensor``\n        The vector to calculate mean.\n    mask : ``torch.Tensor``\n        The mask of the vector. It must be broadcastable with vector.\n    dim : ``int``\n        The dimension to calculate mean\n    keepdim : ``bool``\n        Whether to keep dimension\n    eps : ``float``\n        A small value to avoid zero division problem.\n\n    Returns\n    -------\n    A ``torch.Tensor`` of including the mean values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_change_request_state(change_id, state='approved'):\n    '''\n    \n    '''\n    client = _get_client()\n    client.table = 'change_request'\n    # Get the change record first\n    record = client.get({'number': change_id})\n    if not record:\n        log.error('Failed to fetch change record, maybe it does not exist?')\n        return False\n    # Use the sys_id as the unique system record\n    sys_id = record[0]['sys_id']\n    response = client.update({'approval': state}, sys_id)\n    return response", "output": "Set the approval state of a change request/record\n\n    :param change_id: The ID of the change request, e.g. CHG123545\n    :type  change_id: ``str``\n\n    :param state: The target state, e.g. approved\n    :type  state: ``str``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion servicenow.set_change_request_state CHG000123 declined\n        salt myminion servicenow.set_change_request_state CHG000123 approved", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def module_name(self, jamfile_location):\n        \"\"\"\"\"\"\n        assert isinstance(jamfile_location, basestring)\n        module = self.location2module.get(jamfile_location)\n        if not module:\n            # Root the path, so that locations are always umbiguious.\n            # Without this, we can't decide if '../../exe/program1' and '.'\n            # are the same paths, or not.\n            jamfile_location = os.path.realpath(\n                os.path.join(os.getcwd(), jamfile_location))\n            module = \"Jamfile<%s>\" % jamfile_location\n            self.location2module[jamfile_location] = module\n        return module", "output": "Returns the name of module corresponding to 'jamfile-location'.\n        If no module corresponds to location yet, associates default\n        module name with that location.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate(model:nn.Module, dl:DataLoader, loss_func:OptLossFunc=None, cb_handler:Optional[CallbackHandler]=None,\n             pbar:Optional[PBar]=None, average=True, n_batch:Optional[int]=None)->Iterator[Tuple[Union[Tensor,int],...]]:\n    \"\"\n    model.eval()\n    with torch.no_grad():\n        val_losses,nums = [],[]\n        if cb_handler: cb_handler.set_dl(dl)\n        for xb,yb in progress_bar(dl, parent=pbar, leave=(pbar is not None)):\n            if cb_handler: xb, yb = cb_handler.on_batch_begin(xb, yb, train=False)\n            val_loss = loss_batch(model, xb, yb, loss_func, cb_handler=cb_handler)\n            val_losses.append(val_loss)\n            if not is_listy(yb): yb = [yb]\n            nums.append(yb[0].shape[0])\n            if cb_handler and cb_handler.on_batch_end(val_losses[-1]): break\n            if n_batch and (len(nums)>=n_batch): break\n        nums = np.array(nums, dtype=np.float32)\n        if average: return (to_np(torch.stack(val_losses)) * nums).sum() / nums.sum()\n        else:       return val_losses", "output": "Calculate `loss_func` of `model` on `dl` in evaluation mode.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_meta_graph(self, tags=None):\n    \"\"\"\"\"\"\n    matches = [meta_graph\n               for meta_graph in self.meta_graphs\n               if set(meta_graph.meta_info_def.tags) == set(tags or [])]\n    if not matches:\n      raise KeyError(\"SavedModelHandler has no graph with tags: %r\" % tags)\n    if len(matches) != 1:\n      raise KeyError(\n          \"SavedModelHandler has multiple graphs with tags %r\" % tags)\n    return matches[0]", "output": "Returns the matching MetaGraphDef or raises KeyError.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _prepare_tokens_for_encode(tokens):\n  \"\"\"\n  \"\"\"\n  prepared_tokens = []\n\n  def _prepare_token(t, next_t):\n    skip_next = False\n    t = _escape(t)\n    # If next token is a single space, add _ suffix to token and skip the\n    # empty space.\n    if next_t == \" \":\n      t += \"_\"\n      skip_next = True\n    return t, skip_next\n\n  next_tokens = tokens[1:] + [None]\n  skip_single_token = False\n  for token, next_token in zip(tokens, next_tokens):\n    if skip_single_token:\n      skip_single_token = False\n      continue\n\n    # If the user-supplied string contains the underscore replacement string,\n    # break it into 2 tokens and encode those separately.\n    if token == _UNDERSCORE_REPLACEMENT:\n      t1, t2 = _UNDERSCORE_REPLACEMENT[:2], _UNDERSCORE_REPLACEMENT[2:]\n      t1, _ = _prepare_token(t1, None)\n      t2, _ = _prepare_token(t2, next_token)\n      prepared_tokens.append(t1)\n      prepared_tokens.append(t2)\n      continue\n\n    token, skip_single_token = _prepare_token(token, next_token)\n    prepared_tokens.append(token)\n  return prepared_tokens", "output": "Prepare tokens for encoding.\n\n  Tokens followed by a single space have \"_\" appended and the single space token\n  is dropped.\n\n  If a token is _UNDERSCORE_REPLACEMENT, it is broken up into 2 tokens.\n\n  Args:\n    tokens: `list<str>`, tokens to prepare.\n\n  Returns:\n    `list<str>` prepared tokens.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _compute_hparam_info_from_values(self, name, values):\n    \"\"\"\n    \"\"\"\n    # Figure out the type from the values.\n    # Ignore values whose type is not listed in api_pb2.DataType\n    # If all values have the same type, then that is the type used.\n    # Otherwise, the returned type is DATA_TYPE_STRING.\n    result = api_pb2.HParamInfo(name=name, type=api_pb2.DATA_TYPE_UNSET)\n    distinct_values = set(\n        _protobuf_value_to_string(v) for v in values if _protobuf_value_type(v))\n    for v in values:\n      v_type = _protobuf_value_type(v)\n      if not v_type:\n        continue\n      if result.type == api_pb2.DATA_TYPE_UNSET:\n        result.type = v_type\n      elif result.type != v_type:\n        result.type = api_pb2.DATA_TYPE_STRING\n      if result.type == api_pb2.DATA_TYPE_STRING:\n        # A string result.type does not change, so we can exit the loop.\n        break\n\n    # If we couldn't figure out a type, then we can't compute the hparam_info.\n    if result.type == api_pb2.DATA_TYPE_UNSET:\n      return None\n\n    # If the result is a string, set the domain to be the distinct values if\n    # there aren't too many of them.\n    if (result.type == api_pb2.DATA_TYPE_STRING\n        and len(distinct_values) <= self._max_domain_discrete_len):\n      result.domain_discrete.extend(distinct_values)\n\n    return result", "output": "Builds an HParamInfo message from the hparam name and list of values.\n\n    Args:\n      name: string. The hparam name.\n      values: list of google.protobuf.Value messages. The list of values for the\n        hparam.\n\n    Returns:\n      An api_pb2.HParamInfo message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validateDtd(self, doc, dtd):\n        \"\"\" \"\"\"\n        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        if dtd is None: dtd__o = None\n        else: dtd__o = dtd._o\n        ret = libxml2mod.xmlValidateDtd(self._o, doc__o, dtd__o)\n        return ret", "output": "Try to validate the document against the dtd instance\n          Basically it does check all the definitions in the DtD.\n          Note the the internal subset (if present) is de-coupled\n          (i.e. not used), which could give problems if ID or IDREF\n           is present.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shuffle_and_batch_data(dataset, target_names, features_info, training):\n  \"\"\"\"\"\"\n  def append_targets(example):\n    \"\"\"Append targets to the example dictionary. Needed for Keras.\"\"\"\n    if len(target_names) == 1:\n      return (example, example[target_names[0]])\n    targets = {}\n    for name in target_names:\n      targets[name] = example[name]\n    return (example, targets)\n  dataset = dataset.map(append_targets)\n  if training:\n    dataset = dataset.repeat()\n  shapes = {k: features_info[k].shape for k in features_info}\n  shapes = (shapes, shapes[target_names[0]])\n  dataset = dataset.shuffle(128)\n  dataset = preprocess_fn(dataset, training)\n  dataset = batch_fn(dataset, training, shapes, target_names)\n  return dataset.prefetch(8)", "output": "Shuffle and batch the given dataset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_loss_begin(self, last_output:Tuple[Tensor,Tensor,Tensor], **kwargs):\n        \"\"\n        self.raw_out,self.out = last_output[1],last_output[2]\n        return {'last_output': last_output[0]}", "output": "Save the extra outputs for later and only returns the true output.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve_egg_link(path):\n    \"\"\"\n    \n    \"\"\"\n    referenced_paths = non_empty_lines(path)\n    resolved_paths = (\n        os.path.join(os.path.dirname(path), ref)\n        for ref in referenced_paths\n    )\n    dist_groups = map(find_distributions, resolved_paths)\n    return next(dist_groups, ())", "output": "Given a path to an .egg-link, resolve distributions\n    present in the referenced path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def token(self):\n        \"\"\"\n        \"\"\"\n        prefixes = (\"Bearer\", \"Token\")\n        auth_header = self.headers.get(\"Authorization\")\n\n        if auth_header is not None:\n            for prefix in prefixes:\n                if prefix in auth_header:\n                    return auth_header.partition(prefix)[-1].strip()\n\n        return auth_header", "output": "Attempt to return the auth header token.\n\n        :return: token related to request", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy_value(self, orig_key, new_key):\r\n        \"\"\"\"\"\"\r\n        data = self.model.get_data()\r\n        if isinstance(data, list):\r\n            data.append(data[orig_key])\r\n        if isinstance(data, set):\r\n            data.add(data[orig_key])\r\n        else:\r\n            data[new_key] = data[orig_key]\r\n        self.set_data(data)", "output": "Copy value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_browse_tabs_menu(self):\r\n        \"\"\"\"\"\"\r\n        self.browse_tabs_menu.clear()\r\n        names = []\r\n        dirnames = []\r\n        for index in range(self.count()):\r\n            if self.menu_use_tooltips:\r\n                text = to_text_string(self.tabToolTip(index))\r\n            else:\r\n                text = to_text_string(self.tabText(index))\r\n            names.append(text)\r\n            if osp.isfile(text):\r\n                # Testing if tab names are filenames\r\n                dirnames.append(osp.dirname(text))\r\n        offset = None\r\n        \r\n        # If tab names are all filenames, removing common path:\r\n        if len(names) == len(dirnames):\r\n            common = get_common_path(dirnames)\r\n            if common is None:\r\n                offset = None\r\n            else:\r\n                offset = len(common)+1\r\n                if offset <= 3:\r\n                    # Common path is not a path but a drive letter...\r\n                    offset = None\r\n\r\n        for index, text in enumerate(names):\r\n            tab_action = create_action(self, text[offset:],\r\n                                       icon=self.tabIcon(index),\r\n                                       toggled=lambda state, index=index:\r\n                                               self.setCurrentIndex(index),\r\n                                       tip=self.tabToolTip(index))\r\n            tab_action.setChecked(index == self.currentIndex())\r\n            self.browse_tabs_menu.addAction(tab_action)", "output": "Update browse tabs menu", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _recurse(self, path):\n        '''\n        \n        '''\n        files = {}\n        empty_dirs = []\n        try:\n            sub_paths = os.listdir(path)\n        except OSError as exc:\n            if exc.errno == errno.ENOENT:\n                # Path does not exist\n                sys.stderr.write('{0} does not exist\\n'.format(path))\n                sys.exit(42)\n            elif exc.errno in (errno.EINVAL, errno.ENOTDIR):\n                # Path is a file (EINVAL on Windows, ENOTDIR otherwise)\n                files[path] = self._mode(path)\n        else:\n            if not sub_paths:\n                empty_dirs.append(path)\n            for fn_ in sub_paths:\n                files_, empty_dirs_ = self._recurse(os.path.join(path, fn_))\n                files.update(files_)\n                empty_dirs.extend(empty_dirs_)\n\n        return files, empty_dirs", "output": "Get a list of all specified files", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_sfs_idxs(sfs, last=True):\n    \"\"\"\n    \n    \"\"\"\n    if last:\n        feature_szs = [sfs_feats.features.size()[-1] for sfs_feats in sfs]\n        sfs_idxs = list(np.where(np.array(feature_szs[:-1]) != np.array(feature_szs[1:]))[0])\n        if feature_szs[0] != feature_szs[1]: sfs_idxs = [0] + sfs_idxs\n    else: sfs_idxs = list(range(len(sfs)))\n    return sfs_idxs", "output": "Return the saved feature indexes that will be concatenated\n    Inputs:\n        sfs (list): saved features by hook function, in other words intermediate activations\n        last (bool): whether to concatenate only last different activation, or all from the encoder model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def GetAttribute(self, name):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlTextReaderGetAttribute(self._o, name)\n        return ret", "output": "Provides the value of the attribute with the specified\n           qualified name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ShiftRight(x, **unused_kwargs):\n  \"\"\"\"\"\"\n  if not isinstance(x, (list, tuple)):  # non-chunked inputs\n    pad_widths = [(0, 0), (1, 0)]\n    padded = np.pad(x, pad_widths, mode='constant')\n    return padded[:, :-1]\n  # Handling chunked inputs. Recall that the list of chunks represents a big\n  # sequence (the concatenation of the chunks). We want to shift that sequence,\n  # so we put a 0 in the beginning of the first chunk and the last element of\n  # that chunk is used as the new first element of the next chunk, and so on.\n  padded = []\n  last_value = np.zeros_like(x[0][:, -1])\n  for chunk in x:\n    padded_chunk = np.concatenate([last_value[:, np.newaxis], chunk], axis=1)\n    last_value = chunk[:, -1]\n    padded.append(padded_chunk[:, :-1])\n  return padded", "output": "Layer to shift the tensor to the right by padding on axis 1.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_files(dl_paths, publisher, url_dict):\n  \"\"\"\"\"\"\n  if publisher == 'cnn':\n    top_dir = os.path.join(dl_paths['cnn_stories'], 'cnn', 'stories')\n  elif publisher == 'dm':\n    top_dir = os.path.join(dl_paths['dm_stories'], 'dailymail', 'stories')\n  else:\n    logging.fatal('Unsupported publisher: %s', publisher)\n  files = tf.io.gfile.listdir(top_dir)\n\n  ret_files = []\n  for p in files:\n    basename = os.path.basename(p)\n    if basename[0:basename.find('.story')] in url_dict:\n      ret_files.append(os.path.join(top_dir, p))\n  return ret_files", "output": "Find files corresponding to urls.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def send_friend_request(self):\n        \"\"\"\n        \"\"\"\n        await self._state.http.send_friend_request(username=self.name, discriminator=self.discriminator)", "output": "|coro|\n\n        Sends the user a friend request.\n\n        .. note::\n\n            This only applies to non-bot accounts.\n\n        Raises\n        -------\n        Forbidden\n            Not allowed to send a friend request to the user.\n        HTTPException\n            Sending the friend request failed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mps_device_memory_limit():\n    \"\"\"\n    \n    \"\"\"\n    lib = _load_tcmps_lib()\n    if lib is None:\n        return None\n\n    c_size = _ctypes.c_uint64()\n    ret = lib.TCMPSMetalDeviceMemoryLimit(_ctypes.byref(c_size))\n    return c_size.value if ret == 0 else None", "output": "Returns the memory size in bytes that can be effectively allocated on the\n    MPS device that will be used, or None if no suitable device is available.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_distribution(self, name):\n        \"\"\"\n        \n        \"\"\"\n        result = None\n        name = name.lower()\n        if not self._cache_enabled:\n            for dist in self._yield_distributions():\n                if dist.key == name:\n                    result = dist\n                    break\n        else:\n            self._generate_cache()\n\n            if name in self._cache.name:\n                result = self._cache.name[name][0]\n            elif self._include_egg and name in self._cache_egg.name:\n                result = self._cache_egg.name[name][0]\n        return result", "output": "Looks for a named distribution on the path.\n\n        This function only returns the first result found, as no more than one\n        value is expected. If nothing is found, ``None`` is returned.\n\n        :rtype: :class:`InstalledDistribution`, :class:`EggInfoDistribution`\n                or ``None``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setmem(vm_, memory):\n    '''\n    \n    '''\n    with _get_xapi_session() as xapi:\n        mem_target = int(memory) * 1024 * 1024\n\n        vm_uuid = _get_label_uuid(xapi, 'VM', vm_)\n        if vm_uuid is False:\n            return False\n        try:\n            xapi.VM.set_memory_dynamic_max_live(vm_uuid, mem_target)\n            xapi.VM.set_memory_dynamic_min_live(vm_uuid, mem_target)\n            return True\n        except Exception:\n            return False", "output": "Changes the amount of memory allocated to VM.\n\n    Memory is to be specified in MB\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' virt.setmem myvm 768", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _streaming_request_iterable(self, config, requests):\n        \"\"\"\n        \"\"\"\n        yield self.types.StreamingRecognizeRequest(streaming_config=config)\n        for request in requests:\n            yield request", "output": "A generator that yields the config followed by the requests.\n\n        Args:\n            config (~.speech_v1.types.StreamingRecognitionConfig): The\n                configuration to use for the stream.\n            requests (Iterable[~.speech_v1.types.StreamingRecognizeRequest]):\n                The input objects.\n\n        Returns:\n            Iterable[~.speech_v1.types.StreamingRecognizeRequest]): The\n                correctly formatted input for\n                :meth:`~.speech_v1.SpeechClient.streaming_recognize`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def metadata(self):\n        \"\"\"\"\"\"\n        if not self._operation.HasField(\"metadata\"):\n            return None\n\n        return protobuf_helpers.from_any_pb(\n            self._metadata_type, self._operation.metadata\n        )", "output": "google.protobuf.Message: the current operation metadata.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def common_prefix(s1, s2):\n    \"\"\n    if not s1 or not s2:\n        return \"\"\n    k = 0\n    while s1[k] == s2[k]:\n        k = k + 1\n        if k >= len(s1) or k >= len(s2):\n            return s1[0:k]\n    return s1[0:k]", "output": "Return prefix common of 2 strings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def market_value(self):\n        \"\"\"\n        \n        \"\"\"\n        return sum(position.market_value for position in six.itervalues(self._positions))", "output": "[float] \u5e02\u503c", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_delta_tdi(self, other):\n        \"\"\"\n        \n        \"\"\"\n        assert isinstance(self.freq, Tick)  # checked by calling function\n\n        delta = self._check_timedeltalike_freq_compat(other)\n        return self._addsub_int_array(delta, operator.add).asi8", "output": "Parameters\n        ----------\n        other : TimedeltaArray or ndarray[timedelta64]\n\n        Returns\n        -------\n        result : ndarray[int64]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _clean_text(self, text):\n        \"\"\"\"\"\"\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp in (0, 0xfffd) or self._is_control(char):\n                continue\n            if self._is_whitespace(char):\n                output.append(' ')\n            else:\n                output.append(char)\n        return ''.join(output)", "output": "Performs invalid character removal and whitespace cleanup on text.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_buffers_for_display(s, limit=40):\n  \"\"\"\n  \"\"\"\n  if isinstance(s, (list, tuple)):\n    return [process_buffers_for_display(elem, limit=limit) for elem in s]\n  else:\n    length = len(s)\n    if length > limit:\n      return (binascii.b2a_qp(s[:limit]) +\n              b' (length-%d truncated at %d bytes)' % (length, limit))\n    else:\n      return binascii.b2a_qp(s)", "output": "Process a buffer for human-readable display.\n\n  This function performs the following operation on each of the buffers in `s`.\n    1. Truncate input buffer if the length of the buffer is greater than\n       `limit`, to prevent large strings from overloading the frontend.\n    2. Apply `binascii.b2a_qp` on the truncated buffer to make the buffer\n       printable and convertible to JSON.\n    3. If truncation happened (in step 1), append a string at the end\n       describing the original length and the truncation.\n\n  Args:\n    s: The buffer to be processed, either a single buffer or a nested array of\n      them.\n    limit: Length limit for each buffer, beyond which truncation will occur.\n\n  Return:\n    A single processed buffer or a nested array of processed buffers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_game_for_worker(map_name, directory_id):\n  \"\"\"\"\"\"\n  if map_name == \"v100unfriendly\":\n    games = [\"chopper_command\", \"boxing\", \"asterix\", \"seaquest\"]\n    worker_per_game = 5\n  elif map_name == \"human_nice\":\n    games = gym_env.ATARI_GAMES_WITH_HUMAN_SCORE_NICE\n    worker_per_game = 5\n  else:\n    raise ValueError(\"Unknown worker to game map name: %s\" % map_name)\n  games.sort()\n  game_id = (directory_id - 1) // worker_per_game\n  tf.logging.info(\"Getting game %d from %s.\" % (game_id, games))\n  return games[game_id]", "output": "Get game for the given worker (directory) id.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _append_comment(ret, comment):\n    '''\n    \n    '''\n    if ret['comment']:\n        ret['comment'] = ret['comment'].rstrip() + '\\n' + comment\n    else:\n        ret['comment'] = comment\n\n    return ret", "output": "append ``comment`` to ``ret['comment']``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def colorize(message, color=None):\n    \"\"\"\"\"\"\n    if not color:\n        return message\n\n    colors = {\n        \"green\": colorama.Fore.GREEN,\n        \"yellow\": colorama.Fore.YELLOW,\n        \"blue\": colorama.Fore.BLUE,\n        \"red\": colorama.Fore.RED,\n    }\n\n    return \"{color}{message}{nc}\".format(\n        color=colors.get(color, \"\"), message=message, nc=colorama.Fore.RESET\n    )", "output": "Returns a message in a specified color.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decide(self, package):  # type: (Package) -> None\n        \"\"\"\n        \n        \"\"\"\n        # When we make a new decision after backtracking, count an additional\n        # attempted solution. If we backtrack multiple times in a row, though, we\n        # only want to count one, since we haven't actually started attempting a\n        # new solution.\n        if self._backtracking:\n            self._attempted_solutions += 1\n\n        self._backtracking = False\n        self._decisions[package.name] = package\n\n        self._assign(\n            Assignment.decision(package, self.decision_level, len(self._assignments))\n        )", "output": "Adds an assignment of package as a decision\n        and increments the decision level.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_upgrade(self, index=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, \"_upgrade\"), params=params\n        )", "output": "Monitor how much of one or more index is upgraded.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-upgrade.html>`_\n\n        :arg index: A comma-separated list of index names; use `_all` or empty\n            string to perform the operation on all indices\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default 'open', valid\n            choices are: 'open', 'closed', 'none', 'all'\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lookup_symbols(self,\n                       symbols,\n                       as_of_date,\n                       fuzzy=False,\n                       country_code=None):\n        \"\"\"\n        \n        \"\"\"\n        if not symbols:\n            return []\n\n        multi_country = country_code is None\n        if fuzzy:\n            f = self._lookup_symbol_fuzzy\n            mapping = self._choose_fuzzy_symbol_ownership_map(country_code)\n        else:\n            f = self._lookup_symbol_strict\n            mapping = self._choose_symbol_ownership_map(country_code)\n\n        if mapping is None:\n            raise SymbolNotFound(symbol=symbols[0])\n\n        memo = {}\n        out = []\n        append_output = out.append\n        for sym in symbols:\n            if sym in memo:\n                append_output(memo[sym])\n            else:\n                equity = memo[sym] = f(\n                    mapping,\n                    multi_country,\n                    sym,\n                    as_of_date,\n                )\n                append_output(equity)\n        return out", "output": "Lookup a list of equities by symbol.\n\n        Equivalent to::\n\n            [finder.lookup_symbol(s, as_of, fuzzy) for s in symbols]\n\n        but potentially faster because repeated lookups are memoized.\n\n        Parameters\n        ----------\n        symbols : sequence[str]\n            Sequence of ticker symbols to resolve.\n        as_of_date : pd.Timestamp\n            Forwarded to ``lookup_symbol``.\n        fuzzy : bool, optional\n            Forwarded to ``lookup_symbol``.\n        country_code : str or None, optional\n            The country to limit searches to. If not provided, the search will\n            span all countries which increases the likelihood of an ambiguous\n            lookup.\n\n        Returns\n        -------\n        equities : list[Equity]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def current_boost_dir():\n    \"\"\"\"\"\"\n    # Path to directory containing this script.\n    path = os.path.dirname( os.path.realpath(__file__) )\n    # Making sure it is located in \"${boost-dir}/libs/mpl/preprocessed\".\n    for directory in reversed( [\"libs\", \"mpl\", \"preprocessed\"] ):\n        (head, tail) = os.path.split(path)\n        if tail == directory:\n            path = head\n        else:\n            return None\n    return os.path.relpath( path )", "output": "Returns the (relative) path to the Boost source-directory this file is located in (if any).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _valid_deleted_file(path):\n    '''\n    \n    '''\n    ret = False\n    if path.endswith(' (deleted)'):\n        ret = True\n    if re.compile(r\"\\(path inode=[0-9]+\\)$\").search(path):\n        ret = True\n\n    regex = re.compile(\"|\".join(LIST_DIRS))\n    if regex.match(path):\n        ret = False\n    return ret", "output": "Filters file path against unwanted directories and decides whether file is marked as deleted.\n\n    Returns:\n        True if file is desired deleted file, else False.\n\n    Args:\n        path: A string - path to file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_all_thumbnails(self):\n        \"\"\"\"\"\"\n        for thumbnail in self._thumbnails:\n            self.layout().removeWidget(thumbnail)\n            thumbnail.sig_canvas_clicked.disconnect()\n            thumbnail.sig_remove_figure.disconnect()\n            thumbnail.sig_save_figure.disconnect()\n            thumbnail.deleteLater()\n        self._thumbnails = []\n        self.current_thumbnail = None\n        self.figure_viewer.figcanvas.clear_canvas()", "output": "Remove all thumbnails.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_resource_record_sets(self, max_results=None, page_token=None, client=None):\n        \"\"\"\n        \"\"\"\n        client = self._require_client(client)\n        path = \"/projects/%s/managedZones/%s/rrsets\" % (self.project, self.name)\n        iterator = page_iterator.HTTPIterator(\n            client=client,\n            api_request=client._connection.api_request,\n            path=path,\n            item_to_value=_item_to_resource_record_set,\n            items_key=\"rrsets\",\n            page_token=page_token,\n            max_results=max_results,\n        )\n        iterator.zone = self\n        return iterator", "output": "List resource record sets for this zone.\n\n        See\n        https://cloud.google.com/dns/api/v1/resourceRecordSets/list\n\n        :type max_results: int\n        :param max_results: Optional. The maximum number of resource record\n                            sets to return. Defaults to a sensible value\n                            set by the API.\n\n        :type page_token: str\n        :param page_token: Optional. If present, return the next batch of\n            resource record sets, using the value, which must correspond to\n            the ``nextPageToken`` value returned in the previous response.\n            Deprecated: use the ``pages`` property of the returned iterator\n            instead of manually passing the token.\n\n        :type client: :class:`google.cloud.dns.client.Client`\n        :param client:\n            (Optional) the client to use.  If not passed, falls back to the\n            ``client`` stored on the current zone.\n\n        :rtype: :class:`~google.api_core.page_iterator.Iterator`\n        :returns: Iterator of :class:`~.resource_record_set.ResourceRecordSet`\n                  belonging to this zone.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_assessor_content(experiment_config):\n    ''''''\n    if experiment_config.get('assessor'):\n        if experiment_config['assessor'].get('builtinAssessorName'):\n            experiment_config['assessor']['className'] = experiment_config['assessor']['builtinAssessorName']\n        else:\n            validate_customized_file(experiment_config, 'assessor')", "output": "Validate whether assessor in experiment_config is valid", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def padded_variance_explained(predictions,\n                              labels,\n                              weights_fn=common_layers.weights_all):\n  \"\"\"\"\"\"\n  predictions, labels = common_layers.pad_with_zeros(predictions, labels)\n  targets = labels\n  weights = weights_fn(targets)\n\n  y_bar = tf.reduce_mean(weights * targets)\n  tot_ss = tf.reduce_sum(weights * tf.pow(targets - y_bar, 2))\n  res_ss = tf.reduce_sum(weights * tf.pow(targets - predictions, 2))\n  r2 = 1. - res_ss / tot_ss\n  return r2, tf.reduce_sum(weights)", "output": "Explained variance, also known as R^2.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def file_list(*packages, **kwargs):\n    '''\n    \n    '''\n    errors = []\n    ret = set([])\n    pkgs = {}\n    cmd = 'dpkg -l {0}'.format(' '.join(packages))\n    out = __salt__['cmd.run_all'](cmd, python_shell=False)\n    if out['retcode'] != 0:\n        msg = 'Error:  ' + out['stderr']\n        log.error(msg)\n        return msg\n    out = out['stdout']\n\n    for line in out.splitlines():\n        if line.startswith('ii '):\n            comps = line.split()\n            pkgs[comps[1]] = {'version': comps[2],\n                              'description': ' '.join(comps[3:])}\n        if 'No packages found' in line:\n            errors.append(line)\n    for pkg in pkgs:\n        files = []\n        cmd = 'dpkg -L {0}'.format(pkg)\n        for line in __salt__['cmd.run'](cmd, python_shell=False).splitlines():\n            files.append(line)\n        fileset = set(files)\n        ret = ret.union(fileset)\n    return {'errors': errors, 'files': list(ret)}", "output": "List the files that belong to a package. Not specifying any packages will\n    return a list of _every_ file on the system's package database (not\n    generally recommended).\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' lowpkg.file_list httpd\n        salt '*' lowpkg.file_list httpd postfix\n        salt '*' lowpkg.file_list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def startswith(self, prefix):\n        \"\"\"\n        \n        \"\"\"\n        return ArrayPredicate(\n            term=self,\n            op=LabelArray.startswith,\n            opargs=(prefix,),\n        )", "output": "Construct a Filter matching values starting with ``prefix``.\n\n        Parameters\n        ----------\n        prefix : str\n            String prefix against which to compare values produced by ``self``.\n\n        Returns\n        -------\n        matches : Filter\n            Filter returning True for all sid/date pairs for which ``self``\n            produces a string starting with ``prefix``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main(args):\n  \"\"\"\n  \"\"\"\n  if not args:\n    raise Exception('Please specify at least one JSON config path')\n  inputs = []\n  program = []\n  outputs = []\n  for arg in args:\n    with open(arg) as fd:\n      config = json.load(fd)\n    inputs.extend(config.get('inputs', []))\n    program.extend(config.get('program', []))\n    outputs.extend(config.get('outputs', []))\n  if not program:\n    raise Exception('Please specify a program')\n  return run(inputs, program, outputs)", "output": "Invokes run function using a JSON file config.\n\n  Args:\n    args: CLI args, which can be a JSON file containing an object whose\n        attributes are the parameters to the run function. If multiple JSON\n        files are passed, their contents are concatenated.\n  Returns:\n    0 if succeeded or nonzero if failed.\n  Raises:\n    Exception: If input data is missing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def refresh_grains(**kwargs):\n    '''\n    \n    '''\n    kwargs = salt.utils.args.clean_kwargs(**kwargs)\n    _refresh_pillar = kwargs.pop('refresh_pillar', True)\n    if kwargs:\n        salt.utils.args.invalid_kwargs(kwargs)\n    # Modules and pillar need to be refreshed in case grains changes affected\n    # them, and the module refresh process reloads the grains and assigns the\n    # newly-reloaded grains to each execution module's __grains__ dunder.\n    refresh_modules()\n    if _refresh_pillar:\n        refresh_pillar()\n    return True", "output": ".. versionadded:: 2016.3.6,2016.11.4,2017.7.0\n\n    Refresh the minion's grains without syncing custom grains modules from\n    ``salt://_grains``.\n\n    .. note::\n        The available execution modules will be reloaded as part of this\n        proceess, as grains can affect which modules are available.\n\n    refresh_pillar : True\n        Set to ``False`` to keep pillar data from being refreshed.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.refresh_grains", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _getJavaStorageLevel(self, storageLevel):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(storageLevel, StorageLevel):\n            raise Exception(\"storageLevel must be of type pyspark.StorageLevel\")\n\n        newStorageLevel = self._jvm.org.apache.spark.storage.StorageLevel\n        return newStorageLevel(storageLevel.useDisk,\n                               storageLevel.useMemory,\n                               storageLevel.useOffHeap,\n                               storageLevel.deserialized,\n                               storageLevel.replication)", "output": "Returns a Java StorageLevel based on a pyspark.StorageLevel.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hint(self, name, *parameters):\n        \"\"\"\n        \"\"\"\n        if len(parameters) == 1 and isinstance(parameters[0], list):\n            parameters = parameters[0]\n\n        if not isinstance(name, str):\n            raise TypeError(\"name should be provided as str, got {0}\".format(type(name)))\n\n        allowed_types = (basestring, list, float, int)\n        for p in parameters:\n            if not isinstance(p, allowed_types):\n                raise TypeError(\n                    \"all parameters should be in {0}, got {1} of type {2}\".format(\n                        allowed_types, p, type(p)))\n\n        jdf = self._jdf.hint(name, self._jseq(parameters))\n        return DataFrame(jdf, self.sql_ctx)", "output": "Specifies some hint on the current DataFrame.\n\n        :param name: A name of the hint.\n        :param parameters: Optional parameters.\n        :return: :class:`DataFrame`\n\n        >>> df.join(df2.hint(\"broadcast\"), \"name\").show()\n        +----+---+------+\n        |name|age|height|\n        +----+---+------+\n        | Bob|  5|    85|\n        +----+---+------+", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_sorted_inputs(filename, delimiter=\"\\n\"):\n  \"\"\"\n\n  \"\"\"\n  tf.logging.info(\"Getting sorted inputs\")\n  with tf.gfile.Open(filename) as f:\n    text = f.read()\n    records = text.split(delimiter)\n    inputs = [record.strip() for record in records]\n    # Strip the last empty line.\n    if not inputs[-1]:\n      inputs.pop()\n  input_lens = [(i, -len(line.split())) for i, line in enumerate(inputs)]\n  sorted_input_lens = sorted(input_lens, key=operator.itemgetter(1))\n  # We'll need the keys to rearrange the inputs back into their original order\n  sorted_keys = {}\n  sorted_inputs = []\n  for i, (index, _) in enumerate(sorted_input_lens):\n    sorted_inputs.append(inputs[index])\n    sorted_keys[index] = i\n  return sorted_inputs, sorted_keys", "output": "Returning inputs sorted according to decreasing length.\n\n  This causes inputs of similar lengths to be processed in the same batch,\n  facilitating early stopping for short sequences.\n\n  Longer sequences are sorted first so that if you're going to get OOMs,\n  you'll see it in the first batch.\n\n  Args:\n    filename: path to file with inputs, 1 per line.\n    delimiter: str, delimits records in the file.\n\n  Returns:\n    a sorted list of inputs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prompt_choice_for_config(cookiecutter_dict, env, key, options, no_input):\n    \"\"\"\n    \"\"\"\n    rendered_options = [\n        render_variable(env, raw, cookiecutter_dict) for raw in options\n    ]\n\n    if no_input:\n        return rendered_options[0]\n    return read_user_choice(key, rendered_options)", "output": "Prompt the user which option to choose from the given. Each of the\n    possible choices is rendered beforehand.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _construct_rest_api(self):\n        \"\"\"\n        \"\"\"\n        rest_api = ApiGatewayRestApi(self.logical_id, depends_on=self.depends_on, attributes=self.resource_attributes)\n        rest_api.BinaryMediaTypes = self.binary_media\n        rest_api.MinimumCompressionSize = self.minimum_compression_size\n\n        if self.endpoint_configuration:\n            self._set_endpoint_configuration(rest_api, self.endpoint_configuration)\n\n        elif not RegionConfiguration.is_apigw_edge_configuration_supported():\n            # Since this region does not support EDGE configuration, we explicitly set the endpoint type\n            # to Regional which is the only supported config.\n            self._set_endpoint_configuration(rest_api, \"REGIONAL\")\n\n        if self.definition_uri and self.definition_body:\n            raise InvalidResourceException(self.logical_id,\n                                           \"Specify either 'DefinitionUri' or 'DefinitionBody' property and not both\")\n\n        self._add_cors()\n        self._add_auth()\n        self._add_gateway_responses()\n\n        if self.definition_uri:\n            rest_api.BodyS3Location = self._construct_body_s3_dict()\n        elif self.definition_body:\n            rest_api.Body = self.definition_body\n\n        if self.name:\n            rest_api.Name = self.name\n\n        return rest_api", "output": "Constructs and returns the ApiGateway RestApi.\n\n        :returns: the RestApi to which this SAM Api corresponds\n        :rtype: model.apigateway.ApiGatewayRestApi", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_network(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The create_network function must be called with -f or --function.'\n        )\n\n    if not kwargs or 'name' not in kwargs:\n        log.error(\n            'A name must be specified when creating a network.'\n        )\n        return False\n\n    mode = kwargs.get('mode', 'legacy')\n    cidr = kwargs.get('cidr', None)\n    if cidr is None and mode == 'legacy':\n        log.error(\n            'A network CIDR range must be specified when creating a legacy network.'\n        )\n        return False\n\n    name = kwargs['name']\n    desc = kwargs.get('description', None)\n    conn = get_conn()\n\n    __utils__['cloud.fire_event'](\n        'event',\n        'creating network',\n        'salt/cloud/net/creating',\n        args={\n            'name': name,\n            'cidr': cidr,\n            'description': desc,\n            'mode': mode\n        },\n        sock_dir=__opts__['sock_dir'],\n        transport=__opts__['transport']\n    )\n\n    network = conn.ex_create_network(name, cidr, desc, mode)\n\n    __utils__['cloud.fire_event'](\n        'event',\n        'created network',\n        'salt/cloud/net/created',\n        args={\n            'name': name,\n            'cidr': cidr,\n            'description': desc,\n            'mode': mode\n        },\n        sock_dir=__opts__['sock_dir'],\n        transport=__opts__['transport']\n    )\n    return _expand_item(network)", "output": "... versionchanged:: 2017.7.0\n    Create a GCE network. Must specify name and cidr.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f create_network gce name=mynet cidr=10.10.10.0/24 mode=legacy description=optional\n        salt-cloud -f create_network gce name=mynet description=optional", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def del_store(source, store, saltenv='base'):\n    '''\n    \n    '''\n    cert_file = __salt__['cp.cache_file'](source, saltenv)\n    serial = get_cert_serial(cert_file)\n    cmd = \"certutil.exe -delstore {0} {1}\".format(store, serial)\n    return __salt__['cmd.run'](cmd)", "output": "Delete the given cert into the given Certificate Store\n\n    source\n        The source certificate file this can be in the form\n        salt://path/to/file\n\n    store\n        The certificate store to delete the certificate from\n\n    saltenv\n        The salt environment to use this is ignored if the path\n        is local\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' certutil.del_store salt://cert.cer TrustedPublisher", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_bin_to_numeric_type(bins, dtype):\n    \"\"\"\n    \n    \"\"\"\n    bins_dtype = infer_dtype(bins, skipna=False)\n    if is_timedelta64_dtype(dtype):\n        if bins_dtype in ['timedelta', 'timedelta64']:\n            bins = to_timedelta(bins).view(np.int64)\n        else:\n            raise ValueError(\"bins must be of timedelta64 dtype\")\n    elif is_datetime64_dtype(dtype) or is_datetime64tz_dtype(dtype):\n        if bins_dtype in ['datetime', 'datetime64']:\n            bins = to_datetime(bins).view(np.int64)\n        else:\n            raise ValueError(\"bins must be of datetime64 dtype\")\n\n    return bins", "output": "if the passed bin is of datetime/timedelta type,\n    this method converts it to integer\n\n    Parameters\n    ----------\n    bins : list-like of bins\n    dtype : dtype of data\n\n    Raises\n    ------\n    ValueError if bins are not of a compat dtype to dtype", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_attr(environment, obj, name):\n    \"\"\"\n    \"\"\"\n    try:\n        name = str(name)\n    except UnicodeError:\n        pass\n    else:\n        try:\n            value = getattr(obj, name)\n        except AttributeError:\n            pass\n        else:\n            if environment.sandboxed and not \\\n               environment.is_safe_attribute(obj, name, value):\n                return environment.unsafe_undefined(obj, name)\n            return value\n    return environment.undefined(obj=obj, name=name)", "output": "Get an attribute of an object.  ``foo|attr(\"bar\")`` works like\n    ``foo.bar`` just that always an attribute is returned and items are not\n    looked up.\n\n    See :ref:`Notes on subscriptions <notes-on-subscriptions>` for more details.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_embed(self, embed, vocab_size, embed_size, initializer, dropout, prefix):\n        \"\"\"  \"\"\"\n        if embed is None:\n            assert embed_size is not None, '\"embed_size\" cannot be None if \"word_embed\" or ' \\\n                                           'token_type_embed is not given.'\n            with self.name_scope():\n                embed = nn.HybridSequential(prefix=prefix)\n                with embed.name_scope():\n                    embed.add(nn.Embedding(input_dim=vocab_size, output_dim=embed_size,\n                                           weight_initializer=initializer))\n                    if dropout:\n                        embed.add(nn.Dropout(rate=dropout))\n        assert isinstance(embed, Block)\n        return embed", "output": "Construct an embedding block.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_key_bytes(hash_alg, salt, key, nbytes):\n    \"\"\"\n    \n    \"\"\"\n    keydata = bytes()\n    digest = bytes()\n    if len(salt) > 8:\n        salt = salt[:8]\n    while nbytes > 0:\n        hash_obj = hash_alg()\n        if len(digest) > 0:\n            hash_obj.update(digest)\n        hash_obj.update(b(key))\n        hash_obj.update(salt)\n        digest = hash_obj.digest()\n        size = min(nbytes, len(digest))\n        keydata += digest[:size]\n        nbytes -= size\n    return keydata", "output": "Given a password, passphrase, or other human-source key, scramble it\n    through a secure hash into some keyworthy bytes.  This specific algorithm\n    is used for encrypting/decrypting private key files.\n\n    :param function hash_alg: A function which creates a new hash object, such\n        as ``hashlib.sha256``.\n    :param salt: data to salt the hash with.\n    :type salt: byte string\n    :param str key: human-entered password or passphrase.\n    :param int nbytes: number of bytes to generate.\n    :return: Key data `str`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(sld, tld, nameserver):\n    '''\n    \n    '''\n    opts = salt.utils.namecheap.get_opts('namecheap.domains.ns.delete')\n    opts['SLD'] = sld\n    opts['TLD'] = tld\n    opts['Nameserver'] = nameserver\n\n    response_xml = salt.utils.namecheap.post_request(opts)\n    if response_xml is None:\n        return False\n\n    domainnsdeleteresult = response_xml.getElementsByTagName('DomainNSDeleteResult')[0]\n    return salt.utils.namecheap.string_to_value(domainnsdeleteresult.getAttribute('IsSuccess'))", "output": "Deletes a nameserver. Returns ``True`` if the nameserver was deleted\n    successfully\n\n    sld\n        SLD of the domain name\n\n    tld\n        TLD of the domain name\n\n    nameserver\n        Nameserver to delete\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' namecheap_domains_ns.delete sld tld nameserver", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def multi_perspective_match(vector1: torch.Tensor,\n                            vector2: torch.Tensor,\n                            weight: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    \n    \"\"\"\n    assert vector1.size(0) == vector2.size(0)\n    assert weight.size(1) == vector1.size(2) == vector1.size(2)\n\n    # (batch, seq_len, 1)\n    similarity_single = F.cosine_similarity(vector1, vector2, 2).unsqueeze(2)\n\n    # (1, 1, num_perspectives, hidden_size)\n    weight = weight.unsqueeze(0).unsqueeze(0)\n\n    # (batch, seq_len, num_perspectives, hidden_size)\n    vector1 = weight * vector1.unsqueeze(2)\n    vector2 = weight * vector2.unsqueeze(2)\n\n    similarity_multi = F.cosine_similarity(vector1, vector2, dim=3)\n\n    return similarity_single, similarity_multi", "output": "Calculate multi-perspective cosine matching between time-steps of vectors\n    of the same length.\n\n    Parameters\n    ----------\n    vector1 : ``torch.Tensor``\n        A tensor of shape ``(batch, seq_len, hidden_size)``\n    vector2 : ``torch.Tensor``\n        A tensor of shape ``(batch, seq_len or 1, hidden_size)``\n    weight : ``torch.Tensor``\n        A tensor of shape ``(num_perspectives, hidden_size)``\n\n    Returns\n    -------\n    A tuple of two tensors consisting multi-perspective matching results.\n    The first one is of the shape (batch, seq_len, 1), the second one is of shape\n    (batch, seq_len, num_perspectives)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_settings_bond_1(opts, iface, bond_def):\n\n    '''\n    \n    '''\n    bond = {'mode': '1'}\n\n    for binding in ['miimon', 'downdelay', 'updelay']:\n        if binding in opts:\n            try:\n                int(opts[binding])\n                bond.update({binding: opts[binding]})\n            except Exception:\n                _raise_error_iface(iface, binding, ['integer'])\n        else:\n            _log_default_iface(iface, binding, bond_def[binding])\n            bond.update({binding: bond_def[binding]})\n\n    if 'use_carrier' in opts:\n        if opts['use_carrier'] in _CONFIG_TRUE:\n            bond.update({'use_carrier': '1'})\n        elif opts['use_carrier'] in _CONFIG_FALSE:\n            bond.update({'use_carrier': '0'})\n        else:\n            valid = _CONFIG_TRUE + _CONFIG_FALSE\n            _raise_error_iface(iface, 'use_carrier', valid)\n    else:\n        _log_default_iface(iface, 'use_carrier', bond_def['use_carrier'])\n        bond.update({'use_carrier': bond_def['use_carrier']})\n\n    if 'primary' in opts:\n        bond.update({'primary': opts['primary']})\n\n    return bond", "output": "Filters given options and outputs valid settings for bond1.\n    If an option has a value that is not expected, this\n    function will log what the Interface, Setting and what it was\n    expecting.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def locked(path, timeout=None):\n    \"\"\"\n    \"\"\"\n    def decor(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            lock = FileLock(path, timeout=timeout)\n            lock.acquire()\n            try:\n                return func(*args, **kwargs)\n            finally:\n                lock.release()\n        return wrapper\n    return decor", "output": "Decorator which enables locks for decorated function.\n\n    Arguments:\n     - path: path for lockfile.\n     - timeout (optional): Timeout for acquiring lock.\n\n     Usage:\n         @locked('/var/run/myname', timeout=0)\n         def myname(...):\n             ...", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def require(self, *requirements):\n        \"\"\"\n        \"\"\"\n        needed = self.resolve(parse_requirements(requirements))\n\n        for dist in needed:\n            self.add(dist)\n\n        return needed", "output": "Ensure that distributions matching `requirements` are activated\n\n        `requirements` must be a string or a (possibly-nested) sequence\n        thereof, specifying the distributions and versions required.  The\n        return value is a sequence of the distributions that needed to be\n        activated to fulfill the requirements; all relevant distributions are\n        included, even if they were already activated in this working set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_as_warning(self, color=QColor(\"orange\")):\n        \"\"\"\n        \n        \"\"\"\n        self.format.setUnderlineStyle(\n            QTextCharFormat.WaveUnderline)\n        self.format.setUnderlineColor(color)", "output": "Highlights text as a syntax warning.\n\n        :param color: Underline color\n        :type color: QtGui.QColor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def char_conv(out):\n    \"\"\"\n    \n    \"\"\"\n    out_conv = list()\n    for i in range(out.shape[0]):\n        tmp_str = ''\n        for j in range(out.shape[1]):\n            if int(out[i][j]) >= 0:\n                tmp_char = int2char(int(out[i][j]))\n                if int(out[i][j]) == 27:\n                    tmp_char = ''\n                tmp_str = tmp_str + tmp_char\n        out_conv.append(tmp_str)\n    return out_conv", "output": "Convert integer vectors to character vectors for batch.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _extract_sam_function_codeuri(name, resource_properties, code_property_key):\n        \"\"\"\n        \n        \"\"\"\n        codeuri = resource_properties.get(code_property_key, SamFunctionProvider._DEFAULT_CODEURI)\n        # CodeUri can be a dictionary of S3 Bucket/Key or a S3 URI, neither of which are supported\n        if isinstance(codeuri, dict) or \\\n                (isinstance(codeuri, six.string_types) and codeuri.startswith(\"s3://\")):\n            codeuri = SamFunctionProvider._DEFAULT_CODEURI\n            LOG.warning(\"Lambda function '%s' has specified S3 location for CodeUri which is unsupported. \"\n                        \"Using default value of '%s' instead\", name, codeuri)\n        return codeuri", "output": "Extracts the SAM Function CodeUri from the Resource Properties\n\n        Parameters\n        ----------\n        name str\n            LogicalId of the resource\n        resource_properties dict\n            Dictionary representing the Properties of the Resource\n        code_property_key str\n            Property Key of the code on the Resource\n\n        Returns\n        -------\n        str\n            Representing the local code path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_embeddable_interpretation_doc(indent = 0):\n    \"\"\"\n    \n    \"\"\"\n\n    output_rows = []\n\n    # Pull out the doc string and put it in a table.\n    for name in sorted(dir(_interpretations)):\n        if name.startswith(\"_\") or \"__\" not in name:\n            continue\n\n        interpretation, type_str = name.split(\"__\")\n\n        func = getattr(_interpretations, name)\n\n        output_rows.append(\"%s (%s type):\" % (interpretation, type_str))\n        output_rows += [(\"  \" + line) for line in _textwrap.dedent(func.__doc__).strip().split(\"\\n\")]\n\n        output_rows.append(\"\")\n\n    return \"\\n\".join(\" \"*indent + line for line in output_rows)", "output": "Returns a list of the available interpretations and what they do.\n\n    If indent is specified, then the entire doc string is indented by that amount.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send_error(self, status_code: int = 500, **kwargs: Any) -> None:\n        \"\"\"\n        \"\"\"\n        if self._headers_written:\n            gen_log.error(\"Cannot send error response after headers written\")\n            if not self._finished:\n                # If we get an error between writing headers and finishing,\n                # we are unlikely to be able to finish due to a\n                # Content-Length mismatch. Try anyway to release the\n                # socket.\n                try:\n                    self.finish()\n                except Exception:\n                    gen_log.error(\"Failed to flush partial response\", exc_info=True)\n            return\n        self.clear()\n\n        reason = kwargs.get(\"reason\")\n        if \"exc_info\" in kwargs:\n            exception = kwargs[\"exc_info\"][1]\n            if isinstance(exception, HTTPError) and exception.reason:\n                reason = exception.reason\n        self.set_status(status_code, reason=reason)\n        try:\n            self.write_error(status_code, **kwargs)\n        except Exception:\n            app_log.error(\"Uncaught exception in write_error\", exc_info=True)\n        if not self._finished:\n            self.finish()", "output": "Sends the given HTTP error code to the browser.\n\n        If `flush()` has already been called, it is not possible to send\n        an error, so this method will simply terminate the response.\n        If output has been written but not yet flushed, it will be discarded\n        and replaced with the error page.\n\n        Override `write_error()` to customize the error page that is returned.\n        Additional keyword arguments are passed through to `write_error`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compatible_platforms(provided, required):\n    \"\"\"\n    \"\"\"\n    if provided is None or required is None or provided == required:\n        # easy case\n        return True\n\n    # Mac OS X special cases\n    reqMac = macosVersionString.match(required)\n    if reqMac:\n        provMac = macosVersionString.match(provided)\n\n        # is this a Mac package?\n        if not provMac:\n            # this is backwards compatibility for packages built before\n            # setuptools 0.6. All packages built after this point will\n            # use the new macosx designation.\n            provDarwin = darwinVersionString.match(provided)\n            if provDarwin:\n                dversion = int(provDarwin.group(1))\n                macosversion = \"%s.%s\" % (reqMac.group(1), reqMac.group(2))\n                if dversion == 7 and macosversion >= \"10.3\" or \\\n                        dversion == 8 and macosversion >= \"10.4\":\n                    return True\n            # egg isn't macosx or legacy darwin\n            return False\n\n        # are they the same major version and machine type?\n        if provMac.group(1) != reqMac.group(1) or \\\n                provMac.group(3) != reqMac.group(3):\n            return False\n\n        # is the required OS major update >= the provided one?\n        if int(provMac.group(2)) > int(reqMac.group(2)):\n            return False\n\n        return True\n\n    # XXX Linux and other platforms' special cases should go here\n    return False", "output": "Can code for the `provided` platform run on the `required` platform?\n\n    Returns true if either platform is ``None``, or the platforms are equal.\n\n    XXX Needs compatibility checks for Linux and other unixy OSes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put(self, entity):\n    \"\"\"\n    \"\"\"\n    self._cur_batch.put(entity)\n    self._num_mutations += 1\n    if self._num_mutations >= MAX_MUTATIONS_IN_BATCH:\n      self.commit()\n      self.begin()", "output": "Adds mutation of the entity to the mutation buffer.\n\n    If mutation buffer reaches its capacity then this method commit all pending\n    mutations from the buffer and emties it.\n\n    Args:\n      entity: entity which should be put into the datastore", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def name(self):\n        \"\"\"\n        \"\"\"\n        return self._instance._client.instance_admin_client.cluster_path(\n            self._instance._client.project, self._instance.instance_id, self.cluster_id\n        )", "output": "Cluster name used in requests.\n\n        .. note::\n          This property will not change if ``_instance`` and ``cluster_id``\n          do not, but the return value is not cached.\n\n        For example:\n\n        .. literalinclude:: snippets.py\n            :start-after: [START bigtable_cluster_name]\n            :end-before: [END bigtable_cluster_name]\n\n        The cluster name is of the form\n\n            ``\"projects/{project}/instances/{instance}/clusters/{cluster_id}\"``\n\n        :rtype: str\n        :returns: The cluster name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def np_array_datetime64_compat(arr, *args, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    # is_list_like\n    if (hasattr(arr, '__iter__') and not isinstance(arr, (str, bytes))):\n        arr = [tz_replacer(s) for s in arr]\n    else:\n        arr = tz_replacer(arr)\n\n    return np.array(arr, *args, **kwargs)", "output": "provide compat for construction of an array of strings to a\n    np.array(..., dtype=np.datetime64(..))\n    tz-changes in 1.11 that make '2015-01-01 09:00:00Z' show a deprecation\n    warning, when need to pass '2015-01-01 09:00:00'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enable(states):\n    '''\n    \n\n    '''\n    ret = {\n        'res': True,\n        'msg': ''\n    }\n\n    states = salt.utils.args.split_input(states)\n    log.debug('states %s', states)\n\n    msg = []\n    _disabled = __salt__['grains.get']('state_runs_disabled')\n    if not isinstance(_disabled, list):\n        _disabled = []\n\n    _changed = False\n    for _state in states:\n        log.debug('_state %s', _state)\n        if _state not in _disabled:\n            msg.append('Info: {0} state already enabled.'.format(_state))\n        else:\n            msg.append('Info: {0} state enabled.'.format(_state))\n            _disabled.remove(_state)\n            _changed = True\n\n    if _changed:\n        __salt__['grains.setval']('state_runs_disabled', _disabled)\n\n    ret['msg'] = '\\n'.join(msg)\n\n    # refresh the grains\n    __salt__['saltutil.refresh_modules']()\n\n    return ret", "output": "Enable state function or sls run\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.enable highstate\n\n        salt '*' state.enable test.succeed_without_changes\n\n    .. note::\n        To enable a state file from running provide the same name that would\n        be passed in a state.sls call.\n\n        salt '*' state.disable bind.config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close(self):\n        \"\"\"\"\"\"\n        if not self.is_open:\n            return\n        super(IndexCreator, self).close()\n        self.fidx.close()", "output": "Closes the record and index files.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setupParserForBuffer(self, buffer, filename):\n        \"\"\" \"\"\"\n        libxml2mod.xmlSetupParserForBuffer(self._o, buffer, filename)", "output": "Setup the parser context to parse a new buffer; Clears any\n          prior contents from the parser context. The buffer\n          parameter must not be None, but the filename parameter can\n           be", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(\n        name,\n        region,\n        user=None,\n        opts=False):\n    '''\n    \n    '''\n    ret = {'name': name, 'result': True, 'comment': '', 'changes': {}}\n\n    does_exist = __salt__['aws_sqs.queue_exists'](name, region, opts, user)\n\n    if not does_exist:\n        if __opts__['test']:\n            ret['result'] = None\n            ret['comment'] = 'AWS SQS queue {0} is set to be created'.format(\n                    name)\n            return ret\n        created = __salt__['aws_sqs.create_queue'](name, region, opts, user)\n        if created['retcode'] == 0:\n            ret['changes']['new'] = created['stdout']\n        else:\n            ret['result'] = False\n            ret['comment'] = created['stderr']\n\n    else:\n        ret['comment'] = '{0} exists in {1}'.format(name, region)\n\n    return ret", "output": "Ensure the SQS queue exists.\n\n    name\n        Name of the SQS queue.\n\n    region\n        Region to create the queue\n\n    user\n        Name of the user performing the SQS operations\n\n    opts\n        Include additional arguments and options to the aws command line", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_folder(cls, path:PathOrStr, train:PathOrStr='train', valid:PathOrStr='valid',\n                    valid_pct=None, classes:Collection=None, **kwargs:Any)->'ImageDataBunch':\n        \"\"\n        path=Path(path)\n        il = ImageList.from_folder(path)\n        if valid_pct is None: src = il.split_by_folder(train=train, valid=valid)\n        else: src = il.split_by_rand_pct(valid_pct)\n        src = src.label_from_folder(classes=classes)\n        return cls.create_from_ll(src, **kwargs)", "output": "Create from imagenet style dataset in `path` with `train`,`valid`,`test` subfolders (or provide `valid_pct`).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_credentials():\n    '''\n    \n    '''\n\n    # if the username and password were already found don't go through the\n    # connection process again\n    if 'username' in DETAILS and 'password' in DETAILS:\n        return DETAILS['username'], DETAILS['password']\n\n    passwords = __pillar__['proxy']['passwords']\n    for password in passwords:\n        DETAILS['password'] = password\n        if not __salt__['vsphere.test_vcenter_connection']():\n            # We are unable to authenticate\n            continue\n        # If we have data returned from above, we've successfully authenticated.\n        return DETAILS['username'], password\n    # We've reached the end of the list without successfully authenticating.\n    raise excs.VMwareConnectionError('Cannot complete login due to '\n                                     'incorrect credentials.')", "output": "Cycle through all the possible credentials and return the first one that\n    works.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_host(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The remove_host function must be called with '\n            '-f or --function.'\n        )\n\n    host_name = kwargs.get('host') if kwargs and 'host' in kwargs else None\n\n    if not host_name:\n        raise SaltCloudSystemExit(\n            'You must specify name of the host system.'\n        )\n\n    # Get the service instance\n    si = _get_si()\n\n    host_ref = salt.utils.vmware.get_mor_by_property(si, vim.HostSystem, host_name)\n    if not host_ref:\n        raise SaltCloudSystemExit(\n            'Specified host system does not exist.'\n        )\n\n    try:\n        if isinstance(host_ref.parent, vim.ClusterComputeResource):\n            # This is a host system that is part of a Cluster\n            task = host_ref.Destroy_Task()\n        else:\n            # This is a standalone host system\n            task = host_ref.parent.Destroy_Task()\n        salt.utils.vmware.wait_for_task(task, host_name, 'remove host', log_level='info')\n    except Exception as exc:\n        log.error(\n            'Error while removing host %s: %s',\n            host_name, exc,\n            # Show the traceback if the debug logging level is enabled\n            exc_info_on_loglevel=logging.DEBUG\n        )\n        return {host_name: 'failed to remove host'}\n\n    return {host_name: 'removed host from vcenter'}", "output": "Remove the specified host system from this VMware environment\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f remove_host my-vmware-config host=\"myHostSystemName\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare_data():\n  \"\"\"\"\"\"\n  ((x_train, y_train), (x_test, y_test)) = DATASET.load_data()\n  x_train = x_train.astype(\"float32\")\n  x_test = x_test.astype(\"float32\")\n  x_train /= 255.0\n  x_test /= 255.0\n  return ((x_train, y_train), (x_test, y_test))", "output": "Load and normalize data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait(object_ids, num_returns=1, timeout=None):\n    \"\"\"\n    \"\"\"\n    if isinstance(object_ids, (tuple, np.ndarray)):\n        return ray.wait(\n            list(object_ids), num_returns=num_returns, timeout=timeout)\n\n    return ray.wait(object_ids, num_returns=num_returns, timeout=timeout)", "output": "Return a list of IDs that are ready and a list of IDs that are not.\n\n    This method is identical to `ray.wait` except it adds support for tuples\n    and ndarrays.\n\n    Args:\n        object_ids (List[ObjectID], Tuple(ObjectID), np.array(ObjectID)):\n            List like of object IDs for objects that may or may not be ready.\n            Note that these IDs must be unique.\n        num_returns (int): The number of object IDs that should be returned.\n        timeout (float): The maximum amount of time in seconds to wait before\n            returning.\n\n    Returns:\n        A list of object IDs that are ready and a list of the remaining object\n            IDs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, api_repr):\n        \"\"\"\n        \"\"\"\n        instance = cls(api_repr[\"type\"])\n        instance._properties = api_repr\n        return instance", "output": "Return a :class:`TimePartitioning` object deserialized from a dict.\n\n        This method creates a new ``TimePartitioning`` instance that points to\n        the ``api_repr`` parameter as its internal properties dict. This means\n        that when a ``TimePartitioning`` instance is stored as a property of\n        another object, any changes made at the higher level will also appear\n        here::\n\n            >>> time_partitioning = TimePartitioning()\n            >>> table.time_partitioning = time_partitioning\n            >>> table.time_partitioning.field = 'timecolumn'\n            >>> time_partitioning.field\n            'timecolumn'\n\n        Args:\n            api_repr (Mapping[str, str]):\n                The serialized representation of the TimePartitioning, such as\n                what is output by :meth:`to_api_repr`.\n\n        Returns:\n            google.cloud.bigquery.table.TimePartitioning:\n                The ``TimePartitioning`` object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sys_mgr():\n    '''\n    \n    '''\n    thrift_port = six.text_type(__salt__['config.option']('cassandra.THRIFT_PORT'))\n    host = __salt__['config.option']('cassandra.host')\n    return SystemManager('{0}:{1}'.format(host, thrift_port))", "output": "Return a pycassa system manager connection object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def secret_absent(name, namespace='default', **kwargs):\n    '''\n    \n    '''\n\n    ret = {'name': name,\n           'changes': {},\n           'result': False,\n           'comment': ''}\n\n    secret = __salt__['kubernetes.show_secret'](name, namespace, **kwargs)\n\n    if secret is None:\n        ret['result'] = True if not __opts__['test'] else None\n        ret['comment'] = 'The secret does not exist'\n        return ret\n\n    if __opts__['test']:\n        ret['comment'] = 'The secret is going to be deleted'\n        ret['result'] = None\n        return ret\n\n    __salt__['kubernetes.delete_secret'](name, namespace, **kwargs)\n\n    # As for kubernetes 1.6.4 doesn't set a code when deleting a secret\n    # The kubernetes module will raise an exception if the kubernetes\n    # server will return an error\n    ret['result'] = True\n    ret['changes'] = {\n        'kubernetes.secret': {\n            'new': 'absent', 'old': 'present'}}\n    ret['comment'] = 'Secret deleted'\n    return ret", "output": "Ensures that the named secret is absent from the given namespace.\n\n    name\n        The name of the secret\n\n    namespace\n        The name of the namespace", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def seek(self, pos, whence=os.SEEK_SET):\n        \"\"\"\n        \"\"\"\n        if self.closed:\n            raise ValueError(\"I/O operation on closed file\")\n\n        if whence == os.SEEK_SET:\n            self.position = min(max(pos, 0), self.size)\n        elif whence == os.SEEK_CUR:\n            if pos < 0:\n                self.position = max(self.position + pos, 0)\n            else:\n                self.position = min(self.position + pos, self.size)\n        elif whence == os.SEEK_END:\n            self.position = max(min(self.size + pos, self.size), 0)\n        else:\n            raise ValueError(\"Invalid argument\")\n\n        self.buffer = b\"\"\n        self.fileobj.seek(self.position)", "output": "Seek to a position in the file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _maybe_pack_examples(self, generator):\n    \"\"\"\"\"\"\n    if not self.packed_length:\n      return generator\n    return generator_utils.pack_examples(\n        generator,\n        self.has_inputs,\n        self.packed_length,\n        spacing=self.packed_spacing,\n        chop_long_sequences=not self.has_inputs)", "output": "Wraps generator with packer if self.packed_length.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def selection_r(acquisition_function,\n                samples_y_aggregation,\n                x_bounds,\n                x_types,\n                regressor_gp,\n                num_starting_points=100,\n                minimize_constraints_fun=None):\n    '''\n    \n    '''\n    minimize_starting_points = [lib_data.rand(x_bounds, x_types) \\\n                                    for i in range(0, num_starting_points)]\n    outputs = selection(acquisition_function, samples_y_aggregation,\n                        x_bounds, x_types, regressor_gp,\n                        minimize_starting_points,\n                        minimize_constraints_fun=minimize_constraints_fun)\n\n    return outputs", "output": "Selecte R value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_hash(self):\n        \"\"\"\n        \n        \"\"\"\n        _logger.debug('update hash')\n        layer_in_cnt = [len(layer.input) for layer in self.layers]\n        topo_queue = deque([i for i, layer in enumerate(self.layers) if not layer.is_delete and layer.graph_type == LayerType.input.value])\n        while topo_queue:\n            layer_i = topo_queue.pop()\n            self.layers[layer_i].update_hash(self.layers)\n            for layer_j in self.layers[layer_i].output:\n                layer_in_cnt[layer_j] -= 1\n                if layer_in_cnt[layer_j] == 0:\n                    topo_queue.appendleft(layer_j)", "output": "update hash id of each layer, in topological order/recursively\n        hash id will be used in weight sharing", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rotate_right(self):\n        \"\"\"\n        \n        \"\"\"\n        new_root = self.node.left.node\n        new_left_sub = new_root.right.node\n        old_root = self.node\n\n        self.node = new_root\n        old_root.left.node = new_left_sub\n        new_root.right.node = old_root", "output": "Right rotation", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def createOutputBuffer(file, encoding):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlCreateOutputBuffer(file, encoding)\n    if ret is None:raise treeError('xmlCreateOutputBuffer() failed')\n    return outputBuffer(_obj=ret)", "output": "Create a libxml2 output buffer from a Python file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copyMakeBorder(src, top, bot, left, right, *args, **kwargs):\n    \"\"\"\n    \"\"\"\n    return _internal._cvcopyMakeBorder(src, top, bot, left, right, *args, **kwargs)", "output": "Pad image border with OpenCV.\n\n    Parameters\n    ----------\n    src : NDArray\n        source image\n    top : int, required\n        Top margin.\n    bot : int, required\n        Bottom margin.\n    left : int, required\n        Left margin.\n    right : int, required\n        Right margin.\n    type : int, optional, default='0'\n        Filling type (default=cv2.BORDER_CONSTANT).\n        0 - cv2.BORDER_CONSTANT - Adds a constant colored border.\n        1 - cv2.BORDER_REFLECT - Border will be mirror reflection of the\n        border elements, like this : fedcba|abcdefgh|hgfedcb\n        2 - cv2.BORDER_REFLECT_101 or cv.BORDER_DEFAULT - Same as above,\n        but with a slight change, like this : gfedcb|abcdefgh|gfedcba\n        3 - cv2.BORDER_REPLICATE - Last element is replicated throughout,\n        like this: aaaaaa|abcdefgh|hhhhhhh\n        4 - cv2.BORDER_WRAP - it will look like this : cdefgh|abcdefgh|abcdefg\n    value : double, optional, default=0\n        (Deprecated! Use ``values`` instead.) Fill with single value.\n    values : tuple of <double>, optional, default=[]\n        Fill with value(RGB[A] or gray), up to 4 channels.\n\n    out : NDArray, optional\n        The output NDArray to hold the result.\n\n    Returns\n    -------\n    out : NDArray or list of NDArrays\n        The output of this function.\n\n    Example\n    --------\n    >>> with open(\"flower.jpeg\", 'rb') as fp:\n    ...     str_image = fp.read()\n    ...\n    >>> image = mx.img.imdecode(str_image)\n    >>> image\n    <NDArray 2321x3482x3 @cpu(0)>\n    >>> new_image = mx_border = mx.image.copyMakeBorder(mx_img, 1, 2, 3, 4, type=0)\n    >>> new_image\n    <NDArray 2324x3489x3 @cpu(0)>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _group_tasks_by_name_and_status(task_dict):\n    \"\"\"\n    \n    \"\"\"\n    group_status = {}\n    for task in task_dict:\n        if task.task_family not in group_status:\n            group_status[task.task_family] = []\n        group_status[task.task_family].append(task)\n    return group_status", "output": "Takes a dictionary with sets of tasks grouped by their status and\n    returns a dictionary with dictionaries with an array of tasks grouped by\n    their status and task name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_initial_sync_op(self):\n        \"\"\"\n        \n        \"\"\"\n        def strip_port(s):\n            if s.endswith(':0'):\n                return s[:-2]\n            return s\n        local_vars = tf.local_variables()\n        local_var_by_name = dict([(strip_port(v.name), v) for v in local_vars])\n        ops = []\n        nr_shadow_vars = len(self._shadow_vars)\n        for v in self._shadow_vars:\n            vname = strip_port(v.name)\n            for i in range(self.nr_gpu):\n                name = 'tower%s/%s' % (i, vname)\n                assert name in local_var_by_name, \\\n                    \"Shadow variable {} doesn't match a corresponding local variable!\".format(v.name)\n                copy_to = local_var_by_name[name]\n                # logger.info(\"{} -> {}\".format(v.name, copy_to.name))\n                ops.append(copy_to.assign(v.read_value()))\n        return tf.group(*ops, name='sync_{}_variables_from_ps'.format(nr_shadow_vars))", "output": "Get the op to copy-initialized all local variables from PS.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def swapkey(self, old_key, new_key, new_value=None):\n        \"\"\"\n        \"\"\"\n        if self.has_resolvers:\n            maps = self.resolvers.maps + self.scope.maps\n        else:\n            maps = self.scope.maps\n\n        maps.append(self.temps)\n\n        for mapping in maps:\n            if old_key in mapping:\n                mapping[new_key] = new_value\n                return", "output": "Replace a variable name, with a potentially new value.\n\n        Parameters\n        ----------\n        old_key : str\n            Current variable name to replace\n        new_key : str\n            New variable name to replace `old_key` with\n        new_value : object\n            Value to be replaced along with the possible renaming", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self, statement):\n        \"\"\"\n        \n        \"\"\"\n        Statement = self.get_model('statement')\n        Tag = self.get_model('tag')\n\n        if statement is not None:\n            session = self.Session()\n            record = None\n\n            if hasattr(statement, 'id') and statement.id is not None:\n                record = session.query(Statement).get(statement.id)\n            else:\n                record = session.query(Statement).filter(\n                    Statement.text == statement.text,\n                    Statement.conversation == statement.conversation,\n                ).first()\n\n                # Create a new statement entry if one does not already exist\n                if not record:\n                    record = Statement(\n                        text=statement.text,\n                        conversation=statement.conversation,\n                        persona=statement.persona\n                    )\n\n            # Update the response value\n            record.in_response_to = statement.in_response_to\n\n            record.created_at = statement.created_at\n\n            record.search_text = self.tagger.get_bigram_pair_string(statement.text)\n\n            if statement.in_response_to:\n                record.search_in_response_to = self.tagger.get_bigram_pair_string(statement.in_response_to)\n\n            for tag_name in statement.get_tags():\n                tag = session.query(Tag).filter_by(name=tag_name).first()\n\n                if not tag:\n                    # Create the record\n                    tag = Tag(name=tag_name)\n\n                record.tags.append(tag)\n\n            session.add(record)\n\n            self._session_finish(session)", "output": "Modifies an entry in the database.\n        Creates an entry if one does not exist.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_demonstration(file_path):\n    \"\"\"\n    \n    \"\"\"\n\n    # First 32 bytes of file dedicated to meta-data.\n    INITIAL_POS = 33\n\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(\"The demonstration file {} does not exist.\".format(file_path))\n    file_extension = pathlib.Path(file_path).suffix\n    if file_extension != '.demo':\n        raise ValueError(\"The file is not a '.demo' file. Please provide a file with the \"\n                         \"correct extension.\")\n\n    brain_params = None\n    brain_infos = []\n    data = open(file_path, \"rb\").read()\n    next_pos, pos, obs_decoded = 0, 0, 0\n    total_expected = 0\n    while pos < len(data):\n        next_pos, pos = _DecodeVarint32(data, pos)\n        if obs_decoded == 0:\n            meta_data_proto = DemonstrationMetaProto()\n            meta_data_proto.ParseFromString(data[pos:pos + next_pos])\n            total_expected = meta_data_proto.number_steps\n            pos = INITIAL_POS\n        if obs_decoded == 1:\n            brain_param_proto = BrainParametersProto()\n            brain_param_proto.ParseFromString(data[pos:pos + next_pos])\n            brain_params = BrainParameters.from_proto(brain_param_proto)\n            pos += next_pos\n        if obs_decoded > 1:\n            agent_info = AgentInfoProto()\n            agent_info.ParseFromString(data[pos:pos + next_pos])\n            brain_info = BrainInfo.from_agent_proto([agent_info], brain_params)\n            brain_infos.append(brain_info)\n            if len(brain_infos) == total_expected:\n                break\n            pos += next_pos\n        obs_decoded += 1\n    return brain_params, brain_infos, total_expected", "output": "Loads and parses a demonstration file.\n    :param file_path: Location of demonstration file (.demo).\n    :return: BrainParameter and list of BrainInfos containing demonstration data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pixel_data(self):\n        \"\"\"\n        \n        \"\"\"\n\n        from .. import extensions as _extensions\n        data = _np.zeros((self.height, self.width, self.channels), dtype=_np.uint8)\n        _extensions.image_load_to_numpy(self, data.ctypes.data, data.strides)\n        if self.channels == 1:\n            data = data.squeeze(2)\n        return data", "output": "Returns the pixel data stored in the Image object.\n\n        Returns\n        -------\n        out : numpy.array\n            The pixel data of the Image object. It returns a multi-dimensional\n            numpy array, where the shape of the array represents the shape of\n            the image (height, weight, channels).\n\n        See Also\n        --------\n        width, channels, height\n\n        Examples\n        --------\n        >>> img = turicreate.Image('https://static.turi.com/datasets/images/sample.jpg')\n        >>> image_array = img.pixel_data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def weights_concatenated(labels):\n  \"\"\"\n  \"\"\"\n  eos_mask = tf.to_int32(tf.equal(labels, 1))\n  sentence_num = tf.cumsum(eos_mask, axis=1, exclusive=True)\n  in_target = tf.equal(tf.mod(sentence_num, 2), 1)\n  # first two tokens of each sentence are boilerplate.\n  sentence_num_plus_one = sentence_num + 1\n  shifted = tf.pad(sentence_num_plus_one,\n                   [[0, 0], [2, 0], [0, 0], [0, 0]])[:, :-2, :, :]\n  nonboilerplate = tf.equal(sentence_num_plus_one, shifted)\n  ret = to_float(tf.logical_and(nonboilerplate, in_target))\n  return ret", "output": "Assign weight 1.0 to the \"target\" part of the concatenated labels.\n\n  The labels look like:\n    source English I love you . ID1 target French Je t'aime . ID1 source\n      English the cat ID1 target French le chat ID1 source English ...\n\n  We want to assign weight 1.0 to all words in the target text (including the\n  ID1 end symbol), but not to the source text or the boilerplate.  In the\n  above example, the target words that get positive weight are:\n    Je t'aime . ID1 le chat ID1\n\n  Args:\n    labels: a Tensor\n  Returns:\n    a Tensor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def config_merge_tree(source='running',\n                      merge_config=None,\n                      merge_path=None,\n                      saltenv='base'):\n    '''\n    \n    '''\n    config_txt = __salt__['net.config'](source=source)['out'][source]\n    return __salt__['iosconfig.merge_tree'](initial_config=config_txt,\n                                            merge_config=merge_config,\n                                            merge_path=merge_path,\n                                            saltenv=saltenv)", "output": ".. versionadded:: 2019.2.0\n\n    Return the merge tree of the ``initial_config`` with the ``merge_config``,\n    as a Python dictionary.\n\n    source: ``running``\n        The configuration type to retrieve from the network device. Default:\n        ``running``. Available options: ``running``, ``startup``, ``candidate``.\n\n    merge_config\n        The config to be merged into the initial config, sent as text. This\n        argument is ignored when ``merge_path`` is set.\n\n    merge_path\n        Absolute or remote path from where to load the merge configuration\n        text. This argument allows any URI supported by\n        :py:func:`cp.get_url <salt.modules.cp.get_url>`), e.g., ``salt://``,\n        ``https://``, ``s3://``, ``ftp:/``, etc.\n\n    saltenv: ``base``\n        Salt fileserver environment from which to retrieve the file.\n        Ignored if ``merge_path`` is not a ``salt://`` URL.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm.config_merge_tree merge_path=salt://path/to/merge.cfg", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_cli(function_name, stack_name, filter_pattern, tailing, start_time, end_time):\n    \"\"\"\n    \n    \"\"\"\n\n    LOG.debug(\"'logs' command is called\")\n\n    with LogsCommandContext(function_name,\n                            stack_name=stack_name,\n                            filter_pattern=filter_pattern,\n                            start_time=start_time,\n                            end_time=end_time,\n                            # output_file is not yet supported by CLI\n                            output_file=None) as context:\n\n        if tailing:\n            events_iterable = context.fetcher.tail(context.log_group_name,\n                                                   filter_pattern=context.filter_pattern,\n                                                   start=context.start_time)\n        else:\n            events_iterable = context.fetcher.fetch(context.log_group_name,\n                                                    filter_pattern=context.filter_pattern,\n                                                    start=context.start_time,\n                                                    end=context.end_time)\n\n        formatted_events = context.formatter.do_format(events_iterable)\n\n        for event in formatted_events:\n            # New line is not necessary. It is already in the log events sent by CloudWatch\n            click.echo(event, nl=False)", "output": "Implementation of the ``cli`` method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def orchestrate_show_sls(mods,\n                         saltenv='base',\n                         test=None,\n                         queue=False,\n                         pillar=None,\n                         pillarenv=None,\n                         pillar_enc=None):\n    '''\n    \n    '''\n    if pillar is not None and not isinstance(pillar, dict):\n        raise SaltInvocationError(\n            'Pillar data must be formatted as a dictionary')\n\n    __opts__['file_client'] = 'local'\n    minion = salt.minion.MasterMinion(__opts__)\n    running = minion.functions['state.show_sls'](\n        mods,\n        test,\n        queue,\n        pillar=pillar,\n        pillarenv=pillarenv,\n        pillar_enc=pillar_enc,\n        saltenv=saltenv)\n\n    ret = {minion.opts['id']: running}\n    return ret", "output": "Display the state data from a specific sls, or list of sls files, after\n    being render using the master minion.\n\n    Note, the master minion adds a \"_master\" suffix to it's minion id.\n\n    .. seealso:: The state.show_sls module function\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt-run state.orch_show_sls my-orch-formula.my-orch-state 'pillar={ nodegroup: ng1 }'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def url(self):\n        \"\"\"\"\"\"\n        if self.is_unicode_emoji():\n            return Asset(self._state)\n\n        _format = 'gif' if self.animated else 'png'\n        url = \"https://cdn.discordapp.com/emojis/{0.id}.{1}\".format(self, _format)\n        return Asset(self._state, url)", "output": ":class:`Asset`:Returns an asset of the emoji, if it is custom.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_filepath(changes):\n    '''\n    \n    '''\n    filename = None\n    for change_ in changes:\n        try:\n            cmd, arg = change_.split(' ', 1)\n\n            if cmd not in METHOD_MAP:\n                error = 'Command {0} is not supported (yet)'.format(cmd)\n                raise ValueError(error)\n            method = METHOD_MAP[cmd]\n            parts = salt.utils.args.shlex_split(arg)\n            if method in ['set', 'setm', 'move', 'remove']:\n                filename_ = parts[0]\n            else:\n                _, _, filename_ = parts\n            if not filename_.startswith('/files'):\n                error = 'Changes should be prefixed with ' \\\n                        '/files if no context is provided,' \\\n                        ' change: {0}'.format(change_)\n                raise ValueError(error)\n            filename_ = re.sub('^/files|/$', '', filename_)\n            if filename is not None:\n                if filename != filename_:\n                    error = 'Changes should be made to one ' \\\n                            'file at a time, detected changes ' \\\n                            'to {0} and {1}'.format(filename, filename_)\n                    raise ValueError(error)\n            filename = filename_\n        except (ValueError, IndexError) as err:\n            log.error(err)\n            if 'error' not in locals():\n                error = 'Invalid formatted command, ' \\\n                               'see debug log for details: {0}' \\\n                               .format(change_)\n            else:\n                error = six.text_type(err)\n            raise ValueError(error)\n\n    filename = _workout_filename(filename)\n\n    return filename", "output": "Ensure all changes are fully qualified and affect only one file.\n    This ensures that the diff output works and a state change is not\n    incorrectly reported.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def snli_token_generator(tmp_dir, train, vocab_size):\n  \"\"\"\"\"\"\n  _download_and_parse_dataset(tmp_dir, train)\n\n  symbolizer_vocab = _get_or_generate_vocab(\n      tmp_dir, 'vocab.subword_text_encoder', vocab_size)\n\n  file_name = 'train' if train else 'dev'\n  data_file = os.path.join(tmp_dir, file_name + '.txt')\n  with tf.gfile.GFile(data_file, mode='r') as f:\n    for line in f:\n      sent1, sent2, label = line.strip().split('\\t')\n      sent1_enc = symbolizer_vocab.encode(sent1)\n      sent2_enc = symbolizer_vocab.encode(sent2)\n\n      inputs = sent1_enc + [_SEP] + sent2_enc + [_EOS]\n      yield {\n          'inputs': inputs,\n          'targets': [_LABEL_TO_ID[label]],\n      }", "output": "Generate example dicts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def group_diff(name):\n\n    '''\n    \n    '''\n\n    # Use a compatible structure with yum, so we can leverage the existing state.group_installed\n    # In pacmanworld, everything is the default, but nothing is mandatory\n\n    pkgtypes = ('mandatory', 'optional', 'default', 'conditional')\n    ret = {}\n    for pkgtype in pkgtypes:\n        ret[pkgtype] = {'installed': [], 'not installed': []}\n\n    # use indirect references to simplify unit testing\n    pkgs = __salt__['pkg.list_pkgs']()\n    group_pkgs = __salt__['pkg.group_info'](name)\n    for pkgtype in pkgtypes:\n        for member in group_pkgs.get(pkgtype, []):\n            if member in pkgs:\n                ret[pkgtype]['installed'].append(member)\n            else:\n                ret[pkgtype]['not installed'].append(member)\n    return ret", "output": ".. versionadded:: 2016.11.0\n\n    Lists which of a group's packages are installed and which are not\n    installed\n\n    Compatible with yumpkg.group_diff for easy support of state.pkg.group_installed\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.group_diff 'xorg'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _pad_default(x, padding:int, mode='reflection'):\n    \"\"\n    mode = _pad_mode_convert[mode]\n    return F.pad(x[None], (padding,)*4, mode=mode)[0]", "output": "Pad `x` with `padding` pixels. `mode` fills in space ('zeros','reflection','border').", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dataset_name_and_kwargs_from_name_str(name_str):\n  \"\"\"\"\"\"\n  res = _NAME_REG.match(name_str)\n  if not res:\n    raise ValueError(_NAME_STR_ERR.format(name_str))\n  name = res.group(\"dataset_name\")\n  kwargs = _kwargs_str_to_kwargs(res.group(\"kwargs\"))\n  try:\n    for attr in [\"config\", \"version\"]:\n      val = res.group(attr)\n      if val is None:\n        continue\n      if attr in kwargs:\n        raise ValueError(\"Dataset %s: cannot pass %s twice.\" % (name, attr))\n      kwargs[attr] = val\n    return name, kwargs\n  except:\n    logging.error(_NAME_STR_ERR.format(name_str))   # pylint: disable=logging-format-interpolation\n    raise", "output": "Extract kwargs from name str.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_xgboost_json(model):\n    \"\"\" \n    \"\"\"\n    fnames = model.feature_names\n    model.feature_names = None\n    json_trees = model.get_dump(with_stats=True, dump_format=\"json\")\n    model.feature_names = fnames\n\n    # this fixes a bug where XGBoost can return invalid JSON\n    json_trees = [t.replace(\": inf,\", \": 1000000000000.0,\") for t in json_trees]\n    json_trees = [t.replace(\": -inf,\", \": -1000000000000.0,\") for t in json_trees]\n\n    return json_trees", "output": "This gets a JSON dump of an XGBoost model while ensuring the features names are their indexes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def batch_norm_relu(inputs, is_training, relu=True):\n  \"\"\"\"\"\"\n  inputs = mtf.layers.batch_norm(\n      inputs,\n      is_training,\n      BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      init_zero=(not relu))\n  if relu:\n    inputs = mtf.relu(inputs)\n  return inputs", "output": "Block of batch norm and relu.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def file_list(load):\n    '''\n    \n    '''\n    if 'env' in load:\n        # \"env\" is not supported; Use \"saltenv\".\n        load.pop('env')\n\n    ret = []\n\n    if 'saltenv' not in load:\n        return ret\n\n    saltenv = load['saltenv']\n    metadata = _init()\n\n    if not metadata or saltenv not in metadata:\n        return ret\n    for bucket in _find_files(metadata[saltenv]):\n        for buckets in six.itervalues(bucket):\n            files = [f for f in buckets if not fs.is_file_ignored(__opts__, f)]\n            ret += _trim_env_off_path(files, saltenv)\n\n    return ret", "output": "Return a list of all files on the file server in a specified environment", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _warn_unsafe_extraction_path(path):\n        \"\"\"\n        \n        \"\"\"\n        if os.name == 'nt' and not path.startswith(os.environ['windir']):\n            # On Windows, permissions are generally restrictive by default\n            #  and temp directories are not writable by other users, so\n            #  bypass the warning.\n            return\n        mode = os.stat(path).st_mode\n        if mode & stat.S_IWOTH or mode & stat.S_IWGRP:\n            msg = (\n                \"%s is writable by group/others and vulnerable to attack \"\n                \"when \"\n                \"used with get_resource_filename. Consider a more secure \"\n                \"location (set with .set_extraction_path or the \"\n                \"PYTHON_EGG_CACHE environment variable).\" % path\n            )\n            warnings.warn(msg, UserWarning)", "output": "If the default extraction path is overridden and set to an insecure\n        location, such as /tmp, it opens up an opportunity for an attacker to\n        replace an extracted file with an unauthorized payload. Warn the user\n        if a known insecure location is used.\n\n        See Distribute #375 for more details.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_exception(self, exception):\n        \"\"\"\"\"\"\n        self._exception = exception\n        self._result_set = True\n        self._invoke_callbacks(self)", "output": "Set the Future's exception.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def max_profit_naive(prices):\n    \"\"\"\n    \n    \"\"\"\n    max_so_far = 0\n    for i in range(0, len(prices) - 1):\n        for j in range(i + 1, len(prices)):\n            max_so_far = max(max_so_far, prices[j] - prices[i])\n    return max_so_far", "output": ":type prices: List[int]\n    :rtype: int", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _extract_from_serverless_api(self, logical_id, api_resource, collector):\n        \"\"\"\n        \n        \"\"\"\n\n        properties = api_resource.get(\"Properties\", {})\n        body = properties.get(\"DefinitionBody\")\n        uri = properties.get(\"DefinitionUri\")\n        binary_media = properties.get(\"BinaryMediaTypes\", [])\n\n        if not body and not uri:\n            # Swagger is not found anywhere.\n            LOG.debug(\"Skipping resource '%s'. Swagger document not found in DefinitionBody and DefinitionUri\",\n                      logical_id)\n            return\n\n        reader = SamSwaggerReader(definition_body=body,\n                                  definition_uri=uri,\n                                  working_dir=self.cwd)\n        swagger = reader.read()\n        parser = SwaggerParser(swagger)\n        apis = parser.get_apis()\n        LOG.debug(\"Found '%s' APIs in resource '%s'\", len(apis), logical_id)\n\n        collector.add_apis(logical_id, apis)\n        collector.add_binary_media_types(logical_id, parser.get_binary_media_types())  # Binary media from swagger\n        collector.add_binary_media_types(logical_id, binary_media)", "output": "Extract APIs from AWS::Serverless::Api resource by reading and parsing Swagger documents. The result is added\n        to the collector.\n\n        Parameters\n        ----------\n        logical_id : str\n            Logical ID of the resource\n\n        api_resource : dict\n            Resource definition, including its properties\n\n        collector : ApiCollector\n            Instance of the API collector that where we will save the API information", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def entries(self):\n        \"\"\"\n        \n        \"\"\"\n        # We use DataFrames for serialization of MatrixEntry entries\n        # from Java, so we first convert the RDD of entries to a\n        # DataFrame on the Scala/Java side. Then we map each Row in\n        # the DataFrame back to a MatrixEntry on this side.\n        entries_df = callMLlibFunc(\"getMatrixEntries\", self._java_matrix_wrapper._java_model)\n        entries = entries_df.rdd.map(lambda row: MatrixEntry(row[0], row[1], row[2]))\n        return entries", "output": "Entries of the CoordinateMatrix stored as an RDD of\n        MatrixEntries.\n\n        >>> mat = CoordinateMatrix(sc.parallelize([MatrixEntry(0, 0, 1.2),\n        ...                                        MatrixEntry(6, 4, 2.1)]))\n        >>> entries = mat.entries\n        >>> entries.first()\n        MatrixEntry(0, 0, 1.2)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_sub_intrinsic(data):\n        \"\"\"\n        \n        \"\"\"\n        return isinstance(data, dict) and len(data) == 1 and LambdaUri._FN_SUB in data", "output": "Is this input data a Fn::Sub intrinsic function\n\n        Parameters\n        ----------\n        data\n            Data to check\n\n        Returns\n        -------\n        bool\n            True if the data Fn::Sub intrinsic function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def defuse_activation(self):\n        \"\"\"\n        \n        \"\"\"\n        idx, nb_layers = 0, len(self.layer_list)\n        while idx < nb_layers:\n            layer = self.layer_list[idx]\n            k_layer = self.keras_layer_map[layer]\n            # unwrap time-distributed layers\n            if (isinstance(k_layer, _keras.layers.TimeDistributed)):\n                k_layer = k_layer.layer\n            if (isinstance(k_layer, _keras.layers.convolutional.Convolution2D) or\n                isinstance(k_layer, _keras.layers.convolutional.Convolution1D) or\n                isinstance(k_layer, _keras.layers.core.Dense)):\n\n                import six\n                if six.PY2:\n                    func_name = k_layer.activation.func_name\n                else:\n                    func_name = k_layer.activation.__name__\n\n                if (func_name != 'linear'):\n                    # Create new layer\n                    new_layer = layer + '__activation__'\n                    new_keras_layer = _keras.layers.core.Activation(func_name)\n                    # insert new layer after it\n                    self._insert_layer_after(idx, new_layer, new_keras_layer)\n                    idx += 1\n                    nb_layers += 1\n\n            idx += 1", "output": "Defuse the fused activation layers in the network.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_config(name, xpath=None, commit=False):\n    '''\n    \n\n    '''\n    ret = _default_ret(name)\n\n    if not xpath:\n        return ret\n\n    query = {'type': 'config',\n             'action': 'delete',\n             'xpath': xpath}\n\n    result, response = _validate_response(__proxy__['panos.call'](query))\n\n    ret.update({\n        'changes': response,\n        'result': result\n    })\n\n    if not result:\n        return ret\n\n    if commit is True:\n        ret.update({\n            'commit': __salt__['panos.commit'](),\n            'result': True\n        })\n\n    return ret", "output": "Deletes a Palo Alto XPATH to a specific value.\n\n    Use the xpath parameter to specify the location of the object to be deleted.\n\n    name: The name of the module function to execute.\n\n    xpath(str): The XPATH of the configuration API tree to control.\n\n    commit(bool): If true the firewall will commit the changes, if false do not commit changes.\n\n    SLS Example:\n\n    .. code-block:: yaml\n\n        panos/deletegroup:\n            panos.delete_config:\n              - xpath: /config/devices/entry/vsys/entry[@name='vsys1']/address-group/entry[@name='test']\n              - commit: True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def countByValue(self):\n        \"\"\"\n        \n        \"\"\"\n        def countPartition(iterator):\n            counts = defaultdict(int)\n            for obj in iterator:\n                counts[obj] += 1\n            yield counts\n\n        def mergeMaps(m1, m2):\n            for k, v in m2.items():\n                m1[k] += v\n            return m1\n        return self.mapPartitions(countPartition).reduce(mergeMaps)", "output": "Return the count of each unique value in this RDD as a dictionary of\n        (value, count) pairs.\n\n        >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n        [(1, 2), (2, 3)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def loaddata(settings_module,\n             fixtures,\n             bin_env=None,\n             database=None,\n             pythonpath=None,\n             env=None):\n    '''\n    \n\n    '''\n    args = []\n    kwargs = {}\n    if database:\n        kwargs['database'] = database\n\n    cmd = '{0} {1}'.format('loaddata', ' '.join(fixtures.split(',')))\n\n    return command(settings_module,\n                   cmd,\n                   bin_env,\n                   pythonpath,\n                   env,\n                   *args, **kwargs)", "output": "Load fixture data\n\n    Fixtures:\n        comma separated list of fixtures to load\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' django.loaddata <settings_module> <comma delimited list of fixtures>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def qualifier_encoded(self):\n        \"\"\"\n        \"\"\"\n        prop = self._properties.get(\"qualifierEncoded\")\n        if prop is None:\n            return None\n        return base64.standard_b64decode(_to_bytes(prop))", "output": "Union[str, bytes]: The qualifier encoded in binary.\n\n        The type is ``str`` (Python 2.x) or ``bytes`` (Python 3.x). The module\n        will handle base64 encoding for you.\n\n        See\n        https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.query.tableDefinitions.%28key%29.bigtableOptions.columnFamilies.columns.qualifierEncoded\n        https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#externalDataConfiguration.bigtableOptions.columnFamilies.columns.qualifierEncoded", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _maybe_coerce_values(self, values):\n        \"\"\"\n        \"\"\"\n        if not isinstance(values, self._holder):\n            values = self._holder(values)\n\n        if values.tz is None:\n            raise ValueError(\"cannot create a DatetimeTZBlock without a tz\")\n\n        return values", "output": "Input validation for values passed to __init__. Ensure that\n        we have datetime64TZ, coercing if necessary.\n\n        Parametetrs\n        -----------\n        values : array-like\n            Must be convertible to datetime64\n\n        Returns\n        -------\n        values : DatetimeArray", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_metadata_of(group, parent_group):\n    \"\"\"\"\"\"\n    if group._v_depth <= parent_group._v_depth:\n        return False\n\n    current = group\n    while current._v_depth > 1:\n        parent = current._v_parent\n        if parent == parent_group and current._v_name == 'meta':\n            return True\n        current = current._v_parent\n    return False", "output": "Check if a given group is a metadata group for a given parent_group.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def plot(self, title=LABEL_DEFAULT, xlabel=LABEL_DEFAULT, ylabel=LABEL_DEFAULT):\n        \"\"\"\n        \n        \"\"\"\n\n        if title == \"\":\n            title = \" \"\n        if xlabel == \"\":\n            xlabel = \" \"\n        if ylabel == \"\":\n            ylabel = \" \"\n\n        if title is None:\n            title = \"\" # C++ otherwise gets \"None\" as std::string\n        if xlabel is None:\n            xlabel = \"\"\n        if ylabel is None:\n            ylabel = \"\"\n\n        return Plot(self.__proxy__.plot(title, xlabel, ylabel))", "output": "Create a Plot object representing the SArray.\n\n        Notes\n        -----\n        - The plot will render either inline in a Jupyter Notebook, or in a\n          native GUI window, depending on the value provided in\n          `turicreate.visualization.set_target` (defaults to 'auto').\n\n        Parameters\n        ----------\n        title : str\n            The plot title to show for the resulting visualization.\n            If the title is None, the title will be omitted.\n\n        xlabel : str\n            The X axis label to show for the resulting visualization.\n            If the xlabel is None, the X axis label will be omitted.\n\n        ylabel : str\n            The Y axis label to show for the resulting visualization.\n            If the ylabel is None, the Y axis label will be omitted.\n\n        Returns\n        -------\n        out : Plot\n        A :class: Plot object that is the visualization of the SArray.\n\n        Examples\n        --------\n        Suppose 'sa' is an SArray, we can create a plot of it using:\n\n        >>> plt = sa.plot()\n\n        To override the default plot title and axis labels:\n\n        >>> plt = sa.plot(title=\"My Plot Title\", xlabel=\"My X Axis\", ylabel=\"My Y Axis\")\n\n        We can then visualize the plot using:\n\n        >>> plt.show()", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tree_shap_independent_200(model, data):\n    \"\"\" \n    \"\"\"\n    data_subsample = sklearn.utils.resample(data, replace=False, n_samples=min(200, data.shape[0]), random_state=0)\n    return TreeExplainer(model, data_subsample, feature_dependence=\"independent\").shap_values", "output": "TreeExplainer (independent)\n    color = red_blue_circle(0)\n    linestyle = dashed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def missing_node_cache(prov_dir, node_list, provider, opts):\n    '''\n    \n    '''\n    cached_nodes = []\n    for node in os.listdir(prov_dir):\n        cached_nodes.append(os.path.splitext(node)[0])\n\n    for node in cached_nodes:\n        if node not in node_list:\n            delete_minion_cachedir(node, provider, opts)\n            if 'diff_cache_events' in opts and opts['diff_cache_events']:\n                fire_event(\n                    'event',\n                    'cached node missing from provider',\n                    'salt/cloud/{0}/cache_node_missing'.format(node),\n                    args={'missing node': node},\n                    sock_dir=opts.get(\n                        'sock_dir',\n                        os.path.join(__opts__['sock_dir'], 'master')),\n                    transport=opts.get('transport', 'zeromq')\n                )", "output": "Check list of nodes to see if any nodes which were previously known about\n    in the cache have been removed from the node list.\n\n    This function will only run if configured to do so in the main Salt Cloud\n    configuration file (normally /etc/salt/cloud).\n\n    .. code-block:: yaml\n\n        diff_cache_events: True\n\n    .. versionadded:: 2014.7.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_table_index(self, key, **kwargs):\n        \"\"\" \n\n        \"\"\"\n\n        # version requirements\n        _tables()\n        s = self.get_storer(key)\n        if s is None:\n            return\n\n        if not s.is_table:\n            raise TypeError(\n                \"cannot create table index on a Fixed format store\")\n        s.create_index(**kwargs)", "output": "Create a pytables index on the table\n        Parameters\n        ----------\n        key : object (the node to index)\n\n        Exceptions\n        ----------\n        raises if the node is not a table", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _trim_front(strings):\n    \"\"\"\n    \n    \"\"\"\n    trimmed = strings\n    while len(strings) > 0 and all(x[0] == ' ' for x in trimmed):\n        trimmed = [x[1:] for x in trimmed]\n    return trimmed", "output": "Trims zeros and decimal points.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def relaxNGValidatePopElement(self, doc, elem):\n        \"\"\" \"\"\"\n        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        ret = libxml2mod.xmlRelaxNGValidatePopElement(self._o, doc__o, elem__o)\n        return ret", "output": "Pop the element end from the RelaxNG validation stack.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def disassociate_route_table(association_id, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        if conn.disassociate_route_table(association_id):\n            log.info('Route table with association id %s has been disassociated.', association_id)\n            return {'disassociated': True}\n        else:\n            log.warning('Route table with association id %s has not been disassociated.', association_id)\n            return {'disassociated': False}\n    except BotoServerError as e:\n        return {'disassociated': False, 'error': __utils__['boto.get_error'](e)}", "output": "Dissassociates a route table.\n\n    association_id\n        The Route Table Association ID to disassociate\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_vpc.disassociate_route_table 'rtbassoc-d8ccddba'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def time_to_channels(embedded_video):\n  \"\"\"\"\"\"\n  video_shape = common_layers.shape_list(embedded_video)\n  if len(video_shape) != 5:\n    raise ValueError(\"Assuming videos given as tensors in the format \"\n                     \"[batch, time, height, width, channels] but got one \"\n                     \"of shape: %s\" % str(video_shape))\n  transposed = tf.transpose(embedded_video, [0, 2, 3, 1, 4])\n  return tf.reshape(transposed, [\n      video_shape[0], video_shape[2], video_shape[3],\n      video_shape[1] * video_shape[4]\n  ])", "output": "Put time dimension on channels in an embedded video.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare_graph_for_ui(graph, limit_attr_size=1024,\n                         large_attrs_key='_too_large_attrs'):\n  \"\"\"\n  \"\"\"\n  # Check input for validity.\n  if limit_attr_size is not None:\n    if large_attrs_key is None:\n      raise ValueError('large_attrs_key must be != None when limit_attr_size'\n                       '!= None.')\n\n    if limit_attr_size <= 0:\n      raise ValueError('limit_attr_size must be > 0, but is %d' %\n                       limit_attr_size)\n\n  # Filter only if a limit size is defined.\n  if limit_attr_size is not None:\n    for node in graph.node:\n      # Go through all the attributes and filter out ones bigger than the\n      # limit.\n      keys = list(node.attr.keys())\n      for key in keys:\n        size = node.attr[key].ByteSize()\n        if size > limit_attr_size or size < 0:\n          del node.attr[key]\n          # Add the attribute key to the list of \"too large\" attributes.\n          # This is used in the info card in the graph UI to show the user\n          # that some attributes are too large to be shown.\n          node.attr[large_attrs_key].list.s.append(tf.compat.as_bytes(key))", "output": "Prepares (modifies in-place) the graph to be served to the front-end.\n\n  For now, it supports filtering out attributes that are\n  too large to be shown in the graph UI.\n\n  Args:\n    graph: The GraphDef proto message.\n    limit_attr_size: Maximum allowed size in bytes, before the attribute\n        is considered large. Default is 1024 (1KB). Must be > 0 or None.\n        If None, there will be no filtering.\n    large_attrs_key: The attribute key that will be used for storing attributes\n        that are too large. Default is '_too_large_attrs'. Must be != None if\n        `limit_attr_size` is != None.\n\n  Raises:\n    ValueError: If `large_attrs_key is None` while `limit_attr_size != None`.\n    ValueError: If `limit_attr_size` is defined, but <= 0.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare_editable_requirement(\n        self,\n        req,  # type: InstallRequirement\n        require_hashes,  # type: bool\n        use_user_site,  # type: bool\n        finder  # type: PackageFinder\n    ):\n        # type: (...) -> DistAbstraction\n        \"\"\"\n        \"\"\"\n        assert req.editable, \"cannot prepare a non-editable req as editable\"\n\n        logger.info('Obtaining %s', req)\n\n        with indent_log():\n            if require_hashes:\n                raise InstallationError(\n                    'The editable requirement %s cannot be installed when '\n                    'requiring hashes, because there is no single file to '\n                    'hash.' % req\n                )\n            req.ensure_has_source_dir(self.src_dir)\n            req.update_editable(not self._download_should_save)\n\n            abstract_dist = make_abstract_dist(req)\n            with self.req_tracker.track(req):\n                abstract_dist.prep_for_dist(finder, self.build_isolation)\n\n            if self._download_should_save:\n                req.archive(self.download_dir)\n            req.check_if_exists(use_user_site)\n\n        return abstract_dist", "output": "Prepare an editable requirement", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _assert_can_do_op(self, value):\n        \"\"\"\n        \n        \"\"\"\n        if not is_scalar(value):\n            msg = \"'value' must be a scalar, passed: {0}\"\n            raise TypeError(msg.format(type(value).__name__))", "output": "Check value is valid for scalar op.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _try_coerce_result(self, result):\n        \"\"\"  \"\"\"\n\n        # GH12564: CategoricalBlock is 1-dim only\n        # while returned results could be any dim\n        if ((not is_categorical_dtype(result)) and\n                isinstance(result, np.ndarray)):\n            result = _block_shape(result, ndim=self.ndim)\n\n        return result", "output": "reverse of try_coerce_args", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ToDatetime(self):\n    \"\"\"\"\"\"\n    return datetime.utcfromtimestamp(\n        self.seconds + self.nanos / float(_NANOS_PER_SECOND))", "output": "Converts Timestamp to datetime.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def approx_count_distinct(col, rsd=None):\n    \"\"\"\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    if rsd is None:\n        jc = sc._jvm.functions.approx_count_distinct(_to_java_column(col))\n    else:\n        jc = sc._jvm.functions.approx_count_distinct(_to_java_column(col), rsd)\n    return Column(jc)", "output": "Aggregate function: returns a new :class:`Column` for approximate distinct count of\n    column `col`.\n\n    :param rsd: maximum estimation error allowed (default = 0.05). For rsd < 0.01, it is more\n        efficient to use :func:`countDistinct`\n\n    >>> df.agg(approx_count_distinct(df.age).alias('distinct_ages')).collect()\n    [Row(distinct_ages=2)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def newDocNode(self, doc, name, content):\n        \"\"\" \"\"\"\n        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        ret = libxml2mod.xmlNewDocNode(doc__o, self._o, name, content)\n        if ret is None:raise treeError('xmlNewDocNode() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Creation of a new node element within a document. @ns and\n          @content are optional (None). NOTE: @content is supposed to\n          be a piece of XML CDATA, so it allow entities references,\n          but XML special chars need to be escaped first by using\n          xmlEncodeEntitiesReentrant(). Use xmlNewDocRawNode() if you\n           don't need entities support.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def commit(self):\n        \"\"\"\"\"\"\n        for _, save_dir in self._save_dirs.items():\n            save_dir.cleanup()\n        self._moves = []\n        self._save_dirs = {}", "output": "Commits the uninstall by removing stashed files.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def commit(self):\n        \"\"\"\n        \n        \"\"\"\n        assert self.record\n        result = self.files_written, self.dirs_created\n        self._init_record()\n        return result", "output": "Commit recorded changes, turn off recording, return\n        changes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_steps(a, b):\n    \"\"\"\n    \"\"\"\n    if a.step != 1:\n        raise ValueError('a.step must be equal to 1, got: %s' % a.step)\n    if b.step != 1:\n        raise ValueError('b.step must be equal to 1, got: %s' % b.step)", "output": "Check that the steps of ``a`` and ``b`` are both 1.\n\n    Parameters\n    ----------\n    a : range\n        The first range to check.\n    b : range\n        The second range to check.\n\n    Raises\n    ------\n    ValueError\n        Raised when either step is not 1.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def import_(zone, path):\n    '''\n    \n    '''\n    ret = {'status': True}\n\n    # create from file\n    _dump_cfg(path)\n    res = __salt__['cmd.run_all']('zonecfg -z {zone} -f {path}'.format(\n        zone=zone,\n        path=path,\n    ))\n    ret['status'] = res['retcode'] == 0\n    ret['message'] = res['stdout'] if ret['status'] else res['stderr']\n    if ret['message'] == '':\n        del ret['message']\n    else:\n        ret['message'] = _clean_message(ret['message'])\n\n    return ret", "output": "Import the configuration to memory from stable storage.\n\n    zone : string\n        name of zone\n    path : string\n        path of file to export to\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zonecfg.import epyon /zones/epyon.cfg", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expanding(self, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        from pandas.core.window import ExpandingGroupby\n        return ExpandingGroupby(self, *args, **kwargs)", "output": "Return an expanding grouper, providing expanding\n        functionality per group.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_indices(self, names):\n        \"\"\"\n        \n        \"\"\"\n\n        def get_converter(s):\n            # possibly convert to the actual key types\n            # in the indices, could be a Timestamp or a np.datetime64\n            if isinstance(s, (Timestamp, datetime.datetime)):\n                return lambda key: Timestamp(key)\n            elif isinstance(s, np.datetime64):\n                return lambda key: Timestamp(key).asm8\n            else:\n                return lambda key: key\n\n        if len(names) == 0:\n            return []\n\n        if len(self.indices) > 0:\n            index_sample = next(iter(self.indices))\n        else:\n            index_sample = None     # Dummy sample\n\n        name_sample = names[0]\n        if isinstance(index_sample, tuple):\n            if not isinstance(name_sample, tuple):\n                msg = (\"must supply a tuple to get_group with multiple\"\n                       \" grouping keys\")\n                raise ValueError(msg)\n            if not len(name_sample) == len(index_sample):\n                try:\n                    # If the original grouper was a tuple\n                    return [self.indices[name] for name in names]\n                except KeyError:\n                    # turns out it wasn't a tuple\n                    msg = (\"must supply a same-length tuple to get_group\"\n                           \" with multiple grouping keys\")\n                    raise ValueError(msg)\n\n            converters = [get_converter(s) for s in index_sample]\n            names = (tuple(f(n) for f, n in zip(converters, name))\n                     for name in names)\n\n        else:\n            converter = get_converter(index_sample)\n            names = (converter(name) for name in names)\n\n        return [self.indices.get(name, []) for name in names]", "output": "Safe get multiple indices, translate keys for\n        datelike to underlying repr.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_batch_from_datastore(self, class_batch_id):\n    \"\"\"\"\"\"\n    client = self._datastore_client\n    key = client.key(KIND_CLASSIFICATION_BATCH, class_batch_id)\n    result = client.get(key)\n    if result is not None:\n      return dict(result)\n    else:\n      raise KeyError(\n          'Key {0} not found in the datastore'.format(key.flat_path))", "output": "Reads and returns single batch from the datastore.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_dataloader(self):\n        \"\"\"\n        \n        \"\"\"\n\n        input_transform = transforms.Compose([transforms.ToTensor(), \\\n                                             transforms.Normalize((0.7136, 0.4906, 0.3283), \\\n                                                                  (0.1138, 0.1078, 0.0917))])\n        training_dataset = LipsDataset(self.image_path,\n                                       self.align_path,\n                                       mode='train',\n                                       transform=input_transform,\n                                       seq_len=self.seq_len)\n\n        self.train_dataloader = mx.gluon.data.DataLoader(training_dataset,\n                                                         batch_size=self.batch_size,\n                                                         shuffle=True,\n                                                         num_workers=self.num_workers)\n\n        valid_dataset = LipsDataset(self.image_path,\n                                    self.align_path,\n                                    mode='valid',\n                                    transform=input_transform,\n                                    seq_len=self.seq_len)\n\n        self.valid_dataloader = mx.gluon.data.DataLoader(valid_dataset,\n                                                         batch_size=self.batch_size,\n                                                         shuffle=True,\n                                                         num_workers=self.num_workers)", "output": "Description : Setup the dataloader", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_training_data(batch_size):\n    \"\"\" \"\"\"\n    return gluon.data.DataLoader(\n        CIFAR10(train=True, transform=transformer),\n        batch_size=batch_size, shuffle=True, last_batch='discard')", "output": "helper function to get dataloader", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_screen(self, screen, overwrite=False):\n        \"\"\"\n        \n        \"\"\"\n        if self._screen is not None and not overwrite:\n            raise ValueError(\n                \"set_screen() called with overwrite=False and screen already \"\n                \"set.\\n\"\n                \"If you want to apply multiple filters as a screen use \"\n                \"set_screen(filter1 & filter2 & ...).\\n\"\n                \"If you want to replace the previous screen with a new one, \"\n                \"use set_screen(new_filter, overwrite=True).\"\n            )\n        self._screen = screen", "output": "Set a screen on this Pipeline.\n\n        Parameters\n        ----------\n        filter : zipline.pipeline.Filter\n            The filter to apply as a screen.\n        overwrite : bool\n            Whether to overwrite any existing screen.  If overwrite is False\n            and self.screen is not None, we raise an error.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_config_file_value(key, value):\n    \"\"\"\n    \n    \"\"\"\n\n    filename = get_config_file()\n\n    config = _ConfigParser.SafeConfigParser()\n    config.read(filename)\n\n    __section = \"Environment\"\n\n    if not(config.has_section(__section)):\n        config.add_section(__section)\n\n    config.set(__section, key, value)\n\n    with open(filename, 'w') as config_file:\n        config.write(config_file)", "output": "Writes an environment variable configuration to the current\n    config file.  This will be read in on the next restart.\n    The config file is created if not present.\n\n    Note: The variables will not take effect until after restart.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _run_callback(\n        self, callback: Callable, *args: Any, **kwargs: Any\n    ) -> \"Optional[Future[Any]]\":\n        \"\"\"\n        \"\"\"\n        try:\n            result = callback(*args, **kwargs)\n        except Exception:\n            self.handler.log_exception(*sys.exc_info())\n            self._abort()\n            return None\n        else:\n            if result is not None:\n                result = gen.convert_yielded(result)\n                assert self.stream is not None\n                self.stream.io_loop.add_future(result, lambda f: f.result())\n            return result", "output": "Runs the given callback with exception handling.\n\n        If the callback is a coroutine, returns its Future. On error, aborts the\n        websocket connection and returns None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def execute(tgt,\n            fun,\n            arg=(),\n            timeout=None,\n            tgt_type='glob',\n            ret='',\n            jid='',\n            kwarg=None,\n            **kwargs):\n    '''\n    \n    '''\n    client = salt.client.get_local_client(__opts__['conf_file'])\n    try:\n        ret = client.cmd(tgt,\n                         fun,\n                         arg=arg,\n                         timeout=timeout or __opts__['timeout'],\n                         tgt_type=tgt_type,  # no warn_until, as this is introduced only in 2017.7.0\n                         ret=ret,\n                         jid=jid,\n                         kwarg=kwarg,\n                         **kwargs)\n    except SaltClientError as client_error:\n        log.error('Error while executing %s on %s (%s)', fun, tgt, tgt_type)\n        log.error(client_error)\n        return {}\n    return ret", "output": ".. versionadded:: 2017.7.0\n\n    Execute ``fun`` on all minions matched by ``tgt`` and ``tgt_type``.\n    Parameter ``fun`` is the name of execution module function to call.\n\n    This function should mainly be used as a helper for runner modules,\n    in order to avoid redundant code.\n    For example, when inside a runner one needs to execute a certain function\n    on arbitrary groups of minions, only has to:\n\n    .. code-block:: python\n\n        ret1 = __salt__['salt.execute']('*', 'mod.fun')\n        ret2 = __salt__['salt.execute']('my_nodegroup', 'mod2.fun2', tgt_type='nodegroup')\n\n    It can also be used to schedule jobs directly on the master, for example:\n\n    .. code-block:: yaml\n\n        schedule:\n            collect_bgp_stats:\n                function: salt.execute\n                args:\n                    - edge-routers\n                    - bgp.neighbors\n                kwargs:\n                    tgt_type: nodegroup\n                days: 1\n                returner: redis", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absent(name, protocol=None, service_address=None, server_address=None):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    #check if server exists and remove it\n    server_check = __salt__['lvs.check_server'](protocol=protocol,\n                                                service_address=service_address,\n                                                server_address=server_address)\n    if server_check is True:\n        if __opts__['test']:\n            ret['result'] = None\n            ret['comment'] = 'LVS Server {0} in service {1}({2}) is present and needs to be removed'.format(name, service_address, protocol)\n            return ret\n        server_delete = __salt__['lvs.delete_server'](protocol=protocol,\n                                                      service_address=service_address,\n                                                      server_address=server_address)\n        if server_delete is True:\n            ret['comment'] = 'LVS Server {0} in service {1}({2}) has been removed'.format(name, service_address, protocol)\n            ret['changes'][name] = 'Absent'\n            return ret\n        else:\n            ret['comment'] = 'LVS Server {0} in service {1}({2}) removed failed({3})'.format(name, service_address, protocol, server_delete)\n            ret['result'] = False\n            return ret\n    else:\n        ret['comment'] = 'LVS Server {0} in service {1}({2}) is not present, so it cannot be removed'.format(name, service_address, protocol)\n\n    return ret", "output": "Ensure the LVS Real Server in specified service is absent.\n\n    name\n        The name of the LVS server.\n\n    protocol\n        The service protocol(only support ``tcp``, ``udp`` and ``fwmark`` service).\n\n    service_address\n        The LVS service address.\n\n    server_address\n        The LVS real server address.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send(\n        self, record, message, resource=None, labels=None, trace=None, span_id=None\n    ):\n        \"\"\"\n        \"\"\"\n        info = {\"message\": message, \"python_logger\": record.name}\n        self.logger.log_struct(\n            info,\n            severity=record.levelname,\n            resource=resource,\n            labels=labels,\n            trace=trace,\n            span_id=span_id,\n        )", "output": "Overrides transport.send().\n\n        :type record: :class:`logging.LogRecord`\n        :param record: Python log record that the handler was called with.\n\n        :type message: str\n        :param message: The message from the ``LogRecord`` after being\n                        formatted by the associated log formatters.\n\n        :type resource: :class:`~google.cloud.logging.resource.Resource`\n        :param resource: (Optional) Monitored resource of the entry.\n\n        :type labels: dict\n        :param labels: (Optional) Mapping of labels for the entry.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def publish_minions(self):\n        '''\n        \n        '''\n        log.debug('in publish minions')\n        minions = {}\n\n        log.debug('starting loop')\n        for minion, minion_info in six.iteritems(self.minions):\n            log.debug(minion)\n            # log.debug(minion_info)\n            curr_minion = {}\n            curr_minion.update(minion_info)\n            curr_minion.update({'id': minion})\n            minions[minion] = curr_minion\n        log.debug('ended loop')\n        ret = {'minions': minions}\n        self.handler.write_message(\n            salt.utils.json.dumps(ret) + str('\\n\\n'))", "output": "Publishes minions as a list of dicts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transform_array(array, force_list=False, buffers=None):\n    ''' \n\n    '''\n\n    array = convert_datetime_array(array)\n\n    return serialize_array(array, force_list=force_list, buffers=buffers)", "output": "Transform a NumPy arrays into serialized format\n\n    Converts un-serializable dtypes and returns JSON serializable\n    format\n\n    Args:\n        array (np.ndarray) : a NumPy array to be transformed\n        force_list (bool, optional) : whether to only output to standard lists\n            This function can encode some dtypes using a binary encoding, but\n            setting this argument to True will override that and cause only\n            standard Python lists to be emitted. (default: False)\n\n        buffers (set, optional) :\n            If binary buffers are desired, the buffers parameter may be\n            provided, and any columns that may be sent as binary buffers\n            will be added to the set. If None, then only base64 encoding\n            will be used (default: None)\n\n            If force_list is True, then this value will be ignored, and\n            no buffers will be generated.\n\n            **This is an \"out\" parameter**. The values it contains will be\n            modified in-place.\n\n\n    Returns:\n        JSON", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_sync_model_vars_op(self):\n        \"\"\"\n        \n        \"\"\"\n        ops = []\n        for (shadow_v, local_v) in self._shadow_model_vars:\n            ops.append(shadow_v.assign(local_v.read_value()))\n        assert len(ops)\n        return tf.group(*ops, name='sync_{}_model_variables_to_ps'.format(len(ops)))", "output": "Get the op to sync local model_variables to PS.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_sls(mods, saltenv='base', test=None, **kwargs):\n    '''\n    \n    '''\n    __pillar__.update(kwargs.get('pillar', {}))\n    __opts__['grains'] = __grains__\n    opts = salt.utils.state.get_sls_opts(__opts__, **kwargs)\n    if salt.utils.args.test_mode(test=test, **kwargs):\n        opts['test'] = True\n    else:\n        opts['test'] = __opts__.get('test', None)\n    st_ = salt.client.ssh.state.SSHHighState(\n            opts,\n            __pillar__,\n            __salt__,\n            __context__['fileclient'])\n    st_.push_active()\n    mods = _parse_mods(mods)\n    high_data, errors = st_.render_highstate({saltenv: mods})\n    high_data, ext_errors = st_.state.reconcile_extend(high_data)\n    errors += ext_errors\n    errors += st_.state.verify_high(high_data)\n    if errors:\n        return errors\n    high_data, req_in_errors = st_.state.requisite_in(high_data)\n    errors += req_in_errors\n    high_data = st_.state.apply_exclude(high_data)\n    # Verify that the high data is structurally sound\n    if errors:\n        return errors\n    _cleanup_slsmod_high_data(high_data)\n    return high_data", "output": "Display the state data from a specific sls or list of sls files on the\n    master\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.show_sls core,edit.vim dev", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imagetransformer_base_8l_8h_big_cond_dr03_dan_dilated():\n  \"\"\"\"\"\"\n  hparams = imagetransformer_base_8l_8h_big_cond_dr03_dan()\n  hparams.gap_sizes = [0, 16, 64, 0, 16, 64, 128, 0]\n  hparams.dec_attention_type = cia.AttentionType.DILATED\n  hparams.block_length = 128\n  hparams.block_width = 128\n  hparams.add_hparam(\"num_memory_blocks\", 1)\n  return hparams", "output": "Dilated hparams.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def query(action=None, command=None, args=None, method='GET', data=None):\n    '''\n    \n    '''\n    path = config.get_cloud_config_value(\n        'url', get_configured_provider(), __opts__, search_global=False\n    )\n    auth_handler = _HTTPBasicAuthHandler()\n    auth_handler.add_password(\n        realm='Parallels Instance Manager',\n        uri=path,\n        user=config.get_cloud_config_value(\n            'user', get_configured_provider(), __opts__, search_global=False\n        ),\n        passwd=config.get_cloud_config_value(\n            'password', get_configured_provider(), __opts__,\n            search_global=False\n        )\n    )\n    opener = _build_opener(auth_handler)\n    _install_opener(opener)\n\n    if action:\n        path += action\n\n    if command:\n        path += '/{0}'.format(command)\n\n    if not type(args, dict):\n        args = {}\n\n    kwargs = {'data': data}\n    if isinstance(data, six.string_types) and '<?xml' in data:\n        kwargs['headers'] = {\n            'Content-type': 'application/xml',\n        }\n\n    if args:\n        params = _urlencode(args)\n        req = _Request(url='{0}?{1}'.format(path, params), **kwargs)\n    else:\n        req = _Request(url=path, **kwargs)\n\n    req.get_method = lambda: method\n\n    log.debug('%s %s', method, req.get_full_url())\n    if data:\n        log.debug(data)\n\n    try:\n        result = _urlopen(req)\n        log.debug('PARALLELS Response Status Code: %s', result.getcode())\n\n        if 'content-length' in result.headers:\n            content = result.read()\n            result.close()\n            items = ET.fromstring(content)\n            return items\n\n        return {}\n    except URLError as exc:\n        log.error('PARALLELS Response Status Code: %s %s', exc.code, exc.msg)\n        root = ET.fromstring(exc.read())\n        log.error(root)\n        return {'error': root}", "output": "Make a web call to a Parallels provider", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reduce_by_device(parallelism, data, reduce_fn):\n  \"\"\"\n  \"\"\"\n  unique_devices = []\n  device_to_data = {}\n  for dev, datum in zip(parallelism.devices, data):\n    if dev not in device_to_data:\n      unique_devices.append(dev)\n      device_to_data[dev] = [datum]\n    else:\n      device_to_data[dev].append(datum)\n  device_parallelism = Parallelism(unique_devices)\n  grouped_data = [device_to_data[dev] for dev in unique_devices]\n  return device_parallelism, device_parallelism(reduce_fn, grouped_data)", "output": "Reduces data per device.\n\n  This can be useful, for example, if we want to all-reduce n tensors on k<n\n  devices (like during eval when we have only one device).  We call\n  reduce_by_device() to first sum the tensors per device, then call our usual\n  all-reduce operation to create one sum per device, followed by\n  expand_by_device, to create the appropriate number of pointers to these\n  results.  See all_reduce_ring() below for an example of how this is used.\n\n  Args:\n    parallelism: a expert_utils.Parallelism object\n    data: a list of Tensors with length parallelism.n\n    reduce_fn: a function taking a list of Tensors.  e.g. tf.add_n\n\n  Returns:\n    device_parallelism: a Parallelism object with each device listed only once.\n    reduced_data: A list of Tensors, one per device.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_where(w):\n    \"\"\"\n    \n    \"\"\"\n\n    if not (isinstance(w, (Expr, str)) or is_list_like(w)):\n        raise TypeError(\"where must be passed as a string, Expr, \"\n                        \"or list-like of Exprs\")\n\n    return w", "output": "Validate that the where statement is of the right type.\n\n    The type may either be String, Expr, or list-like of Exprs.\n\n    Parameters\n    ----------\n    w : String term expression, Expr, or list-like of Exprs.\n\n    Returns\n    -------\n    where : The original where clause if the check was successful.\n\n    Raises\n    ------\n    TypeError : An invalid data type was passed in for w (e.g. dict).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render_headers(self):\n        \"\"\"\n        \n        \"\"\"\n        lines = []\n\n        sort_keys = ['Content-Disposition', 'Content-Type', 'Content-Location']\n        for sort_key in sort_keys:\n            if self.headers.get(sort_key, False):\n                lines.append('%s: %s' % (sort_key, self.headers[sort_key]))\n\n        for header_name, header_value in self.headers.items():\n            if header_name not in sort_keys:\n                if header_value:\n                    lines.append('%s: %s' % (header_name, header_value))\n\n        lines.append('\\r\\n')\n        return '\\r\\n'.join(lines)", "output": "Renders the headers for this request field.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mod_list(only_persist=False):\n    '''\n    \n    '''\n    mods = set()\n    if only_persist:\n        conf = _get_modules_conf()\n        if os.path.exists(conf):\n            try:\n                with salt.utils.files.fopen(conf, 'r') as modules_file:\n                    for line in modules_file:\n                        line = line.strip()\n                        mod_name = _strip_module_name(line)\n                        if not line.startswith('#') and mod_name:\n                            mods.add(mod_name)\n            except IOError:\n                log.error('kmod module could not open modules file at %s', conf)\n    else:\n        for mod in lsmod():\n            mods.add(mod['module'])\n    return sorted(list(mods))", "output": "Return a list of the loaded module names\n\n    only_persist\n        Only return the list of loaded persistent modules\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' kmod.mod_list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_oneof(self, definitions, field, value):\n        \"\"\"  \"\"\"\n        valids, _errors = \\\n            self.__validate_logical('oneof', definitions, field, value)\n        if valids != 1:\n            self._error(field, errors.ONEOF, _errors,\n                        valids, len(definitions))", "output": "{'type': 'list', 'logical': 'oneof'}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_surrogate_run_config(hp):\n  \"\"\"\n  \"\"\"\n  save_ckpt_steps = max(FLAGS.iterations_per_loop, FLAGS.local_eval_frequency)\n  save_ckpt_secs = FLAGS.save_checkpoints_secs or None\n  if save_ckpt_secs:\n    save_ckpt_steps = None\n  assert FLAGS.surrogate_output_dir\n  # the various custom getters we have written do not play well together yet.\n  # TODO(noam): ask rsepassi for help here.\n  daisy_chain_variables = (\n      hp.daisy_chain_variables and hp.activation_dtype == \"float32\" and\n      hp.weight_dtype == \"float32\")\n  return trainer_lib.create_run_config(\n      model_name=FLAGS.model,\n      model_dir=os.path.expanduser(FLAGS.surrogate_output_dir),\n      master=FLAGS.master,\n      iterations_per_loop=FLAGS.iterations_per_loop,\n      num_shards=FLAGS.tpu_num_shards,\n      log_device_placement=FLAGS.log_device_placement,\n      save_checkpoints_steps=save_ckpt_steps,\n      save_checkpoints_secs=save_ckpt_secs,\n      keep_checkpoint_max=FLAGS.keep_checkpoint_max,\n      keep_checkpoint_every_n_hours=FLAGS.keep_checkpoint_every_n_hours,\n      num_gpus=FLAGS.worker_gpu,\n      gpu_order=FLAGS.gpu_order,\n      num_async_replicas=FLAGS.worker_replicas,\n      gpu_mem_fraction=FLAGS.worker_gpu_memory_fraction,\n      enable_graph_rewriter=FLAGS.enable_graph_rewriter,\n      use_tpu=FLAGS.use_tpu,\n      schedule=FLAGS.schedule,\n      no_data_parallelism=hp.no_data_parallelism,\n      daisy_chain_variables=daisy_chain_variables,\n      ps_replicas=FLAGS.ps_replicas,\n      ps_job=FLAGS.ps_job,\n      ps_gpu=FLAGS.ps_gpu,\n      sync=FLAGS.sync,\n      worker_id=FLAGS.worker_id,\n      worker_job=FLAGS.worker_job,\n      random_seed=FLAGS.random_seed,\n      tpu_infeed_sleep_secs=FLAGS.tpu_infeed_sleep_secs,\n      inter_op_parallelism_threads=FLAGS.inter_op_parallelism_threads,\n      log_step_count_steps=FLAGS.log_step_count_steps,\n      intra_op_parallelism_threads=FLAGS.intra_op_parallelism_threads)", "output": "Create a run config.\n\n  Args:\n    hp: model hyperparameters\n  Returns:\n    a run config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_array(self, key, start=None, stop=None):\n        \"\"\"  \"\"\"\n        import tables\n        node = getattr(self.group, key)\n        attrs = node._v_attrs\n\n        transposed = getattr(attrs, 'transposed', False)\n\n        if isinstance(node, tables.VLArray):\n            ret = node[0][start:stop]\n        else:\n            dtype = getattr(attrs, 'value_type', None)\n            shape = getattr(attrs, 'shape', None)\n\n            if shape is not None:\n                # length 0 axis\n                ret = np.empty(shape, dtype=dtype)\n            else:\n                ret = node[start:stop]\n\n            if dtype == 'datetime64':\n\n                # reconstruct a timezone if indicated\n                ret = _set_tz(ret, getattr(attrs, 'tz', None), coerce=True)\n\n            elif dtype == 'timedelta64':\n                ret = np.asarray(ret, dtype='m8[ns]')\n\n        if transposed:\n            return ret.T\n        else:\n            return ret", "output": "read an array for the specified node (off of group", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _collect_layer_outputs(mod, data, include_layer=None, max_num_examples=None, logger=None):\n    \"\"\"\"\"\"\n    collector = _LayerOutputCollector(include_layer=include_layer, logger=logger)\n    num_examples = _collect_layer_statistics(mod, data, collector, max_num_examples, logger)\n    return collector.nd_dict, num_examples", "output": "Collect layer outputs and save them in a dictionary mapped by layer names.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def destroy(name, call=None):\n    '''\n    \n\n    '''\n    if call == 'function':\n        raise SaltCloudSystemExit(\n            'The destroy action must be called with -d, --destroy, '\n            '-a or --action.'\n        )\n\n    __utils__['cloud.fire_event'](\n        'event',\n        'destroying instance',\n        'salt/cloud/{0}/destroying'.format(name),\n        args={'name': name},\n        sock_dir=__opts__['sock_dir'],\n        transport=__opts__['transport']\n    )\n\n    node = get_node(name)\n    ret = query(command='my/machines/{0}'.format(node['id']),\n                location=node['location'], method='DELETE')\n\n    __utils__['cloud.fire_event'](\n        'event',\n        'destroyed instance',\n        'salt/cloud/{0}/destroyed'.format(name),\n        args={'name': name},\n        sock_dir=__opts__['sock_dir'],\n        transport=__opts__['transport']\n    )\n\n    if __opts__.get('update_cachedir', False) is True:\n        __utils__['cloud.delete_minion_cachedir'](name, __active_provider_name__.split(':')[0], __opts__)\n\n    return ret[0] in VALID_RESPONSE_CODES", "output": "destroy a machine by name\n\n    :param name: name given to the machine\n    :param call: call value in this case is 'action'\n    :return: array of booleans , true if successfully stopped and true if\n             successfully removed\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -d vm_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_delivery(err, msg, obj):\n    \"\"\"\n        \n    \"\"\"\n    if err is not None:\n        print('Message {} delivery failed for user {} with error {}'.format(\n            obj.id, obj.name, err))\n    else:\n        print('Message {} successfully produced to {} [{}] at offset {}'.format(\n            obj.id, msg.topic(), msg.partition(), msg.offset()))", "output": "Handle delivery reports served from producer.poll.\n        This callback takes an extra argument, obj.\n        This allows the original contents to be included for debugging purposes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absent(name, exports='/etc/exports'):\n    '''\n    \n    '''\n\n    path = name\n    ret = {'name': name,\n           'changes': {},\n           'result': None,\n           'comment': ''}\n\n    old = __salt__['nfs3.list_exports'](exports)\n    if path in old:\n        if __opts__['test']:\n            ret['comment'] = 'Export {0} would be removed'.format(path)\n            ret['changes'][path] = old[path]\n            ret['result'] = None\n            return ret\n\n        __salt__['nfs3.del_export'](exports, path)\n        try_reload = __salt__['nfs3.reload_exports']()\n        if not try_reload['result']:\n            ret['comment'] = try_reload['stderr']\n        else:\n            ret['comment'] = 'Export {0} removed'.format(path)\n\n        ret['result'] = try_reload['result']\n        ret['changes'][path] = old[path]\n    else:\n        ret['comment'] = 'Export {0} already absent'.format(path)\n        ret['result'] = True\n\n    return ret", "output": "Ensure that the named path is not exported\n\n    name\n        The export path to remove", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_cwd(self):\n        \"\"\"\n        \n        \"\"\"\n\n        cwd = os.path.dirname(os.path.abspath(self._template_file))\n        if self._docker_volume_basedir:\n            cwd = self._docker_volume_basedir\n\n        return cwd", "output": "Get the working directory. This is usually relative to the directory that contains the template. If a Docker\n        volume location is specified, it takes preference\n\n        All Lambda function code paths are resolved relative to this working directory\n\n        :return string: Working directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_tool_aux(command):\n    \"\"\" \n    \"\"\"\n    assert isinstance(command, basestring)\n    dirname = os.path.dirname(command)\n    if dirname:\n        if os.path.exists(command):\n            return command\n        # Both NT and Cygwin will run .exe files by their unqualified names.\n        elif on_windows() and os.path.exists(command + '.exe'):\n            return command\n        # Only NT will run .bat files by their unqualified names.\n        elif os_name() == 'NT' and os.path.exists(command + '.bat'):\n            return command\n    else:\n        paths = path.programs_path()\n        if path.glob(paths, [command]):\n            return command", "output": "Checks if 'command' can be found either in path\n        or is a full name to an existing file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_cron_date_time(**kwargs):\n    '''\n    \n    '''\n    # Define ranges (except daymonth, as it depends on the month)\n    range_max = {\n        'minute': list(list(range(60))),\n        'hour': list(list(range(24))),\n        'month': list(list(range(1, 13))),\n        'dayweek': list(list(range(7)))\n    }\n\n    ret = {}\n    for param in ('minute', 'hour', 'month', 'dayweek'):\n        value = six.text_type(kwargs.get(param, '1')).lower()\n        if value == 'random':\n            ret[param] = six.text_type(random.sample(range_max[param], 1)[0])\n        elif len(value.split(':')) == 2:\n            cron_range = sorted(value.split(':'))\n            start, end = int(cron_range[0]), int(cron_range[1])\n            ret[param] = six.text_type(random.randint(start, end))\n        else:\n            ret[param] = value\n\n    if ret['month'] in '1 3 5 7 8 10 12'.split():\n        daymonth_max = 31\n    elif ret['month'] in '4 6 9 11'.split():\n        daymonth_max = 30\n    else:\n        # This catches both '2' and '*'\n        daymonth_max = 28\n\n    daymonth = six.text_type(kwargs.get('daymonth', '1')).lower()\n    if daymonth == 'random':\n        ret['daymonth'] = \\\n            six.text_type(random.sample(list(list(range(1, (daymonth_max + 1)))), 1)[0])\n    else:\n        ret['daymonth'] = daymonth\n\n    return ret", "output": "Returns a dict of date/time values to be used in a cron entry", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def suffixes(self):\n        \"\"\"\"\"\"\n        name = self.name\n        if name.endswith('.'):\n            return []\n        name = name.lstrip('.')\n        return ['.' + suffix for suffix in name.split('.')[1:]]", "output": "A list of the final component's suffixes, if any.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_transition (self, input_symbol, state, action=None, next_state=None):\n\n        ''' '''\n\n        if next_state is None:\n            next_state = state\n        self.state_transitions[(input_symbol, state)] = (action, next_state)", "output": "This adds a transition that associates:\n\n                (input_symbol, current_state) --> (action, next_state)\n\n        The action may be set to None in which case the process() method will\n        ignore the action and only set the next_state. The next_state may be\n        set to None in which case the current state will be unchanged.\n\n        You can also set transitions for a list of symbols by using\n        add_transition_list().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _extract_mnist_images(filename, num_images):\n  \"\"\"\n  \"\"\"\n  with gzip.open(filename) as bytestream:\n    bytestream.read(16)\n    buf = bytestream.read(_MNIST_IMAGE_SIZE * _MNIST_IMAGE_SIZE * num_images)\n    data = np.frombuffer(buf, dtype=np.uint8)\n    data = data.reshape(num_images, _MNIST_IMAGE_SIZE, _MNIST_IMAGE_SIZE, 1)\n  return data", "output": "Extract images from an MNIST file into a numpy array.\n\n  Args:\n    filename: The path to an MNIST images file.\n    num_images: The number of images in the file.\n\n  Returns:\n    A numpy array of shape [number_of_images, height, width, channels].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_corresponding_lineno(self, lineno):\n        \"\"\"\n        \"\"\"\n        for template_line, code_line in reversed(self.debug_info):\n            if code_line <= lineno:\n                return template_line\n        return 1", "output": "Return the source line number of a line number in the\n        generated bytecode as they are not in sync.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_cert_to_database(ca_name, cert, cacert_path=None, status='V'):\n    '''\n    \n    '''\n    set_ca_path(cacert_path)\n    ca_dir = '{0}/{1}'.format(cert_base_path(), ca_name)\n    index_file, expire_date, serial_number, subject = _get_basic_info(\n        ca_name,\n        cert,\n        ca_dir)\n\n    index_data = '{0}\\t{1}\\t\\t{2}\\tunknown\\t{3}'.format(\n        status,\n        expire_date,\n        serial_number,\n        subject\n    )\n\n    with salt.utils.files.fopen(index_file, 'a+') as ofile:\n        ofile.write(salt.utils.stringutils.to_str(index_data))", "output": "write out the index.txt database file in the appropriate directory to\n    track certificates\n\n    ca_name\n        name of the CA\n    cert\n        certificate to be recorded", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_images(self, name:str, images:[Tensor])->None:\n        \"\"\n        tag = self.ds_type.name + ' ' + name\n        self.tbwriter.add_image(tag=tag, img_tensor=vutils.make_grid(images, normalize=True), global_step=self.iteration)", "output": "Writes list of images as tensors to Tensorboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_env(self, env):\r\n        \"\"\"\"\"\"\r\n        self.dialog_manager.show(RemoteEnvDialog(env, parent=self))", "output": "Show environment variables.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absent(name, deployment_id, metric_name, api_key=None, profile=\"telemetry\"):\n    '''\n    \n    '''\n    ret = {'name': metric_name, 'result': True, 'comment': '', 'changes': {}}\n\n    is_present = __salt__['telemetry.get_alert_config'](\n        deployment_id, metric_name, api_key, profile)\n\n    if is_present:\n        alert_id = is_present.get('_id')\n        if __opts__['test']:\n            ret['comment'] = 'alert {0} is set to be removed from deployment: {1}.'.format(metric_name, deployment_id)\n            ret['result'] = None\n            return ret\n        deleted, msg = __salt__['telemetry.delete_alarms'](\n            deployment_id, alert_id, is_present.get('condition', {}).get('metric'), api_key, profile)\n\n        if deleted:\n            ret['changes']['old'] = metric_name\n            ret['changes']['new'] = None\n        else:\n            ret['result'] = False\n            ret['comment'] = 'Failed to delete alert {0} from deployment: {1}'.format(metric_name, msg)\n    else:\n        ret['comment'] = 'alarm on {0} does not exist within {1}.'.format(metric_name, deployment_id)\n    return ret", "output": "Ensure the telemetry alert config is deleted\n\n    name\n        An optional description of the alarms (not currently supported by telemetry API)\n\n    deployment_id\n        Specifies the ID of the root deployment resource\n        (replica set cluster or sharded cluster) to which this alert definition is attached\n\n    metric_name\n        Specifies the unique ID of the metric to whose values these thresholds will be applied\n\n    api_key\n        Telemetry api key for the user\n\n    profile\n        A dict with telemetry config data. If present, will be used instead of\n        api_key.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _websocket_mask_python(mask: bytes, data: bytes) -> bytes:\n    \"\"\"\n    \"\"\"\n    mask_arr = array.array(\"B\", mask)\n    unmasked_arr = array.array(\"B\", data)\n    for i in range(len(data)):\n        unmasked_arr[i] = unmasked_arr[i] ^ mask_arr[i % 4]\n    return unmasked_arr.tobytes()", "output": "Websocket masking function.\n\n    `mask` is a `bytes` object of length 4; `data` is a `bytes` object of any length.\n    Returns a `bytes` object of the same length as `data` with the mask applied\n    as specified in section 5.3 of RFC 6455.\n\n    This pure-python implementation may be replaced by an optimized version when available.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert(model, feature_names, target):\n    \"\"\"\n    \"\"\"\n    if not(_HAS_SKLEARN):\n        raise RuntimeError('scikit-learn not found. scikit-learn conversion API is disabled.')\n\n    _sklearn_util.check_expected_type(model, _NuSVR)\n    return _SVR.convert(model, feature_names, target)", "output": "Convert a Nu Support Vector Regression (NuSVR) model to the protobuf spec.\n    Parameters\n    ----------\n    model: NuSVR\n        A trained NuSVR encoder model.\n\n    feature_names: [str]\n        Name of the input columns.\n\n    target: str\n        Name of the output column.\n\n    Returns\n    -------\n    model_spec: An object of type Model_pb.\n        Protobuf representation of the model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_option(self, opts, dest, action=None, nargs=1, const=None,\n                   obj=None):\n        \"\"\"\n        \"\"\"\n        if obj is None:\n            obj = dest\n        opts = [normalize_opt(opt, self.ctx) for opt in opts]\n        option = Option(opts, dest, action=action, nargs=nargs,\n                        const=const, obj=obj)\n        self._opt_prefixes.update(option.prefixes)\n        for opt in option._short_opts:\n            self._short_opt[opt] = option\n        for opt in option._long_opts:\n            self._long_opt[opt] = option", "output": "Adds a new option named `dest` to the parser.  The destination\n        is not inferred (unlike with optparse) and needs to be explicitly\n        provided.  Action can be any of ``store``, ``store_const``,\n        ``append``, ``appnd_const`` or ``count``.\n\n        The `obj` can be used to identify the option in the order list\n        that is returned from the parser.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_from_datastore(self):\n    \"\"\"\n    \"\"\"\n    self._attacks = {}\n    self._targeted_attacks = {}\n    self._defenses = {}\n    for entity in self._datastore_client.query_fetch(kind=KIND_SUBMISSION):\n      submission_id = entity.key.flat_path[-1]\n      submission_path = entity['submission_path']\n      participant_id = {k: entity[k]\n                        for k in ['team_id', 'baseline_id']\n                        if k in entity}\n      submission_descr = SubmissionDescriptor(path=submission_path,\n                                              participant_id=participant_id)\n      if list(entity.key.flat_path[0:2]) == ATTACKS_ENTITY_KEY:\n        self._attacks[submission_id] = submission_descr\n      elif list(entity.key.flat_path[0:2]) == TARGET_ATTACKS_ENTITY_KEY:\n        self._targeted_attacks[submission_id] = submission_descr\n      elif list(entity.key.flat_path[0:2]) == DEFENSES_ENTITY_KEY:\n        self._defenses[submission_id] = submission_descr", "output": "Init list of submission from Datastore.\n\n    Should be called by each worker during initialization.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fibre_channel_wwns():\n    '''\n    \n    '''\n    grains = {'fc_wwn': False}\n    if salt.utils.platform.is_linux():\n        grains['fc_wwn'] = _linux_wwns()\n    elif salt.utils.platform.is_windows():\n        grains['fc_wwn'] = _windows_wwns()\n    return grains", "output": "Return list of fiber channel HBA WWNs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_docker_image_version(layers, runtime):\n        \"\"\"\n        \n        \"\"\"\n\n        # Docker has a concept of a TAG on an image. This is plus the REPOSITORY is a way to determine\n        # a version of the image. We will produced a TAG for a combination of the runtime with the layers\n        # specified in the template. This will allow reuse of the runtime and layers across different\n        # functions that are defined. If two functions use the same runtime with the same layers (in the\n        # same order), SAM CLI will only produce one image and use this image across both functions for invoke.\n        return runtime + '-' + hashlib.sha256(\n            \"-\".join([layer.name for layer in layers]).encode('utf-8')).hexdigest()[0:25]", "output": "Generate the Docker TAG that will be used to create the image\n\n        Parameters\n        ----------\n        layers list(samcli.commands.local.lib.provider.Layer)\n            List of the layers\n\n        runtime str\n            Runtime of the image to create\n\n        Returns\n        -------\n        str\n            String representing the TAG to be attached to the image", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_origin(self, origin):\n        \"\"\"\n        \n        \"\"\"\n\n        mod_opts = self.application.mod_opts\n\n        if mod_opts.get('cors_origin'):\n            return bool(_check_cors_origin(origin, mod_opts['cors_origin']))\n        else:\n            return super(AllEventsHandler, self).check_origin(origin)", "output": "If cors is enabled, check that the origin is allowed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_type(klass, type_url=None):\n    \"\"\"\n    \"\"\"\n    if type_url is None:\n        type_url = _compute_type_url(klass)\n    if type_url in _TYPE_URL_MAP:\n        if _TYPE_URL_MAP[type_url] is not klass:\n            raise ValueError(\"Conflict: %s\" % (_TYPE_URL_MAP[type_url],))\n\n    _TYPE_URL_MAP[type_url] = klass", "output": "Register a klass as the factory for a given type URL.\n\n    :type klass: :class:`type`\n    :param klass: class to be used as a factory for the given type\n\n    :type type_url: str\n    :param type_url: (Optional) URL naming the type. If not provided,\n                     infers the URL from the type descriptor.\n\n    :raises ValueError: if a registration already exists for the URL.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lmx_base():\n  \"\"\"\"\"\"\n  hparams = transformer.transformer_tpu()\n  # sharing is counterproductive when underparameterized\n  hparams.shared_embedding_and_softmax_weights = False\n  # we judge by log-ppl, so label smoothing hurts.\n  hparams.label_smoothing = 0.0\n  # This makes the batch size on GPU the same as on TPU for a packed problem\n  # with sequence length 256.\n  # TODO(noam): fix the mess that is the data reading pipeline.\n  hparams.max_length = 256\n  # larger batch since we only have a decoder\n  hparams.batch_size = 4096\n  # save some memory so we can have a larger model\n  hparams.activation_dtype = \"bfloat16\"\n  return hparams", "output": "Transformer on languagemodel_lm1b32k_packed.  50M Params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sigmoid_cross_entropy_one_hot(logits, labels, weights_fn=None):\n  \"\"\"\n  \"\"\"\n  with tf.variable_scope(\"sigmoid_cross_entropy_one_hot\",\n                         values=[logits, labels]):\n    del weights_fn\n    cross_entropy = tf.losses.sigmoid_cross_entropy(\n        multi_class_labels=labels, logits=logits)\n    return cross_entropy, tf.constant(1.0)", "output": "Calculate sigmoid cross entropy for one-hot lanels and logits.\n\n  Args:\n    logits: Tensor of size [batch-size, o=1, p=1, num-classes]\n    labels: Tensor of size [batch-size, o=1, p=1, num-classes]\n    weights_fn: Function that takes in labels and weighs examples (unused)\n  Returns:\n    cross_entropy (scalar), weights", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self):\n        \"\"\"\n        \"\"\"\n        if len(self.call_queue):\n            return self.apply(lambda x: x).get()\n\n        return ray.get(self.oid)", "output": "Gets the object out of the plasma store.\n\n        Returns:\n            The object from the plasma store.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def feature_info(self):\n    \"\"\"\n    \"\"\"\n    if self._feature_info is not None:\n      return self._feature_info\n\n    assert self._hparams is not None\n\n    hp = self.get_hparams()\n    if self.has_inputs:\n      in_id = hp.input_space_id\n    out_id = hp.target_space_id\n\n    features = collections.defaultdict(FeatureInfo)\n    for feature_name, modality_cls in six.iteritems(hp.modality):\n      finfo = features[feature_name]\n      finfo.modality = modality_cls\n      finfo.vocab_size = hp.vocab_size[feature_name]\n\n    vocabs = hp.vocabulary\n    for name, encoder in six.iteritems(vocabs):\n      features[name].encoder = encoder\n\n    if self.has_inputs:\n      features[\"inputs\"].space_id = in_id\n    features[\"targets\"].space_id = out_id\n\n    self._feature_info = features\n    return features", "output": "Retrieve dict<feature name, FeatureInfo>.\n\n    Must first call Problem.get_hparams or Problem.dataset to have the problem's\n    internal hparams already constructed.\n\n    Returns:\n      dict<feature name, FeatureInfo>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_resampled(self):\n        \"\"\"\"\"\"\n        return self.resample(data=self.buffer_queue.get(),\n                             input_rate=self.input_rate)", "output": "Return a block of audio data resampled to 16000hz, blocking if necessary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def symlink(src, path):\n    '''\n    \n    '''\n    path = os.path.expanduser(path)\n\n    try:\n        if os.path.normpath(os.readlink(path)) == os.path.normpath(src):\n            log.debug('link already in correct state: %s -> %s', path, src)\n            return True\n    except OSError:\n        pass\n\n    if not os.path.isabs(path):\n        raise SaltInvocationError('File path must be absolute.')\n\n    try:\n        os.symlink(src, path)\n        return True\n    except (OSError, IOError):\n        raise CommandExecutionError('Could not create \\'{0}\\''.format(path))\n    return False", "output": "Create a symbolic link (symlink, soft link) to a file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.symlink /path/to/file /path/to/link", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_weights(self, new_weights):\n        \"\"\"\n        \"\"\"\n        self._check_sess()\n        assign_list = [\n            self.assignment_nodes[name] for name in new_weights.keys()\n            if name in self.assignment_nodes\n        ]\n        assert assign_list, (\"No variables in the input matched those in the \"\n                             \"network. Possible cause: Two networks were \"\n                             \"defined in the same TensorFlow graph. To fix \"\n                             \"this, place each network definition in its own \"\n                             \"tf.Graph.\")\n        self.sess.run(\n            assign_list,\n            feed_dict={\n                self.placeholders[name]: value\n                for (name, value) in new_weights.items()\n                if name in self.placeholders\n            })", "output": "Sets the weights to new_weights.\n\n        Note:\n            Can set subsets of variables as well, by only passing in the\n            variables you want to be set.\n\n        Args:\n            new_weights (Dict): Dictionary mapping variable names to their\n                weights.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_offsetlike(arr_or_obj):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(arr_or_obj, ABCDateOffset):\n        return True\n    elif (is_list_like(arr_or_obj) and len(arr_or_obj) and\n          is_object_dtype(arr_or_obj)):\n        return all(isinstance(x, ABCDateOffset) for x in arr_or_obj)\n    return False", "output": "Check if obj or all elements of list-like is DateOffset\n\n    Parameters\n    ----------\n    arr_or_obj : object\n\n    Returns\n    -------\n    boolean\n        Whether the object is a DateOffset or listlike of DatetOffsets\n\n    Examples\n    --------\n    >>> is_offsetlike(pd.DateOffset(days=1))\n    True\n    >>> is_offsetlike('offset')\n    False\n    >>> is_offsetlike([pd.offsets.Minute(4), pd.offsets.MonthEnd()])\n    True\n    >>> is_offsetlike(np.array([pd.DateOffset(months=3), pd.Timestamp.now()]))\n    False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def optional_args(proxy=None):\n    '''\n    \n    '''\n    opt_args = _get_device_grain('optional_args', proxy=proxy) or {}\n    if opt_args and _FORBIDDEN_OPT_ARGS:\n        for arg in _FORBIDDEN_OPT_ARGS:\n            opt_args.pop(arg, None)\n    return {'optional_args': opt_args}", "output": "Return the connection optional args.\n\n    .. note::\n\n        Sensible data will not be returned.\n\n    .. versionadded:: 2017.7.0\n\n    CLI Example - select all devices connecting via port 1234:\n\n    .. code-block:: bash\n\n        salt -G 'optional_args:port:1234' test.ping\n\n    Output:\n\n    .. code-block:: yaml\n\n        device1:\n            True\n        device2:\n            True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _constrain_glob(glob, paths, limit=5):\n    \"\"\"\n    \n    \"\"\"\n\n    def digit_set_wildcard(chars):\n        \"\"\"\n        Makes a wildcard expression for the set, a bit readable, e.g. [1-5].\n        \"\"\"\n        chars = sorted(chars)\n        if len(chars) > 1 and ord(chars[-1]) - ord(chars[0]) == len(chars) - 1:\n            return '[%s-%s]' % (chars[0], chars[-1])\n        else:\n            return '[%s]' % ''.join(chars)\n\n    current = {glob: paths}\n    while True:\n        pos = list(current.keys())[0].find('[0-9]')\n        if pos == -1:\n            # no wildcard expressions left to specialize in the glob\n            return list(current.keys())\n        char_sets = {}\n        for g, p in six.iteritems(current):\n            char_sets[g] = sorted({path[pos] for path in p})\n        if sum(len(s) for s in char_sets.values()) > limit:\n            return [g.replace('[0-9]', digit_set_wildcard(char_sets[g]), 1) for g in current]\n        for g, s in six.iteritems(char_sets):\n            for c in s:\n                new_glob = g.replace('[0-9]', c, 1)\n                new_paths = list(filter(lambda p: p[pos] == c, current[g]))\n                current[new_glob] = new_paths\n            del current[g]", "output": "Tweaks glob into a list of more specific globs that together still cover paths and not too much extra.\n\n    Saves us minutes long listings for long dataset histories.\n\n    Specifically, in this implementation the leftmost occurrences of \"[0-9]\"\n    give rise to a few separate globs that each specialize the expression to\n    digits that actually occur in paths.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bookmark_present(name, snapshot):\n    '''\n    \n\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    ## log configuration\n    log.debug('zfs.bookmark_present::%s::config::snapshot = %s',\n              name, snapshot)\n\n    ## check we have valid snapshot/bookmark name\n    if not __utils__['zfs.is_snapshot'](snapshot):\n        ret['result'] = False\n        ret['comment'] = 'invalid snapshot name: {0}'.format(name)\n        return ret\n\n    if '#' not in name and '/' not in name:\n        ## NOTE: simple snapshot name\n        #        take the snapshot name and replace the snapshot but with the simple name\n        #        e.g. pool/fs@snap + bm --> pool/fs#bm\n        name = '{0}#{1}'.format(snapshot[:snapshot.index('@')], name)\n        ret['name'] = name\n\n    if not __utils__['zfs.is_bookmark'](name):\n        ret['result'] = False\n        ret['comment'] = 'invalid bookmark name: {0}'.format(name)\n        return ret\n\n    ## ensure bookmark exists\n    if not __salt__['zfs.exists'](name, **{'type': 'bookmark'}):\n        ## NOTE: bookmark the snapshot\n        if not __opts__['test']:\n            mod_res = __salt__['zfs.bookmark'](snapshot, name)\n        else:\n            mod_res = OrderedDict([('bookmarked', True)])\n\n        ret['result'] = mod_res['bookmarked']\n        if ret['result']:\n            ret['changes'][name] = snapshot\n            ret['comment'] = '{0} bookmarked as {1}'.format(snapshot, name)\n        else:\n            ret['comment'] = 'failed to bookmark {0}'.format(snapshot)\n            if 'error' in mod_res:\n                ret['comment'] = mod_res['error']\n    else:\n        ## NOTE: bookmark already exists\n        ret['comment'] = 'bookmark is present'\n\n    return ret", "output": "ensure bookmark exists\n\n    name : string\n        name of bookmark\n    snapshot : string\n        name of snapshot", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def node_ls(server=str):\n    '''\n    \n    '''\n    try:\n        salt_return = {}\n        client = docker.APIClient(base_url='unix://var/run/docker.sock')\n        service = client.nodes(filters=({'name': server}))\n        getdata = salt.utils.json.dumps(service)\n        dump = salt.utils.json.loads(getdata)\n        for items in dump:\n            docker_version = items['Description']['Engine']['EngineVersion']\n            platform = items['Description']['Platform']\n            hostnames = items['Description']['Hostname']\n            ids = items['ID']\n            role = items['Spec']['Role']\n            availability = items['Spec']['Availability']\n            status = items['Status']\n            version = items['Version']['Index']\n            salt_return.update({'Docker Version': docker_version,\n                                'Platform': platform,\n                                'Hostname': hostnames,\n                                'ID': ids,\n                                'Roles': role,\n                                'Availability': availability,\n                                'Status': status,\n                                'Version': version})\n    except TypeError:\n        salt_return = {}\n        salt_return.update({'Error': 'The server arg is missing or you not targeting a Manager node?'})\n    return salt_return", "output": "Displays Information about Swarm Nodes with passing in the server\n\n    server\n        The minion/server name\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' swarm.node_ls server=minion1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        \"\"\"\"\"\"\n        onnx_script = os.path.realpath(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"tools/mypy-onnx.py\"))\n        returncode = subprocess.call([sys.executable, onnx_script])\n        sys.exit(returncode)", "output": "Run command.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def request_time(self) -> float:\n        \"\"\"\"\"\"\n        if self._finish_time is None:\n            return time.time() - self._start_time\n        else:\n            return self._finish_time - self._start_time", "output": "Returns the amount of time it took for this request to execute.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _form_loader(self, _):\n        '''\n        \n        '''\n        data = {}\n        for key in self.request.arguments:\n            val = self.get_arguments(key)\n            if len(val) == 1:\n                data[key] = val[0]\n            else:\n                data[key] = val\n        return data", "output": "function to get the data from the urlencoded forms\n        ignore the data passed in and just get the args from wherever they are", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def relaxNGValidatePushElement(self, doc, elem):\n        \"\"\" \"\"\"\n        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        ret = libxml2mod.xmlRelaxNGValidatePushElement(self._o, doc__o, elem__o)\n        return ret", "output": "Push a new element start on the RelaxNG validation stack.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mmatch(expr,\n           delimiter,\n           greedy,\n           search_type,\n           regex_match=False,\n           exact_match=False,\n           opts=None):\n    '''\n    \n    '''\n    if not opts:\n        opts = __opts__\n\n    ckminions = salt.utils.minions.CkMinions(opts)\n\n    return ckminions._check_cache_minions(expr, delimiter, greedy,\n                                          search_type, regex_match=regex_match,\n                                          exact_match=exact_match)", "output": "Helper function to search for minions in master caches\n    If 'greedy' return accepted minions that matched by the condition or absent in the cache.\n    If not 'greedy' return the only minions have cache data and matched by the condition.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def instance_path(cls, project, instance):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/instances/{instance}\",\n            project=project,\n            instance=instance,\n        )", "output": "Return a fully-qualified instance string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_key(k, seed=None):\n    \"\"\"\n    \n    \"\"\"\n\n    def modinv(a, m):\n        \"\"\"calculate the inverse of a mod m\n        that is, find b such that (a * b) % m == 1\"\"\"\n        b = 1\n        while not (a * b) % m == 1:\n            b += 1\n        return b\n\n    def gen_prime(k, seed=None):\n        \"\"\"generate a prime with k bits\"\"\"\n\n        def is_prime(num):\n            if num == 2:\n                return True\n            for i in range(2, int(num ** 0.5) + 1):\n                if num % i == 0:\n                    return False\n            return True\n\n        random.seed(seed)\n        while True:\n            key = random.randrange(int(2 ** (k - 1)), int(2 ** k))\n            if is_prime(key):\n                return key\n\n    # size in bits of p and q need to add up to the size of n\n    p_size = k / 2\n    q_size = k - p_size\n    \n    e = gen_prime(k, seed)  # in many cases, e is also chosen to be a small constant\n    \n    while True:\n        p = gen_prime(p_size, seed)\n        if p % e != 1:\n            break\n    \n    while True:\n        q = gen_prime(q_size, seed)\n        if q % e != 1:\n            break\n    \n    n = p * q\n    l = (p - 1) * (q - 1)  # calculate totient function\n    d = modinv(e, l)\n    \n    return int(n), int(e), int(d)", "output": "the RSA key generating algorithm\n    k is the number of bits in n", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _to_bfloat16_unbiased(x, noise):\n  \"\"\"\n  \"\"\"\n  x_sign = tf.sign(x)\n  # Make sure x is positive.  If it is zero, the two candidates are identical.\n  x = x * x_sign + 1e-30\n  cand1 = tf.to_bfloat16(x)\n  cand1_f = tf.to_float(cand1)\n  # This relies on the fact that for a positive bfloat16 b,\n  # b * 1.005 gives you the next higher bfloat16 and b*0.995 gives you the\n  # next lower one. Both 1.005 and 0.995 are ballpark estimation.\n  cand2 = tf.to_bfloat16(\n      tf.where(tf.greater(x, cand1_f), cand1_f * 1.005, cand1_f * 0.995))\n  ret = _randomized_roundoff_to_bfloat16(x, noise, cand1, cand2)\n  return ret * tf.to_bfloat16(x_sign)", "output": "Convert a float32 to a bfloat16 using randomized roundoff.\n\n  Args:\n    x: A float32 Tensor.\n    noise: a float32 Tensor with values in [0, 1), broadcastable to tf.shape(x)\n  Returns:\n    A float32 Tensor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_ip(addr):\n    '''\n    \n    '''\n\n    try:\n        addr = addr.rsplit('/', 1)\n    except AttributeError:\n        # Non-string passed\n        return False\n\n    if salt.utils.network.is_ipv4(addr[0]):\n        try:\n            if 1 <= int(addr[1]) <= 32:\n                return True\n        except ValueError:\n            # Non-int subnet notation\n            return False\n        except IndexError:\n            # No subnet notation used (i.e. just an IPv4 address)\n            return True\n\n    if salt.utils.network.is_ipv6(addr[0]):\n        try:\n            if 8 <= int(addr[1]) <= 128:\n                return True\n        except ValueError:\n            # Non-int subnet notation\n            return False\n        except IndexError:\n            # No subnet notation used (i.e. just an IPv4 address)\n            return True\n\n    return False", "output": "Check if address is a valid IP. returns True if valid, otherwise False.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt ns1 dig.check_ip 127.0.0.1\n        salt ns1 dig.check_ip 1111:2222:3333:4444:5555:6666:7777:8888", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def debug_str(self):\n        \"\"\"\n        \"\"\"\n        debug_str = ctypes.c_char_p()\n        check_call(_LIB.MXExecutorPrint(\n            self.handle, ctypes.byref(debug_str)))\n        return py_str(debug_str.value)", "output": "Get a debug string about internal execution plan.\n\n        Returns\n        -------\n        debug_str : string\n            Debug string of the executor.\n\n        Examples\n        --------\n        >>> a = mx.sym.Variable('a')\n        >>> b = mx.sym.sin(a)\n        >>> c = 2 * a + b\n        >>> texec = c.bind(mx.cpu(), {'a': mx.nd.array([1,2]), 'b':mx.nd.array([2,3])})\n        >>> print(texec.debug_str())\n        Symbol Outputs:\n\t            output[0]=_plus0(0)\n        Variable:a\n        --------------------\n        Op:_mul_scalar, Name=_mulscalar0\n        Inputs:\n\t        arg[0]=a(0) version=0\n        Attrs:\n\t        scalar=2\n        --------------------\n        Op:sin, Name=sin0\n        Inputs:\n\t        arg[0]=a(0) version=0\n        --------------------\n        Op:elemwise_add, Name=_plus0\n        Inputs:\n\t        arg[0]=_mulscalar0(0)\n\t        arg[1]=sin0(0)\n        Total 0 MB allocated\n        Total 11 TempSpace resource requested", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_probs(self, x):\n    \"\"\"\n    \n    \"\"\"\n    name = self._get_softmax_name()\n\n    return self.get_layer(x, name)", "output": ":param x: A symbolic representation of the network input.\n    :return: A symbolic representation of the probs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_sum(environment, iterable, attribute=None, start=0):\n    \"\"\"\n    \"\"\"\n    if attribute is not None:\n        iterable = imap(make_attrgetter(environment, attribute), iterable)\n    return sum(iterable, start)", "output": "Returns the sum of a sequence of numbers plus the value of parameter\n    'start' (which defaults to 0).  When the sequence is empty it returns\n    start.\n\n    It is also possible to sum up only certain attributes:\n\n    .. sourcecode:: jinja\n\n        Total: {{ items|sum(attribute='price') }}\n\n    .. versionchanged:: 2.6\n       The `attribute` parameter was added to allow suming up over\n       attributes.  Also the `start` parameter was moved on to the right.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def status(Name,\n             region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        trail = conn.get_trail_status(Name=Name)\n        if trail:\n            keys = ('IsLogging', 'LatestDeliveryError', 'LatestNotificationError',\n                    'LatestDeliveryTime', 'LatestNotificationTime',\n                    'StartLoggingTime', 'StopLoggingTime',\n                    'LatestCloudWatchLogsDeliveryError',\n                    'LatestCloudWatchLogsDeliveryTime',\n                    'LatestDigestDeliveryTime', 'LatestDigestDeliveryError',\n                    'LatestDeliveryAttemptTime',\n                    'LatestNotificationAttemptTime',\n                    'LatestNotificationAttemptSucceeded',\n                    'LatestDeliveryAttemptSucceeded',\n                    'TimeLoggingStarted',\n                    'TimeLoggingStopped')\n            return {'trail': dict([(k, trail.get(k)) for k in keys])}\n        else:\n            return {'trail': None}\n    except ClientError as e:\n        err = __utils__['boto3.get_error'](e)\n        if e.response.get('Error', {}).get('Code') == 'TrailNotFoundException':\n            return {'trail': None}\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "Given a trail name describe its properties.\n\n    Returns a dictionary of interesting properties.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_cloudtrail.describe mytrail", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_endpoint(name, tags=None, region=None, key=None, keyid=None,\n                 profile=None):\n    '''\n    \n\n    '''\n    endpoint = False\n    res = __salt__['boto_rds.exists'](name, tags, region, key, keyid,\n                                      profile)\n    if res.get('exists'):\n        try:\n            conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n            if conn:\n                rds = conn.describe_db_instances(DBInstanceIdentifier=name)\n\n                if rds and 'Endpoint' in rds['DBInstances'][0]:\n                    endpoint = rds['DBInstances'][0]['Endpoint']['Address']\n                    return endpoint\n\n        except ClientError as e:\n            return {'error': __utils__['boto3.get_error'](e)}\n\n    return endpoint", "output": "Return the endpoint of an RDS instance.\n\n    CLI example::\n\n        salt myminion boto_rds.get_endpoint myrds", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def modified(self):\n        \"\"\"\n        \"\"\"\n        value = self._proto.last_modified_time\n        if value is not None and value != 0:\n            # value will be in milliseconds.\n            return google.cloud._helpers._datetime_from_microseconds(\n                1000.0 * float(value)\n            )", "output": "Union[datetime.datetime, None]: Datetime at which the model was last\n        modified (:data:`None` until set from the server).\n\n        Read-only.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_dlq(self):\n        \"\"\"\n        \"\"\"\n        # Validate required logical ids\n        valid_dlq_types = str(list(self.dead_letter_queue_policy_actions.keys()))\n        if not self.DeadLetterQueue.get('Type') or not self.DeadLetterQueue.get('TargetArn'):\n            raise InvalidResourceException(self.logical_id,\n                                           \"'DeadLetterQueue' requires Type and TargetArn properties to be specified\"\n                                           .format(valid_dlq_types))\n\n        # Validate required Types\n        if not self.DeadLetterQueue['Type'] in self.dead_letter_queue_policy_actions:\n            raise InvalidResourceException(self.logical_id,\n                                           \"'DeadLetterQueue' requires Type of {}\".format(valid_dlq_types))", "output": "Validates whether the DeadLetterQueue LogicalId is validation\n        :raise: InvalidResourceException", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _VerifyMethodCall(self):\n    \"\"\"\n    \"\"\"\n\n    expected = self._PopNextMethod()\n\n    # Loop here, because we might have a MethodGroup followed by another\n    # group.\n    while isinstance(expected, MethodGroup):\n      expected, method = expected.MethodCalled(self)\n      if method is not None:\n        return method\n\n    # This is a mock method, so just check equality.\n    if expected != self:\n      raise UnexpectedMethodCallError(self, expected)\n\n    return expected", "output": "Verify the called method is expected.\n\n    This can be an ordered method, or part of an unordered set.\n\n    Returns:\n      The expected mock method.\n\n    Raises:\n      UnexpectedMethodCall if the method called was not expected.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cnn_config(arch):\n    \"\"\n    torch.backends.cudnn.benchmark = True\n    return model_meta.get(arch, _default_meta)", "output": "Get the metadata associated with `arch`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def maybe_download_and_extract(dest_directory, cifar_classnum):\n    \"\"\" \"\"\"\n    assert cifar_classnum == 10 or cifar_classnum == 100\n    if cifar_classnum == 10:\n        cifar_foldername = 'cifar-10-batches-py'\n    else:\n        cifar_foldername = 'cifar-100-python'\n    if os.path.isdir(os.path.join(dest_directory, cifar_foldername)):\n        logger.info(\"Found cifar{} data in {}.\".format(cifar_classnum, dest_directory))\n        return\n    else:\n        DATA_URL = DATA_URL_CIFAR_10 if cifar_classnum == 10 else DATA_URL_CIFAR_100\n        filename = DATA_URL[0].split('/')[-1]\n        filepath = os.path.join(dest_directory, filename)\n        download(DATA_URL[0], dest_directory, expect_size=DATA_URL[1])\n        tarfile.open(filepath, 'r:gz').extractall(dest_directory)", "output": "Download and extract the tarball from Alex's website. Copied from tensorflow example", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _axis(self, axis):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        return self.df.columns if axis == 0 else self.df.index", "output": "Return the corresponding labels taking into account the axis.\r\n\r\n        The axis could be horizontal (0) or vertical (1).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def submodel_has_python_callbacks(models):\n    ''' \n\n    '''\n    has_python_callback = False\n    for model in collect_models(models):\n        if len(model._callbacks) > 0 or len(model._event_callbacks) > 0:\n            has_python_callback = True\n            break\n\n    return has_python_callback", "output": "Traverses submodels to check for Python (event) callbacks", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, resource, bucket):\n        \"\"\"\n        \"\"\"\n        instance = cls(bucket)\n        instance.update(resource)\n        return instance", "output": "Factory:  construct instance from resource.\n\n        :type bucket: :class:`Bucket`\n        :params bucket: Bucket for which this instance is the policy.\n\n        :type resource: dict\n        :param resource: mapping as returned from API call.\n\n        :rtype: :class:`IAMConfiguration`\n        :returns: Instance created from resource.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_historical_minute_data(self, ticker: str):\n        \"\"\"\"\"\"\n        start = self._start\n        stop = self._stop\n\n        if len(stop) > 4:\n            stop = stop[:4]\n\n        if len(start) > 4:\n            start = start[:4]\n\n        for year in range(int(start), int(stop) + 1):\n            beg_time = ('%s0101000000' % year)\n            end_time = ('%s1231235959' % year)\n            msg = \"HIT,%s,60,%s,%s,,,,1,,,s\\r\\n\" % (ticker,\n                                                    beg_time,\n                                                    end_time)\n            try:\n                data = iq.iq_query(message=msg)\n                iq.add_data_to_df(data=data)\n            except Exception as err:\n                log.error('No data returned because %s', err)\n\n        try:\n            self.dfdb.write_points(self._ndf, ticker)\n        except InfluxDBClientError as err:\n            log.error('Write to database failed: %s' % err)", "output": "Request historical 5 minute data from DTN.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def partitioning_type(self):\n        \"\"\"\n        \"\"\"\n        warnings.warn(\n            \"This method will be deprecated in future versions. Please use \"\n            \"TableListItem.time_partitioning.type_ instead.\",\n            PendingDeprecationWarning,\n            stacklevel=2,\n        )\n        if self.time_partitioning is not None:\n            return self.time_partitioning.type_", "output": "Union[str, None]: Time partitioning of the table if it is\n        partitioned (Defaults to :data:`None`).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def face_distance(face_encodings, face_to_compare):\n    \"\"\"\n    \n    \"\"\"\n    if len(face_encodings) == 0:\n        return np.empty((0))\n\n    return np.linalg.norm(face_encodings - face_to_compare, axis=1)", "output": "Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\n    for each comparison face. The distance tells you how similar the faces are.\n\n    :param faces: List of face encodings to compare\n    :param face_to_compare: A face encoding to compare against\n    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def saliency_map(output, input, name=\"saliency_map\"):\n    \"\"\"\n    \n    \"\"\"\n    max_outp = tf.reduce_max(output, 1)\n    saliency_op = tf.gradients(max_outp, input)[:][0]\n    return tf.identity(saliency_op, name=name)", "output": "Produce a saliency map as described in the paper:\n    `Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\n    <https://arxiv.org/abs/1312.6034>`_.\n    The saliency map is the gradient of the max element in output w.r.t input.\n\n    Returns:\n        tf.Tensor: the saliency map. Has the same shape as input.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_ip_address_list(list_name):\n    '''\n    \n\n    '''\n    payload = {\"jsonrpc\": \"2.0\",\n               \"id\": \"ID0\",\n               \"method\": \"add_policy_ip_addresses_list\",\n               \"params\": [{\"list_name\": list_name}]}\n\n    response = __proxy__['bluecoat_sslv.call'](payload, True)\n\n    return _validate_change_result(response)", "output": "Retrieves a list of all IP address lists.\n\n    list_name(str): The name of the specific IP address list to add.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bluecoat_sslv.add_ip_address_list MyIPAddressList", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def notify_systemd():\n    '''\n    \n    '''\n    try:\n        import systemd.daemon\n    except ImportError:\n        if salt.utils.path.which('systemd-notify') \\\n                and systemd_notify_call('--booted'):\n            # Notify systemd synchronously\n            notify_socket = os.getenv('NOTIFY_SOCKET')\n            if notify_socket:\n                # Handle abstract namespace socket\n                if notify_socket.startswith('@'):\n                    notify_socket = '\\0{0}'.format(notify_socket[1:])\n                try:\n                    sock = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)\n                    sock.connect(notify_socket)\n                    sock.sendall('READY=1'.encode())\n                    sock.close()\n                except socket.error:\n                    return systemd_notify_call('--ready')\n                return True\n        return False\n\n    if systemd.daemon.booted():\n        try:\n            return systemd.daemon.notify('READY=1')\n        except SystemError:\n            # Daemon was not started by systemd\n            pass", "output": "Notify systemd that this process has started", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _group_changes(cur, wanted, remove=False):\n    '''\n    \n    '''\n    old = set(cur)\n    new = set(wanted)\n    if (remove and old != new) or (not remove and not new.issubset(old)):\n        return True\n    return False", "output": "Determine if the groups need to be changed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sign_message(privkey_path, message, passphrase=None):\n    '''\n    \n    '''\n    key = get_rsa_key(privkey_path, passphrase)\n    log.debug('salt.crypt.sign_message: Signing message.')\n    if HAS_M2:\n        md = EVP.MessageDigest('sha1')\n        md.update(salt.utils.stringutils.to_bytes(message))\n        digest = md.final()\n        return key.sign(digest)\n    else:\n        signer = PKCS1_v1_5.new(key)\n        return signer.sign(SHA.new(salt.utils.stringutils.to_bytes(message)))", "output": "Use Crypto.Signature.PKCS1_v1_5 to sign a message. Returns the signature.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def access_entries(self):\n        \"\"\"\n        \"\"\"\n        entries = self._properties.get(\"access\", [])\n        return [AccessEntry.from_api_repr(entry) for entry in entries]", "output": "List[google.cloud.bigquery.dataset.AccessEntry]: Dataset's access\n        entries.\n\n        ``role`` augments the entity type and must be present **unless** the\n        entity type is ``view``.\n\n        Raises:\n            TypeError: If 'value' is not a sequence\n            ValueError:\n                If any item in the sequence is not an\n                :class:`~google.cloud.bigquery.dataset.AccessEntry`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert(model, input_features, output_features):\n    \"\"\"\n    \"\"\"\n    if not(_HAS_SKLEARN):\n        raise RuntimeError('scikit-learn not found. scikit-learn conversion API is disabled.')\n\n    # Test the scikit-learn model\n    _sklearn_util.check_expected_type(model, StandardScaler)\n    _sklearn_util.check_fitted(model, lambda m: hasattr(m, 'mean_'))\n    _sklearn_util.check_fitted(model, lambda m: hasattr(m, 'scale_'))\n\n    # Set the interface params.\n    spec = _Model_pb2.Model()\n    spec.specificationVersion = SPECIFICATION_VERSION\n    spec = _set_transform_interface_params(spec, input_features, output_features)\n\n    # Set the parameters\n    tr_spec = spec.scaler\n    for x in model.mean_:\n        tr_spec.shiftValue.append(-x)\n\n    for x in model.scale_:\n        tr_spec.scaleValue.append(1.0 / x)\n\n    return _MLModel(spec)", "output": "Convert a _imputer model to the protobuf spec.\n\n    Parameters\n    ----------\n    model: Imputer\n        A trained Imputer model.\n\n    input_features: str\n        Name of the input column.\n\n    output_features: str\n        Name of the output column.\n\n    Returns\n    -------\n    model_spec: An object of type Model_pb.\n        Protobuf representation of the model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vm_detach_nic(name, kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The vm_detach_nic action must be called with -a or --action.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    nic_id = kwargs.get('nic_id', None)\n    if nic_id is None:\n        raise SaltCloudSystemExit(\n            'The vm_detach_nic function requires a \\'nic_id\\' to be provided.'\n        )\n\n    server, user, password = _get_xml_rpc()\n    auth = ':'.join([user, password])\n    vm_id = int(get_vm_id(kwargs={'name': name}))\n    response = server.one.vm.detachnic(auth, vm_id, int(nic_id))\n\n    data = {\n        'action': 'vm.detachnic',\n        'nic_detached': response[0],\n        'vm_id': response[1],\n        'error_code': response[2],\n    }\n\n    return data", "output": "Detaches a disk from a virtual machine.\n\n    .. versionadded:: 2016.3.0\n\n    name\n        The name of the VM from which to detach the network interface.\n\n    nic_id\n        The ID of the nic to detach.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a vm_detach_nic my-vm nic_id=1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_function_versions(FunctionName,\n                           region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        vers = []\n        for ret in __utils__['boto3.paged_call'](conn.list_versions_by_function,\n                                                 FunctionName=FunctionName):\n            vers.extend(ret['Versions'])\n        if not bool(vers):\n            log.warning('No versions found')\n        return {'Versions': vers}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "List the versions available for the given function.\n\n    Returns list of function versions\n\n    CLI Example:\n\n    .. code-block:: yaml\n\n        versions:\n          - {...}\n          - {...}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def present(name, definition=None):\n    '''\n    \n    '''\n\n    ret = {'name': name, 'changes': {}, 'result': True, 'comment': ''}\n\n    try:\n        index_exists = __salt__['elasticsearch.index_exists'](index=name)\n        if not index_exists:\n            if __opts__['test']:\n                ret['comment'] = 'Index {0} does not exist and will be created'.format(name)\n                ret['changes'] = {'new': definition}\n                ret['result'] = None\n            else:\n                output = __salt__['elasticsearch.index_create'](index=name, body=definition)\n                if output:\n                    ret['comment'] = 'Successfully created index {0}'.format(name)\n                    ret['changes'] = {'new': __salt__['elasticsearch.index_get'](index=name)[name]}\n                else:\n                    ret['result'] = False\n                    ret['comment'] = 'Cannot create index {0}, {1}'.format(name, output)\n        else:\n            ret['comment'] = 'Index {0} is already present'.format(name)\n    except Exception as err:\n        ret['result'] = False\n        ret['comment'] = six.text_type(err)\n\n    return ret", "output": ".. versionadded:: 2015.8.0\n    .. versionchanged:: 2017.3.0\n        Marked ``definition`` as optional.\n\n    Ensure that the named index is present.\n\n    name\n        Name of the index to add\n\n    definition\n        Optional dict for creation parameters as per https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-create-index.html\n\n    **Example:**\n\n    .. code-block:: yaml\n\n        # Default settings\n        mytestindex:\n          elasticsearch_index.present\n\n        # Extra settings\n        mytestindex2:\n          elasticsearch_index.present:\n            - definition:\n                settings:\n                  index:\n                    number_of_shards: 10", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_subfeature_of (parent_property, f):\n    \"\"\" \n    \"\"\"\n    if __debug__:\n        from .property import Property\n        assert isinstance(parent_property, Property)\n        assert isinstance(f, Feature)\n\n    if not f.subfeature:\n        return False\n\n    p = f.parent\n    if not p:\n        return False\n\n    parent_feature = p[0]\n    parent_value = p[1]\n\n    if parent_feature != parent_property.feature:\n        return False\n\n    if parent_value and parent_value != parent_property.value:\n        return False\n\n    return True", "output": "Return true iff f is an ordinary subfeature of the parent_property's\n        feature, or if f is a subfeature of the parent_property's feature\n        specific to the parent_property's value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_items(self, maxlevel):\r\n        \"\"\"\"\"\"\r\n        itemlist = []\r\n        def add_to_itemlist(item, maxlevel, level=1):\r\n            level += 1\r\n            for index in range(item.childCount()):\r\n                citem = item.child(index)\r\n                itemlist.append(citem)\r\n                if level <= maxlevel:\r\n                    add_to_itemlist(citem, maxlevel, level)\r\n        for tlitem in self.get_top_level_items():\r\n            itemlist.append(tlitem)\r\n            if maxlevel > 0:\r\n                add_to_itemlist(tlitem, maxlevel=maxlevel)\r\n        return itemlist", "output": "Return all items with a level <= `maxlevel`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pause_trial(self, trial):\n        \"\"\"\n        \"\"\"\n        assert trial.status == Trial.RUNNING, trial.status\n        try:\n            self.save(trial, Checkpoint.MEMORY)\n            self.stop_trial(trial, stop_logger=False)\n            self.set_status(trial, Trial.PAUSED)\n        except Exception:\n            logger.exception(\"Error pausing runner.\")\n            self.set_status(trial, Trial.ERROR)", "output": "Pauses the trial.\n\n        We want to release resources (specifically GPUs) when pausing an\n        experiment. This results in PAUSED state that similar to TERMINATED.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_name(name, safe_chars):\n    '''\n    \n    '''\n    regexp = re.compile('[^{0}]'.format(safe_chars))\n    if regexp.search(name):\n        raise SaltCloudException(\n            '{0} contains characters not supported by this cloud provider. '\n            'Valid characters are: {1}'.format(\n                name, safe_chars\n            )\n        )", "output": "Check whether the specified name contains invalid characters", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_array(cls, arr, index=None, name=None, dtype=None, copy=False,\n                   fastpath=False):\n        \"\"\"\n        \n        \"\"\"\n        warnings.warn(\"'from_array' is deprecated and will be removed in a \"\n                      \"future version. Please use the pd.Series(..) \"\n                      \"constructor instead.\", FutureWarning, stacklevel=2)\n        if isinstance(arr, ABCSparseArray):\n            from pandas.core.sparse.series import SparseSeries\n            cls = SparseSeries\n        return cls(arr, index=index, name=name, dtype=dtype,\n                   copy=copy, fastpath=fastpath)", "output": "Construct Series from array.\n\n        .. deprecated :: 0.23.0\n            Use pd.Series(..) constructor instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stderr(self):\n        \"\"\"\n        \n        \"\"\"\n        stream = self._log_file_handle if self._log_file_handle else osutils.stderr()\n        return StreamWriter(stream, self._is_debugging)", "output": "Returns stream writer for stderr to output Lambda function errors to\n\n        Returns\n        -------\n        samcli.lib.utils.stream_writer.StreamWriter\n            Stream writer for stderr", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_keyschema(self, schema, field, value):\n        \"\"\"  \"\"\"\n        if isinstance(value, Mapping):\n            validator = self._get_child_validator(\n                document_crumb=field,\n                schema_crumb=(field, 'keyschema'),\n                schema=dict(((k, schema) for k in value.keys())))\n            if not validator(dict(((k, k) for k in value.keys())),\n                             normalize=False):\n                self._drop_nodes_from_errorpaths(validator._errors,\n                                                 [], [2, 4])\n                self._error(field, errors.KEYSCHEMA, validator._errors)", "output": "{'type': ['dict', 'string'], 'validator': 'bulk_schema',\n            'forbidden': ['rename', 'rename_handler']}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_connection(self):\n        \"\"\"\n        \n        \"\"\"\n        conn = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n        try:\n            conn.bind(self._agent._get_filename())\n            conn.listen(1)\n            (r, addr) = conn.accept()\n            return r, addr\n        except:\n            raise", "output": "Return a pair of socket object and string address.\n\n        May block!", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_ide_controller_config(ide_controller_label, operation,\n                                 key, bus_number=0):\n    '''\n    \n    '''\n    log.trace('Configuring IDE controller ide_controller_label=%s',\n              ide_controller_label)\n    ide_spec = vim.vm.device.VirtualDeviceSpec()\n    ide_spec.device = vim.vm.device.VirtualIDEController()\n    if operation == 'add':\n        ide_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add\n    if operation == 'edit':\n        ide_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit\n    ide_spec.device.key = key\n    ide_spec.device.busNumber = bus_number\n    if ide_controller_label:\n        ide_spec.device.deviceInfo = vim.Description()\n        ide_spec.device.deviceInfo.label = ide_controller_label\n        ide_spec.device.deviceInfo.summary = ide_controller_label\n    return ide_spec", "output": "Returns a vim.vm.device.VirtualDeviceSpec object specifying to add/edit an\n    IDE controller\n\n    ide_controller_label\n        Controller label of the IDE adapter\n\n    operation\n        Type of operation: add or edit\n\n    key\n        Unique key of the device\n\n    bus_number\n        Device bus number property", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def invoke(self, *args, **kwargs):\n        \n        \"\"\"\n\n        try:\n            command = args[0]\n        except IndexError:\n            raise TypeError('Missing command to invoke.') from None\n\n        arguments = []\n        if command.cog is not None:\n            arguments.append(command.cog)\n\n        arguments.append(self)\n        arguments.extend(args[1:])\n\n        ret = await command.callback(*arguments, **kwargs)\n        return ret", "output": "r\"\"\"|coro|\n\n        Calls a command with the arguments given.\n\n        This is useful if you want to just call the callback that a\n        :class:`.Command` holds internally.\n\n        Note\n        ------\n        You do not pass in the context as it is done for you.\n\n        Warning\n        ---------\n        The first parameter passed **must** be the command being invoked.\n\n        Parameters\n        -----------\n        command: :class:`.Command`\n            A command or subclass of a command that is going to be called.\n        \\*args\n            The arguments to to use.\n        \\*\\*kwargs\n            The keyword arguments to use.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dashboard_absent(\n        name,\n        hosts=None,\n        profile='grafana'):\n    '''\n    \n    '''\n    ret = {'name': name, 'result': None, 'comment': '', 'changes': {}}\n\n    hosts, index = _parse_profile(profile)\n    if not index:\n        raise SaltInvocationError('index is a required key in the profile.')\n\n    exists = __salt__['elasticsearch.exists'](\n        index=index, id=name, doc_type='dashboard', hosts=hosts\n    )\n\n    if exists:\n        if __opts__['test']:\n            ret['comment'] = 'Dashboard {0} is set to be removed.'.format(\n                name\n            )\n            return ret\n        deleted = __salt__['elasticsearch.delete'](\n            index=index, doc_type='dashboard', id=name, hosts=hosts\n        )\n        if deleted:\n            ret['result'] = True\n            ret['changes']['old'] = name\n            ret['changes']['new'] = None\n        else:\n            ret['result'] = False\n            ret['comment'] = 'Failed to delete {0} dashboard.'.format(name)\n    else:\n        ret['result'] = True\n        ret['comment'] = 'Dashboard {0} does not exist.'.format(name)\n\n    return ret", "output": "Ensure the named grafana dashboard is deleted.\n\n    name\n        Name of the grafana dashboard.\n\n    profile\n        A pillar key or dict that contains a list of hosts and an\n        elasticsearch index to use.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def content_check(self, result):\n        '''\n        \n        '''\n        if not isinstance(result, dict):\n            err_msg = 'Malformed state return. Data must be a dictionary type.'\n        elif not isinstance(result.get('changes'), dict):\n            err_msg = \"'Changes' should be a dictionary.\"\n        else:\n            missing = []\n            for val in ['name', 'result', 'changes', 'comment']:\n                if val not in result:\n                    missing.append(val)\n            if missing:\n                err_msg = 'The following keys were not present in the state return: {0}.'.format(', '.join(missing))\n            else:\n                err_msg = None\n\n        if err_msg:\n            raise SaltException(err_msg)\n\n        return result", "output": "Checks for specific types in the state output.\n        Raises an Exception in case particular rule is broken.\n\n        :param result:\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close_positions_order(self):\n        \"\"\"\n        \"\"\"\n\n        order_list = []\n        time = '{} 15:00:00'.format(self.date)\n        if self.running_environment == RUNNING_ENVIRONMENT.TZERO:\n            for code, amount in self.hold_available.iteritems():\n                order = False\n                if amount < 0:\n                    # \u5148\u5356\u51fa\u7684\u5355\u5b50 \u4e70\u5e73\n                    order = self.send_order(\n                        code=code,\n                        price=0,\n                        amount=abs(amount),\n                        time=time,\n                        towards=ORDER_DIRECTION.BUY,\n                        order_model=ORDER_MODEL.CLOSE,\n                        amount_model=AMOUNT_MODEL.BY_AMOUNT,\n                    )\n                elif amount > 0:\n                    # \u5148\u4e70\u5165\u7684\u5355\u5b50, \u5356\u5e73\n                    order = self.send_order(\n                        code=code,\n                        price=0,\n                        amount=abs(amount),\n                        time=time,\n                        towards=ORDER_DIRECTION.SELL,\n                        order_model=ORDER_MODEL.CLOSE,\n                        amount_model=AMOUNT_MODEL.BY_AMOUNT\n                    )\n                if order:\n                    order_list.append(order)\n            return order_list\n        else:\n            raise RuntimeError(\n                'QAACCOUNT with {} environments cannot use this methods'.format(\n                    self.running_environment\n                )\n            )", "output": "\u5e73\u4ed3\u5355\n\n        Raises:\n            RuntimeError -- if ACCOUNT.RUNNING_ENVIRONMENT is NOT TZERO\n\n        Returns:\n            list -- list with order", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def autosave_all(self):\n        \"\"\"\"\"\"\n        for index in range(self.stack.get_stack_count()):\n            self.autosave(index)", "output": "Autosave all opened files.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def debugDumpNode(self, output, depth):\n        \"\"\" \"\"\"\n        libxml2mod.xmlDebugDumpNode(output, self._o, depth)", "output": "Dumps debug information for the element node, it is\n           recursive", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_user(self, username=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_security\", \"user\", username), params=params\n        )", "output": "`<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-user.html>`_\n\n        :arg username: A comma-separated list of usernames", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_subsystem_handler(self, name, handler, *larg, **kwarg):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            self.lock.acquire()\n            self.subsystem_table[name] = (handler, larg, kwarg)\n        finally:\n            self.lock.release()", "output": "Set the handler class for a subsystem in server mode.  If a request\n        for this subsystem is made on an open ssh channel later, this handler\n        will be constructed and called -- see `.SubsystemHandler` for more\n        detailed documentation.\n\n        Any extra parameters (including keyword arguments) are saved and\n        passed to the `.SubsystemHandler` constructor later.\n\n        :param str name: name of the subsystem.\n        :param handler:\n            subclass of `.SubsystemHandler` that handles this subsystem.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_users():\n    '''\n    \n    '''\n    users = {}\n    _username = ''\n\n    for idx in range(1, 17):\n        cmd = __salt__['cmd.run_all']('racadm getconfig -g \\\n                cfgUserAdmin -i {0}'.format(idx))\n\n        if cmd['retcode'] != 0:\n            log.warning('racadm return an exit code \\'%s\\'.', cmd['retcode'])\n\n        for user in cmd['stdout'].splitlines():\n            if not user.startswith('cfg'):\n                continue\n\n            (key, val) = user.split('=')\n\n            if key.startswith('cfgUserAdminUserName'):\n                _username = val.strip()\n\n                if val:\n                    users[_username] = {'index': idx}\n                else:\n                    break\n            else:\n                users[_username].update({key: val})\n\n    return users", "output": "List all DRAC users\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt dell drac.list_users", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _unlock_cache(w_lock):\n    '''\n    \n    '''\n    if not os.path.exists(w_lock):\n        return\n    try:\n        if os.path.isdir(w_lock):\n            os.rmdir(w_lock)\n        elif os.path.isfile(w_lock):\n            os.unlink(w_lock)\n    except (OSError, IOError) as exc:\n        log.trace('Error removing lockfile %s: %s', w_lock, exc)", "output": "Unlock a FS file/dir based lock", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def var(\r\n        self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs\r\n    ):\r\n        \"\"\"\r\n        \"\"\"\r\n        axis = self._get_axis_number(axis) if axis is not None else 0\r\n        if numeric_only is not None and not numeric_only:\r\n            self._validate_dtypes(numeric_only=True)\r\n        return self._reduce_dimension(\r\n            self._query_compiler.var(\r\n                axis=axis,\r\n                skipna=skipna,\r\n                level=level,\r\n                ddof=ddof,\r\n                numeric_only=numeric_only,\r\n                **kwargs\r\n            )\r\n        )", "output": "Computes variance across the DataFrame.\r\n\r\n        Args:\r\n            axis (int): The axis to take the variance on.\r\n            skipna (bool): True to skip NA values, false otherwise.\r\n            ddof (int): degrees of freedom\r\n\r\n        Returns:\r\n            The variance of the DataFrame.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe_vpc_peering_connection(name,\n                                    region=None,\n                                    key=None,\n                                    keyid=None,\n                                    profile=None):\n    '''\n    \n\n    '''\n    conn = _get_conn3(region=region, key=key, keyid=keyid,\n                      profile=profile)\n    return {\n        'VPC-Peerings': _get_peering_connection_ids(name, conn)\n    }", "output": "Returns any VPC peering connection id(s) for the given VPC\n    peering connection name.\n\n    VPC peering connection ids are only returned for connections that\n    are in the ``active``, ``pending-acceptance`` or ``provisioning``\n    state.\n\n    .. versionadded:: 2016.11.0\n\n    :param name: The string name for this VPC peering connection\n    :param region: The aws region to use\n    :param key: Your aws key\n    :param keyid: The key id associated with this aws account\n    :param profile: The profile to use\n    :return: dict\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_vpc.describe_vpc_peering_connection salt-vpc\n        # Specify a region\n        salt myminion boto_vpc.describe_vpc_peering_connection salt-vpc region=us-west-2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def front(self, n):\n        \"\"\"\n        \"\"\"\n        new_dtypes = (\n            self._dtype_cache if self._dtype_cache is None else self._dtype_cache[:n]\n        )\n        # See head for an explanation of the transposed behavior\n        if self._is_transposed:\n            result = self.__constructor__(\n                self.data.transpose().take(0, n).transpose(),\n                self.index,\n                self.columns[:n],\n                new_dtypes,\n            )\n            result._is_transposed = True\n        else:\n            result = self.__constructor__(\n                self.data.take(1, n), self.index, self.columns[:n], new_dtypes\n            )\n        return result", "output": "Returns the first n columns.\n\n        Args:\n            n: Integer containing the number of columns to return.\n\n        Returns:\n            DataManager containing the first n columns of the original DataManager.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_advanced_relu(builder, layer, input_names, output_names, keras_layer):\n    \"\"\"\n    \n    \"\"\"\n    # Get input and output names\n    input_name, output_name = (input_names[0], output_names[0])\n\n    if keras_layer.max_value is None:\n        builder.add_activation(layer, 'RELU', input_name, output_name)\n        return\n\n    # No direct support of RELU with max-activation value - use negate and\n    # clip layers\n    relu_output_name = output_name + '_relu'\n    builder.add_activation(layer, 'RELU', input_name, relu_output_name)\n    # negate it\n    neg_output_name = relu_output_name + '_neg'\n    builder.add_activation(layer+'__neg__', 'LINEAR', relu_output_name,\n            neg_output_name,[-1.0, 0])\n    # apply threshold\n    clip_output_name = relu_output_name + '_clip'\n    builder.add_unary(layer+'__clip__', neg_output_name, clip_output_name,\n            'threshold', alpha = -keras_layer.max_value)\n    # negate it back\n    builder.add_activation(layer+'_neg2', 'LINEAR', clip_output_name,\n            output_name,[-1.0, 0])", "output": "Convert an ReLU layer with maximum value from keras to coreml.\n\n    Parameters\n    ----------\n    keras_layer: layer\n        A keras layer object.\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_string(self, rows: List[Row], column: StringColumn) -> List[str]:\n        \"\"\"\n        \n        \"\"\"\n        return [str(row.values[column.name]) for row in rows if row.values[column.name] is not None]", "output": "Select function takes a list of rows and a column name and returns a list of strings as\n        in cells.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def security_rule_delete(security_rule, security_group, resource_group,\n                         **kwargs):\n    '''\n    \n\n    '''\n    result = False\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        secrule = netconn.security_rules.delete(\n            network_security_group_name=security_group,\n            resource_group_name=resource_group,\n            security_rule_name=security_rule\n        )\n        secrule.wait()\n        result = True\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Delete a security rule within a specified security group.\n\n    :param name: The name of the security rule to delete.\n\n    :param security_group: The network security group containing the\n        security rule.\n\n    :param resource_group: The resource group name assigned to the\n        network security group.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.security_rule_delete testrule1 testnsg testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def connect_post_namespaced_service_proxy_with_path(self, name, namespace, path, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_post_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)\n        else:\n            (data) = self.connect_post_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)\n            return data", "output": "connect POST requests to proxy of Service\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_post_namespaced_service_proxy_with_path(name, namespace, path, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ServiceProxyOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str path: path to the resource (required)\n        :param str path2: Path is the part of URLs that include service endpoints, suffixes, and parameters to use for the current proxy request to service. For example, the whole request URL is http://localhost/api/v1/namespaces/kube-system/services/elasticsearch-logging/_search?q=user:kimchy. Path is _search?q=user:kimchy.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rm_fstab(name, device, config='/etc/fstab'):\n    '''\n    \n    '''\n    modified = False\n\n    if __grains__['kernel'] == 'SunOS':\n        criteria = _vfstab_entry(name=name, device=device)\n    else:\n        criteria = _fstab_entry(name=name, device=device)\n\n    lines = []\n    try:\n        with salt.utils.files.fopen(config, 'r') as ifile:\n            for line in ifile:\n                line = salt.utils.stringutils.to_unicode(line)\n                try:\n                    if criteria.match(line):\n                        modified = True\n                    else:\n                        lines.append(line)\n\n                except _fstab_entry.ParseError:\n                    lines.append(line)\n                except _vfstab_entry.ParseError:\n                    lines.append(line)\n\n    except (IOError, OSError) as exc:\n        msg = \"Couldn't read from {0}: {1}\"\n        raise CommandExecutionError(msg.format(config, exc))\n\n    if modified:\n        try:\n            with salt.utils.files.fopen(config, 'wb') as ofile:\n                ofile.writelines(salt.utils.data.encode(lines))\n        except (IOError, OSError) as exc:\n            msg = \"Couldn't write to {0}: {1}\"\n            raise CommandExecutionError(msg.format(config, exc))\n\n    # Note: not clear why we always return 'True'\n    # --just copying previous behavior at this point...\n    return True", "output": ".. versionchanged:: 2016.3.2\n\n    Remove the mount point from the fstab\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mount.rm_fstab /mnt/foo /dev/sdg", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _register_info(self, server):\n    \"\"\"\n    \"\"\"\n    server_url = urllib.parse.urlparse(server.get_url())\n    info = manager.TensorBoardInfo(\n        version=version.VERSION,\n        start_time=int(time.time()),\n        port=server_url.port,\n        pid=os.getpid(),\n        path_prefix=self.flags.path_prefix,\n        logdir=self.flags.logdir,\n        db=self.flags.db,\n        cache_key=self.cache_key,\n    )\n    atexit.register(manager.remove_info_file)\n    manager.write_info_file(info)", "output": "Write a TensorBoardInfo file and arrange for its cleanup.\n\n    Args:\n      server: The result of `self._make_server()`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write(self, frames):\n        \"\"\"\n        \n        \"\"\"\n        with HDFStore(self._path, 'w',\n                      complevel=self._complevel, complib=self._complib) \\\n                as store:\n            panel = pd.Panel.from_dict(dict(frames))\n            panel.to_hdf(store, 'updates')\n        with tables.open_file(self._path, mode='r+') as h5file:\n            h5file.set_node_attr('/', 'version', 0)", "output": "Write the frames to the target HDF5 file, using the format used by\n        ``pd.Panel.to_hdf``\n\n        Parameters\n        ----------\n        frames : iter[(int, DataFrame)] or dict[int -> DataFrame]\n            An iterable or other mapping of sid to the corresponding OHLCV\n            pricing data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_images(input_dir, batch_shape):\n  \"\"\"\n  \"\"\"\n  images = np.zeros(batch_shape)\n  filenames = []\n  idx = 0\n  batch_size = batch_shape[0]\n  for filepath in tf.gfile.Glob(os.path.join(input_dir, '*.png')):\n    with tf.gfile.Open(filepath) as f:\n      image = np.array(Image.open(f).convert('RGB')).astype(np.float) / 255.0\n    # Images for inception classifier are normalized to be in [-1, 1] interval.\n    images[idx, :, :, :] = image * 2.0 - 1.0\n    filenames.append(os.path.basename(filepath))\n    idx += 1\n    if idx == batch_size:\n      yield filenames, images\n      filenames = []\n      images = np.zeros(batch_shape)\n      idx = 0\n  if idx > 0:\n    yield filenames, images", "output": "Read png images from input directory in batches.\n\n  Args:\n    input_dir: input directory\n    batch_shape: shape of minibatch array, i.e. [batch_size, height, width, 3]\n\n  Yields:\n    filenames: list file names without path of each image\n      Lenght of this list could be less than batch_size, in this case only\n      first few images of the result are elements of the minibatch.\n    images: array with all images from this batch", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy_on_s3(self, src_file_name, dst_file_name, bucket_name):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            self.s3_client.head_bucket(Bucket=bucket_name)\n        except botocore.exceptions.ClientError as e:  # pragma: no cover\n            # If a client error is thrown, then check that it was a 404 error.\n            # If it was a 404 error, then the bucket does not exist.\n            error_code = int(e.response['Error']['Code'])\n            if error_code == 404:\n                return False\n\n        copy_src = {\n            \"Bucket\": bucket_name,\n            \"Key\": src_file_name\n        }\n        try:\n            self.s3_client.copy(\n                CopySource=copy_src,\n                Bucket=bucket_name,\n                Key=dst_file_name\n            )\n            return True\n        except botocore.exceptions.ClientError:  # pragma: no cover\n            return False", "output": "Copies src file to destination within a bucket.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validateElement(self, ctxt, elem):\n        \"\"\" \"\"\"\n        if ctxt is None: ctxt__o = None\n        else: ctxt__o = ctxt._o\n        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        ret = libxml2mod.xmlValidateElement(ctxt__o, self._o, elem__o)\n        return ret", "output": "Try to validate the subtree under an element", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __expand_subfeatures_aux (property_, dont_validate = False):\n    \"\"\" \n    \"\"\"\n    from . import property  # no __debug__ since Property is used elsewhere\n    assert isinstance(property_, property.Property)\n    assert isinstance(dont_validate, int)  # matches bools\n\n    f = property_.feature\n    v = property_.value\n    if not dont_validate:\n        validate_value_string(f, v)\n\n    components = v.split (\"-\")\n\n    v = components[0]\n\n    result = [property.Property(f, components[0])]\n\n    subvalues = components[1:]\n\n    while len(subvalues) > 0:\n        subvalue = subvalues [0]    # pop the head off of subvalues\n        subvalues = subvalues [1:]\n\n        subfeature = __find_implied_subfeature (f, subvalue, v)\n\n        # If no subfeature was found, reconstitute the value string and use that\n        if not subfeature:\n            return [property.Property(f, '-'.join(components))]\n\n        result.append(property.Property(subfeature, subvalue))\n\n    return result", "output": "Helper for expand_subfeatures.\n        Given a feature and value, or just a value corresponding to an\n        implicit feature, returns a property set consisting of all component\n        subfeatures and their values. For example:\n\n          expand_subfeatures <toolset>gcc-2.95.2-linux-x86\n              -> <toolset>gcc <toolset-version>2.95.2 <toolset-os>linux <toolset-cpu>x86\n          equivalent to:\n              expand_subfeatures gcc-2.95.2-linux-x86\n\n        feature:        The name of the feature, or empty if value corresponds to an implicit property\n        value:          The value of the feature.\n        dont_validate:  If True, no validation of value string will be done.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_id(name=None, tags=None, region=None, key=None,\n           keyid=None, profile=None, in_states=None, filters=None):\n\n    '''\n    \n\n    '''\n    instance_ids = find_instances(name=name, tags=tags, region=region, key=key,\n                                  keyid=keyid, profile=profile, in_states=in_states,\n                                  filters=filters)\n    if instance_ids:\n        log.info(\"Instance ids: %s\", \" \".join(instance_ids))\n        if len(instance_ids) == 1:\n            return instance_ids[0]\n        else:\n            raise CommandExecutionError('Found more than one instance '\n                                        'matching the criteria.')\n    else:\n        log.warning('Could not find instance.')\n        return None", "output": "Given instance properties, return the instance id if it exists.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_ec2.get_id myinstance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_auto_login():\n    '''\n    \n    '''\n    cmd = ['defaults',\n           'read',\n           '/Library/Preferences/com.apple.loginwindow.plist',\n           'autoLoginUser']\n    ret = __salt__['cmd.run_all'](cmd, ignore_retcode=True)\n    return False if ret['retcode'] else ret['stdout']", "output": ".. versionadded:: 2016.3.0\n\n    Gets the current setting for Auto Login\n\n    :return: If enabled, returns the user name, otherwise returns False\n    :rtype: str, bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.get_auto_login", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _api_post(path, data, server=None):\n    '''\n    \n    '''\n    server = _get_server(server)\n    response = requests.post(\n            url=_get_url(server['ssl'], server['url'], server['port'], path),\n            auth=_get_auth(server['user'], server['password']),\n            headers=_get_headers(),\n            data=salt.utils.json.dumps(data),\n            verify=False\n    )\n    return _api_response(response)", "output": "Do a POST request to the API", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def isFlexible(self):\n        \"\"\"\n        \n        \"\"\"\n        for key, value in self.arrayShapeRange.items():\n            if key in _CONSTRAINED_KEYS:\n                if value.isFlexible:\n                    return True\n\n        return False", "output": "Returns true if any one of the channel, height, or width ranges of this shape allow more than one input value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def translate_poco_step(self, step):\n        \"\"\"\n        \n\n        \"\"\"\n        ret = {}\n        prev_step = self._steps[-1]\n        if prev_step:\n            ret.update(prev_step)\n        ret['type'] = step[1].get(\"name\", \"\")\n        if step.get('trace'):\n            ret['trace'] = step['trace']\n            ret['traceback'] = step.get('traceback')\n        if ret['type'] == 'touch':\n            # \u53d6\u51fa\u70b9\u51fb\u4f4d\u7f6e\n            if step[1]['args'] and len(step[1]['args'][0]) == 2:\n                pos = step[1]['args'][0]\n                ret['target_pos'] = [int(pos[0]), int(pos[1])]\n                ret['top'] = ret['target_pos'][1]\n                ret['left'] = ret['target_pos'][0]\n        elif ret['type'] == 'swipe':\n            if step[1]['args'] and len(step[1]['args'][0]) == 2:\n                pos = step[1]['args'][0]\n                ret['target_pos'] = [int(pos[0]), int(pos[1])]\n                ret['top'] = ret['target_pos'][1]\n                ret['left'] = ret['target_pos'][0]\n            # swipe \u9700\u8981\u663e\u793a\u4e00\u4e2a\u65b9\u5411\n            vector = step[1][\"kwargs\"].get(\"vector\")\n            if vector:\n                ret['swipe'] = self.dis_vector(vector)\n                ret['vector'] = vector\n\n        ret['desc'] = self.func_desc_poco(ret)\n        ret['title'] = self._translate_title(ret)\n        return ret", "output": "\u5904\u7406poco\u7684\u76f8\u5173\u64cd\u4f5c\uff0c\u53c2\u6570\u4e0eairtest\u7684\u4e0d\u540c\uff0c\u7531\u4e00\u4e2a\u622a\u56fe\u548c\u4e00\u4e2a\u64cd\u4f5c\u6784\u6210\uff0c\u9700\u8981\u5408\u6210\u4e00\u4e2a\u6b65\u9aa4\n        Parameters\n        ----------\n        step \u4e00\u4e2a\u5b8c\u6574\u7684\u64cd\u4f5c\uff0c\u5982click\n        prev_step \u524d\u4e00\u4e2a\u6b65\u9aa4\uff0c\u5e94\u8be5\u662f\u622a\u56fe\n\n        Returns\n        -------", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compare_networks(self, other):\n        \"\"\"\n\n        \"\"\"\n        # does this need to raise a ValueError?\n        if self._version != other._version:\n            raise TypeError('%s and %s are not of the same type' % (\n                            self, other))\n        # self._version == other._version below here:\n        if self.network_address < other.network_address:\n            return -1\n        if self.network_address > other.network_address:\n            return 1\n        # self.network_address == other.network_address below here:\n        if self.netmask < other.netmask:\n            return -1\n        if self.netmask > other.netmask:\n            return 1\n        return 0", "output": "Compare two IP objects.\n\n        This is only concerned about the comparison of the integer\n        representation of the network addresses.  This means that the\n        host bits aren't considered at all in this method.  If you want\n        to compare host bits, you can easily enough do a\n        'HostA._ip < HostB._ip'\n\n        Args:\n            other: An IP object.\n\n        Returns:\n            If the IP versions of self and other are the same, returns:\n\n            -1 if self < other:\n              eg: IPv4Network('192.0.2.0/25') < IPv4Network('192.0.2.128/25')\n              IPv6Network('2001:db8::1000/124') <\n                  IPv6Network('2001:db8::2000/124')\n            0 if self == other\n              eg: IPv4Network('192.0.2.0/24') == IPv4Network('192.0.2.0/24')\n              IPv6Network('2001:db8::1000/124') ==\n                  IPv6Network('2001:db8::1000/124')\n            1 if self > other\n              eg: IPv4Network('192.0.2.128/25') > IPv4Network('192.0.2.0/25')\n                  IPv6Network('2001:db8::2000/124') >\n                      IPv6Network('2001:db8::1000/124')\n\n          Raises:\n              TypeError if the IP versions are different.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset_state(self):\n        \"\"\"  \"\"\"\n        super(AugmentorList, self).reset_state()\n        for a in self.augmentors:\n            a.reset_state()", "output": "Will reset state of each augmentor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_data(self):\n        \"\"\"\n        \"\"\"\n        if self._stype != 'default':\n            raise RuntimeError(\"Cannot return copies of Parameter '%s' on all contexts via \" \\\n                               \"list_data() because its storage type is %s. Please use \" \\\n                               \"row_sparse_data() instead.\" % (self.name, self._stype))\n        return self._check_and_get(self._data, list)", "output": "Returns copies of this parameter on all contexts, in the same order\n        as creation. For sparse parameters, use :py:meth:`Parameter.list_row_sparse_data`\n        instead.\n\n        Returns\n        -------\n        list of NDArrays", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def collapse_shape(shape, a, b):\n  \"\"\"\n  \"\"\"\n  shape = list(shape)\n  if a < 0:\n    n_pad = -a\n    pad = n_pad * [1]\n    return collapse_shape(pad + shape, a + n_pad, b + n_pad)\n  if b > len(shape):\n    n_pad = b - len(shape)\n    pad = n_pad * [1]\n    return collapse_shape(shape + pad, a, b)\n  return [product(shape[:a])] + shape[a:b] + [product(shape[b:])]", "output": "Collapse `shape` outside the interval (`a`,`b`).\n\n  This function collapses `shape` outside the interval (`a`,`b`) by\n  multiplying the dimensions before `a` into a single dimension,\n  and mutliplying the dimensions after `b` into a single dimension.\n\n  Args:\n    shape: a tensor shape\n    a: integer, position in shape\n    b: integer, position in shape\n\n  Returns:\n    The collapsed shape, represented as a list.\n\n  Examples:\n    [1, 2, 3, 4, 5], (a=0, b=2) => [1, 1, 2, 60]\n    [1, 2, 3, 4, 5], (a=1, b=3) => [1, 2, 3, 20]\n    [1, 2, 3, 4, 5], (a=2, b=4) => [2, 3, 4, 5 ]\n    [1, 2, 3, 4, 5], (a=3, b=5) => [6, 4, 5, 1 ]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dependencies_from_wheel_cache(ireq):\n    \"\"\"\n    \"\"\"\n\n    if ireq.editable or not is_pinned_requirement(ireq):\n        return\n    matches = WHEEL_CACHE.get(ireq.link, name_from_req(ireq.req))\n    if matches:\n        matches = set(matches)\n        if not DEPENDENCY_CACHE.get(ireq):\n            DEPENDENCY_CACHE[ireq] = [format_requirement(m) for m in matches]\n        return matches\n    return", "output": "Retrieves dependencies for the given install requirement from the wheel cache.\n\n    :param ireq: A single InstallRequirement\n    :type ireq: :class:`~pip._internal.req.req_install.InstallRequirement`\n    :return: A set of dependency lines for generating new InstallRequirements.\n    :rtype: set(str) or None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_panes_settings(self):\r\n        \"\"\"\"\"\"\r\n        for plugin in (self.widgetlist + self.thirdparty_plugins):\r\n            features = plugin.FEATURES\r\n            if CONF.get('main', 'vertical_dockwidget_titlebars'):\r\n                features = features | QDockWidget.DockWidgetVerticalTitleBar\r\n            plugin.dockwidget.setFeatures(features)\r\n            plugin.update_margins()", "output": "Update dockwidgets features settings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _compare_acl(current, desired, region, key, keyid, profile):\n    '''\n    \n    '''\n    ocid = _get_canonical_id(region, key, keyid, profile)\n    return __utils__['boto3.json_objs_equal'](current, _acl_to_grant(desired, ocid))", "output": "ACLs can be specified using macro-style names that get expanded to\n    something more complex. There's no predictable way to reverse it.\n    So expand all syntactic sugar in our input, and compare against that\n    rather than the input itself.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_util_get_pre_trade_date(cursor_date, n=1):\n    \"\"\"\n    \n    \"\"\"\n\n    cursor_date = QA_util_format_date2str(cursor_date)\n    if cursor_date in trade_date_sse:\n        return QA_util_date_gap(cursor_date, n, \"lt\")\n    real_aft_trade_date = QA_util_get_real_date(cursor_date)\n    return QA_util_date_gap(real_aft_trade_date, n, \"lt\")", "output": "\u5f97\u5230\u524d n \u4e2a\u4ea4\u6613\u65e5 (\u4e0d\u5305\u542b\u5f53\u524d\u4ea4\u6613\u65e5)\n    :param date:\n    :param n:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def removed(name,\n            user=None,\n            env=None):\n    '''\n    \n    '''\n\n    ret = {'name': name, 'result': None, 'comment': '', 'changes': {}}\n\n    try:\n        installed_pkgs = __salt__['cabal.list'](\n            user=user, installed=True, env=env)\n    except (CommandNotFoundError, CommandExecutionError) as err:\n        ret['result'] = False\n        ret['comment'] = 'Error looking up \\'{0}\\': {1}'.format(name, err)\n\n    if name not in installed_pkgs:\n        ret['result'] = True\n        ret['comment'] = 'Package \\'{0}\\' is not installed'.format(name)\n        return ret\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Package \\'{0}\\' is set to be removed'.format(name)\n        return ret\n\n    if __salt__['cabal.uninstall'](pkg=name, user=user, env=env):\n        ret['result'] = True\n        ret['changes'][name] = 'Removed'\n        ret['comment'] = 'Package \\'{0}\\' was successfully removed'.format(name)\n    else:\n        ret['result'] = False\n        ret['comment'] = 'Error removing package \\'{0}\\''.format(name)\n\n    return ret", "output": "Verify that given package is not installed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def finish(self) -> None:\n        \"\"\"\"\"\"\n        if (\n            self._expected_content_remaining is not None\n            and self._expected_content_remaining != 0\n            and not self.stream.closed()\n        ):\n            self.stream.close()\n            raise httputil.HTTPOutputError(\n                \"Tried to write %d bytes less than Content-Length\"\n                % self._expected_content_remaining\n            )\n        if self._chunking_output:\n            if not self.stream.closed():\n                self._pending_write = self.stream.write(b\"0\\r\\n\\r\\n\")\n                self._pending_write.add_done_callback(self._on_write_complete)\n        self._write_finished = True\n        # If the app finished the request while we're still reading,\n        # divert any remaining data away from the delegate and\n        # close the connection when we're done sending our response.\n        # Closing the connection is the only way to avoid reading the\n        # whole input body.\n        if not self._read_finished:\n            self._disconnect_on_finish = True\n        # No more data is coming, so instruct TCP to send any remaining\n        # data immediately instead of waiting for a full packet or ack.\n        self.stream.set_nodelay(True)\n        if self._pending_write is None:\n            self._finish_request(None)\n        else:\n            future_add_done_callback(self._pending_write, self._finish_request)", "output": "Implements `.HTTPConnection.finish`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def change_format(self):\r\n        \"\"\"\"\"\"\r\n        format, valid = QInputDialog.getText(self, _( 'Format'),\r\n                                 _( \"Float formatting\"),\r\n                                 QLineEdit.Normal, self.model.get_format())\r\n        if valid:\r\n            format = str(format)\r\n            try:\r\n                format % 1.1\r\n            except:\r\n                QMessageBox.critical(self, _(\"Error\"),\r\n                                     _(\"Format (%s) is incorrect\") % format)\r\n                return\r\n            self.model.set_format(format)", "output": "Change display format", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def oldest_frame(self, raw=False):\n        \"\"\"\n        \n        \"\"\"\n        if raw:\n            return self.buffer.values[:, self._oldest_frame_idx(), :]\n        return self.buffer.iloc[:, self._oldest_frame_idx(), :]", "output": "Get the oldest frame in the panel.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_top(queue=False, **kwargs):\n    '''\n    \n    '''\n    if 'env' in kwargs:\n        # \"env\" is not supported; Use \"saltenv\".\n        kwargs.pop('env')\n\n    conflict = _check_queue(queue, kwargs)\n    if conflict is not None:\n        return conflict\n\n    opts = salt.utils.state.get_sls_opts(__opts__, **kwargs)\n    try:\n        st_ = salt.state.HighState(opts,\n                                   proxy=__proxy__,\n                                   initial_pillar=_get_initial_pillar(opts))\n    except NameError:\n        st_ = salt.state.HighState(opts, initial_pillar=_get_initial_pillar(opts))\n\n    errors = _get_pillar_errors(kwargs, pillar=st_.opts['pillar'])\n    if errors:\n        __context__['retcode'] = salt.defaults.exitcodes.EX_PILLAR_FAILURE\n        raise CommandExecutionError('Pillar failed to render', info=errors)\n\n    errors = []\n    top_ = st_.get_top()\n    errors += st_.verify_tops(top_)\n    if errors:\n        __context__['retcode'] = salt.defaults.exitcodes.EX_STATE_COMPILER_ERROR\n        return errors\n    matches = st_.top_matches(top_)\n    return matches", "output": "Return the top data that the minion will use for a highstate\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.show_top", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lsp_server_ready(self, language, configuration):\r\n        \"\"\"\"\"\"\r\n        for editorstack in self.editorstacks:\r\n            editorstack.notify_server_ready(language, configuration)", "output": "Notify all stackeditors about LSP server availability.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_config(self):\n        \"\"\"\n        \"\"\"\n        config = super(BoltzmannQPolicy, self).get_config()\n        config['tau'] = self.tau\n        config['clip'] = self.clip\n        return config", "output": "Return configurations of BoltzmannQPolicy\n\n        # Returns\n            Dict of config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _diff(state_data, resource_object):\n    '''\n    '''\n    objects_differ = None\n\n    for k, v in state_data['service'].items():\n        if k == 'escalation_policy_id':\n            resource_value = resource_object['escalation_policy']['id']\n        elif k == 'service_key':\n            # service_key on create must 'foo' but the GET will return 'foo@bar.pagerduty.com'\n            resource_value = resource_object['service_key']\n            if '@' in resource_value:\n                resource_value = resource_value[0:resource_value.find('@')]\n        else:\n            resource_value = resource_object[k]\n        if v != resource_value:\n            objects_differ = '{0} {1} {2}'.format(k, v, resource_value)\n            break\n\n    if objects_differ:\n        return state_data\n    else:\n        return {}", "output": "helper method to compare salt state info with the PagerDuty API json structure,\n    and determine if we need to update.\n\n    returns the dict to pass to the PD API to perform the update, or empty dict if no update.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_write(self, frame, node=None):\n        \"\"\"\"\"\"\n        if frame.buffer is None:\n            self.writeline('yield ', node)\n        else:\n            self.writeline('%s.append(' % frame.buffer, node)", "output": "Yield or write into the frame buffer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(cwd, targets=None, user=None, username=None, password=None, *opts):\n    '''\n    \n    '''\n    if targets:\n        opts += tuple(salt.utils.args.shlex_split(targets))\n    return _run_svn('update', cwd, user, username, password, opts)", "output": "Update the current directory, files, or directories from\n    the remote Subversion repository\n\n    cwd\n        The path to the Subversion repository\n\n    targets : None\n        files and directories to pass to the command as arguments\n        Default: svn uses '.'\n\n    user : None\n        Run svn as a user other than what the minion runs as\n\n    password : None\n        Connect to the Subversion server with this password\n\n        .. versionadded:: 0.17.0\n\n    username : None\n        Connect to the Subversion server as another user\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' svn.update /path/to/repo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def move_tab(self, index_from, index_to):\r\n        \"\"\"\"\"\"\r\n        self.move_data.emit(index_from, index_to)\r\n\r\n        tip, text = self.tabToolTip(index_from), self.tabText(index_from)\r\n        icon, widget = self.tabIcon(index_from), self.widget(index_from)\r\n        current_widget = self.currentWidget()\r\n        \r\n        self.removeTab(index_from)\r\n        self.insertTab(index_to, widget, icon, text)\r\n        self.setTabToolTip(index_to, tip)\r\n        \r\n        self.setCurrentWidget(current_widget)\r\n        self.move_tab_finished.emit()", "output": "Move tab inside a tabwidget", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def union(self, other):\n        \"\"\" \n        \"\"\"\n        return DataFrame(self._jdf.union(other._jdf), self.sql_ctx)", "output": "Return a new :class:`DataFrame` containing union of rows in this and another frame.\n\n        This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n        (that does deduplication of elements), use this function followed by :func:`distinct`.\n\n        Also as standard in SQL, this function resolves columns by position (not by name).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fix_input_files(headerDir, sourceDir, containers=['vector', 'list', 'set', 'map'],\n                    seqType='both', verbose=False):\n    \"\"\"\"\"\"\n    # The new modification time.\n    timestamp = datetime.datetime.now();\n    # Fix the input files for containers in their variadic form.\n    if seqType == \"both\" or seqType == \"variadic\":\n        if verbose:\n            print \"Fix input files for pre-processing Boost.MPL variadic containers.\"\n        fix_input_files_for_variadic_seq(headerDir, sourceDir, timestamp)\n    # Fix the input files for containers in their numbered form.\n    if seqType == \"both\" or seqType == \"numbered\":\n        if verbose:\n            print \"Fix input files for pre-processing Boost.MPL numbered containers.\"\n        fix_input_files_for_numbered_seq(headerDir, \".hpp\", timestamp, containers)\n        fix_input_files_for_numbered_seq(sourceDir, \".cpp\", timestamp, containers)", "output": "Fixes source- and header-files used as input when pre-processing MPL-containers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def buy_open(id_or_ins, amount, price=None, style=None):\n    \"\"\"\n    \n    \"\"\"\n    return order(id_or_ins, amount, SIDE.BUY, POSITION_EFFECT.OPEN, cal_style(price, style))", "output": "\u4e70\u5165\u5f00\u4ed3\u3002\n\n    :param id_or_ins: \u4e0b\u5355\u6807\u7684\u7269\n    :type id_or_ins: :class:`~Instrument` object | `str` | List[:class:`~Instrument`] | List[`str`]\n\n    :param int amount: \u4e0b\u5355\u624b\u6570\n\n    :param float price: \u4e0b\u5355\u4ef7\u683c\uff0c\u9ed8\u8ba4\u4e3aNone\uff0c\u8868\u793a :class:`~MarketOrder`, \u6b64\u53c2\u6570\u4e3b\u8981\u7528\u4e8e\u7b80\u5316 `style` \u53c2\u6570\u3002\n\n    :param style: \u4e0b\u5355\u7c7b\u578b, \u9ed8\u8ba4\u662f\u5e02\u4ef7\u5355\u3002\u76ee\u524d\u652f\u6301\u7684\u8ba2\u5355\u7c7b\u578b\u6709 :class:`~LimitOrder` \u548c :class:`~MarketOrder`\n    :type style: `OrderStyle` object\n\n    :return: :class:`~Order` object | None\n\n    :example:\n\n    .. code-block:: python\n\n        #\u4ee5\u4ef7\u683c\u4e3a3500\u7684\u9650\u4ef7\u5355\u5f00\u4ed3\u4e70\u51652\u5f20\u4e0a\u671f\u6240AG1607\u5408\u7ea6\uff1a\n        buy_open('AG1607', amount=2, price=3500))", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send_order(\n            self,\n            accounts,\n            code='000001',\n            price=9,\n            amount=100,\n            order_direction=ORDER_DIRECTION.BUY,\n            order_model=ORDER_MODEL.LIMIT\n    ):\n        \"\"\"\n        \"\"\"\n        try:\n            #print(code, price, amount)\n            return self.call_post(\n                'orders',\n                {\n                    'client': accounts,\n                    \"action\": 'BUY' if order_direction == 1 else 'SELL',\n                    \"symbol\": code,\n                    \"type\": order_model,\n                    \"priceType\": 0 if order_model == ORDER_MODEL.LIMIT else 4,\n                    \"price\": price,\n                    \"amount\": amount\n                }\n            )\n        except json.decoder.JSONDecodeError:\n            print(RuntimeError('TRADE ERROR'))\n            return None", "output": "[summary]\n\n        Arguments:\n            accounts {[type]} -- [description]\n            code {[type]} -- [description]\n            price {[type]} -- [description]\n            amount {[type]} -- [description]\n\n        Keyword Arguments:\n            order_direction {[type]} -- [description] (default: {ORDER_DIRECTION.BUY})\n            order_model {[type]} -- [description] (default: {ORDER_MODEL.LIMIT})\n\n\n\n        priceType \u53ef\u9009\u62e9\uff1a \u4e0a\u6d77\u4ea4\u6613\u6240\uff1a\n\n            0 - \u9650\u4ef7\u59d4\u6258\n            4 - \u4e94\u6863\u5373\u65f6\u6210\u4ea4\u5269\u4f59\u64a4\u9500\n            6 - \u4e94\u6863\u5373\u65f6\u6210\u4ea4\u5269\u4f59\u8f6c\u9650\n\n        \u6df1\u5733\u4ea4\u6613\u6240\uff1a\n\n            0 - \u9650\u4ef7\u59d4\u6258\n            1 - \u5bf9\u624b\u65b9\u6700\u4f18\u4ef7\u683c\u59d4\u6258\n            2 - \u672c\u65b9\u6700\u4f18\u4ef7\u683c\u59d4\u6258\n            3 - \u5373\u65f6\u6210\u4ea4\u5269\u4f59\u64a4\u9500\u59d4\u6258\n            4 - \u4e94\u6863\u5373\u65f6\u6210\u4ea4\u5269\u4f59\u64a4\u9500\n            5 - \u5168\u989d\u6210\u4ea4\u6216\u64a4\u9500\u59d4\u6258\n\n        Returns:\n            [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_serialization_exclude(serializers, exclude, kwargs):\n    \"\"\"\n    \"\"\"\n    exclude = list(exclude)\n    # Split to support file names like meta.json\n    options = [name.split(\".\")[0] for name in serializers]\n    for key, value in kwargs.items():\n        if key in (\"vocab\",) and value is False:\n            deprecation_warning(Warnings.W015.format(arg=key))\n            exclude.append(key)\n        elif key.split(\".\")[0] in options:\n            raise ValueError(Errors.E128.format(arg=key))\n        # TODO: user warning?\n    return exclude", "output": "Helper function to validate serialization args and manage transition from\n    keyword arguments (pre v2.1) to exclude argument.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def role_update(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    if 'new_name' in kwargs:\n        kwargs['name'] = kwargs.pop('new_name')\n    return cloud.update_role(**kwargs)", "output": "Update a role\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystoneng.role_update name=role1 new_name=newrole\n        salt '*' keystoneng.role_update name=1eb6edd5525e4ac39af571adee673559 new_name=newrole", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def data(self, index, role=Qt.DisplayRole):\r\n        \"\"\"\"\"\"\r\n        if not index.isValid():\r\n            return to_qvariant()\r\n        if role == Qt.DisplayRole or role == Qt.EditRole:\r\n            column = index.column()\r\n            row = index.row()\r\n            value = self.get_value(row, column)\r\n            if isinstance(value, float):\r\n                try:\r\n                    return to_qvariant(self._format % value)\r\n                except (ValueError, TypeError):\r\n                    # may happen if format = '%d' and value = NaN;\r\n                    # see issue 4139\r\n                    return to_qvariant(DEFAULT_FORMAT % value)\r\n            elif is_type_text_string(value):\r\n                # Don't perform any conversion on strings\r\n                # because it leads to differences between\r\n                # the data present in the dataframe and\r\n                # what is shown by Spyder\r\n                return value\r\n            else:\r\n                try:\r\n                    return to_qvariant(to_text_string(value))\r\n                except Exception:\r\n                    self.display_error_idxs.append(index)\r\n                    return u'Display Error!'\r\n        elif role == Qt.BackgroundColorRole:\r\n            return to_qvariant(self.get_bgcolor(index))\r\n        elif role == Qt.FontRole:\r\n            return to_qvariant(get_font(font_size_delta=DEFAULT_SMALL_DELTA))\r\n        elif role == Qt.ToolTipRole:\r\n            if index in self.display_error_idxs:\r\n                return _(\"It is not possible to display this value because\\n\"\r\n                         \"an error ocurred while trying to do it\")\r\n        return to_qvariant()", "output": "Cell content", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_base_form(self, univ_pos, morphology=None):\n        \"\"\"\n        \n        \"\"\"\n        morphology = {} if morphology is None else morphology\n        others = [key for key in morphology\n                  if key not in (POS, 'Number', 'POS', 'VerbForm', 'Tense')]\n        if univ_pos == 'noun' and morphology.get('Number') == 'sing':\n            return True\n        elif univ_pos == 'verb' and morphology.get('VerbForm') == 'inf':\n            return True\n        # This maps 'VBP' to base form -- probably just need 'IS_BASE'\n        # morphology\n        elif univ_pos == 'verb' and (morphology.get('VerbForm') == 'fin' and\n                                     morphology.get('Tense') == 'pres' and\n                                     morphology.get('Number') is None and\n                                     not others):\n            return True\n        elif univ_pos == 'adj' and morphology.get('Degree') == 'pos':\n            return True\n        elif VerbForm_inf in morphology:\n            return True\n        elif VerbForm_none in morphology:\n            return True\n        elif Number_sing in morphology:\n            return True\n        elif Degree_pos in morphology:\n            return True\n        else:\n            return False", "output": "Check whether we're dealing with an uninflected paradigm, so we can\n        avoid lemmatization entirely.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def group_norm(x, filters=None, num_groups=8, epsilon=1e-5):\n  \"\"\"\"\"\"\n  x_shape = shape_list(x)\n  if filters is None:\n    filters = x_shape[-1]\n  assert len(x_shape) == 4\n  assert filters % num_groups == 0\n  # Prepare variables.\n  scale = tf.get_variable(\n      \"group_norm_scale\", [filters], initializer=tf.ones_initializer())\n  bias = tf.get_variable(\n      \"group_norm_bias\", [filters], initializer=tf.zeros_initializer())\n  epsilon, scale, bias = [cast_like(t, x) for t in [epsilon, scale, bias]]\n  # Reshape and compute group norm.\n  x = tf.reshape(x, x_shape[:-1] + [num_groups, filters // num_groups])\n  # Calculate mean and variance on heights, width, channels (not groups).\n  mean, variance = tf.nn.moments(x, [1, 2, 4], keep_dims=True)\n  norm_x = (x - mean) * tf.rsqrt(variance + epsilon)\n  return tf.reshape(norm_x, x_shape) * scale + bias", "output": "Group normalization as in https://arxiv.org/abs/1803.08494.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def A(*a):\n    \"\"\"\"\"\"\n    return np.array(a[0]) if len(a)==1 else [np.array(o) for o in a]", "output": "convert iterable object into numpy array", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def query(self, search, query):\n        \"\"\"\n        \n        \"\"\"\n        if query:\n            if self.fields:\n                return search.query('multi_match', fields=self.fields, query=query)\n            else:\n                return search.query('multi_match', query=query)\n        return search", "output": "Add query part to ``search``.\n\n        Override this if you wish to customize the query used.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_shadow(img, vertices_list):\n    \"\"\"\n\n    \"\"\"\n    non_rgb_warning(img)\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype('uint8'))\n        needs_float = True\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError('Unexpected dtype {} for RandomSnow augmentation'.format(input_dtype))\n\n    image_hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n    mask = np.zeros_like(img)\n\n    # adding all shadow polygons on empty mask, single 255 denotes only red channel\n    for vertices in vertices_list:\n        cv2.fillPoly(mask, vertices, 255)\n\n    # if red channel is hot, image's \"Lightness\" channel's brightness is lowered\n    red_max_value_ind = mask[:, :, 0] == 255\n    image_hls[:, :, 1][red_max_value_ind] = image_hls[:, :, 1][red_max_value_ind] * 0.5\n\n    image_rgb = cv2.cvtColor(image_hls, cv2.COLOR_HLS2RGB)\n\n    if needs_float:\n        image_rgb = to_float(image_rgb, max_value=255)\n\n    return image_rgb", "output": "Add shadows to the image.\n\n    From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n\n    Args:\n        img (np.array):\n        vertices_list (list):\n\n    Returns:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parents(self):\n        \"\"\"\n        \"\"\"\n        entries = []\n        command = self\n        while command.parent is not None:\n            command = command.parent\n            entries.append(command)\n\n        return entries", "output": "Retrieves the parents of this command.\n\n        If the command has no parents then it returns an empty :class:`list`.\n\n        For example in commands ``?a b c test``, the parents are ``[c, b, a]``.\n\n        .. versionadded:: 1.1.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transform(self, fn, lazy=True):\n        \"\"\"\n        \"\"\"\n        trans = _LazyTransformDataset(self, fn)\n        if lazy:\n            return trans\n        return SimpleDataset([i for i in trans])", "output": "Returns a new dataset with each sample transformed by the\n        transformer function `fn`.\n\n        Parameters\n        ----------\n        fn : callable\n            A transformer function that takes a sample as input and\n            returns the transformed sample.\n        lazy : bool, default True\n            If False, transforms all samples at once. Otherwise,\n            transforms each sample on demand. Note that if `fn`\n            is stochastic, you must set lazy to True or you will\n            get the same result on all epochs.\n\n        Returns\n        -------\n        Dataset\n            The transformed dataset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sort_index(self, **kwargs):\n        \"\"\"\n        \"\"\"\n        axis = kwargs.pop(\"axis\", 0)\n        index = self.columns if axis else self.index\n\n        # sort_index can have ascending be None and behaves as if it is False.\n        # sort_values cannot have ascending be None. Thus, the following logic is to\n        # convert the ascending argument to one that works with sort_values\n        ascending = kwargs.pop(\"ascending\", True)\n        if ascending is None:\n            ascending = False\n        kwargs[\"ascending\"] = ascending\n\n        def sort_index_builder(df, **kwargs):\n            if axis:\n                df.columns = index\n            else:\n                df.index = index\n            return df.sort_index(axis=axis, **kwargs)\n\n        func = self._prepare_method(sort_index_builder, **kwargs)\n        new_data = self._map_across_full_axis(axis, func)\n        if axis:\n            new_columns = pandas.Series(self.columns).sort_values(**kwargs)\n            new_index = self.index\n        else:\n            new_index = pandas.Series(self.index).sort_values(**kwargs)\n            new_columns = self.columns\n        return self.__constructor__(\n            new_data, new_index, new_columns, self.dtypes.copy()\n        )", "output": "Sorts the data with respect to either the columns or the indices.\n\n        Returns:\n            DataManager containing the data sorted by columns or indices.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_mode(path, mode):\n    '''\n    \n    '''\n    func_name = '{0}.set_mode'.format(__virtualname__)\n    if __opts__.get('fun', '') == func_name:\n        log.info('The function %s should not be used on Windows systems; '\n                 'see function docs for details. The value returned is '\n                 'always None. Use set_perms instead.', func_name)\n\n    return get_mode(path)", "output": "Set the mode of a file\n\n    This just calls get_mode, which returns None because we don't use mode on\n    Windows\n\n    Args:\n        path: The path to the file or directory\n        mode: The mode (not used)\n\n    Returns:\n        None\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.set_mode /etc/passwd 0644", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setAll(self, pairs):\n        \"\"\"\n        \n        \"\"\"\n        for (k, v) in pairs:\n            self.set(k, v)\n        return self", "output": "Set multiple parameters, passed as a list of key-value pairs.\n\n        :param pairs: list of key-value pairs to set", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_process_mapping():\n    \"\"\"\n    \"\"\"\n    try:\n        output = subprocess.check_output([\n            'ps', '-ww', '-o', 'pid=', '-o', 'ppid=', '-o', 'args=',\n        ])\n    except OSError as e:    # Python 2-compatible FileNotFoundError.\n        if e.errno != errno.ENOENT:\n            raise\n        raise PsNotAvailable('ps not found')\n    except subprocess.CalledProcessError as e:\n        # `ps` can return 1 if the process list is completely empty.\n        # (sarugaku/shellingham#15)\n        if not e.output.strip():\n            return {}\n        raise\n    if not isinstance(output, str):\n        encoding = sys.getfilesystemencoding() or sys.getdefaultencoding()\n        output = output.decode(encoding)\n    processes = {}\n    for line in output.split('\\n'):\n        try:\n            pid, ppid, args = line.strip().split(None, 2)\n            # XXX: This is not right, but we are really out of options.\n            # ps does not offer a sane way to decode the argument display,\n            # and this is \"Good Enough\" for obtaining shell names. Hopefully\n            # people don't name their shell with a space, or have something\n            # like \"/usr/bin/xonsh is uber\". (sarugaku/shellingham#14)\n            args = tuple(a.strip() for a in args.split(' '))\n        except ValueError:\n            continue\n        processes[pid] = Process(args=args, pid=pid, ppid=ppid)\n    return processes", "output": "Try to look up the process tree via the output of `ps`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_aws_variables(self):\n        \"\"\"\n        \n        \"\"\"\n\n        result = {\n            # Variable that says this function is running in Local Lambda\n            \"AWS_SAM_LOCAL\": \"true\",\n\n            # Function configuration\n            \"AWS_LAMBDA_FUNCTION_MEMORY_SIZE\": str(self.memory),\n            \"AWS_LAMBDA_FUNCTION_TIMEOUT\": str(self.timeout),\n            \"AWS_LAMBDA_FUNCTION_HANDLER\": str(self._function[\"handler\"]),\n\n            # AWS Credentials - Use the input credentials or use the defaults\n            \"AWS_REGION\": self.aws_creds.get(\"region\", self._DEFAULT_AWS_CREDS[\"region\"]),\n\n            \"AWS_DEFAULT_REGION\": self.aws_creds.get(\"region\", self._DEFAULT_AWS_CREDS[\"region\"]),\n\n            \"AWS_ACCESS_KEY_ID\": self.aws_creds.get(\"key\", self._DEFAULT_AWS_CREDS[\"key\"]),\n\n            \"AWS_SECRET_ACCESS_KEY\": self.aws_creds.get(\"secret\", self._DEFAULT_AWS_CREDS[\"secret\"])\n\n            # Additional variables we don't fill in\n            # \"AWS_ACCOUNT_ID=\"\n            # \"AWS_LAMBDA_EVENT_BODY=\",\n            # \"AWS_LAMBDA_FUNCTION_NAME=\",\n            # \"AWS_LAMBDA_FUNCTION_VERSION=\",\n        }\n\n        # Session Token should be added **only** if the input creds have a token and the value is not empty.\n        if self.aws_creds.get(\"sessiontoken\"):\n            result[\"AWS_SESSION_TOKEN\"] = self.aws_creds.get(\"sessiontoken\")\n\n        return result", "output": "Returns the AWS specific environment variables that should be available in the Lambda runtime.\n        They are prefixed it \"AWS_*\".\n\n        :return dict: Name and value of AWS environment variable", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def batch_dense(inputs,\n                units,\n                activation=None,\n                kernel_initializer=None,\n                reuse=None,\n                name=None):\n  \"\"\"\n  \"\"\"\n  inputs_shape = shape_list(inputs)\n  if len(inputs_shape) != 3:\n    raise ValueError(\"inputs must have 3 dimensions\")\n  batch = inputs_shape[0]\n  input_units = inputs_shape[2]\n  if not isinstance(batch, int) or not isinstance(input_units, int):\n    raise ValueError(\"inputs must have static dimensions 0 and 2\")\n  with tf.variable_scope(\n      name,\n      default_name=\"batch_dense\",\n      values=[inputs],\n      reuse=reuse,\n      dtype=inputs.dtype):\n    if kernel_initializer is None:\n      kernel_initializer = tf.random_normal_initializer(\n          stddev=input_units**-0.5)\n    w = tf.get_variable(\n        \"w\", [batch, input_units, units],\n        initializer=kernel_initializer,\n        dtype=inputs.dtype)\n    y = tf.matmul(inputs, w)\n    if activation is not None:\n      y = activation(y)\n    return y", "output": "Multiply a batch of input matrices by a batch of parameter matrices.\n\n  Each input matrix is multiplied by the corresponding parameter matrix.\n\n  This is useful in a mixture-of-experts where the batch represents different\n  experts with different inputs.\n\n  Args:\n    inputs: a Tensor with shape [batch, length, input_units]\n    units: an integer\n    activation: an optional activation function to apply to the output\n    kernel_initializer: an optional initializer\n    reuse: whether to reuse the varaible scope\n    name: an optional string\n\n  Returns:\n    a Tensor with shape [batch, length, units]\n\n  Raises:\n    ValueError: if the \"batch\" or \"input_units\" dimensions of inputs are not\n      statically known.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _save_config(section, token, value):\n    '''\n    \n    '''\n    cmd = NIRTCFG_PATH\n    cmd += ' --set section={0},token=\\'{1}\\',value=\\'{2}\\''.format(section, token, value)\n    if __salt__['cmd.run_all'](cmd)['retcode'] != 0:\n        exc_msg = 'Error: could not set {} to {} for {}\\n'.format(token, value, section)\n        raise salt.exceptions.CommandExecutionError(exc_msg)", "output": "Helper function to persist a configuration in the ini file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def output_scores(self, name=None):\n        \"\"\"\n        \n        \"\"\"\n        scores = [head.output_scores('cascade_scores_stage{}'.format(idx + 1))\n                  for idx, head in enumerate(self._heads)]\n        return tf.multiply(tf.add_n(scores), (1.0 / self.num_cascade_stages), name=name)", "output": "Returns:\n            Nx#class", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expand_env_variables(lines_enum):\n    # type: (ReqFileLines) -> ReqFileLines\n    \"\"\"\n    \"\"\"\n    for line_number, line in lines_enum:\n        for env_var, var_name in ENV_VAR_RE.findall(line):\n            value = os.getenv(var_name)\n            if not value:\n                continue\n\n            line = line.replace(env_var, value)\n\n        yield line_number, line", "output": "Replace all environment variables that can be retrieved via `os.getenv`.\n\n    The only allowed format for environment variables defined in the\n    requirement file is `${MY_VARIABLE_1}` to ensure two things:\n\n    1. Strings that contain a `$` aren't accidentally (partially) expanded.\n    2. Ensure consistency across platforms for requirement files.\n\n    These points are the result of a discusssion on the `github pull\n    request #3514 <https://github.com/pypa/pip/pull/3514>`_.\n\n    Valid characters in variable names follow the `POSIX standard\n    <http://pubs.opengroup.org/onlinepubs/9699919799/>`_ and are limited\n    to uppercase letter, digits and the `_` (underscore).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_changed_cfg_pkgs(self, data):\n        '''\n        \n        '''\n        f_data = dict()\n        for pkg_name, pkg_files in data.items():\n            cfgs = list()\n            cfg_data = list()\n            if self.grains_core.os_data().get('os_family') == 'Debian':\n                cfg_data = salt.utils.stringutils.to_str(self._syscall(\"dpkg\", None, None, '--verify',\n                                                           pkg_name)[0]).split(os.linesep)\n            elif self.grains_core.os_data().get('os_family') in ['Suse', 'redhat']:\n                cfg_data = salt.utils.stringutils.to_str(self._syscall(\"rpm\", None, None, '-V', '--nodeps', '--nodigest',\n                                                           '--nosignature', '--nomtime', '--nolinkto',\n                                                           pkg_name)[0]).split(os.linesep)\n            for line in cfg_data:\n                line = line.strip()\n                if not line or line.find(\" c \") < 0 or line.split(\" \")[0].find(\"5\") < 0:\n                    continue\n                cfg_file = line.split(\" \")[-1]\n                if cfg_file in pkg_files:\n                    cfgs.append(cfg_file)\n            if cfgs:\n                f_data[pkg_name] = cfgs\n\n        return f_data", "output": "Filter out unchanged packages on the Debian or RPM systems.\n\n        :param data: Structure {package-name -> [ file .. file1 ]}\n        :return: Same structure as data, except only files that were changed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_font(self, font, color_scheme=None):\n        \"\"\"\"\"\"\n        self.editor.set_font(font, color_scheme=color_scheme)", "output": "Set font", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_ip_address(list_name, item_name):\n    '''\n    \n\n    '''\n    payload = {\"jsonrpc\": \"2.0\",\n               \"id\": \"ID0\",\n               \"method\": \"add_policy_ip_addresses\",\n               \"params\": [list_name, {\"item_name\": item_name}]}\n\n    response = __proxy__['bluecoat_sslv.call'](payload, True)\n\n    return _validate_change_result(response)", "output": "Add an IP address to an IP address list.\n\n    list_name(str): The name of the specific policy IP address list to append to.\n\n    item_name(str): The IP address to append to the list.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bluecoat_sslv.add_ip_address MyIPAddressList 10.0.0.0/24", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_homogeneous_type(self):\n        \"\"\"\n        \n        \"\"\"\n        if self._data.any_extension_types:\n            return len({block.dtype for block in self._data.blocks}) == 1\n        else:\n            return not self._data.is_mixed_type", "output": "Whether all the columns in a DataFrame have the same type.\n\n        Returns\n        -------\n        bool\n\n        Examples\n        --------\n        >>> DataFrame({\"A\": [1, 2], \"B\": [3, 4]})._is_homogeneous_type\n        True\n        >>> DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.0]})._is_homogeneous_type\n        False\n\n        Items with the same type but different sizes are considered\n        different types.\n\n        >>> DataFrame({\n        ...    \"A\": np.array([1, 2], dtype=np.int32),\n        ...    \"B\": np.array([1, 2], dtype=np.int64)})._is_homogeneous_type\n        False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unionByName(self, other):\n        \"\"\" \n        \"\"\"\n        return DataFrame(self._jdf.unionByName(other._jdf), self.sql_ctx)", "output": "Returns a new :class:`DataFrame` containing union of rows in this and another frame.\n\n        This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n        union (that does deduplication of elements), use this function followed by :func:`distinct`.\n\n        The difference between this function and :func:`union` is that this function\n        resolves columns by name (not by position):\n\n        >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n        >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n        >>> df1.unionByName(df2).show()\n        +----+----+----+\n        |col0|col1|col2|\n        +----+----+----+\n        |   1|   2|   3|\n        |   6|   4|   5|\n        +----+----+----+", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _resize_image_if_necessary(image_fobj, target_pixels=None):\n  \"\"\"\n  \"\"\"\n  if target_pixels is None:\n    return image_fobj\n\n  cv2 = tfds.core.lazy_imports.cv2\n  # Decode image using OpenCV2.\n  image = cv2.imdecode(\n      np.fromstring(image_fobj.read(), dtype=np.uint8), flags=3)\n  # Get image height and width.\n  height, width, _ = image.shape\n  actual_pixels = height * width\n  if actual_pixels > target_pixels:\n    factor = np.sqrt(target_pixels / actual_pixels)\n    image = cv2.resize(image, dsize=None, fx=factor, fy=factor)\n  # Encode the image with quality=72 and store it in a BytesIO object.\n  _, buff = cv2.imencode(\".jpg\", image, [int(cv2.IMWRITE_JPEG_QUALITY), 72])\n  return io.BytesIO(buff.tostring())", "output": "Resize an image to have (roughly) the given number of target pixels.\n\n  Args:\n    image_fobj: File object containing the original image.\n    target_pixels: If given, number of pixels that the image must have.\n\n  Returns:\n    A file object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_file_extension_assertion(extension):\n    \"\"\"\n\n    \"\"\"\n    def file_extension_assertion(file_path):\n        base, ext = os.path.splitext(file_path)\n        if ext.lower() != extension:\n            raise argparse.ArgumentTypeError('File must have ' + extension + ' extension')\n        return file_path\n    return file_extension_assertion", "output": "Function factory for file extension argparse assertion\n        Args:\n            extension (string): the file extension to assert\n\n        Returns:\n            string: the supplied extension, if assertion is successful.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_lib_path():\n    \"\"\"\n    \"\"\"\n    curr_path = os.path.dirname(os.path.abspath(os.path.expanduser(__file__)))\n    # make pythonpack hack: copy this directory one level upper for setup.py\n    dll_path = [curr_path, os.path.join(curr_path, '../../wrapper/'),\n                os.path.join(curr_path, './wrapper/')]\n    if os.name == 'nt':\n        if platform.architecture()[0] == '64bit':\n            dll_path.append(os.path.join(curr_path, '../../windows/x64/Release/'))\n            # hack for pip installation when copy all parent source directory here\n            dll_path.append(os.path.join(curr_path, './windows/x64/Release/'))\n        else:\n            dll_path.append(os.path.join(curr_path, '../../windows/Release/'))\n            # hack for pip installation when copy all parent source directory here\n            dll_path.append(os.path.join(curr_path, './windows/Release/'))\n    if os.name == 'nt':\n        dll_path = [os.path.join(p, 'xgboost_wrapper.dll') for p in dll_path]\n    else:\n        dll_path = [os.path.join(p, 'libxgboostwrapper.so') for p in dll_path]\n    lib_path = [p for p in dll_path if os.path.exists(p) and os.path.isfile(p)]\n    #From github issues, most of installation errors come from machines w/o compilers\n    if len(lib_path) == 0 and not os.environ.get('XGBOOST_BUILD_DOC', False):\n        raise XGBoostLibraryNotFound(\n            'Cannot find XGBoost Libarary in the candicate path, ' +\n            'did you install compilers and run build.sh in root path?\\n'\n            'List of candidates:\\n' + ('\\n'.join(dll_path)))\n    return lib_path", "output": "Load find the path to xgboost dynamic library files.\n\n    Returns\n    -------\n    lib_path: list(string)\n       List of all found library path to xgboost", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def poplistitem(self, last=True):\n        \"\"\"\n        \n        \"\"\"\n        if not self._items:\n            s = 'poplistitem(): %s is empty' % self.__class__.__name__\n            raise KeyError(s)\n\n        key = self.keys()[-1 if last else 0]\n        return key, self.poplist(key)", "output": "Pop and return a key:valuelist item comprised of a key and that key's\n        list of values. If <last> is False, a key:valuelist item comprised of\n        keys()[0] and its list of values is popped and returned. If <last> is\n        True, a key:valuelist item comprised of keys()[-1] and its list of\n        values is popped and returned.\n\n        Example:\n          omd = omdict([(1,1), (1,11), (1,111), (2,2), (3,3)])\n          omd.poplistitem(last=True) == (3,[3])\n          omd.poplistitem(last=False) == (1,[1,11,111])\n\n        Params:\n          last: Boolean whether to pop the first or last key and its associated\n            list of values.\n        Raises: KeyError if the dictionary is empty.\n        Returns: A two-tuple comprised of the first or last key and its\n          associated list of values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_body(arch:Callable, pretrained:bool=True, cut:Optional[Union[int, Callable]]=None):\n    \"\"\n    model = arch(pretrained)\n    cut = ifnone(cut, cnn_config(arch)['cut'])\n    if cut is None:\n        ll = list(enumerate(model.children()))\n        cut = next(i for i,o in reversed(ll) if has_pool_type(o))\n    if   isinstance(cut, int):      return nn.Sequential(*list(model.children())[:cut])\n    elif isinstance(cut, Callable): return cut(model)\n    else:                           raise NamedError(\"cut must be either integer or a function\")", "output": "Cut off the body of a typically pretrained `model` at `cut` (int) or cut the model as specified by `cut(model)` (function).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self, input_dir, output_dir, epsilon):\n    \"\"\"\n    \"\"\"\n    logging.info('Running attack %s', self.submission_id)\n    tmp_run_dir = self.temp_copy_extracted_submission()\n    cmd = ['--network=none',\n           '-m=24g',\n           '--cpus=3.75',\n           '-v', '{0}:/input_images:ro'.format(input_dir),\n           '-v', '{0}:/output_images'.format(output_dir),\n           '-v', '{0}:/code'.format(tmp_run_dir),\n           '-w', '/code',\n           self.container_name,\n           './' + self.entry_point,\n           '/input_images',\n           '/output_images',\n           str(epsilon)]\n    elapsed_time_sec = self.run_with_time_limit(cmd)\n    sudo_remove_dirtree(tmp_run_dir)\n    return elapsed_time_sec", "output": "Runs attack inside Docker.\n\n    Args:\n      input_dir: directory with input (dataset).\n      output_dir: directory where output (adversarial images) should be written.\n      epsilon: maximum allowed size of adversarial perturbation,\n        should be in range [0, 255].\n\n    Returns:\n      how long it took to run submission in seconds", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def groupby(\n            self,\n            by=None,\n            axis=0,\n            level=None,\n            as_index=True,\n            sort=False,\n            group_keys=False,\n            squeeze=False,\n            **kwargs\n    ):\n        \"\"\"\n        \"\"\"\n\n        if by == self.index.names[1]:\n            by = None\n            level = 1\n        elif by == self.index.names[0]:\n            by = None\n            level = 0\n        return self.data.groupby(\n            by=by,\n            axis=axis,\n            level=level,\n            as_index=as_index,\n            sort=sort,\n            group_keys=group_keys,\n            squeeze=squeeze\n        )", "output": "\u4effdataframe\u7684groupby\u5199\u6cd5,\u4f46\u63a7\u5236\u4e86by\u7684code\u548cdatetime\n\n        Keyword Arguments:\n            by {[type]} -- [description] (default: {None})\n            axis {int} -- [description] (default: {0})\n            level {[type]} -- [description] (default: {None})\n            as_index {bool} -- [description] (default: {True})\n            sort {bool} -- [description] (default: {True})\n            group_keys {bool} -- [description] (default: {True})\n            squeeze {bool} -- [description] (default: {False})\n            observed {bool} -- [description] (default: {False})\n\n        Returns:\n            [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_dict(self, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        d = {}\n        if self.query:\n            d[\"query\"] = self.query.to_dict()\n\n        if self._script:\n            d['script'] = self._script\n\n        d.update(self._extra)\n\n        d.update(kwargs)\n        return d", "output": "Serialize the search into the dictionary that will be sent over as the\n        request'ubq body.\n\n        All additional keyword arguments will be included into the dictionary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def map_across_full_axis(self, axis, map_func):\n        \"\"\"\n        \"\"\"\n        # Since we are already splitting the DataFrame back up after an\n        # operation, we will just use this time to compute the number of\n        # partitions as best we can right now.\n        num_splits = self._compute_num_partitions()\n        preprocessed_map_func = self.preprocess_func(map_func)\n        partitions = self.column_partitions if not axis else self.row_partitions\n        # For mapping across the entire axis, we don't maintain partitioning because we\n        # may want to line to partitioning up with another BlockPartitions object. Since\n        # we don't need to maintain the partitioning, this gives us the opportunity to\n        # load-balance the data as well.\n        result_blocks = np.array(\n            [\n                part.apply(preprocessed_map_func, num_splits=num_splits)\n                for part in partitions\n            ]\n        )\n        # If we are mapping over columns, they are returned to use the same as\n        # rows, so we need to transpose the returned 2D numpy array to return\n        # the structure to the correct order.\n        return (\n            self.__constructor__(result_blocks.T)\n            if not axis\n            else self.__constructor__(result_blocks)\n        )", "output": "Applies `map_func` to every partition.\n\n        Note: This method should be used in the case that `map_func` relies on\n            some global information about the axis.\n\n        Args:\n            axis: The axis to perform the map across (0 - index, 1 - columns).\n            map_func: The function to apply.\n\n        Returns:\n            A new BaseFrameManager object, the type of object that called this.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def detach_session(self):\n        \"\"\"\"\"\"\n        if self._session is not None:\n            self._session.unsubscribe(self)\n            self._session = None", "output": "Allow the session to be discarded and don't get change notifications from it anymore", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self):\n        \"\"\"\n\n        \"\"\"\n        assert self.binded and self.params_initialized and self.optimizer_initialized\n        self._params_dirty = True\n        self._curr_module.update()", "output": "Updates parameters according to installed optimizer and the gradient computed\n        in the previous forward-backward cycle.\n\n        When KVStore is used to update parameters for multi-device or multi-machine training,\n        a copy of the parameters are stored in KVStore. Note that for `row_sparse` parameters,\n        this function does update the copy of parameters in KVStore, but doesn't broadcast the\n        updated parameters to all devices / machines. Please call `prepare` to broadcast\n        `row_sparse` parameters with the next batch of data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imagetransformer_b10l_4h_big_uncond_dr03_lr025_tpu():\n  \"\"\"\"\"\"\n  hparams = imagetransformer_bas8l_8h_big_uncond_dr03_imgnet()\n  update_hparams_for_tpu(hparams)\n  hparams.batch_size = 4\n  hparams.num_heads = 4   # heads are expensive on tpu\n  hparams.num_decoder_layers = 10\n  hparams.learning_rate = 0.25\n  hparams.learning_rate_warmup_steps = 8000\n  hparams.layer_preprocess_sequence = \"none\"\n  hparams.layer_postprocess_sequence = \"dan\"\n  # hparams.unconditional = True\n  return hparams", "output": "TPU related small model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def elementInActiveFormattingElements(self, name):\n        \"\"\"\"\"\"\n\n        for item in self.activeFormattingElements[::-1]:\n            # Check for Marker first because if it's a Marker it doesn't have a\n            # name attribute.\n            if item == Marker:\n                break\n            elif item.name == name:\n                return item\n        return False", "output": "Check if an element exists between the end of the active\n        formatting elements and the last marker. If it does, return it, else\n        return false", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_tpu_1b():\n  \"\"\"\"\"\"\n  hparams = transformer_tpu()\n  hparams.hidden_size = 2048\n  hparams.filter_size = 8192\n  hparams.num_hidden_layers = 8\n  # smaller batch size to avoid OOM\n  hparams.batch_size = 1024\n  hparams.activation_dtype = \"bfloat16\"\n  hparams.weight_dtype = \"bfloat16\"\n  # maximize number of parameters relative to computation by not sharing.\n  hparams.shared_embedding_and_softmax_weights = False\n  return hparams", "output": "Hparams for machine translation with ~1.1B parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ensure_rng(random_state=None):\n    \"\"\"\n    \n    \"\"\"\n    if random_state is None:\n        random_state = np.random.RandomState()\n    elif isinstance(random_state, int):\n        random_state = np.random.RandomState(random_state)\n    else:\n        assert isinstance(random_state, np.random.RandomState)\n    return random_state", "output": "Creates a random number generator based on an optional seed.  This can be\n    an integer or another random state for a seeded rng, or None for an\n    unseeded rng.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def competitions_submissions_submit(self, blob_file_tokens, submission_description, id, **kwargs):  # noqa: E501\n        \"\"\"\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.competitions_submissions_submit_with_http_info(blob_file_tokens, submission_description, id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.competitions_submissions_submit_with_http_info(blob_file_tokens, submission_description, id, **kwargs)  # noqa: E501\n            return data", "output": "Submit to competition  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.competitions_submissions_submit(blob_file_tokens, submission_description, id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str blob_file_tokens: Token identifying location of uploaded submission file (required)\n        :param str submission_description: Description of competition submission (required)\n        :param str id: Competition name (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put_multipart(self, local_path, destination_s3_path, part_size=DEFAULT_PART_SIZE, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        self._check_deprecated_argument(**kwargs)\n\n        from boto3.s3.transfer import TransferConfig\n        # default part size for boto3 is 8Mb, changing it to fit part_size\n        # provided as a parameter\n        transfer_config = TransferConfig(multipart_chunksize=part_size)\n\n        (bucket, key) = self._path_to_bucket_and_key(destination_s3_path)\n\n        self.s3.meta.client.upload_fileobj(\n            Fileobj=open(local_path, 'rb'), Bucket=bucket, Key=key, Config=transfer_config, ExtraArgs=kwargs)", "output": "Put an object stored locally to an S3 path\n        using S3 multi-part upload (for files > 8Mb).\n        :param local_path: Path to source local file\n        :param destination_s3_path: URL for target S3 location\n        :param part_size: Part size in bytes. Default: 8388608 (8MB)\n        :param kwargs: Keyword arguments are passed to the boto function `upload_fileobj` as ExtraArgs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _pop_header_name(row, index_col):\n    \"\"\"\n    \n    \"\"\"\n    # Pop out header name and fill w/blank.\n    i = index_col if not is_list_like(index_col) else max(index_col)\n\n    header_name = row[i]\n    header_name = None if header_name == \"\" else header_name\n\n    return header_name, row[:i] + [''] + row[i + 1:]", "output": "Pop the header name for MultiIndex parsing.\n\n    Parameters\n    ----------\n    row : list\n        The data row to parse for the header name.\n    index_col : int, list\n        The index columns for our data. Assumed to be non-null.\n\n    Returns\n    -------\n    header_name : str\n        The extracted header name.\n    trimmed_row : list\n        The original data row with the header name removed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_account_created(name):\n    '''\n    \n    '''\n    ret = _get_account_policy_data_value(name, 'creationTime')\n\n    unix_timestamp = salt.utils.mac_utils.parse_return(ret)\n\n    date_text = _convert_to_datetime(unix_timestamp)\n\n    return date_text", "output": "Get the date/time the account was created\n\n    :param str name: The username of the account\n\n    :return: The date/time the account was created (yyyy-mm-dd hh:mm:ss)\n    :rtype: str\n\n    :raises: CommandExecutionError on user not found or any other unknown error\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' shadow.get_account_created admin", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _format_repo_args(comment=None, component=None, distribution=None,\n                      uploaders_file=None, saltenv='base'):\n    '''\n    \n    '''\n    ret = list()\n    cached_uploaders_path = None\n    settings = {'comment': comment, 'component': component,\n                'distribution': distribution}\n\n    if uploaders_file:\n        cached_uploaders_path = __salt__['cp.cache_file'](uploaders_file, saltenv)\n\n        if not cached_uploaders_path:\n            log.error('Unable to get cached copy of file: %s', uploaders_file)\n            return False\n\n    for setting in settings:\n        if settings[setting] is not None:\n            ret.append('-{}={}'.format(setting, settings[setting]))\n\n    if cached_uploaders_path:\n        ret.append('-uploaders-file={}'.format(cached_uploaders_path))\n\n    return ret", "output": "Format the common arguments for creating or editing a repository.\n\n    :param str comment: The description of the repository.\n    :param str component: The default component to use when publishing.\n    :param str distribution: The default distribution to use when publishing.\n    :param str uploaders_file: The repository upload restrictions config.\n    :param str saltenv: The environment the file resides in.\n\n    :return: A list of the arguments formatted as aptly arguments.\n    :rtype: list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def auth(username, password):\n    '''\n    \n    '''\n    _cred = __get_yubico_users(username)\n\n    client = Yubico(_cred['id'], _cred['key'])\n\n    try:\n        return client.verify(password)\n    except yubico_exceptions.StatusCodeError as e:\n        log.info('Unable to verify YubiKey `%s`', e)\n        return False", "output": "Authenticate against yubico server", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tfrecord_iterator(filenames, gzipped=False, example_spec=None):\n  \"\"\"\n  \"\"\"\n  with tf.Graph().as_default():\n    dataset = tf.data.Dataset.from_tensor_slices(filenames)\n\n    def _load_records(filename):\n      return tf.data.TFRecordDataset(\n          filename,\n          compression_type=tf.constant(\"GZIP\") if gzipped else None,\n          buffer_size=16 * 1000 * 1000)\n\n    dataset = dataset.flat_map(_load_records)\n\n    def _parse_example(ex_ser):\n      return tf.parse_single_example(ex_ser, example_spec)\n\n    if example_spec:\n      dataset = dataset.map(_parse_example, num_parallel_calls=32)\n    dataset = dataset.prefetch(100)\n    record_it = dataset.make_one_shot_iterator().get_next()\n\n    with tf.Session() as sess:\n      while True:\n        try:\n          ex = sess.run(record_it)\n          yield ex\n        except tf.errors.OutOfRangeError:\n          break", "output": "Yields records from TFRecord files.\n\n  Args:\n    filenames: list<str>, list of TFRecord filenames to read from.\n    gzipped: bool, whether the TFRecord files are gzip-encoded.\n    example_spec: dict<str feature name, tf.VarLenFeature/tf.FixedLenFeature>,\n      if provided, will parse each record as a tensorflow.Example proto.\n\n  Yields:\n    Records (or parsed Examples, if example_spec is provided) from files.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_layer(self, x, layer):\n    \"\"\"\n    \n    \"\"\"\n    # Return the symbolic representation for this layer.\n    output = self.fprop(x)\n    try:\n      requested = output[layer]\n    except KeyError:\n      raise NoSuchLayerError()\n    return requested", "output": "Expose the hidden features of a model given a layer name.\n    :param x: A symbolic representation of the network input\n    :param layer: The name of the hidden layer to return features at.\n    :return: A symbolic representation of the hidden features\n    :raise: NoSuchLayerError if `layer` is not in the model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def package(self, output=None):\n        \"\"\"\n        \n        \"\"\"\n        # Make sure we're in a venv.\n        self.check_venv()\n\n        # force not to delete the local zip\n        self.override_stage_config_setting('delete_local_zip', False)\n        # Execute the prebuild script\n        if self.prebuild_script:\n            self.execute_prebuild_script()\n        # Create the Lambda Zip\n        self.create_package(output)\n        self.callback('zip')\n        size = human_size(os.path.getsize(self.zip_path))\n        click.echo(click.style(\"Package created\", fg=\"green\", bold=True) + \": \" + click.style(self.zip_path, bold=True) + \" (\" + size + \")\")", "output": "Only build the package", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_tall_train_uniencdec():\n  \"\"\"\"\"\"\n  hparams = transformer_tall()\n  hparams.max_input_seq_length = 750\n  hparams.max_target_seq_length = 100\n  hparams.optimizer = \"true_adam\"\n  hparams.learning_rate_schedule = (\"linear_warmup*constant*cosdecay\")\n  hparams.learning_rate_decay_steps = 150000\n  hparams.learning_rate_constant = 2e-4\n  hparams.unidirectional_encoder = True\n  return hparams", "output": "Train CNN/DM with a unidirectional encoder and decoder.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def collect_episodes(local_evaluator=None,\n                     remote_evaluators=[],\n                     timeout_seconds=180):\n    \"\"\"\"\"\"\n\n    pending = [\n        a.apply.remote(lambda ev: ev.get_metrics()) for a in remote_evaluators\n    ]\n    collected, _ = ray.wait(\n        pending, num_returns=len(pending), timeout=timeout_seconds * 1.0)\n    num_metric_batches_dropped = len(pending) - len(collected)\n    if pending and len(collected) == 0:\n        raise ValueError(\n            \"Timed out waiting for metrics from workers. You can configure \"\n            \"this timeout with `collect_metrics_timeout`.\")\n\n    metric_lists = ray_get_and_free(collected)\n    if local_evaluator:\n        metric_lists.append(local_evaluator.get_metrics())\n    episodes = []\n    for metrics in metric_lists:\n        episodes.extend(metrics)\n    return episodes, num_metric_batches_dropped", "output": "Gathers new episodes metrics tuples from the given evaluators.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_ca_signed_key(ca_name,\n                      CN='localhost',\n                      as_text=False,\n                      cacert_path=None,\n                      key_filename=None):\n    '''\n    \n    '''\n    set_ca_path(cacert_path)\n    if not key_filename:\n        key_filename = CN\n\n    keyp = '{0}/{1}/certs/{2}.key'.format(\n        cert_base_path(),\n        ca_name,\n        key_filename)\n    if not os.path.exists(keyp):\n        raise ValueError('Certificate does not exists for {0}'.format(CN))\n    else:\n        if as_text:\n            with salt.utils.files.fopen(keyp) as fic:\n                keyp = salt.utils.stringutils.to_unicode(fic.read())\n    return keyp", "output": "Get the certificate path or content\n\n    ca_name\n        name of the CA\n    CN\n        common name of the certificate\n    as_text\n        if true, return the certificate content instead of the path\n    cacert_path\n        absolute path to certificates root directory\n    key_filename\n        alternative filename for the key, useful when using special characters\n\n        .. versionadded:: 2015.5.3\n\n        in the CN\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' tls.get_ca_signed_key \\\n                test_ca CN=localhost \\\n                as_text=False \\\n                cacert_path=/etc/certs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_virtual_environment(path):\n    \"\"\"\n    \"\"\"\n    if not path.is_dir():\n        return False\n    for bindir_name in ('bin', 'Scripts'):\n        for python in path.joinpath(bindir_name).glob('python*'):\n            try:\n                exeness = python.is_file() and os.access(str(python), os.X_OK)\n            except OSError:\n                exeness = False\n            if exeness:\n                return True\n    return False", "output": "Check if a given path is a virtual environment's root.\n\n    This is done by checking if the directory contains a Python executable in\n    its bin/Scripts directory. Not technically correct, but good enough for\n    general usage.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_binary_mathfunction(name, doc=\"\"):\n    \"\"\" \"\"\"\n    def _(col1, col2):\n        sc = SparkContext._active_spark_context\n        # For legacy reasons, the arguments here can be implicitly converted into floats,\n        # if they are not columns or strings.\n        if isinstance(col1, Column):\n            arg1 = col1._jc\n        elif isinstance(col1, basestring):\n            arg1 = _create_column_from_name(col1)\n        else:\n            arg1 = float(col1)\n\n        if isinstance(col2, Column):\n            arg2 = col2._jc\n        elif isinstance(col2, basestring):\n            arg2 = _create_column_from_name(col2)\n        else:\n            arg2 = float(col2)\n\n        jc = getattr(sc._jvm.functions, name)(arg1, arg2)\n        return Column(jc)\n    _.__name__ = name\n    _.__doc__ = doc\n    return _", "output": "Create a binary mathfunction by name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def highlight_block(self, text):\r\n        \"\"\"\"\"\"\r\n        text = to_text_string(text)\r\n        if text.startswith(\"+++\"):\r\n            self.setFormat(0, len(text), self.formats[\"keyword\"])\r\n        elif text.startswith(\"---\"):\r\n            self.setFormat(0, len(text), self.formats[\"keyword\"])\r\n        elif text.startswith(\"+\"):\r\n            self.setFormat(0, len(text), self.formats[\"string\"])\r\n        elif text.startswith(\"-\"):\r\n            self.setFormat(0, len(text), self.formats[\"number\"])\r\n        elif text.startswith(\"@\"):\r\n            self.setFormat(0, len(text), self.formats[\"builtin\"])\r\n        \r\n        self.highlight_spaces(text)", "output": "Implement highlight specific Diff/Patch files.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _should_queue(self, link, referrer, rel):\n        \"\"\"\n        \n        \"\"\"\n        scheme, netloc, path, _, _, _ = urlparse(link)\n        if path.endswith(self.source_extensions + self.binary_extensions +\n                         self.excluded_extensions):\n            result = False\n        elif self.skip_externals and not link.startswith(self.base_url):\n            result = False\n        elif not referrer.startswith(self.base_url):\n            result = False\n        elif rel not in ('homepage', 'download'):\n            result = False\n        elif scheme not in ('http', 'https', 'ftp'):\n            result = False\n        elif self._is_platform_dependent(link):\n            result = False\n        else:\n            host = netloc.split(':', 1)[0]\n            if host.lower() == 'localhost':\n                result = False\n            else:\n                result = True\n        logger.debug('should_queue: %s (%s) from %s -> %s', link, rel,\n                     referrer, result)\n        return result", "output": "Determine whether a link URL from a referring page and with a\n        particular \"rel\" attribute should be queued for scraping.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def count(self):\n        \"\"\"\n        \n        \"\"\"\n        if hasattr(self, '_response'):\n            return self._response.hits.total\n\n        es = connections.get_connection(self._using)\n\n        d = self.to_dict(count=True)\n        # TODO: failed shards detection\n        return es.count(\n            index=self._index,\n            body=d,\n            **self._params\n        )['count']", "output": "Return the number of hits matching the query and filters. Note that\n        only the actual number is returned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imports(symbol_file, input_names, param_file=None, ctx=None):\n        \"\"\"\n        \"\"\"\n        sym = symbol.load(symbol_file)\n        if isinstance(input_names, str):\n            input_names = [input_names]\n        inputs = [symbol.var(i) for i in input_names]\n        ret = SymbolBlock(sym, inputs)\n        if param_file is not None:\n            ret.collect_params().load(param_file, ctx=ctx)\n        return ret", "output": "Import model previously saved by `HybridBlock.export` or\n        `Module.save_checkpoint` as a SymbolBlock for use in Gluon.\n\n        Parameters\n        ----------\n        symbol_file : str\n            Path to symbol file.\n        input_names : list of str\n            List of input variable names\n        param_file : str, optional\n            Path to parameter file.\n        ctx : Context, default None\n            The context to initialize SymbolBlock on.\n\n        Returns\n        -------\n        SymbolBlock\n            SymbolBlock loaded from symbol and parameter files.\n\n        Examples\n        --------\n        >>> net1 = gluon.model_zoo.vision.resnet18_v1(\n        ...     prefix='resnet', pretrained=True)\n        >>> net1.hybridize()\n        >>> x = mx.nd.random.normal(shape=(1, 3, 32, 32))\n        >>> out1 = net1(x)\n        >>> net1.export('net1', epoch=1)\n        >>>\n        >>> net2 = gluon.SymbolBlock.imports(\n        ...     'net1-symbol.json', ['data'], 'net1-0001.params')\n        >>> out2 = net2(x)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_stock_type(stock_code):\n    \"\"\"'\"\"\"\n    stock_code = str(stock_code)\n    if stock_code.startswith((\"sh\", \"sz\")):\n        return stock_code[:2]\n    if stock_code.startswith(\n        (\"50\", \"51\", \"60\", \"73\", \"90\", \"110\", \"113\", \"132\", \"204\", \"78\")\n    ):\n        return \"sh\"\n    if stock_code.startswith(\n        (\"00\", \"13\", \"18\", \"15\", \"16\", \"18\", \"20\", \"30\", \"39\", \"115\", \"1318\")\n    ):\n        return \"sz\"\n    if stock_code.startswith((\"5\", \"6\", \"9\")):\n        return \"sh\"\n    return \"sz\"", "output": "\u5224\u65ad\u80a1\u7968ID\u5bf9\u5e94\u7684\u8bc1\u5238\u5e02\u573a\n    \u5339\u914d\u89c4\u5219\n    ['50', '51', '60', '90', '110'] \u4e3a sh\n    ['00', '13', '18', '15', '16', '18', '20', '30', '39', '115'] \u4e3a sz\n    ['5', '6', '9'] \u5f00\u5934\u7684\u4e3a sh\uff0c \u5176\u4f59\u4e3a sz\n    :param stock_code:\u80a1\u7968ID, \u82e5\u4ee5 'sz', 'sh' \u5f00\u5934\u76f4\u63a5\u8fd4\u56de\u5bf9\u5e94\u7c7b\u578b\uff0c\u5426\u5219\u4f7f\u7528\u5185\u7f6e\u89c4\u5219\u5224\u65ad\n    :return 'sh' or 'sz", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def output(data, **kwargs):  # pylint: disable=unused-argument\n    '''\n    \n    '''\n    high_out = __salt__['highstate'](data)\n    return subprocess.check_output(['ponysay', salt.utils.data.decode(high_out)])", "output": "Mane function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __get_cfg_pkgs_rpm(self):\n        '''\n        \n        '''\n        out, err = self._syscall('rpm', None, None, '-qa', '--configfiles',\n                                 '--queryformat', '%{name}-%{version}-%{release}\\\\n')\n        data = dict()\n        pkg_name = None\n        pkg_configs = []\n\n        out = salt.utils.stringutils.to_str(out)\n        for line in out.split(os.linesep):\n            line = line.strip()\n            if not line:\n                continue\n            if not line.startswith(\"/\"):\n                if pkg_name and pkg_configs:\n                    data[pkg_name] = pkg_configs\n                pkg_name = line\n                pkg_configs = []\n            else:\n                pkg_configs.append(line)\n\n        if pkg_name and pkg_configs:\n            data[pkg_name] = pkg_configs\n\n        return data", "output": "Get packages with configuration files on RPM systems.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def root_mean_squared_error(pred:Tensor, targ:Tensor)->Rank0Tensor:\n    \"\"\n    pred,targ = flatten_check(pred,targ)\n    return torch.sqrt(F.mse_loss(pred, targ))", "output": "Root mean squared error between `pred` and `targ`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def CEscape(text, as_utf8):\n  \"\"\"\n  \"\"\"\n  # PY3 hack: make Ord work for str and bytes:\n  # //platforms/networking/data uses unicode here, hence basestring.\n  Ord = ord if isinstance(text, six.string_types) else lambda x: x\n  if as_utf8:\n    return ''.join(_cescape_utf8_to_str[Ord(c)] for c in text)\n  return ''.join(_cescape_byte_to_str[Ord(c)] for c in text)", "output": "Escape a bytes string for use in an ascii protocol buffer.\n\n  text.encode('string_escape') does not seem to satisfy our needs as it\n  encodes unprintable characters using two-digit hex escapes whereas our\n  C++ unescaping function allows hex escapes to be any length.  So,\n  \"\\0011\".encode('string_escape') ends up being \"\\\\x011\", which will be\n  decoded in C++ as a single-character string with char code 0x11.\n\n  Args:\n    text: A byte string to be escaped\n    as_utf8: Specifies if result should be returned in UTF-8 encoding\n  Returns:\n    Escaped string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absent(\n        name,\n        region,\n        user=None,\n        opts=False):\n    '''\n    \n    '''\n    ret = {'name': name, 'result': True, 'comment': '', 'changes': {}}\n\n    does_exist = __salt__['aws_sqs.queue_exists'](name, region, opts, user)\n\n    if does_exist:\n        if __opts__['test']:\n            ret['result'] = None\n            ret['comment'] = 'AWS SQS queue {0} is set to be removed'.format(\n                    name)\n            return ret\n        removed = __salt__['aws_sqs.delete_queue'](name, region, opts, user)\n        if removed['retcode'] == 0:\n            ret['changes']['removed'] = removed['stdout']\n        else:\n            ret['result'] = False\n            ret['comment'] = removed['stderr']\n    else:\n        ret['comment'] = '{0} does not exist in {1}'.format(name, region)\n\n    return ret", "output": "Remove the named SQS queue if it exists.\n\n    name\n        Name of the SQS queue.\n\n    region\n        Region to remove the queue from\n\n    user\n        Name of the user performing the SQS operations\n\n    opts\n        Include additional arguments and options to the aws command line", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_upsample(builder, layer, input_names, output_names, keras_layer):\n    \"\"\"\n    \n    \"\"\"\n    _check_data_format(keras_layer)\n    # Get input and output names\n    input_name, output_name = (input_names[0], output_names[0])\n\n    is_1d = isinstance(keras_layer, _keras.layers.UpSampling1D)\n\n    # Currently, we only support upsample of same dims\n    fh = fw = 1\n    if is_1d:\n        if type(keras_layer.size) is tuple and len(keras_layer.size) == 1:\n            fh, fw = 1, keras_layer.size[0]\n        elif type(keras_layer.size) is int:\n            fh, fw = 1, keras_layer.size\n        else:\n            raise ValueError(\"Unrecognized upsample factor format %s\" % (str(keras_layer.size)))\n    else:\n        if type(keras_layer.size) is int:\n            fh = fw = keras_layer.size\n        elif len(keras_layer.size) == 2:\n            if keras_layer.size[0] != keras_layer.size[1]:\n                raise ValueError(\"Upsample with different rows and columns not supported.\")\n            else:\n                fh = keras_layer.size[0]\n                fw = keras_layer.size[1]\n        else:\n            raise ValueError(\"Unrecognized upsample factor format %s\" % (str(keras_layer.size)))\n\n    builder.add_upsample(name = layer,\n             scaling_factor_h = fh,\n             scaling_factor_w = fw,\n             input_name = input_name,\n             output_name = output_name)", "output": "Convert convolution layer from keras to coreml.\n\n    Parameters\n    ----------\n    keras_layer: layer\n        A keras layer object.\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_typed_parameter_typed_value(values):\n    '''\n    \n    '''\n    type_, value = _expand_one_key_dictionary(values)\n\n    _current_parameter_value.type = type_\n    if _is_simple_type(value):\n        arg = Argument(value)\n        _current_parameter_value.add_argument(arg)\n    elif isinstance(value, list):\n        for idx in value:\n            arg = Argument(idx)\n            _current_parameter_value.add_argument(arg)", "output": "Creates Arguments in a TypedParametervalue.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_data(self):\r\n        \"\"\"\r\n        \"\"\"\r\n\r\n        for index in reversed(self.stack_history):\r\n            text = self.tabs.tabText(index)\r\n            text = text.replace('&', '')\r\n            item = QListWidgetItem(ima.icon('TextFileIcon'), text)\r\n            self.addItem(item)", "output": "Fill ListWidget with the tabs texts.\r\n\r\n        Add elements in inverse order of stack_history.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def GroupSizer(field_number, is_repeated, is_packed):\n  \"\"\"\"\"\"\n\n  tag_size = _TagSize(field_number) * 2\n  assert not is_packed\n  if is_repeated:\n    def RepeatedFieldSize(value):\n      result = tag_size * len(value)\n      for element in value:\n        result += element.ByteSize()\n      return result\n    return RepeatedFieldSize\n  else:\n    def FieldSize(value):\n      return tag_size + value.ByteSize()\n    return FieldSize", "output": "Returns a sizer for a group field.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cancel_entrust(self, entrust_no):\n        \"\"\"\n        \n        \"\"\"\n        xq_entrust_list = self._get_xq_history()\n        is_have = False\n        for xq_entrusts in xq_entrust_list:\n            status = xq_entrusts[\"status\"]  # \u8c03\u4ed3\u72b6\u6001\n            for entrust in xq_entrusts[\"rebalancing_histories\"]:\n                if entrust[\"id\"] == entrust_no and status == \"pending\":\n                    is_have = True\n                    buy_or_sell = (\n                        \"buy\"\n                        if entrust[\"target_weight\"] < entrust[\"weight\"]\n                        else \"sell\"\n                    )\n                    if (\n                        entrust[\"target_weight\"] == 0\n                        and entrust[\"weight\"] == 0\n                    ):\n                        raise exceptions.TradeError(u\"\u79fb\u9664\u7684\u80a1\u7968\u64cd\u4f5c\u65e0\u6cd5\u64a4\u9500,\u5efa\u8bae\u91cd\u65b0\u4e70\u5165\")\n                    balance = self.get_balance()[0]\n                    volume = (\n                        abs(entrust[\"target_weight\"] - entrust[\"weight\"])\n                        * balance[\"asset_balance\"]\n                        / 100\n                    )\n                    r = self._trade(\n                        security=entrust[\"stock_symbol\"],\n                        volume=volume,\n                        entrust_bs=buy_or_sell,\n                    )\n                    if len(r) > 0 and \"error_info\" in r[0]:\n                        raise exceptions.TradeError(\n                            u\"\u64a4\u9500\u5931\u8d25!%s\" % (\"error_info\" in r[0])\n                        )\n        if not is_have:\n            raise exceptions.TradeError(u\"\u64a4\u9500\u5bf9\u8c61\u5df2\u5931\u6548\")\n        return True", "output": "\u5bf9\u672a\u6210\u4ea4\u7684\u8c03\u4ed3\u8fdb\u884c\u4f2a\u64a4\u5355\n        :param entrust_no:\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def diff_mtime_map(map1, map2):\n    '''\n    \n    '''\n    # check if the mtimes are the same\n    if sorted(map1) != sorted(map2):\n        return True\n\n    # map1 and map2 are guaranteed to have same keys,\n    # so compare mtimes\n    for filename, mtime in six.iteritems(map1):\n        if map2[filename] != mtime:\n            return True\n\n    # we made it, that means we have no changes\n    return False", "output": "Is there a change to the mtime map? return a boolean", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getenv(option, default=undefined, cast=undefined):\n    \"\"\"\n    \n    \"\"\"\n\n    # We can't avoid __contains__ because value may be empty.\n    if option in os.environ:\n        value = os.environ[option]\n    else:\n        if isinstance(default, Undefined):\n            raise UndefinedValueError('{} not found. Declare it as envvar or define a default value.'.format(option))\n\n        value = default\n\n    if isinstance(cast, Undefined):\n        return value\n\n    if cast is bool:\n        value = _cast_boolean(value)\n    elif cast is list:\n        value = [x for x in value.split(',') if x]\n    else:\n        value = cast(value)\n\n    return value", "output": "Return the value for option or default if defined.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _process_all_any(self, func, **kwargs):\n        \"\"\"\n        \"\"\"\n        axis = kwargs.get(\"axis\", 0)\n        axis = 0 if axis is None else axis\n        kwargs[\"axis\"] = axis\n        builder_func = self._build_mapreduce_func(func, **kwargs)\n        return self._full_reduce(axis, builder_func)", "output": "Calculates if any or all the values are true.\n\n        Return:\n            A new QueryCompiler object containing boolean values or boolean.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _bar_status(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.isnan or np.isnan(self.limit_up):\n            return BAR_STATUS.ERROR\n        if self.close >= self.limit_up:\n            return BAR_STATUS.LIMIT_UP\n        if self.close <= self.limit_down:\n            return BAR_STATUS.LIMIT_DOWN\n        return BAR_STATUS.NORMAL", "output": "WARNING: \u83b7\u53d6 bar_status \u6bd4\u8f83\u8017\u8d39\u6027\u80fd\uff0c\u800c\u4e14\u662flazy_compute\uff0c\u56e0\u6b64\u4e0d\u8981\u591a\u6b21\u8c03\u7528\uff01\uff01\uff01\uff01", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def product_set_path(cls, project, location, product_set):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/locations/{location}/productSets/{product_set}\",\n            project=project,\n            location=location,\n            product_set=product_set,\n        )", "output": "Return a fully-qualified product_set string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_group_estimate(call=None, for_output=True, **kwargs):\n    '''\n    \n    '''\n    for key, value in kwargs.items():\n        group = \"\"\n        location = \"\"\n        if key == \"group\":\n            group = value\n        if key == \"location\":\n            location = value\n    creds = get_creds()\n    clc.v1.SetCredentials(creds[\"token\"], creds[\"token_pass\"])\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The list_nodes_full function must be called with -f or --function.'\n        )\n    try:\n        billing_raw = clc.v1.Billing.GetGroupEstimate(group=group, alias=creds[\"accountalias\"], location=location)\n        billing_raw = salt.utils.json.dumps(billing_raw)\n        billing = salt.utils.json.loads(billing_raw)\n        estimate = round(billing[\"MonthlyEstimate\"], 2)\n        month_to_date = round(billing[\"MonthToDate\"], 2)\n        return {\"Monthly Estimate\": estimate, \"Month to Date\": month_to_date}\n    except RuntimeError:\n        return 0", "output": "Return a list of the VMs that are on the provider\n    usage: \"salt-cloud -f get_group_estimate clc group=Dev location=VA1\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _map_across_full_axis_select_indices(\n        self, axis, func, indices, keep_remaining=False\n    ):\n        \"\"\"\n        \"\"\"\n        return self.data.apply_func_to_select_indices_along_full_axis(\n            axis, func, indices, keep_remaining\n        )", "output": "Maps function to select indices along full axis.\n\n        Args:\n            axis: 0 for columns and 1 for rows.\n            func: Callable mapping function over the BlockParitions.\n            indices: indices along axis to map over.\n            keep_remaining: True if keep indices where function was not applied.\n\n        Returns:\n            BaseFrameManager containing the result of mapping func over axis on indices.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def junos_facts(**kwargs):\n    '''\n    \n    '''\n    prep = _junos_prep_fun(napalm_device)  # pylint: disable=undefined-variable\n    if not prep['result']:\n        return prep\n    facts = dict(napalm_device['DRIVER'].device.facts)  # pylint: disable=undefined-variable\n    if 'version_info' in facts:\n        facts['version_info'] = \\\n            dict(facts['version_info'])\n    # For backward compatibility. 'junos_info' is present\n    # only of in newer versions of facts.\n    if 'junos_info' in facts:\n        for re in facts['junos_info']:\n            facts['junos_info'][re]['object'] = \\\n                dict(facts['junos_info'][re]['object'])\n    return facts", "output": ".. versionadded:: 2019.2.0\n\n    The complete list of Junos facts collected by ``junos-eznc``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm.junos_facts", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_image_path(name, default=\"not_found.png\"):\r\n    \"\"\"\"\"\"\r\n    for img_path in IMG_PATH:\r\n        full_path = osp.join(img_path, name)\r\n        if osp.isfile(full_path):\r\n            return osp.abspath(full_path)\r\n    if default is not None:\r\n        img_path = osp.join(get_module_path('spyder'), 'images')\r\n        return osp.abspath(osp.join(img_path, default))", "output": "Return image absolute path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def str_strip(arr, to_strip=None, side='both'):\n    \"\"\"\n    \n    \"\"\"\n    if side == 'both':\n        f = lambda x: x.strip(to_strip)\n    elif side == 'left':\n        f = lambda x: x.lstrip(to_strip)\n    elif side == 'right':\n        f = lambda x: x.rstrip(to_strip)\n    else:  # pragma: no cover\n        raise ValueError('Invalid side')\n    return _na_map(f, arr)", "output": "Strip whitespace (including newlines) from each string in the\n    Series/Index.\n\n    Parameters\n    ----------\n    to_strip : str or unicode\n    side : {'left', 'right', 'both'}, default 'both'\n\n    Returns\n    -------\n    Series or Index", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pause_trial(self, trial):\n        \"\"\"\n        \"\"\"\n\n        trial_future = self._find_item(self._running, trial)\n        if trial_future:\n            self._paused[trial_future[0]] = trial\n        super(RayTrialExecutor, self).pause_trial(trial)", "output": "Pauses the trial.\n\n        If trial is in-flight, preserves return value in separate queue\n        before pausing, which is restored when Trial is resumed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ffill_query_in_range(expr,\n                         lower,\n                         upper,\n                         checkpoints=None,\n                         odo_kwargs=None,\n                         ts_field=TS_FIELD_NAME):\n    \"\"\"\n    \"\"\"\n    odo_kwargs = odo_kwargs or {}\n    computed_lower, materialized_checkpoints = get_materialized_checkpoints(\n        checkpoints,\n        expr.fields,\n        lower,\n        odo_kwargs,\n    )\n\n    pred = expr[ts_field] <= upper\n\n    if computed_lower is not None:\n        # only constrain the lower date if we computed a new lower date\n        pred &= expr[ts_field] >= computed_lower\n\n    raw = pd.concat(\n        (\n            materialized_checkpoints,\n            odo(\n                expr[pred],\n                pd.DataFrame,\n                **odo_kwargs\n            ),\n        ),\n        ignore_index=True,\n    )\n    raw.loc[:, ts_field] = raw.loc[:, ts_field].astype('datetime64[ns]')\n    return raw", "output": "Query a blaze expression in a given time range properly forward filling\n    from values that fall before the lower date.\n\n    Parameters\n    ----------\n    expr : Expr\n        Bound blaze expression.\n    lower : datetime\n        The lower date to query for.\n    upper : datetime\n        The upper date to query for.\n    checkpoints : Expr, optional\n        Bound blaze expression for a checkpoints table from which to get a\n        computed lower bound.\n    odo_kwargs : dict, optional\n        The extra keyword arguments to pass to ``odo``.\n    ts_field : str, optional\n        The name of the timestamp field in the given blaze expression.\n\n    Returns\n    -------\n    raw : pd.DataFrame\n        A strict dataframe for the data in the given date range. This may\n        start before the requested start date if a value is needed to ffill.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def jdbc_datasource_absent(name, both=True, server=None):\n    '''\n    \n    '''\n    ret = {'name': name, 'result': None, 'comment': None, 'changes': {}}\n    pool_ret = _do_element_absent(name, 'jdbc_connection_pool', {'cascade': both}, server)\n\n    if not pool_ret['error']:\n        if __opts__['test'] and pool_ret['delete']:\n            ret['comment'] = 'JDBC Datasource set to be deleted'\n        elif pool_ret['delete']:\n            ret['result'] = True\n            ret['comment'] = 'JDBC Datasource deleted'\n        else:\n            ret['result'] = True\n            ret['comment'] = 'JDBC Datasource doesn\\'t exist'\n    else:\n        ret['result'] = False\n        ret['comment'] = 'Error: {0}'.format(pool_ret['error'])\n    return ret", "output": "Ensures the JDBC Datasource doesn't exists\n\n    name\n        Name of the datasource\n    both\n        Delete both the pool and the resource, defaults to ``true``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def call_for_each_tower(self, tower_fn):\n        \"\"\"\n        \n        \"\"\"\n        # if tower_fn returns [(grad, var), ...], this returns #GPU x #VAR x 2\n        return DataParallelBuilder.build_on_towers(\n            self.towers,\n            tower_fn,\n            # use no variable scope for the first tower\n            use_vs=[False] + [True] * (len(self.towers) - 1))", "output": "Call the function `tower_fn` under :class:`TowerContext` for each tower.\n\n        Returns:\n            a list, contains the return values of `tower_fn` on each tower.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_ciphers(self):\n        \"\"\"\n        \n        \"\"\"\n        ciphers = (Security.SSLCipherSuite * len(CIPHER_SUITES))(*CIPHER_SUITES)\n        result = Security.SSLSetEnabledCiphers(\n            self.context, ciphers, len(CIPHER_SUITES)\n        )\n        _assert_no_error(result)", "output": "Sets up the allowed ciphers. By default this matches the set in\n        util.ssl_.DEFAULT_CIPHERS, at least as supported by macOS. This is done\n        custom and doesn't allow changing at this time, mostly because parsing\n        OpenSSL cipher strings is going to be a freaking nightmare.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def corrgroups60(display=False):\n    \"\"\" \n    \"\"\"\n\n    # set a constant seed\n    old_seed = np.random.seed()\n    np.random.seed(0)\n\n    # generate dataset with known correlation\n    N = 1000\n    M = 60\n\n    # set one coefficent from each group of 3 to 1\n    beta = np.zeros(M)\n    beta[0:30:3] = 1\n\n    # build a correlation matrix with groups of 3 tightly correlated features\n    C = np.eye(M)\n    for i in range(0,30,3):\n        C[i,i+1] = C[i+1,i] = 0.99\n        C[i,i+2] = C[i+2,i] = 0.99\n        C[i+1,i+2] = C[i+2,i+1] = 0.99\n    f = lambda X: np.matmul(X, beta)\n\n    # Make sure the sample correlation is a perfect match\n    X_start = np.random.randn(N, M)\n    X_centered = X_start - X_start.mean(0)\n    Sigma = np.matmul(X_centered.T, X_centered) / X_centered.shape[0]\n    W = np.linalg.cholesky(np.linalg.inv(Sigma)).T\n    X_white = np.matmul(X_centered, W.T)\n    assert np.linalg.norm(np.corrcoef(np.matmul(X_centered, W.T).T) - np.eye(M)) < 1e-6 # ensure this decorrelates the data\n\n    # create the final data\n    X_final = np.matmul(X_white, np.linalg.cholesky(C).T)\n    X = X_final\n    y = f(X) + np.random.randn(N) * 1e-2\n\n    # restore the previous numpy random seed\n    np.random.seed(old_seed)\n\n    return pd.DataFrame(X), y", "output": "Correlated Groups 60\n    \n    A simulated dataset with tight correlations among distinct groups of features.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rows(self):\n        \"\"\"\n        \n        \"\"\"\n        with self.input().open('r') as fobj:\n            for line in fobj:\n                yield line.strip('\\n').split('\\t')", "output": "Return/yield tuples or lists corresponding to each row to be inserted.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_custom_resource_definition(self, name, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_custom_resource_definition_with_http_info(name, **kwargs)\n        else:\n            (data) = self.read_custom_resource_definition_with_http_info(name, **kwargs)\n            return data", "output": "read the specified CustomResourceDefinition\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_custom_resource_definition(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the CustomResourceDefinition (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'. Deprecated. Planned for removal in 1.18.\n        :param bool export: Should this value be exported.  Export strips fields that a user can not specify. Deprecated. Planned for removal in 1.18.\n        :return: V1beta1CustomResourceDefinition\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_function_node(node, annotation):\n    \"\"\"\n    \"\"\"\n    target, funcs = parse_nni_function(annotation)\n    FuncReplacer(funcs, target).visit(node)\n    return node", "output": "Replace a node annotated by `nni.function_choice`.\n    node: the AST node to replace\n    annotation: annotation string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_init_score(self):\n        \"\"\"\n        \"\"\"\n        if self.init_score is None:\n            self.init_score = self.get_field('init_score')\n        return self.init_score", "output": "Get the initial score of the Dataset.\n\n        Returns\n        -------\n        init_score : numpy array or None\n            Init score of Booster.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def schemaNewParserCtxt(URL):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlSchemaNewParserCtxt(URL)\n    if ret is None:raise parserError('xmlSchemaNewParserCtxt() failed')\n    return SchemaParserCtxt(_obj=ret)", "output": "Create an XML Schemas parse context for that file/resource\n       expected to contain an XML Schemas file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def initialize(self, init=initializer.Uniform(), ctx=None, verbose=False,\n                   force_reinit=False):\n        \"\"\"\n        \"\"\"\n        if verbose:\n            init.set_verbosity(verbose=verbose)\n        for _, v in self.items():\n            v.initialize(None, ctx, init, force_reinit=force_reinit)", "output": "Initializes all Parameters managed by this dictionary to be used for :py:class:`NDArray`\n        API. It has no effect when using :py:class:`Symbol` API.\n\n        Parameters\n        ----------\n        init : Initializer\n            Global default Initializer to be used when :py:meth:`Parameter.init` is ``None``.\n            Otherwise, :py:meth:`Parameter.init` takes precedence.\n        ctx : Context or list of Context\n            Keeps a copy of Parameters on one or many context(s).\n        verbose : bool, default False\n            Whether to verbosely print out details on initialization.\n        force_reinit : bool, default False\n            Whether to force re-initialization if parameter is already initialized.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def json(self, *, encoding: str=None,\n                   loads: JSONDecoder=DEFAULT_JSON_DECODER,\n                   content_type: Optional[str]='application/json') -> Any:\n        \"\"\"\"\"\"\n        if self._body is None:\n            await self.read()\n\n        if content_type:\n            ctype = self.headers.get(hdrs.CONTENT_TYPE, '').lower()\n            if not _is_expected_content_type(ctype, content_type):\n                raise ContentTypeError(\n                    self.request_info,\n                    self.history,\n                    message=('Attempt to decode JSON with '\n                             'unexpected mimetype: %s' % ctype),\n                    headers=self.headers)\n\n        if encoding is None:\n            encoding = self.get_encoding()\n\n        return loads(self._body.decode(encoding))", "output": "Read and decodes JSON response.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _prepare_group2ctxs(group2ctxs, ctx_len):\n    \"\"\"\n    \"\"\"\n    if group2ctxs is None:\n        return [None] * ctx_len\n    elif isinstance(group2ctxs, list):\n        assert(len(group2ctxs) == ctx_len), \"length of group2ctxs\\\n            should be %d\" % ctx_len\n        return group2ctxs\n    elif isinstance(group2ctxs, dict):\n        ret = [{} for i in range(ctx_len)]\n        for k, v in group2ctxs.items():\n            ctxs = None\n            if isinstance(v, ctx.Context):\n                ctxs = [v] * ctx_len\n            else:\n                if len(v) == 1:\n                    ctxs = v * ctx_len\n                else:\n                    assert(len(v) == ctx_len), \"length of group2ctxs[%s]\\\n                        should be %d or 1\" % (k, ctx_len)\n                    ctxs = v\n            for i in range(ctx_len):\n                ret[i][k] = ctxs[i]\n        return ret\n    else:\n        assert(False), \"group2ctxs should be list of dict of str to context,\\\n            or dict of str to context or list of context\"\n        return False", "output": "Prepare the group2contexts, will duplicate the context\n    if some ctx_group map to only one context.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_dashboard(self):\n        \"\"\"\"\"\"\n        stdout_file, stderr_file = self.new_log_files(\"dashboard\", True)\n        self._webui_url, process_info = ray.services.start_dashboard(\n            self.redis_address,\n            self._temp_dir,\n            stdout_file=stdout_file,\n            stderr_file=stderr_file,\n            redis_password=self._ray_params.redis_password)\n        assert ray_constants.PROCESS_TYPE_DASHBOARD not in self.all_processes\n        if process_info is not None:\n            self.all_processes[ray_constants.PROCESS_TYPE_DASHBOARD] = [\n                process_info\n            ]\n            redis_client = self.create_redis_client()\n            redis_client.hmset(\"webui\", {\"url\": self._webui_url})", "output": "Start the dashboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def multiply(self, a, b):\n    \"\"\"\n    \n    \"\"\"\n    if a is None or b is None: return None\n    m, n, l = len(a), len(b[0]), len(b[0])\n    if len(b) != n:\n        raise Exception(\"A's column number must be equal to B's row number.\")\n    c = [[0 for _ in range(l)] for _ in range(m)]\n    for i, row in enumerate(a):\n        for k, eleA in enumerate(row):\n            if eleA:\n                for j, eleB in enumerate(b[k]):\n                    if eleB: c[i][j] += eleA * eleB\n    return c", "output": ":type A: List[List[int]]\n    :type B: List[List[int]]\n    :rtype: List[List[int]]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def accuracy_thresh(y_pred:Tensor, y_true:Tensor, thresh:float=0.5, sigmoid:bool=True)->Rank0Tensor:\n    \"\"\n    if sigmoid: y_pred = y_pred.sigmoid()\n    return ((y_pred>thresh)==y_true.byte()).float().mean()", "output": "Compute accuracy when `y_pred` and `y_true` are the same size.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def roll_forward(self, dt):\n        \"\"\"\n        \n        \"\"\"\n        dt = pd.Timestamp(dt, tz='UTC')\n\n        trading_days = self.all_sessions()\n        try:\n            return trading_days[trading_days.searchsorted(dt)]\n        except IndexError:\n            raise ValueError(\n                \"Date {} was past the last session for domain {}. \"\n                \"The last session for this domain is {}.\".format(\n                    dt.date(),\n                    self,\n                    trading_days[-1].date()\n                )\n            )", "output": "Given a date, align it to the calendar of the pipeline's domain.\n\n        Parameters\n        ----------\n        dt : pd.Timestamp\n\n        Returns\n        -------\n        pd.Timestamp", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self, iteration, fobj):\n        \"\"\"\"\"\"\"\n        self.bst.update(self.dtrain, iteration, fobj)", "output": "Update the boosters for one iteration", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _detect_bom(input):\n    \"\"\"\"\"\"\n    if input.startswith(b'\\xFF\\xFE'):\n        return _UTF16LE, input[2:]\n    if input.startswith(b'\\xFE\\xFF'):\n        return _UTF16BE, input[2:]\n    if input.startswith(b'\\xEF\\xBB\\xBF'):\n        return UTF8, input[3:]\n    return None, input", "output": "Return (bom_encoding, input), with any BOM removed from the input.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def paged_object_to_list(paged_object):\n    '''\n    \n    '''\n    paged_return = []\n    while True:\n        try:\n            page = next(paged_object)\n            paged_return.append(page.as_dict())\n        except CloudError:\n            raise\n        except StopIteration:\n            break\n\n    return paged_return", "output": "Extract all pages within a paged object as a list of dictionaries", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fn_device_dependency(name, device=\"\"):\n  \"\"\"\"\"\"\n  key = name + \"_\" + device\n  outs = []\n\n  def body():\n    with tf.control_dependencies(fn_device_dependency_dict()[key]):\n      yield outs\n      assert outs\n\n      deps = outs\n      if isinstance(outs[0], (list, tuple)):\n        assert len(outs) == 1\n        deps = outs[0]\n      fn_device_dependency_dict()[key] = deps\n\n  if device:\n    with tf.device(device):\n      return body()\n  else:\n    return body()", "output": "Add control deps for name and device.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def envdict2listdict(envdict):\r\n    \"\"\"\"\"\"\r\n    sep = os.path.pathsep\r\n    for key in envdict:\r\n        if sep in envdict[key]:\r\n            envdict[key] = [path.strip() for path in envdict[key].split(sep)]\r\n    return envdict", "output": "Dict --> Dict of lists", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _train(self, trial):\n        \"\"\"\"\"\"\n\n        assert trial.status == Trial.RUNNING, trial.status\n        remote = trial.runner.train.remote()\n\n        # Local Mode\n        if isinstance(remote, dict):\n            remote = _LocalWrapper(remote)\n\n        self._running[remote] = trial", "output": "Start one iteration of training and save remote id.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def export_coreml(self, filename):\n        \"\"\"\n        \n        \"\"\"\n        print('This model is exported as a custom Core ML model. In order to use it in your\\n'\n              'application, you must also include \"libRecommender.dylib\". For additional\\n'\n              'details see:\\n'\n              'https://apple.github.io/turicreate/docs/userguide/recommender/coreml-deployment.html')\n\n        import turicreate as tc\n        self.__proxy__.export_to_coreml(filename)", "output": "Export the model in Core ML format.\n\n        Parameters\n        ----------\n        filename: str\n          A valid filename where the model can be saved.\n\n        Examples\n        --------\n        >>> model.export_coreml('myModel.mlmodel')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_workspace_value(self, result, assets):\n        \"\"\"\n        \n        \"\"\"\n        return result.unstack().fillna(self.missing_value).reindex(\n            columns=assets,\n            fill_value=self.missing_value,\n        ).values", "output": "Called with a column of the result of a pipeline. This needs to put\n        the data into a format that can be used in a workspace to continue\n        doing computations.\n\n        Parameters\n        ----------\n        result : pd.Series\n            A multiindexed series with (dates, assets) whose values are the\n            results of running this pipeline term over the dates.\n        assets : pd.Index\n            All of the assets being requested. This allows us to correctly\n            shape the workspace value.\n\n        Returns\n        -------\n        workspace_value : array-like\n            An array like value that the engine can consume.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mean(self, axis=0, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        nv.validate_mean(args, kwargs)\n        valid_vals = self._valid_sp_values\n        sp_sum = valid_vals.sum()\n        ct = len(valid_vals)\n\n        if self._null_fill_value:\n            return sp_sum / ct\n        else:\n            nsparse = self.sp_index.ngaps\n            return (sp_sum + self.fill_value * nsparse) / (ct + nsparse)", "output": "Mean of non-NA/null values\n\n        Returns\n        -------\n        mean : float", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stderr_avail(self):\r\n        \"\"\"\"\"\"\r\n        data = self.interpreter.stderr_write.empty_queue()\r\n        if data:\r\n            self.write(data, error=True)\r\n            self.flush(error=True)", "output": "Data is available in stderr, let's empty the queue and write it!", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare_attacks(self):\n    \"\"\"\"\"\"\n    print_header('PREPARING ATTACKS DATA')\n    # verify that attacks data not written yet\n    if not self.ask_when_work_is_populated(self.attack_work):\n      return\n    self.attack_work = eval_lib.AttackWorkPieces(\n        datastore_client=self.datastore_client)\n    # prepare submissions\n    print_header('Initializing submissions')\n    self.submissions.init_from_storage_write_to_datastore()\n    if self.verbose:\n      print(self.submissions)\n    # prepare dataset batches\n    print_header('Initializing dataset batches')\n    self.dataset_batches.init_from_storage_write_to_datastore(\n        batch_size=self.batch_size,\n        allowed_epsilon=ALLOWED_EPS,\n        skip_image_ids=[],\n        max_num_images=self.max_dataset_num_images)\n    if self.verbose:\n      print(self.dataset_batches)\n    # prepare adversarial batches\n    print_header('Initializing adversarial batches')\n    self.adv_batches.init_from_dataset_and_submissions_write_to_datastore(\n        dataset_batches=self.dataset_batches,\n        attack_submission_ids=self.submissions.get_all_attack_ids())\n    if self.verbose:\n      print(self.adv_batches)\n    # prepare work pieces\n    print_header('Preparing attack work pieces')\n    self.attack_work.init_from_adversarial_batches(self.adv_batches.data)\n    self.attack_work.write_all_to_datastore()\n    if self.verbose:\n      print(self.attack_work)", "output": "Prepares all data needed for evaluation of attacks.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _from_any(any_pb):\n    \"\"\"\n    \"\"\"\n    klass = _TYPE_URL_MAP[any_pb.type_url]\n    return klass.FromString(any_pb.value)", "output": "Convert an ``Any`` protobuf into the actual class.\n\n    Uses the type URL to do the conversion.\n\n    .. note::\n\n        This assumes that the type URL is already registered.\n\n    :type any_pb: :class:`google.protobuf.any_pb2.Any`\n    :param any_pb: An any object to be converted.\n\n    :rtype: object\n    :returns: The instance (of the correct type) stored in the any\n              instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_backend_symbol(self, backend):\n        \"\"\"\n        \"\"\"\n        out = SymbolHandle()\n        check_call(_LIB.MXGenBackendSubgraph(self.handle, c_str(backend), ctypes.byref(out)))\n        return Symbol(out)", "output": "Return symbol for target backend.\n\n        Parameters\n        ----------\n        backend : str\n            The backend names.\n\n        Returns\n        -------\n        out : Symbol\n            The created Symbol for target backend.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _project_perturbation(perturbation, epsilon, input_image, clip_min=None,\n                          clip_max=None):\n  \"\"\"\n  \"\"\"\n\n  if clip_min is None or clip_max is None:\n    raise NotImplementedError(\"_project_perturbation currently has clipping \"\n                              \"hard-coded in.\")\n\n  # Ensure inputs are in the correct range\n  with tf.control_dependencies([\n      utils_tf.assert_less_equal(input_image,\n                                 tf.cast(clip_max, input_image.dtype)),\n      utils_tf.assert_greater_equal(input_image,\n                                    tf.cast(clip_min, input_image.dtype))\n  ]):\n    clipped_perturbation = utils_tf.clip_by_value(\n        perturbation, -epsilon, epsilon)\n    new_image = utils_tf.clip_by_value(\n        input_image + clipped_perturbation, clip_min, clip_max)\n    return new_image - input_image", "output": "Project `perturbation` onto L-infinity ball of radius `epsilon`.\n  Also project into hypercube such that the resulting adversarial example\n  is between clip_min and clip_max, if applicable.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_chunks(file, size=io.DEFAULT_BUFFER_SIZE):\n    \"\"\"\"\"\"\n    while True:\n        chunk = file.read(size)\n        if not chunk:\n            break\n        yield chunk", "output": "Yield pieces of data from a file-like object until EOF.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_svc_alias():\n    '''\n    \n    '''\n\n    ret = {}\n    for d in AVAIL_SVR_DIRS:\n        for el in glob.glob(os.path.join(d, '*')):\n            if not os.path.islink(el):\n                continue\n            psvc = os.readlink(el)\n            if not os.path.isabs(psvc):\n                psvc = os.path.join(d, psvc)\n            nsvc = os.path.basename(psvc)\n            if nsvc not in ret:\n                ret[nsvc] = []\n            ret[nsvc].append(el)\n    return ret", "output": "Returns the list of service's name that are aliased and their alias path(s)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_ssh_exe():\n    '''\n    \n    '''\n    # Known locations for Git's ssh.exe in Windows\n    globmasks = [os.path.join(os.getenv('SystemDrive'), os.sep,\n                              'Program Files*', 'Git', 'usr', 'bin',\n                              'ssh.exe'),\n                 os.path.join(os.getenv('SystemDrive'), os.sep,\n                              'Program Files*', 'Git', 'bin',\n                              'ssh.exe')]\n    for globmask in globmasks:\n        ssh_exe = glob.glob(globmask)\n        if ssh_exe and os.path.isfile(ssh_exe[0]):\n            ret = ssh_exe[0]\n            break\n    else:\n        ret = None\n\n    return ret", "output": "Windows only: search for Git's bundled ssh.exe in known locations", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show(ndarray, min_val=None, max_val=None):\n  \"\"\"\n  \n  \"\"\"\n\n  # Create a temporary file with the suffix '.png'.\n  fd, path = mkstemp(suffix='.png')\n  os.close(fd)\n  save(path, ndarray, min_val, max_val)\n  shell_call(VIEWER_COMMAND + [path])", "output": "Display an image.\n  :param ndarray: The image as an ndarray\n  :param min_val: The minimum pixel value in the image format\n  :param max_val: The maximum pixel valie in the image format\n    If min_val and max_val are not specified, attempts to\n    infer whether the image is in any of the common ranges:\n      [0, 1], [-1, 1], [0, 255]\n    This can be ambiguous, so it is better to specify if known.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_pool_member(self, name, port, pool_name):\n        '''\n        \n        '''\n        if not self.check_pool(pool_name):\n            raise CommandExecutionError(\n                '{0} pool does not exists'.format(pool_name)\n            )\n\n        members_seq = self.bigIP.LocalLB.Pool.typefactory.create(\n            'Common.IPPortDefinitionSequence'\n        )\n        members_seq.items = []\n\n        member = self.bigIP.LocalLB.Pool.typefactory.create(\n            'Common.IPPortDefinition'\n        )\n\n        member.address = name\n        member.port = port\n\n        members_seq.items.append(member)\n\n        try:\n            self.bigIP.LocalLB.Pool.add_member(pool_names=[pool_name],\n                                               members=[members_seq])\n        except Exception as e:\n            raise Exception(\n                'Unable to add `{0}` to `{1}`\\n\\n{2}'.format(name,\n                                                             pool_name,\n                                                             e)\n            )\n        return True", "output": "Add a node to a pool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dump_ast(ast, indent=' ', newline='\\n'):\n    '''\n    \n    \n    \n    '''\n    \n    visitor = ASTPrinter(indent=indent, level=0, newline=newline)\n    visitor.visit(ast)\n    return visitor.dumps()", "output": "Returns a string representing the ast.\n    \n    :param ast: the ast to print.\n    :param indent: how far to indent a newline.\n    :param newline: The newline character.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_header(self, name: str, value: _HeaderTypes) -> None:\n        \"\"\"\n        \"\"\"\n        self._headers.add(name, self._convert_header_value(value))", "output": "Adds the given response header and value.\n\n        Unlike `set_header`, `add_header` may be called multiple times\n        to return multiple values for the same header.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_current_user():\n    \"\"\"\n    \n    \"\"\"\n    process = OpenProcessToken(\n        ctypes.windll.kernel32.GetCurrentProcess(), TokenAccess.TOKEN_QUERY\n    )\n    return GetTokenInformation(process, TOKEN_USER)", "output": "Return a TOKEN_USER for the owner of this process.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_host(zone, name, ttl, ip, keyname, keyfile, nameserver, timeout,\n             port=53, keyalgorithm='hmac-md5'):\n    '''\n    \n    '''\n    res = []\n    if zone in name:\n        name = name.replace(zone, '').rstrip('.')\n    fqdn = '{0}.{1}'.format(name, zone)\n\n    ret = create(zone, name, ttl, 'A', ip, keyname, keyfile, nameserver,\n                 timeout, port, keyalgorithm)\n    res.append(ret[fqdn])\n\n    parts = ip.split('.')[::-1]\n    i = len(parts)\n    popped = []\n\n    # Iterate over possible reverse zones\n    while i > 1:\n        p = parts.pop(0)\n        i -= 1\n        popped.append(p)\n\n        zone = '{0}.{1}'.format('.'.join(parts), 'in-addr.arpa.')\n        name = '.'.join(popped)\n        rev_fqdn = '{0}.{1}'.format(name, zone)\n        ret = create(zone, name, ttl, 'PTR', \"{0}.\".format(fqdn), keyname,\n                     keyfile, nameserver, timeout, port, keyalgorithm)\n\n        if \"Created\" in ret[rev_fqdn]:\n            res.append(ret[rev_fqdn])\n            return {fqdn: res}\n\n    res.append(ret[rev_fqdn])\n\n    return {fqdn: res}", "output": "Create both A and PTR (reverse) records for a host.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run ddns.add_host domain.com my-test-vm 3600 10.20.30.40 my-tsig-key /etc/salt/tsig.keyring 10.0.0.1 5", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def size(self) -> Optional[int]:\n        \"\"\"\"\"\"\n        if not self._parts:\n            return 0\n\n        total = 0\n        for part, encoding, te_encoding in self._parts:\n            if encoding or te_encoding or part.size is None:\n                return None\n\n            total += int(\n                2 + len(self._boundary) + 2 +  # b'--'+self._boundary+b'\\r\\n'\n                part.size + len(part._binary_headers) +\n                2  # b'\\r\\n'\n            )\n\n        total += 2 + len(self._boundary) + 4  # b'--'+self._boundary+b'--\\r\\n'\n        return total", "output": "Size of the payload.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def evaluate(op, op_str, a, b, use_numexpr=True,\n             **eval_kwargs):\n    \"\"\" \n        \"\"\"\n\n    use_numexpr = use_numexpr and _bool_arith_check(op_str, a, b)\n    if use_numexpr:\n        return _evaluate(op, op_str, a, b, **eval_kwargs)\n    return _evaluate_standard(op, op_str, a, b)", "output": "evaluate and return the expression of the op on a and b\n\n        Parameters\n        ----------\n\n        op :    the actual operand\n        op_str: the string version of the op\n        a :     left operand\n        b :     right operand\n        use_numexpr : whether to try to use numexpr (default True)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tlsa_rec(rdata):\n    '''\n    \n    '''\n    rschema = OrderedDict((\n        ('usage', RFC.TLSA_USAGE),\n        ('selector', RFC.TLSA_SELECT),\n        ('matching', RFC.TLSA_MATCHING),\n        ('pub', str)\n    ))\n\n    return _data2rec(rschema, rdata)", "output": "Validate and parse DNS record data for TLSA record(s)\n    :param rdata: DNS record data\n    :return: dict w/fields", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_argument(script, from_, to):\n    \"\"\"\"\"\"\n    replaced_in_the_end = re.sub(u' {}$'.format(re.escape(from_)), u' {}'.format(to),\n                                 script, count=1)\n    if replaced_in_the_end != script:\n        return replaced_in_the_end\n    else:\n        return script.replace(\n            u' {} '.format(from_), u' {} '.format(to), 1)", "output": "Replaces command line argument.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _search_url(search_term:str, size:str='>400*300', format:str='jpg') -> str:\n    \"\"\n    return ('https://www.google.com/search?q=' + quote(search_term) +\n            '&espv=2&biw=1366&bih=667&site=webhp&source=lnms&tbm=isch' +\n            _url_params(size, format) + '&sa=X&ei=XosDVaCXD8TasATItgE&ved=0CAcQ_AUoAg')", "output": "Return a Google Images Search URL for a given search term.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_masked_values(tensor: torch.Tensor, mask: torch.Tensor, replace_with: float) -> torch.Tensor:\n    \"\"\"\n    \n    \"\"\"\n    if tensor.dim() != mask.dim():\n        raise ConfigurationError(\"tensor.dim() (%d) != mask.dim() (%d)\" % (tensor.dim(), mask.dim()))\n    return tensor.masked_fill((1 - mask).byte(), replace_with)", "output": "Replaces all masked values in ``tensor`` with ``replace_with``.  ``mask`` must be broadcastable\n    to the same shape as ``tensor``. We require that ``tensor.dim() == mask.dim()``, as otherwise we\n    won't know which dimensions of the mask to unsqueeze.\n\n    This just does ``tensor.masked_fill()``, except the pytorch method fills in things with a mask\n    value of 1, where we want the opposite.  You can do this in your own code with\n    ``tensor.masked_fill((1 - mask).byte(), replace_with)``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send(self, conn):\n        ''' \n\n        '''\n        if conn is None:\n            raise ValueError(\"Cannot send to connection None\")\n\n        with (yield conn.write_lock.acquire()):\n            sent = 0\n\n            yield conn.write_message(self.header_json, locked=False)\n            sent += len(self.header_json)\n\n            # uncomment this to make it a lot easier to reproduce lock-related bugs\n            #yield gen.sleep(0.1)\n\n            yield conn.write_message(self.metadata_json, locked=False)\n            sent += len(self.metadata_json)\n\n            # uncomment this to make it a lot easier to reproduce lock-related bugs\n            #yield gen.sleep(0.1)\n\n            yield conn.write_message(self.content_json, locked=False)\n            sent += len(self.content_json)\n\n            sent += yield self.write_buffers(conn, locked=False)\n\n            raise gen.Return(sent)", "output": "Send the message on the given connection.\n\n        Args:\n            conn (WebSocketHandler) : a WebSocketHandler to send messages\n\n        Returns:\n            int : number of bytes sent", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_requirements() -> Tuple[PackagesType, PackagesType, Set[str]]:\n    \"\"\"\"\"\"\n    essential_packages: PackagesType = {}\n    other_packages: PackagesType = {}\n    duplicates: Set[str] = set()\n    with open(\"requirements.txt\", \"r\") as req_file:\n        section: str = \"\"\n        for line in req_file:\n            line = line.strip()\n\n            if line.startswith(\"####\"):\n                # Line is a section name.\n                section = parse_section_name(line)\n                continue\n\n            if not line or line.startswith(\"#\"):\n                # Line is empty or just regular comment.\n                continue\n\n            module, version = parse_package(line)\n            if module in essential_packages or module in other_packages:\n                duplicates.add(module)\n\n            if section.startswith(\"ESSENTIAL\"):\n                essential_packages[module] = version\n            else:\n                other_packages[module] = version\n\n    return essential_packages, other_packages, duplicates", "output": "Parse all dependencies out of the requirements.txt file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(self, value):\n        \"\"\"\n        \"\"\"\n        # If the value is out of bounds, bring it in bounds.\n        value = int(value)\n        if value < 10:\n            value = 10\n        if value > 600:\n            value = 600\n\n        # Add the value to the histogram's data dictionary.\n        self._data.setdefault(value, 0)\n        self._data[value] += 1\n        self._len += 1", "output": "Add the value to this histogram.\n\n        Args:\n            value (int): The value. Values outside of ``10 <= x <= 600``\n                will be raised to ``10`` or reduced to ``600``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nltk_tree_to_logical_form(tree: Tree) -> str:\n    \"\"\"\n    \n    \"\"\"\n    # nltk.Tree actually inherits from `list`, so you use `len()` to get the number of children.\n    # We're going to be explicit about checking length, instead of using `if tree:`, just to avoid\n    # any funny business nltk might have done (e.g., it's really odd if `if tree:` evaluates to\n    # `False` if there's a single leaf node with no children).\n    if len(tree) == 0:  # pylint: disable=len-as-condition\n        return tree.label()\n    if len(tree) == 1:\n        return tree[0].label()\n    return '(' + ' '.join(nltk_tree_to_logical_form(child) for child in tree) + ')'", "output": "Given an ``nltk.Tree`` representing the syntax tree that generates a logical form, this method\n    produces the actual (lisp-like) logical form, with all of the non-terminal symbols converted\n    into the correct number of parentheses.\n\n    This is used in the logic that converts action sequences back into logical forms.  It's very\n    unlikely that you will need this anywhere else.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __get_num_preds(self, num_iteration, nrow, predict_type):\n        \"\"\"\"\"\"\n        if nrow > MAX_INT32:\n            raise LightGBMError('LightGBM cannot perform prediction for data'\n                                'with number of rows greater than MAX_INT32 (%d).\\n'\n                                'You can split your data into chunks'\n                                'and then concatenate predictions for them' % MAX_INT32)\n        n_preds = ctypes.c_int64(0)\n        _safe_call(_LIB.LGBM_BoosterCalcNumPredict(\n            self.handle,\n            ctypes.c_int(nrow),\n            ctypes.c_int(predict_type),\n            ctypes.c_int(num_iteration),\n            ctypes.byref(n_preds)))\n        return n_preds.value", "output": "Get size of prediction result.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _initialize(cls, port, secret):\n        \"\"\"\n        \n        \"\"\"\n        cls._port = port\n        cls._secret = secret", "output": "Initialize BarrierTaskContext, other methods within BarrierTaskContext can only be called\n        after BarrierTaskContext is initialized.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cache_dataset(dataset, prefix):\n    \"\"\"\n    \"\"\"\n    if not os.path.exists(_constants.CACHE_PATH):\n        os.makedirs(_constants.CACHE_PATH)\n    src_data = np.concatenate([e[0] for e in dataset])\n    tgt_data = np.concatenate([e[1] for e in dataset])\n    src_cumlen = np.cumsum([0]+[len(e[0]) for e in dataset])\n    tgt_cumlen = np.cumsum([0]+[len(e[1]) for e in dataset])\n    np.savez(os.path.join(_constants.CACHE_PATH, prefix + '.npz'),\n             src_data=src_data, tgt_data=tgt_data,\n             src_cumlen=src_cumlen, tgt_cumlen=tgt_cumlen)", "output": "Cache the processed npy dataset the dataset into a npz\n\n    Parameters\n    ----------\n    dataset : SimpleDataset\n    file_path : str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Add(self, file_desc_proto):\n    \"\"\"\n    \"\"\"\n    proto_name = file_desc_proto.name\n    if proto_name not in self._file_desc_protos_by_file:\n      self._file_desc_protos_by_file[proto_name] = file_desc_proto\n    elif self._file_desc_protos_by_file[proto_name] != file_desc_proto:\n      raise DescriptorDatabaseConflictingDefinitionError(\n          '%s already added, but with different descriptor.' % proto_name)\n\n    # Add all the top-level descriptors to the index.\n    package = file_desc_proto.package\n    for message in file_desc_proto.message_type:\n      self._file_desc_protos_by_symbol.update(\n          (name, file_desc_proto) for name in _ExtractSymbols(message, package))\n    for enum in file_desc_proto.enum_type:\n      self._file_desc_protos_by_symbol[\n          '.'.join((package, enum.name))] = file_desc_proto\n    for extension in file_desc_proto.extension:\n      self._file_desc_protos_by_symbol[\n          '.'.join((package, extension.name))] = file_desc_proto\n    for service in file_desc_proto.service:\n      self._file_desc_protos_by_symbol[\n          '.'.join((package, service.name))] = file_desc_proto", "output": "Adds the FileDescriptorProto and its types to this database.\n\n    Args:\n      file_desc_proto: The FileDescriptorProto to add.\n    Raises:\n      DescriptorDatabaseConflictingDefinitionError: if an attempt is made to\n        add a proto with the same name but different definition than an\n        exisiting proto in the database.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _wrap_result(self, result, block=None, obj=None):\n        \"\"\"\n        \n        \"\"\"\n\n        if obj is None:\n            obj = self._selected_obj\n        index = obj.index\n\n        if isinstance(result, np.ndarray):\n\n            # coerce if necessary\n            if block is not None:\n                if is_timedelta64_dtype(block.values.dtype):\n                    from pandas import to_timedelta\n                    result = to_timedelta(\n                        result.ravel(), unit='ns').values.reshape(result.shape)\n\n            if result.ndim == 1:\n                from pandas import Series\n                return Series(result, index, name=obj.name)\n\n            return type(obj)(result, index=index, columns=block.columns)\n        return result", "output": "Wrap a single result.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def categorical__int(self, column_name, output_column_prefix):\n        \"\"\"\n        \n        \"\"\"\n\n        return [_ColumnFunctionTransformation(\n            features = [column_name],\n            output_column_prefix = output_column_prefix,\n            transform_function = lambda col: col.astype(str),\n            transform_function_name = \"astype(str)\")]", "output": "Interprets an integer column as a categorical variable.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def first_spark_call():\n    \"\"\"\n    \n    \"\"\"\n    tb = traceback.extract_stack()\n    if len(tb) == 0:\n        return None\n    file, line, module, what = tb[len(tb) - 1]\n    sparkpath = os.path.dirname(file)\n    first_spark_frame = len(tb) - 1\n    for i in range(0, len(tb)):\n        file, line, fun, what = tb[i]\n        if file.startswith(sparkpath):\n            first_spark_frame = i\n            break\n    if first_spark_frame == 0:\n        file, line, fun, what = tb[0]\n        return CallSite(function=fun, file=file, linenum=line)\n    sfile, sline, sfun, swhat = tb[first_spark_frame]\n    ufile, uline, ufun, uwhat = tb[first_spark_frame - 1]\n    return CallSite(function=sfun, file=ufile, linenum=uline)", "output": "Return a CallSite representing the first Spark call in the current call stack.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def not_reaped(subset=None, show_ip=False, show_ipv4=None):\n    '''\n    \n    '''\n    show_ip = _show_ip_migration(show_ip, show_ipv4)\n    return list_not_state(subset=subset, show_ip=show_ip)", "output": ".. versionadded:: 2015.8.0\n    .. versionchanged:: 2019.2.0\n        The 'show_ipv4' argument has been renamed to 'show_ip' as it now\n        includes IPv6 addresses for IPv6-connected minions.\n\n    Print a list of all minions that are NOT up according to Salt's presence\n    detection (no commands will be sent)\n\n    subset : None\n        Pass in a CIDR range to filter minions by IP address.\n\n    show_ip : False\n        Also show the IP address each minion is connecting from.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run manage.not_reaped", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def neural_gpu_body(inputs, hparams, name=None):\n  \"\"\"\"\"\"\n  with tf.variable_scope(name, \"neural_gpu\"):\n\n    def step(state, inp):  # pylint: disable=missing-docstring\n      x = tf.nn.dropout(state, 1.0 - hparams.dropout)\n      for layer in range(hparams.num_hidden_layers):\n        x = common_layers.conv_gru(\n            x, (hparams.kernel_height, hparams.kernel_width),\n            hparams.hidden_size,\n            name=\"cgru_%d\" % layer)\n      # Padding input is zeroed-out in the modality, we check this by summing.\n      padding_inp = tf.less(tf.reduce_sum(tf.abs(inp), axis=[1, 2]), 0.00001)\n      new_state = tf.where(padding_inp, state, x)  # No-op where inp is padding.\n      return new_state\n\n    return tf.foldl(\n        step,\n        tf.transpose(inputs, [1, 0, 2, 3]),\n        initializer=inputs,\n        parallel_iterations=1,\n        swap_memory=True)", "output": "The core Neural GPU.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def restart(name, path=None, lxc_config=None, force=False):\n    '''\n    \n    '''\n    _ensure_exists(name, path=path)\n    orig_state = state(name, path=path)\n    if orig_state != 'stopped':\n        stop(name, kill=force, path=path)\n    ret = start(name, path=path, lxc_config=lxc_config)\n    ret['state']['old'] = orig_state\n    if orig_state != 'stopped':\n        ret['restarted'] = True\n    return ret", "output": ".. versionadded:: 2015.5.0\n\n    Restart the named container. If the container was not running, the\n    container will merely be started.\n\n    name\n        The name of the container\n\n    path\n        path to the container parent directory\n        default: /var/lib/lxc (system)\n\n        .. versionadded:: 2015.8.0\n\n    lxc_config\n        path to a lxc config file\n        config file will be guessed from container name otherwise\n\n        .. versionadded:: 2015.8.0\n\n    force : False\n        If ``True``, the container will be force-stopped instead of gracefully\n        shut down\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion lxc.restart name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def timing(name=''):\n  \"\"\"\"\"\"\n  start = datetime.datetime.now()\n  timestamp = start.strftime('%H:%M')\n  tf.logging.info('Starting job [%s] at %s', name, timestamp)\n  yield\n  end = datetime.datetime.now()\n  timestamp = end.strftime('%H:%M')\n  tf.logging.info('Finished job [%s] at %s', name, timestamp)\n  duration = end - start\n  duration_mins = duration.total_seconds() / 60\n  tf.logging.info('Total time [%s] (m): %d', name, int(duration_mins))", "output": "Log start, end, and duration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _selu(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    new_attrs = translation_utils._add_extra_attributes(attrs, {'act_type': 'selu'})\n    return 'LeakyReLU', new_attrs, inputs", "output": "Selu function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_link_target(self, tarinfo):\n        \"\"\"\n        \"\"\"\n        if tarinfo.issym():\n            # Always search the entire archive.\n            linkname = os.path.dirname(tarinfo.name) + \"/\" + tarinfo.linkname\n            limit = None\n        else:\n            # Search the archive before the link, because a hard link is\n            # just a reference to an already archived file.\n            linkname = tarinfo.linkname\n            limit = tarinfo\n\n        member = self._getmember(linkname, tarinfo=limit, normalize=True)\n        if member is None:\n            raise KeyError(\"linkname %r not found\" % linkname)\n        return member", "output": "Find the target member of a symlink or hardlink member in the\n           archive.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vm_diskstats(vm_=None):\n    '''\n    \n    '''\n    with _get_xapi_session() as xapi:\n        def _info(vm_):\n            ret = {}\n            vm_uuid = _get_label_uuid(xapi, 'VM', vm_)\n            if vm_uuid is False:\n                return False\n            for vbd in xapi.VM.get_VBDs(vm_uuid):\n                vbd_rec = _get_record(xapi, 'VBD', vbd)\n                ret[vbd_rec['device']] = _get_metrics_record(xapi, 'VBD',\n                                                             vbd_rec)\n                del ret[vbd_rec['device']]['last_updated']\n\n            return ret\n\n        info = {}\n        if vm_:\n            info[vm_] = _info(vm_)\n        else:\n            for vm_ in list_domains():\n                info[vm_] = _info(vm_)\n        return info", "output": "Return disk usage counters used by the vms on this hyper in a\n    list of dicts:\n\n    .. code-block:: python\n\n        [\n            'your-vm': {\n                'io_read_kbs'   : 0,\n                'io_write_kbs'  : 0\n                },\n            ...\n            ]\n\n    If you pass a VM name in as an argument then it will return info\n    for just the named VM, otherwise it will return all VMs.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' virt.vm_diskstats", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read_socket(socket):\n    \"\"\"\n    \n    \"\"\"\n\n    # Keep reading the stream until the stream terminates\n    while True:\n\n        try:\n\n            payload_type, payload_size = _read_header(socket)\n            if payload_size < 0:\n                # Something is wrong with the data stream. Payload size can't be less than zero\n                break\n\n            for data in _read_payload(socket, payload_size):\n                yield payload_type, data\n\n        except timeout:\n            # Timeouts are normal during debug sessions and long running tasks\n            LOG.debug(\"Ignoring docker socket timeout\")\n\n        except SocketError:\n            # There isn't enough data in the stream. Probably the socket terminated\n            break", "output": "The stdout and stderr data from the container multiplexed into one stream of response from the Docker API.\n    It follows the protocol described here https://docs.docker.com/engine/api/v1.30/#operation/ContainerAttach.\n    The stream starts with a 8 byte header that contains the frame type and also payload size. Follwing that is the\n    actual payload of given size. Once you read off this payload, we are ready to read the next header.\n\n    This method will follow this protocol to read payload from the stream and return an iterator that returns\n    a tuple containing the frame type and frame data. Callers can handle the data appropriately based on the frame\n    type.\n\n        Stdout => Frame Type = 1\n        Stderr => Frame Type = 2\n\n\n    Parameters\n    ----------\n    socket\n        Socket to read responses from\n\n    Yields\n    -------\n    int\n        Type of the stream (1 => stdout, 2 => stderr)\n    str\n        Data in the stream", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def create_group(self, *recipients):\n        \n        \"\"\"\n\n        from .channel import GroupChannel\n\n        if len(recipients) < 2:\n            raise ClientException('You must have two or more recipients to create a group.')\n\n        users = [str(u.id) for u in recipients]\n        data = await self._state.http.start_group(self.id, users)\n        return GroupChannel(me=self, data=data, state=self._state)", "output": "r\"\"\"|coro|\n\n        Creates a group direct message with the recipients\n        provided. These recipients must be have a relationship\n        of type :attr:`RelationshipType.friend`.\n\n        .. note::\n\n            This only applies to non-bot accounts.\n\n        Parameters\n        -----------\n        \\*recipients: :class:`User`\n            An argument :class:`list` of :class:`User` to have in\n            your group.\n\n        Raises\n        -------\n        HTTPException\n            Failed to create the group direct message.\n        ClientException\n            Attempted to create a group with only one recipient.\n            This does not include yourself.\n\n        Returns\n        -------\n        :class:`GroupChannel`\n            The new group channel.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hold_price(self, datetime=None):\n        \"\"\"\n        \"\"\"\n\n        def weights(x):\n            if sum(x['amount']) != 0:\n                return np.average(\n                    x['price'],\n                    weights=x['amount'],\n                    returned=True\n                )\n            else:\n                return np.nan\n\n        if datetime is None:\n            return self.history_table.set_index(\n                'datetime',\n                drop=False\n            ).sort_index().groupby('code').apply(weights).dropna()\n        else:\n            return self.history_table.set_index(\n                'datetime',\n                drop=False\n            ).sort_index().loc[:datetime].groupby('code').apply(weights\n                                                               ).dropna()", "output": "\u8ba1\u7b97\u6301\u4ed3\u6210\u672c  \u5982\u679c\u7ed9\u7684\u662f\u65e5\u671f,\u5219\u8fd4\u56de\u5f53\u65e5\u5f00\u76d8\u524d\u7684\u6301\u4ed3\n\n        Keyword Arguments:\n            datetime {[type]} -- [description] (default: {None})\n\n        Returns:\n            [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dataset_initialize(self, folder):\n        \"\"\" \n        \"\"\"\n        if not os.path.isdir(folder):\n            raise ValueError('Invalid folder: ' + folder)\n\n        ref = self.config_values[self.CONFIG_NAME_USER] + '/INSERT_SLUG_HERE'\n        licenses = []\n        default_license = {'name': 'CC0-1.0'}\n        licenses.append(default_license)\n\n        meta_data = {\n            'title': 'INSERT_TITLE_HERE',\n            'id': ref,\n            'licenses': licenses\n        }\n        meta_file = os.path.join(folder, self.DATASET_METADATA_FILE)\n        with open(meta_file, 'w') as f:\n            json.dump(meta_data, f, indent=2)\n\n        print('Data package template written to: ' + meta_file)\n        return meta_file", "output": "initialize a folder with a a dataset configuration (metadata) file\n\n            Parameters\n            ==========\n            folder: the folder to initialize the metadata file in", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_dns_subdomain(name):\n    '''  '''\n\n    dns_subdomain = re.compile(r\"\"\"^[a-z0-9\\.-]{1,253}$\"\"\")\n    if dns_subdomain.match(name):\n        log.debug(\"Name: %s is valid DNS subdomain\", name)\n        return True\n    else:\n        log.debug(\"Name: %s is not valid DNS subdomain\", name)\n        return False", "output": "Check that name is DNS subdomain: One or more lowercase rfc1035/rfc1123\n    labels separated by '.' with a maximum length of 253 characters", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keyPressEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        ctrl = event.modifiers() & Qt.ControlModifier\r\n        key = event.key()\r\n        handled = False\r\n        if ctrl and self.count() > 0:\r\n            index = self.currentIndex()\r\n            if key == Qt.Key_PageUp:\r\n                if index > 0:\r\n                    self.setCurrentIndex(index - 1)\r\n                else:\r\n                    self.setCurrentIndex(self.count() - 1)\r\n                handled = True\r\n            elif key == Qt.Key_PageDown:\r\n                if index < self.count() - 1:\r\n                    self.setCurrentIndex(index + 1)\r\n                else:\r\n                    self.setCurrentIndex(0)\r\n                handled = True\r\n        if not handled:\r\n            QTabWidget.keyPressEvent(self, event)", "output": "Override Qt method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def quantiles(data, nbins_or_partition_bounds):\n    \"\"\"\n    \n    \"\"\"\n    return apply_along_axis(\n        qcut,\n        1,\n        data,\n        q=nbins_or_partition_bounds, labels=False,\n    )", "output": "Compute rowwise array quantiles on an input.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Indirect(self, off):\n        \"\"\"\"\"\"\n        N.enforce_number(off, N.UOffsetTFlags)\n        return off + encode.Get(N.UOffsetTFlags.packer_type, self.Bytes, off)", "output": "Indirect retrieves the relative offset stored at `offset`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_categories(self, categories, fastpath=False):\n        \"\"\"\n        \n        \"\"\"\n\n        if fastpath:\n            new_dtype = CategoricalDtype._from_fastpath(categories,\n                                                        self.ordered)\n        else:\n            new_dtype = CategoricalDtype(categories, ordered=self.ordered)\n        if (not fastpath and self.dtype.categories is not None and\n                len(new_dtype.categories) != len(self.dtype.categories)):\n            raise ValueError(\"new categories need to have the same number of \"\n                             \"items than the old categories!\")\n\n        self._dtype = new_dtype", "output": "Sets new categories inplace\n\n        Parameters\n        ----------\n        fastpath : bool, default False\n           Don't perform validation of the categories for uniqueness or nulls\n\n        Examples\n        --------\n        >>> c = pd.Categorical(['a', 'b'])\n        >>> c\n        [a, b]\n        Categories (2, object): [a, b]\n\n        >>> c._set_categories(pd.Index(['a', 'c']))\n        >>> c\n        [a, c]\n        Categories (2, object): [a, c]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def search_complete(self, completed):\r\n        \"\"\"\"\"\"\r\n        self.result_browser.set_sorting(ON)\r\n        self.find_options.ok_button.setEnabled(True)\r\n        self.find_options.stop_button.setEnabled(False)\r\n        self.status_bar.hide()\r\n        self.result_browser.expandAll()\r\n        if self.search_thread is None:\r\n            return\r\n        self.sig_finished.emit()\r\n        found = self.search_thread.get_results()\r\n        self.stop_and_reset_thread()\r\n        if found is not None:\r\n            results, pathlist, nb, error_flag = found\r\n            self.result_browser.show()", "output": "Current search thread has finished", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tokenize(self, text):\n        '''\n        \n        '''\n        start = -1\n        tokens = []\n        for i, character in enumerate(text):\n            if character == ' ' or character == '\\t':\n                if start >= 0:\n                    word = text[start:i]\n                    tokens.append({\n                        'word': word,\n                        'original_text': word,\n                        'char_begin': start,\n                        'char_end': i})\n                    start = -1\n            else:\n                if start < 0:\n                    start = i\n        if start >= 0:\n            tokens.append({\n                'word': text[start:len(text)],\n                'original_text': text[start:len(text)],\n                'char_begin': start,\n                'char_end': len(text)\n            })\n        return tokens", "output": "tokenize function in Tokenizer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_grammar_string(grammar_dictionary: Dict[str, List[str]]) -> str:\n    \"\"\"\n    \n    \"\"\"\n    grammar_string = '\\n'.join([f\"{nonterminal} = {' / '.join(right_hand_side)}\"\n                                for nonterminal, right_hand_side in grammar_dictionary.items()])\n    return grammar_string.replace(\"\\\\\", \"\\\\\\\\\")", "output": "Formats a dictionary of production rules into the string format expected\n    by the Parsimonious Grammar class.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_snapshot(name, config_path=_DEFAULT_CONFIG_PATH, with_packages=False):\n    '''\n    \n    '''\n    _validate_config(config_path)\n\n    sources = list()\n\n    cmd = ['snapshot', 'show', '-config={}'.format(config_path),\n           '-with-packages={}'.format(str(with_packages).lower()),\n           name]\n\n    cmd_ret = _cmd_run(cmd)\n\n    ret = _parse_show_output(cmd_ret=cmd_ret)\n\n    if ret:\n        log.debug('Found shapshot: %s', name)\n    else:\n        log.debug('Unable to find snapshot: %s', name)\n\n    return ret", "output": "Get detailed information about a snapshot.\n\n    :param str name: The name of the snapshot given during snapshot creation.\n    :param str config_path: The path to the configuration file for the aptly instance.\n    :param bool with_packages: Return a list of packages in the snapshot.\n\n    :return: A dictionary containing information about the snapshot.\n    :rtype: dict\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' aptly.get_snapshot name=\"test-repo\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def request_encode_url(self, method, url, fields=None, headers=None,\n                           **urlopen_kw):\n        \"\"\"\n        \n        \"\"\"\n        if headers is None:\n            headers = self.headers\n\n        extra_kw = {'headers': headers}\n        extra_kw.update(urlopen_kw)\n\n        if fields:\n            url += '?' + urlencode(fields)\n\n        return self.urlopen(method, url, **extra_kw)", "output": "Make a request using :meth:`urlopen` with the ``fields`` encoded in\n        the url. This is useful for request methods like GET, HEAD, DELETE, etc.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def move(self, path, dest):\n        \"\"\"\n        \n        \"\"\"\n        parts = dest.rstrip('/').split('/')\n        if len(parts) > 1:\n            dir_path = '/'.join(parts[0:-1])\n            if not self.exists(dir_path):\n                self.mkdir(dir_path, parents=True)\n        return list(self.get_bite().rename(self.list_path(path), dest))", "output": "Use snakebite.rename, if available.\n\n        :param path: source file(s)\n        :type path: either a string or sequence of strings\n        :param dest: destination file (single input) or directory (multiple)\n        :type dest: string\n        :return: list of renamed items", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_local_block(x, depth, batch, heads, num_blocks, block_length):\n  \"\"\"\"\"\"\n  prev_block = tf.slice(x, [0, 0, 0, 0, 0],\n                        [-1, -1, num_blocks - 1, -1, -1])\n  cur_block = tf.slice(x, [0, 0, 1, 0, 0], [-1, -1, -1, -1, -1])\n  local_block = tf.concat([prev_block, cur_block], 3)\n  return tf.reshape(local_block,\n                    [batch, heads, num_blocks - 1, block_length * 2, depth])", "output": "Helper function to create a local version of the keys or values for 1d.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_examples(self, file_id, extracted_dirs):\n    \"\"\"\"\"\"\n    filedir = os.path.join(extracted_dirs[\"img_align_celeba\"],\n                           \"img_align_celeba\")\n    img_list_path = extracted_dirs[\"list_eval_partition\"]\n    landmarks_path = extracted_dirs[\"landmarks_celeba\"]\n    attr_path = extracted_dirs[\"list_attr_celeba\"]\n\n    with tf.io.gfile.GFile(img_list_path) as f:\n      files = [\n          line.split()[0]\n          for line in f.readlines()\n          if int(line.split()[1]) == file_id\n      ]\n\n    attributes = self._process_celeba_config_file(attr_path)\n    landmarks = self._process_celeba_config_file(landmarks_path)\n\n    for file_name in sorted(files):\n      path = os.path.join(filedir, file_name)\n\n      yield {\n          \"image\": path,\n          \"landmarks\": {\n              k: v for k, v in zip(landmarks[0], landmarks[1][file_name])\n          },\n          \"attributes\": {\n              # atributes value are either 1 or -1, so convert to bool\n              k: v > 0 for k, v in zip(attributes[0], attributes[1][file_name])\n          },\n      }", "output": "Yields examples.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_minmax_axis(axis):\n    \"\"\"\n    \n    \"\"\"\n    ndim = 1  # hard-coded for Index\n    if axis is None:\n        return\n    if axis >= ndim or (axis < 0 and ndim + axis < 0):\n        raise ValueError(\"`axis` must be fewer than the number of \"\n                         \"dimensions ({ndim})\".format(ndim=ndim))", "output": "Ensure that the axis argument passed to min, max, argmin, or argmax is\n    zero or None, as otherwise it will be incorrectly ignored.\n\n    Parameters\n    ----------\n    axis : int or None\n\n    Raises\n    ------\n    ValueError", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_czeng(*paths, **kwargs):\n  \"\"\"\"\"\"\n  filter_path = kwargs.get(\"filter_path\", None)\n  if filter_path:\n    re_block = re.compile(r\"^[^-]+-b(\\d+)-\\d\\d[tde]\")\n    with tf.io.gfile.GFile(filter_path) as f:\n      bad_blocks = {\n          blk for blk in re.search(\n              r\"qw{([\\s\\d]*)}\", f.read()).groups()[0].split()\n      }\n    logging.info(\n        \"Loaded %d bad blocks to filter from CzEng v1.6 to make v1.7.\",\n        len(bad_blocks))\n\n  for path in paths:\n    for gz_path in tf.io.gfile.glob(path):\n      with tf.io.gfile.GFile(gz_path, \"rb\") as g, gzip.GzipFile(fileobj=g) as f:\n        for line in f:\n          line = line.decode(\"utf-8\")  # required for py3\n          if not line.strip():\n            continue\n          id_, unused_score, cs, en = line.split(\"\\t\")\n          if filter_path:\n            block_match = re.match(re_block, id_)\n            if block_match and block_match.groups()[0] in bad_blocks:\n              continue\n          yield {\n              \"cs\": cs.strip(),\n              \"en\": en.strip(),\n          }", "output": "Generates examples from CzEng v1.6, with optional filtering for v1.7.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_login_shell(proc_cmd):\n    \"\"\"\n    \"\"\"\n    login_shell = os.environ.get('SHELL', '')\n    if login_shell:\n        proc_cmd = login_shell\n    else:\n        proc_cmd = proc_cmd[1:]\n    return (os.path.basename(proc_cmd).lower(), proc_cmd)", "output": "Form shell information from the SHELL environment variable if possible.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _chkconfig_add(name):\n    '''\n    \n    '''\n    cmd = '/sbin/chkconfig --add {0}'.format(name)\n    if __salt__['cmd.retcode'](cmd, python_shell=False) == 0:\n        log.info('Added initscript \"%s\" to chkconfig', name)\n        return True\n    else:\n        log.error('Unable to add initscript \"%s\" to chkconfig', name)\n        return False", "output": "Run 'chkconfig --add' for a service whose script is installed in\n    /etc/init.d.  The service is initially configured to be disabled at all\n    run-levels.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expire(key, seconds, host=None, port=None, db=None, password=None):\n    '''\n    \n    '''\n    server = _connect(host, port, db, password)\n    return server.expire(key, seconds)", "output": "Set a keys time to live in seconds\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.expire foo 300", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_tag(repo, name):\n    '''\n    \n    '''\n    try:\n        return [x for x in _all_tags(repo) if x[0] == name][0]\n    except IndexError:\n        return False", "output": "Find the requested tag in the specified repo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_time(time):\n    ''''''\n    unit = time[-1]\n    if unit not in ['s', 'm', 'h', 'd']:\n        print_error('the unit of time could only from {s, m, h, d}')\n        exit(1)\n    time = time[:-1]\n    if not time.isdigit():\n        print_error('time format error!')\n        exit(1)\n    parse_dict = {'s':1, 'm':60, 'h':3600, 'd':86400}\n    return int(time) * parse_dict[unit]", "output": "Change the time to seconds", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deserialize(cls, file_path, **kwargs):\n        \"\"\"\n        \"\"\"\n        # idx_to_token is of dtype 'O' so we need to allow pickle\n        npz_dict = np.load(file_path, allow_pickle=True)\n\n        unknown_token = npz_dict['unknown_token']\n        if not unknown_token:\n            unknown_token = None\n        else:\n            if isinstance(unknown_token, np.ndarray):\n                if unknown_token.dtype.kind == 'S':\n                    unknown_token = unknown_token.tobytes().decode()\n                else:\n                    unknown_token = str(unknown_token)\n        idx_to_token = npz_dict['idx_to_token'].tolist()\n        idx_to_vec = nd.array(npz_dict['idx_to_vec'])\n\n        embedding = cls(unknown_token=unknown_token, **kwargs)\n        if unknown_token:\n            assert unknown_token == idx_to_token[C.UNK_IDX]\n            embedding._token_to_idx = DefaultLookupDict(C.UNK_IDX)\n        else:\n            embedding._token_to_idx = {}\n\n        embedding._idx_to_token = idx_to_token\n        embedding._idx_to_vec = idx_to_vec\n        embedding._token_to_idx.update((token, idx) for idx, token in enumerate(idx_to_token))\n\n        return embedding", "output": "Create a new TokenEmbedding from a serialized one.\n\n        TokenEmbedding is serialized by converting the list of tokens, the\n        array of word embeddings and other metadata to numpy arrays, saving all\n        in a single (optionally compressed) Zipfile. See\n        https://docs.scipy.org/doc/numpy-1.14.2/neps/npy-format.html for more\n        information on the format.\n\n\n        Parameters\n        ----------\n        file_path : str or file\n            The path to a file that holds the serialized TokenEmbedding.\n        kwargs : dict\n            Keyword arguments are passed to the TokenEmbedding initializer.\n            Useful for attaching unknown_lookup.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def start(self, *args, **kwargs):\n        \"\"\"\n        \"\"\"\n\n        bot = kwargs.pop('bot', True)\n        reconnect = kwargs.pop('reconnect', True)\n        await self.login(*args, bot=bot)\n        await self.connect(reconnect=reconnect)", "output": "|coro|\n\n        A shorthand coroutine for :meth:`login` + :meth:`connect`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _yum():\n    '''\n    \n    '''\n    contextkey = 'yum_bin'\n    if contextkey not in __context__:\n        if ('fedora' in __grains__['os'].lower()\n           and int(__grains__['osrelease']) >= 22):\n            __context__[contextkey] = 'dnf'\n        elif 'photon' in __grains__['os'].lower():\n            __context__[contextkey] = 'tdnf'\n        else:\n            __context__[contextkey] = 'yum'\n    return __context__[contextkey]", "output": "Determine package manager name (yum or dnf),\n    depending on the system version.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(grains=False, grain_keys=None, pillar=False, pillar_keys=None):\n    '''\n    \n    '''\n    state = salt.thorium.ThorState(\n            __opts__,\n            grains,\n            grain_keys,\n            pillar,\n            pillar_keys)\n    state.start_runtime()", "output": "Execute the Thorium runtime", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_exception(e):\n    ''''''\n    args = ('exception in ldap backend: {0}'.format(repr(e)), e)\n    if six.PY2:\n        six.reraise(LDAPError, args, sys.exc_info()[2])\n    else:\n        six.raise_from(LDAPError(*args), e)", "output": "Convert an ldap backend exception to an LDAPError and raise it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetch(self,\n              minion_id,\n              pillar,  # pylint: disable=W0613\n              *args,\n              **kwargs):\n        '''\n        \n        '''\n        db_name = self._db_name()\n        log.info('Querying %s for information for %s', db_name, minion_id)\n        #\n        #    log.debug('ext_pillar %s args: %s', db_name, args)\n        #    log.debug('ext_pillar %s kwargs: %s', db_name, kwargs)\n        #\n        # Most of the heavy lifting is in this class for ease of testing.\n        qbuffer = self.extract_queries(args, kwargs)\n        with self._get_cursor() as cursor:\n            for root, details in qbuffer:\n                # Run the query\n                cursor.execute(details['query'], (minion_id,))\n\n                # Extract the field names the db has returned and process them\n                self.process_fields([row[0] for row in cursor.description], details['depth'])\n                self.enter_root(root)\n                self.as_list = details['as_list']\n                if details['with_lists']:\n                    self.with_lists = details['with_lists']\n                else:\n                    self.with_lists = []\n                self.ignore_null = details['ignore_null']\n                self.process_results(cursor.fetchall())\n\n                log.debug('ext_pillar %s: Return data: %s', db_name, self)\n        return self.result", "output": "Execute queries, merge and return as a dict.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def layer_covariance(layer1, layer2=None):\n    \"\"\"\"\"\"\n    layer2 = layer2 or layer1\n    act1, act2 = layer1.activations, layer2.activations\n    num_datapoints = act1.shape[0]  # cast to avoid numpy type promotion during division\n    return np.matmul(act1.T, act2) / float(num_datapoints)", "output": "Computes the covariance matrix between the neurons of two layers. If only one\n    layer is passed, computes the symmetric covariance matrix of that layer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compute_index(self, axis, data_object, compute_diff=True):\n        \"\"\"\n        \"\"\"\n\n        def pandas_index_extraction(df, axis):\n            if not axis:\n                return df.index\n            else:\n                try:\n                    return df.columns\n                except AttributeError:\n                    return pandas.Index([])\n\n        index_obj = self.index if not axis else self.columns\n        old_blocks = self.data if compute_diff else None\n        new_indices = data_object.get_indices(\n            axis=axis,\n            index_func=lambda df: pandas_index_extraction(df, axis),\n            old_blocks=old_blocks,\n        )\n        return index_obj[new_indices] if compute_diff else new_indices", "output": "Computes the index after a number of rows have been removed.\n\n        Note: In order for this to be used properly, the indexes must not be\n            changed before you compute this.\n\n        Args:\n            axis: The axis to extract the index from.\n            data_object: The new data object to extract the index from.\n            compute_diff: True to use `self` to compute the index from self\n                rather than data_object. This is used when the dimension of the\n                index may have changed, but the deleted rows/columns are\n                unknown.\n\n        Returns:\n            A new pandas.Index object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sink_path(cls, project, sink):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/sinks/{sink}\", project=project, sink=sink\n        )", "output": "Return a fully-qualified sink string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def newline(self, node=None, extra=0):\n        \"\"\"\"\"\"\n        self._new_lines = max(self._new_lines, 1 + extra)\n        if node is not None and node.lineno != self._last_line:\n            self._write_debug_info = node.lineno\n            self._last_line = node.lineno", "output": "Add one or more newlines before the next write.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getOrDefault(self, param):\n        \"\"\"\n        \n        \"\"\"\n        param = self._resolveParam(param)\n        if param in self._paramMap:\n            return self._paramMap[param]\n        else:\n            return self._defaultParamMap[param]", "output": "Gets the value of a param in the user-supplied param map or its\n        default value. Raises an error if neither is set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mtf_bitransformer_all_layers_tiny():\n  \"\"\"\"\"\"\n  hparams = mtf_bitransformer_tiny()\n  hparams.moe_num_experts = 4\n  hparams.moe_expert_x = 4\n  hparams.moe_expert_y = 4\n  hparams.moe_hidden_size = 512\n  hparams.encoder_layers = [\n      \"self_att\", \"local_self_att\", \"moe_1d\", \"moe_2d\", \"drd\"]\n  hparams.decoder_layers = [\n      \"self_att\", \"local_self_att\", \"enc_att\", \"moe_1d\", \"moe_2d\", \"drd\"]\n  return hparams", "output": "Test out all the layers on local CPU.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_host(ip, alias):\n    '''\n    \n    '''\n    hfn = _get_or_create_hostfile()\n    ovr = False\n    if not os.path.isfile(hfn):\n        return False\n\n    # Make sure future calls to _list_hosts() will re-read the file\n    __context__.pop('hosts._list_hosts', None)\n\n    line_to_add = salt.utils.stringutils.to_bytes(\n        ip + '\\t\\t' + alias + os.linesep\n    )\n    # support removing a host entry by providing an empty string\n    if not alias.strip():\n        line_to_add = b''\n\n    with salt.utils.files.fopen(hfn, 'rb') as fp_:\n        lines = fp_.readlines()\n    for ind, _ in enumerate(lines):\n        tmpline = lines[ind].strip()\n        if not tmpline:\n            continue\n        if tmpline.startswith(b'#'):\n            continue\n        comps = tmpline.split()\n        if comps[0] == salt.utils.stringutils.to_bytes(ip):\n            if not ovr:\n                lines[ind] = line_to_add\n                ovr = True\n            else:  # remove other entries\n                lines[ind] = b''\n    linesep_bytes = salt.utils.stringutils.to_bytes(os.linesep)\n    if not ovr:\n        # make sure there is a newline\n        if lines and not lines[-1].endswith(linesep_bytes):\n            lines[-1] += linesep_bytes\n        line = line_to_add\n        lines.append(line)\n    with salt.utils.files.fopen(hfn, 'wb') as ofile:\n        ofile.writelines(lines)\n    return True", "output": "Set the host entry in the hosts file for the given ip, this will overwrite\n    any previous entry for the given ip\n\n    .. versionchanged:: 2016.3.0\n        If ``alias`` does not include any host names (it is the empty\n        string or contains only whitespace), all entries for the given\n        IP address are removed.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' hosts.set_host <ip> <alias>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def store(bank, key, data):\n    '''\n    \n    '''\n    _init_client()\n    etcd_key = '{0}/{1}/{2}'.format(path_prefix, bank, key)\n    try:\n        value = __context__['serial'].dumps(data)\n        client.write(etcd_key, base64.b64encode(value))\n    except Exception as exc:\n        raise SaltCacheError(\n            'There was an error writing the key, {0}: {1}'.format(etcd_key, exc)\n        )", "output": "Store a key value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zero_add(previous_value, x, name=None, reuse=None):\n  \"\"\"\n  \"\"\"\n  with tf.variable_scope(name, default_name=\"zero_add\", reuse=reuse):\n    gamma = tf.get_variable(\"gamma\", (), initializer=tf.zeros_initializer())\n    return previous_value + gamma * x", "output": "Resnet connection with zero initialization.\n\n  Another type of resnet connection which returns previous_value + gamma * x.\n  gamma is a trainable scalar and initialized with zero. It is useful when a\n  module is plugged into a trained model and we want to make sure it matches the\n  original model's performance.\n\n  Args:\n    previous_value:  A tensor.\n    x: A tensor.\n    name: name of variable scope; defaults to zero_add.\n    reuse: reuse scope.\n\n  Returns:\n    previous_value + gamma * x.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _getOrCreate(cls):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(cls._taskContext, BarrierTaskContext):\n            cls._taskContext = object.__new__(cls)\n        return cls._taskContext", "output": "Internal function to get or create global BarrierTaskContext. We need to make sure\n        BarrierTaskContext is returned from here because it is needed in python worker reuse\n        scenario, see SPARK-25921 for more details.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _gluster_xml(cmd):\n    '''\n    \n    '''\n    # We will pass the command string as stdin to allow for much longer\n    # command strings. This is especially useful for creating large volumes\n    # where the list of bricks exceeds 128 characters.\n    if _get_version() < (3, 6,):\n        result = __salt__['cmd.run'](\n            'script -q -c \"gluster --xml --mode=script\"', stdin=\"{0}\\n\\004\".format(cmd)\n        )\n    else:\n        result = __salt__['cmd.run'](\n            'gluster --xml --mode=script', stdin=\"{0}\\n\".format(cmd)\n        )\n\n    try:\n        root = ET.fromstring(_gluster_output_cleanup(result))\n    except ET.ParseError:\n        raise CommandExecutionError('\\n'.join(result.splitlines()[:-1]))\n\n    if _gluster_ok(root):\n        output = root.find('output')\n        if output is not None:\n            log.info('Gluster call \"%s\" succeeded: %s',\n                     cmd,\n                     root.find('output').text)\n        else:\n            log.info('Gluster call \"%s\" succeeded', cmd)\n    else:\n        log.error('Failed gluster call: %s: %s',\n                  cmd,\n                  root.find('opErrstr').text)\n\n    return root", "output": "Perform a gluster --xml command and log result.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vserver_exists(v_name, v_ip=None, v_port=None, v_type=None, **connection_args):\n    '''\n    \n    '''\n    vserver = _vserver_get(v_name, **connection_args)\n    if vserver is None:\n        return False\n    if v_ip is not None and vserver.get_ipv46() != v_ip:\n        return False\n    if v_port is not None and vserver.get_port() != v_port:\n        return False\n    if v_type is not None and vserver.get_servicetype().upper() != v_type.upper():\n        return False\n    return True", "output": "Checks if a vserver exists\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' netscaler.vserver_exists 'vserverName'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_one(func_name):\n    \"\"\"\n    \n    \"\"\"\n    doc = Docstring(func_name)\n    errs, wrns, examples_errs = get_validation_data(doc)\n    return {'type': doc.type,\n            'docstring': doc.clean_doc,\n            'deprecated': doc.deprecated,\n            'file': doc.source_file_name,\n            'file_line': doc.source_file_def_line,\n            'github_link': doc.github_url,\n            'errors': errs,\n            'warnings': wrns,\n            'examples_errors': examples_errs}", "output": "Validate the docstring for the given func_name\n\n    Parameters\n    ----------\n    func_name : function\n        Function whose docstring will be evaluated (e.g. pandas.read_csv).\n\n    Returns\n    -------\n    dict\n        A dictionary containing all the information obtained from validating\n        the docstring.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _notify_mutated(self, obj, old, hint=None):\n        ''' \n\n        '''\n        value = self.__get__(obj, obj.__class__)\n\n        # re-validate because the contents of 'old' have changed,\n        # in some cases this could give us a new object for the value\n        value = self.property.prepare_value(obj, self.name, value)\n\n        self._real_set(obj, old, value, hint=hint)", "output": "A method to call when a container is mutated \"behind our back\"\n        and we detect it with our |PropertyContainer| wrappers.\n\n        Args:\n            obj (HasProps) :\n                The object who's container value was mutated\n\n            old (object) :\n                The \"old\" value of the container\n\n                In this case, somewhat weirdly, ``old`` is a copy and the\n                new value should already be set unless we change it due to\n                validation.\n\n            hint (event hint or None, optional)\n                An optional update event hint, e.g. ``ColumnStreamedEvent``\n                (default: None)\n\n                Update event hints are usually used at times when better\n                update performance can be obtained by special-casing in\n                some way (e.g. streaming or patching column data sources)\n\n        Returns:\n            None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def text_classifier_learner(data:DataBunch, arch:Callable, bptt:int=70, max_len:int=70*20, config:dict=None, \n                            pretrained:bool=True, drop_mult:float=1., lin_ftrs:Collection[int]=None, \n                            ps:Collection[float]=None, **learn_kwargs) -> 'TextClassifierLearner':\n    \"\"\n    model = get_text_classifier(arch, len(data.vocab.itos), data.c, bptt=bptt, max_len=max_len,\n                                config=config, drop_mult=drop_mult, lin_ftrs=lin_ftrs, ps=ps)\n    meta = _model_meta[arch]\n    learn = RNNLearner(data, model, split_func=meta['split_clas'], **learn_kwargs)\n    if pretrained:\n        if 'url' not in meta: \n            warn(\"There are no pretrained weights for that architecture yet!\")\n            return learn\n        model_path = untar_data(meta['url'], data=False)\n        fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n        learn.load_pretrained(*fnames, strict=False)\n        learn.freeze()\n    return learn", "output": "Create a `Learner` with a text classifier from `data` and `arch`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode_interactively(estimator, hparams, decode_hp, checkpoint_path=None):\n  \"\"\"\"\"\"\n\n  is_image = \"image\" in hparams.problem.name\n  is_text2class = isinstance(hparams.problem,\n                             text_problems.Text2ClassProblem)\n  skip_eos_postprocess = (\n      is_image or is_text2class or decode_hp.skip_eos_postprocess)\n\n  def input_fn():\n    gen_fn = make_input_fn_from_generator(\n        _interactive_input_fn(hparams, decode_hp))\n    example = gen_fn()\n    example = _interactive_input_tensor_to_features_dict(example, hparams)\n    return example\n\n  result_iter = estimator.predict(input_fn, checkpoint_path=checkpoint_path)\n  for result in result_iter:\n    targets_vocab = hparams.problem_hparams.vocabulary[\"targets\"]\n\n    if decode_hp.return_beams:\n      beams = np.split(result[\"outputs\"], decode_hp.beam_size, axis=0)\n      scores = None\n      if \"scores\" in result:\n        if np.isscalar(result[\"scores\"]):\n          result[\"scores\"] = result[\"scores\"].reshape(1)\n        scores = np.split(result[\"scores\"], decode_hp.beam_size, axis=0)\n      for k, beam in enumerate(beams):\n        tf.logging.info(\"BEAM %d:\" % k)\n        beam_string = targets_vocab.decode(_save_until_eos(\n            beam, skip_eos_postprocess))\n        if scores is not None:\n          tf.logging.info(\"\\\"%s\\\"\\tScore:%f\" % (beam_string, scores[k]))\n        else:\n          tf.logging.info(\"\\\"%s\\\"\" % beam_string)\n    else:\n      if decode_hp.identity_output:\n        tf.logging.info(\" \".join(map(str, result[\"outputs\"].flatten())))\n      else:\n        tf.logging.info(\n            targets_vocab.decode(_save_until_eos(\n                result[\"outputs\"], skip_eos_postprocess)))", "output": "Interactive decoding.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hw_addr(iface):\n    '''\n    \n\n    '''\n    if salt.utils.platform.is_aix():\n        return _hw_addr_aix\n\n    iface_info, error = _get_iface_info(iface)\n\n    if error is False:\n        return iface_info.get(iface, {}).get('hwaddr', '')\n    else:\n        return error", "output": "Return the hardware address (a.k.a. MAC address) for a given interface\n\n    .. versionchanged:: 2016.11.4\n        Added support for AIX", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_delete(users, **kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'user.delete'\n            if not isinstance(users, list):\n                params = [users]\n            else:\n                params = users\n\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result']['userids']\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": "Delete zabbix users.\n\n    .. versionadded:: 2016.3.0\n\n    :param users: array of users (userids) to delete\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    :return: On success array with userids of deleted users.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.user_delete 15", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _scalar_from_string(\n            self,\n            value: str,\n    ) -> Union[Period, Timestamp, Timedelta, NaTType]:\n        \"\"\"\n        \n        \"\"\"\n        raise AbstractMethodError(self)", "output": "Construct a scalar type from a string.\n\n        Parameters\n        ----------\n        value : str\n\n        Returns\n        -------\n        Period, Timestamp, or Timedelta, or NaT\n            Whatever the type of ``self._scalar_type`` is.\n\n        Notes\n        -----\n        This should call ``self._check_compatible_with`` before\n        unboxing the result.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def data(self, index, role=Qt.DisplayRole):\n        \"\"\"\"\"\"\n        row = index.row()\n        if not index.isValid() or not (0 <= row < len(self.servers)):\n            return to_qvariant()\n\n        server = self.servers[row]\n        column = index.column()\n\n        if role == Qt.DisplayRole:\n            if column == LANGUAGE:\n                return to_qvariant(server.language)\n            elif column == ADDR:\n                text = '{0}:{1}'.format(server.host, server.port)\n                return to_qvariant(text)\n            elif column == CMD:\n                text = '&nbsp;<tt style=\"color:{0}\">{{0}} {{1}}</tt>'\n                text = text.format(self.text_color)\n                if server.external:\n                    text = '&nbsp;<tt>External server</tt>'\n                return to_qvariant(text.format(server.cmd, server.args))\n        elif role == Qt.TextAlignmentRole:\n            return to_qvariant(int(Qt.AlignHCenter | Qt.AlignVCenter))\n        return to_qvariant()", "output": "Qt Override.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def skip(self, chars=spaceCharactersBytes):\n        \"\"\"\"\"\"\n        p = self.position               # use property for the error-checking\n        while p < len(self):\n            c = self[p:p + 1]\n            if c not in chars:\n                self._position = p\n                return c\n            p += 1\n        self._position = p\n        return None", "output": "Skip past a list of characters", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def worker(sock, authenticated):\n    \"\"\"\n    \n    \"\"\"\n    signal.signal(SIGHUP, SIG_DFL)\n    signal.signal(SIGCHLD, SIG_DFL)\n    signal.signal(SIGTERM, SIG_DFL)\n    # restore the handler for SIGINT,\n    # it's useful for debugging (show the stacktrace before exit)\n    signal.signal(SIGINT, signal.default_int_handler)\n\n    # Read the socket using fdopen instead of socket.makefile() because the latter\n    # seems to be very slow; note that we need to dup() the file descriptor because\n    # otherwise writes also cause a seek that makes us miss data on the read side.\n    infile = os.fdopen(os.dup(sock.fileno()), \"rb\", 65536)\n    outfile = os.fdopen(os.dup(sock.fileno()), \"wb\", 65536)\n\n    if not authenticated:\n        client_secret = UTF8Deserializer().loads(infile)\n        if os.environ[\"PYTHON_WORKER_FACTORY_SECRET\"] == client_secret:\n            write_with_length(\"ok\".encode(\"utf-8\"), outfile)\n            outfile.flush()\n        else:\n            write_with_length(\"err\".encode(\"utf-8\"), outfile)\n            outfile.flush()\n            sock.close()\n            return 1\n\n    exit_code = 0\n    try:\n        worker_main(infile, outfile)\n    except SystemExit as exc:\n        exit_code = compute_real_exit_code(exc.code)\n    finally:\n        try:\n            outfile.flush()\n        except Exception:\n            pass\n    return exit_code", "output": "Called by a worker process after the fork().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transform_column_source_data(data, buffers=None, cols=None):\n    ''' \n\n    '''\n    to_transform = set(data) if cols is None else set(cols)\n\n    data_copy = {}\n    for key in to_transform:\n        if pd and isinstance(data[key], (pd.Series, pd.Index)):\n            data_copy[key] = transform_series(data[key], buffers=buffers)\n        elif isinstance(data[key], np.ndarray):\n            data_copy[key] = transform_array(data[key], buffers=buffers)\n        else:\n            data_copy[key] = traverse_data(data[key], buffers=buffers)\n\n    return data_copy", "output": "Transform ``ColumnSourceData`` data to a serialized format\n\n    Args:\n        data (dict) : the mapping of names to data columns to transform\n\n        buffers (set, optional) :\n            If binary buffers are desired, the buffers parameter may be\n            provided, and any columns that may be sent as binary buffers\n            will be added to the set. If None, then only base64 encoding\n            will be used (default: None)\n\n            **This is an \"out\" parameter**. The values it contains will be\n            modified in-place.\n\n        cols (list[str], optional) :\n            Optional list of subset of columns to transform. If None, all\n            columns will be transformed (default: None)\n\n    Returns:\n        JSON compatible dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avatar_url_as(self, *, format=None, size=1024):\n        \"\"\"\n        \"\"\"\n        if self.avatar is None:\n            # Default is always blurple apparently\n            return Asset(self._state, 'https://cdn.discordapp.com/embed/avatars/0.png')\n\n        if not utils.valid_icon_size(size):\n            raise InvalidArgument(\"size must be a power of 2 between 16 and 1024\")\n\n        format = format or 'png'\n\n        if format not in ('png', 'jpg', 'jpeg'):\n            raise InvalidArgument(\"format must be one of 'png', 'jpg', or 'jpeg'.\")\n\n        url = 'https://cdn.discordapp.com/avatars/{0.id}/{0.avatar}.{1}?size={2}'.format(self, format, size)\n        return Asset(self._state, url)", "output": "Returns a friendly URL version of the avatar the webhook has.\n\n        If the webhook does not have a traditional avatar, their default\n        avatar URL is returned instead.\n\n        The format must be one of 'jpeg', 'jpg', or 'png'.\n        The size must be a power of 2 between 16 and 1024.\n\n        Parameters\n        -----------\n        format: Optional[:class:`str`]\n            The format to attempt to convert the avatar to.\n            If the format is ``None``, then it is equivalent to png.\n        size: :class:`int`\n            The size of the image to display.\n\n        Raises\n        ------\n        InvalidArgument\n            Bad image format passed to ``format`` or invalid ``size``.\n\n        Returns\n        --------\n        :class:`Asset`\n            The resulting CDN asset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render(opts, functions, states=None, proxy=None, context=None):\n    '''\n    \n    '''\n    if context is None:\n        context = {}\n\n    pack = {'__salt__': functions,\n            '__grains__': opts.get('grains', {}),\n            '__context__': context}\n\n    if states:\n        pack['__states__'] = states\n    pack['__proxy__'] = proxy or {}\n    ret = LazyLoader(\n        _module_dirs(\n            opts,\n            'renderers',\n            'render',\n            ext_type_dirs='render_dirs',\n        ),\n        opts,\n        tag='render',\n        pack=pack,\n    )\n    rend = FilterDictWrapper(ret, '.render')\n\n    if not check_render_pipe_str(opts['renderer'], rend, opts['renderer_blacklist'], opts['renderer_whitelist']):\n        err = ('The renderer {0} is unavailable, this error is often because '\n               'the needed software is unavailable'.format(opts['renderer']))\n        log.critical(err)\n        raise LoaderError(err)\n    return rend", "output": "Returns the render modules", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send(self, local, remote, makedirs=False):\n        '''\n        \n        '''\n        if makedirs:\n            self.exec_cmd('mkdir -p {0}'.format(os.path.dirname(remote)))\n\n        # scp needs [<ipv6}\n        host = self.host\n        if ':' in host:\n            host = '[{0}]'.format(host)\n\n        cmd = '{0} {1}:{2}'.format(local, host, remote)\n        cmd = self._cmd_str(cmd, ssh='scp')\n\n        logmsg = 'Executing command: {0}'.format(cmd)\n        if self.passwd:\n            logmsg = logmsg.replace(self.passwd, ('*' * 6))\n        log.debug(logmsg)\n\n        return self._run_cmd(cmd)", "output": "scp a file or files to a remote system", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_patch(self, path: str, handler: _WebHandler,\n                  **kwargs: Any) -> AbstractRoute:\n        \"\"\"\n        \n        \"\"\"\n        return self.add_route(hdrs.METH_PATCH, path, handler, **kwargs)", "output": "Shortcut for add_route with method PATCH", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def print_action(self, test_succeed, action):\n        '''\n        \n        '''\n        #self.info_print(\">>> {0}\",action.keys())\n        if not test_succeed or action['info']['always_show_run_output']:\n            output = action['output'].strip()\n            if output != \"\":\n                p = self.fail_print if action['result'] == 'fail' else self.p_print\n                self.info_print(\"\")\n                self.info_print(\"({0}) {1}\",action['info']['name'],action['info']['path'])\n                p(\"\")\n                p(\"{0}\",action['command'].strip())\n                p(\"\")\n                for line in output.splitlines():\n                    p(\"{0}\",line.encode('utf-8'))", "output": "Print the detailed info of failed or always print tests.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _compare_or_regex_search(a, b, regex=False):\n    \"\"\"\n    \n    \"\"\"\n    if not regex:\n        op = lambda x: operator.eq(x, b)\n    else:\n        op = np.vectorize(lambda x: bool(re.search(b, x)) if isinstance(x, str)\n                          else False)\n\n    is_a_array = isinstance(a, np.ndarray)\n    is_b_array = isinstance(b, np.ndarray)\n\n    # numpy deprecation warning to have i8 vs integer comparisons\n    if is_datetimelike_v_numeric(a, b):\n        result = False\n\n    # numpy deprecation warning if comparing numeric vs string-like\n    elif is_numeric_v_string_like(a, b):\n        result = False\n    else:\n        result = op(a)\n\n    if is_scalar(result) and (is_a_array or is_b_array):\n        type_names = [type(a).__name__, type(b).__name__]\n\n        if is_a_array:\n            type_names[0] = 'ndarray(dtype={dtype})'.format(dtype=a.dtype)\n\n        if is_b_array:\n            type_names[1] = 'ndarray(dtype={dtype})'.format(dtype=b.dtype)\n\n        raise TypeError(\n            \"Cannot compare types {a!r} and {b!r}\".format(a=type_names[0],\n                                                          b=type_names[1]))\n    return result", "output": "Compare two array_like inputs of the same shape or two scalar values\n\n    Calls operator.eq or re.search, depending on regex argument. If regex is\n    True, perform an element-wise regex matching.\n\n    Parameters\n    ----------\n    a : array_like or scalar\n    b : array_like or scalar\n    regex : bool, default False\n\n    Returns\n    -------\n    mask : array_like of bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sorted(self, iterator, key=None, reverse=False):\n        \"\"\"\n        \n        \"\"\"\n        global MemoryBytesSpilled, DiskBytesSpilled\n        batch, limit = 100, self._next_limit()\n        chunks, current_chunk = [], []\n        iterator = iter(iterator)\n        while True:\n            # pick elements in batch\n            chunk = list(itertools.islice(iterator, batch))\n            current_chunk.extend(chunk)\n            if len(chunk) < batch:\n                break\n\n            used_memory = get_used_memory()\n            if used_memory > limit:\n                # sort them inplace will save memory\n                current_chunk.sort(key=key, reverse=reverse)\n                path = self._get_path(len(chunks))\n                with open(path, 'wb') as f:\n                    self.serializer.dump_stream(current_chunk, f)\n\n                def load(f):\n                    for v in self.serializer.load_stream(f):\n                        yield v\n                    # close the file explicit once we consume all the items\n                    # to avoid ResourceWarning in Python3\n                    f.close()\n                chunks.append(load(open(path, 'rb')))\n                current_chunk = []\n                MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20\n                DiskBytesSpilled += os.path.getsize(path)\n                os.unlink(path)  # data will be deleted after close\n\n            elif not chunks:\n                batch = min(int(batch * 1.5), 10000)\n\n        current_chunk.sort(key=key, reverse=reverse)\n        if not chunks:\n            return current_chunk\n\n        if current_chunk:\n            chunks.append(iter(current_chunk))\n\n        return heapq.merge(chunks, key=key, reverse=reverse)", "output": "Sort the elements in iterator, do external sort when the memory\n        goes above the limit.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_states(self, states=None, value=None):\n        \"\"\"\n        \"\"\"\n        assert self.binded and self.params_initialized\n        self._curr_module.set_states(states, value)", "output": "Sets value for states. Only one of states & values can be specified.\n\n        Parameters\n        ----------\n        states : list of list of NDArrays\n            Source states arrays formatted like ``[[state1_dev1, state1_dev2],\n            [state2_dev1, state2_dev2]]``.\n        value : number\n            A single scalar value for all state arrays.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_sdist(sdist_directory, config_settings=None):\n    \"\"\"\"\"\"\n    poetry = Poetry.create(\".\")\n\n    path = SdistBuilder(poetry, SystemEnv(Path(sys.prefix)), NullIO()).build(\n        Path(sdist_directory)\n    )\n\n    return unicode(path.name)", "output": "Builds an sdist, places it in sdist_directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sentinel_get_master_ip(master, host=None, port=None, password=None):\n    '''\n    \n    '''\n    server = _sconnect(host, port, password)\n    ret = server.sentinel_get_master_addr_by_name(master)\n    return dict(list(zip(('master_host', 'master_port'), ret)))", "output": "Get ip for sentinel master\n\n    .. versionadded: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.sentinel_get_master_ip 'mymaster'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_dim_size(start, stop, step):\n    \"\"\"\"\"\"\n    assert step != 0\n    if step > 0:\n        assert start < stop\n        dim_size = (stop - start - 1) // step + 1\n    else:\n        assert stop < start\n        dim_size = (start - stop - 1) // (-step) + 1\n    return dim_size", "output": "Given start, stop, and stop, calculate the number of elements\n    of this slice.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_network_settings(**settings):\n    '''\n    \n    '''\n    if __grains__['lsb_distrib_id'] == 'nilrt':\n        raise salt.exceptions.CommandExecutionError('Not supported in this version.')\n    changes = []\n    if 'networking' in settings:\n        if settings['networking'] in _CONFIG_TRUE:\n            __salt__['service.enable']('connman')\n        else:\n            __salt__['service.disable']('connman')\n\n    if 'hostname' in settings:\n        new_hostname = settings['hostname'].split('.', 1)[0]\n        settings['hostname'] = new_hostname\n        old_hostname = __salt__['network.get_hostname']\n        if new_hostname != old_hostname:\n            __salt__['network.mod_hostname'](new_hostname)\n            changes.append('hostname={0}'.format(new_hostname))\n\n    return changes", "output": "Build the global network script.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ip.build_network_settings <settings>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate(config):\n    '''\n    \n    '''\n    # Must be a list of dicts.\n    if not isinstance(config, list):\n        return False, 'Configuration for napalm beacon must be a list.'\n    for mod in config:\n        fun = mod.keys()[0]\n        fun_cfg = mod.values()[0]\n        if not isinstance(fun_cfg, dict):\n            return False, 'The match structure for the {} execution function output must be a dictionary'.format(fun)\n        if fun not in __salt__:\n            return False, 'Execution function {} is not availabe!'.format(fun)\n    return True, 'Valid configuration for the napal beacon!'", "output": "Validate the beacon configuration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(name):\n    '''\n    \n    '''\n    ret = {}\n    cmd = '{0} startvm {1}'.format(vboxcmd(), name)\n    ret = salt.modules.cmdmod.run(cmd).splitlines()\n    return ret", "output": "Start a VM\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' vboxmanage.start my_vm", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _gotitem(self, key, ndim, subset=None):\n        \"\"\"\n        \n        \"\"\"\n        # create a new object to prevent aliasing\n        if subset is None:\n            subset = self.obj\n\n        # we need to make a shallow copy of ourselves\n        # with the same groupby\n        kwargs = {attr: getattr(self, attr) for attr in self._attributes}\n\n        # Try to select from a DataFrame, falling back to a Series\n        try:\n            groupby = self._groupby[key]\n        except IndexError:\n            groupby = self._groupby\n\n        self = self.__class__(subset,\n                              groupby=groupby,\n                              parent=self,\n                              **kwargs)\n        self._reset_cache()\n        if subset.ndim == 2:\n            if is_scalar(key) and key in subset or is_list_like(key):\n                self._selection = key\n        return self", "output": "Sub-classes to define. Return a sliced object.\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : 1,2\n            requested ndim of result\n        subset : object, default None\n            subset to act on", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def muc(clusters, mention_to_gold):\n        \"\"\"\n        \n        \"\"\"\n        true_p, all_p = 0, 0\n        for cluster in clusters:\n            all_p += len(cluster) - 1\n            true_p += len(cluster)\n            linked = set()\n            for mention in cluster:\n                if mention in mention_to_gold:\n                    linked.add(mention_to_gold[mention])\n                else:\n                    true_p -= 1\n            true_p -= len(linked)\n        return true_p, all_p", "output": "Counts the mentions in each predicted cluster which need to be re-allocated in\n        order for each predicted cluster to be contained by the respective gold cluster.\n        <http://aclweb.org/anthology/M/M95/M95-1005.pdf>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def definitions_errors(self):\n        \"\"\" \n            \"\"\"\n        if not self.is_logic_error:\n            return None\n\n        result = defaultdict(list)\n        for error in self.child_errors:\n            i = error.schema_path[len(self.schema_path)]\n            result[i].append(error)\n        return result", "output": "Dictionary with errors of an *of-rule mapped to the index of the\n            definition it occurred in. Returns :obj:`None` if not applicable.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_docker_tag(platform: str, registry: str) -> str:\n    \"\"\"\"\"\"\n    platform = platform if any(x in platform for x in ['build.', 'publish.']) else 'build.{}'.format(platform)\n    if not registry:\n        registry = \"mxnet_local\"\n    return \"{0}/{1}\".format(registry, platform)", "output": ":return: docker tag to be used for the container", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_tensorboard(args):\n    ''''''\n    experiment_id = check_experiment_id(args)\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    config_file_name = experiment_dict[experiment_id]['fileName']\n    nni_config = Config(config_file_name)\n    rest_port = nni_config.get_config('restServerPort')\n    rest_pid = nni_config.get_config('restServerPid')\n    if not detect_process(rest_pid):\n        print_error('Experiment is not running...')\n        return\n    running, response = check_rest_server_quick(rest_port)\n    trial_content = None\n    if running:\n        response = rest_get(trial_jobs_url(rest_port), REST_TIME_OUT)\n        if response and check_response(response):\n            trial_content = json.loads(response.text)\n        else:\n            print_error('List trial failed...')\n    else:\n        print_error('Restful server is not running...')\n    if not trial_content:\n        print_error('No trial information!')\n        exit(1)\n    if len(trial_content) > 1 and not args.trial_id:\n        print_error('There are multiple trials, please set trial id!')\n        exit(1)\n    experiment_id = nni_config.get_config('experimentId')\n    temp_nni_path = os.path.join(tempfile.gettempdir(), 'nni', experiment_id)\n    os.makedirs(temp_nni_path, exist_ok=True)\n\n    path_list = get_path_list(args, nni_config, trial_content, temp_nni_path)\n    start_tensorboard_process(args, nni_config, path_list, temp_nni_path)", "output": "start tensorboard", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_reference(self, reference):\n        \"\"\"\n        \"\"\"\n        self.set_categorical_feature(reference.categorical_feature) \\\n            .set_feature_name(reference.feature_name) \\\n            ._set_predictor(reference._predictor)\n        # we're done if self and reference share a common upstrem reference\n        if self.get_ref_chain().intersection(reference.get_ref_chain()):\n            return self\n        if self.data is not None:\n            self.reference = reference\n            return self._free_handle()\n        else:\n            raise LightGBMError(\"Cannot set reference after freed raw data, \"\n                                \"set free_raw_data=False when construct Dataset to avoid this.\")", "output": "Set reference Dataset.\n\n        Parameters\n        ----------\n        reference : Dataset\n            Reference that is used as a template to construct the current Dataset.\n\n        Returns\n        -------\n        self : Dataset\n            Dataset with set reference.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _resizeCurrentColumnToContents(self, new_index, old_index):\r\n        \"\"\"\"\"\"\r\n        if new_index.column() not in self._autosized_cols:\r\n            # Ensure the requested column is fully into view after resizing\r\n            self._resizeVisibleColumnsToContents()\r\n            self.dataTable.scrollTo(new_index)", "output": "Resize the current column to its contents.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_stack_index(self, stack_index, plugin_index):\n        \"\"\"\"\"\"\n        other_plugins_count = sum([other_tabs[0].count() \\\n                                   for other_tabs in \\\n                                   self.plugins_tabs[:plugin_index]])\n        real_index = stack_index - other_plugins_count\n\n        return real_index", "output": "Get the real index of the selected item.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _unpickle_panel_compat(self, state):  # pragma: no cover\n        \"\"\"\n        \n        \"\"\"\n        from pandas.io.pickle import _unpickle_array\n\n        _unpickle = _unpickle_array\n        vals, items, major, minor = state\n\n        items = _unpickle(items)\n        major = _unpickle(major)\n        minor = _unpickle(minor)\n        values = _unpickle(vals)\n        wp = Panel(values, items, major, minor)\n        self._data = wp._data", "output": "Unpickle the panel.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_pbar(self, pbar_num=1, pbar_msg=None):\n        \"\"\"\n        \"\"\"\n        if not isinstance(self._pbar, type(None)):\n            if self.verbosity > 2 and pbar_msg is not None:\n                self._pbar.write(pbar_msg, file=self._file)\n            if not self._pbar.disable:\n                self._pbar.update(pbar_num)", "output": "Update self._pbar and error message during pipeline evaluation.\n\n        Parameters\n        ----------\n        pbar_num: int\n            How many pipelines has been processed\n        pbar_msg: None or string\n            Error message\n\n        Returns\n        -------\n        None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def move_right(self, keep_anchor=False, nb_chars=1):\n        \"\"\"\n        \n        \"\"\"\n        text_cursor = self._editor.textCursor()\n        text_cursor.movePosition(\n            text_cursor.Right, text_cursor.KeepAnchor if keep_anchor else\n            text_cursor.MoveAnchor, nb_chars)\n        self._editor.setTextCursor(text_cursor)", "output": "Moves the cursor on the right.\n\n        :param keep_anchor: True to keep anchor (to select text) or False to\n            move the anchor (no selection)\n        :param nb_chars: Number of characters to move.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _rowwise_unsorted_segment_sum(values, indices, n):\n  \"\"\"\n  \"\"\"\n  batch, k = tf.unstack(tf.shape(indices), num=2)\n  indices_flat = tf.reshape(indices, [-1]) + tf.div(tf.range(batch * k), k) * n\n  ret_flat = tf.unsorted_segment_sum(\n      tf.reshape(values, [-1]), indices_flat, batch * n)\n  return tf.reshape(ret_flat, [batch, n])", "output": "UnsortedSegmentSum on each row.\n\n  Args:\n    values: a `Tensor` with shape `[batch_size, k]`.\n    indices: an integer `Tensor` with shape `[batch_size, k]`.\n    n: an integer.\n  Returns:\n    A `Tensor` with the same type as `values` and shape `[batch_size, n]`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_query_parameters(parameters):\n    \"\"\"\n    \"\"\"\n    if parameters is None:\n        return []\n\n    if isinstance(parameters, collections_abc.Mapping):\n        return to_query_parameters_dict(parameters)\n\n    return to_query_parameters_list(parameters)", "output": "Converts DB-API parameter values into query parameters.\n\n    :type parameters: Mapping[str, Any] or Sequence[Any]\n    :param parameters: A dictionary or sequence of query parameter values.\n\n    :rtype: List[google.cloud.bigquery.query._AbstractQueryParameter]\n    :returns: A list of query parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rlmb_tiny_stochastic():\n  \"\"\"\"\"\"\n  hparams = rlmb_ppo_tiny()\n  hparams.epochs = 1  # Too slow with 2 for regular runs.\n  hparams.generative_model = \"next_frame_basic_stochastic\"\n  hparams.generative_model_params = \"next_frame_basic_stochastic\"\n  return hparams", "output": "Tiny setting with a stochastic next-frame model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_route_table(route_table_id=None, route_table_name=None,\n                       region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    return _delete_resource(resource='route_table', name=route_table_name,\n                            resource_id=route_table_id, region=region, key=key,\n                            keyid=keyid, profile=profile)", "output": "Deletes a route table.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt myminion boto_vpc.delete_route_table route_table_id='rtb-1f382e7d'\n        salt myminion boto_vpc.delete_route_table route_table_name='myroutetable'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _CheckForOutOfOrderStepAndMaybePurge(self, event):\n    \"\"\"\n    \"\"\"\n    if event.step < self.most_recent_step and event.HasField('summary'):\n      self._Purge(event, by_tags=True)\n    else:\n      self.most_recent_step = event.step\n      self.most_recent_wall_time = event.wall_time", "output": "Check for out-of-order event.step and discard expired events for tags.\n\n    Check if the event is out of order relative to the global most recent step.\n    If it is, purge outdated summaries for tags that the event contains.\n\n    Args:\n      event: The event to use as reference. If the event is out-of-order, all\n        events with the same tags, but with a greater event.step will be purged.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def master_pub(self):\n        '''\n        \n        '''\n        return _get_master_uri(self.opts['master_ip'],\n                               self.publish_port,\n                               source_ip=self.opts.get('source_ip'),\n                               source_port=self.opts.get('source_publish_port'))", "output": "Return the master publish port", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def snippets(session):\n    \"\"\"\"\"\"\n\n    # Sanity check: Only run snippets tests if the environment variable is set.\n    if not os.environ.get(\"GOOGLE_APPLICATION_CREDENTIALS\", \"\"):\n        session.skip(\"Credentials must be set via environment variable.\")\n\n    # Install all test dependencies, then install local packages in place.\n    session.install(\"mock\", \"pytest\")\n    for local_dep in LOCAL_DEPS:\n        session.install(\"-e\", local_dep)\n    session.install(\"-e\", os.path.join(\"..\", \"storage\"))\n    session.install(\"-e\", os.path.join(\"..\", \"test_utils\"))\n    session.install(\"-e\", \".[all]\")\n\n    # Run py.test against the snippets tests.\n    session.run(\"py.test\", os.path.join(\"docs\", \"snippets.py\"), *session.posargs)\n    session.run(\"py.test\", \"samples\", *session.posargs)", "output": "Run the snippets test suite.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(self):\n        \"\"\"\n        \"\"\"\n        api = self._client.instance_admin_api\n        metadata = _metadata_with_prefix(self.name)\n\n        try:\n            api.get_instance(self.name, metadata=metadata)\n        except NotFound:\n            return False\n\n        return True", "output": "Test whether this instance exists.\n\n        See\n        https://cloud.google.com/spanner/reference/rpc/google.spanner.admin.instance.v1#google.spanner.admin.instance.v1.InstanceAdmin.GetInstanceConfig\n\n        :rtype: bool\n        :returns: True if the instance exists, else false", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_environment_vars(self, prefix=SANIC_PREFIX):\n        \"\"\"\n        \n        \"\"\"\n        for k, v in os.environ.items():\n            if k.startswith(prefix):\n                _, config_key = k.split(prefix, 1)\n                try:\n                    self[config_key] = int(v)\n                except ValueError:\n                    try:\n                        self[config_key] = float(v)\n                    except ValueError:\n                        try:\n                            self[config_key] = strtobool(v)\n                        except ValueError:\n                            self[config_key] = v", "output": "Looks for prefixed environment variables and applies\n        them to the configuration if present.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sortByColumn(self, index):\r\n        \"\"\"\"\"\"\r\n        if self.sort_old == [None]:\r\n            self.header_class.setSortIndicatorShown(True)\r\n        sort_order = self.header_class.sortIndicatorOrder()\r\n        self.sig_sort_by_column.emit()\r\n        if not self.model().sort(index, sort_order):\r\n            if len(self.sort_old) != 2:\r\n                self.header_class.setSortIndicatorShown(False)\r\n            else:\r\n                self.header_class.setSortIndicator(self.sort_old[0],\r\n                                                   self.sort_old[1])\r\n            return\r\n        self.sort_old = [index, self.header_class.sortIndicatorOrder()]", "output": "Implement a column sort.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_installed(path, user=None):\n    '''\n    \n    '''\n    retcode = __salt__['cmd.retcode']((\n        'wp --path={0} core is-installed'\n    ).format(path), runas=user)\n    if retcode == 0:\n        return True\n    return False", "output": "Check if wordpress is installed and setup\n\n    path\n        path to wordpress install location\n\n    user\n        user to run the command as\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' wordpress.is_installed /var/www/html apache", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate(self, key, value):\n    \"\"\"\"\"\"\n    if self._validator is not None:\n      self._validator(key, value)", "output": "Validation function run before setting. Uses function from __init__.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vq_discrete_unbottleneck(x, hidden_size):\n  \"\"\"\"\"\"\n  x_shape = common_layers.shape_list(x)\n  x = tf.to_float(x)\n  bottleneck_size = common_layers.shape_list(x)[-1]\n  means, _, _ = get_vq_codebook(bottleneck_size, hidden_size)\n  result = tf.matmul(tf.reshape(x, [-1, x_shape[-1]]), means)\n  return tf.reshape(result, x_shape[:-1] + [hidden_size])", "output": "Simple undiscretization from vector quantized representation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_hostmask(self, ip_str):\n        \"\"\"\n\n        \"\"\"\n        bits = ip_str.split('.')\n        try:\n            parts = [x for x in map(int, bits) if x in self._valid_mask_octets]\n        except ValueError:\n            return False\n        if len(parts) != len(bits):\n            return False\n        if parts[0] < parts[-1]:\n            return True\n        return False", "output": "Test if the IP string is a hostmask (rather than a netmask).\n\n        Args:\n            ip_str: A string, the potential hostmask.\n\n        Returns:\n            A boolean, True if the IP string is a hostmask.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_error(self, errstr):\n        '''\n        \n        '''\n        for line in errstr.split('\\n'):\n            if line.startswith('ssh:'):\n                return line\n            if line.startswith('Pseudo-terminal'):\n                continue\n            if 'to the list of known hosts.' in line:\n                continue\n            return line\n        return errstr", "output": "Parse out an error and return a targeted error string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def idx_sequence(self):\n        \"\"\"\n        \"\"\"\n        return [x[1] for x in sorted(zip(self._record, list(range(len(self._record)))))]", "output": "Indices of sentences when enumerating data set from batches.\n        Useful when retrieving the correct order of sentences\n\n        Returns\n        -------\n        list\n            List of ids ranging from 0 to #sent -1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_files_distributed(generator,\n                               output_name,\n                               output_dir,\n                               num_shards=1,\n                               max_cases=None,\n                               task_id=0):\n  \"\"\"\"\"\"\n  assert task_id < num_shards\n  output_filename = sharded_name(output_name, task_id, num_shards)\n  output_file = os.path.join(output_dir, output_filename)\n  tf.logging.info(\"Writing to file %s\", output_file)\n  writer = tf.python_io.TFRecordWriter(output_file)\n\n  counter = 0\n  for case in generator:\n    if counter % 100000 == 0:\n      tf.logging.info(\"Generating case %d for %s.\" % (counter, output_name))\n    counter += 1\n    if max_cases and counter > max_cases:\n      break\n    example = to_example(case)\n    writer.write(example.SerializeToString())\n\n  writer.close()\n  return output_file", "output": "generate_files but with a single writer writing to shard task_id.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compute_values(edge_compatibility, v):\n  \"\"\"\n  \"\"\"\n\n  # Computes the incoming value vectors for each node by weighting them\n  # according to the attention weights. These values are still segregated by\n  # edge type.\n  # Shape = [B, T, N, V].\n  all_edge_values = tf.matmul(tf.to_float(edge_compatibility), v)\n\n  # Combines the weighted value vectors together across edge types into a\n  # single N x V matrix for each batch.\n  output = tf.reduce_sum(all_edge_values, axis=1)  # Shape [B, N, V].\n  return output", "output": "Compute values. If edge compatibilities is just adjacency, we get ggnn.\n\n  Args:\n    edge_compatibility: A tensor of shape [batch, num_transforms, length, depth]\n    v: A tensor of shape [batch, num_transforms, length, depth]\n\n  Returns:\n    output: A [batch, length, depth] tensor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cli(name, format='text', **kwargs):\n    '''\n    \n    '''\n    ret = {'name': name, 'changes': {}, 'result': True, 'comment': ''}\n    ret['changes'] = __salt__['junos.cli'](name, format, **kwargs)\n    return ret", "output": "Executes the CLI commands and reuturns the text output.\n\n    .. code-block:: yaml\n\n            show version:\n              junos:\n                - cli\n                - format: xml\n\n    Parameters:\n      Required\n        * command:\n          The command that need to be executed on Junos CLI. (default = None)\n      Optional\n        * format:\n          Format in which to get the CLI output. (text or xml, \\\n            default = 'text')\n        * kwargs: Keyworded arguments which can be provided like-\n            * timeout:\n              Set NETCONF RPC timeout. Can be used for commands which\n              take a while to execute. (default = 30 seconds)\n            * dest:\n              The destination file where the CLI output can be stored.\\\n               (default = None)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _coerce_to_ndarray(self):\n        \"\"\"\n        \n        \"\"\"\n\n        # TODO(jreback) make this better\n        data = self._data.astype(object)\n        data[self._mask] = self._na_value\n        return data", "output": "coerce to an ndarary of object dtype", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_item_intersection_info(self, item_pairs):\n        \"\"\"\n        \n        \"\"\"\n\n        if type(item_pairs) is list:\n            if not all(type(t) in [list, tuple] and len(t) == 2 for t in item_pairs):\n                raise TypeError(\"item_pairs must be 2-column SFrame of two item \"\n                                \"columns, or a list of (item_1, item_2) tuples. \")\n\n            item_name = self.item_id\n            item_pairs = _turicreate.SFrame({item_name + \"_1\" : [v1 for v1, v2 in item_pairs],\n                                           item_name + \"_2\" : [v2 for v1, v2 in item_pairs]})\n\n        if not isinstance(item_pairs, _turicreate.SFrame):\n            raise TypeError(\"item_pairs must be 2-column SFrame of two item \"\n                            \"columns, or a list of (item_1, item_2) tuples. \")\n        \n        response = self.__proxy__.get_item_intersection_info(item_pairs)\n        return response", "output": "For a collection of item -> item pairs, returns information about the\n        users in that intersection.\n\n        Parameters\n        ----------\n\n        item_pairs : 2-column SFrame of two item columns, or a list of\n           (item_1, item_2) tuples.\n\n        Returns\n        -------\n        out : SFrame\n           A SFrame with the two item columns given above, the number of\n           users that rated each, and a dictionary mapping the user to a\n           pair of the ratings, with the first rating being the rating of\n           the first item and the second being the rating of the second item.\n           If no ratings are provided, these values are always 1.0.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_network(batch_size, update_freq):\n    \"\"\"\n    \"\"\"\n    import logging\n    head = '%(asctime)-15s %(message)s'\n    logging.basicConfig(level=logging.INFO, format=head)\n\n    train_data = np.random.randint(1, 5, [1000, 2])\n    weights = np.array([1.0, 2.0])\n    train_label = train_data.dot(weights)\n\n    di = mx.io.NDArrayIter(train_data, train_label, batch_size=batch_size, shuffle=True, label_name='lin_reg_label')\n    X = mx.sym.Variable('data')\n    Y = mx.symbol.Variable('lin_reg_label')\n    fully_connected_layer = mx.sym.FullyConnected(data=X, name='fc1', num_hidden=1)\n    lro = mx.sym.LinearRegressionOutput(data=fully_connected_layer, label=Y, name=\"lro\")\n\n    mod = SVRGModule(\n        symbol=lro,\n        data_names=['data'],\n        label_names=['lin_reg_label'], update_freq=update_freq, logger=logging\n    )\n\n    return di, mod", "output": "Create a linear regression network for performing SVRG optimization.\n    Parameters\n    ----------\n    batch_size: int\n        Size of data split\n    update_freq: int\n        Update Frequency for calculating full gradients\n\n    Returns\n    ----------\n    di: mx.io.NDArrayIter\n        Data iterator\n    update_freq: SVRGModule\n        An instance of SVRGModule for performing SVRG optimization", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_output(script):\n    \"\"\"\n\n    \"\"\"\n    if six.PY2:\n        logs.warn('Experimental instant mode is Python 3+ only')\n        return None\n\n    if 'THEFUCK_OUTPUT_LOG' not in os.environ:\n        logs.warn(\"Output log isn't specified\")\n        return None\n\n    if const.USER_COMMAND_MARK not in os.environ.get('PS1', ''):\n        logs.warn(\n            \"PS1 doesn't contain user command mark, please ensure \"\n            \"that PS1 is not changed after The Fuck alias initialization\")\n        return None\n\n    try:\n        with logs.debug_time(u'Read output from log'):\n            fd = os.open(os.environ['THEFUCK_OUTPUT_LOG'], os.O_RDONLY)\n            buffer = mmap.mmap(fd, const.LOG_SIZE_IN_BYTES, mmap.MAP_SHARED, mmap.PROT_READ)\n            _skip_old_lines(buffer)\n            lines = _get_output_lines(script, buffer)\n            output = '\\n'.join(lines).strip()\n            logs.debug(u'Received output: {}'.format(output))\n            return output\n    except OSError:\n        logs.warn(\"Can't read output log\")\n        return None\n    except ScriptNotInLog:\n        logs.warn(\"Script not found in output log\")\n        return None", "output": "Reads script output from log.\n\n    :type script: str\n    :rtype: str | None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def linux_interfaces():\n    '''\n    \n    '''\n    ifaces = dict()\n    ip_path = salt.utils.path.which('ip')\n    ifconfig_path = None if ip_path else salt.utils.path.which('ifconfig')\n    if ip_path:\n        cmd1 = subprocess.Popen(\n            '{0} link show'.format(ip_path),\n            shell=True,\n            close_fds=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT).communicate()[0]\n        cmd2 = subprocess.Popen(\n            '{0} addr show'.format(ip_path),\n            shell=True,\n            close_fds=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT).communicate()[0]\n        ifaces = _interfaces_ip(\"{0}\\n{1}\".format(\n            salt.utils.stringutils.to_str(cmd1),\n            salt.utils.stringutils.to_str(cmd2)))\n    elif ifconfig_path:\n        cmd = subprocess.Popen(\n            '{0} -a'.format(ifconfig_path),\n            shell=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT).communicate()[0]\n        ifaces = _interfaces_ifconfig(salt.utils.stringutils.to_str(cmd))\n    return ifaces", "output": "Obtain interface information for *NIX/BSD variants", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_egg_info(ireq):\n    \"\"\"\n    \"\"\"\n    root = ireq.setup_py_dir\n\n    directory_iterator = _iter_egg_info_directories(root, ireq.name)\n    try:\n        top_egg_info = next(directory_iterator)\n    except StopIteration:   # No egg-info found. Wat.\n        return None\n    directory_iterator = itertools.chain([top_egg_info], directory_iterator)\n\n    # Read the sdist's PKG-INFO to determine which egg_info is best.\n    pkg_info = _read_pkg_info(root)\n\n    # PKG-INFO not readable. Just return whatever comes first, I guess.\n    if pkg_info is None:\n        return top_egg_info\n\n    # Walk the sdist to find the egg-info with matching PKG-INFO.\n    for directory in directory_iterator:\n        egg_pkg_info = _read_pkg_info(directory)\n        if egg_pkg_info == pkg_info:\n            return directory\n\n    # Nothing matches...? Use the first one we found, I guess.\n    return top_egg_info", "output": "Find this package's .egg-info directory.\n\n    Due to how sdists are designed, the .egg-info directory cannot be reliably\n    found without running setup.py to aggregate all configurations. This\n    function instead uses some heuristics to locate the egg-info directory\n    that most likely represents this package.\n\n    The best .egg-info directory's path is returned as a string. None is\n    returned if no matches can be found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ensure_directory(path):\n    \"\"\"\"\"\"\n    dirname = os.path.dirname(path)\n    if not os.path.isdir(dirname):\n        os.makedirs(dirname)", "output": "Ensure that the parent directory of `path` exists", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_published(name, config_path=_DEFAULT_CONFIG_PATH, endpoint='', prefix=None):\n    '''\n    \n    '''\n    _validate_config(config_path)\n\n    ret = dict()\n    sources = list()\n    cmd = ['publish', 'show', '-config={}'.format(config_path), name]\n\n    if prefix:\n        cmd.append('{}:{}'.format(endpoint, prefix))\n\n    cmd_ret = _cmd_run(cmd)\n\n    ret = _parse_show_output(cmd_ret=cmd_ret)\n\n    if ret:\n        log.debug('Found published repository: %s', name)\n    else:\n        log.debug('Unable to find published repository: %s', name)\n\n    return ret", "output": "Get the details of a published repository.\n\n    :param str name: The distribution name of the published repository.\n    :param str config_path: The path to the configuration file for the aptly instance.\n    :param str endpoint: The publishing endpoint.\n    :param str prefix: The prefix for publishing.\n\n    :return: A dictionary containing information about the published repository.\n    :rtype: dict\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' aptly.get_published name=\"test-dist\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _separate_objects_by_boxes(self, objects: Set[Object]) -> Dict[Box, List[Object]]:\n        \"\"\"\n        \n        \"\"\"\n        objects_per_box: Dict[Box, List[Object]] = defaultdict(list)\n        for box in self.boxes:\n            for object_ in objects:\n                if object_ in box.objects:\n                    objects_per_box[box].append(object_)\n        return objects_per_box", "output": "Given a set of objects, separate them by the boxes they belong to and return a dict.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_attributes(self, model):\n        \"\"\"\n        \n        \"\"\"\n        for child in model.children():\n            if 'nn.modules.container' in str(type(child)):\n                self.remove_attributes(child)\n            else:\n                try:\n                    del child.x\n                except AttributeError:\n                    pass\n                try:\n                    del child.y\n                except AttributeError:\n                    pass", "output": "Removes the x and y attributes which were added by the forward handles\n        Recursively searches for non-container layers", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert(self, values, nan_rep, encoding, errors):\n        \"\"\"  \"\"\"\n\n        # values is a recarray\n        if values.dtype.fields is not None:\n            values = values[self.cname]\n\n        values = _maybe_convert(values, self.kind, encoding, errors)\n\n        kwargs = dict()\n        if self.freq is not None:\n            kwargs['freq'] = _ensure_decoded(self.freq)\n        if self.index_name is not None:\n            kwargs['name'] = _ensure_decoded(self.index_name)\n        # making an Index instance could throw a number of different errors\n        try:\n            self.values = Index(values, **kwargs)\n        except Exception:  # noqa: E722\n\n            # if the output freq is different that what we recorded,\n            # it should be None (see also 'doc example part 2')\n            if 'freq' in kwargs:\n                kwargs['freq'] = None\n            self.values = Index(values, **kwargs)\n\n        self.values = _set_tz(self.values, self.tz)\n\n        return self", "output": "set the values from this selection: take = take ownership", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(Bucket, MFA=None, RequestPayer=None, Force=False,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        if Force:\n            empty(Bucket, MFA=MFA, RequestPayer=RequestPayer, region=region,\n                  key=key, keyid=keyid, profile=profile)\n        conn.delete_bucket(Bucket=Bucket)\n        return {'deleted': True}\n    except ClientError as e:\n        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Given a bucket name, delete it, optionally emptying it first.\n\n    Returns {deleted: true} if the bucket was deleted and returns\n    {deleted: false} if the bucket was not deleted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.delete mybucket", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _to_java(self):\n        \"\"\"\n        \n        \"\"\"\n        sc = SparkContext._active_spark_context\n        java_models = [model._to_java() for model in self.models]\n        java_models_array = JavaWrapper._new_java_array(\n            java_models, sc._gateway.jvm.org.apache.spark.ml.classification.ClassificationModel)\n        metadata = JavaParams._new_java_obj(\"org.apache.spark.sql.types.Metadata\")\n        _java_obj = JavaParams._new_java_obj(\"org.apache.spark.ml.classification.OneVsRestModel\",\n                                             self.uid, metadata.empty(), java_models_array)\n        _java_obj.set(\"classifier\", self.getClassifier()._to_java())\n        _java_obj.set(\"featuresCol\", self.getFeaturesCol())\n        _java_obj.set(\"labelCol\", self.getLabelCol())\n        _java_obj.set(\"predictionCol\", self.getPredictionCol())\n        return _java_obj", "output": "Transfer this instance to a Java OneVsRestModel. Used for ML persistence.\n\n        :return: Java object equivalent to this instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def completed_key(self, id_or_name):\n        \"\"\"\n        \"\"\"\n        if not self.is_partial:\n            raise ValueError(\"Only a partial key can be completed.\")\n\n        if isinstance(id_or_name, six.string_types):\n            id_or_name_key = \"name\"\n        elif isinstance(id_or_name, six.integer_types):\n            id_or_name_key = \"id\"\n        else:\n            raise ValueError(id_or_name, \"ID/name was not a string or integer.\")\n\n        new_key = self._clone()\n        new_key._path[-1][id_or_name_key] = id_or_name\n        new_key._flat_path += (id_or_name,)\n        return new_key", "output": "Creates new key from existing partial key by adding final ID/name.\n\n        :type id_or_name: str or integer\n        :param id_or_name: ID or name to be added to the key.\n\n        :rtype: :class:`google.cloud.datastore.key.Key`\n        :returns: A new ``Key`` instance with the same data as the current one\n                  and an extra ID or name added.\n        :raises: :class:`ValueError` if the current key is not partial or if\n                 ``id_or_name`` is not a string or integer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def junos_rpc(cmd=None, dest=None, format=None, **kwargs):\n    '''\n    \n    '''\n    prep = _junos_prep_fun(napalm_device)  # pylint: disable=undefined-variable\n    if not prep['result']:\n        return prep\n    if not format:\n        format = 'xml'\n    rpc_ret = __salt__['junos.rpc'](cmd=cmd,\n                                    dest=dest,\n                                    format=format,\n                                    **kwargs)\n    rpc_ret['comment'] = rpc_ret.pop('message', '')\n    rpc_ret['result'] = rpc_ret.pop('out', False)\n    rpc_ret['out'] = rpc_ret.pop('rpc_reply', None)\n    # The comment field is \"message\" in the Junos module\n    return rpc_ret", "output": ".. versionadded:: 2019.2.0\n\n    Execute an RPC request on the remote Junos device.\n\n    cmd\n        The RPC request to the executed. To determine the RPC request, you can\n        check the from the command line of the device, by executing the usual\n        command followed by ``| display xml rpc``, e.g.,\n        ``show lldp neighbors | display xml rpc``.\n\n    dest\n        Destination file where the RPC output is stored. Note that the file will\n        be stored on the Proxy Minion. To push the files to the Master, use\n        :mod:`cp.push <salt.modules.cp.push>` Execution function.\n\n    format: ``xml``\n        The format in which the RPC reply is received from the device.\n\n    dev_timeout: ``30``\n        The NETCONF RPC timeout.\n\n    filter\n        Used with the ``get-config`` RPC request to filter out the config tree.\n\n    terse: ``False``\n        Whether to return terse output.\n\n        .. note::\n\n            Some RPC requests may not support this argument.\n\n    interface_name\n        Name of the interface to query.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm.junos_rpc get-lldp-neighbors-information\n        salt '*' napalm.junos_rcp get-config <configuration><system><ntp/></system></configuration>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_examples(input_file):\n    \"\"\"\"\"\"\n    examples = []\n    unique_id = 0\n    with open(input_file, \"r\", encoding='utf-8') as reader:\n        while True:\n            line = reader.readline()\n            if not line:\n                break\n            line = line.strip()\n            text_a = None\n            text_b = None\n            m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n            if m is None:\n                text_a = line\n            else:\n                text_a = m.group(1)\n                text_b = m.group(2)\n            examples.append(\n                InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b))\n            unique_id += 1\n    return examples", "output": "Read a list of `InputExample`s from an input file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, resource):\n        \"\"\"\n        \"\"\"\n        project = resource[\"projectId\"]\n        dataset_id = resource[\"datasetId\"]\n        return cls(project, dataset_id)", "output": "Factory: construct a dataset reference given its API representation\n\n        Args:\n            resource (Dict[str, str]):\n                Dataset reference resource representation returned from the API\n\n        Returns:\n            google.cloud.bigquery.dataset.DatasetReference:\n                Dataset reference parsed from ``resource``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_current_future_chain(self, continuous_future, dt):\n        \"\"\"\n        \n        \"\"\"\n        rf = self._roll_finders[continuous_future.roll_style]\n        session = self.trading_calendar.minute_to_session_label(dt)\n        contract_center = rf.get_contract_center(\n            continuous_future.root_symbol, session,\n            continuous_future.offset)\n        oc = self.asset_finder.get_ordered_contracts(\n            continuous_future.root_symbol)\n        chain = oc.active_chain(contract_center, session.value)\n        return self.asset_finder.retrieve_all(chain)", "output": "Retrieves the future chain for the contract at the given `dt` according\n        the `continuous_future` specification.\n\n        Returns\n        -------\n\n        future_chain : list[Future]\n            A list of active futures, where the first index is the current\n            contract specified by the continuous future definition, the second\n            is the next upcoming contract and so on.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def length_longest_path(input):\n    \"\"\"\n    \n    \"\"\"\n    curr_len, max_len = 0, 0    # running length and max length\n    stack = []    # keep track of the name length\n    for s in input.split('\\n'):\n        print(\"---------\")\n        print(\"<path>:\", s)\n        depth = s.count('\\t')    # the depth of current dir or file\n        print(\"depth: \", depth)\n        print(\"stack: \", stack)\n        print(\"curlen: \", curr_len)\n        while len(stack) > depth:    # go back to the correct depth\n            curr_len -= stack.pop()\n        stack.append(len(s.strip('\\t'))+1)   # 1 is the length of '/'\n        curr_len += stack[-1]    # increase current length\n        print(\"stack: \", stack)\n        print(\"curlen: \", curr_len)\n        if '.' in s:    # update maxlen only when it is a file\n            max_len = max(max_len, curr_len-1)    # -1 is to minus one '/'\n    return max_len", "output": ":type input: str\n    :rtype: int", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def predict(self, x):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(x, RDD):\n            return self.call(\"predict\", x.map(_convert_to_vector))\n\n        else:\n            return self.call(\"predict\", _convert_to_vector(x))", "output": "Predict values for a single data point or an RDD of points using\n        the model trained.\n\n        .. note:: In Python, predict cannot currently be used within an RDD\n            transformation or action.\n            Call predict directly on the RDD instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _trim_zeros_float(str_floats, na_rep='NaN'):\n    \"\"\"\n    \n    \"\"\"\n    trimmed = str_floats\n\n    def _is_number(x):\n        return (x != na_rep and not x.endswith('inf'))\n\n    def _cond(values):\n        finite = [x for x in values if _is_number(x)]\n        return (len(finite) > 0 and all(x.endswith('0') for x in finite) and\n                not (any(('e' in x) or ('E' in x) for x in finite)))\n\n    while _cond(trimmed):\n        trimmed = [x[:-1] if _is_number(x) else x for x in trimmed]\n\n    # leave one 0 after the decimal points if need be.\n    return [x + \"0\" if x.endswith('.') and _is_number(x) else x\n            for x in trimmed]", "output": "Trims zeros, leaving just one before the decimal points if need be.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepend_name_scope(name, import_scope):\n  \"\"\"\"\"\"\n  # Based on tensorflow/python/framework/ops.py implementation.\n  if import_scope:\n    try:\n      str_to_replace = r\"([\\^]|loc:@|^)(.*)\"\n      return re.sub(str_to_replace, r\"\\1\" + import_scope + r\"/\\2\",\n                    tf.compat.as_str_any(name))\n    except TypeError as e:\n      # If the name is not of a type we can process, simply return it.\n      logging.warning(e)\n      return name\n  else:\n    return name", "output": "Prepends name scope to a name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_input_grads(self, merge_multi_context=True):\n        \"\"\"\n        \"\"\"\n        assert self.binded and self.params_initialized and self.inputs_need_grad\n        return self._curr_module.get_input_grads(merge_multi_context=merge_multi_context)", "output": "Gets the gradients with respect to the inputs of the module.\n\n        Parameters\n        ----------\n        merge_multi_context : bool\n            Defaults to ``True``. In the case when data-parallelism is used, the outputs\n            will be collected from multiple devices. A ``True`` value indicate that we\n            should merge the collected results so that they look like from a single\n            executor.\n\n        Returns\n        -------\n        list of NDArrays or list of list of NDArrays\n            If `merge_multi_context` is ``True``, it is like ``[grad1, grad2]``. Otherwise, it\n            is like ``[[grad1_dev1, grad1_dev2], [grad2_dev1, grad2_dev2]]``. All the output\n            elements are `NDArray`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compile_template_str(template, renderers, default, blacklist, whitelist):\n    '''\n    \n    '''\n    fn_ = salt.utils.files.mkstemp()\n    with salt.utils.files.fopen(fn_, 'wb') as ofile:\n        ofile.write(SLS_ENCODER(template)[0])\n    return compile_template(fn_, renderers, default, blacklist, whitelist)", "output": "Take template as a string and return the high data structure\n    derived from the template.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _transform_fast(self, result, obj, func_nm):\n        \"\"\"\n        \n        \"\"\"\n        # if there were groups with no observations (Categorical only?)\n        # try casting data to original dtype\n        cast = self._transform_should_cast(func_nm)\n\n        # for each col, reshape to to size of original frame\n        # by take operation\n        ids, _, ngroup = self.grouper.group_info\n        output = []\n        for i, _ in enumerate(result.columns):\n            res = algorithms.take_1d(result.iloc[:, i].values, ids)\n            if cast:\n                res = self._try_cast(res, obj.iloc[:, i])\n            output.append(res)\n\n        return DataFrame._from_arrays(output, columns=result.columns,\n                                      index=obj.index)", "output": "Fast transform path for aggregations", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _format_attrs(self):\n        \"\"\"\n        \n        \"\"\"\n        attrs = self._get_data_as_items()\n        if self.name is not None:\n            attrs.append(('name', ibase.default_pprint(self.name)))\n        return attrs", "output": "Return a list of tuples of the (attr, formatted_value)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_user_autocompletions(ctx, args, incomplete, cmd_param):\n    \"\"\"\n    \n    \"\"\"\n    results = []\n    if isinstance(cmd_param.type, Choice):\n        # Choices don't support descriptions.\n        results = [(c, None)\n                   for c in cmd_param.type.choices if str(c).startswith(incomplete)]\n    elif cmd_param.autocompletion is not None:\n        dynamic_completions = cmd_param.autocompletion(ctx=ctx,\n                                                       args=args,\n                                                       incomplete=incomplete)\n        results = [c if isinstance(c, tuple) else (c, None)\n                   for c in dynamic_completions]\n    return results", "output": ":param ctx: context associated with the parsed command\n    :param args: full list of args\n    :param incomplete: the incomplete text to autocomplete\n    :param cmd_param: command definition\n    :return: all the possible user-specified completions for the param", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_attrgetter(environment, attribute, postprocess=None):\n    \"\"\"\n    \"\"\"\n    if attribute is None:\n        attribute = []\n    elif isinstance(attribute, string_types):\n        attribute = [int(x) if x.isdigit() else x for x in attribute.split('.')]\n    else:\n        attribute = [attribute]\n\n    def attrgetter(item):\n        for part in attribute:\n            item = environment.getitem(item, part)\n\n        if postprocess is not None:\n            item = postprocess(item)\n\n        return item\n\n    return attrgetter", "output": "Returns a callable that looks up the given attribute from a\n    passed object with the rules of the environment.  Dots are allowed\n    to access attributes of attributes.  Integer parts in paths are\n    looked up as integers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, resource):\n        \"\"\"\n        \"\"\"\n        from google.cloud.bigquery import dataset\n\n        if (\n            \"tableReference\" not in resource\n            or \"tableId\" not in resource[\"tableReference\"]\n        ):\n            raise KeyError(\n                \"Resource lacks required identity information:\"\n                '[\"tableReference\"][\"tableId\"]'\n            )\n        project_id = resource[\"tableReference\"][\"projectId\"]\n        table_id = resource[\"tableReference\"][\"tableId\"]\n        dataset_id = resource[\"tableReference\"][\"datasetId\"]\n        dataset_ref = dataset.DatasetReference(project_id, dataset_id)\n\n        table = cls(dataset_ref.table(table_id))\n        table._properties = resource\n\n        return table", "output": "Factory: construct a table given its API representation\n\n        Args:\n            resource (Dict[str, object]):\n                Table resource representation from the API\n\n        Returns:\n            google.cloud.bigquery.table.Table: Table parsed from ``resource``.\n\n        Raises:\n            KeyError:\n                If the ``resource`` lacks the key ``'tableReference'``, or if\n                the ``dict`` stored within the key ``'tableReference'`` lacks\n                the keys ``'tableId'``, ``'projectId'``, or ``'datasetId'``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _reset_docs(self):\n        \"\"\"\n        \n        \"\"\"\n        _LOGGER.debug(\"resetting documents\")\n        self.change_map.clear()\n        self.resume_token = None\n\n        # Mark each document as deleted. If documents are not deleted\n        # they will be sent again by the server.\n        for snapshot in self.doc_tree.keys():\n            name = snapshot.reference._document_path\n            self.change_map[name] = ChangeType.REMOVED\n\n        self.current = False", "output": "Helper to clear the docs on RESET or filter mismatch.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init(creds, bucket, multiple_env, environment, prefix, s3_cache_expire):\n    '''\n    \n    '''\n\n    cache_file = _get_buckets_cache_filename(bucket, prefix)\n    exp = time.time() - s3_cache_expire\n\n    # check if cache_file exists and its mtime\n    if os.path.isfile(cache_file):\n        cache_file_mtime = os.path.getmtime(cache_file)\n    else:\n        # file does not exists then set mtime to 0 (aka epoch)\n        cache_file_mtime = 0\n\n    expired = (cache_file_mtime <= exp)\n\n    log.debug(\n        'S3 bucket cache file %s is %sexpired, mtime_diff=%ss, expiration=%ss',\n        cache_file,\n        '' if expired else 'not ',\n        cache_file_mtime - exp,\n        s3_cache_expire\n    )\n\n    if expired:\n        pillars = _refresh_buckets_cache_file(creds, cache_file, multiple_env,\n                                           environment, prefix)\n    else:\n        pillars = _read_buckets_cache_file(cache_file)\n\n    log.debug('S3 bucket retrieved pillars %s', pillars)\n    return pillars", "output": "Connect to S3 and download the metadata for each file in all buckets\n    specified and cache the data to disk.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_and_copy_one_submission(self, submission_path):\n    \"\"\"\n    \"\"\"\n    if os.path.exists(self.download_dir):\n      shutil.rmtree(self.download_dir)\n    os.makedirs(self.download_dir)\n    if os.path.exists(self.validate_dir):\n      shutil.rmtree(self.validate_dir)\n    os.makedirs(self.validate_dir)\n    logging.info('\\n' + ('#' * 80) + '\\n# Processing submission: %s\\n'\n                 + '#' * 80, submission_path)\n    local_path = self.copy_submission_locally(submission_path)\n    metadata = self.base_validator.validate_submission(local_path)\n    if not metadata:\n      logging.error('Submission \"%s\" is INVALID', submission_path)\n      self.stats.add_failure()\n      return\n    submission_type = metadata['type']\n    container_name = metadata['container_gpu']\n    logging.info('Submission \"%s\" is VALID', submission_path)\n    self.list_of_containers.add(container_name)\n    self.stats.add_success(submission_type)\n    if self.do_copy:\n      submission_id = '{0:04}'.format(self.cur_submission_idx)\n      self.cur_submission_idx += 1\n      self.copy_submission_to_destination(submission_path,\n                                          TYPE_TO_DIR[submission_type],\n                                          submission_id)\n      self.id_to_path_mapping[submission_id] = submission_path", "output": "Validates one submission and copies it to target directory.\n\n    Args:\n      submission_path: path in Google Cloud Storage of the submission file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close(self):\n        \"\"\"  \"\"\"\n        if hasattr(self, \"_browser\"):\n            self.loop.run_until_complete(self._browser.close())\n        super().close()", "output": "If a browser was created close it first.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_metric(self, metric, labels, pre_sliced=False):\n        \"\"\"\"\"\"\n        for current_exec, (texec, islice) in enumerate(zip(self.train_execs, self.slices)):\n            if not pre_sliced:\n                labels_slice = [label[islice] for label in labels]\n            else:\n                labels_slice = labels[current_exec]\n            metric.update(labels_slice, texec.outputs)", "output": "Update evaluation metric with label and current outputs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_by_id(self, schema_id):\n        \"\"\"\n        \n        \"\"\"\n        if schema_id in self.id_to_schema:\n            return self.id_to_schema[schema_id]\n        # fetch from the registry\n        url = '/'.join([self.url, 'schemas', 'ids', str(schema_id)])\n\n        result, code = self._send_request(url)\n        if code == 404:\n            log.error(\"Schema not found:\" + str(code))\n            return None\n        elif not (code >= 200 and code <= 299):\n            log.error(\"Unable to get schema for the specific ID:\" + str(code))\n            return None\n        else:\n            # need to parse the schema\n            schema_str = result.get(\"schema\")\n            try:\n                result = loads(schema_str)\n                # cache it\n                self._cache_schema(result, schema_id)\n                return result\n            except ClientError as e:\n                # bad schema - should not happen\n                raise ClientError(\"Received bad schema (id %s) from registry: %s\" % (schema_id, e))", "output": "GET /schemas/ids/{int: id}\n        Retrieve a parsed avro schema by id or None if not found\n        :param int schema_id: int value\n        :returns: Avro schema\n        :rtype: schema", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dmlc_opts(opts):\n    \"\"\"\n    \"\"\"\n    args = ['--num-workers', str(opts.num_workers),\n            '--num-servers', str(opts.num_servers),\n            '--cluster', opts.launcher,\n            '--host-file', opts.hostfile,\n            '--sync-dst-dir', opts.sync_dst_dir]\n\n    # convert to dictionary\n    dopts = vars(opts)\n    for key in ['env_server', 'env_worker', 'env']:\n        for v in dopts[key]:\n            args.append('--' + key.replace(\"_\",\"-\"))\n            args.append(v)\n    args += opts.command\n    try:\n        from dmlc_tracker import opts\n    except ImportError:\n        print(\"Can't load dmlc_tracker package.  Perhaps you need to run\")\n        print(\"    git submodule update --init --recursive\")\n        raise\n    dmlc_opts = opts.get_opts(args)\n    return dmlc_opts", "output": "convert from mxnet's opts to dmlc's opts", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def symbol_top(body_output, targets, model_hparams, vocab_size):\n  \"\"\"\n  \"\"\"\n  del targets  # unused arg\n  if model_hparams.shared_embedding_and_softmax_weights:\n    scope_name = \"shared\"\n    reuse = tf.AUTO_REUSE\n  else:\n    scope_name = \"softmax\"\n    reuse = False\n  with tf.variable_scope(scope_name, reuse=reuse):\n    body_output_shape = common_layers.shape_list(body_output)\n    var = get_weights(model_hparams, vocab_size, body_output_shape[-1])\n    if (model_hparams.factored_logits and\n        model_hparams.mode == tf.estimator.ModeKeys.TRAIN):\n      # insert channels dimension\n      body_output = tf.expand_dims(body_output, 3)\n      return common_layers.FactoredTensor(body_output, var)\n    else:\n      body_output = tf.reshape(body_output, [-1, body_output_shape[-1]])\n      logits = tf.matmul(body_output, var, transpose_b=True)\n      return tf.reshape(logits,\n                        body_output_shape[:-1] + [1, vocab_size])", "output": "Generate logits.\n\n  Args:\n    body_output: A Tensor with shape\n      [batch, p0, p1, model_hparams.hidden_size].\n    targets: Unused.\n    model_hparams: HParams, model hyperparmeters.\n    vocab_size: int, vocabulary size.\n\n  Returns:\n    logits: A Tensor with shape  [batch, p0, p1, ?, vocab_size].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tempdeny(ip=None, ttl=None, port=None, direction=None, comment=''):\n    '''\n    \n    '''\n    return _tmp_access_rule('tempdeny', ip, ttl, port, direction, comment)", "output": "Add a rule to the temporary ip deny list.\n    See :func:`_access_rule`.\n    1- Add an IP:\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' csf.tempdeny 127.0.0.1 300 port=22 direction='in' comment='# Brute force attempt'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dice(input:Tensor, targs:Tensor, iou:bool=False)->Rank0Tensor:\n    \"\"\n    n = targs.shape[0]\n    input = input.argmax(dim=1).view(n,-1)\n    targs = targs.view(n,-1)\n    intersect = (input * targs).sum().float()\n    union = (input+targs).sum().float()\n    if not iou: return (2. * intersect / union if union > 0 else union.new([1.]).squeeze())\n    else: return intersect / (union-intersect+1.0)", "output": "Dice coefficient metric for binary target. If iou=True, returns iou metric, classic for segmentation problems.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def join_parsed_parts(self, drv, root, parts, drv2, root2, parts2):\n        \"\"\"\n        \n        \"\"\"\n        if root2:\n            if not drv2 and drv:\n                return drv, root2, [drv + root2] + parts2[1:]\n        elif drv2:\n            if drv2 == drv or self.casefold(drv2) == self.casefold(drv):\n                # Same drive => second path is relative to the first\n                return drv, root, parts + parts2[1:]\n        else:\n            # Second path is non-anchored (common case)\n            return drv, root, parts + parts2\n        return drv2, root2, parts2", "output": "Join the two paths represented by the respective\n        (drive, root, parts) tuples.  Return a new (drive, root, parts) tuple.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_max_norm(self, norms:[])->None:\n        \"\"\n        max_norm = max(norms)\n        self._add_gradient_scalar('max_norm', scalar_value=max_norm)", "output": "Writes the maximum norm of the gradients to Tensorboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_instance(project_id, instance_zone, name, service):\n    '''\n    \n    '''\n    return service.instances().get(project=project_id,\n                                   zone=instance_zone,\n                                   instance=name).execute()", "output": "Get instance details", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rename(self, fnames=None):\r\n        \"\"\"\"\"\"\r\n        if fnames is None:\r\n            fnames = self.get_selected_filenames()\r\n        if not isinstance(fnames, (tuple, list)):\r\n            fnames = [fnames]\r\n        for fname in fnames:\r\n            self.rename_file(fname)", "output": "Rename files", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def systemd_running_state(name, path=None):\n    '''\n    \n\n    '''\n    try:\n        ret = run_all(name,\n                      'systemctl is-system-running',\n                      path=path,\n                      ignore_retcode=True)['stdout']\n    except CommandExecutionError:\n        ret = ''\n    return ret", "output": "Get the operational state of a systemd based container\n\n    path\n        path to the container parent\n        default: /var/lib/lxc (system default)\n\n        .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion lxc.systemd_running_state ubuntu", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_results(fields, data):\n    \"\"\"\n    \n    \"\"\"\n    master = []\n\n    for record in data['records']:  # for each 'record' in response\n        row = [None] * len(fields)  # create null list the length of number of columns\n        for obj, value in record.iteritems():  # for each obj in record\n            if not isinstance(value, (dict, list, tuple)):  # if not data structure\n                if obj in fields:\n                    row[fields.index(obj)] = ensure_utf(value)\n\n            elif isinstance(value, dict) and obj != 'attributes':  # traverse down into object\n                path = obj\n                _traverse_results(value, fields, row, path)\n\n        master.append(row)\n    return master", "output": "Traverses ordered dictionary, calls _traverse_results() to recursively read into the dictionary depth of data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_snapshot(self, callback):\n        \"\"\"\n        \"\"\"\n        return Watch.for_query(\n            self, callback, document.DocumentSnapshot, document.DocumentReference\n        )", "output": "Monitor the documents in this collection that match this query.\n\n        This starts a watch on this query using a background thread. The\n        provided callback is run on the snapshot of the documents.\n\n        Args:\n            callback(~.firestore.query.QuerySnapshot): a callback to run when\n                a change occurs.\n\n        Example:\n            from google.cloud import firestore_v1beta1\n\n            db = firestore_v1beta1.Client()\n            query_ref = db.collection(u'users').where(\"user\", \"==\", u'Ada')\n\n            def on_snapshot(docs, changes, read_time):\n                for doc in docs:\n                    print(u'{} => {}'.format(doc.id, doc.to_dict()))\n\n            # Watch this query\n            query_watch = query_ref.on_snapshot(on_snapshot)\n\n            # Terminate this watch\n            query_watch.unsubscribe()", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def inspect_image(name):\n    '''\n    \n    '''\n    ret = _client_wrapper('inspect_image', name)\n    for param in ('Size', 'VirtualSize'):\n        if param in ret:\n            ret['{0}_Human'.format(param)] = _size_fmt(ret[param])\n    return ret", "output": "Retrieves image information. Equivalent to running the ``docker inspect``\n    Docker CLI command, but will only look for image information.\n\n    .. note::\n        To inspect an image, it must have been pulled from a registry or built\n        locally. Images on a Docker registry which have not been pulled cannot\n        be inspected.\n\n    name\n        Image name or ID\n\n\n    **RETURN DATA**\n\n    A dictionary of image information\n\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt myminion docker.inspect_image busybox\n        salt myminion docker.inspect_image centos:6\n        salt myminion docker.inspect_image 0123456789ab", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_valid(self, search_space):\n        \"\"\"\n        \n        \"\"\"\n        if not len(search_space) == 1:\n            raise RuntimeError('BatchTuner only supprt one combined-paramreters key.')\n        \n        for param in search_space:\n            param_type = search_space[param][TYPE]\n            if not param_type == CHOICE:\n                raise RuntimeError('BatchTuner only supprt one combined-paramreters type is choice.')\n            else:\n                if isinstance(search_space[param][VALUE], list):\n                    return search_space[param][VALUE]\n                raise RuntimeError('The combined-paramreters value in BatchTuner is not a list.')\n        return None", "output": "Check the search space is valid: only contains 'choice' type\n        \n        Parameters\n        ----------\n        search_space : dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_subnet(self, subnet, name=None):\n        '''\n        \n        '''\n        subnet_id = self._find_subnet_id(subnet)\n        return self.network_conn.update_subnet(\n            subnet=subnet_id, body={'subnet': {'name': name}})", "output": "Updates a subnet", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _filter_dict(input_dict, search_key, search_value):\n\n    '''\n    \n    '''\n\n    output_dict = dict()\n\n    for key, key_list in six.iteritems(input_dict):\n        key_list_filtered = _filter_list(key_list, search_key, search_value)\n        if key_list_filtered:\n            output_dict[key] = key_list_filtered\n\n    return output_dict", "output": "Filters a dictionary of dictionaries by a key-value pair.\n\n    :param input_dict:    is a dictionary whose values are lists of dictionaries\n    :param search_key:    is the key in the leaf dictionaries\n    :param search_values: is the value in the leaf dictionaries\n    :return:              filtered dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _prompt_choice(var_name, options):\n    '''\n    \n    '''\n    choice_map = OrderedDict(\n        ('{0}'.format(i), value) for i, value in enumerate(options, 1) if value[0] != 'test'\n    )\n    choices = choice_map.keys()\n    default = '1'\n\n    choice_lines = ['{0} - {1} - {2}'.format(c[0], c[1][0], c[1][1]) for c in choice_map.items()]\n    prompt = '\\n'.join((\n        'Select {0}:'.format(var_name),\n        '\\n'.join(choice_lines),\n        'Choose from {0}'.format(', '.join(choices))\n    ))\n\n    user_choice = click.prompt(\n        prompt, type=click.Choice(choices), default=default\n    )\n    return choice_map[user_choice]", "output": "Prompt the user to choose between a list of options, index each one by adding an enumerator\n    based on https://github.com/audreyr/cookiecutter/blob/master/cookiecutter/prompt.py#L51\n\n    :param var_name: The question to ask the user\n    :type  var_name: ``str``\n\n    :param options: A list of options\n    :type  options: ``list`` of ``tupple``\n\n    :rtype: ``tuple``\n    :returns: The selected user", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unsubscribe(SubscriptionArn, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    if not SubscriptionArn.startswith('arn:aws:sns:'):\n        # Grrr, AWS sent us an ARN that's NOT and ARN....\n        # This can happen if, for instance, a subscription is left in PendingAcceptance or similar\n        # Note that anything left in PendingConfirmation will be auto-deleted by AWS after 30 days\n        # anyway, so this isn't as ugly a hack as it might seem at first...\n        log.info('Invalid subscription ARN `%s` passed - likely a PendingConfirmaton or such.  '\n                 'Skipping unsubscribe attempt as it would almost certainly fail...',\n                 SubscriptionArn)\n        return True\n    subs = list_subscriptions(region=region, key=key, keyid=keyid, profile=profile)\n    sub = [s for s in subs if s.get('SubscriptionArn') == SubscriptionArn]\n    if not sub:\n        log.error('Subscription ARN %s not found', SubscriptionArn)\n        return False\n    TopicArn = sub[0]['TopicArn']\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        conn.unsubscribe(SubscriptionArn=SubscriptionArn)\n        log.info('Deleted subscription %s from SNS topic %s',\n                 SubscriptionArn, TopicArn)\n        return True\n    except botocore.exceptions.ClientError as e:\n        log.error('Failed to delete subscription %s: %s', SubscriptionArn, e)\n        return False", "output": "Unsubscribe a specific SubscriptionArn of a topic.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto3_sns.unsubscribe my_subscription_arn region=us-east-1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_api_key(self, api_id, stage_name):\n        \"\"\"\n        \n        \"\"\"\n        response = self.apigateway_client.create_api_key(\n            name='{}_{}'.format(stage_name, api_id),\n            description='Api Key for {}'.format(api_id),\n            enabled=True,\n            stageKeys=[\n                {\n                    'restApiId': '{}'.format(api_id),\n                    'stageName': '{}'.format(stage_name)\n                },\n            ]\n        )\n        print('Created a new x-api-key: {}'.format(response['id']))", "output": "Create new API key and link it with an api_id and a stage_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _on_error(self, exc):\n        \"\"\"\"\"\"\n        # restart the read scan from AFTER the last successfully read row\n        retry_request = self.request\n        if self.last_scanned_row_key:\n            retry_request = self._create_retry_request()\n\n        self.response_iterator = self.read_method(retry_request)", "output": "Helper for :meth:`__iter__`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unlock(self):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        pid = readlink(self.name)\r\n        if int(pid) != os.getpid():\r\n            raise ValueError(\"Lock %r not owned by this process\" % (self.name,))\r\n        rmlink(self.name)\r\n        self.locked = False", "output": "Release this lock.\r\n\r\n        This deletes the directory with the given name.\r\n\r\n        @raise: Any exception os.readlink() may raise, or\r\n        ValueError if the lock is not owned by this process.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpointerNewRangeNodes(self, end):\n        \"\"\" \"\"\"\n        if end is None: end__o = None\n        else: end__o = end._o\n        ret = libxml2mod.xmlXPtrNewRangeNodes(self._o, end__o)\n        if ret is None:raise treeError('xmlXPtrNewRangeNodes() failed')\n        return xpathObjectRet(ret)", "output": "Create a new xmlXPathObjectPtr of type range using 2 nodes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def datasets_status(self, owner_slug, dataset_slug, **kwargs):  # noqa: E501\n        \"\"\"\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.datasets_status_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501\n        else:\n            (data) = self.datasets_status_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501\n            return data", "output": "Get dataset creation status  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.datasets_status(owner_slug, dataset_slug, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str owner_slug: Dataset owner (required)\n        :param str dataset_slug: Dataset name (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _jit_update_fun(predict_fun, loss_fun, optimizer, lr_fun, num_devices):\n  \"\"\"\"\"\"\n  if num_devices == 1:  # TODO(lukaszkaiser): remove branch when not needed.\n    def single_update(i, opt_state, batch, rng):\n      rng, subrng = jax_random.split(rng[0])\n      _, opt_update = optimizer(lr_fun)\n      params = trax_opt.get_params(opt_state)\n      return opt_update(i, backend.grad(loss_fun)(\n          params, batch, predict_fun, rng), opt_state), [subrng]\n    return backend.jit(single_update)\n\n  @functools.partial(backend.pmap, axis_name=\"batch\")\n  def mapped_update(i, opt_state, batch, rng):\n    \"\"\"This is a multi-device version of the update function above.\"\"\"\n    # We assume all tensors have the first dimension = num_devices.\n    rng, subrng = jax_random.split(rng)\n    _, opt_update = optimizer(lr_fun)\n    params = trax_opt.get_params(opt_state)\n    grads = backend.grad(loss_fun)(params, batch, predict_fun, rng)\n    grads = jax.tree_util.tree_map(\n        lambda g: lax.psum(g, \"batch\"), grads)\n    return opt_update(i, grads, opt_state), subrng\n\n  def update(i, opt_state, batch, rng):\n    return mapped_update(jax.replicate(i), opt_state, batch, rng)\n\n  return update", "output": "Get jit-ed update function for loss, optimizer, learning rate function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_specific_user():\n    '''\n    \n    '''\n    user = get_user()\n    if salt.utils.platform.is_windows():\n        if _win_current_user_is_admin():\n            return 'sudo_{0}'.format(user)\n    else:\n        env_vars = ('SUDO_USER',)\n        if user == 'root':\n            for evar in env_vars:\n                if evar in os.environ:\n                    return 'sudo_{0}'.format(os.environ[evar])\n    return user", "output": "Get a user name for publishing. If you find the user is \"root\" attempt to be\n    more specific", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def argmax(self, rows: List[Row], column: ComparableColumn) -> List[Row]:\n        \"\"\"\n        \n        \"\"\"\n        if not rows:\n            return []\n        value_row_pairs = [(row.values[column.name], row) for row in rows]\n        if not value_row_pairs:\n            return []\n        # Returns a list containing the row with the max cell value.\n        return [sorted(value_row_pairs, key=lambda x: x[0], reverse=True)[0][1]]", "output": "Takes a list of rows and a column name and returns a list containing a single row (dict from\n        columns to cells) that has the maximum numerical value in the given column. We return a list\n        instead of a single dict to be consistent with the return type of ``select`` and\n        ``all_rows``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nanargmax(values, axis=None, skipna=True, mask=None):\n    \"\"\"\n    \n    \"\"\"\n    values, mask, dtype, _, _ = _get_values(\n        values, skipna, fill_value_typ='-inf', mask=mask)\n    result = values.argmax(axis)\n    result = _maybe_arg_null_out(result, axis, mask, skipna)\n    return result", "output": "Parameters\n    ----------\n    values : ndarray\n    axis: int, optional\n    skipna : bool, default True\n    mask : ndarray[bool], optional\n        nan-mask if known\n\n    Returns\n    --------\n    result : int\n        The index of max value in specified axis or -1 in the NA case\n\n    Examples\n    --------\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1, 2, 3, np.nan, 4])\n    >>> nanops.nanargmax(s)\n    4", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_certificate_signing_request_approval(self, name, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_certificate_signing_request_approval_with_http_info(name, body, **kwargs)\n        else:\n            (data) = self.replace_certificate_signing_request_approval_with_http_info(name, body, **kwargs)\n            return data", "output": "replace approval of the specified CertificateSigningRequest\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_certificate_signing_request_approval(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the CertificateSigningRequest (required)\n        :param V1beta1CertificateSigningRequest body: (required)\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1beta1CertificateSigningRequest\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show(uuid):\n    '''\n    \n    '''\n    ret = {}\n    fmdump = _check_fmdump()\n    cmd = '{cmd} -u {uuid} -V'.format(\n        cmd=fmdump,\n        uuid=uuid\n    )\n    res = __salt__['cmd.run_all'](cmd)\n    retcode = res['retcode']\n    result = {}\n    if retcode != 0:\n        result['Error'] = 'error executing fmdump'\n    else:\n        result = _parse_fmdump_verbose(res['stdout'])\n\n    return result", "output": "Display log details\n\n    uuid: string\n        uuid of fault\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' fmadm.show 11b4070f-4358-62fa-9e1e-998f485977e1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deployment_check_existence(name, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    result = False\n    resconn = __utils__['azurearm.get_client']('resource', **kwargs)\n    try:\n        result = resconn.deployments.check_existence(\n            deployment_name=name,\n            resource_group_name=resource_group\n        )\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('resource', str(exc), **kwargs)\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Check the existence of a deployment.\n\n    :param name: The name of the deployment to query.\n\n    :param resource_group: The resource group name assigned to the\n        deployment.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_resource.deployment_check_existence testdeploy testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def record_absent(name, zone, type, data, profile):\n    '''\n    \n    '''\n    zones = __salt__['libcloud_dns.list_zones'](profile)\n    try:\n        matching_zone = [z for z in zones if z['domain'] == zone][0]\n    except IndexError:\n        return state_result(False, 'Zone could not be found', name)\n    records = __salt__['libcloud_dns.list_records'](matching_zone['id'], profile)\n    matching_records = [record for record in records\n                        if record['name'] == name and\n                        record['type'] == type and\n                        record['data'] == data]\n    if matching_records:\n        result = []\n        for record in matching_records:\n            result.append(__salt__['libcloud_dns.delete_record'](\n                matching_zone['id'],\n                record['id'],\n                profile))\n        return state_result(all(result), 'Removed {0} records'.format(len(result)), name)\n    else:\n        return state_result(True, 'Records already absent', name)", "output": "Ensures a record is absent.\n\n    :param name: Record name without the domain name (e.g. www).\n                 Note: If you want to create a record for a base domain\n                 name, you should specify empty string ('') for this\n                 argument.\n    :type  name: ``str``\n\n    :param zone: Zone where the requested record is created, the domain name\n    :type  zone: ``str``\n\n    :param type: DNS record type (A, AAAA, ...).\n    :type  type: ``str``\n\n    :param data: Data for the record (depends on the record type).\n    :type  data: ``str``\n\n    :param profile: The profile key\n    :type  profile: ``str``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def by_category(self):\n        \"\"\"\n        \"\"\"\n        grouped = defaultdict(list)\n        for channel in self._channels.values():\n            if isinstance(channel, CategoryChannel):\n                continue\n\n            grouped[channel.category_id].append(channel)\n\n        def key(t):\n            k, v = t\n            return ((k.position, k.id) if k else (-1, -1), v)\n\n        _get = self._channels.get\n        as_list = [(_get(k), v) for k, v in grouped.items()]\n        as_list.sort(key=key)\n        for _, channels in as_list:\n            channels.sort(key=lambda c: (c._sorting_bucket, c.position, c.id))\n        return as_list", "output": "Returns every :class:`CategoryChannel` and their associated channels.\n\n        These channels and categories are sorted in the official Discord UI order.\n\n        If the channels do not have a category, then the first element of the tuple is\n        ``None``.\n\n        Returns\n        --------\n        List[Tuple[Optional[:class:`CategoryChannel`], List[:class:`abc.GuildChannel`]]]:\n            The categories and their associated channels.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def system_which(command, mult=False):\n    \"\"\"\"\"\"\n    _which = \"which -a\" if not os.name == \"nt\" else \"where\"\n    os.environ = {\n        vistir.compat.fs_str(k): vistir.compat.fs_str(val)\n        for k, val in os.environ.items()\n    }\n    result = None\n    try:\n        c = delegator.run(\"{0} {1}\".format(_which, command))\n        try:\n            # Which Not found\u2026\n            if c.return_code == 127:\n                click.echo(\n                    \"{}: the {} system utility is required for Pipenv to find Python installations properly.\"\n                    \"\\n  Please install it.\".format(\n                        crayons.red(\"Warning\", bold=True), crayons.red(_which)\n                    ),\n                    err=True,\n                )\n            assert c.return_code == 0\n        except AssertionError:\n            result = fallback_which(command, allow_global=True)\n    except TypeError:\n        if not result:\n            result = fallback_which(command, allow_global=True)\n    else:\n        if not result:\n            result = next(iter([c.out, c.err]), \"\").split(\"\\n\")\n            result = next(iter(result)) if not mult else result\n            return result\n        if not result:\n            result = fallback_which(command, allow_global=True)\n    result = [result] if mult else result\n    return result", "output": "Emulates the system's which. Returns None if not found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_node(conn, name):\n    '''\n    \n    '''\n    datacenter_id = get_datacenter_id()\n\n    for item in conn.list_servers(datacenter_id)['items']:\n        if item['properties']['name'] == name:\n            node = {'id': item['id']}\n            node.update(item['properties'])\n            return node", "output": "Return a node for the named VM", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_time_range(time_range, now):\n    '''\n    \n    '''\n    if _TIME_SUPPORTED:\n        _start = dateutil_parser.parse(time_range['start'])\n        _end = dateutil_parser.parse(time_range['end'])\n\n        return bool(_start <= now <= _end)\n    else:\n        log.error('Dateutil is required.')\n        return False", "output": "Check time range", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bokehjssrcdir(self):\n        ''' \n\n        '''\n        if self._is_dev or self.debugjs:\n            bokehjssrcdir = abspath(join(ROOT_DIR, '..', 'bokehjs', 'src'))\n\n            if isdir(bokehjssrcdir):\n                return bokehjssrcdir\n\n        return None", "output": "The absolute path of the BokehJS source code in the installed\n        Bokeh source tree.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_prediction_score(self, node_id):\n        \"\"\"\n        \n\n        \"\"\"\n        _raise_error_if_not_of_type(node_id, [int,long], \"node_id\")\n        _numeric_param_check_range(\"node_id\", node_id, 0, self.num_nodes - 1)\n        node = self.nodes[node_id]\n        return None if node.is_leaf is False else node.value", "output": "Return the prediction score (if leaf node) or None if its an\n        intermediate node.\n\n        Parameters\n        ----------\n        node_id: id of the node to get the prediction value.\n\n        Returns\n        -------\n        float or None: returns float value of prediction if leaf node and None\n        if not.\n\n        Examples\n        --------\n        .. sourcecode:: python\n\n            >>> tree.get_prediction_score(120)  # Leaf node\n            0.251092\n\n            >>> tree.get_prediction_score(120)  # Not a leaf node\n            None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def delete(self, *, reason=None):\n        \"\"\"\n        \"\"\"\n\n        await self._state.http.delete_custom_emoji(self.guild.id, self.id, reason=reason)", "output": "|coro|\n\n        Deletes the custom emoji.\n\n        You must have :attr:`~Permissions.manage_emojis` permission to\n        do this.\n\n        Parameters\n        -----------\n        reason: Optional[:class:`str`]\n            The reason for deleting this emoji. Shows up on the audit log.\n\n        Raises\n        -------\n        Forbidden\n            You are not allowed to delete emojis.\n        HTTPException\n            An error occurred deleting the emoji.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transform(self, data):\n        \"\"\"\n        \n        \"\"\"\n\n        if not self._get(\"fitted\"):\n            raise RuntimeError(\"`transform` called before `fit` or `fit_transform`.\")\n\n        data = data.copy()\n        output_column_prefix = self._get(\"output_column_prefix\")\n\n        if output_column_prefix is None:\n            prefix = \"\"\n        else:\n            prefix = output_column_prefix + '.'\n\n        transform_function = self._get(\"transform_function\")\n\n        feature_columns = self._get(\"features\")\n        feature_columns = _internal_utils.select_feature_subset(data, feature_columns)\n\n        for f in feature_columns:\n            data[prefix + f] = transform_function(data[f])\n\n        return data", "output": "Transforms the data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(self):\n        \"\"\"\n        \n        \"\"\"\n\n        routing_list = self._make_routing_list(self.api_provider)\n\n        if not routing_list:\n            raise NoApisDefined(\"No APIs available in SAM template\")\n\n        static_dir_path = self._make_static_dir_path(self.cwd, self.static_dir)\n\n        # We care about passing only stderr to the Service and not stdout because stdout from Docker container\n        # contains the response to the API which is sent out as HTTP response. Only stderr needs to be printed\n        # to the console or a log file. stderr from Docker container contains runtime logs and output of print\n        # statements from the Lambda function\n        service = LocalApigwService(routing_list=routing_list,\n                                    lambda_runner=self.lambda_runner,\n                                    static_dir=static_dir_path,\n                                    port=self.port,\n                                    host=self.host,\n                                    stderr=self.stderr_stream)\n\n        service.create()\n\n        # Print out the list of routes that will be mounted\n        self._print_routes(self.api_provider, self.host, self.port)\n        LOG.info(\"You can now browse to the above endpoints to invoke your functions. \"\n                 \"You do not need to restart/reload SAM CLI while working on your functions, \"\n                 \"changes will be reflected instantly/automatically. You only need to restart \"\n                 \"SAM CLI if you update your AWS SAM template\")\n\n        service.run()", "output": "Creates and starts the local API Gateway service. This method will block until the service is stopped\n        manually using an interrupt. After the service is started, callers can make HTTP requests to the endpoint\n        to invoke the Lambda function and receive a response.\n\n        NOTE: This is a blocking call that will not return until the thread is interrupted with SIGINT/SIGTERM", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def text2text_generate_encoded(sample_generator,\n                               vocab,\n                               targets_vocab=None,\n                               has_inputs=True,\n                               inputs_prefix=\"\",\n                               targets_prefix=\"\"):\n  \"\"\"\"\"\"\n  targets_vocab = targets_vocab or vocab\n  for sample in sample_generator:\n    if has_inputs:\n      sample[\"inputs\"] = vocab.encode(inputs_prefix + sample[\"inputs\"])\n      sample[\"inputs\"].append(text_encoder.EOS_ID)\n    sample[\"targets\"] = targets_vocab.encode(targets_prefix + sample[\"targets\"])\n    sample[\"targets\"].append(text_encoder.EOS_ID)\n    yield sample", "output": "Encode Text2Text samples from the generator with the vocab.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_buckets(min_length, max_length, bucket_count):\n    '''\n    \n    '''\n    if bucket_count <= 0:\n        return [max_length]\n    unit_length = int((max_length - min_length) // (bucket_count))\n    buckets = [min_length + unit_length *\n               (i + 1) for i in range(0, bucket_count)]\n    buckets[-1] = max_length\n    return buckets", "output": "Get bucket by length.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cumcount_array(self, ascending=True):\n        \"\"\"\n        \n        \"\"\"\n        ids, _, ngroups = self.grouper.group_info\n        sorter = get_group_index_sorter(ids, ngroups)\n        ids, count = ids[sorter], len(ids)\n\n        if count == 0:\n            return np.empty(0, dtype=np.int64)\n\n        run = np.r_[True, ids[:-1] != ids[1:]]\n        rep = np.diff(np.r_[np.nonzero(run)[0], count])\n        out = (~run).cumsum()\n\n        if ascending:\n            out -= np.repeat(out[run], rep)\n        else:\n            out = np.repeat(out[np.r_[run[1:], True]], rep) - out\n\n        rev = np.empty(count, dtype=np.intp)\n        rev[sorter] = np.arange(count, dtype=np.intp)\n        return out[rev].astype(np.int64, copy=False)", "output": "Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from length of group - 1 to 0.\n\n        Notes\n        -----\n        this is currently implementing sort=False\n        (though the default is sort=True) for groupby in general", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_locations(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_locations function must be called with '\n            '-f or --function, or with the --list-locations option'\n        )\n\n    items = query(method='regions')\n    ret = {}\n    for region in items['regions']:\n        ret[region['name']] = {}\n        for item in six.iterkeys(region):\n            ret[region['name']][item] = six.text_type(region[item])\n\n    return ret", "output": "Return a dict of all available VM locations on the cloud provider with\n    relevant data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_json(json_file):\n    \"\"\"\n    \"\"\"\n    if not os.path.exists(json_file):\n        return None\n\n    try:\n        with open(json_file, \"r\") as f:\n            info_str = f.readlines()\n            info_str = \"\".join(info_str)\n            json_info = json.loads(info_str)\n            return unicode2str(json_info)\n    except BaseException as e:\n        logging.error(e.message)\n        return None", "output": "Parse a whole json record from the given file.\n\n    Return None if the json file does not exists or exception occurs.\n\n    Args:\n        json_file (str): File path to be parsed.\n\n    Returns:\n        A dict of json info.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_json(raw):\n    '''\n    \n    '''\n    ret = {}\n    lines = __split(raw)\n    for ind, _ in enumerate(lines):\n        try:\n            working = '\\n'.join(lines[ind:])\n        except UnicodeDecodeError:\n            working = '\\n'.join(salt.utils.data.decode(lines[ind:]))\n\n        try:\n            ret = json.loads(working)  # future lint: blacklisted-function\n        except ValueError:\n            continue\n        if ret:\n            return ret\n    if not ret:\n        # Not json, raise an error\n        raise ValueError", "output": "Pass in a raw string and load the json when it starts. This allows for a\n    string to start with garbage and end with json but be cleanly loaded", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def import_from_string(self, text, title=None):\r\n        \"\"\"\"\"\"\r\n        data = self.model.get_data()\r\n        # Check if data is a dict\r\n        if not hasattr(data, \"keys\"):\r\n            return\r\n        editor = ImportWizard(self, text, title=title,\r\n                              contents_title=_(\"Clipboard contents\"),\r\n                              varname=fix_reference_name(\"data\",\r\n                                                         blacklist=list(data.keys())))\r\n        if editor.exec_():\r\n            var_name, clip_data = editor.get_data()\r\n            self.new_value(var_name, clip_data)", "output": "Import data from string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def glob(dirs, patterns, exclude_patterns=None):\n    \"\"\"\n    \"\"\"\n\n    assert(isinstance(patterns, list))\n    assert(isinstance(dirs, list))\n\n    if not exclude_patterns:\n        exclude_patterns = []\n    else:\n       assert(isinstance(exclude_patterns, list))\n\n    real_patterns = [os.path.join(d, p) for p in patterns for d in dirs]\n    real_exclude_patterns = [os.path.join(d, p) for p in exclude_patterns\n                             for d in dirs]\n\n    inc = [os.path.normpath(name) for p in real_patterns\n           for name in builtin_glob(p)]\n    exc = [os.path.normpath(name) for p in real_exclude_patterns\n           for name in builtin_glob(p)]\n    return [x for x in inc if x not in exc]", "output": "Returns the list of files matching the given pattern in the\n    specified directory.  Both directories and patterns are\n    supplied as portable paths. Each pattern should be non-absolute\n    path, and can't contain '.' or '..' elements. Each slash separated\n    element of pattern can contain the following special characters:\n    -  '?', which match any character\n    -  '*', which matches arbitrary number of characters.\n    A file $(d)/e1/e2/e3 (where 'd' is in $(dirs)) matches pattern p1/p2/p3\n    if and only if e1 matches p1, e2 matches p2 and so on.\n    For example:\n        [ glob . : *.cpp ]\n        [ glob . : */build/Jamfile ]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(self):\n        \"\"\"\n\n        \"\"\"\n        modification = table_admin_v2_pb2.ModifyColumnFamiliesRequest.Modification(\n            id=self.column_family_id, drop=True\n        )\n\n        client = self._table._instance._client\n        # data it contains are the GC rule and the column family ID already\n        # stored on this instance.\n        client.table_admin_client.modify_column_families(\n            self._table.name, [modification]\n        )", "output": "Delete this column family.\n\n        For example:\n\n        .. literalinclude:: snippets_table.py\n            :start-after: [START bigtable_delete_column_family]\n            :end-before: [END bigtable_delete_column_family]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_environment_from_config_file():\n    \"\"\"\n    \n    \"\"\"\n\n    from os.path import exists\n\n    config_file = get_config_file()\n\n    if not exists(config_file):\n        return\n\n    try:\n        config = _ConfigParser.SafeConfigParser()\n        config.read(config_file)\n\n        __section = \"Environment\"\n\n        if config.has_section(__section):\n            items = config.items(__section)\n\n            for k, v in items:\n                try:\n                    os.environ[k.upper()] = v\n                except Exception as e:\n                    print((\"WARNING: Error setting environment variable \"\n                           \"'%s = %s' from config file '%s': %s.\")\n                          % (k, str(v), config_file, str(e)) )\n    except Exception as e:\n        print(\"WARNING: Error reading config file '%s': %s.\" % (config_file, str(e)))", "output": "Imports the environmental configuration settings from the\n    config file, if present, and sets the environment\n    variables to test it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_networks(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The list_networks function must be called with '\n            '-f or --function.'\n        )\n\n    return {'Networks': salt.utils.vmware.list_networks(_get_si())}", "output": "List all the standard networks for this VMware environment\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f list_networks my-vmware-config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_info(info, name):\n    \"\"\"  \"\"\"\n    try:\n        idx = info[name]\n    except KeyError:\n        idx = info[name] = dict()\n    return idx", "output": "get/create the info for this name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_fs_model(self):\r\n        \"\"\"\"\"\"\r\n        filters = QDir.AllDirs | QDir.Files | QDir.Drives | QDir.NoDotAndDotDot\r\n        self.fsmodel = QFileSystemModel(self)\r\n        self.fsmodel.setFilter(filters)\r\n        self.fsmodel.setNameFilterDisables(False)", "output": "Setup filesystem model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_input_layers(self):\n        \"\"\"\n        \n        \"\"\"\n        self.input_layers = []\n        if hasattr(self.model, 'input_layers'):\n            input_keras_layers = self.model.input_layers[:]\n            self.input_layers = [None] * len(input_keras_layers)\n            for layer in self.layer_list:\n                keras_layer = self.keras_layer_map[layer]\n                if isinstance(keras_layer, _keras.engine.topology.InputLayer):\n                    if keras_layer in input_keras_layers:\n                        idx = input_keras_layers.index(keras_layer)\n                        self.input_layers[idx] = layer\n        elif len(self.model.inbound_nodes) <= 1:\n            for ts in _to_list(self.model.input):\n                # search for the InputLayer that matches this ts\n                for l in self.layer_list:\n                    kl = self.keras_layer_map[l]\n                    if isinstance(kl, _keras.engine.topology.InputLayer) and kl.input == ts:\n                        self.input_layers.append(l)\n        else:\n            raise ValueError(\"Input values cannot be identified.\")", "output": "Extract the ordering of the input layers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_header_size(self):\r\n        \"\"\"\"\"\"\r\n        column_count = self.table_header.model().columnCount()\r\n        for index in range(0, column_count):\r\n            if index < column_count:\r\n                column_width = self.dataTable.columnWidth(index)\r\n                self.table_header.setColumnWidth(index, column_width)\r\n            else:\r\n                break", "output": "Update the column width of the header.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_descriptors(self, base_name):\n        ''' \n        '''\n        units_name = base_name + \"_units\"\n        units_props = self._units_type.make_descriptors(units_name)\n        return units_props + [ UnitsSpecPropertyDescriptor(base_name, self, units_props[0]) ]", "output": "Return a list of ``PropertyDescriptor`` instances to install on a\n        class, in order to delegate attribute access to this property.\n\n        Unlike simpler property types, ``UnitsSpec`` returns multiple\n        descriptors to install. In particular, descriptors for the base\n        property as well as the associated units property are returned.\n\n        Args:\n            name (str) : the name of the property these descriptors are for\n\n        Returns:\n            list[PropertyDescriptor]\n\n        The descriptors returned are collected by the ``MetaHasProps``\n        metaclass and added to ``HasProps`` subclasses during class creation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nni_log(log_type, log_message):\n    ''''''\n    dt = datetime.now()\n    print('[{0}] {1} {2}'.format(dt, log_type.value, log_message))", "output": "Log message into stdout", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_signed_value(\n        self, name: str, value: Union[str, bytes], version: int = None\n    ) -> bytes:\n        \"\"\"\n        \"\"\"\n        self.require_setting(\"cookie_secret\", \"secure cookies\")\n        secret = self.application.settings[\"cookie_secret\"]\n        key_version = None\n        if isinstance(secret, dict):\n            if self.application.settings.get(\"key_version\") is None:\n                raise Exception(\"key_version setting must be used for secret_key dicts\")\n            key_version = self.application.settings[\"key_version\"]\n\n        return create_signed_value(\n            secret, name, value, version=version, key_version=key_version\n        )", "output": "Signs and timestamps a string so it cannot be forged.\n\n        Normally used via set_secure_cookie, but provided as a separate\n        method for non-cookie uses.  To decode a value not stored\n        as a cookie use the optional value argument to get_secure_cookie.\n\n        .. versionchanged:: 3.2.1\n\n           Added the ``version`` argument.  Introduced cookie version 2\n           and made it the default.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deep_discriminator(x,\n                       batch_norm,\n                       is_training,\n                       filters=64,\n                       filter_size=4,\n                       stride=2,\n                       output_size=1024):\n  \"\"\"\"\"\"\n  with tf.variable_scope(\n      \"discriminator\", initializer=tf.random_normal_initializer(stddev=0.02)):\n    batch_size, height, width = shape_list(x)[:3]  # pylint: disable=unbalanced-tuple-unpacking\n    net = layers().Conv2D(\n        filters, filter_size, strides=stride, padding=\"SAME\", name=\"conv1\")(x)\n    net = lrelu(net)\n    net = layers().Conv2D(\n        2 * filters,\n        filter_size,\n        strides=stride,\n        padding=\"SAME\",\n        name=\"conv2\")(net)\n    # [bs, h/4, w/4, 128]\n    if batch_norm:\n      net = layers().BatchNormalization(\n          training=is_training, momentum=0.999, name=\"d_bn2\")(net)\n    net = lrelu(net)\n    size = height * width\n    x_shape = x.get_shape().as_list()\n    if x_shape[1] is None or x_shape[2] is None:\n      net = tf.reduce_mean(net, axis=[1, 2])\n    else:\n      net = tf.reshape(net, [batch_size, size * 8])\n    net = layers().Dense(output_size, name=\"d_fc3\")(net)\n    if batch_norm:\n      net = layers().BatchNormalization(\n          training=is_training, momentum=0.999, name=\"d_bn3\")(net)\n    net = lrelu(net)\n    return net", "output": "Discriminator architecture based on InfoGAN.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _to_protobuf(self):\n        \"\"\"\n        \"\"\"\n        projection = self._normalize_projection(self._projection)\n        orders = self._normalize_orders()\n        start_at = self._normalize_cursor(self._start_at, orders)\n        end_at = self._normalize_cursor(self._end_at, orders)\n\n        query_kwargs = {\n            \"select\": projection,\n            \"from\": [\n                query_pb2.StructuredQuery.CollectionSelector(\n                    collection_id=self._parent.id\n                )\n            ],\n            \"where\": self._filters_pb(),\n            \"order_by\": orders,\n            \"start_at\": _cursor_pb(start_at),\n            \"end_at\": _cursor_pb(end_at),\n        }\n        if self._offset is not None:\n            query_kwargs[\"offset\"] = self._offset\n        if self._limit is not None:\n            query_kwargs[\"limit\"] = wrappers_pb2.Int32Value(value=self._limit)\n\n        return query_pb2.StructuredQuery(**query_kwargs)", "output": "Convert the current query into the equivalent protobuf.\n\n        Returns:\n            google.cloud.firestore_v1beta1.types.StructuredQuery: The\n            query protobuf.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def status_load():\n    '''\n    \n    '''\n    data = status()\n    if 'LOADPCT' in data:\n        load = data['LOADPCT'].split()\n        if load[1].lower() == 'percent':\n            return float(load[0])\n\n    return {'Error': 'Load not available.'}", "output": "Return load\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' apcups.status_load", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def alexnet(num_classes=1000, pretrained='imagenet'):\n    \n    \"\"\"\n    # https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py\n    model = models.alexnet(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['alexnet'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_alexnet(model)\n    return model", "output": "r\"\"\"AlexNet model architecture from the\n    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shutdown(opts):\n    '''\n    \n    '''\n    try:\n        if not NETWORK_DEVICE.get('UP', False):\n            raise Exception('not connected!')\n        NETWORK_DEVICE.get('DRIVER').close()\n    except Exception as error:\n        port = NETWORK_DEVICE.get('OPTIONAL_ARGS', {}).get('port')\n        log.error(\n            'Cannot close connection with %s%s! Please check error: %s',\n            NETWORK_DEVICE.get('HOSTNAME', '[unknown hostname]'),\n            ':{0}'.format(port) if port else '',\n            error\n        )\n\n    return True", "output": "Closes connection with the device.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ConvertMessage(self, value, message):\n    \"\"\"\n    \"\"\"\n    message_descriptor = message.DESCRIPTOR\n    full_name = message_descriptor.full_name\n    if _IsWrapperMessage(message_descriptor):\n      self._ConvertWrapperMessage(value, message)\n    elif full_name in _WKTJSONMETHODS:\n      methodcaller(_WKTJSONMETHODS[full_name][1], value, message)(self)\n    else:\n      self._ConvertFieldValuePair(value, message)", "output": "Convert a JSON object into a message.\n\n    Args:\n      value: A JSON object.\n      message: A WKT or regular protocol message to record the data.\n\n    Raises:\n      ParseError: In case of convert problems.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_config_file(name):\n    '''\n    \n\n    '''\n    global __SYSLOG_NG_CONFIG_FILE\n    old = __SYSLOG_NG_CONFIG_FILE\n    __SYSLOG_NG_CONFIG_FILE = name\n    changes = _format_changes(old, name)\n    return _format_state_result(name, result=True, changes=changes)", "output": "Sets the configuration's name. This function is intended to be used from\n    states.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' syslog_ng.set_config_file name=/etc/syslog-ng", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flags(self, index):\r\n        \"\"\"\"\"\"\r\n        return Qt.ItemFlags(QAbstractTableModel.flags(self, index) |\r\n                            Qt.ItemIsEditable)", "output": "Set flags", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hasNsProp(self, name, nameSpace):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlHasNsProp(self._o, name, nameSpace)\n        if ret is None:return None\n        __tmp = xmlAttr(_obj=ret)\n        return __tmp", "output": "Search for an attribute associated to a node This attribute\n          has to be anchored in the namespace specified. This does\n          the entity substitution. This function looks in DTD\n          attribute declaration for #FIXED or default declaration\n          values unless DTD use has been turned off. Note that a\n           namespace of None indicates to use the default namespace.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_environment(self, environment):\n        \"\"\"\n        \n        \"\"\"\n        for name, value in environment.items():\n            try:\n                self.set_environment_variable(name, value)\n            except SSHException as e:\n                err = 'Failed to set environment variable \"{}\".'\n                raise SSHException(err.format(name), e)", "output": "Updates this channel's remote shell environment.\n\n        .. note::\n            This operation is additive - i.e. the current environment is not\n            reset before the given environment variables are set.\n\n        .. warning::\n            Servers may silently reject some environment variables; see the\n            warning in `set_environment_variable` for details.\n\n        :param dict environment:\n            a dictionary containing the name and respective values to set\n        :raises:\n            `.SSHException` -- if any of the environment variables was rejected\n            by the server or the channel was closed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __deserialize(self, data, klass):\n        \"\"\"\n        \n        \"\"\"\n        if data is None:\n            return None\n\n        if type(klass) == str:\n            if klass.startswith('list['):\n                sub_kls = re.match('list\\[(.*)\\]', klass).group(1)\n                return [self.__deserialize(sub_data, sub_kls)\n                        for sub_data in data]\n\n            if klass.startswith('dict('):\n                sub_kls = re.match('dict\\(([^,]*), (.*)\\)', klass).group(2)\n                return {k: self.__deserialize(v, sub_kls)\n                        for k, v in iteritems(data)}\n\n            # convert str to class\n            if klass in self.NATIVE_TYPES_MAPPING:\n                klass = self.NATIVE_TYPES_MAPPING[klass]\n            else:\n                klass = getattr(models, klass)\n\n        if klass in self.PRIMITIVE_TYPES:\n            return self.__deserialize_primitive(data, klass)\n        elif klass == object:\n            return self.__deserialize_object(data)\n        elif klass == date:\n            return self.__deserialize_date(data)\n        elif klass == datetime:\n            return self.__deserialize_datatime(data)\n        else:\n            return self.__deserialize_model(data, klass)", "output": "Deserializes dict, list, str into an object.\n\n        :param data: dict, list or str.\n        :param klass: class literal, or string of class name.\n\n        :return: object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expect_loop(self, searcher, timeout=-1, searchwindowsize=-1):\n        ''' '''\n\n        exp = Expecter(self, searcher, searchwindowsize)\n        return exp.expect_loop(timeout)", "output": "This is the common loop used inside expect. The 'searcher' should be\n        an instance of searcher_re or searcher_string, which describes how and\n        what to search for in the input.\n\n        See expect() for other arguments, return value and exceptions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def min(self, **kwargs):\n        \"\"\"\n        \"\"\"\n        if self._is_transposed:\n            kwargs[\"axis\"] = kwargs.get(\"axis\", 0) ^ 1\n            return self.transpose().min(**kwargs)\n        mapreduce_func = self._build_mapreduce_func(pandas.DataFrame.min, **kwargs)\n        return self._full_reduce(kwargs.get(\"axis\", 0), mapreduce_func)", "output": "Returns the minimum from each column or row.\n\n        Return:\n            A new QueryCompiler object with the minimum value from each column or row.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, resource, client):\n        \"\"\"\n        \"\"\"\n        job_id, config = cls._get_resource_config(resource)\n        query = config[\"query\"][\"query\"]\n        job = cls(job_id, query, client=client)\n        job._set_properties(resource)\n        return job", "output": "Factory:  construct a job given its API representation\n\n        :type resource: dict\n        :param resource: dataset job representation returned from the API\n\n        :type client: :class:`google.cloud.bigquery.client.Client`\n        :param client: Client which holds credentials and project\n                       configuration for the dataset.\n\n        :rtype: :class:`google.cloud.bigquery.job.QueryJob`\n        :returns: Job parsed from ``resource``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_position_timing_signal(x, step, hparams):\n  \"\"\"\n\n  \"\"\"\n\n  if not hparams.position_start_index:\n    index = 0\n\n  elif hparams.position_start_index == \"random\":\n    # Shift all positions randomly\n    # TODO(dehghani): What would be reasonable for max number of shift?\n    index = tf.random_uniform(\n        [], maxval=common_layers.shape_list(x)[1], dtype=tf.int32)\n\n  elif hparams.position_start_index == \"step\":\n    # Shift positions based on the step\n    if hparams.recurrence_type == \"act\":\n      num_steps = hparams.act_max_steps\n    else:\n      num_steps = hparams.num_rec_steps\n    index = tf.cast(\n        common_layers.shape_list(x)[1] * step / num_steps, dtype=tf.int32)\n\n  # No need for the timing signal in the encoder/decoder input preparation\n  assert hparams.pos is None\n\n  length = common_layers.shape_list(x)[1]\n  channels = common_layers.shape_list(x)[2]\n  signal = common_attention.get_timing_signal_1d(\n      length, channels, start_index=index)\n\n  if hparams.add_or_concat_timing_signal == \"add\":\n    x_with_timing = x + common_layers.cast_like(signal, x)\n\n  elif hparams.add_or_concat_timing_signal == \"concat\":\n    batch_size = common_layers.shape_list(x)[0]\n    signal_tiled = tf.tile(signal, [batch_size, 1, 1])\n    x_with_timing = tf.concat((x, signal_tiled), axis=-1)\n\n  return x_with_timing", "output": "Add n-dimensional embedding as the position (horizontal) timing signal.\n\n  Args:\n    x: a tensor with shape [batch, length, depth]\n    step: step\n    hparams: model hyper parameters\n\n  Returns:\n    a Tensor with the same shape as x.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_distro_release_content(line):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(line, bytes):\n            line = line.decode('utf-8')\n        matches = _DISTRO_RELEASE_CONTENT_REVERSED_PATTERN.match(\n            line.strip()[::-1])\n        distro_info = {}\n        if matches:\n            # regexp ensures non-None\n            distro_info['name'] = matches.group(3)[::-1]\n            if matches.group(2):\n                distro_info['version_id'] = matches.group(2)[::-1]\n            if matches.group(1):\n                distro_info['codename'] = matches.group(1)[::-1]\n        elif line:\n            distro_info['name'] = line.strip()\n        return distro_info", "output": "Parse a line from a distro release file.\n\n        Parameters:\n        * line: Line from the distro release file. Must be a unicode string\n                or a UTF-8 encoded byte string.\n\n        Returns:\n            A dictionary containing all information items.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_items(action, key, profile_dict=None, api_key=None, opts=None):\n    '''\n    \n    '''\n    items = salt.utils.json.loads(query(\n        profile_dict=profile_dict,\n        api_key=api_key,\n        action=action,\n        opts=opts\n    ))\n    ret = {}\n    for item in items[action]:\n        ret[item[key]] = item\n    return ret", "output": "List items belonging to an API call. Used for list_services() and\n    list_incidents()", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def valid_certificate(name,\n                      weeks=0,\n                      days=0,\n                      hours=0,\n                      minutes=0,\n                      seconds=0):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': False,\n           'comment': ''}\n\n    now = time.time()\n    try:\n        cert_info = __salt__['tls.cert_info'](name)\n    except IOError as exc:\n        ret['comment'] = '{}'.format(exc)\n        ret['result'] = False\n        log.error(ret['comment'])\n        return ret\n\n    # verify that the cert is valid *now*\n    if now < cert_info['not_before']:\n        ret['comment'] = 'Certificate is not yet valid'\n        return ret\n    if now > cert_info['not_after']:\n        ret['comment'] = 'Certificate is expired'\n        return ret\n\n    # verify the cert will be valid for defined time\n    delta_remaining = datetime.timedelta(seconds=cert_info['not_after']-now)\n    delta_kind_map = {\n        'weeks': weeks,\n        'days': days,\n        'hours': hours,\n        'minutes': minutes,\n        'seconds': seconds,\n    }\n\n    delta_min = datetime.timedelta(**delta_kind_map)\n    # if ther eisn't enough time remaining, we consider it a failure\n    if delta_remaining < delta_min:\n        ret['comment'] = 'Certificate will expire in {0}, which is less than {1}'.format(delta_remaining, delta_min)\n        return ret\n\n    ret['result'] = True\n    ret['comment'] = 'Certificate is valid for {0}'.format(delta_remaining)\n    return ret", "output": "Verify that a TLS certificate is valid now and (optionally) will be valid\n    for the time specified through weeks, days, hours, minutes, and seconds.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clicked(self):\r\n        \"\"\"\"\"\"\r\n        fnames = self.get_selected_filenames()\r\n        for fname in fnames:\r\n            if osp.isdir(fname):\r\n                self.directory_clicked(fname)\r\n            else:\r\n                self.open([fname])", "output": "Selected item was double-clicked or enter/return was pressed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def quote_chinese(url, encodeing=\"utf-8\"):\n    \"\"\"\"\"\"\n    if isinstance(url, six.text_type):\n        return quote_chinese(url.encode(encodeing))\n    if six.PY3:\n        res = [six.int2byte(b).decode('latin-1') if b < 128 else '%%%02X' % b for b in url]\n    else:\n        res = [b if ord(b) < 128 else '%%%02X' % ord(b) for b in url]\n    return \"\".join(res)", "output": "Quote non-ascii characters", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _toolkit_get_topk_bottomk(values, k=5):\n    \"\"\"\n    \n    \"\"\"\n\n    top_values = values.topk('value', k=k)\n    top_values = top_values[top_values['value'] > 0]\n\n    bottom_values = values.topk('value', k=k, reverse=True)\n    bottom_values = bottom_values[bottom_values['value'] < 0]\n\n    return (top_values, bottom_values)", "output": "Returns a tuple of the top k values from the positive and\n    negative values in a SArray\n\n    Parameters\n    ----------\n    values : SFrame of model coefficients\n\n    k: Maximum number of largest positive and k lowest negative numbers to return\n\n    Returns\n    -------\n    (topk_positive, bottomk_positive) : tuple\n            topk_positive : list\n                floats that represent the top 'k' ( or less ) positive\n                values\n            bottomk_positive : list\n                floats that represent the top 'k' ( or less ) negative\n                values", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def create_invite(self, *, reason=None, **fields):\n        \"\"\"\n        \"\"\"\n\n        data = await self._state.http.create_invite(self.id, reason=reason, **fields)\n        return Invite.from_incomplete(data=data, state=self._state)", "output": "|coro|\n\n        Creates an instant invite.\n\n        You must have :attr:`~.Permissions.create_instant_invite` permission to\n        do this.\n\n        Parameters\n        ------------\n        max_age: :class:`int`\n            How long the invite should last. If it's 0 then the invite\n            doesn't expire. Defaults to 0.\n        max_uses: :class:`int`\n            How many uses the invite could be used for. If it's 0 then there\n            are unlimited uses. Defaults to 0.\n        temporary: :class:`bool`\n            Denotes that the invite grants temporary membership\n            (i.e. they get kicked after they disconnect). Defaults to False.\n        unique: :class:`bool`\n            Indicates if a unique invite URL should be created. Defaults to True.\n            If this is set to False then it will return a previously created\n            invite.\n        reason: Optional[:class:`str`]\n            The reason for creating this invite. Shows up on the audit log.\n\n        Raises\n        -------\n        HTTPException\n            Invite creation failed.\n\n        Returns\n        --------\n        :class:`Invite`\n            The invite that was created.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def BNReLU(x, name=None):\n    \"\"\"\n    \n    \"\"\"\n    x = BatchNorm('bn', x)\n    x = tf.nn.relu(x, name=name)\n    return x", "output": "A shorthand of BatchNormalization + ReLU.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_seq(tokens, options):\n    \"\"\"\"\"\"\n    result = []\n    while tokens.current() not in [None, ']', ')', '|']:\n        atom = parse_atom(tokens, options)\n        if tokens.current() == '...':\n            atom = [OneOrMore(*atom)]\n            tokens.move()\n        result += atom\n    return result", "output": "seq ::= ( atom [ '...' ] )* ;", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_snapshot(vm, name, key='uuid'):\n    '''\n    \n    '''\n    ret = {}\n    if key not in ['uuid', 'alias', 'hostname']:\n        ret['Error'] = 'Key must be either uuid, alias or hostname'\n        return ret\n    vm = lookup('{0}={1}'.format(key, vm), one=True)\n    if 'Error' in vm:\n        return vm\n    vmobj = get(vm)\n    if 'datasets' in vmobj:\n        ret['Error'] = 'VM cannot have datasets'\n        return ret\n    if vmobj['brand'] in ['kvm']:\n        ret['Error'] = 'VM must be of type OS'\n        return ret\n    if vmobj['zone_state'] not in ['running']:  # work around a vmadm bug\n        ret['Error'] = 'VM must be running to take a snapshot'\n        return ret\n    # vmadm create-snapshot <uuid> <snapname>\n    cmd = 'vmadm create-snapshot {uuid} {snapshot}'.format(\n        snapshot=name,\n        uuid=vm\n    )\n    res = __salt__['cmd.run_all'](cmd)\n    retcode = res['retcode']\n    if retcode != 0:\n        ret['Error'] = res['stderr'] if 'stderr' in res else _exit_status(retcode)\n        return ret\n    return True", "output": "Create snapshot of a vm\n\n    vm : string\n        vm to be targeted\n    name : string\n        snapshot name\n            The snapname must be 64 characters or less\n            and must only contain alphanumeric characters and\n            characters in the set [-_.:%] to comply with ZFS restrictions.\n    key : string [uuid|alias|hostname]\n        value type of 'vm' parameter\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' vmadm.create_snapshot 186da9ab-7392-4f55-91a5-b8f1fe770543 baseline\n        salt '*' vmadm.create_snapshot nacl baseline key=alias", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def finger_all(self, hash_type=None):\n        '''\n        \n        '''\n        if hash_type is None:\n            hash_type = __opts__['hash_type']\n\n        ret = {}\n        for status, keys in six.iteritems(self.all_keys()):\n            ret[status] = {}\n            for key in keys:\n                if status == 'local':\n                    path = os.path.join(self.opts['pki_dir'], key)\n                else:\n                    path = os.path.join(self.opts['pki_dir'], status, key)\n                ret[status][key] = salt.utils.crypt.pem_finger(path, sum_type=hash_type)\n        return ret", "output": "Return fingerprints for all keys", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def total_commission(self):\n        \"\"\"\n        \"\"\"\n        return float(\n            -abs(round(self.account.history_table.commission.sum(),\n                       2))\n        )", "output": "\u603b\u624b\u7eed\u8d39", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def open(self, callback):\n        \"\"\"\n        \"\"\"\n        if self.is_active:\n            raise ValueError(\"This manager is already open.\")\n\n        if self._closed:\n            raise ValueError(\"This manager has been closed and can not be re-used.\")\n\n        self._callback = functools.partial(_wrap_callback_errors, callback)\n\n        # Create the RPC\n        self._rpc = bidi.ResumableBidiRpc(\n            start_rpc=self._client.api.streaming_pull,\n            initial_request=self._get_initial_request,\n            should_recover=self._should_recover,\n        )\n        self._rpc.add_done_callback(self._on_rpc_done)\n\n        # Create references to threads\n        self._dispatcher = dispatcher.Dispatcher(self, self._scheduler.queue)\n        self._consumer = bidi.BackgroundConsumer(self._rpc, self._on_response)\n        self._leaser = leaser.Leaser(self)\n        self._heartbeater = heartbeater.Heartbeater(self)\n\n        # Start the thread to pass the requests.\n        self._dispatcher.start()\n\n        # Start consuming messages.\n        self._consumer.start()\n\n        # Start the lease maintainer thread.\n        self._leaser.start()\n\n        # Start the stream heartbeater thread.\n        self._heartbeater.start()", "output": "Begin consuming messages.\n\n        Args:\n            callback (Callable[None, google.cloud.pubsub_v1.message.Messages]):\n                A callback that will be called for each message received on the\n                stream.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_sid_attrs(self, sid, **kwargs):\n        \"\"\"\n        \"\"\"\n        table = self._ensure_ctable(sid)\n        for k, v in kwargs.items():\n            table.attrs[k] = v", "output": "Write all the supplied kwargs as attributes of the sid's file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def regexpCompile(regexp):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlRegexpCompile(regexp)\n    if ret is None:raise treeError('xmlRegexpCompile() failed')\n    return xmlReg(_obj=ret)", "output": "Parses a regular expression conforming to XML Schemas Part\n      2 Datatype Appendix F and builds an automata suitable for\n       testing strings against that regular expression", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def report_missing_dependencies(self):\r\n        \"\"\"\"\"\"\r\n        missing_deps = dependencies.missing_dependencies()\r\n        if missing_deps:\r\n            QMessageBox.critical(self, _('Error'),\r\n                _(\"<b>You have missing dependencies!</b>\"\r\n                  \"<br><br><tt>%s</tt><br><br>\"\r\n                  \"<b>Please install them to avoid this message.</b>\"\r\n                  \"<br><br>\"\r\n                  \"<i>Note</i>: Spyder could work without some of these \"\r\n                  \"dependencies, however to have a smooth experience when \"\r\n                  \"using Spyder we <i>strongly</i> recommend you to install \"\r\n                  \"all the listed missing dependencies.<br><br>\"\r\n                  \"Failing to install these dependencies might result in bugs. \"\r\n                  \"Please be sure that any found bugs are not the direct \"\r\n                  \"result of missing dependencies, prior to reporting a new \"\r\n                  \"issue.\"\r\n                  ) % missing_deps, QMessageBox.Ok)", "output": "Show a QMessageBox with a list of missing hard dependencies", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare(self, data_batch, sparse_row_id_fn=None):\n        '''\n        '''\n        # perform bind if haven't done so\n        assert self.binded and self.params_initialized\n        bucket_key = data_batch.bucket_key\n        original_bucket_key = self._curr_bucket_key\n        data_shapes = data_batch.provide_data\n        label_shapes = data_batch.provide_label\n        self.switch_bucket(bucket_key, data_shapes, label_shapes)\n        self._curr_module.prepare(data_batch, sparse_row_id_fn=sparse_row_id_fn)\n        # switch back\n        self.switch_bucket(original_bucket_key, None, None)", "output": "Prepares the module for processing a data batch.\n\n        Usually involves switching bucket and reshaping.\n        For modules that contain `row_sparse` parameters in KVStore,\n        it prepares the `row_sparse` parameters based on the sparse_row_id_fn.\n\n        Parameters\n        ----------\n        data_batch : DataBatch\n            The current batch of data for forward computation.\n\n        sparse_row_id_fn : A callback function\n            The function  takes `data_batch` as an input and returns a dict of\n            str -> NDArray. The resulting dict is used for pulling row_sparse\n            parameters from the kvstore, where the str key is the name of the param,\n            and the value is the row id of the param to pull.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def soft_kill(jid, state_id=None):\n    '''\n    \n    '''\n    jid = six.text_type(jid)\n    if state_id is None:\n        state_id = '__all__'\n    data, pause_path = _get_pause(jid, state_id)\n    data[state_id]['kill'] = True\n    with salt.utils.files.fopen(pause_path, 'wb') as fp_:\n        fp_.write(salt.utils.msgpack.dumps(data))", "output": "Set up a state run to die before executing the given state id,\n    this instructs a running state to safely exit at a given\n    state id. This needs to pass in the jid of the running state.\n    If a state_id is not passed then the jid referenced will be safely exited\n    at the beginning of the next state run.\n\n    The given state id is the id got a given state execution, so given a state\n    that looks like this:\n\n    .. code-block:: yaml\n\n        vim:\n          pkg.installed: []\n\n    The state_id to pass to `soft_kill` is `vim`\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' state.soft_kill 20171130110407769519\n        salt '*' state.soft_kill 20171130110407769519 vim", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def target_path (self):\n        \"\"\" \n        \"\"\"\n        if not self.target_path_:\n            # The <location> feature can be used to explicitly\n            # change the location of generated targets\n            l = self.get ('<location>')\n            if l:\n                computed = l[0]\n                is_relative = False\n\n            else:\n                p = self.as_path()\n                if hash_maybe:\n                    p = hash_maybe(p)\n\n                # Really, an ugly hack. Boost regression test system requires\n                # specific target paths, and it seems that changing it to handle\n                # other directory layout is really hard. For that reason,\n                # we teach V2 to do the things regression system requires.\n                # The value o '<location-prefix>' is predended to the path.\n                prefix = self.get ('<location-prefix>')\n\n                if prefix:\n                    if len (prefix) > 1:\n                        raise AlreadyDefined (\"Two <location-prefix> properties specified: '%s'\" % prefix)\n\n                    computed = os.path.join(prefix[0], p)\n\n                else:\n                    computed = p\n\n                if not computed:\n                    computed = \".\"\n\n                is_relative = True\n\n            self.target_path_ = (computed, is_relative)\n\n        return self.target_path_", "output": "Computes the target path that should be used for\n            target with these properties.\n            Returns a tuple of\n              - the computed path\n              - if the path is relative to build directory, a value of\n                'true'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def window(self, window_name):\n        \"\"\"\n        \n        \"\"\"\n        if self._driver.w3c:\n            self._w3c_window(window_name)\n            return\n        data = {'name': window_name}\n        self._driver.execute(Command.SWITCH_TO_WINDOW, data)", "output": "Switches focus to the specified window.\n\n        :Args:\n         - window_name: The name or window handle of the window to switch to.\n\n        :Usage:\n            ::\n\n                driver.switch_to.window('main')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_pandas(self):\r\n        \"\"\"\r\n        \"\"\"\r\n        df = self.data.to_pandas(is_transposed=self._is_transposed)\r\n        if df.empty:\r\n            dtype_dict = {\r\n                col_name: pandas.Series(dtype=self.dtypes[col_name])\r\n                for col_name in self.columns\r\n            }\r\n            df = pandas.DataFrame(dtype_dict, self.index)\r\n        else:\r\n            ErrorMessage.catch_bugs_and_request_email(\r\n                len(df.index) != len(self.index) or len(df.columns) != len(self.columns)\r\n            )\r\n            df.index = self.index\r\n            df.columns = self.columns\r\n        return df", "output": "Converts Modin DataFrame to Pandas DataFrame.\r\n\r\n        Returns:\r\n            Pandas DataFrame of the DataManager.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ack(self):\n        \"\"\"\n        \"\"\"\n        time_to_ack = math.ceil(time.time() - self._received_timestamp)\n        self._request_queue.put(\n            requests.AckRequest(\n                ack_id=self._ack_id, byte_size=self.size, time_to_ack=time_to_ack\n            )\n        )", "output": "Acknowledge the given message.\n\n        Acknowledging a message in Pub/Sub means that you are done\n        with it, and it will not be delivered to this subscription again.\n        You should avoid acknowledging messages until you have\n        *finished* processing them, so that in the event of a failure,\n        you receive the message again.\n\n        .. warning::\n            Acks in Pub/Sub are best effort. You should always\n            ensure that your processing code is idempotent, as you may\n            receive any given message more than once.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_grpc_request_fn(servable_name, server, timeout_secs):\n  \"\"\"\"\"\"\n  stub = _create_stub(server)\n\n  def _make_grpc_request(examples):\n    \"\"\"Builds and sends request to TensorFlow model server.\"\"\"\n    request = predict_pb2.PredictRequest()\n    request.model_spec.name = servable_name\n    request.inputs[\"input\"].CopyFrom(\n        tf.make_tensor_proto(\n            [ex.SerializeToString() for ex in examples], shape=[len(examples)]))\n    response = stub.Predict(request, timeout_secs)\n    outputs = tf.make_ndarray(response.outputs[\"outputs\"])\n    scores = tf.make_ndarray(response.outputs[\"scores\"])\n    assert len(outputs) == len(scores)\n    return [{  # pylint: disable=g-complex-comprehension\n        \"outputs\": output,\n        \"scores\": score\n    } for output, score in zip(outputs, scores)]\n\n  return _make_grpc_request", "output": "Wraps function to make grpc requests with runtime args.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_arn(name, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    if name.startswith('arn:aws:sns:'):\n        return name\n\n    account_id = __salt__['boto_iam.get_account_id'](\n        region=region, key=key, keyid=keyid, profile=profile\n    )\n    return 'arn:aws:sns:{0}:{1}:{2}'.format(_get_region(region, profile),\n                                            account_id, name)", "output": "Returns the full ARN for a given topic name.\n\n    CLI example::\n\n        salt myminion boto_sns.get_arn mytopic", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mod_aggregate(low, chunks, running):\n    '''\n    \n    '''\n    pkgs = []\n    pkg_type = None\n    agg_enabled = [\n        'installed',\n        'removed',\n    ]\n    if low.get('fun') not in agg_enabled:\n        return low\n    for chunk in chunks:\n        tag = __utils__['state.gen_tag'](chunk)\n        if tag in running:\n            # Already ran the pkg state, skip aggregation\n            continue\n        if chunk.get('state') == 'pip':\n            if '__agg__' in chunk:\n                continue\n            # Check for the same function\n            if chunk.get('fun') != low.get('fun'):\n                continue\n            # Check first if 'sources' was passed so we don't aggregate pkgs\n            # and sources together.\n            if pkg_type is None:\n                pkg_type = 'pkgs'\n            if pkg_type == 'pkgs':\n                # Pull out the pkg names!\n                if 'pkgs' in chunk:\n                    pkgs.extend(chunk['pkgs'])\n                    chunk['__agg__'] = True\n                elif 'name' in chunk:\n                    version = chunk.pop('version', None)\n                    if version is not None:\n                        pkgs.append({chunk['name']: version})\n                    else:\n                        pkgs.append(chunk['name'])\n                    chunk['__agg__'] = True\n    if pkg_type is not None and pkgs:\n        if pkg_type in low:\n            low[pkg_type].extend(pkgs)\n        else:\n            low[pkg_type] = pkgs\n    return low", "output": "The mod_aggregate function which looks up all packages in the available\n    low chunks and merges them into a single pkgs ref in the present low data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _token_to_ids(self, token):\n    \"\"\"\"\"\"\n    # Check cache\n    cache_location = hash(token) % self._cache_size\n    cache_key, cache_value = self._token_to_ids_cache[cache_location]\n    if cache_key == token:\n      return cache_value\n\n    subwords = self._token_to_subwords(token)\n    ids = []\n    for subword in subwords:\n      if subword == _UNDERSCORE_REPLACEMENT:\n        ids.append(len(self._subwords) + ord(\"_\"))\n        continue\n      subword_id = self._subword_to_id.get(subword)\n      if subword_id is None:\n        # Byte-encode\n        ids.extend(self._byte_encode(subword))\n      else:\n        ids.append(subword_id)\n\n    # Update cache\n    self._token_to_ids_cache[cache_location] = (token, ids)\n\n    return ids", "output": "Convert a single token to a list of integer ids.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_graph_def(self, graph_def, device_name, wall_time):\n    \"\"\"\n    \"\"\"\n    # For now, we do nothing with the graph def. However, we must define this\n    # method to satisfy the handler's interface. Furthermore, we may use the\n    # graph in the future (for instance to provide a graph if there is no graph\n    # provided otherwise).\n    del wall_time\n    self._graph_defs[device_name] = graph_def\n\n    if not self._graph_defs_arrive_first:\n      self._add_graph_def(device_name, graph_def)\n      self._incoming_channel.get()", "output": "Implementation of the GraphDef-carrying Event proto callback.\n\n    Args:\n      graph_def: A GraphDef proto. N.B.: The GraphDef is from\n        the core runtime of a debugged Session::Run() call, after graph\n        partition. Therefore it may differ from the GraphDef available to\n        the general TensorBoard. For example, the GraphDef in general\n        TensorBoard may get partitioned for multiple devices (CPUs and GPUs),\n        each of which will generate a GraphDef event proto sent to this\n        method.\n      device_name: Name of the device on which the graph was created.\n      wall_time: An epoch timestamp (in microseconds) for the graph.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def step(self)->None:\n        \"\"\n        # weight decay outside of optimizer step (AdamW)\n        if self.true_wd:\n            for lr,wd,pg1,pg2 in zip(self._lr,self._wd,self.opt.param_groups[::2],self.opt.param_groups[1::2]):\n                for p in pg1['params']: p.data.mul_(1 - wd*lr)\n                if self.bn_wd:\n                    for p in pg2['params']: p.data.mul_(1 - wd*lr)\n            self.set_val('weight_decay', listify(0, self._wd))\n        self.opt.step()", "output": "Set weight decay and step optimizer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear(self):\n        \"\"\"\"\"\"\n        if self.prefix is not None:\n            self._current_page = [self.prefix]\n            self._count = len(self.prefix) + 1 # prefix + newline\n        else:\n            self._current_page = []\n            self._count = 0\n        self._pages = []", "output": "Clears the paginator to have no pages.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _complete_options(self):\r\n        \"\"\"\"\"\"\r\n        text = to_text_string(self.currentText())\r\n        opts = glob.glob(text + \"*\")\r\n        opts = sorted([opt for opt in opts if osp.isdir(opt)])\r\n        self.setCompleter(QCompleter(opts, self))\r\n        return opts", "output": "Find available completion options.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def basic_lstm(inputs, state, num_units, name=None):\n  \"\"\"\"\"\"\n  input_shape = common_layers.shape_list(inputs)\n  # reuse parameters across time-steps.\n  cell = tf.nn.rnn_cell.BasicLSTMCell(\n      num_units, name=name, reuse=tf.AUTO_REUSE)\n  if state is None:\n    state = cell.zero_state(input_shape[0], tf.float32)\n  outputs, new_state = cell(inputs, state)\n  return outputs, new_state", "output": "Basic LSTM.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def palindromic_substrings_iter(s):\n    \"\"\"\n    \n    \"\"\"\n    if not s:\n        yield []\n        return\n    for i in range(len(s), 0, -1):\n        sub = s[:i]\n        if sub == sub[::-1]:\n            for rest in palindromic_substrings_iter(s[i:]):\n                yield [sub] + rest", "output": "A slightly more Pythonic approach with a recursive generator", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_data(data_path, size, dataset):\n    \"\"\"\n    \"\"\"\n    image_size = 32\n    if dataset == \"cifar10\":\n        label_bytes = 1\n        label_offset = 0\n    elif dataset == \"cifar100\":\n        label_bytes = 1\n        label_offset = 1\n    depth = 3\n    image_bytes = image_size * image_size * depth\n    record_bytes = label_bytes + label_offset + image_bytes\n\n    def load_transform(value):\n        # Convert these examples to dense labels and processed images.\n        record = tf.reshape(tf.decode_raw(value, tf.uint8), [record_bytes])\n        label = tf.cast(tf.slice(record, [label_offset], [label_bytes]),\n                        tf.int32)\n        # Convert from string to [depth * height * width] to\n        # [depth, height, width].\n        depth_major = tf.reshape(\n            tf.slice(record, [label_bytes], [image_bytes]),\n            [depth, image_size, image_size])\n        # Convert from [depth, height, width] to [height, width, depth].\n        image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n        return (image, label)\n    # Read examples from files in the filename queue.\n    data_files = tf.gfile.Glob(data_path)\n    data = tf.contrib.data.FixedLengthRecordDataset(data_files,\n                                                    record_bytes=record_bytes)\n    data = data.map(load_transform)\n    data = data.batch(size)\n    iterator = data.make_one_shot_iterator()\n    return iterator.get_next()", "output": "Creates the queue and preprocessing operations for the dataset.\n\n    Args:\n        data_path: Filename for cifar10 data.\n        size: The number of images in the dataset.\n        dataset: The dataset we are using.\n\n    Returns:\n        queue: A Tensorflow queue for extracting the images and labels.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def traverse_data(obj, use_numpy=True, buffers=None):\n    ''' \n    '''\n    if use_numpy and all(isinstance(el, np.ndarray) for el in obj):\n        return [transform_array(el, buffers=buffers) for el in obj]\n    obj_copy = []\n    for item in obj:\n        # Check the base/common case first for performance reasons\n        # Also use type(x) is float because it's faster than isinstance\n        if type(item) is float:\n            if math.isnan(item):\n                item = 'NaN'\n            elif math.isinf(item):\n                if item > 0:\n                    item = 'Infinity'\n                else:\n                    item = '-Infinity'\n            obj_copy.append(item)\n        elif isinstance(item, (list, tuple)):  # check less common type second\n            obj_copy.append(traverse_data(item))\n        else:\n            obj_copy.append(item)\n    return obj_copy", "output": "Recursively traverse an object until a flat list is found.\n\n    If NumPy is available, the flat list is converted to a numpy array\n    and passed to transform_array() to handle ``nan``, ``inf``, and\n    ``-inf``.\n\n    Otherwise, iterate through all items, converting non-JSON items\n\n    Args:\n        obj (list) : a list of values or lists\n        use_numpy (bool, optional) toggle NumPy as a dependency for testing\n            This argument is only useful for testing (default: True)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vm_info(vm_=None):\n    '''\n    \n    '''\n    with _get_xapi_session() as xapi:\n\n        def _info(vm_):\n            vm_rec = _get_record_by_label(xapi, 'VM', vm_)\n            if vm_rec is False:\n                return False\n            vm_metrics_rec = _get_metrics_record(xapi, 'VM', vm_rec)\n\n            return {'cpu': vm_metrics_rec['VCPUs_number'],\n                    'maxCPU': _get_val(vm_rec, ['VCPUs_max']),\n                    'cputime': vm_metrics_rec['VCPUs_utilisation'],\n                    'disks': get_disks(vm_),\n                    'nics': get_nics(vm_),\n                    'maxMem': int(_get_val(vm_rec, ['memory_dynamic_max'])),\n                    'mem': int(vm_metrics_rec['memory_actual']),\n                    'state': _get_val(vm_rec, ['power_state'])\n                    }\n        info = {}\n        if vm_:\n            ret = _info(vm_)\n            if ret is not None:\n                info[vm_] = ret\n        else:\n            for vm_ in list_domains():\n                ret = _info(vm_)\n                if ret is not None:\n                    info[vm_] = _info(vm_)\n        return info", "output": "Return detailed information about the vms.\n\n    If you pass a VM name in as an argument then it will return info\n    for just the named VM, otherwise it will return all VMs.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' virt.vm_info", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode_bbox_target(boxes, anchors):\n    \"\"\"\n    \n    \"\"\"\n    anchors_x1y1x2y2 = tf.reshape(anchors, (-1, 2, 2))\n    anchors_x1y1, anchors_x2y2 = tf.split(anchors_x1y1x2y2, 2, axis=1)\n    waha = anchors_x2y2 - anchors_x1y1\n    xaya = (anchors_x2y2 + anchors_x1y1) * 0.5\n\n    boxes_x1y1x2y2 = tf.reshape(boxes, (-1, 2, 2))\n    boxes_x1y1, boxes_x2y2 = tf.split(boxes_x1y1x2y2, 2, axis=1)\n    wbhb = boxes_x2y2 - boxes_x1y1\n    xbyb = (boxes_x2y2 + boxes_x1y1) * 0.5\n\n    # Note that here not all boxes are valid. Some may be zero\n    txty = (xbyb - xaya) / waha\n    twth = tf.log(wbhb / waha)  # may contain -inf for invalid boxes\n    encoded = tf.concat([txty, twth], axis=1)  # (-1x2x2)\n    return tf.reshape(encoded, tf.shape(boxes))", "output": "Args:\n        boxes: (..., 4), float32\n        anchors: (..., 4), float32\n\n    Returns:\n        box_encoded: (..., 4), float32 with the same shape.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_dense(builder, layer, input_names, output_names, keras_layer):\n    \"\"\"\n    \n    \"\"\"\n    # Get input and output names\n    input_name, output_name = (input_names[0], output_names[0])\n\n    has_bias = keras_layer.use_bias\n    # Get the weights from keras\n    W = keras_layer.get_weights ()[0].T\n    Wb = keras_layer.get_weights ()[1].T if has_bias else None\n    output_channels, input_channels = W.shape\n\n    builder.add_inner_product(name = layer,\n            W = W,\n            b = Wb,\n            input_channels = input_channels,\n            output_channels = output_channels,\n            has_bias = has_bias,\n            input_name = input_name,\n            output_name = output_name)", "output": "Convert a dense layer from keras to coreml.\n\n    Parameters\n    keras_layer: layer\n    ----------\n        A keras layer object.\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clip_eta(eta, ord, eps):\n  \"\"\"\n  \n  \"\"\"\n\n  # Clipping perturbation eta to self.ord norm ball\n  if ord not in [np.inf, 1, 2]:\n    raise ValueError('ord must be np.inf, 1, or 2.')\n  axis = list(range(1, len(eta.get_shape())))\n  avoid_zero_div = 1e-12\n  if ord == np.inf:\n    eta = tf.clip_by_value(eta, -eps, eps)\n  else:\n    if ord == 1:\n      raise NotImplementedError(\"\")\n      # This is not the correct way to project on the L1 norm ball:\n      # norm = tf.maximum(avoid_zero_div, reduce_sum(tf.abs(eta), reduc_ind, keepdims=True))\n    elif ord == 2:\n      # avoid_zero_div must go inside sqrt to avoid a divide by zero in the gradient through this operation\n      norm = tf.sqrt(\n        tf.maximum(avoid_zero_div, tf.reduce_sum(tf.square(eta), axis, keepdims=True)))\n    # We must *clip* to within the norm ball, not *normalize* onto the surface of the ball\n    factor = tf.minimum(1., tf.math.divide(eps, norm))\n    eta = eta * factor\n  return eta", "output": "Helper function to clip the perturbation to epsilon norm ball.\n  :param eta: A tensor with the current perturbation.\n  :param ord: Order of the norm (mimics Numpy).\n              Possible values: np.inf, 1 or 2.\n  :param eps: Epsilon, bound of the perturbation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extract_code_globals(cls, co):\n        \"\"\"\n        \n        \"\"\"\n        out_names = cls._extract_code_globals_cache.get(co)\n        if out_names is None:\n            try:\n                names = co.co_names\n            except AttributeError:\n                # PyPy \"builtin-code\" object\n                out_names = set()\n            else:\n                out_names = set(names[oparg]\n                                for op, oparg in _walk_global_ops(co))\n\n                # see if nested function have any global refs\n                if co.co_consts:\n                    for const in co.co_consts:\n                        if type(const) is types.CodeType:\n                            out_names |= cls.extract_code_globals(const)\n\n            cls._extract_code_globals_cache[co] = out_names\n\n        return out_names", "output": "Find all globals names read or written to by codeblock co", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_get_extensionmarket_list(ip=None, port=None):\n    ''\n    ip, port = get_extensionmarket_ip(ip, port)\n    apix = TdxExHq_API()\n    with apix.connect(ip, port):\n        num = apix.get_instrument_count()\n        return pd.concat([apix.to_df(\n            apix.get_instrument_info((int(num / 500) - i) * 500, 500))\n            for i in range(int(num / 500) + 1)], axis=0).set_index('code', drop=False)", "output": "\u671f\u8d27\u4ee3\u7801list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_util_get_1st_of_next_month(dt):\n    \"\"\"\n    \n    \"\"\"\n    year = dt.year\n    month = dt.month\n    if month == 12:\n        month = 1\n        year += 1\n    else:\n        month += 1\n    res = datetime.datetime(year, month, 1)\n    return res", "output": "\u83b7\u53d6\u4e0b\u4e2a\u6708\u7b2c\u4e00\u5929\u7684\u65e5\u671f\n    :return: \u8fd4\u56de\u65e5\u671f", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ensure_annotations(dashboard):\n    ''''''\n    if 'annotation_tags' not in dashboard:\n        return\n    tags = dashboard['annotation_tags']\n    annotations = {\n        'enable': True,\n        'list': [],\n    }\n    for tag in tags:\n        annotations['list'].append({\n            'datasource': \"graphite\",\n            'enable': False,\n            'iconColor': \"#C0C6BE\",\n            'iconSize': 13,\n            'lineColor': \"rgba(255, 96, 96, 0.592157)\",\n            'name': tag,\n            'showLine': True,\n            'tags': tag,\n        })\n    del dashboard['annotation_tags']\n    dashboard['annotations'] = annotations", "output": "Explode annotation_tags into annotations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _replace_property(property_key, property_value, resource, logical_id):\n        \"\"\"\n        \n\n        \"\"\"\n        if property_key and property_value:\n            resource.get(PROPERTIES_KEY, {})[property_key] = property_value\n        elif property_key or property_value:\n            LOG.info(\"WARNING: Ignoring Metadata for Resource %s. Metadata contains only aws:asset:path or \"\n                     \"aws:assert:property but not both\", logical_id)", "output": "Replace a property with an asset on a given resource\n\n        This method will mutate the template\n\n        Parameters\n        ----------\n        property str\n            The property to replace on the resource\n        property_value str\n            The new value of the property\n        resource dict\n            Dictionary representing the Resource to change\n        logical_id str\n            LogicalId of the Resource", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _factorize_from_iterables(iterables):\n    \"\"\"\n    \n    \"\"\"\n    if len(iterables) == 0:\n        # For consistency, it should return a list of 2 lists.\n        return [[], []]\n    return map(list, lzip(*[_factorize_from_iterable(it) for it in iterables]))", "output": "A higher-level wrapper over `_factorize_from_iterable`.\n\n    *This is an internal function*\n\n    Parameters\n    ----------\n    iterables : list-like of list-likes\n\n    Returns\n    -------\n    codes_list : list of ndarrays\n    categories_list : list of Indexes\n\n    Notes\n    -----\n    See `_factorize_from_iterable` for more info.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_oshape_of_gather_nd_op(dshape, ishape):\n    \"\"\"\"\"\"\n    assert len(dshape) > 0 and len(ishape) > 0\n    oshape = list(ishape[1:])\n    if ishape[0] < len(dshape):\n        oshape.extend(dshape[ishape[0]:])\n    return tuple(oshape)", "output": "Given data and index shapes, get the output `NDArray` shape.\n    This basically implements the infer shape logic of op gather_nd.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _construct_target(self, function):\n        \"\"\"\n        \"\"\"\n        target = {\n                'Arn': function.get_runtime_attr(\"arn\"),\n                'Id': self.logical_id + 'LambdaTarget'\n        }\n        if self.Input is not None:\n            target['Input'] = self.Input\n\n        if self.InputPath is not None:\n            target['InputPath'] = self.InputPath\n        return target", "output": "Constructs the Target property for the CloudWatch Events Rule.\n\n        :returns: the Target property\n        :rtype: dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _purge_children(self):\n        \"\"\"\n        \n        \"\"\"\n        for task_id, p in six.iteritems(self._running_tasks):\n            if not p.is_alive() and p.exitcode:\n                error_msg = 'Task {} died unexpectedly with exit code {}'.format(task_id, p.exitcode)\n                p.task.trigger_event(Event.PROCESS_FAILURE, p.task, error_msg)\n            elif p.timeout_time is not None and time.time() > float(p.timeout_time) and p.is_alive():\n                p.terminate()\n                error_msg = 'Task {} timed out after {} seconds and was terminated.'.format(task_id, p.worker_timeout)\n                p.task.trigger_event(Event.TIMEOUT, p.task, error_msg)\n            else:\n                continue\n\n            logger.info(error_msg)\n            self._task_result_queue.put((task_id, FAILED, error_msg, [], []))", "output": "Find dead children and put a response on the result queue.\n\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rlmb_tiny_sv2p():\n  \"\"\"\"\"\"\n  hparams = rlmb_ppo_tiny()\n  hparams.generative_model = \"next_frame_sv2p\"\n  hparams.generative_model_params = \"next_frame_sv2p_tiny\"\n  hparams.grayscale = False\n  return hparams", "output": "Tiny setting with a tiny sv2p model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_to(maybe_device, convert_to):\n    '''\n    \n\n    '''\n\n    # Fast path. If we already have the information required, we can\n    # save one blkid call\n    if not convert_to or \\\n       (convert_to == 'device' and maybe_device.startswith('/')) or \\\n       maybe_device.startswith('{}='.format(convert_to.upper())):\n        return maybe_device\n\n    # Get the device information\n    if maybe_device.startswith('/'):\n        blkid = __salt__['disk.blkid'](maybe_device)\n    else:\n        blkid = __salt__['disk.blkid'](token=maybe_device)\n\n    result = None\n    if len(blkid) == 1:\n        if convert_to == 'device':\n            result = list(blkid.keys())[0]\n        else:\n            key = convert_to.upper()\n            result = '{}={}'.format(key, list(blkid.values())[0][key])\n\n    return result", "output": "Convert a device name, UUID or LABEL to a device name, UUID or\n    LABEL.\n\n    Return the fs_spec required for fstab.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_script_with_context(script_path, cwd, context):\n    \"\"\"\n    \"\"\"\n    _, extension = os.path.splitext(script_path)\n\n    contents = io.open(script_path, 'r', encoding='utf-8').read()\n\n    with tempfile.NamedTemporaryFile(\n        delete=False,\n        mode='wb',\n        suffix=extension\n    ) as temp:\n        env = StrictEnvironment(\n            context=context,\n            keep_trailing_newline=True,\n        )\n        template = env.from_string(contents)\n        output = template.render(**context)\n        temp.write(output.encode('utf-8'))\n\n    run_script(temp.name, cwd)", "output": "Execute a script after rendering it with Jinja.\n\n    :param script_path: Absolute path to the script to run.\n    :param cwd: The directory to run the script from.\n    :param context: Cookiecutter project template context.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_int(d, key, default_to_zero=False, default=None, required=True):\n    \"\"\"\n\n    \"\"\"\n    value = d.get(key) or default\n    if (value in [\"\", None]) and default_to_zero:\n        return 0\n    if value is None:\n        if required:\n            raise ParseError(\"Unable to read %s from %s\" % (key, d))\n    else:\n        return int(value)", "output": "Pull a value from the dict and convert to int\n\n    :param default_to_zero: If the value is None or empty, treat it as zero\n    :param default: If the value is missing in the dict use this default", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def finish_target(self, name):\n        \"\"\"\"\"\"\n        # We have to write a msg about finished target\n        with self._lock:\n            pbar = self._bar(name, 100, 100)\n\n            if sys.stdout.isatty():\n                self.clearln()\n\n            self._print(pbar)\n\n            self._n_finished += 1\n            self._line = None", "output": "Finishes progress bar for a specified target.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show(thing, domain=(0, 1), **kwargs):\n  \"\"\"\n  \"\"\"\n  if isinstance(thing, np.ndarray):\n    rank = len(thing.shape)\n    if rank == 4:\n      log.debug(\"Show is assuming rank 4 tensor to be a list of images.\")\n      images(thing, domain=domain, **kwargs)\n    elif rank in (2, 3):\n      log.debug(\"Show is assuming rank 2 or 3 tensor to be an image.\")\n      image(thing, domain=domain, **kwargs)\n    else:\n      log.warning(\"Show only supports numpy arrays of rank 2-4. Using repr().\")\n      print(repr(thing))\n  elif isinstance(thing, (list, tuple)):\n    log.debug(\"Show is assuming list or tuple to be a collection of images.\")\n    images(thing, domain=domain, **kwargs)\n  else:\n    log.warning(\"Show only supports numpy arrays so far. Using repr().\")\n    print(repr(thing))", "output": "Display a nupmy array without having to specify what it represents.\n\n  This module will attempt to infer how to display your tensor based on its\n  rank, shape and dtype. rank 4 tensors will be displayed as image grids, rank\n  2 and 3 tensors as images.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def grid(metadata, layout, params):\n  \"\"\"\n  \n  \"\"\"\n  x = layout[\"x\"]\n  y = layout[\"y\"]\n  x_min = np.min(x)\n  x_max = np.max(x)\n  y_min = np.min(y)\n  y_max = np.max(y)\n\n  # this creates the grid\n  bins = np.linspace(x_min, x_max, params[\"n_layer\"] - 1)\n  xd = np.digitize(x, bins)\n  bins = np.linspace(y_min, y_max, params[\"n_layer\"] - 1)\n  yd = np.digitize(y, bins)\n\n  # the number of tiles is the number of cells divided by the number of cells in each tile\n  num_tiles = int(params[\"n_layer\"]/params[\"n_tile\"])\n  print(\"num tiles\", num_tiles)\n  # we will save the tiles in an array indexed by the tile coordinates\n  tiles = {}\n  for ti in range(num_tiles):\n    for tj in range(num_tiles):\n      tiles[(ti,tj)] = {\n        \"x\": [],\n        \"y\": [],\n        \"ci\": [], # cell-space x coordinate\n        \"cj\": [], # cell-space y coordinate\n        \"gi\": [], # global index\n      }\n\n  for i,xi in enumerate(x):\n    if(i % 1000 == 0 or i+1 == len(x)):\n      print(\"point\", i+1, \"/\", len(x), end=\"\\r\")\n    # layout-space coordinates\n    yi = y[i]\n    # grid-space cell coordinates\n    ci = xd[i]\n    cj = yd[i]\n    # tile coordinate\n    ti = math.floor(ci / params[\"n_tile\"])\n    tj = math.floor(cj / params[\"n_tile\"])\n\n    # TODO: don't append a point if it doesn't match a filter function provided in params\n    filter = params.get(\"filter\", lambda i,metadata: True)\n    if(filter(i, metadata=metadata)):\n      tiles[(ti,tj)][\"x\"].append(xi)\n      tiles[(ti,tj)][\"y\"].append(yi)\n      tiles[(ti,tj)][\"ci\"].append(ci)\n      tiles[(ti,tj)][\"cj\"].append(cj)\n      tiles[(ti,tj)][\"gi\"].append(i)\n    \n  return tiles", "output": "layout: numpy arrays x, y\n  metadata: user-defined numpy arrays with metadata\n  n_layer: number of cells in the layer (squared)\n  n_tile: number of cells in the tile (squared)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def visualize_instance_html(self, exp, label, div_name, exp_object_name,\n                                text=True, opacity=True):\n        \"\"\"\n        \"\"\"\n        if not text:\n            return u''\n        text = (self.indexed_string.raw_string()\n                .encode('utf-8', 'xmlcharrefreplace').decode('utf-8'))\n        text = re.sub(r'[<>&]', '|', text)\n        exp = [(self.indexed_string.word(x[0]),\n                self.indexed_string.string_position(x[0]),\n                x[1]) for x in exp]\n        all_occurrences = list(itertools.chain.from_iterable(\n            [itertools.product([x[0]], x[1], [x[2]]) for x in exp]))\n        all_occurrences = [(x[0], int(x[1]), x[2]) for x in all_occurrences]\n        ret = '''\n            %s.show_raw_text(%s, %d, %s, %s, %s);\n            ''' % (exp_object_name, json.dumps(all_occurrences), label,\n                   json.dumps(text), div_name, json.dumps(opacity))\n        return ret", "output": "Adds text with highlighted words to visualization.\n\n        Args:\n             exp: list of tuples [(id, weight), (id,weight)]\n             label: label id (integer)\n             div_name: name of div object to be used for rendering(in js)\n             exp_object_name: name of js explanation object\n             text: if False, return empty\n             opacity: if True, fade colors according to weight", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_user_sign_in(username, password):\n    \"\"\"\n    \"\"\"\n    #user = QA_User(name= name, password=password)\n    cursor = DATABASE.user.find_one(\n        {'username': username, 'password': password})\n    if cursor is None:\n        QA_util_log_info('SOMETHING WRONG')\n        return False\n    else:\n        return True", "output": "\u7528\u6237\u767b\u9646\n    \u4e0d\u4f7f\u7528 QAUSER\u5e93\n    \u53ea\u8fd4\u56de TRUE/FALSE", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_atom(self, block, block_items, existing_col, min_itemsize,\n                 nan_rep, info, encoding=None, errors='strict'):\n        \"\"\"  \"\"\"\n\n        self.values = list(block_items)\n\n        # short-cut certain block types\n        if block.is_categorical:\n            return self.set_atom_categorical(block, items=block_items,\n                                             info=info)\n        elif block.is_datetimetz:\n            return self.set_atom_datetime64tz(block, info=info)\n        elif block.is_datetime:\n            return self.set_atom_datetime64(block)\n        elif block.is_timedelta:\n            return self.set_atom_timedelta64(block)\n        elif block.is_complex:\n            return self.set_atom_complex(block)\n\n        dtype = block.dtype.name\n        inferred_type = lib.infer_dtype(block.values, skipna=False)\n\n        if inferred_type == 'date':\n            raise TypeError(\n                \"[date] is not implemented as a table column\")\n        elif inferred_type == 'datetime':\n            # after 8260\n            # this only would be hit for a mutli-timezone dtype\n            # which is an error\n\n            raise TypeError(\n                \"too many timezones in this block, create separate \"\n                \"data columns\"\n            )\n        elif inferred_type == 'unicode':\n            raise TypeError(\n                \"[unicode] is not implemented as a table column\")\n\n        # this is basically a catchall; if say a datetime64 has nans then will\n        # end up here ###\n        elif inferred_type == 'string' or dtype == 'object':\n            self.set_atom_string(\n                block, block_items,\n                existing_col,\n                min_itemsize,\n                nan_rep,\n                encoding,\n                errors)\n\n        # set as a data block\n        else:\n            self.set_atom_data(block)", "output": "create and setup my atom from the block b", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def usermacro_updateglobal(globalmacroid, value, **kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            params = {}\n            method = 'usermacro.updateglobal'\n            params['globalmacroid'] = globalmacroid\n            params['value'] = value\n            params = _params_extend(params, _ignore_name=True, **kwargs)\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result']['globalmacroids'][0]\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": "Update existing global usermacro.\n\n    :param globalmacroid: id of the host usermacro\n    :param value: new value of the host usermacro\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    return: ID of the update global usermacro.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zabbix.usermacro_updateglobal 1 'public'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def authenticate_redirect(\n        self,\n        callback_uri: str = None,\n        ax_attrs: List[str] = [\"name\", \"email\", \"language\", \"username\"],\n    ) -> None:\n        \"\"\"\n        \"\"\"\n        handler = cast(RequestHandler, self)\n        callback_uri = callback_uri or handler.request.uri\n        assert callback_uri is not None\n        args = self._openid_args(callback_uri, ax_attrs=ax_attrs)\n        endpoint = self._OPENID_ENDPOINT  # type: ignore\n        handler.redirect(endpoint + \"?\" + urllib.parse.urlencode(args))", "output": "Redirects to the authentication URL for this service.\n\n        After authentication, the service will redirect back to the given\n        callback URI with additional parameters including ``openid.mode``.\n\n        We request the given attributes for the authenticated user by\n        default (name, email, language, and username). If you don't need\n        all those attributes for your app, you can request fewer with\n        the ax_attrs keyword argument.\n\n        .. versionchanged:: 6.0\n\n            The ``callback`` argument was removed and this method no\n            longer returns an awaitable object. It is now an ordinary\n            synchronous function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_font(self):\r\n        \"\"\"\"\"\"\r\n        font = self.get_plugin_font()\r\n        color_scheme = self.get_color_scheme()\r\n        for editorstack in self.editorstacks:\r\n            editorstack.set_default_font(font, color_scheme)\r\n            completion_size = CONF.get('main', 'completion/size')\r\n            for finfo in editorstack.data:\r\n                comp_widget = finfo.editor.completion_widget\r\n                comp_widget.setup_appearance(completion_size, font)", "output": "Update font from Preferences", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def memoized_func(key=view_cache_key, attribute_in_key=None):\n    \"\"\"\n    \"\"\"\n    def wrap(f):\n        if tables_cache:\n            def wrapped_f(self, *args, **kwargs):\n                if not kwargs.get('cache', True):\n                    return f(self, *args, **kwargs)\n\n                if attribute_in_key:\n                    cache_key = key(*args, **kwargs).format(\n                        getattr(self, attribute_in_key))\n                else:\n                    cache_key = key(*args, **kwargs)\n                o = tables_cache.get(cache_key)\n                if not kwargs.get('force') and o is not None:\n                    return o\n                o = f(self, *args, **kwargs)\n                tables_cache.set(cache_key, o,\n                                 timeout=kwargs.get('cache_timeout'))\n                return o\n        else:\n            # noop\n            def wrapped_f(self, *args, **kwargs):\n                return f(self, *args, **kwargs)\n        return wrapped_f\n    return wrap", "output": "Use this decorator to cache functions that have predefined first arg.\n\n    enable_cache is treated as True by default,\n    except enable_cache = False is passed to the decorated function.\n\n    force means whether to force refresh the cache and is treated as False by default,\n    except force = True is passed to the decorated function.\n\n    timeout of cache is set to 600 seconds by default,\n    except cache_timeout = {timeout in seconds} is passed to the decorated function.\n\n    memoized_func uses simple_cache and stored the data in memory.\n    Key is a callable function that takes function arguments and\n    returns the caching key.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flatten(items,enter=lambda x:isinstance(x, list)):\n    # http://stackoverflow.com/a/40857703\n    # https://github.com/ctmakro/canton/blob/master/canton/misc.py\n    \"\"\"\"\"\"\n    for x in items:\n        if enter(x):\n            yield from flatten(x)\n        else:\n            yield x", "output": "Yield items from any nested iterable; see REF.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _prepare_reserved_tokens(reserved_tokens):\n  \"\"\"\"\"\"\n  reserved_tokens = [tf.compat.as_text(tok) for tok in reserved_tokens or []]\n  dups = _find_duplicates(reserved_tokens)\n  if dups:\n    raise ValueError(\"Duplicates found in tokens: %s\" % dups)\n  reserved_tokens_re = _make_reserved_tokens_re(reserved_tokens)\n  return reserved_tokens, reserved_tokens_re", "output": "Prepare reserved tokens and a regex for splitting them out of strings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absent(name, ip):  # pylint: disable=C0103\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': None,\n           'comment': ''}\n\n    if not isinstance(ip, list):\n        ip = [ip]\n\n    comments = []\n    for _ip in ip:\n        if not __salt__['hosts.has_pair'](_ip, name):\n            ret['result'] = True\n            comments.append('Host {0} ({1}) already absent'.format(name, _ip))\n        else:\n            if __opts__['test']:\n                comments.append('Host {0} ({1}) needs to be removed'.format(name, _ip))\n            else:\n                if __salt__['hosts.rm_host'](_ip, name):\n                    ret['changes'] = {'host': name}\n                    ret['result'] = True\n                    comments.append('Removed host {0} ({1})'.format(name, _ip))\n                else:\n                    ret['result'] = False\n                    comments.append('Failed to remove host')\n    ret['comment'] = '\\n'.join(comments)\n    return ret", "output": "Ensure that the named host is absent\n\n    name\n        The host to remove\n\n    ip\n        The ip addr(s) of the host to remove", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_page(pno, zoom = False, max_size = None, first = False):\n    \"\"\"\n    \"\"\"\n    dlist = dlist_tab[pno]   # get display list of page number\n    if not dlist:            # create if not yet there\n        dlist_tab[pno] = doc[pno].getDisplayList()\n        dlist = dlist_tab[pno]\n    r = dlist.rect           # the page rectangle\n    clip = r\n    # ensure image fits screen:\n    # exploit, but do not exceed width or height\n    zoom_0 = 1\n    if max_size:\n        zoom_0 = min(1, max_size[0] / r.width, max_size[1] / r.height)\n        if zoom_0 == 1:\n            zoom_0 = min(max_size[0] / r.width, max_size[1] / r.height)\n    mat_0 = fitz.Matrix(zoom_0, zoom_0)\n\n    if not zoom:             # show total page\n        pix = dlist.getPixmap(matrix = mat_0, alpha=False)\n    else:\n        mp = r.tl + (r.br - r.tl) * 0.5     # page rect center\n        w2 = r.width / 2\n        h2 = r.height / 2\n        clip = r * 0.5\n        tl = zoom[0]          # old top-left\n        tl.x += zoom[1] * (w2 / 2)\n        tl.x = max(0, tl.x)\n        tl.x = min(w2, tl.x)\n        tl.y += zoom[2] * (h2 / 2)\n        tl.y = max(0, tl.y)\n        tl.y = min(h2, tl.y)\n        clip = fitz.Rect(tl, tl.x + w2, tl.y + h2)\n\n        mat = mat_0 * fitz.Matrix(2, 2)      # zoom matrix\n        pix = dlist.getPixmap(alpha=False, matrix=mat, clip=clip)\n\n    if first:                     # first call: tkinter still inactive\n        img = pix.getPNGData()    # so use fitz png output\n    else:                         # else take tk photo image\n        pilimg = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n        img = ImageTk.PhotoImage(pilimg)\n\n    return img, clip.tl", "output": "Return a PNG image for a document page number.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def split_and_operate(self, mask, f, inplace):\n        \"\"\"\n        \n        \"\"\"\n\n        if mask is None:\n            mask = np.ones(self.shape, dtype=bool)\n        new_values = self.values\n\n        def make_a_block(nv, ref_loc):\n            if isinstance(nv, Block):\n                block = nv\n            elif isinstance(nv, list):\n                block = nv[0]\n            else:\n                # Put back the dimension that was taken from it and make\n                # a block out of the result.\n                try:\n                    nv = _block_shape(nv, ndim=self.ndim)\n                except (AttributeError, NotImplementedError):\n                    pass\n                block = self.make_block(values=nv,\n                                        placement=ref_loc)\n            return block\n\n        # ndim == 1\n        if self.ndim == 1:\n            if mask.any():\n                nv = f(mask, new_values, None)\n            else:\n                nv = new_values if inplace else new_values.copy()\n            block = make_a_block(nv, self.mgr_locs)\n            return [block]\n\n        # ndim > 1\n        new_blocks = []\n        for i, ref_loc in enumerate(self.mgr_locs):\n            m = mask[i]\n            v = new_values[i]\n\n            # need a new block\n            if m.any():\n                nv = f(m, v, i)\n            else:\n                nv = v if inplace else v.copy()\n\n            block = make_a_block(nv, [ref_loc])\n            new_blocks.append(block)\n\n        return new_blocks", "output": "split the block per-column, and apply the callable f\n        per-column, return a new block for each. Handle\n        masking which will not change a block unless needed.\n\n        Parameters\n        ----------\n        mask : 2-d boolean mask\n        f : callable accepting (1d-mask, 1d values, indexer)\n        inplace : boolean\n\n        Returns\n        -------\n        list of blocks", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_ctx(self, subctx=None):\n        \"\"\n        l = []\n        if self.ctx is not None:      l.append(self.ctx)\n        if subctx is not None:        l.append(subctx)\n        return '' if len(l) == 0 else f\" ({': '.join(l)})\"", "output": "Return ' (ctx: subctx)' or ' (ctx)' or ' (subctx)' or '' depending on this and constructor arguments", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def serialize(self, x):\n        \"\"\"\n        \n        \"\"\"\n        weeks = x.days // 7\n        days = x.days % 7\n        hours = x.seconds // 3600\n        minutes = (x.seconds % 3600) // 60\n        seconds = (x.seconds % 3600) % 60\n        result = \"{} w {} d {} h {} m {} s\".format(weeks, days, hours, minutes, seconds)\n        return result", "output": "Converts datetime.timedelta to a string\n\n        :param x: the value to serialize.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def embed_sentence(self, sentence: List[str]) -> numpy.ndarray:\n        \"\"\"\n        \n        \"\"\"\n\n        return self.embed_batch([sentence])[0]", "output": "Computes the ELMo embeddings for a single tokenized sentence.\n\n        Please note that ELMo has internal state and will give different results for the same input.\n        See the comment under the class definition.\n\n        Parameters\n        ----------\n        sentence : ``List[str]``, required\n            A tokenized sentence.\n\n        Returns\n        -------\n        A tensor containing the ELMo vectors.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_array(tensor):  # type: (TensorProto) -> np.ndarray[Any]\n    \"\"\"\n    \"\"\"\n    if tensor.HasField(\"segment\"):\n        raise ValueError(\n            \"Currently not supporting loading segments.\")\n    if tensor.data_type == TensorProto.UNDEFINED:\n        raise ValueError(\"The data type is not defined.\")\n\n    tensor_dtype = tensor.data_type\n    np_dtype = mapping.TENSOR_TYPE_TO_NP_TYPE[tensor_dtype]\n    storage_type = mapping.TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE[tensor_dtype]\n    storage_np_dtype = mapping.TENSOR_TYPE_TO_NP_TYPE[storage_type]\n    storage_field = mapping.STORAGE_TENSOR_TYPE_TO_FIELD[storage_type]\n    dims = tensor.dims\n\n    if tensor.data_type == TensorProto.STRING:\n        utf8_strings = getattr(tensor, storage_field)\n        ss = list(s.decode('utf-8') for s in utf8_strings)\n        return np.asarray(ss).astype(np_dtype).reshape(dims)\n\n    if tensor.HasField(\"raw_data\"):\n        # Raw_bytes support: using frombuffer.\n        return np.frombuffer(\n            tensor.raw_data,\n            dtype=np_dtype).reshape(dims)\n    else:\n        data = getattr(tensor, storage_field),  # type: Sequence[np.complex64]\n        if (tensor_dtype == TensorProto.COMPLEX64\n                or tensor_dtype == TensorProto.COMPLEX128):\n            data = combine_pairs_to_complex(data)\n        return (\n            np.asarray(\n                data,\n                dtype=storage_np_dtype)\n            .astype(np_dtype)\n            .reshape(dims)\n        )", "output": "Converts a tensor def object to a numpy array.\n\n    Inputs:\n        tensor: a TensorProto object.\n    Returns:\n        arr: the converted array.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def default_dataset(self):\n        \"\"\"\n        \"\"\"\n        prop = self._get_sub_prop(\"defaultDataset\")\n        if prop is not None:\n            prop = DatasetReference.from_api_repr(prop)\n        return prop", "output": "google.cloud.bigquery.dataset.DatasetReference: the default dataset\n        to use for unqualified table names in the query or :data:`None` if not\n        set.\n\n        The ``default_dataset`` setter accepts:\n\n        - a :class:`~google.cloud.bigquery.dataset.Dataset`, or\n        - a :class:`~google.cloud.bigquery.dataset.DatasetReference`, or\n        - a :class:`str` of the fully-qualified dataset ID in standard SQL\n          format. The value must included a project ID and dataset ID\n          separated by ``.``. For example: ``your-project.your_dataset``.\n\n        See\n        https://g.co/cloud/bigquery/docs/reference/v2/jobs#configuration.query.defaultDataset", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _tensor_proto_to_health_pill(self, tensor_event, node_name, device,\n                                   output_slot):\n    \"\"\"\n    \"\"\"\n    return self._process_health_pill_value(\n        wall_time=tensor_event.wall_time,\n        step=tensor_event.step,\n        device_name=device,\n        output_slot=output_slot,\n        node_name=node_name,\n        tensor_proto=tensor_event.tensor_proto)", "output": "Converts an event_accumulator.TensorEvent to a HealthPillEvent.\n\n    Args:\n      tensor_event: The event_accumulator.TensorEvent to convert.\n      node_name: The name of the node (without the output slot).\n      device: The device.\n      output_slot: The integer output slot this health pill is relevant to.\n\n    Returns:\n      A HealthPillEvent.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chshell(name, shell):\n    '''\n    \n    '''\n    pre_info = info(name)\n    if not pre_info:\n        raise CommandExecutionError(\n            'User \\'{0}\\' does not exist'.format(name)\n        )\n    if shell == pre_info['shell']:\n        return True\n    cmd = ['usermod', '-s', shell, name]\n    __salt__['cmd.run'](cmd, python_shell=False)\n    return info(name).get('shell') == shell", "output": "Change the default shell of the user\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.chshell foo /bin/zsh", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_input_data(filename):\n    \"\"\"\"\"\"\n    logging.info('Opening file %s for reading input', filename)\n    input_file = open(filename, 'r')\n    data = []\n    labels = []\n    for line in input_file:\n        tokens = line.split(',', 1)\n        labels.append(tokens[0].strip())\n        data.append(tokens[1].strip())\n    return labels, data", "output": "Helper function to get training data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_cert_format(name):\n    '''\n    \n    '''\n    cert_formats = ['cer', 'pfx']\n\n    if name not in cert_formats:\n        message = (\"Invalid certificate format '{0}' specified. Valid formats:\"\n                   ' {1}').format(name, cert_formats)\n        raise SaltInvocationError(message)", "output": "Ensure that the certificate format, as determind from user input, is valid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def complete_handshake(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.__timer:\n            self.__timer.cancel()\n            self.__timer_expired = False\n            self.__handshake_complete = True", "output": "Tells `Packetizer` that the handshake has completed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(self, ignore_warnings=None):\n        \"\"\"\n        \"\"\"\n        return self.from_pb(\n            self.instance_admin_client.create_app_profile(\n                parent=self._instance.name,\n                app_profile_id=self.app_profile_id,\n                app_profile=self._to_pb(),\n                ignore_warnings=ignore_warnings,\n            ),\n            self._instance,\n        )", "output": "Create this AppProfile.\n\n        .. note::\n\n            Uses the ``instance`` and ``app_profile_id`` on the current\n            :class:`AppProfile` in addition to the ``routing_policy_type``,\n            ``description``, ``cluster_id`` and ``allow_transactional_writes``.\n            To change them before creating, reset the values via\n\n            .. code:: python\n\n                app_profile.app_profile_id = 'i-changed-my-mind'\n                app_profile.routing_policy_type = (\n                    google.cloud.bigtable.enums.RoutingPolicyType.SINGLE\n                )\n                app_profile.description = 'new-description'\n                app-profile.cluster_id = 'other-cluster-id'\n                app-profile.allow_transactional_writes = True\n\n            before calling :meth:`create`.\n\n        :type: ignore_warnings: bool\n        :param: ignore_warnings: (Optional) If true, ignore safety checks when\n                                 creating the AppProfile.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_tcpip_interface_info(interface):\n    '''\n    \n    '''\n    base_information = _get_base_interface_info(interface)\n    if base_information['ipv4']['requestmode'] == 'static':\n        settings = _load_config(interface.name, ['IP_Address', 'Subnet_Mask', 'Gateway', 'DNS_Address'])\n        base_information['ipv4']['address'] = settings['IP_Address']\n        base_information['ipv4']['netmask'] = settings['Subnet_Mask']\n        base_information['ipv4']['gateway'] = settings['Gateway']\n        base_information['ipv4']['dns'] = [settings['DNS_Address']]\n    elif base_information['up']:\n        base_information['ipv4']['address'] = interface.sockaddrToStr(interface.addr)\n        base_information['ipv4']['netmask'] = interface.sockaddrToStr(interface.netmask)\n        base_information['ipv4']['gateway'] = '0.0.0.0'\n        base_information['ipv4']['dns'] = _get_dns_info()\n        with salt.utils.files.fopen('/proc/net/route', 'r') as route_file:\n            pattern = re.compile(r'^{interface}\\t[0]{{8}}\\t([0-9A-Z]{{8}})'.format(interface=interface.name),\n                                 re.MULTILINE)\n            match = pattern.search(route_file.read())\n            iface_gateway_hex = None if not match else match.group(1)\n        if iface_gateway_hex is not None and len(iface_gateway_hex) == 8:\n            base_information['ipv4']['gateway'] = '.'.join([str(int(iface_gateway_hex[i:i + 2], 16))\n                                                            for i in range(6, -1, -2)])\n    return base_information", "output": "return details about given tcpip interface", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_symbol_lookup_date(self, dt):\n        \"\"\"\n        \"\"\"\n        try:\n            self._symbol_lookup_date = pd.Timestamp(dt, tz='UTC')\n        except ValueError:\n            raise UnsupportedDatetimeFormat(input=dt,\n                                            method='set_symbol_lookup_date')", "output": "Set the date for which symbols will be resolved to their assets\n        (symbols may map to different firms or underlying assets at\n        different times)\n\n        Parameters\n        ----------\n        dt : datetime\n            The new symbol lookup date.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_instance(name, conn=None, call=None):\n    '''\n    \n\n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The show_instance action must be called with -a or --action.'\n        )\n    if conn is None:\n        conn = get_conn()\n\n    node = conn.get_server(name, bare=True)\n    ret = dict(node)\n    ret['id'] = node.id\n    ret['name'] = node.name\n    ret['size'] = conn.get_flavor(node.flavor.id).name\n    ret['state'] = node.status\n    ret['private_ips'] = _get_ips(node, 'private')\n    ret['public_ips'] = _get_ips(node, 'public')\n    ret['floating_ips'] = _get_ips(node, 'floating')\n    ret['fixed_ips'] = _get_ips(node, 'fixed')\n    if isinstance(node.image, six.string_types):\n        ret['image'] = node.image\n    else:\n        ret['image'] = conn.get_image(node.image.id).name\n    return ret", "output": "Get VM on this OpenStack account\n\n    name\n\n        name of the instance\n\n    CLI Example\n\n    .. code-block:: bash\n\n        salt-cloud -a show_instance myserver", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def moveaxis(tensor, source, destination):\n    \"\"\"\n    \"\"\"\n    try:\n        source = np.core.numeric.normalize_axis_tuple(source, tensor.ndim)\n    except IndexError:\n        raise ValueError('Source should verify 0 <= source < tensor.ndim'\n                         'Got %d' % source)\n    try:\n        destination = np.core.numeric.normalize_axis_tuple(destination, tensor.ndim)\n    except IndexError:\n        raise ValueError('Destination should verify 0 <= destination < tensor.ndim (%d).'\n                         % tensor.ndim, 'Got %d' % destination)\n\n    if len(source) != len(destination):\n        raise ValueError('`source` and `destination` arguments must have '\n                         'the same number of elements')\n\n    order = [n for n in range(tensor.ndim) if n not in source]\n\n    for dest, src in sorted(zip(destination, source)):\n        order.insert(dest, src)\n\n    return op.transpose(tensor, order)", "output": "Moves the `source` axis into the `destination` position\n    while leaving the other axes in their original order\n\n    Parameters\n    ----------\n    tensor : mx.nd.array\n        The array which axes should be reordered\n    source : int or sequence of int\n        Original position of the axes to move. Can be negative but must be unique.\n    destination : int or sequence of int\n        Destination position for each of the original axes. Can be negative but must be unique.\n\n    Returns\n    -------\n    result : mx.nd.array\n        Array with moved axes.\n\n    Examples\n    --------\n    >>> X = mx.nd.array([[1, 2, 3], [4, 5, 6]])\n    >>> mx.nd.moveaxis(X, 0, 1).shape\n    (3L, 2L)\n\n    >>> X = mx.nd.zeros((3, 4, 5))\n    >>> mx.nd.moveaxis(X, [0, 1], [-1, -2]).shape\n    (5, 4, 3)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_fillna_kwargs(value, method, validate_scalar_dict_value=True):\n    \"\"\"\n    \"\"\"\n    from pandas.core.missing import clean_fill_method\n\n    if value is None and method is None:\n        raise ValueError(\"Must specify a fill 'value' or 'method'.\")\n    elif value is None and method is not None:\n        method = clean_fill_method(method)\n\n    elif value is not None and method is None:\n        if validate_scalar_dict_value and isinstance(value, (list, tuple)):\n            raise TypeError('\"value\" parameter must be a scalar or dict, but '\n                            'you passed a \"{0}\"'.format(type(value).__name__))\n\n    elif value is not None and method is not None:\n        raise ValueError(\"Cannot specify both 'value' and 'method'.\")\n\n    return value, method", "output": "Validate the keyword arguments to 'fillna'.\n\n    This checks that exactly one of 'value' and 'method' is specified.\n    If 'method' is specified, this validates that it's a valid method.\n\n    Parameters\n    ----------\n    value, method : object\n        The 'value' and 'method' keyword arguments for 'fillna'.\n    validate_scalar_dict_value : bool, default True\n        Whether to validate that 'value' is a scalar or dict. Specifically,\n        validate that it is not a list or tuple.\n\n    Returns\n    -------\n    value, method : object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_pending_reboot():\n    '''\n    \n    '''\n\n    # Order the checks for reboot pending in most to least likely.\n    checks = (get_pending_update,\n              get_pending_file_rename,\n              get_pending_servermanager,\n              get_pending_component_servicing,\n              get_reboot_required_witnessed,\n              get_pending_computer_name,\n              get_pending_domain_join)\n\n    for check in checks:\n        if check():\n            return True\n\n    return False", "output": "Determine whether there is a reboot pending.\n\n    .. versionadded:: 2016.11.0\n\n    Returns:\n        bool: ``True`` if the system is pending reboot, otherwise ``False``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' system.get_pending_reboot", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parquet(self, path, mode=None, partitionBy=None, compression=None):\n        \"\"\"\n        \"\"\"\n        self.mode(mode)\n        if partitionBy is not None:\n            self.partitionBy(partitionBy)\n        self._set_opts(compression=compression)\n        self._jwrite.parquet(path)", "output": "Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n\n        :param path: the path in any Hadoop supported file system\n        :param mode: specifies the behavior of the save operation when data already exists.\n\n            * ``append``: Append contents of this :class:`DataFrame` to existing data.\n            * ``overwrite``: Overwrite existing data.\n            * ``ignore``: Silently ignore this operation if data already exists.\n            * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n                exists.\n        :param partitionBy: names of partitioning columns\n        :param compression: compression codec to use when saving to file. This can be one of the\n                            known case-insensitive shorten names (none, uncompressed, snappy, gzip,\n                            lzo, brotli, lz4, and zstd). This will override\n                            ``spark.sql.parquet.compression.codec``. If None is set, it uses the\n                            value specified in ``spark.sql.parquet.compression.codec``.\n\n        >>> df.write.parquet(os.path.join(tempfile.mkdtemp(), 'data'))", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_proxy_conf(proxyfile):\n    '''\n    \n    '''\n    msg = 'Invalid value for proxy file provided!, Supplied value = {0}' \\\n        .format(proxyfile)\n\n    log.trace('Salt Proxy Module: write proxy conf')\n\n    if proxyfile:\n        log.debug('Writing proxy conf file')\n        with salt.utils.files.fopen(proxyfile, 'w') as proxy_conf:\n            proxy_conf.write(salt.utils.stringutils.to_str('master = {0}'\n                             .format(__grains__['master'])))\n        msg = 'Wrote proxy file {0}'.format(proxyfile)\n        log.debug(msg)\n\n    return msg", "output": "write to file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def IntersectPath(self, path, intersection):\n    \"\"\"\n    \"\"\"\n    node = self._root\n    for name in path.split('.'):\n      if name not in node:\n        return\n      elif not node[name]:\n        intersection.AddPath(path)\n        return\n      node = node[name]\n    intersection.AddLeafNodes(path, node)", "output": "Calculates the intersection part of a field path with this tree.\n\n    Args:\n      path: The field path to calculates.\n      intersection: The out tree to record the intersection part.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def info(self, node_id=None, metric=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_nodes\", node_id, metric), params=params\n        )", "output": "The cluster nodes info API allows to retrieve one or more (or all) of\n        the cluster nodes information.\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-info.html>`_\n\n        :arg node_id: A comma-separated list of node IDs or names to limit the\n            returned information; use `_local` to return information from the\n            node you're connecting to, leave empty to get information from all\n            nodes\n        :arg metric: A comma-separated list of metrics you wish returned. Leave\n            empty to return all.\n        :arg flat_settings: Return settings in flat format (default: false)\n        :arg timeout: Explicit operation timeout", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(name, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The show_instance action must be called with -a or --action.'\n        )\n\n    data = query(action='ve', command='{0}/start'.format(name), method='PUT')\n\n    if 'error' in data:\n        return data['error']\n\n    return {'Started': '{0} was started.'.format(name)}", "output": "Start a node.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a start mymachine", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _job_statistics(self):\n        \"\"\"\"\"\"\n        statistics = self._properties.get(\"statistics\", {})\n        return statistics.get(self._JOB_TYPE, {})", "output": "Helper for job-type specific statistics-based properties.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def standard_exc_info(self):\n        \"\"\"\"\"\"\n        tb = self.frames[0]\n        # the frame will be an actual traceback (or transparent proxy) if\n        # we are on pypy or a python implementation with support for tproxy\n        if type(tb) is not TracebackType:\n            tb = tb.tb\n        return self.exc_type, self.exc_value, tb", "output": "Standard python exc_info for re-raising", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uninitialize(cls) -> None:\n        \"\"\"\"\"\"\n        if not cls._initialized:\n            return\n        signal.signal(signal.SIGCHLD, cls._old_sigchld)\n        cls._initialized = False", "output": "Removes the ``SIGCHLD`` handler.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_enabled():\n    '''\n    \n    '''\n    cmd = 'service -e'\n    services = __salt__['cmd.run'](cmd, python_shell=False)\n    for service in services.split('\\\\n'):\n        if re.search('jail', service):\n            return True\n    return False", "output": "See if jail service is actually enabled on boot\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' jail.is_enabled <jail name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_recording():\n    \"\"\"\n    \"\"\"\n    curr = ctypes.c_bool()\n    check_call(_LIB.MXAutogradIsRecording(ctypes.byref(curr)))\n    return curr.value", "output": "Get status on recording/not recording.\n\n    Returns\n    -------\n    Current state of recording.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def schemaValidateDoc(self, doc):\n        \"\"\" \"\"\"\n        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        ret = libxml2mod.xmlSchemaValidateDoc(self._o, doc__o)\n        return ret", "output": "Validate a document tree in memory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def version(ruby=None, runas=None, gem_bin=None):\n    '''\n    \n    '''\n    cmd = ['--version']\n    stdout = _gem(cmd,\n                  ruby,\n                  gem_bin=gem_bin,\n                  runas=runas)\n    ret = {}\n    for line in salt.utils.itertools.split(stdout, '\\n'):\n        match = re.match(r'[.0-9]+', line)\n        if match:\n            ret = line\n            break\n    return ret", "output": "Print out the version of gem\n\n    :param gem_bin: string : None\n        Full path to ``gem`` binary to use.\n    :param ruby: string : None\n        If RVM or rbenv are installed, the ruby version and gemset to use.\n        Ignored if ``gem_bin`` is specified.\n    :param runas: string : None\n        The user to run gem as.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' gem.version", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dump_to_json(self, with_stats):\n        \"\"\"\n        \n        \"\"\"\n        import json\n        trees_json_str = tc.extensions._xgboost_dump_model(self.__proxy__, with_stats=with_stats, format='json')\n        trees_json = [json.loads(x) for x in trees_json_str]\n\n        # To avoid lose precision when using libjson, _dump_model with json format encode\n        # numerical values in hexadecimal (little endian).\n        # Now we need to convert them back to floats, using unpack. '<f' means single precision float\n        # in little endian\n        import struct\n        import sys\n        def hexadecimal_to_float(s):\n            if sys.version_info[0] >= 3:\n                return struct.unpack('<f', bytes.fromhex(s))[0] # unpack always return a tuple\n            else:\n                return struct.unpack('<f', s.decode('hex'))[0] # unpack always return a tuple\n\n        for d in trees_json:\n            nodes = d['vertices']\n            for n in nodes:\n                if 'value_hexadecimal' in n:\n                    n['value'] = hexadecimal_to_float(n['value_hexadecimal'])\n        return trees_json", "output": "Dump the models into a list of strings. Each\n        string is a text representation of a tree.\n\n        Parameters\n        ----------\n        with_stats : bool\n            If true, include node statistics in the output.\n\n        Returns\n        -------\n        out : SFrame\n            A table with two columns: feature, count,\n            ordered by 'count' in descending order.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _player_step_tuple(self, envs_step_tuples):\n    \"\"\"\n    \"\"\"\n    ob_real, reward_real, _, _ = envs_step_tuples[\"real_env\"]\n    ob_sim, reward_sim, _, _ = envs_step_tuples[\"sim_env\"]\n    ob_err = absolute_hinge_difference(ob_sim, ob_real)\n\n    ob_real_aug = self._augment_observation(ob_real, reward_real,\n                                            self.cumulative_real_reward)\n    ob_sim_aug = self._augment_observation(ob_sim, reward_sim,\n                                           self.cumulative_sim_reward)\n    ob_err_aug = self._augment_observation(\n        ob_err, reward_sim - reward_real,\n        self.cumulative_sim_reward - self.cumulative_real_reward\n    )\n    ob = np.concatenate([ob_sim_aug, ob_real_aug, ob_err_aug], axis=1)\n    _, reward, done, info = envs_step_tuples[\"real_env\"]\n    return ob, reward, done, info", "output": "Construct observation, return usual step tuple.\n\n    Args:\n      envs_step_tuples: tuples.\n\n    Returns:\n      Step tuple: ob, reward, done, info\n        ob: concatenated images [simulated observation, real observation,\n          difference], with additional informations in header.\n        reward: real environment reward\n        done: True iff. envs_step_tuples['real_env'][2] is True\n        info: real environment info", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_training_metrics(self):\n        \"\"\"\n        \n        \"\"\"\n        for brain_name in self.trainers.keys():\n            if brain_name in self.trainer_metrics:\n                self.trainers[brain_name].write_training_metrics()", "output": "Write all CSV metrics\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def map_vals(val, *names, **extra_opts):\n    '''\n    \n    '''\n    fill = extra_opts.pop('fill', NOTSET)\n    expected_num_elements = len(names)\n    val = translate_stringlist(val)\n    for idx, item in enumerate(val):\n        if not isinstance(item, dict):\n            elements = [x.strip() for x in item.split(':')]\n            num_elements = len(elements)\n            if num_elements < expected_num_elements:\n                if fill is NOTSET:\n                    raise SaltInvocationError(\n                        '\\'{0}\\' contains {1} value(s) (expected {2})'.format(\n                            item, num_elements, expected_num_elements\n                        )\n                    )\n                elements.extend([fill] * (expected_num_elements - num_elements))\n            elif num_elements > expected_num_elements:\n                raise SaltInvocationError(\n                    '\\'{0}\\' contains {1} value(s) (expected {2})'.format(\n                        item,\n                        num_elements,\n                        expected_num_elements if fill is NOTSET\n                            else 'up to {0}'.format(expected_num_elements)\n                    )\n                )\n            val[idx] = dict(zip(names, elements))\n    return val", "output": "Many arguments come in as a list of VAL1:VAL2 pairs, but map to a list\n    of dicts in the format {NAME1: VAL1, NAME2: VAL2}. This function\n    provides common code to handle these instances.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def option(self, key, value):\n        \"\"\"\n        \"\"\"\n        self._jreader = self._jreader.option(key, to_str(value))\n        return self", "output": "Adds an input option for the underlying data source.\n\n        You can set the following option(s) for reading files:\n            * ``timeZone``: sets the string that indicates a timezone to be used to parse timestamps\n                in the JSON/CSV datasources or partition values.\n                If it isn't set, it uses the default value, session local timezone.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unpack_all(self, all_packed, devices):\n        \"\"\"\n        \n        \"\"\"\n        all_grads = []  # #GPU x #Var\n        for dev, packed_grads_single_device in zip(devices, all_packed):\n            with tf.device(dev):\n                all_grads.append(self.unpack(packed_grads_single_device))\n        return all_grads", "output": "Args:\n            all_packed: K lists of packed gradients.", "category": "Python"}]