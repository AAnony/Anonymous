[{"instruction": "Summarizing the code.", "input": "def data(self, index, role):\r\n        \"\"\"\"\"\"\r\n        if role == Qt.ToolTipRole:\r\n            root_dir = self.path_list[0].split(osp.sep)[-1]\r\n            if index.data() == root_dir:\r\n                return osp.join(self.root_path, root_dir)\r\n        return QSortFilterProxyModel.data(self, index, role)", "output": "Show tooltip with full path only for the root directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(self, file_obj):\n        \"\"\"\n        \n        \"\"\"\n        host = {\"host\": [\"*\"], \"config\": {}}\n        for line in file_obj:\n            # Strip any leading or trailing whitespace from the line.\n            # Refer to https://github.com/paramiko/paramiko/issues/499\n            line = line.strip()\n            if not line or line.startswith(\"#\"):\n                continue\n\n            match = re.match(self.SETTINGS_REGEX, line)\n            if not match:\n                raise Exception(\"Unparsable line {}\".format(line))\n            key = match.group(1).lower()\n            value = match.group(2)\n\n            if key == \"host\":\n                self._config.append(host)\n                host = {\"host\": self._get_hosts(value), \"config\": {}}\n            elif key == \"proxycommand\" and value.lower() == \"none\":\n                # Store 'none' as None; prior to 3.x, it will get stripped out\n                # at the end (for compatibility with issue #415). After 3.x, it\n                # will simply not get stripped, leaving a nice explicit marker.\n                host[\"config\"][key] = None\n            else:\n                if value.startswith('\"') and value.endswith('\"'):\n                    value = value[1:-1]\n\n                # identityfile, localforward, remoteforward keys are special\n                # cases, since they are allowed to be specified multiple times\n                # and they should be tried in order of specification.\n                if key in [\"identityfile\", \"localforward\", \"remoteforward\"]:\n                    if key in host[\"config\"]:\n                        host[\"config\"][key].append(value)\n                    else:\n                        host[\"config\"][key] = [value]\n                elif key not in host[\"config\"]:\n                    host[\"config\"][key] = value\n        self._config.append(host)", "output": "Read an OpenSSH config from the given file object.\n\n        :param file_obj: a file-like object to read the config file from", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_dispatcher_logger():\n    \"\"\" \"\"\"\n    logger_file_path = 'dispatcher.log'\n    if dispatcher_env_vars.NNI_LOG_DIRECTORY is not None:\n        logger_file_path = os.path.join(dispatcher_env_vars.NNI_LOG_DIRECTORY, logger_file_path)\n    init_logger(logger_file_path, dispatcher_env_vars.NNI_LOG_LEVEL)", "output": "Initialize dispatcher logging configuration", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_ada_lmpackedbase_dialog():\n  \"\"\"\"\"\"\n  hparams = transformer_base_vq_ada_32ex_packed()\n  hparams.max_length = 1024\n  hparams.ffn_layer = \"dense_relu_dense\"\n  hparams.batch_size = 4096\n  return hparams", "output": "Set of hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_next_tick_callback(self, callback, callback_id=None):\n        \"\"\" \"\"\"\n        def wrapper(*args, **kwargs):\n            # this 'removed' flag is a hack because Tornado has no way\n            # to remove a \"next tick\" callback added with\n            # IOLoop.add_callback. So instead we make our wrapper skip\n            # invoking the callback.\n            if not wrapper.removed:\n                self.remove_next_tick_callback(callback_id)\n                return callback(*args, **kwargs)\n            else:\n                return None\n\n        wrapper.removed = False\n\n        def remover():\n            wrapper.removed = True\n\n        callback_id = self._assign_remover(callback, callback_id, self._next_tick_callback_removers, remover)\n        self._loop.add_callback(wrapper)\n        return callback_id", "output": "Adds a callback to be run on the next tick.\n        Returns an ID that can be used with remove_next_tick_callback.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_fold_levels(editor):\n    \"\"\"\n    \n    \"\"\"\n    block = editor.document().firstBlock()\n    oed = editor.get_outlineexplorer_data()\n\n    folds = []\n    parents = []\n    prev = None\n\n    while block.isValid():\n        if TextBlockHelper.is_fold_trigger(block):\n            try:\n                data = oed[block.firstLineNumber()]\n\n                if data.def_type in (OED.CLASS, OED.FUNCTION):\n                    fsh = FoldScopeHelper(FoldScope(block), data)\n\n                    # Determine the parents of the item using a stack.\n                    _adjust_parent_stack(fsh, prev, parents)\n\n                    # Update the parents of this FoldScopeHelper item\n                    fsh.parents = copy.copy(parents)\n                    folds.append(fsh)\n                    prev = fsh\n            except KeyError:\n                pass\n\n        block = block.next()\n\n    return folds", "output": "Return a list of all the class/function definition ranges.\n\n    Parameters\n    ----------\n    editor : :class:`spyder.plugins.editor.widgets.codeeditor.CodeEditor`\n\n    Returns\n    -------\n    folds : list of :class:`FoldScopeHelper`\n        A list of all the class or function defintion fold points.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop_and_reset_thread(self, ignore_results=False):\r\n        \"\"\"\"\"\"\r\n        if self.search_thread is not None:\r\n            if self.search_thread.isRunning():\r\n                if ignore_results:\r\n                    self.search_thread.sig_finished.disconnect(\r\n                        self.search_complete)\r\n                self.search_thread.stop()\r\n                self.search_thread.wait()\r\n            self.search_thread.setParent(None)\r\n            self.search_thread = None", "output": "Stop current search thread and clean-up", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def position(self):\n        \"\"\"\"\"\"\n        line, col = self._position(self.chunkOffset)\n        return (line + 1, col)", "output": "Returns (line, col) of the current position in the stream.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def connect_head_namespaced_pod_proxy(self, name, namespace, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_head_namespaced_pod_proxy_with_http_info(name, namespace, **kwargs)\n        else:\n            (data) = self.connect_head_namespaced_pod_proxy_with_http_info(name, namespace, **kwargs)\n            return data", "output": "connect HEAD requests to proxy of Pod\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_head_namespaced_pod_proxy(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodProxyOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str path: Path is the URL path to use for the current proxy request to pod.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(self, x):\n    \"\"\"\n    \"\"\"\n    with tf.name_scope(\"pad_reduce/remove\"):\n      x_shape = x.get_shape().as_list()\n      x = tf.gather_nd(\n          x,\n          indices=self.nonpad_ids,\n      )\n      if not tf.executing_eagerly():\n        # This is a hack but for some reason, gather_nd return a tensor of\n        # undefined shape, so the shape is set up manually\n        x.set_shape([None] + x_shape[1:])\n    return x", "output": "Remove padding from the given tensor.\n\n    Args:\n      x (tf.Tensor): of shape [dim_origin,...]\n\n    Returns:\n      a tensor of shape [dim_compressed,...] with dim_compressed <= dim_origin", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode_example(self, tfexample_data):\n    \"\"\"\"\"\"\n    # TODO(epot): Support dynamic shape\n    if self.shape.count(None) < 2:\n      # Restore the shape if possible. TF Example flattened it.\n      shape = [-1 if i is None else i for i in self.shape]\n      tfexample_data = tf.reshape(tfexample_data, shape)\n    if tfexample_data.dtype != self.dtype:\n      tfexample_data = tf.dtypes.cast(tfexample_data, self.dtype)\n    return tfexample_data", "output": "See base class for details.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pre_release(version):\n    \"\"\"\"\"\"\n    announce(version)\n    regen()\n    changelog(version, write_out=True)\n    fix_formatting()\n\n    msg = \"Preparing release version {}\".format(version)\n    check_call([\"git\", \"commit\", \"-a\", \"-m\", msg])\n\n    print()\n    print(f\"{Fore.CYAN}[generate.pre_release] {Fore.GREEN}All done!\")\n    print()\n    print(f\"Please push your branch and open a PR.\")", "output": "Generates new docs, release announcements and creates a local tag.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_densenet(num_layers, pretrained=False, ctx=cpu(),\n                 root=os.path.join(base.data_dir(), 'models'), **kwargs):\n    \n    \"\"\"\n    num_init_features, growth_rate, block_config = densenet_spec[num_layers]\n    net = DenseNet(num_init_features, growth_rate, block_config, **kwargs)\n    if pretrained:\n        from ..model_store import get_model_file\n        net.load_parameters(get_model_file('densenet%d'%(num_layers), root=root), ctx=ctx)\n    return net", "output": "r\"\"\"Densenet-BC model from the\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_ paper.\n\n    Parameters\n    ----------\n    num_layers : int\n        Number of layers for the variant of densenet. Options are 121, 161, 169, 201.\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    ctx : Context, default CPU\n        The context in which to load the pretrained weights.\n    root : str, default $MXNET_HOME/models\n        Location for keeping the model parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def createElement(self, token):\n        \"\"\"\"\"\"\n        name = token[\"name\"]\n        namespace = token.get(\"namespace\", self.defaultNamespace)\n        element = self.elementClass(name, namespace)\n        element.attributes = token[\"data\"]\n        return element", "output": "Create an element but don't insert it anywhere", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fit(self, X, y,\n            sample_weight=None, init_score=None,\n            eval_set=None, eval_names=None, eval_sample_weight=None,\n            eval_init_score=None, eval_metric=None, early_stopping_rounds=None,\n            verbose=True, feature_name='auto', categorical_feature='auto', callbacks=None):\n        \"\"\"\"\"\"\n        super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n                                       init_score=init_score, eval_set=eval_set,\n                                       eval_names=eval_names,\n                                       eval_sample_weight=eval_sample_weight,\n                                       eval_init_score=eval_init_score,\n                                       eval_metric=eval_metric,\n                                       early_stopping_rounds=early_stopping_rounds,\n                                       verbose=verbose, feature_name=feature_name,\n                                       categorical_feature=categorical_feature,\n                                       callbacks=callbacks)\n        return self", "output": "Docstring is inherited from the LGBMModel.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_util_to_datetime(time):\n    \"\"\"\n    \n    \"\"\"\n    if len(str(time)) == 10:\n        _time = '{} 00:00:00'.format(time)\n    elif len(str(time)) == 19:\n        _time = str(time)\n    else:\n        QA_util_log_info('WRONG DATETIME FORMAT {}'.format(time))\n    return datetime.datetime.strptime(_time, '%Y-%m-%d %H:%M:%S')", "output": "\u5b57\u7b26\u4e32 '2018-01-01'  \u8f6c\u53d8\u6210 datatime \u7c7b\u578b\n    :param time: \u5b57\u7b26\u4e32str -- \u683c\u5f0f\u5fc5\u987b\u662f 2018-01-01 \uff0c\u957f\u5ea610\n    :return: \u7c7b\u578bdatetime.datatime", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _timestamp_query_param_from_json(value, field):\n    \"\"\"\n    \"\"\"\n    if _not_null(value, field):\n        # Canonical formats for timestamps in BigQuery are flexible. See:\n        # g.co/cloud/bigquery/docs/reference/standard-sql/data-types#timestamp-type\n        # The separator between the date and time can be 'T' or ' '.\n        value = value.replace(\" \", \"T\", 1)\n        # The UTC timezone may be formatted as Z or +00:00.\n        value = value.replace(\"Z\", \"\")\n        value = value.replace(\"+00:00\", \"\")\n\n        if \".\" in value:\n            # YYYY-MM-DDTHH:MM:SS.ffffff\n            return datetime.datetime.strptime(value, _RFC3339_MICROS_NO_ZULU).replace(\n                tzinfo=UTC\n            )\n        else:\n            # YYYY-MM-DDTHH:MM:SS\n            return datetime.datetime.strptime(value, _RFC3339_NO_FRACTION).replace(\n                tzinfo=UTC\n            )\n    else:\n        return None", "output": "Coerce 'value' to a datetime, if set or not nullable.\n\n    Args:\n        value (str): The timestamp.\n        field (.SchemaField): The field corresponding to the value.\n\n    Returns:\n        Optional[datetime.datetime]: The parsed datetime object from\n        ``value`` if the ``field`` is not null (otherwise it is\n        :data:`None`).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _original_vocab(tmp_dir):\n  \"\"\"\n  \"\"\"\n  vocab_url = (\"http://download.tensorflow.org/models/LM_LSTM_CNN/\"\n               \"vocab-2016-09-10.txt\")\n  vocab_filename = os.path.basename(vocab_url + \".en\")\n  vocab_filepath = os.path.join(tmp_dir, vocab_filename)\n  if not os.path.exists(vocab_filepath):\n    generator_utils.maybe_download(tmp_dir, vocab_filename, vocab_url)\n  return set([\n      text_encoder.native_to_unicode(l.strip())\n      for l in tf.gfile.Open(vocab_filepath)\n  ])", "output": "Returns a set containing the original vocabulary.\n\n  This is important for comparing with published results.\n\n  Args:\n    tmp_dir: directory containing dataset.\n\n  Returns:\n    a set of strings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_font(self):\r\n        \"\"\"\"\"\"\r\n        color_scheme = self.get_color_scheme()\r\n        font = self.get_plugin_font()\r\n        for editor in self.editors:\r\n            editor.set_font(font, color_scheme)", "output": "Update font from Preferences", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __gen_opts(self, opts_in, grains, saltenv=None, ext=None, pillarenv=None):\n        '''\n        \n        '''\n        opts = copy.deepcopy(opts_in)\n        opts['file_client'] = 'local'\n        if not grains:\n            opts['grains'] = {}\n        else:\n            opts['grains'] = grains\n        # Allow minion/CLI saltenv/pillarenv to take precedence over master\n        opts['saltenv'] = saltenv \\\n            if saltenv is not None \\\n            else opts.get('saltenv')\n        opts['pillarenv'] = pillarenv \\\n            if pillarenv is not None \\\n            else opts.get('pillarenv')\n        opts['id'] = self.minion_id\n        if opts['state_top'].startswith('salt://'):\n            opts['state_top'] = opts['state_top']\n        elif opts['state_top'].startswith('/'):\n            opts['state_top'] = salt.utils.url.create(opts['state_top'][1:])\n        else:\n            opts['state_top'] = salt.utils.url.create(opts['state_top'])\n        if self.ext and self.__valid_on_demand_ext_pillar(opts):\n            if 'ext_pillar' in opts:\n                opts['ext_pillar'].append(self.ext)\n            else:\n                opts['ext_pillar'] = [self.ext]\n        if '__env__' in opts['pillar_roots']:\n            env = opts.get('pillarenv') or opts.get('saltenv') or 'base'\n            if env not in opts['pillar_roots']:\n                log.debug(\"pillar environment '%s' maps to __env__ pillar_roots directory\", env)\n                opts['pillar_roots'][env] = opts['pillar_roots'].pop('__env__')\n            else:\n                log.debug(\"pillar_roots __env__ ignored (environment '%s' found in pillar_roots)\",\n                          env)\n                opts['pillar_roots'].pop('__env__')\n        return opts", "output": "The options need to be altered to conform to the file client", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send(vm, target, key='uuid'):\n    '''\n    \n    '''\n    ret = {}\n    if key not in ['uuid', 'alias', 'hostname']:\n        ret['Error'] = 'Key must be either uuid, alias or hostname'\n        return ret\n    if not os.path.isdir(target):\n        ret['Error'] = 'Target must be a directory or host'\n        return ret\n    vm = lookup('{0}={1}'.format(key, vm), one=True)\n    if 'Error' in vm:\n        return vm\n    # vmadm send <uuid> [target]\n    cmd = 'vmadm send {uuid} > {target}'.format(\n        uuid=vm,\n        target=os.path.join(target, '{0}.vmdata'.format(vm))\n    )\n    res = __salt__['cmd.run_all'](cmd, python_shell=True)\n    retcode = res['retcode']\n    if retcode != 0:\n        ret['Error'] = res['stderr'] if 'stderr' in res else _exit_status(retcode)\n        return ret\n    vmobj = get(vm)\n    if 'datasets' not in vmobj:\n        return True\n    log.warning('one or more datasets detected, this is not supported!')\n    log.warning('trying to zfs send datasets...')\n    for dataset in vmobj['datasets']:\n        name = dataset.split('/')\n        name = name[-1]\n        cmd = 'zfs send {dataset} > {target}'.format(\n            dataset=dataset,\n            target=os.path.join(target, '{0}-{1}.zfsds'.format(vm, name))\n        )\n        res = __salt__['cmd.run_all'](cmd, python_shell=True)\n        retcode = res['retcode']\n        if retcode != 0:\n            ret['Error'] = res['stderr'] if 'stderr' in res else _exit_status(retcode)\n            return ret\n    return True", "output": "Send a vm to a directory\n\n    vm : string\n        vm to be sent\n    target : string\n        target directory\n    key : string [uuid|alias|hostname]\n        value type of 'vm' parameter\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' vmadm.send 186da9ab-7392-4f55-91a5-b8f1fe770543 /opt/backups\n        salt '*' vmadm.send vm=nacl target=/opt/backups key=alias", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_models(client, dataset_id):\n    \"\"\"\"\"\"\n\n    # [START bigquery_list_models]\n    from google.cloud import bigquery\n\n    # TODO(developer): Construct a BigQuery client object.\n    # client = bigquery.Client()\n\n    # TODO(developer): Set dataset_id to the ID of the dataset that contains\n    #                  the models you are listing.\n    # dataset_id = 'your-project.your_dataset'\n\n    models = client.list_models(dataset_id)\n\n    print(\"Models contained in '{}':\".format(dataset_id))\n    for model in models:\n        full_model_id = \"{}.{}.{}\".format(\n            model.project, model.dataset_id, model.model_id\n        )\n        friendly_name = model.friendly_name\n        print(\"{}: friendly_name='{}'\".format(full_model_id, friendly_name))", "output": "Sample ID: go/samples-tracker/1512", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_home_dir():\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    try:\r\n        # expanduser() returns a raw byte string which needs to be\r\n        # decoded with the codec that the OS is using to represent\r\n        # file paths.\r\n        path = encoding.to_unicode_from_fs(osp.expanduser('~'))\r\n    except Exception:\r\n        path = ''\r\n\r\n    if osp.isdir(path):\r\n        return path\r\n    else:\r\n        # Get home from alternative locations\r\n        for env_var in ('HOME', 'USERPROFILE', 'TMP'):\r\n            # os.environ.get() returns a raw byte string which needs to be\r\n            # decoded with the codec that the OS is using to represent\r\n            # environment variables.\r\n            path = encoding.to_unicode_from_fs(os.environ.get(env_var, ''))\r\n            if osp.isdir(path):\r\n                return path\r\n            else:\r\n                path = ''\r\n\r\n        if not path:\r\n            raise RuntimeError('Please set the environment variable HOME to '\r\n                               'your user/home directory path so Spyder can '\r\n                               'start properly.')", "output": "Return user home directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def atrm(*args):\n    '''\n    \n    '''\n\n    # Need to do this here also since we use atq()\n    if not salt.utils.path.which('at'):\n        return '\\'at.atrm\\' is not available.'\n\n    if not args:\n        return {'jobs': {'removed': [], 'tag': None}}\n\n    # Convert all to strings\n    args = salt.utils.data.stringify(args)\n\n    if args[0] == 'all':\n        if len(args) > 1:\n            opts = list(list(map(str, [j['job'] for j in atq(args[1])['jobs']])))\n            ret = {'jobs': {'removed': opts, 'tag': args[1]}}\n        else:\n            opts = list(list(map(str, [j['job'] for j in atq()['jobs']])))\n            ret = {'jobs': {'removed': opts, 'tag': None}}\n    else:\n        opts = list(list(map(str, [i['job'] for i in atq()['jobs']\n            if six.text_type(i['job']) in args])))\n        ret = {'jobs': {'removed': opts, 'tag': None}}\n\n    # Shim to produce output similar to what __virtual__() should do\n    # but __salt__ isn't available in __virtual__()\n    output = _cmd('at', '-d', ' '.join(opts))\n    if output is None:\n        return '\\'at.atrm\\' is not available.'\n\n    return ret", "output": "Remove jobs from the queue.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' at.atrm <jobid> <jobid> .. <jobid>\n        salt '*' at.atrm all\n        salt '*' at.atrm all [tag]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(self, name, definition):\n        \"\"\"  \"\"\"\n        self._storage[name] = self._expand_definition(definition)", "output": "Register a definition to the registry. Existing definitions are\n        replaced silently.\n\n        :param name: The name which can be used as reference in a validation\n                     schema.\n        :type name: :class:`str`\n        :param definition: The definition.\n        :type definition: any :term:`mapping`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_runtime_attr(self, attr_name):\n        \"\"\"\n        \n        \"\"\"\n\n        if attr_name in self.runtime_attrs:\n            return self.runtime_attrs[attr_name](self)\n        else:\n            raise NotImplementedError(attr_name + \" attribute is not implemented for resource \" + self.resource_type)", "output": "Returns a CloudFormation construct that provides value for this attribute. If the resource does not provide\n        this attribute, then this method raises an exception\n\n        :return: Dictionary that will resolve to value of the attribute when CloudFormation stack update is executed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mkdir_p(dirname):\n    \"\"\" \n    \"\"\"\n    assert dirname is not None\n    if dirname == '' or os.path.isdir(dirname):\n        return\n    try:\n        os.makedirs(dirname)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise e", "output": "Like \"mkdir -p\", make a dir recursively, but do nothing if the dir exists\n\n    Args:\n        dirname(str):", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _determine_default_project(project=None):\n    \"\"\"\n    \"\"\"\n    if project is None:\n        project = _get_gcd_project()\n\n    if project is None:\n        project = _base_default_project(project=project)\n\n    return project", "output": "Determine default project explicitly or implicitly as fall-back.\n\n    In implicit case, supports four environments. In order of precedence, the\n    implicit environments are:\n\n    * DATASTORE_DATASET environment variable (for ``gcd`` / emulator testing)\n    * GOOGLE_CLOUD_PROJECT environment variable\n    * Google App Engine application ID\n    * Google Compute Engine project ID (from metadata server)\n\n    :type project: str\n    :param project: Optional. The project to use as default.\n\n    :rtype: str or ``NoneType``\n    :returns: Default project if it can be determined.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def inputs(num_devices, dataset_name, data_dir=None, input_name=None,\n           num_chunks=0, append_targets=False):\n  \"\"\"\n  \"\"\"\n  assert data_dir, \"Must provide a data directory\"\n  data_dir = os.path.expanduser(data_dir)\n\n  (train_batches, train_eval_batches, eval_batches,\n   input_name, input_shape) = _train_and_eval_batches(\n       dataset_name, data_dir, input_name, num_devices)\n\n  def numpy_stream(dataset):\n    return dataset_to_stream(\n        dataset, input_name,\n        num_chunks=num_chunks, append_targets=append_targets)\n\n  if num_chunks > 0:\n    length = input_shape[0]\n    input_shape = tuple(\n        [tuple([length // num_chunks] + list(input_shape)[1:])] * num_chunks)\n\n  return Inputs(train_stream=lambda: numpy_stream(train_batches),\n                train_eval_stream=lambda: numpy_stream(train_eval_batches),\n                eval_stream=lambda: numpy_stream(eval_batches),\n                input_shape=input_shape)", "output": "Make Inputs for built-in datasets.\n\n  Args:\n    num_devices: how many devices to build the inputs for.\n    dataset_name: a TFDS or T2T dataset name. If it's a T2T dataset name, prefix\n      with \"t2t_\".\n    data_dir: data directory.\n    input_name: optional, name of the inputs from the dictionary.\n    num_chunks: optional, into how many pieces should we chunk (large inputs).\n    append_targets: optional, instead of inputs return a pair (inputs, targets)\n      which is useful for autoregressive models.\n\n  Returns:\n    trax.inputs.Inputs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_implicit_value (value_string):\n    \"\"\" \n    \"\"\"\n    assert isinstance(value_string, basestring)\n    if value_string in __implicit_features:\n        return __implicit_features[value_string]\n\n    v = value_string.split('-')\n\n    if v[0] not in __implicit_features:\n        return False\n\n    feature = __implicit_features[v[0]]\n\n    for subvalue in (v[1:]):\n        if not __find_implied_subfeature(feature, subvalue, v[0]):\n            return False\n\n    return True", "output": "Returns true iff 'value_string' is a value_string\n    of an implicit feature.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def qloguniform(low, high, q, random_state):\n    '''\n    \n    '''\n    return np.round(loguniform(low, high, random_state) / q) * q", "output": "low: an float that represent an lower bound\n    high: an float that represent an upper bound\n    q: sample step\n    random_state: an object of numpy.random.RandomState", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def restart_local(drain=False):\n    '''\n    \n    '''\n    if _TRAFFICCTL:\n        cmd = _traffic_ctl('server', 'restart', '--manager')\n    else:\n        cmd = _traffic_line('-L')\n\n    if drain:\n        cmd = cmd + ['--drain']\n\n    return _subprocess(cmd)", "output": "Restart the traffic_manager and traffic_server processes on the local node.\n\n    drain\n        This option modifies the restart behavior such that\n        ``traffic_server`` is not shut down until the number of\n        active client connections drops to the number given by the\n        ``proxy.config.restart.active_client_threshold`` configuration\n        variable.\n\n    .. code-block:: bash\n\n        salt '*' trafficserver.restart_local\n        salt '*' trafficserver.restart_local drain=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_module(sym, data_shapes, label_shapes, label_names, gpus=''):\n    \"\"\"\n    \"\"\"\n    if gpus == '':\n        devices = mx.cpu()\n    else:\n        devices = [mx.gpu(int(i)) for i in gpus.split(',')]\n\n    data_names = [data_shape[0] for data_shape in data_shapes]\n\n    mod = mx.mod.Module(\n        symbol=sym,\n        data_names=data_names,\n        context=devices,\n        label_names=label_names\n    )\n    mod.bind(\n        for_training=False,\n        data_shapes=data_shapes,\n        label_shapes=label_shapes\n    )\n    return mod", "output": "Creates a new MXNet module.\n\n    Parameters\n    ----------\n    sym : Symbol\n        An MXNet symbol.\n\n    input_shape: tuple\n        The shape of the input data in the form of (batch_size, channels, height, width)\n\n    files: list of strings\n        List of URLs pertaining to files that need to be downloaded in order to use the model.\n\n    data_shapes: list of tuples.\n        List of tuples where each tuple is a pair of input variable name and its shape.\n\n    label_shapes: list of (str, tuple)\n        Typically is ``data_iter.provide_label``.\n\n    label_names: list of str\n        Name of the output labels in the MXNet symbolic graph.\n\n    gpus: str\n        Comma separated string of gpu ids on which inferences are executed. E.g. 3,5,6 would refer to GPUs 3, 5 and 6.\n        If empty, we use CPU.\n\n    Returns\n    -------\n    MXNet module", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fgm(self, x, labels, targeted=False):\n    \"\"\"\n    \n    \"\"\"\n    # Compute loss\n    with tf.GradientTape() as tape:\n      # input should be watched because it may be\n      # combination of trainable and non-trainable variables\n      tape.watch(x)\n      loss_obj = LossCrossEntropy(self.model, smoothing=0.)\n      loss = loss_obj.fprop(x=x, y=labels)\n      if targeted:\n        loss = -loss\n\n    # Define gradient of loss wrt input\n    grad = tape.gradient(loss, x)\n    optimal_perturbation = attacks.optimize_linear(grad, self.eps, self.ord)\n\n    # Add perturbation to original example to obtain adversarial example\n    adv_x = x + optimal_perturbation\n\n    # If clipping is needed\n    # reset all values outside of [clip_min, clip_max]\n    if (self.clip_min is not None) and (self.clip_max is not None):\n      adv_x = tf.clip_by_value(adv_x, self.clip_min, self.clip_max)\n    return adv_x", "output": "TensorFlow Eager implementation of the Fast Gradient Method.\n    :param x: the input variable\n    :param targeted: Is the attack targeted or untargeted? Untargeted, the\n                     default, will try to make the label incorrect.\n                     Targeted will instead try to move in the direction\n                     of being more like y.\n    :return: a tensor for the adversarial example", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def max(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        # TODO: skipna is broken with max.\n        # See https://github.com/pandas-dev/pandas/issues/24265\n        nv.validate_max(args, kwargs)\n        nv.validate_minmax_axis(axis)\n\n        mask = self.isna()\n        if skipna:\n            values = self[~mask].asi8\n        elif mask.any():\n            return NaT\n        else:\n            values = self.asi8\n\n        if not len(values):\n            # short-circut for empty max / min\n            return NaT\n\n        result = nanops.nanmax(values, skipna=skipna)\n        # Don't have to worry about NA `result`, since no NA went in.\n        return self._box_func(result)", "output": "Return the maximum value of the Array or maximum along\n        an axis.\n\n        See Also\n        --------\n        numpy.ndarray.max\n        Index.max : Return the maximum value in an Index.\n        Series.max : Return the maximum value in a Series.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cache_clean(path=None, runas=None, env=None, force=False):\n    '''\n    \n\n    '''\n    env = env or {}\n\n    if runas:\n        uid = salt.utils.user.get_uid(runas)\n        if uid:\n            env.update({'SUDO_UID': uid, 'SUDO_USER': ''})\n\n    cmd = ['npm', 'cache', 'clean']\n    if path:\n        cmd.append(path)\n    if force is True:\n        cmd.append('--force')\n\n    cmd = ' '.join(cmd)\n    result = __salt__['cmd.run_all'](\n        cmd, cwd=None, runas=runas, env=env, python_shell=True, ignore_retcode=True)\n\n    if result['retcode'] != 0:\n        log.error(result['stderr'])\n        return False\n    return True", "output": "Clean cached NPM packages.\n\n    If no path for a specific package is provided the entire cache will be cleared.\n\n    path\n        The cache subpath to delete, or None to clear the entire cache\n\n    runas\n        The user to run NPM with\n\n    env\n        Environment variables to set when invoking npm. Uses the same ``env``\n        format as the :py:func:`cmd.run <salt.modules.cmdmod.run>` execution\n        function.\n\n    force\n        Force cleaning of cache.  Required for npm@5 and greater\n\n        .. versionadded:: 2016.11.6\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' npm.cache_clean force=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_net_size(mask):\n    '''\n    \n    '''\n    binary_str = ''\n    for octet in mask.split('.'):\n        binary_str += bin(int(octet))[2:].zfill(8)\n    return len(binary_str.rstrip('0'))", "output": "Turns an IPv4 netmask into it's corresponding prefix length\n    (255.255.255.0 -> 24 as in 192.168.1.10/24).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_trace_id():\n    \"\"\"\n    \"\"\"\n    checkers = (\n        get_trace_id_from_django,\n        get_trace_id_from_flask,\n        get_trace_id_from_webapp2,\n    )\n\n    for checker in checkers:\n        trace_id = checker()\n        if trace_id is not None:\n            return trace_id\n\n    return None", "output": "Helper to get trace_id from web application request header.\n\n    :rtype: str\n    :returns: TraceID in HTTP request headers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_file(path, tgt_env='base', **kwargs):  # pylint: disable=W0613\n    '''\n    \n    '''\n    fnd = {'path': '', 'rel': ''}\n    if os.path.isabs(path):\n        return fnd\n    if tgt_env not in envs():\n        return fnd\n    if os.path.basename(path) == 'top.sls':\n        log.debug(\n            'minionfs will NOT serve top.sls '\n            'for security reasons (path requested: %s)', path\n        )\n        return fnd\n\n    mountpoint = salt.utils.url.strip_proto(__opts__['minionfs_mountpoint'])\n    # Remove the mountpoint to get the \"true\" path\n    path = path[len(mountpoint):].lstrip(os.path.sep)\n    try:\n        minion, pushed_file = path.split(os.sep, 1)\n    except ValueError:\n        return fnd\n    if not _is_exposed(minion):\n        return fnd\n    full = os.path.join(\n        __opts__['cachedir'], 'minions', minion, 'files', pushed_file\n    )\n    if os.path.isfile(full) \\\n            and not salt.fileserver.is_file_ignored(__opts__, full):\n        fnd['path'] = full\n        fnd['rel'] = path\n        fnd['stat'] = list(os.stat(full))\n        return fnd\n    return fnd", "output": "Search the environment for the relative path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def round_if_near_integer(a, epsilon=1e-4):\n    \"\"\"\n    \n    \"\"\"\n    if abs(a - round(a)) <= epsilon:\n        return round(a)\n    else:\n        return a", "output": "Round a to the nearest integer if that integer is within an epsilon\n    of a.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download_data_impl(self, run, tag, response_format):\n    \"\"\"\n    \"\"\"\n    scalars_plugin_instance = self._get_scalars_plugin()\n    if not scalars_plugin_instance:\n      raise ValueError(('Failed to respond to request for /download_data. '\n                        'The scalars plugin is oddly not registered.'))\n\n    body, mime_type = scalars_plugin_instance.scalars_impl(\n        tag, run, None, response_format)\n    return body, mime_type", "output": "Provides a response for downloading scalars data for a data series.\n\n    Args:\n      run: The run.\n      tag: The specific tag.\n      response_format: A string. One of the values of the OutputFormat enum of\n        the scalar plugin.\n\n    Raises:\n      ValueError: If the scalars plugin is not registered.\n\n    Returns:\n      2 entities:\n        - A JSON object response body.\n        - A mime type (string) for the response.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def call_lights(*args, **kwargs):\n    '''\n    \n    '''\n    res = dict()\n    lights = _get_lights()\n    for dev_id in 'id' in kwargs and _get_devices(kwargs) or sorted(lights.keys()):\n        if lights.get(six.text_type(dev_id)):\n            res[dev_id] = lights[six.text_type(dev_id)]\n\n    return res or False", "output": "Get info about all available lamps.\n\n    Options:\n\n    * **id**: Specifies a device ID. Can be a comma-separated values. All, if omitted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' hue.lights\n        salt '*' hue.lights id=1\n        salt '*' hue.lights id=1,2,3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _expand_pattern_lists(pattern, **mappings):\n    '''\n    \n    '''\n    expanded_patterns = []\n    f = string.Formatter()\n    '''\n    This function uses a string.Formatter to get all the formatting tokens from\n    the pattern, then recursively replaces tokens whose expanded value is a\n    list. For a list with N items, it will create N new pattern strings and\n    then continue with the next token. In practice this is expected to not be\n    very expensive, since patterns will typically involve a handful of lists at\n    most.\n    '''  # pylint: disable=W0105\n    for (_, field_name, _, _) in f.parse(pattern):\n        if field_name is None:\n            continue\n        (value, _) = f.get_field(field_name, None, mappings)\n        if isinstance(value, list):\n            token = '{{{0}}}'.format(field_name)\n            expanded = [pattern.replace(token, six.text_type(elem)) for elem in value]\n            for expanded_item in expanded:\n                result = _expand_pattern_lists(expanded_item, **mappings)\n                expanded_patterns += result\n            return expanded_patterns\n    return [pattern]", "output": "Expands the pattern for any list-valued mappings, such that for any list of\n    length N in the mappings present in the pattern, N copies of the pattern are\n    returned, each with an element of the list substituted.\n\n    pattern:\n        A pattern to expand, for example ``by-role/{grains[roles]}``\n\n    mappings:\n        A dictionary of variables that can be expanded into the pattern.\n\n    Example: Given the pattern `` by-role/{grains[roles]}`` and the below grains\n\n    .. code-block:: yaml\n\n        grains:\n            roles:\n                - web\n                - database\n\n    This function will expand into two patterns,\n    ``[by-role/web, by-role/database]``.\n\n    Note that this method does not expand any non-list patterns.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_preprocessor(space):\n    \"\"\"\"\"\"\n\n    legacy_patch_shapes(space)\n    obs_shape = space.shape\n\n    if isinstance(space, gym.spaces.Discrete):\n        preprocessor = OneHotPreprocessor\n    elif obs_shape == ATARI_OBS_SHAPE:\n        preprocessor = GenericPixelPreprocessor\n    elif obs_shape == ATARI_RAM_OBS_SHAPE:\n        preprocessor = AtariRamPreprocessor\n    elif isinstance(space, gym.spaces.Tuple):\n        preprocessor = TupleFlatteningPreprocessor\n    elif isinstance(space, gym.spaces.Dict):\n        preprocessor = DictFlatteningPreprocessor\n    else:\n        preprocessor = NoPreprocessor\n\n    return preprocessor", "output": "Returns an appropriate preprocessor class for the given space.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def match_val_type(vals, vals_bounds, vals_types):\n    '''\n    \n    '''\n    vals_new = []\n\n    for i, _ in enumerate(vals_types):\n        if vals_types[i] == \"discrete_int\":\n            # Find the closest integer in the array, vals_bounds\n            vals_new.append(min(vals_bounds[i], key=lambda x: abs(x - vals[i])))\n        elif vals_types[i] == \"range_int\":\n            # Round down to the nearest integer\n            vals_new.append(math.floor(vals[i]))\n        elif vals_types[i] == \"range_continuous\":\n            # Don't do any processing for continous numbers\n            vals_new.append(vals[i])\n        else:\n            return None\n\n    return vals_new", "output": "Update values in the array, to match their corresponding type", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_pool_member(lb, name, port, pool_name):\n    '''\n    \n    '''\n    if __opts__['load_balancers'].get(lb, None):\n        (username, password) = list(__opts__['load_balancers'][lb].values())\n    else:\n        raise Exception('Unable to find `{0}` load balancer'.format(lb))\n    F5 = F5Mgmt(lb, username, password)\n    F5.add_pool_member(name, port, pool_name)\n    return True", "output": "Add a node to a pool\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-run f5.add_pool_member load_balancer 10.0.0.1 80 my_pool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset_parameter(self, params):\n        \"\"\"\n        \"\"\"\n        if any(metric_alias in params for metric_alias in ('metric', 'metrics', 'metric_types')):\n            self.__need_reload_eval_info = True\n        params_str = param_dict_to_str(params)\n        if params_str:\n            _safe_call(_LIB.LGBM_BoosterResetParameter(\n                self.handle,\n                c_str(params_str)))\n        self.params.update(params)\n        return self", "output": "Reset parameters of Booster.\n\n        Parameters\n        ----------\n        params : dict\n            New parameters for Booster.\n\n        Returns\n        -------\n        self : Booster\n            Booster with new parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _paginate(url, topkey, *args, **kwargs):\n    '''\n    \n    '''\n\n    ret = salt.utils.http.query(url, **kwargs)\n    if 'errors' in ret['dict']:\n        return ret['dict']\n\n    lim = int(ret['dict']['page']['limit'])\n    total = int(ret['dict']['page']['total'])\n\n    if total == 0:\n        return {}\n\n    numpages = (total / lim) + 1\n\n    # If the count returned is less than the page size, just return the dict\n    if numpages == 1:\n        return ret['dict'][topkey]\n\n    aggregate_ret = ret['dict'][topkey]\n    url = args[0]\n    for p in range(2, numpages):\n        param_url = url + '?offset={0}'.format(lim * (p - 1))\n        next_ret = salt.utils.http.query(param_url, kwargs)\n        aggregate_ret[topkey].extend(next_ret['dict'][topkey])\n\n    return aggregate_ret", "output": "Wrapper to assist with paginated responses from Digicert's REST API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transcode(text, input=PREFERRED_ENCODING, output=PREFERRED_ENCODING):\r\n    \"\"\"\"\"\"\r\n    try:\r\n        return text.decode(\"cp437\").encode(\"cp1252\")\r\n    except UnicodeError:\r\n        try:\r\n            return text.decode(\"cp437\").encode(output)\r\n        except UnicodeError:\r\n            return text", "output": "Transcode a text string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def project_update(project_id=None, name=None, description=None,\n                   enabled=None, profile=None, **connection_args):\n    '''\n    \n    '''\n    auth(profile, **connection_args)\n\n    if _OS_IDENTITY_API_VERSION > 2:\n        return tenant_update(tenant_id=project_id, name=name, description=description,\n                             enabled=enabled, profile=profile, **connection_args)\n    else:\n        return False", "output": "Update a tenant's information (keystone project-update)\n    The following fields may be updated: name, description, enabled.\n    Can only update name if targeting by ID\n\n    Overrides keystone tenant_update form api V2.\n    For keystone api V3 only.\n\n    .. versionadded:: 2016.11.0\n\n    project_id\n        The project id.\n\n    name\n        The project name, which must be unique within the owning domain.\n\n    description\n        The project description.\n\n    enabled\n        Enables or disables the project.\n\n    profile\n        Configuration profile - if configuration for multiple openstack accounts required.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.project_update name=admin enabled=True\n        salt '*' keystone.project_update c965f79c4f864eaaa9c3b41904e67082 name=admin email=admin@domain.com", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expand_batch_coordinates(bc, length_factor):\n  \"\"\"\n  \"\"\"\n  assert bc.get_shape().as_list() == [1, None, 1]\n  # bc has shape [1, length, 1]\n  bc *= tf.constant([[1] * length_factor])\n  # bc has shape [1, length, length_factor]\n  bc = tf.reshape(bc, [1, -1, 1])\n  # bc has shape [1, length*length_factor]\n  return bc", "output": "Duplicate elements of bc by length_factor.\n\n  Args:\n    bc (tf.Tensor): int32 tensor of shape [1, length, 1]\n    length_factor (int):\n\n  Returns:\n    tf.Tensor: of shape [1, length*length_factor, 1] where every elements has\n      been duplicated length_factor times.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fix_paths(job):\n    \"\"\"\n    \n    \"\"\"\n    tmp_files = []\n    args = []\n    for x in job.args():\n        if isinstance(x, luigi.contrib.hdfs.HdfsTarget):  # input/output\n            if x.exists() or not job.atomic_output():  # input\n                args.append(x.path)\n            else:  # output\n                x_path_no_slash = x.path[:-1] if x.path[-1] == '/' else x.path\n                y = luigi.contrib.hdfs.HdfsTarget(x_path_no_slash + '-luigi-tmp-%09d' % random.randrange(0, 1e10))\n                tmp_files.append((y, x_path_no_slash))\n                logger.info('Using temp path: %s for path %s', y.path, x.path)\n                args.append(y.path)\n        else:\n            try:\n                # hopefully the target has a path to use\n                args.append(x.path)\n            except AttributeError:\n                # if there's no path then hope converting it to a string will work\n                args.append(str(x))\n\n    return (tmp_files, args)", "output": "Coerce input arguments to use temporary files when used for output.\n\n    Return a list of temporary file pairs (tmpfile, destination path) and\n    a list of arguments.\n\n    Converts each HdfsTarget to a string for the path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _normalize_projection(projection):\n        \"\"\"\"\"\"\n        if projection is not None:\n\n            fields = list(projection.fields)\n\n            if not fields:\n                field_ref = query_pb2.StructuredQuery.FieldReference(\n                    field_path=\"__name__\"\n                )\n                return query_pb2.StructuredQuery.Projection(fields=[field_ref])\n\n        return projection", "output": "Helper:  convert field paths to message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_mutant_tuples(example_protos, original_feature, index_to_mutate,\n                       viz_params):\n  \"\"\"\n  \"\"\"\n  mutant_features = make_mutant_features(original_feature, index_to_mutate,\n                                         viz_params)\n  mutant_examples = []\n  for example_proto in example_protos:\n    for mutant_feature in mutant_features:\n      copied_example = copy.deepcopy(example_proto)\n      feature_name = mutant_feature.original_feature.feature_name\n\n      try:\n        feature_list = proto_value_for_feature(copied_example, feature_name)\n        if index_to_mutate is None:\n          new_values = mutant_feature.mutant_value\n        else:\n          new_values = list(feature_list)\n          new_values[index_to_mutate] = mutant_feature.mutant_value\n\n        del feature_list[:]\n        feature_list.extend(new_values)\n        mutant_examples.append(copied_example)\n      except (ValueError, IndexError):\n        # If the mutant value can't be set, still add the example to the\n        # mutant_example even though no change was made. This is necessary to\n        # allow for computation of global PD plots when not all examples have\n        # the same number of feature values for a feature.\n        mutant_examples.append(copied_example)\n\n  return mutant_features, mutant_examples", "output": "Return a list of `MutantFeatureValue`s and a list of mutant Examples.\n\n  Args:\n    example_protos: The examples to mutate.\n    original_feature: A `OriginalFeatureList` that encapsulates the feature to\n      mutate.\n    index_to_mutate: The index of the int64_list or float_list to mutate.\n    viz_params: A `VizParams` object that contains the UI state of the request.\n\n  Returns:\n    A list of `MutantFeatureValue`s and a list of mutant examples.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absent(name, auth=None):\n    '''\n    \n\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    __salt__['neutronng.setup_clouds'](auth)\n\n    subnet = __salt__['neutronng.subnet_get'](name=name)\n\n    if subnet:\n        if __opts__['test'] is True:\n            ret['result'] = None\n            ret['changes'] = {'id': subnet.id}\n            ret['comment'] = 'Project will be deleted.'\n            return ret\n\n        __salt__['neutronng.subnet_delete'](name=subnet)\n        ret['changes']['id'] = name\n        ret['comment'] = 'Deleted subnet'\n\n    return ret", "output": "Ensure a subnet does not exists\n\n    name\n        Name of the subnet", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nth_char(char_map, index):\n    \"\"\"\"\"\"\n    for char in char_map:\n        if index < char_map[char]:\n            return char\n        index = index - char_map[char]\n    return None", "output": "Returns the nth character of a character->occurrence map", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_(package='', cyg_arch='x86_64'):\n    '''\n    \n    '''\n    pkgs = {}\n    args = ' '.join(['-c', '-d', package])\n    stdout = _cygcheck(args, cyg_arch=cyg_arch)\n    lines = []\n    if isinstance(stdout, six.string_types):\n        lines = salt.utils.stringutils.to_unicode(stdout).splitlines()\n    for line in lines:\n        match = re.match(r'^([^ ]+) *([^ ]+)', line)\n        if match:\n            pkg = match.group(1)\n            version = match.group(2)\n            pkgs[pkg] = version\n    return pkgs", "output": "List locally installed packages.\n\n    package : ''\n        package name to check. else all\n\n    cyg_arch :\n        Cygwin architecture to use\n        Options are x86 and x86_64\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cyg.list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_bot_mapping(self):\n        \"\"\"\"\"\"\n        bot = self.context.bot\n        mapping = {\n            cog: cog.get_commands()\n            for cog in bot.cogs.values()\n        }\n        mapping[None] = [c for c in bot.all_commands.values() if c.cog is None]\n        return mapping", "output": "Retrieves the bot mapping passed to :meth:`send_bot_help`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def retrieve_metadata_server(metadata_key):\n    \"\"\"\n    \"\"\"\n    url = METADATA_URL + metadata_key\n\n    try:\n        response = requests.get(url, headers=METADATA_HEADERS)\n\n        if response.status_code == requests.codes.ok:\n            return response.text\n\n    except requests.exceptions.RequestException:\n        # Ignore the exception, connection failed means the attribute does not\n        # exist in the metadata server.\n        pass\n\n    return None", "output": "Retrieve the metadata key in the metadata server.\n\n    See: https://cloud.google.com/compute/docs/storing-retrieving-metadata\n\n    :type metadata_key: str\n    :param metadata_key: Key of the metadata which will form the url. You can\n                         also supply query parameters after the metadata key.\n                         e.g. \"tags?alt=json\"\n\n    :rtype: str\n    :returns: The value of the metadata key returned by the metadata server.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toListInt(value):\n        \"\"\"\n        \n        \"\"\"\n        if TypeConverters._can_convert_to_list(value):\n            value = TypeConverters.toList(value)\n            if all(map(lambda v: TypeConverters._is_integer(v), value)):\n                return [int(v) for v in value]\n        raise TypeError(\"Could not convert %s to list of ints\" % value)", "output": "Convert a value to list of ints, if possible.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _run_svn(cmd, cwd, user, username, password, opts, **kwargs):\n    '''\n    \n    '''\n    cmd = ['svn', '--non-interactive', cmd]\n\n    options = list(opts)\n    if username:\n        options.extend(['--username', username])\n    if password:\n        options.extend(['--password', password])\n    cmd.extend(options)\n\n    result = __salt__['cmd.run_all'](cmd, python_shell=False, cwd=cwd, runas=user, **kwargs)\n\n    retcode = result['retcode']\n\n    if retcode == 0:\n        return result['stdout']\n    raise CommandExecutionError(result['stderr'] + '\\n\\n' + ' '.join(cmd))", "output": "Execute svn\n    return the output of the command\n\n    cmd\n        The command to run.\n\n    cwd\n        The path to the Subversion repository\n\n    user\n        Run svn as a user other than what the minion runs as\n\n    username\n        Connect to the Subversion server as another user\n\n    password\n        Connect to the Subversion server with this password\n\n        .. versionadded:: 0.17.0\n\n    opts\n        Any additional options to add to the command line\n\n    kwargs\n        Additional options to pass to the run-cmd", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_():\n    '''\n    \n\n    '''\n    ret = []\n    states_path = _states_path()\n    if not os.path.isdir(states_path):\n        return ret\n\n    for state in os.listdir(states_path):\n        if state.endswith(('-pkgs.yml', '-reps.yml')):\n            # Remove the suffix, as both share the same size\n            ret.append(state[:-9])\n    return sorted(set(ret))", "output": "Return the list of frozen states.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' freezer.list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_not_tuple_of_2_elements(obj, obj_name='obj'):\n    \"\"\"\"\"\"\n    if not isinstance(obj, tuple) or len(obj) != 2:\n        raise TypeError('%s must be a tuple of 2 elements.' % obj_name)", "output": "Check object is not tuple or does not have 2 elements.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_build_platform():\n    \"\"\"\n    \"\"\"\n    from sysconfig import get_platform\n\n    plat = get_platform()\n    if sys.platform == \"darwin\" and not plat.startswith('macosx-'):\n        try:\n            version = _macosx_vers()\n            machine = os.uname()[4].replace(\" \", \"_\")\n            return \"macosx-%d.%d-%s\" % (\n                int(version[0]), int(version[1]),\n                _macosx_arch(machine),\n            )\n        except ValueError:\n            # if someone is running a non-Mac darwin system, this will fall\n            # through to the default implementation\n            pass\n    return plat", "output": "Return this platform's string for platform-specific distributions\n\n    XXX Currently this is the same as ``distutils.util.get_platform()``, but it\n    needs some hacks for Linux and Mac OS X.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_lines_to_file(cls_name, filename, lines, metadata_dict):\n  \"\"\"\"\"\"\n  metadata_dict = metadata_dict or {}\n  header_line = \"%s%s\" % (_HEADER_PREFIX, cls_name)\n  metadata_line = \"%s%s\" % (_METADATA_PREFIX,\n                            json.dumps(metadata_dict, sort_keys=True))\n  with tf.io.gfile.GFile(filename, \"wb\") as f:\n    for line in [header_line, metadata_line]:\n      f.write(tf.compat.as_bytes(line))\n      f.write(tf.compat.as_bytes(\"\\n\"))\n    if lines:\n      f.write(tf.compat.as_bytes(\"\\n\".join(lines)))\n      f.write(tf.compat.as_bytes(\"\\n\"))", "output": "Writes lines to file prepended by header and metadata.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_topic_attributes(TopicArn, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        return conn.get_topic_attributes(TopicArn=TopicArn).get('Attributes')\n    except botocore.exceptions.ClientError as e:\n        log.error('Failed to garner attributes for SNS topic %s: %s', TopicArn, e)\n        return None", "output": "Returns all of the properties of a topic.  Topic properties returned might differ based on the\n    authorization of the user.\n\n    CLI example::\n\n        salt myminion boto3_sns.get_topic_attributes someTopic region=us-west-1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(prefix, epoch, ctx=None, **kwargs):\n        \"\"\"\n        \"\"\"\n        symbol, arg_params, aux_params = load_checkpoint(prefix, epoch)\n        return FeedForward(symbol, ctx=ctx,\n                           arg_params=arg_params, aux_params=aux_params,\n                           begin_epoch=epoch,\n                           **kwargs)", "output": "Load model checkpoint from file.\n\n        Parameters\n        ----------\n        prefix : str\n            Prefix of model name.\n        epoch : int\n            epoch number of model we would like to load.\n        ctx : Context or list of Context, optional\n            The device context of training and prediction.\n        kwargs : dict\n            Other parameters for model, including `num_epoch`, optimizer and `numpy_batch_size`.\n\n        Returns\n        -------\n        model : FeedForward\n            The loaded model that can be used for prediction.\n\n        Notes\n        -----\n        - ``prefix-symbol.json`` will be saved for symbol.\n        - ``prefix-epoch.params`` will be saved for parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_time_step(cls,\n                       observation=None,\n                       done=False,\n                       raw_reward=None,\n                       processed_reward=None,\n                       action=None):\n    \"\"\"\"\"\"\n\n    return cls(observation, done, raw_reward, processed_reward, action)", "output": "Creates a TimeStep with both rewards and actions as optional.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mean(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    concat_input = [symbol.expand_dims(op_input, axis=0) for op_input in inputs]\n    concat_sym = symbol.concat(*concat_input, dim=0)\n    mean_sym = symbol.mean(concat_sym, axis=0)\n    return mean_sym, attrs, inputs", "output": "Mean of all the input tensors.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render(template_file, saltenv='base', sls='', context=None, tmplpath=None, **kws):\n    '''\n    \n    '''\n    tmp_data = salt.utils.templates.MAKO(template_file, to_str=True,\n                    salt=__salt__,\n                    grains=__grains__,\n                    opts=__opts__,\n                    pillar=__pillar__,\n                    saltenv=saltenv,\n                    sls=sls,\n                    context=context,\n                    tmplpath=tmplpath,\n                    **kws)\n    if not tmp_data.get('result', False):\n        raise SaltRenderError(tmp_data.get('data',\n            'Unknown render error in mako renderer'))\n    return six.moves.StringIO(tmp_data['data'])", "output": "Render the template_file, passing the functions and grains into the\n    Mako rendering system.\n\n    :rtype: string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render(json_data, saltenv='base', sls='', **kws):\n    '''\n    \n    '''\n    if not isinstance(json_data, six.string_types):\n        json_data = json_data.read()\n\n    if json_data.startswith('#!'):\n        json_data = json_data[(json_data.find('\\n') + 1):]\n    if not json_data.strip():\n        return {}\n    return json.loads(json_data)", "output": "Accepts JSON as a string or as a file object and runs it through the JSON\n    parser.\n\n    :rtype: A Python data structure", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_resource(self):\n        \"\"\"\"\"\"\n        resource = {\"name\": self.name}\n\n        if self.dns_name is not None:\n            resource[\"dnsName\"] = self.dns_name\n\n        if self.description is not None:\n            resource[\"description\"] = self.description\n\n        if self.name_server_set is not None:\n            resource[\"nameServerSet\"] = self.name_server_set\n\n        return resource", "output": "Generate a resource for ``create`` or ``update``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_bond(iface):\n    '''\n    \n    '''\n    path = os.path.join(_RH_NETWORK_CONF_FILES, '{0}.conf'.format(iface))\n    return _read_file(path)", "output": "Return the content of a bond script\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ip.get_bond bond0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        \"\"\"\"\"\"\n        while not self.stop_flag:\n            timestamp = time.time()\n            cpu_percent = self.process.cpu_percent() / self.cpu_num\n            # mem_percent = mem = self.process.memory_percent()\n            mem_info = dict(self.process.memory_info()._asdict())\n            mem_gb_num = mem_info.get('rss', 0) / 1024 / 1024\n            # \u8bb0\u5f55\u7c7b\u53d8\u91cf\n            self.profile_data.append({\"mem_gb_num\": mem_gb_num, \"cpu_percent\": cpu_percent, \"timestamp\": timestamp})\n            # \u8bb0\u5f55cpu\u548cmem_gb_num\n            time.sleep(self.interval)", "output": "\u5f00\u59cb\u7ebf\u7a0b.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _lookup_proxmox_task(upid):\n    '''\n    \n    '''\n    log.debug('Getting creation status for upid: %s', upid)\n    tasks = query('get', 'cluster/tasks')\n\n    if tasks:\n        for task in tasks:\n            if task['upid'] == upid:\n                log.debug('Found upid task: %s', task)\n                return task\n\n    return False", "output": "Retrieve the (latest) logs and retrieve the status for a UPID.\n    This can be used to verify whether a task has completed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(pipfile_path=None, inject_env=True):\n    \"\"\"\n    \"\"\"\n\n    if pipfile_path is None:\n        pipfile_path = Pipfile.find()\n\n    return Pipfile.load(filename=pipfile_path, inject_env=inject_env)", "output": "Loads a pipfile from a given path.\n    If none is provided, one will try to be found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _batchify(self, batch_data, batch_label, start=0):\n        \"\"\"\"\"\"\n        i = start\n        batch_size = self.batch_size\n        try:\n            while i < batch_size:\n                label, s = self.next_sample()\n                data = self.imdecode(s)\n                try:\n                    self.check_valid_image([data])\n                    label = self._parse_label(label)\n                    data, label = self.augmentation_transform(data, label)\n                    self._check_valid_label(label)\n                except RuntimeError as e:\n                    logging.debug('Invalid image, skipping:  %s', str(e))\n                    continue\n                for datum in [data]:\n                    assert i < batch_size, 'Batch size must be multiples of augmenter output length'\n                    batch_data[i] = self.postprocess_data(datum)\n                    num_object = label.shape[0]\n                    batch_label[i][0:num_object] = nd.array(label)\n                    if num_object < batch_label[i].shape[0]:\n                        batch_label[i][num_object:] = -1\n                    i += 1\n        except StopIteration:\n            if not i:\n                raise StopIteration\n\n        return i", "output": "Override the helper function for batchifying data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _format_info(data):\n    '''\n    \n    '''\n    return {'name': data.gr_name,\n            'gid': data.gr_gid,\n            'passwd': data.gr_passwd,\n            'members': data.gr_mem}", "output": "Return formatted information in a pretty way.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_relative(path, filename):\n  \"\"\"\"\"\"\n  return os.path.abspath(os.path.join(path, filename)).startswith(path)", "output": "Checks if the filename is relative, not absolute.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def public_ip_addresses_list(resource_group, **kwargs):\n    '''\n    \n\n    '''\n    result = {}\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        pub_ips = __utils__['azurearm.paged_object_to_list'](\n            netconn.public_ip_addresses.list(\n                resource_group_name=resource_group\n            )\n        )\n\n        for ip in pub_ips:\n            result[ip['name']] = ip\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    List all public IP addresses within a resource group.\n\n    :param resource_group: The resource group name to list public IP\n        addresses within.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.public_ip_addresses_list testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sshfp_data(key_t, hash_t, pub):\n    '''\n    \n    '''\n    key_t = RFC.validate(key_t, RFC.SSHFP_ALGO, 'in')\n    hash_t = RFC.validate(hash_t, RFC.SSHFP_HASH)\n\n    hasher = hashlib.new(hash_t)\n    hasher.update(\n        base64.b64decode(pub)\n    )\n    ssh_fp = hasher.hexdigest()\n\n    return _rec2data(key_t, hash_t, ssh_fp)", "output": "Generate an SSHFP record\n    :param key_t: rsa/dsa/ecdsa/ed25519\n    :param hash_t: sha1/sha256\n    :param pub: the SSH public key", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def video_augmentation(features, hue=False, saturate=False, contrast=False):\n  \"\"\"\n  \"\"\"\n  inputs, targets = features[\"inputs\"], features[\"targets\"]\n  in_steps = common_layers.shape_list(inputs)[0]\n\n  # makes sure that the same augmentation is applied to both input and targets.\n  # if input is 4-D, then tf.image applies the same transform across the batch.\n  video = tf.concat((inputs, targets), axis=0)\n  if hue:\n    video = tf.image.random_hue(video, max_delta=0.2)\n  if saturate:\n    video = tf.image.random_saturation(video, lower=0.5, upper=1.5)\n  if contrast:\n    video = tf.image.random_contrast(video, lower=0.5, upper=1.5)\n  features[\"inputs\"], features[\"targets\"] = video[:in_steps], video[in_steps:]\n  return features", "output": "Augments video with optional hue, saturation and constrast.\n\n  Args:\n    features: dict, with keys \"inputs\", \"targets\".\n              features[\"inputs\"], 4-D Tensor, shape=(THWC)\n              features[\"targets\"], 4-D Tensor, shape=(THWC)\n    hue: bool, apply hue_transform.\n    saturate: bool, apply saturation transform.\n    contrast: bool, apply constrast transform.\n  Returns:\n    augment_features: dict with transformed \"inputs\" and \"targets\".", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compiler_info(compiler):\n    \"\"\"\"\"\"\n    (out, err) = subprocess.Popen(\n        ['/bin/sh', '-c', '{0} -v'.format(compiler)],\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE\n    ).communicate('')\n\n    gcc_clang = re.compile('(gcc|clang) version ([0-9]+(\\\\.[0-9]+)*)')\n\n    for line in (out + err).split('\\n'):\n        mtch = gcc_clang.search(line)\n        if mtch:\n            return mtch.group(1) + ' ' + mtch.group(2)\n\n    return compiler", "output": "Determine the name + version of the compiler", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_scope(scope=None, scope_fn=None):\n  \"\"\"\n  \"\"\"\n  def decorator(f):\n\n    @functools.wraps(f)\n    def decorated(*args, **kwargs):\n      name = kwargs.pop(\"name\", None)  # Python 2 hack for keyword only args\n      with scope_fn(name or scope or f.__name__):\n        return f(*args, **kwargs)\n\n    return decorated\n\n  return decorator", "output": "Return a decorator which add a TF name/variable scope to a function.\n\n  Note that the function returned by the decorator accept an additional 'name'\n  parameter, which can overwrite the name scope given when the function is\n  created.\n\n  Args:\n    scope (str): name of the scope. If None, the function name is used.\n    scope_fn (fct): Either tf.name_scope or tf.variable_scope\n\n  Returns:\n    fct: the add_scope decorator", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def index(request):\n    \"\"\"\"\"\"\n    recent_jobs = JobRecord.objects.order_by(\"-start_time\")[0:100]\n    recent_trials = TrialRecord.objects.order_by(\"-start_time\")[0:500]\n\n    total_num = len(recent_trials)\n    running_num = sum(t.trial_status == Trial.RUNNING for t in recent_trials)\n    success_num = sum(\n        t.trial_status == Trial.TERMINATED for t in recent_trials)\n    failed_num = sum(t.trial_status == Trial.ERROR for t in recent_trials)\n\n    job_records = []\n    for recent_job in recent_jobs:\n        job_records.append(get_job_info(recent_job))\n    context = {\n        \"log_dir\": AUTOMLBOARD_LOG_DIR,\n        \"reload_interval\": AUTOMLBOARD_RELOAD_INTERVAL,\n        \"recent_jobs\": job_records,\n        \"job_num\": len(job_records),\n        \"trial_num\": total_num,\n        \"running_num\": running_num,\n        \"success_num\": success_num,\n        \"failed_num\": failed_num\n    }\n    return render(request, \"index.html\", context)", "output": "View for the home page.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _replace_local_codeuri(self):\n        \"\"\"\n        \n        \"\"\"\n\n        all_resources = self.sam_template.get(\"Resources\", {})\n\n        for _, resource in all_resources.items():\n\n            resource_type = resource.get(\"Type\")\n            resource_dict = resource.get(\"Properties\")\n\n            if resource_type == \"AWS::Serverless::Function\":\n\n                SamTemplateValidator._update_to_s3_uri(\"CodeUri\", resource_dict)\n\n            if resource_type == \"AWS::Serverless::LayerVersion\":\n\n                SamTemplateValidator._update_to_s3_uri(\"ContentUri\", resource_dict)\n\n            if resource_type == \"AWS::Serverless::Api\":\n                if \"DefinitionBody\" not in resource_dict:\n                    SamTemplateValidator._update_to_s3_uri(\"DefinitionUri\", resource_dict)", "output": "Replaces the CodeUri in AWS::Serverless::Function and DefinitionUri in AWS::Serverless::Api to a fake\n        S3 Uri. This is to support running the SAM Translator with valid values for these fields. If this in not done,\n        the template is invalid in the eyes of SAM Translator (the translator does not support local paths)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_attr(self):\n        \"\"\"  \"\"\"\n        self.values = getattr(self.attrs, self.kind_attr, None)\n        self.dtype = getattr(self.attrs, self.dtype_attr, None)\n        self.meta = getattr(self.attrs, self.meta_attr, None)\n        self.set_kind()", "output": "get the data for this column", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_calendar_event(self, calendar_id, event_id, params=None):\n        \"\"\"\n        \n        \"\"\"\n        for param in (calendar_id, event_id):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"DELETE\",\n            _make_path(\"_ml\", \"calendars\", calendar_id, \"events\", event_id),\n            params=params,\n        )", "output": "`<>`_\n\n        :arg calendar_id: The ID of the calendar to modify\n        :arg event_id: The ID of the event to remove from the calendar", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exp4(x, c, a, b, alpha):\n    \"\"\"\n    \"\"\"\n    return c - np.exp(-a*(x**alpha)+b)", "output": "exp4\n\n    Parameters\n    ----------\n    x: int\n    c: float\n    a: float\n    b: float\n    alpha: float\n\n    Returns\n    -------\n    float\n        c - np.exp(-a*(x**alpha)+b)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_task(self, task):\n        \"\"\"\n        \"\"\"\n        try:\n            if callable(task):\n                try:\n                    self.loop.create_task(task(self))\n                except TypeError:\n                    self.loop.create_task(task())\n            else:\n                self.loop.create_task(task)\n        except SanicException:\n\n            @self.listener(\"before_server_start\")\n            def run(app, loop):\n                if callable(task):\n                    try:\n                        loop.create_task(task(self))\n                    except TypeError:\n                        loop.create_task(task())\n                else:\n                    loop.create_task(task)", "output": "Schedule a task to run later, after the loop has started.\n        Different from asyncio.ensure_future in that it does not\n        also return a future, and the actual ensure_future call\n        is delayed until before server start.\n\n        :param task: future, couroutine or awaitable", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def symlink(src, link):\n    '''\n    \n    '''\n    # When Python 3.2 or later becomes the minimum version, this function can be\n    # replaced with the built-in os.symlink function, which supports Windows.\n    if sys.getwindowsversion().major < 6:\n        raise SaltInvocationError('Symlinks are only supported on Windows Vista or later.')\n\n    if not os.path.exists(src):\n        raise SaltInvocationError('The given source path does not exist.')\n\n    if not os.path.isabs(src):\n        raise SaltInvocationError('File path must be absolute.')\n\n    # ensure paths are using the right slashes\n    src = os.path.normpath(src)\n    link = os.path.normpath(link)\n\n    is_dir = os.path.isdir(src)\n\n    try:\n        win32file.CreateSymbolicLink(link, src, int(is_dir))\n        return True\n    except pywinerror as exc:\n        raise CommandExecutionError(\n            'Could not create \\'{0}\\' - [{1}] {2}'.format(\n                link,\n                exc.winerror,\n                exc.strerror\n            )\n        )", "output": "Create a symbolic link to a file\n\n    This is only supported with Windows Vista or later and must be executed by\n    a user with the SeCreateSymbolicLink privilege.\n\n    The behavior of this function matches the Unix equivalent, with one\n    exception - invalid symlinks cannot be created. The source path must exist.\n    If it doesn't, an error will be raised.\n\n    Args:\n        src (str): The path to a file or directory\n        link (str): The path to the link\n\n    Returns:\n        bool: True if successful, otherwise False\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.symlink /path/to/file /path/to/link", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pq(self) -> PyQuery:\n        \"\"\"\n        \"\"\"\n        if self._pq is None:\n            self._pq = PyQuery(self.lxml)\n\n        return self._pq", "output": "`PyQuery <https://pythonhosted.org/pyquery/>`_ representation\n        of the :class:`Element <Element>` or :class:`HTML <HTML>`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def feature_encoders(self, data_dir):\n    \"\"\"\n\n    \"\"\"\n    encoders = (super(BabiQa, self).feature_encoders(data_dir))\n    label_encoder = self.get_labels_encoder(data_dir)\n    encoders[\"targets\"] = label_encoder  # bAbi as a classification task\n    return encoders", "output": "Return a dict for encoding and decoding inference input/output.\n\n    Args:\n      data_dir: data directory\n\n    Returns:\n      A dict of <feature name, TextEncoder>.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _mutate_rows_request(table_name, rows, app_profile_id=None):\n    \"\"\"\n    \"\"\"\n    request_pb = data_messages_v2_pb2.MutateRowsRequest(\n        table_name=table_name, app_profile_id=app_profile_id\n    )\n    mutations_count = 0\n    for row in rows:\n        _check_row_table_name(table_name, row)\n        _check_row_type(row)\n        mutations = row._get_mutations()\n        request_pb.entries.add(row_key=row.row_key, mutations=mutations)\n        mutations_count += len(mutations)\n    if mutations_count > _MAX_BULK_MUTATIONS:\n        raise TooManyMutationsError(\n            \"Maximum number of mutations is %s\" % (_MAX_BULK_MUTATIONS,)\n        )\n    return request_pb", "output": "Creates a request to mutate rows in a table.\n\n    :type table_name: str\n    :param table_name: The name of the table to write to.\n\n    :type rows: list\n    :param rows: List or other iterable of :class:`.DirectRow` instances.\n\n    :type: app_profile_id: str\n    :param app_profile_id: (Optional) The unique name of the AppProfile.\n\n    :rtype: :class:`data_messages_v2_pb2.MutateRowsRequest`\n    :returns: The ``MutateRowsRequest`` protobuf corresponding to the inputs.\n    :raises: :exc:`~.table.TooManyMutationsError` if the number of mutations is\n             greater than 100,000", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def auto(name):\n    '''\n    \n\n    '''\n    ret = {'name': name,\n           'result': True,\n           'comment': '',\n           'changes': {}}\n\n    display = __salt__['alternatives.display'](name)\n    line = display.splitlines()[0]\n    if line.endswith(' auto mode'):\n        ret['comment'] = '{0} already in auto mode'.format(name)\n        return ret\n\n    if __opts__['test']:\n        ret['comment'] = '{0} will be put in auto mode'.format(name)\n        ret['result'] = None\n        return ret\n    ret['changes']['result'] = __salt__['alternatives.auto'](name)\n    return ret", "output": ".. versionadded:: 0.17.0\n\n    Instruct alternatives to use the highest priority\n    path for <name>\n\n    name\n        is the master name for this link group\n        (e.g. pager)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_toggle_view_action(self):\n        \"\"\"\"\"\"\n        title = self.get_plugin_title()\n        if self.CONF_SECTION == 'editor':\n            title = _('Editor')\n        if self.shortcut is not None:\n            action = create_action(self, title,\n                             toggled=lambda checked: self.toggle_view(checked),\n                             shortcut=QKeySequence(self.shortcut),\n                             context=Qt.WidgetShortcut)\n        else:\n            action = create_action(self, title, toggled=lambda checked:\n                                                self.toggle_view(checked))\n        self.toggle_view_action = action", "output": "Associate a toggle view action with each plugin", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_install_build_global(options, check_options=None):\n    # type: (Values, Optional[Values]) -> None\n    \"\"\"\n    \"\"\"\n    if check_options is None:\n        check_options = options\n\n    def getname(n):\n        return getattr(check_options, n, None)\n    names = [\"build_options\", \"global_options\", \"install_options\"]\n    if any(map(getname, names)):\n        control = options.format_control\n        control.disallow_binaries()\n        warnings.warn(\n            'Disabling all use of wheels due to the use of --build-options '\n            '/ --global-options / --install-options.', stacklevel=2,\n        )", "output": "Disable wheels if per-setup.py call options are set.\n\n    :param options: The OptionParser options to update.\n    :param check_options: The options to check, if not supplied defaults to\n        options.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_resource_groups(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The list_hosted_services function must be called with '\n            '-f or --function'\n        )\n\n    resconn = get_conn(client_type='resource')\n    ret = {}\n    try:\n        groups = resconn.resource_groups.list()\n\n        for group_obj in groups:\n            group = group_obj.as_dict()\n            ret[group['name']] = group\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('resource', exc.message)\n        ret = {'Error': exc.message}\n\n    return ret", "output": "List resource groups associated with the subscription", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_fine_tune_model(symbol, arg_params, num_classes, layer_name, dtype='float32'):\n    \"\"\"\n    \n    \"\"\"\n    all_layers = symbol.get_internals()\n    net = all_layers[layer_name+'_output']\n    net = mx.symbol.FullyConnected(data=net, num_hidden=num_classes, name='fc')\n    if dtype == 'float16':\n        net = mx.sym.Cast(data=net, dtype=np.float32)\n    net = mx.symbol.SoftmaxOutput(data=net, name='softmax')\n    new_args = dict({k:arg_params[k] for k in arg_params if 'fc' not in k})\n    return (net, new_args)", "output": "symbol: the pre-trained network symbol\n    arg_params: the argument parameters of the pre-trained model\n    num_classes: the number of classes for the fine-tune datasets\n    layer_name: the layer name before the last fully-connected layer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_best_encoding(stream):\n    \"\"\"\"\"\"\n    rv = getattr(stream, 'encoding', None) or sys.getdefaultencoding()\n    if is_ascii_encoding(rv):\n        return 'utf-8'\n    return rv", "output": "Returns the default stream encoding if not found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_image_decoder(targets,\n                              encoder_output,\n                              ed_attention_bias,\n                              hparams,\n                              name=None):\n  \"\"\"\n  \"\"\"\n  with tf.variable_scope(name, default_name=\"transformer_dec\"):\n    batch_size = common_layers.shape_list(targets)[0]\n    targets = tf.reshape(targets, [batch_size,\n                                   hparams.img_len,\n                                   hparams.img_len,\n                                   hparams.num_channels * hparams.hidden_size])\n    decoder_input, _, _ = cia.prepare_decoder(targets, hparams)\n    decoder_output = cia.transformer_decoder_layers(\n        decoder_input,\n        encoder_output,\n        hparams.num_decoder_layers or hparams.num_hidden_layers,\n        hparams,\n        attention_type=hparams.dec_attention_type,\n        encoder_decoder_attention_bias=ed_attention_bias,\n        name=\"decoder\")\n    decoder_output = tf.reshape(decoder_output,\n                                [batch_size,\n                                 hparams.img_len,\n                                 hparams.img_len * hparams.num_channels,\n                                 hparams.hidden_size])\n    return decoder_output", "output": "Transformer image decoder over targets with local attention.\n\n  Args:\n    targets: Tensor of shape [batch, ...], and whose size is batch * height *\n      width * hparams.num_channels * hparams.hidden_size.\n    encoder_output: Tensor of shape [batch, length_kv, hparams.hidden_size].\n    ed_attention_bias: Tensor which broadcasts with shape [batch,\n      hparams.num_heads, length_q, length_kv]. Encoder-decoder attention bias.\n    hparams: HParams.\n    name: string, variable scope.\n\n  Returns:\n    Tensor of shape [batch, height, width * hparams.num_channels,\n    hparams.hidden_size].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_requirements(self, reqs):\n        # type: (Iterable[str]) -> Tuple[Set[Tuple[str, str]], Set[str]]\n        \"\"\"\n        \"\"\"\n        missing = set()\n        conflicting = set()\n        if reqs:\n            ws = WorkingSet(self._lib_dirs)\n            for req in reqs:\n                try:\n                    if ws.find(Requirement.parse(req)) is None:\n                        missing.add(req)\n                except VersionConflict as e:\n                    conflicting.add((str(e.args[0].as_requirement()),\n                                     str(e.args[1])))\n        return conflicting, missing", "output": "Return 2 sets:\n            - conflicting requirements: set of (installed, wanted) reqs tuples\n            - missing requirements: set of reqs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_create(name, passwd, user=None, password=None, host=None, port=None,\n                database='admin', authdb=None, roles=None):\n    '''\n    \n    '''\n    conn = _connect(user, password, host, port, authdb=authdb)\n    if not conn:\n        return 'Failed to connect to mongo database'\n\n    if not roles:\n        roles = []\n\n    try:\n        log.info('Creating user %s', name)\n        mdb = pymongo.database.Database(conn, database)\n        mdb.add_user(name, passwd, roles=roles)\n    except pymongo.errors.PyMongoError as err:\n        log.error('Creating database %s failed with error: %s', name, err)\n        return six.text_type(err)\n    return True", "output": "Create a MongoDB user\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mongodb.user_create <user_name> <user_password> <roles> <user> <password> <host> <port> <database>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_param(self, params, value=None):\n        \"\"\"\n        \"\"\"\n        if isinstance(params, Mapping):\n            params = params.items()\n        elif isinstance(params, STRING_TYPES) and value is not None:\n            params = [(params, value)]\n        for key, val in params:\n            _check_call(_LIB.XGBoosterSetParam(self.handle, c_str(key), c_str(str(val))))", "output": "Set parameters into the Booster.\n\n        Parameters\n        ----------\n        params: dict/list/str\n           list of key,value pairs, dict of key to value or simply str key\n        value: optional\n           value of the specified parameter, when params is str key", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def img2img_transformer2d_n31():\n  \"\"\"\"\"\"\n  hparams = img2img_transformer2d_base()\n  hparams.batch_size = 1\n  hparams.num_encoder_layers = 6\n  hparams.num_decoder_layers = 12\n  hparams.num_heads = 8\n  hparams.query_shape = (16, 32)\n  hparams.memory_flange = (16, 32)\n  return hparams", "output": "Set of hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_size(self):\n        '''\n        \n        '''\n        if self.graph_type == LayerType.attention.value or \\\n            LayerType.rnn.value or LayerType.self_attention.value:\n            self.size = None", "output": "Clear size", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transform_first(self, fn, lazy=True):\n        \"\"\"\n        \"\"\"\n        return self.transform(_TransformFirstClosure(fn), lazy)", "output": "Returns a new dataset with the first element of each sample\n        transformed by the transformer function `fn`.\n\n        This is useful, for example, when you only want to transform data\n        while keeping label as is.\n\n        Parameters\n        ----------\n        fn : callable\n            A transformer function that takes the first elemtn of a sample\n            as input and returns the transformed element.\n        lazy : bool, default True\n            If False, transforms all samples at once. Otherwise,\n            transforms each sample on demand. Note that if `fn`\n            is stochastic, you must set lazy to True or you will\n            get the same result on all epochs.\n\n        Returns\n        -------\n        Dataset\n            The transformed dataset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_block_symbol_data(editor, block):\n    \"\"\"\n    \n    \"\"\"\n    def list_symbols(editor, block, character):\n        \"\"\"\n        Retuns  a list of symbols found in the block text\n\n        :param editor: code editor instance\n        :param block: block to parse\n        :param character: character to look for.\n        \"\"\"\n        text = block.text()\n        symbols = []\n        cursor = QTextCursor(block)\n        cursor.movePosition(cursor.StartOfBlock)\n        pos = text.find(character, 0)\n        cursor.movePosition(cursor.Right, cursor.MoveAnchor, pos)\n\n        while pos != -1:\n            if not TextHelper(editor).is_comment_or_string(cursor):\n                # skips symbols in string literal or comment\n                info = ParenthesisInfo(pos, character)\n                symbols.append(info)\n            pos = text.find(character, pos + 1)\n            cursor.movePosition(cursor.StartOfBlock)\n            cursor.movePosition(cursor.Right, cursor.MoveAnchor, pos)\n        return symbols\n\n    parentheses = sorted(\n        list_symbols(editor, block, '(') + list_symbols(editor, block, ')'),\n        key=lambda x: x.position)\n    square_brackets = sorted(\n        list_symbols(editor, block, '[') + list_symbols(editor, block, ']'),\n        key=lambda x: x.position)\n    braces = sorted(\n        list_symbols(editor, block, '{') + list_symbols(editor, block, '}'),\n        key=lambda x: x.position)\n    return parentheses, square_brackets, braces", "output": "Gets the list of ParenthesisInfo for specific text block.\n\n    :param editor: Code editor instance\n    :param block: block to parse", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_operators(num, target):\n    \"\"\"\n    \n    \"\"\"\n\n    def dfs(res, path, num, target, pos, prev, multed):\n        if pos == len(num):\n            if target == prev:\n                res.append(path)\n            return\n        for i in range(pos, len(num)):\n            if i != pos and num[pos] == '0':  # all digits have to be used\n                break\n            cur = int(num[pos:i+1])\n            if pos == 0:\n                dfs(res, path + str(cur), num, target, i+1, cur, cur)\n            else:\n                dfs(res, path + \"+\" + str(cur), num, target,\n                    i+1, prev + cur, cur)\n                dfs(res, path + \"-\" + str(cur), num, target,\n                    i+1, prev - cur, -cur)\n                dfs(res, path + \"*\" + str(cur), num, target,\n                    i+1, prev - multed + multed * cur, multed * cur)\n\n    res = []\n    if not num:\n        return res\n    dfs(res, \"\", num, target, 0, 0, 0)\n    return res", "output": ":type num: str\n    :type target: int\n    :rtype: List[str]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put(bucket, path=None, return_bin=False, action=None, local_file=None,\n        key=None, keyid=None, service_url=None, verify_ssl=None,\n        kms_keyid=None, location=None, role_arn=None, path_style=None,\n        https_enable=None, headers=None, full_headers=False):\n    '''\n    \n    '''\n\n    if not headers:\n        headers = {}\n    else:\n        full_headers = True\n\n    key, keyid, service_url, verify_ssl, kms_keyid, location, role_arn, path_style, https_enable = _get_key(\n        key,\n        keyid,\n        service_url,\n        verify_ssl,\n        kms_keyid,\n        location,\n        role_arn,\n        path_style,\n        https_enable,\n    )\n\n    return __utils__['s3.query'](method='PUT',\n                                 bucket=bucket,\n                                 path=path,\n                                 return_bin=return_bin,\n                                 local_file=local_file,\n                                 action=action,\n                                 key=key,\n                                 keyid=keyid,\n                                 kms_keyid=kms_keyid,\n                                 service_url=service_url,\n                                 verify_ssl=verify_ssl,\n                                 location=location,\n                                 role_arn=role_arn,\n                                 path_style=path_style,\n                                 https_enable=https_enable,\n                                 headers=headers,\n                                 full_headers=full_headers)", "output": "Create a new bucket, or upload an object to a bucket.\n\n    CLI Example to create a bucket:\n\n    .. code-block:: bash\n\n        salt myminion s3.put mybucket\n\n    CLI Example to upload an object to a bucket:\n\n    .. code-block:: bash\n\n        salt myminion s3.put mybucket remotepath local_file=/path/to/file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _toolkit_serialize_summary_struct(model, sections, section_titles):\n    \"\"\"\n      \n    \"\"\"\n    output_dict = dict()\n    output_dict['sections'] = [ [ ( field[0], __extract_model_summary_value(model, field[1]) ) \\\n                                                                            for field in section ]\n                                                                            for section in sections ]\n    output_dict['section_titles'] = section_titles\n    return output_dict", "output": "Serialize model summary into a dict with ordered lists of sections and section titles\n\n    Parameters\n    ----------\n    model : Model object\n    sections : Ordered list of lists (sections) of tuples (field,value)\n      [\n        [(field1, value1), (field2, value2)],\n        [(field3, value3), (field4, value4)],\n\n      ]\n    section_titles : Ordered list of section titles\n\n\n    Returns\n    -------\n    output_dict : A dict with two entries:\n                    'sections' : ordered list with tuples of the form ('label',value)\n                    'section_titles' : ordered list of section labels", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _createFromLocal(self, data, schema):\n        \"\"\"\n        \n        \"\"\"\n        # make sure data could consumed multiple times\n        if not isinstance(data, list):\n            data = list(data)\n\n        if schema is None or isinstance(schema, (list, tuple)):\n            struct = self._inferSchemaFromList(data, names=schema)\n            converter = _create_converter(struct)\n            data = map(converter, data)\n            if isinstance(schema, (list, tuple)):\n                for i, name in enumerate(schema):\n                    struct.fields[i].name = name\n                    struct.names[i] = name\n            schema = struct\n\n        elif not isinstance(schema, StructType):\n            raise TypeError(\"schema should be StructType or list or None, but got: %s\" % schema)\n\n        # convert python objects to sql data\n        data = [schema.toInternal(row) for row in data]\n        return self._sc.parallelize(data), schema", "output": "Create an RDD for DataFrame from a list or pandas.DataFrame, returns\n        the RDD and schema.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imagetransformer_b10l_4h_big_uncond_dr03_tpu():\n  \"\"\"\"\"\"\n  hparams = imagetransformer_bas8l_8h_big_uncond_dr03_imgnet()\n  update_hparams_for_tpu(hparams)\n  hparams.batch_size = 4\n  hparams.num_heads = 4   # heads are expensive on tpu\n  hparams.num_decoder_layers = 10\n  hparams.block_length = 128\n  hparams.hidden_size = 512\n  hparams.filter_size = 1024\n  hparams.learning_rate = 0.2\n  hparams.layer_preprocess_sequence = \"none\"\n  hparams.layer_postprocess_sequence = \"dan\"\n  return hparams", "output": "Small model for tpu cifar 10.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def addPyFile(self, path):\n        \"\"\"\n        \n        \"\"\"\n        self.addFile(path)\n        (dirname, filename) = os.path.split(path)  # dirname may be directory or HDFS/S3 prefix\n        if filename[-4:].lower() in self.PACKAGE_EXTENSIONS:\n            self._python_includes.append(filename)\n            # for tests in local mode\n            sys.path.insert(1, os.path.join(SparkFiles.getRootDirectory(), filename))\n        if sys.version > '3':\n            import importlib\n            importlib.invalidate_caches()", "output": "Add a .py or .zip dependency for all tasks to be executed on this\n        SparkContext in the future.  The C{path} passed can be either a local\n        file, a file in HDFS (or other Hadoop-supported filesystems), or an\n        HTTP, HTTPS or FTP URI.\n\n        .. note:: A path can be added only once. Subsequent additions of the same path are ignored.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_kernel_error(self, error):\r\n        \"\"\"\"\"\"\r\n        # Replace end of line chars with <br>\r\n        eol = sourcecode.get_eol_chars(error)\r\n        if eol:\r\n            error = error.replace(eol, '<br>')\r\n\r\n        # Don't break lines in hyphens\r\n        # From https://stackoverflow.com/q/7691569/438386\r\n        error = error.replace('-', '&#8209')\r\n\r\n        # Create error page\r\n        message = _(\"An error ocurred while starting the kernel\")\r\n        kernel_error_template = Template(KERNEL_ERROR)\r\n        self.info_page = kernel_error_template.substitute(\r\n            css_path=self.css_path,\r\n            message=message,\r\n            error=error)\r\n\r\n        # Show error\r\n        self.set_info_page()\r\n        self.shellwidget.hide()\r\n        self.infowidget.show()\r\n\r\n        # Tell the client we're in error mode\r\n        self.is_error_shown = True", "output": "Show kernel initialization errors in infowidget.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_history(self, filename, color_scheme, font, wrap):\n        \"\"\"\n        \n        \"\"\"\n        filename = encoding.to_unicode_from_fs(filename)\n        if filename in self.filenames:\n            return\n        editor = codeeditor.CodeEditor(self)\n        if osp.splitext(filename)[1] == '.py':\n            language = 'py'\n        else:\n            language = 'bat'\n        editor.setup_editor(linenumbers=False,\n                            language=language,\n                            scrollflagarea=False,\n                            show_class_func_dropdown=False)\n        editor.focus_changed.connect(lambda: self.focus_changed.emit())\n        editor.setReadOnly(True)\n\n        editor.set_font(font, color_scheme)\n        editor.toggle_wrap_mode(wrap)\n\n        text, _ = encoding.read(filename)\n        editor.set_text(text)\n        editor.set_cursor_position('eof')\n\n        self.editors.append(editor)\n        self.filenames.append(filename)\n        index = self.tabwidget.addTab(editor, osp.basename(filename))\n        self.find_widget.set_editor(editor)\n        self.tabwidget.setTabToolTip(index, filename)\n        self.tabwidget.setCurrentIndex(index)", "output": "Add new history tab.\n\n        Args:\n            filename (str): file to be loaded in a new tab.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def range(self, start, end=None, step=1, numSlices=None):\n        \"\"\"\n        \n        \"\"\"\n        if end is None:\n            end = start\n            start = 0\n\n        return self.parallelize(xrange(start, end, step), numSlices)", "output": "Create a new RDD of int containing elements from `start` to `end`\n        (exclusive), increased by `step` every element. Can be called the same\n        way as python's built-in range() function. If called with a single argument,\n        the argument is interpreted as `end`, and `start` is set to 0.\n\n        :param start: the start value\n        :param end: the end value (exclusive)\n        :param step: the incremental step (default: 1)\n        :param numSlices: the number of partitions of the new RDD\n        :return: An RDD of int\n\n        >>> sc.range(5).collect()\n        [0, 1, 2, 3, 4]\n        >>> sc.range(2, 4).collect()\n        [2, 3]\n        >>> sc.range(1, 7, 2).collect()\n        [1, 3, 5]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def join_images(img_files, out_file):\n    \"\"\"\"\"\"\n    images = [PIL.Image.open(f) for f in img_files]\n    joined = PIL.Image.new(\n        'RGB',\n        (sum(i.size[0] for i in images), max(i.size[1] for i in images))\n    )\n    left = 0\n    for img in images:\n        joined.paste(im=img, box=(left, 0))\n        left = left + img.size[0]\n    joined.save(out_file)", "output": "Join the list of images into the out file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_images(call=None):\n    '''\n    \n    '''\n    all_servers = list_nodes_full()\n    templates = {}\n    for server in all_servers:\n        if server[\"IsTemplate\"]:\n            templates.update({\"Template Name\": server[\"Name\"]})\n    return templates", "output": "returns a list of images available to you", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forward(self, src_seq, tgt_seq, src_valid_length=None, tgt_valid_length=None):  #pylint: disable=arguments-differ\n        \"\"\"\n        \"\"\"\n        additional_outputs = []\n        encoder_outputs, encoder_additional_outputs = self.encode(src_seq,\n                                                                  valid_length=src_valid_length)\n        decoder_states = self.decoder.init_state_from_encoder(encoder_outputs,\n                                                              encoder_valid_length=src_valid_length)\n        outputs, _, decoder_additional_outputs =\\\n            self.decode_seq(tgt_seq, decoder_states, tgt_valid_length)\n        additional_outputs.append(encoder_additional_outputs)\n        additional_outputs.append(decoder_additional_outputs)\n        return outputs, additional_outputs", "output": "Generate the prediction given the src_seq and tgt_seq.\n\n        This is used in training an NMT model.\n\n        Parameters\n        ----------\n        src_seq : NDArray\n        tgt_seq : NDArray\n        src_valid_length : NDArray or None\n        tgt_valid_length : NDArray or None\n\n        Returns\n        -------\n        outputs : NDArray\n            Shape (batch_size, tgt_length, tgt_word_num)\n        additional_outputs : list of list\n            Additional outputs of encoder and decoder, e.g, the attention weights", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _config():\n    '''\n    \n    '''\n    status_url = __salt__['config.get']('nagios.status_url') or \\\n        __salt__['config.get']('nagios:status_url')\n    if not status_url:\n        raise CommandExecutionError('Missing Nagios URL in the configuration.')\n\n    username = __salt__['config.get']('nagios.username') or \\\n        __salt__['config.get']('nagios:username')\n    password = __salt__['config.get']('nagios.password') or \\\n        __salt__['config.get']('nagios:password')\n    return {\n        'url': status_url,\n        'username': username,\n        'password': password\n    }", "output": "Get configuration items for URL, Username and Password", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def min_depth(self, root):\n    \"\"\"\n    \n    \"\"\"\n    if root is None:\n        return 0\n    if root.left is not None or root.right is not None:\n        return max(self.minDepth(root.left), self.minDepth(root.right))+1\n    return min(self.minDepth(root.left), self.minDepth(root.right)) + 1", "output": ":type root: TreeNode\n    :rtype: int", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _try_cast_result(self, result, dtype=None):\n        \"\"\" \n        \"\"\"\n        if dtype is None:\n            dtype = self.dtype\n\n        if self.is_integer or self.is_bool or self.is_datetime:\n            pass\n        elif self.is_float and result.dtype == self.dtype:\n\n            # protect against a bool/object showing up here\n            if isinstance(dtype, str) and dtype == 'infer':\n                return result\n            if not isinstance(dtype, type):\n                dtype = dtype.type\n            if issubclass(dtype, (np.bool_, np.object_)):\n                if issubclass(dtype, np.bool_):\n                    if isna(result).all():\n                        return result.astype(np.bool_)\n                    else:\n                        result = result.astype(np.object_)\n                        result[result == 1] = True\n                        result[result == 0] = False\n                        return result\n                else:\n                    return result.astype(np.object_)\n\n            return result\n\n        # may need to change the dtype here\n        return maybe_downcast_to_dtype(result, dtype)", "output": "try to cast the result to our original type, we may have\n        roundtripped thru object in the mean-time", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def short_doc(self):\n        \"\"\"\n        \"\"\"\n        if self.brief is not None:\n            return self.brief\n        if self.help is not None:\n            return self.help.split('\\n', 1)[0]\n        return ''", "output": "Gets the \"short\" documentation of a command.\n\n        By default, this is the :attr:`brief` attribute.\n        If that lookup leads to an empty string then the first line of the\n        :attr:`help` attribute is used instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def listdict2envdict(listdict):\r\n    \"\"\"\"\"\"\r\n    for key in listdict:\r\n        if isinstance(listdict[key], list):\r\n            listdict[key] = os.path.pathsep.join(listdict[key])\r\n    return listdict", "output": "Dict of lists --> Dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zip_nested(arg0, *args, **kwargs):\n  \"\"\"\"\"\"\n  # Python 2 do not support kwargs only arguments\n  dict_only = kwargs.pop(\"dict_only\", False)\n  assert not kwargs\n\n  # Could add support for more exotic data_struct, like OrderedDict\n  if isinstance(arg0, dict):\n    return {\n        k: zip_nested(*a, dict_only=dict_only) for k, a in zip_dict(arg0, *args)\n    }\n  elif not dict_only:\n    if isinstance(arg0, list):\n      return [zip_nested(*a, dict_only=dict_only) for a in zip(arg0, *args)]\n  # Singleton\n  return (arg0,) + args", "output": "Zip data struct together and return a data struct with the same shape.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def undeploy(jboss_config, deployment):\n    '''\n    \n       '''\n    log.debug(\"======================== MODULE FUNCTION: jboss7.undeploy, deployment=%s\", deployment)\n    command = 'undeploy {deployment} '.format(deployment=deployment)\n    return __salt__['jboss7_cli.run_command'](jboss_config, command)", "output": "Undeploy the application from jboss instance\n\n    jboss_config\n        Configuration dictionary with properties specified above.\n    deployment\n        Deployment name to undeploy\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' jboss7.undeploy '{\"cli_path\": \"integration.modules.sysmod.SysModuleTest.test_valid_docs\", \"controller\": \"10.11.12.13:9999\", \"cli_user\": \"jbossadm\", \"cli_password\": \"jbossadm\"}' my_deployment", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_offset_day(self, other):\n        \"\"\"\n        \n        \"\"\"\n        mstart = datetime(other.year, other.month, 1)\n        wday = mstart.weekday()\n        shift_days = (self.weekday - wday) % 7\n        return 1 + shift_days + self.week * 7", "output": "Find the day in the same month as other that has the same\n        weekday as self.weekday and is the self.week'th such day in the month.\n\n        Parameters\n        ----------\n        other : datetime\n\n        Returns\n        -------\n        day : int", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dhcp_options(dhcp_options_name=None, dhcp_options_id=None,\n                     region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    if not any((dhcp_options_name, dhcp_options_id)):\n        raise SaltInvocationError('At least one of the following must be specified: '\n                                  'dhcp_options_name, dhcp_options_id.')\n\n    if not dhcp_options_id and dhcp_options_name:\n        dhcp_options_id = _get_resource_id('dhcp_options', dhcp_options_name,\n                                            region=region, key=key,\n                                            keyid=keyid, profile=profile)\n    if not dhcp_options_id:\n        return {'dhcp_options': {}}\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        r = conn.get_all_dhcp_options(dhcp_options_ids=[dhcp_options_id])\n    except BotoServerError as e:\n        return {'error': __utils__['boto.get_error'](e)}\n\n    if not r:\n        return {'dhcp_options': None}\n\n    keys = ('domain_name', 'domain_name_servers', 'ntp_servers',\n            'netbios_name_servers', 'netbios_node_type')\n\n    return {'dhcp_options': dict((k, r[0].options.get(k)) for k in keys)}", "output": "Return a dict with the current values of the requested DHCP options set\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_vpc.get_dhcp_options 'myfunnydhcpoptionsname'\n\n    .. versionadded:: 2016.3.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ngroup(self, ascending=True):\n        \"\"\"\n        \n        \"\"\"\n\n        with _group_selection_context(self):\n            index = self._selected_obj.index\n            result = Series(self.grouper.group_info[0], index)\n            if not ascending:\n                result = self.ngroups - 1 - result\n            return result", "output": "Number each group from 0 to the number of groups - 1.\n\n        This is the enumerative complement of cumcount.  Note that the\n        numbers given to the groups match the order in which the groups\n        would be seen when iterating over the groupby object, not the\n        order they are first observed.\n\n        .. versionadded:: 0.20.2\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from number of group - 1 to 0.\n\n        See Also\n        --------\n        .cumcount : Number the rows in each group.\n\n        Examples\n        --------\n\n        >>> df = pd.DataFrame({\"A\": list(\"aaabba\")})\n        >>> df\n           A\n        0  a\n        1  a\n        2  a\n        3  b\n        4  b\n        5  a\n        >>> df.groupby('A').ngroup()\n        0    0\n        1    0\n        2    0\n        3    1\n        4    1\n        5    0\n        dtype: int64\n        >>> df.groupby('A').ngroup(ascending=False)\n        0    1\n        1    1\n        2    1\n        3    0\n        4    0\n        5    1\n        dtype: int64\n        >>> df.groupby([\"A\", [1,1,2,3,2,1]]).ngroup()\n        0    0\n        1    0\n        2    1\n        3    3\n        4    2\n        5    0\n        dtype: int64", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_aws_format(tags):\n    \"\"\"\"\"\"\n\n    if TAG_RAY_NODE_NAME in tags:\n        tags[\"Name\"] = tags[TAG_RAY_NODE_NAME]\n        del tags[TAG_RAY_NODE_NAME]\n    return tags", "output": "Convert the Ray node name tag to the AWS-specific 'Name' tag.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_dl(self, rows, col_max=30, col_spacing=2):\n        \"\"\"\n        \"\"\"\n        rows = list(rows)\n        widths = measure_table(rows)\n        if len(widths) != 2:\n            raise TypeError('Expected two columns for definition list')\n\n        first_col = min(widths[0], col_max) + col_spacing\n\n        for first, second in iter_rows(rows, len(widths)):\n            self.write('%*s%s' % (self.current_indent, '', first))\n            if not second:\n                self.write('\\n')\n                continue\n            if term_len(first) <= first_col - col_spacing:\n                self.write(' ' * (first_col - term_len(first)))\n            else:\n                self.write('\\n')\n                self.write(' ' * (first_col + self.current_indent))\n\n            text_width = max(self.width - first_col - 2, 10)\n            lines = iter(wrap_text(second, text_width).splitlines())\n            if lines:\n                self.write(next(lines) + '\\n')\n                for line in lines:\n                    self.write('%*s%s\\n' % (\n                        first_col + self.current_indent, '', line))\n            else:\n                self.write('\\n')", "output": "Writes a definition list into the buffer.  This is how options\n        and commands are usually formatted.\n\n        :param rows: a list of two item tuples for the terms and values.\n        :param col_max: the maximum width of the first column.\n        :param col_spacing: the number of spaces between the first and\n                            second column.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def combine(self, blocks, copy=True):\n        \"\"\"  \"\"\"\n        if len(blocks) == 0:\n            return self.make_empty()\n\n        # FIXME: optimization potential\n        indexer = np.sort(np.concatenate([b.mgr_locs.as_array\n                                          for b in blocks]))\n        inv_indexer = lib.get_reverse_indexer(indexer, self.shape[0])\n\n        new_blocks = []\n        for b in blocks:\n            b = b.copy(deep=copy)\n            b.mgr_locs = algos.take_1d(inv_indexer, b.mgr_locs.as_array,\n                                       axis=0, allow_fill=False)\n            new_blocks.append(b)\n\n        axes = list(self.axes)\n        axes[0] = self.items.take(indexer)\n\n        return self.__class__(new_blocks, axes, do_integrity_check=False)", "output": "return a new manager with the blocks", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_boolean_cli_param(params, key, value):\n    '''\n    \n    '''\n    if value is True:\n        params.append('--{0}'.format(key))", "output": "Adds key as a command line parameter to params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def policy_assignments_list(**kwargs):\n    '''\n    \n\n    '''\n    result = {}\n    polconn = __utils__['azurearm.get_client']('policy', **kwargs)\n    try:\n        policy_assign = __utils__['azurearm.paged_object_to_list'](polconn.policy_assignments.list())\n\n        for assign in policy_assign:\n            result[assign['name']] = assign\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('resource', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    List all policy assignments for a subscription.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_resource.policy_assignments_list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_util_get_trade_datetime(dt=datetime.datetime.now()):\n    \"\"\"\n    \"\"\"\n\n    #dt= datetime.datetime.now()\n\n    if QA_util_if_trade(str(dt.date())) and dt.time() < datetime.time(15, 0, 0):\n        return str(dt.date())\n    else:\n        return QA_util_get_real_date(str(dt.date()), trade_date_sse, 1)", "output": "\u4ea4\u6613\u7684\u771f\u5b9e\u65e5\u671f\n\n    Returns:\n        [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def active(extended=False):\n    '''\n    \n    '''\n    ret = {}\n    if __grains__['os'] == 'FreeBSD':\n        _active_mounts_freebsd(ret)\n    elif 'AIX' in __grains__['kernel']:\n        _active_mounts_aix(ret)\n    elif __grains__['kernel'] == 'SunOS':\n        _active_mounts_solaris(ret)\n    elif __grains__['os'] == 'OpenBSD':\n        _active_mounts_openbsd(ret)\n    elif __grains__['os'] in ['MacOS', 'Darwin']:\n        _active_mounts_darwin(ret)\n    else:\n        if extended:\n            try:\n                _active_mountinfo(ret)\n            except CommandExecutionError:\n                _active_mounts(ret)\n        else:\n            _active_mounts(ret)\n    return ret", "output": "List the active mounts.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mount.active", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def profile(curr_step, start_step, end_step, profile_name='profile.json',\n            early_exit=True):\n    \"\"\"\"\"\"\n    if curr_step == start_step:\n        mx.nd.waitall()\n        mx.profiler.set_config(profile_memory=False, profile_symbolic=True,\n                               profile_imperative=True, filename=profile_name,\n                               aggregate_stats=True)\n        mx.profiler.set_state('run')\n    elif curr_step == end_step:\n        mx.nd.waitall()\n        mx.profiler.set_state('stop')\n        logging.info(mx.profiler.dumps())\n        mx.profiler.dump()\n        if early_exit:\n            exit()", "output": "profile the program between [start_step, end_step).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main():\n    \"\"\"\"\"\"\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\n        '--boost_dir',\n        required=False,\n        type=existing_path,\n        help='The path to the include/boost directory of Metaparse'\n    )\n    parser.add_argument(\n        '--max_length_limit',\n        required=False,\n        default=2048,\n        type=positive_integer,\n        help='The maximum supported length limit'\n    )\n    parser.add_argument(\n        '--length_limit_step',\n        required=False,\n        default=128,\n        type=positive_integer,\n        help='The longest step at which headers are generated'\n    )\n    args = parser.parse_args()\n\n    if args.boost_dir is None:\n        tools_path = os.path.dirname(os.path.abspath(__file__))\n        boost_dir = os.path.join(\n            os.path.dirname(tools_path),\n            'include',\n            'boost'\n        )\n    else:\n        boost_dir = args.boost_dir\n\n    if args.max_length_limit < 1:\n        sys.stderr.write('Invalid maximum length limit')\n        sys.exit(-1)\n\n    generate_string(\n        os.path.join(\n            boost_dir,\n            'metaparse',\n            'v{0}'.format(VERSION),\n            'cpp11',\n            'impl'\n        ),\n        length_limits(args.max_length_limit, args.length_limit_step)\n    )", "output": "The main function of the script", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def as_tuple(ireq):\n    \"\"\"\n    \n    \"\"\"\n\n    if not is_pinned_requirement(ireq):\n        raise TypeError(\"Expected a pinned InstallRequirement, got {}\".format(ireq))\n\n    name = key_from_req(ireq.req)\n    version = first(ireq.specifier._specs)._spec[1]\n    extras = tuple(sorted(ireq.extras))\n    return name, version, extras", "output": "Pulls out the (name: str, version:str, extras:(str)) tuple from the pinned InstallRequirement.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_to_uid(user):\n    '''\n    \n    '''\n    if user is None:\n        user = salt.utils.user.get_user()\n\n    return salt.utils.win_dacl.get_sid_string(user)", "output": "Convert user name to a uid\n\n    Args:\n        user (str): The user to lookup\n\n    Returns:\n        str: The user id of the user\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.user_to_uid myusername", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remote_run(cmd, instance_name, detach=False, retries=1):\n  \"\"\"\"\"\"\n  if detach:\n    cmd = SCREEN.format(command=cmd)\n  args = SSH.format(instance_name=instance_name).split()\n  args.append(cmd)\n  for i in range(retries + 1):\n    try:\n      if i > 0:\n        tf.logging.info(\"Retry %d for %s\", i, args)\n      return sp.check_call(args)\n    except sp.CalledProcessError as e:\n      if i == retries:\n        raise e", "output": "Run command on GCS instance, optionally detached.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def conflicts_with_a_neighbouring_module(directory_path):\n    \"\"\"\n    \n    \"\"\"\n    parent_dir_path, current_dir_name = os.path.split(os.path.normpath(directory_path))\n    neighbours = os.listdir(parent_dir_path)\n    conflicting_neighbour_filename = current_dir_name+'.py'\n    return conflicting_neighbour_filename in neighbours", "output": "Checks if a directory lies in the same directory as a .py file with the same name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def collection(self, collection_id):\n        \"\"\"\n        \"\"\"\n        child_path = self._path + (collection_id,)\n        return self._client.collection(*child_path)", "output": "Create a sub-collection underneath the current document.\n\n        Args:\n            collection_id (str): The sub-collection identifier (sometimes\n                referred to as the \"kind\").\n\n        Returns:\n            ~.firestore_v1beta1.collection.CollectionReference: The\n            child collection.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def connect_timeout(self):\n        \"\"\" \n        \"\"\"\n        if self.total is None:\n            return self._connect\n\n        if self._connect is None or self._connect is self.DEFAULT_TIMEOUT:\n            return self.total\n\n        return min(self._connect, self.total)", "output": "Get the value to use when setting a connection timeout.\n\n        This will be a positive float or integer, the value None\n        (never timeout), or the default system timeout.\n\n        :return: Connect timeout.\n        :rtype: int, float, :attr:`Timeout.DEFAULT_TIMEOUT` or None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _plaintext_data_key():\n    '''\n    \n    '''\n    response = getattr(_plaintext_data_key, 'response', None)\n    cache_hit = response is not None\n    if not cache_hit:\n        response = _api_decrypt()\n        setattr(_plaintext_data_key, 'response', response)\n    key_id = response['KeyId']\n    plaintext = response['Plaintext']\n    if hasattr(plaintext, 'encode'):\n        plaintext = plaintext.encode(__salt_system_encoding__)\n    log.debug('Using key %s from %s', key_id, 'cache' if cache_hit else 'api call')\n    return plaintext", "output": "Return the configured KMS data key decrypted and encoded in urlsafe base64.\n\n    Cache the result to minimize API calls to AWS.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_default(self, ctx):\n        \"\"\"\"\"\"\n        # Otherwise go with the regular default.\n        if callable(self.default):\n            rv = self.default()\n        else:\n            rv = self.default\n        return self.type_cast_value(ctx, rv)", "output": "Given a context variable this calculates the default value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_api_repr(self):\n        \"\"\"\n        \"\"\"\n        values = self.values\n        if self.array_type == \"RECORD\" or self.array_type == \"STRUCT\":\n            reprs = [value.to_api_repr() for value in values]\n            a_type = reprs[0][\"parameterType\"]\n            a_values = [repr_[\"parameterValue\"] for repr_ in reprs]\n        else:\n            a_type = {\"type\": self.array_type}\n            converter = _SCALAR_VALUE_TO_JSON_PARAM.get(self.array_type)\n            if converter is not None:\n                values = [converter(value) for value in values]\n            a_values = [{\"value\": value} for value in values]\n        resource = {\n            \"parameterType\": {\"type\": \"ARRAY\", \"arrayType\": a_type},\n            \"parameterValue\": {\"arrayValues\": a_values},\n        }\n        if self.name is not None:\n            resource[\"name\"] = self.name\n        return resource", "output": "Construct JSON API representation for the parameter.\n\n        :rtype: dict\n        :returns: JSON mapping", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def recoverFile(filename):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlRecoverFile(filename)\n    if ret is None:raise treeError('xmlRecoverFile() failed')\n    return xmlDoc(_obj=ret)", "output": "parse an XML file and build a tree. Automatic support for\n      ZLIB/Compress compressed document is provided by default if\n      found at compile-time. In the case the document is not Well\n       Formed, it attempts to build a tree anyway", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_sentence_vector(self, text):\n        \"\"\"\n        \n        \"\"\"\n        if text.find('\\n') != -1:\n            raise ValueError(\n                \"predict processes one line at a time (remove \\'\\\\n\\')\"\n            )\n        text += \"\\n\"\n        dim = self.get_dimension()\n        b = fasttext.Vector(dim)\n        self.f.getSentenceVector(b, text)\n        return np.array(b)", "output": "Given a string, get a single vector represenation. This function\n        assumes to be given a single line of text. We split words on\n        whitespace (space, newline, tab, vertical tab) and the control\n        characters carriage return, formfeed and the null character.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _client_wrapper(attr, *args, **kwargs):\n    '''\n    \n    '''\n    catch_api_errors = kwargs.pop('catch_api_errors', True)\n    func = getattr(__context__['docker.client'], attr, None)\n    if func is None or not hasattr(func, '__call__'):\n        raise SaltInvocationError('Invalid client action \\'{0}\\''.format(attr))\n    if attr in ('push', 'pull'):\n        try:\n            # Refresh auth config from config.json\n            __context__['docker.client'].reload_config()\n        except AttributeError:\n            pass\n    err = ''\n    try:\n        log.debug(\n            'Attempting to run docker-py\\'s \"%s\" function '\n            'with args=%s and kwargs=%s', attr, args, kwargs\n        )\n        ret = func(*args, **kwargs)\n    except docker.errors.APIError as exc:\n        if catch_api_errors:\n            # Generic handling of Docker API errors\n            raise CommandExecutionError(\n                'Error {0}: {1}'.format(exc.response.status_code,\n                                        exc.explanation)\n            )\n        else:\n            # Allow API errors to be caught further up the stack\n            raise\n    except docker.errors.DockerException as exc:\n        # More general docker exception (catches InvalidVersion, etc.)\n        raise CommandExecutionError(exc.__str__())\n    except Exception as exc:\n        err = exc.__str__()\n    else:\n        return ret\n\n    # If we're here, it's because an exception was caught earlier, and the\n    # API command failed.\n    msg = 'Unable to perform {0}'.format(attr)\n    if err:\n        msg += ': {0}'.format(err)\n    raise CommandExecutionError(msg)", "output": "Common functionality for running low-level API calls", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tokenize(qp_pair, tokenizer=None, is_training=False):\n    '''\n    \n    '''\n    question_tokens = tokenizer.tokenize(qp_pair['question'])\n    passage_tokens = tokenizer.tokenize(qp_pair['passage'])\n    if is_training:\n        question_tokens = question_tokens[:300]\n        passage_tokens = passage_tokens[:300]\n    passage_tokens.insert(\n        0, {'word': '<BOS>', 'original_text': '<BOS>', 'char_begin': 0, 'char_end': 0})\n    passage_tokens.append(\n        {'word': '<EOS>', 'original_text': '<EOS>', 'char_begin': 0, 'char_end': 0})\n    qp_pair['question_tokens'] = question_tokens\n    qp_pair['passage_tokens'] = passage_tokens", "output": "tokenize function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def safe_range(*args):\n    \"\"\"\n    \"\"\"\n    rng = range(*args)\n    if len(rng) > MAX_RANGE:\n        raise OverflowError('range too big, maximum size for range is %d' %\n                            MAX_RANGE)\n    return rng", "output": "A range that can't generate ranges with a length of more than\n    MAX_RANGE items.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _to_corrected_pandas_type(dt):\n    \"\"\"\n    \n    \"\"\"\n    import numpy as np\n    if type(dt) == ByteType:\n        return np.int8\n    elif type(dt) == ShortType:\n        return np.int16\n    elif type(dt) == IntegerType:\n        return np.int32\n    elif type(dt) == FloatType:\n        return np.float32\n    else:\n        return None", "output": "When converting Spark SQL records to Pandas DataFrame, the inferred data type may be wrong.\n    This method gets the corrected data type for Pandas if that type may be inferred uncorrectly.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _start_reader_thread(self, stream, chunks):\n    \"\"\"\n    \"\"\"\n    import io  # pylint: disable=g-import-not-at-top\n    import threading  # pylint: disable=g-import-not-at-top\n    def target():\n      while True:\n        chunk = stream.read(io.DEFAULT_BUFFER_SIZE)\n        if not chunk:\n          break\n        chunks.append(chunk)\n    thread = threading.Thread(target=target)\n    thread.start()\n    return thread", "output": "Starts a thread for reading output from FFMPEG.\n\n    The thread reads consecutive chunks from the stream and saves them in\n    the given list.\n\n    Args:\n      stream: output stream of the FFMPEG process.\n      chunks: list to save output chunks to.\n\n    Returns:\n      Thread", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_dependencies_from_pip(ireq, sources):\n    \"\"\"\n    \"\"\"\n    extras = ireq.extras or ()\n    try:\n        wheel = build_wheel(ireq, sources)\n    except WheelBuildError:\n        # XXX: This depends on a side effect of `build_wheel`. This block is\n        # reached when it fails to build an sdist, where the sdist would have\n        # been downloaded, extracted into `ireq.source_dir`, and partially\n        # built (hopefully containing .egg-info).\n        metadata = read_sdist_metadata(ireq)\n        if not metadata:\n            raise\n    else:\n        metadata = wheel.metadata\n    requirements = _read_requirements(metadata, extras)\n    requires_python = _read_requires_python(metadata)\n    return requirements, requires_python", "output": "Retrieves dependencies for the requirement from pipenv.patched.notpip internals.\n\n    The current strategy is to try the followings in order, returning the\n    first successful result.\n\n    1. Try to build a wheel out of the ireq, and read metadata out of it.\n    2. Read metadata out of the egg-info directory if it is present.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dense_to_deeper_block(dense_layer, weighted=True):\n    '''\n    '''\n    units = dense_layer.units\n    weight = np.eye(units)\n    bias = np.zeros(units)\n    new_dense_layer = StubDense(units, units)\n    if weighted:\n        new_dense_layer.set_weights(\n            (add_noise(weight, np.array([0, 1])), add_noise(bias, np.array([0, 1])))\n        )\n    return [StubReLU(), new_dense_layer]", "output": "deeper dense layer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_experiences(self, current_info: AllBrainInfo, next_info: AllBrainInfo):\n        \"\"\"\n        \n        \"\"\"\n        info_teacher = next_info[self.brain_to_imitate]\n        for l in range(len(info_teacher.agents)):\n            teacher_action_list = len(self.demonstration_buffer[info_teacher.agents[l]]['actions'])\n            horizon_reached = teacher_action_list > self.trainer_parameters['time_horizon']\n            teacher_filled = len(self.demonstration_buffer[info_teacher.agents[l]]['actions']) > 0\n            if (info_teacher.local_done[l] or horizon_reached) and teacher_filled:\n                agent_id = info_teacher.agents[l]\n                self.demonstration_buffer.append_update_buffer(\n                    agent_id, batch_size=None, training_length=self.policy.sequence_length)\n                self.demonstration_buffer[agent_id].reset_agent()\n\n        super(OnlineBCTrainer, self).process_experiences(current_info, next_info)", "output": "Checks agent histories for processing condition, and processes them as necessary.\n        Processing involves calculating value and advantage targets for model updating step.\n        :param current_info: Current AllBrainInfo\n        :param next_info: Next AllBrainInfo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_cache_security_group(name, region=None, key=None, keyid=None,\n                                profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    deleted = conn.delete_cache_security_group(name)\n    if deleted:\n        log.info('Deleted cache security group %s.', name)\n        return True\n    else:\n        msg = 'Failed to delete cache security group {0}.'.format(name)\n        log.error(msg)\n        return False", "output": "Delete a cache security group.\n\n    CLI example::\n\n        salt myminion boto_elasticache.delete_cache_security_group myelasticachesg 'My Cache Security Group'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _split_what(what):\n    \"\"\"\n    \n    \"\"\"\n    return (\n        frozenset(cls for cls in what if isclass(cls)),\n        frozenset(cls for cls in what if isinstance(cls, Attribute)),\n    )", "output": "Returns a tuple of `frozenset`s of classes and attributes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_metadata(field, expr, metadata_expr, no_metadata_rule):\n    \"\"\"\n    \"\"\"\n    if isinstance(metadata_expr, bz.Expr) or metadata_expr is None:\n        return metadata_expr\n\n    try:\n        return expr._child['_'.join(((expr._name or ''), field))]\n    except (ValueError, AttributeError):\n        if no_metadata_rule == 'raise':\n            raise ValueError(\n                \"no %s table could be reflected for %s\" % (field, expr)\n            )\n        elif no_metadata_rule == 'warn':\n            warnings.warn(NoMetaDataWarning(expr, field), stacklevel=4)\n    return None", "output": "Find the correct metadata expression for the expression.\n\n    Parameters\n    ----------\n    field : {'deltas', 'checkpoints'}\n        The kind of metadata expr to lookup.\n    expr : Expr\n        The baseline expression.\n    metadata_expr : Expr, 'auto', or None\n        The metadata argument. If this is 'auto', then the metadata table will\n        be searched for by walking up the expression tree. If this cannot be\n        reflected, then an action will be taken based on the\n        ``no_metadata_rule``.\n    no_metadata_rule : {'warn', 'raise', 'ignore'}\n        How to handle the case where the metadata_expr='auto' but no expr\n        could be found.\n\n    Returns\n    -------\n    metadata : Expr or None\n        The deltas or metadata table to use.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _batchify(self, batch_data, batch_label, start=0):\n        \"\"\"\"\"\"\n        i = start\n        batch_size = self.batch_size\n        try:\n            while i < batch_size:\n                label, s = self.next_sample()\n                data = self.imdecode(s)\n                try:\n                    self.check_valid_image(data)\n                except RuntimeError as e:\n                    logging.debug('Invalid image, skipping:  %s', str(e))\n                    continue\n                data = self.augmentation_transform(data)\n                assert i < batch_size, 'Batch size must be multiples of augmenter output length'\n                batch_data[i] = self.postprocess_data(data)\n                batch_label[i] = label\n                i += 1\n        except StopIteration:\n            if not i:\n                raise StopIteration\n        return i", "output": "Helper function for batchifying data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_wake_on_network():\n    '''\n    \n    '''\n    ret = salt.utils.mac_utils.execute_return_result(\n        'systemsetup -getwakeonnetworkaccess')\n    return salt.utils.mac_utils.validate_enabled(\n        salt.utils.mac_utils.parse_return(ret)) == 'on'", "output": "Displays whether 'wake on network' is on or off if supported\n\n    :return: A string value representing the \"wake on network\" settings\n    :rtype: string\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' power.get_wake_on_network", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def valid_id(opts, id_):\n    '''\n    \n    '''\n    try:\n        if any(x in id_ for x in ('/', '\\\\', str('\\0'))):\n            return False\n        return bool(clean_path(opts['pki_dir'], id_))\n    except (AttributeError, KeyError, TypeError, UnicodeDecodeError):\n        return False", "output": "Returns if the passed id is valid", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_perspective(coords:FlowField, coeffs:Points)->FlowField:\n    \"\"\n    size = coords.flow.size()\n    #compress all the dims expect the last one ang adds ones, coords become N * 3\n    coords.flow = coords.flow.view(-1,2)\n    #Transform the coeffs in a 3*3 matrix with a 1 at the bottom left\n    coeffs = torch.cat([coeffs, FloatTensor([1])]).view(3,3)\n    coords.flow = torch.addmm(coeffs[:,2], coords.flow, coeffs[:,:2].t())\n    coords.flow.mul_(1/coords.flow[:,2].unsqueeze(1))\n    coords.flow = coords.flow[:,:2].view(size)\n    return coords", "output": "Transform `coords` with `coeffs`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zeros(shape, ctx=None, dtype=None, **kwargs):\n    \"\"\"\n    \"\"\"\n    # pylint: disable= unused-argument\n    if ctx is None:\n        ctx = current_context()\n    dtype = mx_real_t if dtype is None else dtype\n    # pylint: disable= no-member, protected-access\n    return _internal._zeros(shape=shape, ctx=ctx, dtype=dtype, **kwargs)", "output": "Returns a new array filled with all zeros, with the given shape and type.\n\n    Parameters\n    ----------\n    shape : int or tuple of int\n        The shape of the empty array.\n    ctx : Context, optional\n        An optional device context (default is the current default context).\n    dtype : str or numpy.dtype, optional\n        An optional value type (default is `float32`).\n    out : NDArray, optional\n        The output NDArray (default is `None`).\n\n    Returns\n    -------\n    NDArray\n        A created array\n\n    Examples\n    --------\n    >>> mx.nd.zeros(1).asnumpy()\n    array([ 0.], dtype=float32)\n    >>> mx.nd.zeros((1,2), mx.gpu(0))\n    <NDArray 1x2 @gpu(0)>\n    >>> mx.nd.zeros((1,2), mx.gpu(0), 'float16').asnumpy()\n    array([[ 0.,  0.]], dtype=float16)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _mac(model, obs, h):\n    \"\"\"\n    \"\"\"\n    B, n_agents = obs.size(0), obs.size(1)\n    obs_flat = obs.reshape([B * n_agents, -1])\n    h_flat = [s.reshape([B * n_agents, -1]) for s in h]\n    q_flat, _, _, h_flat = model.forward({\"obs\": obs_flat}, h_flat)\n    return q_flat.reshape(\n        [B, n_agents, -1]), [s.reshape([B, n_agents, -1]) for s in h_flat]", "output": "Forward pass of the multi-agent controller.\n\n    Arguments:\n        model: TorchModel class\n        obs: Tensor of shape [B, n_agents, obs_size]\n        h: List of tensors of shape [B, n_agents, h_size]\n\n    Returns:\n        q_vals: Tensor of shape [B, n_agents, n_actions]\n        h: Tensor of shape [B, n_agents, h_size]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_upgrades(ruby=None,\n                  runas=None,\n                  gem_bin=None):\n    '''\n    \n    '''\n    result = _gem(['outdated'],\n                  ruby,\n                  gem_bin=gem_bin,\n                  runas=runas)\n    ret = {}\n    for line in salt.utils.itertools.split(result, '\\n'):\n        match = re.search(r'(\\S+) \\(\\S+ < (\\S+)\\)', line)\n        if match:\n            name, version = match.groups()\n        else:\n            log.error('Can\\'t parse line \\'%s\\'', line)\n            continue\n        ret[name] = version\n    return ret", "output": ".. versionadded:: 2015.8.0\n\n    Check if an upgrade is available for installed gems\n\n    gem_bin : None\n        Full path to ``gem`` binary to use.\n    ruby : None\n        If RVM or rbenv are installed, the ruby version and gemset to use.\n        Ignored if ``gem_bin`` is specified.\n    runas : None\n        The user to run gem as.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' gem.list_upgrades", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_protobuf(self):\n        \"\"\"\n        \"\"\"\n        key = _entity_pb2.Key()\n        key.partition_id.project_id = self.project\n\n        if self.namespace:\n            key.partition_id.namespace_id = self.namespace\n\n        for item in self.path:\n            element = key.path.add()\n            if \"kind\" in item:\n                element.kind = item[\"kind\"]\n            if \"id\" in item:\n                element.id = item[\"id\"]\n            if \"name\" in item:\n                element.name = item[\"name\"]\n\n        return key", "output": "Return a protobuf corresponding to the key.\n\n        :rtype: :class:`.entity_pb2.Key`\n        :returns: The protobuf representing the key.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate(module_name, code):\n    \"\"\"\n    \"\"\"\n    try:\n        ast_tree = ast.parse(code)\n    except Exception:\n        raise RuntimeError('Bad Python code')\n\n    visitor = SearchSpaceGenerator(module_name)\n    try:\n        visitor.visit(ast_tree)\n    except AssertionError as exc:\n        raise RuntimeError('%d: %s' % (visitor.last_line, exc.args[0]))\n    return visitor.search_space, astor.to_source(ast_tree)", "output": "Generate search space.\n    Return a serializable search space object.\n    module_name: name of the module (str)\n    code: user code (str)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def recognize_verify_code(image_path, broker=\"ht\"):\n    \"\"\"\"\"\"\n\n    if broker == \"gf\":\n        return detect_gf_result(image_path)\n    if broker in [\"yh_client\", \"gj_client\"]:\n        return detect_yh_client_result(image_path)\n    # \u8c03\u7528 tesseract \u8bc6\u522b\n    return default_verify_code_detect(image_path)", "output": "\u8bc6\u522b\u9a8c\u8bc1\u7801\uff0c\u8fd4\u56de\u8bc6\u522b\u540e\u7684\u5b57\u7b26\u4e32\uff0c\u4f7f\u7528 tesseract \u5b9e\u73b0\n    :param image_path: \u56fe\u7247\u8def\u5f84\n    :param broker: \u5238\u5546 ['ht', 'yjb', 'gf', 'yh']\n    :return recognized: verify code string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_data(self, context, data, dt):\n        \"\"\"\n        \n        \"\"\"\n        if self.rule.should_trigger(dt):\n            self.callback(context, data)", "output": "Calls the callable only when the rule is triggered.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wheel_check(self, auth_list, fun, args):\n        '''\n        \n        '''\n        return self.spec_check(auth_list, fun, args, 'wheel')", "output": "Check special API permissions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rolling_window(array, length):\n    \"\"\"\n    \n    \"\"\"\n    orig_shape = array.shape\n    if not orig_shape:\n        raise IndexError(\"Can't restride a scalar.\")\n    elif orig_shape[0] <= length:\n        raise IndexError(\n            \"Can't restride array of shape {shape} with\"\n            \" a window length of {len}\".format(\n                shape=orig_shape,\n                len=length,\n            )\n        )\n\n    num_windows = (orig_shape[0] - length + 1)\n    new_shape = (num_windows, length) + orig_shape[1:]\n\n    new_strides = (array.strides[0],) + array.strides\n\n    return as_strided(array, new_shape, new_strides)", "output": "Restride an array of shape\n\n        (X_0, ... X_N)\n\n    into an array of shape\n\n        (length, X_0 - length + 1, ... X_N)\n\n    where each slice at index i along the first axis is equivalent to\n\n        result[i] = array[length * i:length * (i + 1)]\n\n    Parameters\n    ----------\n    array : np.ndarray\n        The base array.\n    length : int\n        Length of the synthetic first axis to generate.\n\n    Returns\n    -------\n    out : np.ndarray\n\n    Example\n    -------\n    >>> from numpy import arange\n    >>> a = arange(25).reshape(5, 5)\n    >>> a\n    array([[ 0,  1,  2,  3,  4],\n           [ 5,  6,  7,  8,  9],\n           [10, 11, 12, 13, 14],\n           [15, 16, 17, 18, 19],\n           [20, 21, 22, 23, 24]])\n\n    >>> rolling_window(a, 2)\n    array([[[ 0,  1,  2,  3,  4],\n            [ 5,  6,  7,  8,  9]],\n    <BLANKLINE>\n           [[ 5,  6,  7,  8,  9],\n            [10, 11, 12, 13, 14]],\n    <BLANKLINE>\n           [[10, 11, 12, 13, 14],\n            [15, 16, 17, 18, 19]],\n    <BLANKLINE>\n           [[15, 16, 17, 18, 19],\n            [20, 21, 22, 23, 24]]])", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Name(self, number):\n    \"\"\"\"\"\"\n    if number in self._enum_type.values_by_number:\n      return self._enum_type.values_by_number[number].name\n    raise ValueError('Enum %s has no name defined for value %d' % (\n        self._enum_type.name, number))", "output": "Returns a string containing the name of an enum value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cogroup(self, other, numPartitions=None):\n        \"\"\"\n        \n        \"\"\"\n        if numPartitions is None:\n            numPartitions = self._sc.defaultParallelism\n        return self.transformWith(lambda a, b: a.cogroup(b, numPartitions), other)", "output": "Return a new DStream by applying 'cogroup' between RDDs of this\n        DStream and `other` DStream.\n\n        Hash partitioning is used to generate the RDDs with `numPartitions` partitions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toNDArray(self, image):\n        \"\"\"\n        \n        \"\"\"\n\n        if not isinstance(image, Row):\n            raise TypeError(\n                \"image argument should be pyspark.sql.types.Row; however, \"\n                \"it got [%s].\" % type(image))\n\n        if any(not hasattr(image, f) for f in self.imageFields):\n            raise ValueError(\n                \"image argument should have attributes specified in \"\n                \"ImageSchema.imageSchema [%s].\" % \", \".join(self.imageFields))\n\n        height = image.height\n        width = image.width\n        nChannels = image.nChannels\n        return np.ndarray(\n            shape=(height, width, nChannels),\n            dtype=np.uint8,\n            buffer=image.data,\n            strides=(width * nChannels, nChannels, 1))", "output": "Converts an image to an array with metadata.\n\n        :param `Row` image: A row that contains the image to be converted. It should\n            have the attributes specified in `ImageSchema.imageSchema`.\n        :return: a `numpy.ndarray` that is an image.\n\n        .. versionadded:: 2.3.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_parser(subparsers, parent_parser):\n    \"\"\"\"\"\"\n    INIT_HELP = \"Initialize DVC in the current directory.\"\n    INIT_DESCRIPTION = (\n        \"Initialize DVC in the current directory. Expects directory\\n\"\n        \"to be a Git repository unless --no-scm option is specified.\"\n    )\n\n    init_parser = subparsers.add_parser(\n        \"init\",\n        parents=[parent_parser],\n        description=append_doc_link(INIT_DESCRIPTION, \"init\"),\n        help=INIT_HELP,\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    init_parser.add_argument(\n        \"--no-scm\",\n        action=\"store_true\",\n        default=False,\n        help=\"Initiate dvc in directory that is \"\n        \"not tracked by any scm tool (e.g. git).\",\n    )\n    init_parser.add_argument(\n        \"-f\",\n        \"--force\",\n        action=\"store_true\",\n        default=False,\n        help=(\n            \"Overwrite existing '.dvc' directory. \"\n            \"This operation removes local cache.\"\n        ),\n    )\n    init_parser.set_defaults(func=CmdInit)", "output": "Setup parser for `dvc init`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode(self, tags, encoding, values_to_sub):\n        \"\"\"\n        \n        \"\"\"\n\n        for tag in tags:\n            if tags[tag].get(encoding) != \"None\":\n                if tags[tag].get(encoding) == \"url\":\n                    values_to_sub[tag] = self.url_encode(values_to_sub[tag])\n                if tags[tag].get(encoding) == \"base64\":\n                    values_to_sub[tag] = self.base64_utf_encode(values_to_sub[tag])\n        return values_to_sub", "output": "reads the encoding type from the event-mapping.json\n        and determines whether a value needs encoding\n\n        Parameters\n        ----------\n        tags: dict\n            the values of a particular event that can be substituted\n            within the event json\n        encoding: string\n            string that helps navigate to the encoding field of the json\n        values_to_sub: dict\n            key/value pairs that will be substituted into the json\n        Returns\n        -------\n        values_to_sub: dict\n            the encoded (if need be) values to substitute into the json.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge_overlapping_predictions(tags1: List[str], tags2: List[str]) -> List[str]:\n    \"\"\"\n    \n    \"\"\"\n    ret_sequence = []\n    prev_label = \"O\"\n\n    # Build a coherent sequence out of two\n    # spans which predicates' overlap\n\n    for tag1, tag2 in zip(tags1, tags2):\n        label1 = tag1.split(\"-\")[-1]\n        label2 = tag2.split(\"-\")[-1]\n        if (label1 == \"V\") or (label2 == \"V\"):\n            # Construct maximal predicate length -\n            # add predicate tag if any of the sequence predict it\n            cur_label = \"V\"\n\n        # Else - prefer an argument over 'O' label\n        elif label1 != \"O\":\n            cur_label = label1\n        else:\n            cur_label = label2\n\n        # Append cur tag to the returned sequence\n        cur_tag = get_coherent_next_tag(prev_label, cur_label)\n        prev_label = cur_label\n        ret_sequence.append(cur_tag)\n    return ret_sequence", "output": "Merge two predictions into one. Assumes the predicate in tags1 overlap with\n    the predicate of tags2.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_installable_dir(path):\n    \"\"\"\"\"\"\n    if not os.path.isdir(path):\n        return False\n    setup_py = os.path.join(path, \"setup.py\")\n    if os.path.isfile(setup_py):\n        return True\n    return False", "output": "Return True if `path` is a directory containing a setup.py file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def snapshots_get(container, name, remote_addr=None,\n                  cert=None, key=None, verify_cert=True):\n    '''\n    \n    '''\n    container = container_get(\n        container, remote_addr, cert, key, verify_cert, _raw=True\n    )\n    return container.snapshots.get(name)", "output": "Get information about snapshot for a container\n\n    container :\n        The name of the container to get.\n\n    name :\n        The name of the snapshot.\n\n    remote_addr :\n        An URL to a remote server. The 'cert' and 'key' fields must also be\n        provided if 'remote_addr' is defined.\n\n        Examples:\n            https://myserver.lan:8443\n            /var/lib/mysocket.sock\n\n    cert :\n        PEM Formatted SSL Certificate.\n\n        Examples:\n            ~/.config/lxc/client.crt\n\n    key :\n        PEM Formatted SSL Key.\n\n        Examples:\n            ~/.config/lxc/client.key\n\n    verify_cert : True\n        Verify the ssl certificate.  Default: True\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        $ salt '*' lxd.snapshots_get test-container test-snapshot", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_json(cls, json_info):\n        \"\"\"\"\"\"\n        if json_info is None:\n            return None\n        return ResultRecord(\n            trial_id=json_info[\"trial_id\"],\n            timesteps_total=json_info[\"timesteps_total\"],\n            done=json_info.get(\"done\", None),\n            episode_reward_mean=json_info.get(\"episode_reward_mean\", None),\n            mean_accuracy=json_info.get(\"mean_accuracy\", None),\n            mean_loss=json_info.get(\"mean_loss\", None),\n            trainning_iteration=json_info.get(\"training_iteration\", None),\n            timesteps_this_iter=json_info.get(\"timesteps_this_iter\", None),\n            time_this_iter_s=json_info.get(\"time_this_iter_s\", None),\n            time_total_s=json_info.get(\"time_total_s\", None),\n            date=json_info.get(\"date\", None),\n            hostname=json_info.get(\"hostname\", None),\n            node_ip=json_info.get(\"node_ip\", None),\n            config=json_info.get(\"config\", None))", "output": "Build a Result instance from a json string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(prefix, params, hint):\n        \"\"\"\"\"\"\n        current = getattr(_BlockScope._current, \"value\", None)\n        if current is None:\n            if prefix is None:\n                if not hasattr(_name.NameManager._current, \"value\"):\n                    _name.NameManager._current.value = _name.NameManager()\n                prefix = _name.NameManager._current.value.get(None, hint) + '_'\n            if params is None:\n                params = ParameterDict(prefix)\n            else:\n                params = ParameterDict(params.prefix, params)\n            return prefix, params\n\n        if prefix is None:\n            count = current._counter.get(hint, 0)\n            prefix = '%s%d_'%(hint, count)\n            current._counter[hint] = count + 1\n        if params is None:\n            parent = current._block.params\n            params = ParameterDict(parent.prefix+prefix, parent._shared)\n        else:\n            params = ParameterDict(params.prefix, params)\n        return current._block.prefix+prefix, params", "output": "Creates prefix and params for new `Block`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_application(*args):\n    '''\n    \n    '''\n    opts_tuple = args\n\n    def wsgi_app(environ, start_response):\n        root, _, conf = opts_tuple or bootstrap_app()\n        cherrypy.config.update({'environment': 'embedded'})\n\n        cherrypy.tree.mount(root, '/', conf)\n        return cherrypy.tree(environ, start_response)\n\n    return wsgi_app", "output": "Returns a WSGI application function. If you supply the WSGI app and config\n    it will use that, otherwise it will try to obtain them from a local Salt\n    installation", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_exit(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.zip_path:\n            # Only try to remove uploaded zip if we're running a command that has loaded credentials\n            if self.load_credentials:\n                self.remove_uploaded_zip()\n\n            self.remove_local_zip()", "output": "Cleanup after the command finishes.\n        Always called: SystemExit, KeyboardInterrupt and any other Exception that occurs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def last(self, rows: List[Row]) -> List[Row]:\n        \"\"\"\n        \n        \"\"\"\n        if not rows:\n            logger.warning(\"Trying to get last row from an empty list\")\n            return []\n        return [rows[-1]]", "output": "Takes an expression that evaluates to a list of rows, and returns the last one in that\n        list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show(self):\n        \"\"\"\"\"\"\n        thr_is_alive = self._spin_thread and self._spin_thread.is_alive()\n\n        if thr_is_alive and self._hide_spin.is_set():\n            # clear the hidden spinner flag\n            self._hide_spin.clear()\n\n            # clear the current line so the spinner is not appended to it\n            sys.stdout.write(\"\\r\")\n            self._clear_line()", "output": "Show the hidden spinner.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def awd_lstm_lm_split(model:nn.Module) -> List[nn.Module]:\n    \"\"\n    groups = [[rnn, dp] for rnn, dp in zip(model[0].rnns, model[0].hidden_dps)]\n    return groups + [[model[0].encoder, model[0].encoder_dp, model[1]]]", "output": "Split a RNN `model` in groups for differential learning rates.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_gpu_ids():\n    \"\"\"\n    \"\"\"\n    if _mode() == LOCAL_MODE:\n        raise Exception(\"ray.get_gpu_ids() currently does not work in PYTHON \"\n                        \"MODE.\")\n\n    all_resource_ids = global_worker.raylet_client.resource_ids()\n    assigned_ids = [\n        resource_id for resource_id, _ in all_resource_ids.get(\"GPU\", [])\n    ]\n    # If the user had already set CUDA_VISIBLE_DEVICES, then respect that (in\n    # the sense that only GPU IDs that appear in CUDA_VISIBLE_DEVICES should be\n    # returned).\n    if global_worker.original_gpu_ids is not None:\n        assigned_ids = [\n            global_worker.original_gpu_ids[gpu_id] for gpu_id in assigned_ids\n        ]\n\n    return assigned_ids", "output": "Get the IDs of the GPUs that are available to the worker.\n\n    If the CUDA_VISIBLE_DEVICES environment variable was set when the worker\n    started up, then the IDs returned by this method will be a subset of the\n    IDs in CUDA_VISIBLE_DEVICES. If not, the IDs will fall in the range\n    [0, NUM_GPUS - 1], where NUM_GPUS is the number of GPUs that the node has.\n\n    Returns:\n        A list of GPU IDs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare(self, data_batch, sparse_row_id_fn=None):\n        \"\"\"\n        \"\"\"\n        super(SVRGModule, self).prepare(data_batch, sparse_row_id_fn=sparse_row_id_fn)\n        self._mod_aux.prepare(data_batch, sparse_row_id_fn=sparse_row_id_fn)", "output": "Prepares two modules for processing a data batch.\n\n        Usually involves switching bucket and reshaping.\n        For modules that contain `row_sparse` parameters in KVStore,\n        it prepares the `row_sparse` parameters based on the sparse_row_id_fn.\n\n        When KVStore is used to update parameters for multi-device or multi-machine training,\n        a copy of the parameters are stored in KVStore. Note that for `row_sparse` parameters,\n        the `update()` updates the copy of parameters in KVStore, but doesn't broadcast\n        the updated parameters to all devices / machines. The `prepare` function is used to\n        broadcast `row_sparse` parameters with the next batch of data.\n\n        Parameters\n        ----------\n        data_batch : DataBatch\n            The current batch of data for forward computation.\n\n        sparse_row_id_fn : A callback function\n            The function  takes `data_batch` as an input and returns a dict of\n            str -> NDArray. The resulting dict is used for pulling row_sparse\n            parameters from the kvstore, where the str key is the name of the param,\n            and the value is the row id of the param to pull.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def import_from_dict(session, data, sync=[]):\n    \"\"\"\"\"\"\n    if isinstance(data, dict):\n        logging.info('Importing %d %s',\n                     len(data.get(DATABASES_KEY, [])),\n                     DATABASES_KEY)\n        for database in data.get(DATABASES_KEY, []):\n            Database.import_from_dict(session, database, sync=sync)\n\n        logging.info('Importing %d %s',\n                     len(data.get(DRUID_CLUSTERS_KEY, [])),\n                     DRUID_CLUSTERS_KEY)\n        for datasource in data.get(DRUID_CLUSTERS_KEY, []):\n            DruidCluster.import_from_dict(session, datasource, sync=sync)\n        session.commit()\n    else:\n        logging.info('Supplied object is not a dictionary.')", "output": "Imports databases and druid clusters from dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_format(self, full_check=True):\n        \"\"\"\n        \"\"\"\n        check_call(_LIB.MXNDArraySyncCheckFormat(self.handle, ctypes.c_bool(full_check)))", "output": "Check whether the NDArray format is valid.\n\n        Parameters\n        ----------\n        full_check : bool, optional\n            If `True`, rigorous check, O(N) operations. Otherwise\n            basic check, O(1) operations (default True).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_host_ipv6addr_info(ipv6addr=None, mac=None,\n                                        discovered_data=None,\n                                        return_fields=None, **api_opts):\n    '''\n    \n    '''\n    infoblox = _get_infoblox(**api_opts)\n    return infoblox.get_host_ipv6addr_object(ipv6addr, mac, discovered_data, return_fields)", "output": "Get host ipv6addr information\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call infoblox.get_host_ipv6addr_info ipv6addr=2001:db8:85a3:8d3:1349:8a2e:370:7348", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_font(self, font):\r\n        \"\"\"\"\"\"\r\n        self.setFont(font)\r\n        self.set_pythonshell_font(font)\r\n        cursor = self.textCursor()\r\n        cursor.select(QTextCursor.Document)\r\n        charformat = QTextCharFormat()\r\n        charformat.setFontFamily(font.family())\r\n        charformat.setFontPointSize(font.pointSize())\r\n        cursor.mergeCharFormat(charformat)", "output": "Set shell styles font", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _send_command_to_servers(self, head, body):\n        \"\"\"\n        \"\"\"\n        check_call(_LIB.MXKVStoreSendCommmandToServers(\n            self.handle, mx_uint(head), c_str(body)))", "output": "Sends a command to all server nodes.\n\n        Sending command to a server node will cause that server node to invoke\n        ``KVStoreServer.controller`` to execute the command.\n\n        This function returns after the command has been executed on all server\n        nodes.\n\n        Parameters\n        ----------\n        head : int\n            the head of the command.\n        body : str\n            the body of the command.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_single_click_to_open(self, value):\r\n        \"\"\"\"\"\"\r\n        self.single_click_to_open = value\r\n        self.parent_widget.sig_option_changed.emit('single_click_to_open',\r\n                                                   value)", "output": "Set single click to open items.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_answer_spans(answer_list, answer_start_list):\n        \"\"\"\n        \"\"\"\n        return [(answer_start_list[i], answer_start_list[i] + len(answer))\n                for i, answer in enumerate(answer_list)]", "output": "Find all answer spans from the context, returning start_index and end_index\n\n        :param list[str] answer_list: List of all answers\n        :param list[int] answer_start_list: List of all answers' start indices\n\n        Returns\n        -------\n        List[Tuple]\n            list of Tuple(answer_start_index answer_end_index) per question", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __setup_fileserver(self):\n        '''\n        \n        '''\n        fs_ = salt.fileserver.Fileserver(self.opts)\n        self._serve_file = fs_.serve_file\n        self._file_find = fs_._find_file\n        self._file_hash = fs_.file_hash\n        self._file_list = fs_.file_list\n        self._file_list_emptydirs = fs_.file_list_emptydirs\n        self._dir_list = fs_.dir_list\n        self._symlink_list = fs_.symlink_list\n        self._file_envs = fs_.envs", "output": "Set the local file objects from the file server interface", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def glob(self, filename):\n        \"\"\"\"\"\"\n        # Only support prefix with * at the end and no ? in the string\n        star_i = filename.find('*')\n        quest_i = filename.find('?')\n        if quest_i >= 0:\n            raise NotImplementedError(\n                \"{} not supported by compat glob\".format(filename))\n        if star_i != len(filename) - 1:\n            # Just return empty so we can use glob from directory watcher\n            #\n            # TODO: Remove and instead handle in GetLogdirSubdirectories.\n            # However, we would need to handle it for all non-local registered\n            # filesystems in some way.\n            return []\n        filename = filename[:-1]\n        client = boto3.client(\"s3\")\n        bucket, path = self.bucket_and_path(filename)\n        p = client.get_paginator(\"list_objects\")\n        keys = []\n        for r in p.paginate(Bucket=bucket, Prefix=path):\n            for o in r.get(\"Contents\", []):\n                key = o[\"Key\"][len(path):]\n                if key:  # Skip the base dir, which would add an empty string\n                    keys.append(filename + key)\n        return keys", "output": "Returns a list of files that match the given pattern(s).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rename_dont_move(self, path, dest):\n        \"\"\"\n        \n        \"\"\"\n        from snakebite.errors import FileAlreadyExistsException\n        try:\n            self.get_bite().rename2(path, dest, overwriteDest=False)\n        except FileAlreadyExistsException:\n            # Unfortunately python2 don't allow exception chaining.\n            raise luigi.target.FileAlreadyExists()", "output": "Use snakebite.rename_dont_move, if available.\n\n        :param path: source path (single input)\n        :type path: string\n        :param dest: destination path\n        :type dest: string\n        :return: True if succeeded\n        :raises: snakebite.errors.FileAlreadyExistsException", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def same_shape(self, objects: Set[Object]) -> Set[Object]:\n        \"\"\"\n        \n        \"\"\"\n        return self._get_objects_with_same_attribute(objects, lambda x: x.shape)", "output": "Filters the set of objects, and returns those objects whose color is the most frequent\n        color in the initial set of objects, if the highest frequency is greater than 1, or an\n        empty set otherwise.\n\n        This is an unusual name for what the method does, but just as ``triangle`` filters objects\n        to those that are triangles, this filters objects to those that are of the same shape.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def workflow_template_path(cls, project, region, workflow_template):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/regions/{region}/workflowTemplates/{workflow_template}\",\n            project=project,\n            region=region,\n            workflow_template=workflow_template,\n        )", "output": "Return a fully-qualified workflow_template string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init(opts=None):\n    '''\n    \n    '''\n    global CONNECTION\n    if __opts__.get('proxy').get('connection') is not None:\n        CONNECTION = __opts__.get('proxy').get('connection')\n\n    if CONNECTION == 'ssh':\n        log.info('NXOS PROXY: Initialize ssh proxy connection')\n        return _init_ssh(opts)\n    elif CONNECTION == 'nxapi':\n        log.info('NXOS PROXY: Initialize nxapi proxy connection')\n        return _init_nxapi(opts)\n    else:\n        log.error('Unknown Connection Type: %s', CONNECTION)\n        return False", "output": "Required.\n    Initialize device connection using ssh or nxapi connection type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def originalTextFor(expr, asString=True):\n    \"\"\"\n    \"\"\"\n    locMarker = Empty().setParseAction(lambda s,loc,t: loc)\n    endlocMarker = locMarker.copy()\n    endlocMarker.callPreparse = False\n    matchExpr = locMarker(\"_original_start\") + expr + endlocMarker(\"_original_end\")\n    if asString:\n        extractText = lambda s,l,t: s[t._original_start:t._original_end]\n    else:\n        def extractText(s,l,t):\n            t[:] = [s[t.pop('_original_start'):t.pop('_original_end')]]\n    matchExpr.setParseAction(extractText)\n    matchExpr.ignoreExprs = expr.ignoreExprs\n    return matchExpr", "output": "Helper to return the original, untokenized text for a given\n    expression.  Useful to restore the parsed fields of an HTML start\n    tag into the raw tag text itself, or to revert separate tokens with\n    intervening whitespace back to the original matching input text. By\n    default, returns astring containing the original parsed text.\n\n    If the optional ``asString`` argument is passed as\n    ``False``, then the return value is\n    a :class:`ParseResults` containing any results names that\n    were originally matched, and a single token containing the original\n    matched text from the input string.  So if the expression passed to\n    :class:`originalTextFor` contains expressions with defined\n    results names, you must set ``asString`` to ``False`` if you\n    want to preserve those results name values.\n\n    Example::\n\n        src = \"this is test <b> bold <i>text</i> </b> normal text \"\n        for tag in (\"b\",\"i\"):\n            opener,closer = makeHTMLTags(tag)\n            patt = originalTextFor(opener + SkipTo(closer) + closer)\n            print(patt.searchString(src)[0])\n\n    prints::\n\n        ['<b> bold <i>text</i> </b>']\n        ['<i>text</i>']", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_pkgng_version(jail=None, chroot=None, root=None):\n    '''\n    \n    '''\n    cmd = _pkg(jail, chroot, root) + ['--version']\n    return __salt__['cmd.run'](cmd).strip()", "output": "return the version of 'pkg'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rank(self, method='average', ascending=True, na_option='keep',\n             pct=False, axis=0):\n        \"\"\"\n        \n        \"\"\"\n        if na_option not in {'keep', 'top', 'bottom'}:\n            msg = \"na_option must be one of 'keep', 'top', or 'bottom'\"\n            raise ValueError(msg)\n        return self._cython_transform('rank', numeric_only=False,\n                                      ties_method=method, ascending=ascending,\n                                      na_option=na_option, pct=pct, axis=axis)", "output": "Provide the rank of values within each group.\n\n        Parameters\n        ----------\n        method : {'average', 'min', 'max', 'first', 'dense'}, default 'average'\n            * average: average rank of group\n            * min: lowest rank in group\n            * max: highest rank in group\n            * first: ranks assigned in order they appear in the array\n            * dense: like 'min', but rank always increases by 1 between groups\n        ascending : boolean, default True\n            False for ranks by high (1) to low (N)\n        na_option :  {'keep', 'top', 'bottom'}, default 'keep'\n            * keep: leave NA values where they are\n            * top: smallest rank if ascending\n            * bottom: smallest rank if descending\n        pct : boolean, default False\n            Compute percentage rank of data within each group\n        axis : int, default 0\n            The axis of the object over which to compute the rank.\n\n        Returns\n        -------\n        DataFrame with ranking of values within each group", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _trade(self, event):\n        \"\"\n        print('==================================market enging: trade')\n        print(self.order_handler.order_queue.pending)\n        print('==================================')\n        self.order_handler._trade()\n        print('done')", "output": "\u5185\u90e8\u51fd\u6570", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def htmlReadFile(filename, encoding, options):\n    \"\"\" \"\"\"\n    ret = libxml2mod.htmlReadFile(filename, encoding, options)\n    if ret is None:raise treeError('htmlReadFile() failed')\n    return xmlDoc(_obj=ret)", "output": "parse an XML file from the filesystem or the network.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_trivial(root):\n    '''\n    \n    '''\n\n    gen = GatherAssignments()\n    gen.visit(root)\n\n    to_remove = []\n\n    for symbol, assignments in gen.assign_id_map.items():\n        if len(assignments) < 2:\n            continue\n\n        for j in range(len(assignments) - 1):\n            i1 = root.body.index(assignments[j].root)\n            i2 = root.body.index(assignments[j + 1].root)\n\n            body = root.body[i1 + 1:i2]\n            grapher = GraphGen()\n            for stmnt in body:\n                grapher.visit(stmnt)\n\n            if symbol not in grapher.used:\n                to_remove.extend(assignments[j].assignments)\n\n    Pass = lambda node: _ast.Pass(lineno=node.lineno, col_offset=node.col_offset)\n\n    for old in to_remove:\n        replace_nodes(root, old, Pass(old))", "output": "Remove redundant statements.\n    \n    The statement `a = 1` will be removed::\n        \n        a = 1\n        a = 2\n\n    The statement `a = 1` will not be removed because `b` depends on it::\n        \n        a = 1\n        b = a + 2\n        a = 2\n        \n    :param root: ast node", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sample_uniform(domain, rng):\n  \"\"\"\n  \"\"\"\n  if isinstance(domain, hp.IntInterval):\n    return rng.randint(domain.min_value, domain.max_value)\n  elif isinstance(domain, hp.RealInterval):\n    return rng.uniform(domain.min_value, domain.max_value)\n  elif isinstance(domain, hp.Discrete):\n    return rng.choice(domain.values)\n  else:\n    raise TypeError(\"unknown domain type: %r\" % (domain,))", "output": "Sample a value uniformly from a domain.\n\n  Args:\n    domain: An `IntInterval`, `RealInterval`, or `Discrete` domain.\n    rng: A `random.Random` object; defaults to the `random` module.\n\n  Raises:\n    TypeError: If `domain` is not a known kind of domain.\n    IndexError: If the domain is empty.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_instance(name, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The show_instance action must be called with -a or --action.'\n        )\n    node = _get_node(name)\n    __utils__['cloud.cache_node'](node, __active_provider_name__, __opts__)\n    return node", "output": "Show the details from DigitalOcean concerning a droplet", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update():\n    '''\n    \n    '''\n\n    metadata = _init()\n\n    if S3_SYNC_ON_UPDATE:\n        # sync the buckets to the local cache\n        log.info('Syncing local cache from S3...')\n        for saltenv, env_meta in six.iteritems(metadata):\n            for bucket_files in _find_files(env_meta):\n                for bucket, files in six.iteritems(bucket_files):\n                    for file_path in files:\n                        cached_file_path = _get_cached_file_name(bucket, saltenv, file_path)\n                        log.info('%s - %s : %s', bucket, saltenv, file_path)\n\n                        # load the file from S3 if it's not in the cache or it's old\n                        _get_file_from_s3(metadata, saltenv, bucket, file_path, cached_file_path)\n\n        log.info('Sync local cache from S3 completed.')", "output": "Update the cache file for the bucket.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _maybe_flush_gcs(self):\n        \"\"\"\n        \"\"\"\n        if not self.issue_gcs_flushes:\n            return\n        if self.gcs_flush_policy is None:\n            serialized = self.redis.get(\"gcs_flushing_policy\")\n            if serialized is None:\n                # Client has not set any policy; by default flushing is off.\n                return\n            self.gcs_flush_policy = pickle.loads(serialized)\n\n        if not self.gcs_flush_policy.should_flush(self.redis_shard):\n            return\n\n        max_entries_to_flush = self.gcs_flush_policy.num_entries_to_flush()\n        num_flushed = self.redis_shard.execute_command(\n            \"HEAD.FLUSH {}\".format(max_entries_to_flush))\n        logger.info(\"Monitor: num_flushed {}\".format(num_flushed))\n\n        # This flushes event log and log files.\n        ray.experimental.flush_redis_unsafe(self.redis)\n\n        self.gcs_flush_policy.record_flush()", "output": "Experimental: issue a flush request to the GCS.\n\n        The purpose of this feature is to control GCS memory usage.\n\n        To activate this feature, Ray must be compiled with the flag\n        RAY_USE_NEW_GCS set, and Ray must be started at run time with the flag\n        as well.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copyMakeBorder(src, top, bot, left, right, border_type=cv2.BORDER_CONSTANT, value=0):\n    \"\"\"\n    \"\"\"\n    hdl = NDArrayHandle()\n    check_call(_LIB.MXCVcopyMakeBorder(src.handle, ctypes.c_int(top), ctypes.c_int(bot),\n                                       ctypes.c_int(left), ctypes.c_int(right),\n                                       ctypes.c_int(border_type), ctypes.c_double(value),\n                                       ctypes.byref(hdl)))\n    return mx.nd.NDArray(hdl)", "output": "Pad image border\n    Wrapper for cv2.copyMakeBorder that uses mx.nd.NDArray\n\n    Parameters\n    ----------\n    src : NDArray\n        Image in (width, height, channels).\n        Others are the same with cv2.copyMakeBorder\n\n    Returns\n    -------\n    img : NDArray\n        padded image", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_relative_pythonpath(self):\r\n        \"\"\"\"\"\"\r\n        # Workaround to replace os.path.relpath (new in Python v2.6):\r\n        offset = len(self.root_path)+len(os.pathsep)\r\n        return [path[offset:] for path in self.pythonpath]", "output": "Return PYTHONPATH list as relative paths", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_warning(message):\r\n    \"\"\"\"\"\"\r\n    try:\r\n        # If Tkinter is installed (highly probable), showing an error pop-up\r\n        import Tkinter, tkMessageBox\r\n        root = Tkinter.Tk()\r\n        root.withdraw()\r\n        tkMessageBox.showerror(\"Spyder\", message)\r\n    except ImportError:\r\n        pass\r\n    raise RuntimeError(message)", "output": "Show warning using Tkinter if available", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def info(dev):\n    '''\n    \n    '''\n    if 'sys' in dev:\n        qtype = 'path'\n    else:\n        qtype = 'name'\n\n    cmd = 'udevadm info --export --query=all --{0}={1}'.format(qtype, dev)\n    udev_result = __salt__['cmd.run_all'](cmd, output_loglevel='quiet')\n\n    if udev_result['retcode'] != 0:\n        raise CommandExecutionError(udev_result['stderr'])\n\n    return _parse_udevadm_info(udev_result['stdout'])[0]", "output": "Extract all info delivered by udevadm\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' udev.info /dev/sda\n        salt '*' udev.info /sys/class/net/eth0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def schedule(self, callback, *args, **kwargs):\n        \"\"\"\n        \"\"\"\n        self._executor.submit(callback, *args, **kwargs)", "output": "Schedule the callback to be called asynchronously in a thread pool.\n\n        Args:\n            callback (Callable): The function to call.\n            args: Positional arguments passed to the function.\n            kwargs: Key-word arguments passed to the function.\n\n        Returns:\n            None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def insert(self, loc, item):\n        \"\"\"\n        \n        \"\"\"\n        # Pad the key with empty strings if lower levels of the key\n        # aren't specified:\n        if not isinstance(item, tuple):\n            item = (item, ) + ('', ) * (self.nlevels - 1)\n        elif len(item) != self.nlevels:\n            raise ValueError('Item must have length equal to number of '\n                             'levels.')\n\n        new_levels = []\n        new_codes = []\n        for k, level, level_codes in zip(item, self.levels, self.codes):\n            if k not in level:\n                # have to insert into level\n                # must insert at end otherwise you have to recompute all the\n                # other codes\n                lev_loc = len(level)\n                level = level.insert(lev_loc, k)\n            else:\n                lev_loc = level.get_loc(k)\n\n            new_levels.append(level)\n            new_codes.append(np.insert(\n                ensure_int64(level_codes), loc, lev_loc))\n\n        return MultiIndex(levels=new_levels, codes=new_codes,\n                          names=self.names, verify_integrity=False)", "output": "Make new MultiIndex inserting new item at location\n\n        Parameters\n        ----------\n        loc : int\n        item : tuple\n            Must be same length as number of levels in the MultiIndex\n\n        Returns\n        -------\n        new_index : Index", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_bond(iface, **settings):\n    '''\n    \n    '''\n    deb_major = __grains__['osrelease'][:1]\n\n    opts = _parse_settings_bond(settings, iface)\n    try:\n        template = JINJA.get_template('conf.jinja')\n    except jinja2.exceptions.TemplateNotFound:\n        log.error('Could not load template conf.jinja')\n        return ''\n    data = template.render({'name': iface, 'bonding': opts})\n\n    if 'test' in settings and settings['test']:\n        return _read_temp(data)\n\n    _write_file(iface, data, _DEB_NETWORK_CONF_FILES, '{0}.conf'.format(iface))\n    path = os.path.join(_DEB_NETWORK_CONF_FILES, '{0}.conf'.format(iface))\n    if deb_major == '5':\n        for line_type in ('alias', 'options'):\n            cmd = ['sed', '-i', '-e', r'/^{0}\\s{1}.*/d'.format(line_type, iface),\n                   '/etc/modprobe.conf']\n            __salt__['cmd.run'](cmd, python_shell=False)\n        __salt__['file.append']('/etc/modprobe.conf', path)\n\n    # Load kernel module\n    __salt__['kmod.load']('bonding')\n\n    # install ifenslave-2.6\n    __salt__['pkg.install']('ifenslave-2.6')\n\n    return _read_file(path)", "output": "Create a bond script in /etc/modprobe.d with the passed settings\n    and load the bonding kernel module.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ip.build_bond bond0 mode=balance-alb", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def top(**kwargs):\n    '''\n    \n    '''\n    url = __opts__['cobbler.url']\n    user = __opts__['cobbler.user']\n    password = __opts__['cobbler.password']\n\n    minion_id = kwargs['opts']['id']\n\n    log.info(\"Querying cobbler for information for %r\", minion_id)\n    try:\n        server = salt.ext.six.moves.xmlrpc_client.Server(url, allow_none=True)\n        if user:\n            server.login(user, password)\n        data = server.get_blended_data(None, minion_id)\n    except Exception:\n        log.exception(\n            'Could not connect to cobbler.'\n        )\n        return {}\n\n    return {data['status']: data['mgmt_classes']}", "output": "Look up top data in Cobbler for a minion.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_module(mod_name):\n    \"\"\"\n    \n    \"\"\"\n    path = None\n    for part in mod_name.split('.'):\n        if path is not None:\n            path = [path]\n        file, path, description = imp.find_module(part, path)\n        if file is not None:\n            file.close()\n    return path, description", "output": "Iterate over each part instead of calling imp.find_module directly.\n    This function is able to find submodules (e.g. sickit.tree)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _flush(self, close=False):\n        \"\"\"\n        \"\"\"\n        for channel in self.forward_channels:\n            if close is True:\n                channel.queue.put_next(None)\n            channel.queue._flush_writes()\n        for channels in self.shuffle_channels:\n            for channel in channels:\n                if close is True:\n                    channel.queue.put_next(None)\n                channel.queue._flush_writes()\n        for channels in self.shuffle_key_channels:\n            for channel in channels:\n                if close is True:\n                    channel.queue.put_next(None)\n                channel.queue._flush_writes()\n        for channels in self.round_robin_channels:\n            for channel in channels:\n                if close is True:\n                    channel.queue.put_next(None)\n                channel.queue._flush_writes()", "output": "Flushes remaining output records in the output queues to plasma.\n\n        None is used as special type of record that is propagated from sources\n        to sink to notify that the end of data in a stream.\n\n        Attributes:\n             close (bool): A flag denoting whether the channel should be\n             also marked as 'closed' (True) or not (False) after flushing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_index_min_adv(\n        code,\n        start, end=None,\n        frequence='1min',\n        if_drop_index=True,\n        collections=DATABASE.index_min):\n    '''\n    \n    '''\n    if frequence in ['1min', '1m']:\n        frequence = '1min'\n    elif frequence in ['5min', '5m']:\n        frequence = '5min'\n    elif frequence in ['15min', '15m']:\n        frequence = '15min'\n    elif frequence in ['30min', '30m']:\n        frequence = '30min'\n    elif frequence in ['60min', '60m']:\n        frequence = '60min'\n\n    # __data = [] \u6ca1\u6709\u4f7f\u7528\n\n    end = start if end is None else end\n    if len(start) == 10:\n        start = '{} 09:30:00'.format(start)\n    if len(end) == 10:\n        end = '{} 15:00:00'.format(end)\n\n    # \ud83d\udee0 todo \u62a5\u544a\u9519\u8bef \u5982\u679c\u5f00\u59cb\u65f6\u95f4 \u5728 \u7ed3\u675f\u65f6\u95f4\u4e4b\u540e\n\n    # if start == end:\n    # \ud83d\udee0 todo \u5982\u679c\u76f8\u7b49\uff0c\u6839\u636e frequence \u83b7\u53d6\u5f00\u59cb\u65f6\u95f4\u7684 \u65f6\u95f4\u6bb5 QA_fetch_index_min_adv\uff0c \u4e0d\u652f\u6301start end\u662f\u76f8\u7b49\u7684\n    #print(\"QA Error QA_fetch_index_min_adv parameter code=%s , start=%s, end=%s is equal, should have time span! \" % (code, start, end))\n    # return None\n\n    res = QA_fetch_index_min(\n        code, start, end, format='pd', frequence=frequence)\n    if res is None:\n        print(\"QA Error QA_fetch_index_min_adv parameter code=%s start=%s end=%s frequence=%s call QA_fetch_index_min return None\" % (\n            code, start, end, frequence))\n    else:\n        res_reset_index = res.set_index(\n            ['datetime', 'code'], drop=if_drop_index)\n        # if res_reset_index is None:\n        #     print(\"QA Error QA_fetch_index_min_adv set index 'date, code' return None\")\n        return QA_DataStruct_Index_min(res_reset_index)", "output": "'\u83b7\u53d6\u80a1\u7968\u5206\u949f\u7ebf'\n    :param code:\n    :param start:\n    :param end:\n    :param frequence:\n    :param if_drop_index:\n    :param collections:\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def subsets(self):\n    \"\"\"\"\"\"\n    source, target = self.builder_config.language_pair\n    filtered_subsets = {}\n    for split, ss_names in self._subsets.items():\n      filtered_subsets[split] = []\n      for ss_name in ss_names:\n        ds = DATASET_MAP[ss_name]\n        if ds.target != target or source not in ds.sources:\n          logging.info(\n              \"Skipping sub-dataset that does not include language pair: %s\",\n              ss_name)\n        else:\n          filtered_subsets[split].append(ss_name)\n    logging.info(\"Using sub-datasets: %s\", filtered_subsets)\n    return filtered_subsets", "output": "Subsets that make up each split of the dataset for the language pair.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _valid(m, comment=VALID_RESPONSE, out=None):\n    '''\n    \n    '''\n    return _set_status(m, status=True, comment=comment, out=out)", "output": "Return valid status.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _list_audio_files(self, root, skip_rows=0):\n        \"\"\"\n        \"\"\"\n        self.synsets = []\n        self.items = []\n        if not self._train_csv:\n            # The audio files are organized in folder structure with\n            # directory name as label and audios in them\n            self._folder_structure(root)\n        else:\n            # train_csv contains mapping between filename and label\n            self._csv_labelled_dataset(root, skip_rows=skip_rows)\n\n        # Generating the synset.txt file now\n        if not os.path.exists(\"./synset.txt\"):\n            with open(\"./synset.txt\", \"w\") as synsets_file:\n                for item in self.synsets:\n                    synsets_file.write(item+os.linesep)\n            print(\"Synsets is generated as synset.txt\")\n        else:\n            warnings.warn(\"Synset file already exists in the current directory! Not generating synset.txt.\")", "output": "Populates synsets - a map of index to label for the data items.\n        Populates the data in the dataset, making tuples of (data, label)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup(app):\n    '''  '''\n    app.add_node(\n        collapsible_code_block,\n        html=(\n            html_visit_collapsible_code_block,\n            html_depart_collapsible_code_block\n        )\n    )\n    app.add_directive('collapsible-code-block', CollapsibleCodeBlock)", "output": "Required Sphinx extension setup function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def disable(name, **kwargs):\n    '''\n    \n    '''\n    osmajor = _osrel()[0]\n    if osmajor < '6':\n        cmd = 'update-rc.d -f {0} remove'.format(name)\n    else:\n        cmd = 'update-rc.d {0} disable'.format(name)\n    return not __salt__['cmd.retcode'](cmd)", "output": "Disable the named service to start at boot\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.disable <service name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_unique_figname(dirname, root, ext):\n    \"\"\"\n    \n    \"\"\"\n    i = 1\n    figname = root + '_%d' % i + ext\n    while True:\n        if osp.exists(osp.join(dirname, figname)):\n            i += 1\n            figname = root + '_%d' % i + ext\n        else:\n            return osp.join(dirname, figname)", "output": "Append a number to \"root\" to form a filename that does not already exist\n    in \"dirname\".", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_peers(*peers, **options):\n\n    '''\n    \n    '''\n\n    test = options.pop('test', False)\n    commit = options.pop('commit', True)\n\n    return __salt__['net.load_template']('set_ntp_peers',\n                                         peers=peers,\n                                         test=test,\n                                         commit=commit,\n                                         inherit_napalm_device=napalm_device)", "output": "Configures a list of NTP peers on the device.\n\n    :param peers: list of IP Addresses/Domain Names\n    :param test (bool): discard loaded config. By default ``test`` is False\n        (will not dicard the changes)\n    :commit commit (bool): commit loaded config. By default ``commit`` is True\n        (will commit the changes). Useful when the user does not want to commit\n        after each change, but after a couple.\n\n    By default this function will commit the config changes (if any). To load without committing, use the `commit`\n    option. For dry run use the `test` argument.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ntp.set_peers 192.168.0.1 172.17.17.1 time.apple.com\n        salt '*' ntp.set_peers 172.17.17.1 test=True  # only displays the diff\n        salt '*' ntp.set_peers 192.168.0.1 commit=False  # preserves the changes, but does not commit", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clean_by_request(self, request):\n        '''\n        \n        '''\n        if request not in self.request_map:\n            return\n        for tag, matcher, future in self.request_map[request]:\n            # timeout the future\n            self._timeout_future(tag, matcher, future)\n            # remove the timeout\n            if future in self.timeout_map:\n                tornado.ioloop.IOLoop.current().remove_timeout(self.timeout_map[future])\n                del self.timeout_map[future]\n\n        del self.request_map[request]", "output": "Remove all futures that were waiting for request `request` since it is done waiting", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _refresh_outlineexplorer(self, index=None, update=True, clear=False):\r\n        \"\"\"\"\"\"\r\n        oe = self.outlineexplorer\r\n        if oe is None:\r\n            return\r\n        if index is None:\r\n            index = self.get_stack_index()\r\n        if self.data:\r\n            finfo = self.data[index]\r\n            oe.setEnabled(True)\r\n            if finfo.editor.oe_proxy is None:\r\n                finfo.editor.oe_proxy = OutlineExplorerProxyEditor(\r\n                    finfo.editor, finfo.filename)\r\n            oe.set_current_editor(finfo.editor.oe_proxy,\r\n                                  update=update, clear=clear)\r\n            if index != self.get_stack_index():\r\n                # The last file added to the outline explorer is not the\r\n                # currently focused one in the editor stack. Therefore,\r\n                # we need to force a refresh of the outline explorer to set\r\n                # the current editor to the currently focused one in the\r\n                # editor stack. See PR #8015.\r\n                self._refresh_outlineexplorer(update=False)\r\n                return\r\n        self._sync_outlineexplorer_file_order()", "output": "Refresh outline explorer panel", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def task_context(self):\n        \"\"\"\n        \"\"\"\n        if not hasattr(self._task_context, \"initialized\"):\n            # Initialize task_context for the current thread.\n            if ray.utils.is_main_thread():\n                # If this is running on the main thread, initialize it to\n                # NIL. The actual value will set when the worker receives\n                # a task from raylet backend.\n                self._task_context.current_task_id = TaskID.nil()\n            else:\n                # If this is running on a separate thread, then the mapping\n                # to the current task ID may not be correct. Generate a\n                # random task ID so that the backend can differentiate\n                # between different threads.\n                self._task_context.current_task_id = TaskID(_random_string())\n                if getattr(self, \"_multithreading_warned\", False) is not True:\n                    logger.warning(\n                        \"Calling ray.get or ray.wait in a separate thread \"\n                        \"may lead to deadlock if the main thread blocks on \"\n                        \"this thread and there are not enough resources to \"\n                        \"execute more tasks\")\n                    self._multithreading_warned = True\n\n            self._task_context.task_index = 0\n            self._task_context.put_index = 1\n            self._task_context.initialized = True\n        return self._task_context", "output": "A thread-local that contains the following attributes.\n\n        current_task_id: For the main thread, this field is the ID of this\n            worker's current running task; for other threads, this field is a\n            fake random ID.\n        task_index: The number of tasks that have been submitted from the\n            current task.\n        put_index: The number of objects that have been put from the current\n            task.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def junos_cli(command, format=None, dev_timeout=None, dest=None, **kwargs):\n    '''\n    \n    '''\n    prep = _junos_prep_fun(napalm_device)  # pylint: disable=undefined-variable\n    if not prep['result']:\n        return prep\n    return __salt__['junos.cli'](command,\n                                 format=format,\n                                 dev_timeout=dev_timeout,\n                                 dest=dest,\n                                 **kwargs)", "output": ".. versionadded:: 2019.2.0\n\n    Execute a CLI command and return the output in the specified format.\n\n    command\n        The command to execute on the Junos CLI.\n\n    format: ``text``\n        Format in which to get the CLI output (either ``text`` or ``xml``).\n\n    dev_timeout: ``30``\n        The NETCONF RPC timeout (in seconds).\n\n    dest\n        Destination file where the RPC output is stored. Note that the file will\n        be stored on the Proxy Minion. To push the files to the Master, use\n        :mod:`cp.push <salt.modules.cp.push>`.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm.junos_cli 'show lldp neighbors'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _maybe_init_run(self, experiment_name, run_name):\n    \"\"\"\n    \"\"\"\n    experiment_id = self._maybe_init_experiment(experiment_name)\n    cursor = self._db.cursor()\n    cursor.execute(\n        \"\"\"\n        SELECT run_id FROM Runs\n        WHERE experiment_id = ? AND run_name = ?\n        \"\"\",\n        (experiment_id, run_name))\n    row = cursor.fetchone()\n    if row:\n      return row[0]\n    run_id = self._create_id()\n    # TODO: track actual run start times\n    started_time = 0\n    cursor.execute(\n        \"\"\"\n        INSERT INTO Runs (\n          experiment_id, run_id, run_name, inserted_time, started_time\n        ) VALUES (?, ?, ?, ?, ?)\n        \"\"\",\n        (experiment_id, run_id, run_name, time.time(), started_time))\n    return run_id", "output": "Returns the ID for the given run, creating the row if needed.\n\n    Args:\n      experiment_name: name of experiment containing this run.\n      run_name: name of run.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_vocab(self, token_generator, add_reserved_tokens=True):\n    \"\"\"\"\"\"\n\n    self._id_to_token = {}\n    non_reserved_start_index = 0\n\n    if add_reserved_tokens:\n      self._id_to_token.update(enumerate(RESERVED_TOKENS))\n      non_reserved_start_index = len(RESERVED_TOKENS)\n\n    self._id_to_token.update(\n        enumerate(token_generator, start=non_reserved_start_index))\n\n    # _token_to_id is the reverse of _id_to_token\n    self._token_to_id = dict((v, k)\n                             for k, v in six.iteritems(self._id_to_token))", "output": "Initialize vocabulary with tokens from token_generator.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def levelise(level):\n    '''\n    \n\n    '''\n\n    if not level:  # False, 0, [] ...\n        return False, False\n    if level is True:\n        return True, True\n    if isinstance(level, int):\n        return True, level - 1\n    try:  # a sequence\n        deep, subs = int(level[0]), level[1:]\n        return bool(deep), subs\n    except Exception as error:\n        log.warning(error)\n        raise", "output": "Describe which levels are allowed to do deep merging.\n\n    level can be:\n\n    True\n        all levels are True\n\n    False\n        all levels are False\n\n    an int\n        only the first levels are True, the others are False\n\n    a sequence\n        it describes which levels are True, it can be:\n\n        * a list of bool and int values\n        * a string of 0 and 1 characters", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _normalize_apis(apis):\n        \"\"\"\n        \n        \"\"\"\n\n        result = list()\n        for api in apis:\n            for normalized_method in SamApiProvider._normalize_http_methods(api.method):\n                # _replace returns a copy of the namedtuple. This is the official way of creating copies of namedtuple\n                result.append(api._replace(method=normalized_method))\n\n        return result", "output": "Normalize the APIs to use standard method name\n\n        Parameters\n        ----------\n        apis : list of samcli.commands.local.lib.provider.Api\n            List of APIs to replace normalize\n\n        Returns\n        -------\n        list of samcli.commands.local.lib.provider.Api\n            List of normalized APIs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_common_actions(self):\r\n        \"\"\"\"\"\"\r\n        actions = super(ExplorerTreeWidget, self).setup_common_actions()\r\n        if self.show_cd_only is None:\r\n            # Enabling the 'show current directory only' option but do not\r\n            # allow the user to disable it\r\n            self.show_cd_only = True\r\n        else:\r\n            # Show current directory only\r\n            cd_only_action = create_action(self,\r\n                                           _(\"Show current directory only\"),\r\n                                           toggled=self.toggle_show_cd_only)\r\n            cd_only_action.setChecked(self.show_cd_only)\r\n            self.toggle_show_cd_only(self.show_cd_only)\r\n            actions.append(cd_only_action)\r\n        return actions", "output": "Setup context menu common actions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy_data_from_remote(args, nni_config, trial_content, path_list, host_list, temp_nni_path):\n    ''''''\n    machine_list = nni_config.get_config('experimentConfig').get('machineList')\n    machine_dict = {}\n    local_path_list = []\n    for machine in machine_list:\n        machine_dict[machine['ip']] = {'port': machine['port'], 'passwd': machine['passwd'], 'username': machine['username']}\n    for index, host in enumerate(host_list):\n        local_path = os.path.join(temp_nni_path, trial_content[index].get('id'))\n        local_path_list.append(local_path)\n        print_normal('Copying log data from %s to %s' % (host + ':' + path_list[index], local_path))\n        sftp = create_ssh_sftp_client(host, machine_dict[host]['port'], machine_dict[host]['username'], machine_dict[host]['passwd'])\n        copy_remote_directory_to_local(sftp, path_list[index], local_path)\n    print_normal('Copy done!')\n    return local_path_list", "output": "use ssh client to copy data from remote machine to local machien", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def MethodCalled(self, mock_method):\n    \"\"\"\n    \"\"\"\n\n    # Check to see if this method exists, and if so, remove it from the set\n    # and return it.\n    for method in self._methods:\n      if method == mock_method:\n        # Remove the called mock_method instead of the method in the group.\n        # The called method will match any comparators when equality is checked\n        # during removal.  The method in the group could pass a comparator to\n        # another comparator during the equality check.\n        self._methods.remove(mock_method)\n\n        # If this group is not empty, put it back at the head of the queue.\n        if not self.IsSatisfied():\n          mock_method._call_queue.appendleft(self)\n\n        return self, method\n\n    raise UnexpectedMethodCallError(mock_method, self)", "output": "Remove a method call from the group.\n\n    If the method is not in the set, an UnexpectedMethodCallError will be\n    raised.\n\n    Args:\n      mock_method: a mock method that should be equal to a method in the group.\n\n    Returns:\n      The mock method from the group\n\n    Raises:\n      UnexpectedMethodCallError if the mock_method was not in the group.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def retcode_pillar(pillar_name):\n    '''\n    \n    '''\n    groups = __salt__['pillar.get'](pillar_name)\n\n    check = {}\n    data = {}\n\n    for group in groups:\n        commands = groups[group]\n        for command in commands:\n            # Check if is a dict to get the arguments\n            # in command if not set the arguments to empty string\n            if isinstance(command, dict):\n                plugin = next(six.iterkeys(command))\n                args = command[plugin]\n            else:\n                plugin = command\n                args = ''\n\n            check.update(retcode(plugin, args, group))\n\n            current_value = 0\n            new_value = int(check[group]['status'])\n            if group in data:\n                current_value = int(data[group]['status'])\n\n            if (new_value > current_value) or (group not in data):\n\n                if group not in data:\n                    data[group] = {}\n                data[group]['status'] = new_value\n\n    return data", "output": "Run one or more nagios plugins from pillar data and get the result of cmd.retcode\n    The pillar have to be in this format::\n\n        ------\n        webserver:\n            Ping_google:\n                - check_icmp: 8.8.8.8\n                - check_icmp: google.com\n            Load:\n                - check_load: -w 0.8 -c 1\n            APT:\n                - check_apt\n        -------\n\n    webserver is the role to check, the next keys are the group and the items\n    the check with the arguments if needed\n\n    You must to group different checks(one o more) and always it will return\n    the highest value of all the checks\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' nagios.retcode webserver", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getActiveSession(cls):\n        \"\"\"\n        \n        \"\"\"\n        from pyspark import SparkContext\n        sc = SparkContext._active_spark_context\n        if sc is None:\n            return None\n        else:\n            if sc._jvm.SparkSession.getActiveSession().isDefined():\n                SparkSession(sc, sc._jvm.SparkSession.getActiveSession().get())\n                return SparkSession._activeSession\n            else:\n                return None", "output": "Returns the active SparkSession for the current thread, returned by the builder.\n        >>> s = SparkSession.getActiveSession()\n        >>> l = [('Alice', 1)]\n        >>> rdd = s.sparkContext.parallelize(l)\n        >>> df = s.createDataFrame(rdd, ['name', 'age'])\n        >>> df.select(\"age\").collect()\n        [Row(age=1)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_installed_use(cp, use=\"USE\"):\n    '''\n    \n    '''\n    portage = _get_portage()\n    cpv = _get_cpv(cp)\n    return portage.db[portage.root][\"vartree\"].dbapi.aux_get(cpv, [use])[0].split()", "output": ".. versionadded:: 2015.8.0\n\n    Gets the installed USE flags from the VARDB.\n\n    @type: cp: string\n    @param cp: cat/pkg\n    @type use: string\n    @param use: 1 of [\"USE\", \"PKGUSE\"]\n    @rtype list\n    @returns [] or the list of IUSE flags", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def train_epoch(model:nn.Module, dl:DataLoader, opt:optim.Optimizer, loss_func:LossFunction)->None:\n    \"\"\n    model.train()\n    for xb,yb in dl:\n        loss = loss_func(model(xb), yb)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()", "output": "Simple training of `model` for 1 epoch of `dl` using optim `opt` and loss function `loss_func`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attack(self, imgs, targets):\n    \"\"\"\n    \n    \"\"\"\n\n    batch_size = self.batch_size\n    r = []\n    for i in range(0, len(imgs) // batch_size):\n      _logger.debug(\n          (\"Running EAD attack on instance %s of %s\",\n           i * batch_size, len(imgs)))\n      r.extend(\n          self.attack_batch(\n              imgs[i * batch_size:(i + 1) * batch_size],\n              targets[i * batch_size:(i + 1) * batch_size]))\n    if len(imgs) % batch_size != 0:\n      last_elements = len(imgs) - (len(imgs) % batch_size)\n      _logger.debug(\n          (\"Running EAD attack on instance %s of %s\",\n           last_elements, len(imgs)))\n      temp_imgs = np.zeros((batch_size, ) + imgs.shape[2:])\n      temp_targets = np.zeros((batch_size, ) + targets.shape[2:])\n      temp_imgs[:(len(imgs) % batch_size)] = imgs[last_elements:]\n      temp_targets[:(len(imgs) % batch_size)] = targets[last_elements:]\n      temp_data = self.attack_batch(temp_imgs, temp_targets)\n      r.extend(temp_data[:(len(imgs) % batch_size)],\n               targets[last_elements:])\n    return np.array(r)", "output": "Perform the EAD attack on the given instance for the given targets.\n\n    If self.targeted is true, then the targets represents the target labels\n    If self.targeted is false, then targets are the original class labels", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_timestamp_range_edges(first, last, offset, closed='left', base=0):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(offset, Tick):\n        if isinstance(offset, Day):\n            # _adjust_dates_anchored assumes 'D' means 24H, but first/last\n            # might contain a DST transition (23H, 24H, or 25H).\n            # So \"pretend\" the dates are naive when adjusting the endpoints\n            tz = first.tz\n            first = first.tz_localize(None)\n            last = last.tz_localize(None)\n\n        first, last = _adjust_dates_anchored(first, last, offset,\n                                             closed=closed, base=base)\n        if isinstance(offset, Day):\n            first = first.tz_localize(tz)\n            last = last.tz_localize(tz)\n        return first, last\n\n    else:\n        first = first.normalize()\n        last = last.normalize()\n\n    if closed == 'left':\n        first = Timestamp(offset.rollback(first))\n    else:\n        first = Timestamp(first - offset)\n\n    last = Timestamp(last + offset)\n\n    return first, last", "output": "Adjust the `first` Timestamp to the preceeding Timestamp that resides on\n    the provided offset. Adjust the `last` Timestamp to the following\n    Timestamp that resides on the provided offset. Input Timestamps that\n    already reside on the offset will be adjusted depending on the type of\n    offset and the `closed` parameter.\n\n    Parameters\n    ----------\n    first : pd.Timestamp\n        The beginning Timestamp of the range to be adjusted.\n    last : pd.Timestamp\n        The ending Timestamp of the range to be adjusted.\n    offset : pd.DateOffset\n        The dateoffset to which the Timestamps will be adjusted.\n    closed : {'right', 'left'}, default None\n        Which side of bin interval is closed.\n    base : int, default 0\n        The \"origin\" of the adjusted Timestamps.\n\n    Returns\n    -------\n    A tuple of length 2, containing the adjusted pd.Timestamp objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next(self):\n        \"\"\"\"\"\"\n        batch_size = self.batch_size\n        c, h, w = self.data_shape\n        # if last batch data is rolled over\n        if self._cache_data is not None:\n            # check both the data and label have values\n            assert self._cache_label is not None, \"_cache_label didn't have values\"\n            assert self._cache_idx is not None, \"_cache_idx didn't have values\"\n            batch_data = self._cache_data\n            batch_label = self._cache_label\n            i = self._cache_idx\n        else:\n            batch_data = nd.zeros((batch_size, c, h, w))\n            batch_label = nd.empty(self.provide_label[0][1])\n            batch_label[:] = -1\n            i = self._batchify(batch_data, batch_label)\n        # calculate the padding\n        pad = batch_size - i\n        # handle padding for the last batch\n        if pad != 0:\n            if self.last_batch_handle == 'discard':\n                raise StopIteration\n            # if the option is 'roll_over', throw StopIteration and cache the data\n            elif self.last_batch_handle == 'roll_over' and \\\n                self._cache_data is None:\n                self._cache_data = batch_data\n                self._cache_label = batch_label\n                self._cache_idx = i\n                raise StopIteration\n            else:\n                _ = self._batchify(batch_data, batch_label, i)\n                if self.last_batch_handle == 'pad':\n                    self._allow_read = False\n                else:\n                    self._cache_data = None\n                    self._cache_label = None\n                    self._cache_idx = None\n\n        return io.DataBatch([batch_data], [batch_label], pad=pad)", "output": "Override the function for returning next batch.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_simulated_env_fn(**env_kwargs):\n  \"\"\"\n  \"\"\"\n  def env_fn(in_graph):\n    class_ = SimulatedBatchEnv if in_graph else SimulatedBatchGymEnv\n    return class_(**env_kwargs)\n  return env_fn", "output": "Returns a function creating a simulated env, in or out of graph.\n\n  Args:\n    **env_kwargs: kwargs to pass to the simulated env constructor.\n\n  Returns:\n    Function in_graph -> env.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def call(self, cleanup_protecteds):\n        '''\n        \n        '''\n        try:\n            self.prepare()\n            if check_user(self.config['user']):\n                self.minion.opts['__role'] = kinds.APPL_KIND_NAMES[kinds.applKinds.caller]\n                self.minion.call_in()\n        except (KeyboardInterrupt, SaltSystemExit) as exc:\n            self.action_log_info('Stopping')\n            if isinstance(exc, KeyboardInterrupt):\n                log.warning('Exiting on Ctrl-c')\n                self.shutdown()\n            else:\n                log.error(exc)\n                self.shutdown(exc.code)", "output": "Start the actual minion as a caller minion.\n\n        cleanup_protecteds is list of yard host addresses that should not be\n        cleaned up this is to fix race condition when salt-caller minion starts up\n\n        If sub-classed, don't **ever** forget to run:\n\n            super(YourSubClass, self).start()\n\n        NOTE: Run any required code before calling `super()`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare_kwargs(all_kwargs, class_init_kwargs):\n    '''\n    \n    '''\n    fun_kwargs = {}\n    init_kwargs = {}\n    for karg, warg in six.iteritems(all_kwargs):\n        if karg not in class_init_kwargs:\n            if warg is not None:\n                fun_kwargs[karg] = warg\n            continue\n        if warg is not None:\n            init_kwargs[karg] = warg\n    return init_kwargs, fun_kwargs", "output": "Filter out the kwargs used for the init of the class and the kwargs used to\n    invoke the command required.\n\n    all_kwargs\n        All the kwargs the Execution Function has been invoked.\n\n    class_init_kwargs\n        The kwargs of the ``__init__`` of the class.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def standardize_mapping(into):\n    \"\"\"\n    \n    \"\"\"\n    if not inspect.isclass(into):\n        if isinstance(into, collections.defaultdict):\n            return partial(\n                collections.defaultdict, into.default_factory)\n        into = type(into)\n    if not issubclass(into, abc.Mapping):\n        raise TypeError('unsupported type: {into}'.format(into=into))\n    elif into == collections.defaultdict:\n        raise TypeError(\n            'to_dict() only accepts initialized defaultdicts')\n    return into", "output": "Helper function to standardize a supplied mapping.\n\n    .. versionadded:: 0.21.0\n\n    Parameters\n    ----------\n    into : instance or subclass of collections.abc.Mapping\n        Must be a class, an initialized collections.defaultdict,\n        or an instance of a collections.abc.Mapping subclass.\n\n    Returns\n    -------\n    mapping : a collections.abc.Mapping subclass or other constructor\n        a callable object that can accept an iterator to create\n        the desired Mapping.\n\n    See Also\n    --------\n    DataFrame.to_dict\n    Series.to_dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_reg(data):\n    '''\n    \n    '''\n    reg_dir = _reg_dir()\n    regfile = os.path.join(reg_dir, 'register')\n    try:\n        if not os.path.exists(reg_dir):\n            os.makedirs(reg_dir)\n    except OSError as exc:\n        if exc.errno == errno.EEXIST:\n            pass\n        else:\n            raise\n    try:\n        with salt.utils.files.fopen(regfile, 'a') as fh_:\n            salt.utils.msgpack.dump(data, fh_)\n    except Exception:\n        log.error('Could not write to msgpack file %s', __opts__['outdir'])\n        raise", "output": "Save the register to msgpack files", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_batch_begin(self, last_input, last_target, **kwargs):\n        \"\"\n        if self.clip is not None:\n            for p in self.critic.parameters(): p.data.clamp_(-self.clip, self.clip)\n        return {'last_input':last_input,'last_target':last_target} if self.gen_mode else {'last_input':last_target,'last_target':last_input}", "output": "Clamp the weights with `self.clip` if it's not None, return the correct input.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cardinality(gym_space):\n  \"\"\"\n  \"\"\"\n\n  if (gym_space.dtype == np.float32) or (gym_space.dtype == np.float64):\n    tf.logging.error(\"Returning None for a float gym space's cardinality: \",\n                     gym_space)\n    return None\n\n  if isinstance(gym_space, Discrete):\n    return gym_space.n\n\n  if isinstance(gym_space, Box):\n    # Construct a box with all possible values in this box and take a product.\n    return np.prod(gym_space.high - gym_space.low + 1)\n\n  raise NotImplementedError", "output": "Number of elements that can be represented by the space.\n\n  Makes the most sense for Discrete or Box type with integral dtype, ex: number\n  of actions in an action space.\n\n  Args:\n    gym_space: The gym space.\n\n  Returns:\n    np.int64 number of observations that can be represented by this space, or\n    returns None when this doesn't make sense, i.e. float boxes etc.\n\n  Raises:\n    NotImplementedError when a space's cardinality makes sense but we haven't\n    implemented it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _maybe_coerce_indexer(self, indexer):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(indexer, np.ndarray) and indexer.dtype.kind == 'i':\n            indexer = indexer.astype(self._codes.dtype)\n        return indexer", "output": "return an indexer coerced to the codes dtype", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_lrn(node, **kwargs):\n    \"\"\"\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    alpha = float(attrs.get(\"alpha\", 0.0001))\n    beta = float(attrs.get(\"beta\", 0.75))\n    bias = float(attrs.get(\"knorm\", 1.0))\n    size = int(attrs.get(\"nsize\"))\n\n    lrn_node = onnx.helper.make_node(\n        \"LRN\",\n        inputs=input_nodes,\n        outputs=[name],\n        name=name,\n        alpha=alpha,\n        beta=beta,\n        bias=bias,\n        size=size\n    )\n\n    return [lrn_node]", "output": "Map MXNet's LRN operator attributes to onnx's LRN operator\n    and return the created node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process(fname, allow_type):\n    \"\"\"\"\"\"\n    fname = str(fname)\n    # HACK: ignore op.h which is automatically generated\n    if fname.endswith('op.h'):\n      return\n    arr = fname.rsplit('.', 1)\n    if fname.find('#') != -1 or arr[-1] not in allow_type:\n        return\n    if arr[-1] in CXX_SUFFIX:\n        _HELPER.process_cpp(fname, arr[-1])\n    if arr[-1] in PYTHON_SUFFIX:\n        _HELPER.process_python(fname)", "output": "Process a file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_dataset(instruction_dicts,\n                  dataset_from_file_fn,\n                  shuffle_files=False,\n                  parallel_reads=64):\n  \"\"\"\n  \"\"\"\n\n  # First case: All examples are taken (No value skipped)\n  if _no_examples_skipped(instruction_dicts):\n    # Only use the filenames as instruction\n    instruction_ds = tf.data.Dataset.from_tensor_slices([\n        d[\"filepath\"] for d in instruction_dicts\n    ])\n    build_ds_from_instruction = dataset_from_file_fn\n  # Second case: Use the instructions to read the examples\n  else:\n    instruction_ds = _build_instruction_ds(instruction_dicts)\n    build_ds_from_instruction = functools.partial(\n        _build_ds_from_instruction,\n        ds_from_file_fn=dataset_from_file_fn,\n    )\n\n  # If shuffle is True, we shuffle the instructions/shards\n  if shuffle_files:\n    instruction_ds = instruction_ds.shuffle(len(instruction_dicts))\n\n  # Use interleave to parallel read files and decode records\n  ds = instruction_ds.interleave(\n      build_ds_from_instruction,\n      cycle_length=parallel_reads,\n      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  return ds", "output": "Constructs a `tf.data.Dataset` from TFRecord files.\n\n  Args:\n    instruction_dicts: `list` of {'filepath':, 'mask':, 'offset_mask':}\n      containing the information about which files and which examples to use.\n      The boolean mask will be repeated and zipped with the examples from\n      filepath.\n    dataset_from_file_fn: function returning a `tf.data.Dataset` given a\n      filename.\n    shuffle_files: `bool`, Whether to shuffle the input filenames.\n    parallel_reads: `int`, how many files to read in parallel.\n\n  Returns:\n    `tf.data.Dataset`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _join_index_objects(self, axis, other_index, how, sort=True):\n        \"\"\"\n        \"\"\"\n        if isinstance(other_index, list):\n            joined_obj = self.columns if not axis else self.index\n            # TODO: revisit for performance\n            for obj in other_index:\n                joined_obj = joined_obj.join(obj, how=how)\n\n            return joined_obj\n        if not axis:\n            return self.columns.join(other_index, how=how, sort=sort)\n        else:\n            return self.index.join(other_index, how=how, sort=sort)", "output": "Joins a pair of index objects (columns or rows) by a given strategy.\n\n        Args:\n            axis: The axis index object to join (0 for columns, 1 for index).\n            other_index: The other_index to join on.\n            how: The type of join to join to make (e.g. right, left).\n\n        Returns:\n            Joined indices.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_so_far(self):\n        \"\"\"\n        \n        \"\"\"\n        position = self.packet.tell()\n        self.rewind()\n        return self.packet.read(position)", "output": "Returns the `str` bytes of this message that have been parsed and\n        returned. The string passed into a message's constructor can be\n        regenerated by concatenating ``get_so_far`` and `get_remainder`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_queue(queue, kwargs):\n    '''\n    \n    '''\n    if queue:\n        _wait(kwargs.get('__pub_jid'))\n    else:\n        conflict = running(concurrent=kwargs.get('concurrent', False))\n        if conflict:\n            __context__['retcode'] = 1\n            return conflict", "output": "Utility function to queue the state run if requested\n    and to check for conflicts in currently running states", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def name_match(self, match, full=False):\n        '''\n        \n        '''\n        if full:\n            matches = self.all_keys()\n        else:\n            matches = self.list_keys()\n        ret = {}\n        if ',' in match and isinstance(match, six.string_types):\n            match = match.split(',')\n        for status, keys in six.iteritems(matches):\n            for key in salt.utils.data.sorted_ignorecase(keys):\n                if isinstance(match, list):\n                    for match_item in match:\n                        if fnmatch.fnmatch(key, match_item):\n                            if status not in ret:\n                                ret[status] = []\n                            ret[status].append(key)\n                else:\n                    if fnmatch.fnmatch(key, match):\n                        if status not in ret:\n                            ret[status] = []\n                        ret[status].append(key)\n        return ret", "output": "Accept a glob which to match the of a key and return the key's location", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _can_hold_identifiers_and_holds_name(self, name):\n        \"\"\"\n        \n        \"\"\"\n        if self.is_object() or self.is_categorical():\n            return name in self\n        return False", "output": "Faster check for ``name in self`` when we know `name` is a Python\n        identifier (e.g. in NDFrame.__getattr__, which hits this to support\n        . key lookup). For indexes that can't hold identifiers (everything\n        but object & categorical) we just return False.\n\n        https://github.com/pandas-dev/pandas/issues/19764", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_trial_complete(self,\n                          trial_id,\n                          result=None,\n                          error=False,\n                          early_terminated=False):\n        \"\"\"\"\"\"\n        if result:\n            self.optimizer.register(\n                params=self._live_trial_mapping[trial_id],\n                target=result[self._reward_attr])\n\n        del self._live_trial_mapping[trial_id]", "output": "Passes the result to BayesOpt unless early terminated or errored", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_restart_power_failure(enabled):\n    '''\n    \n    '''\n    state = salt.utils.mac_utils.validate_enabled(enabled)\n    cmd = 'systemsetup -setrestartpowerfailure {0}'.format(state)\n    salt.utils.mac_utils.execute_return_success(cmd)\n\n    return salt.utils.mac_utils.confirm_updated(\n        state,\n        get_restart_power_failure,\n    )", "output": "Set whether or not the computer will automatically restart after a power\n    failure.\n\n    :param bool enabled: True to enable, False to disable. \"On\" and \"Off\" are\n        also acceptable values. Additionally you can pass 1 and 0 to represent\n        True and False respectively\n\n    :return: True if successful, False if not\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' power.set_restart_power_failure True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def id(self):\n        \"\"\"\n        \"\"\"\n        def normalize(distro_id, table):\n            distro_id = distro_id.lower().replace(' ', '_')\n            return table.get(distro_id, distro_id)\n\n        distro_id = self.os_release_attr('id')\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_OS_ID)\n\n        distro_id = self.lsb_release_attr('distributor_id')\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_LSB_ID)\n\n        distro_id = self.distro_release_attr('id')\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_DISTRO_ID)\n\n        distro_id = self.uname_attr('id')\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_DISTRO_ID)\n\n        return ''", "output": "Return the distro ID of the OS distribution, as a string.\n\n        For details, see :func:`distro.id`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_indexes_all_same(indexes, message=\"Indexes are not equal.\"):\n    \"\"\"\n    \"\"\"\n    iterator = iter(indexes)\n    first = next(iterator)\n    for other in iterator:\n        same = (first == other)\n        if not same.all():\n            bad_loc = np.flatnonzero(~same)[0]\n            raise ValueError(\n                \"{}\\nFirst difference is at index {}: \"\n                \"{} != {}\".format(\n                    message, bad_loc, first[bad_loc], other[bad_loc]\n                ),\n            )", "output": "Check that a list of Index objects are all equal.\n\n    Parameters\n    ----------\n    indexes : iterable[pd.Index]\n        Iterable of indexes to check.\n\n    Raises\n    ------\n    ValueError\n        If the indexes are not all the same.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_groups(name):\n    '''\n    \n    '''\n    if six.PY2:\n        name = _to_unicode(name)\n\n    ugrp = set()\n    try:\n        user = info(name)['groups']\n    except KeyError:\n        return False\n    for group in user:\n        ugrp.add(group.strip(' *'))\n\n    return sorted(list(ugrp))", "output": "Return a list of groups the named user belongs to\n\n    Args:\n        name (str): The user name for which to list groups\n\n    Returns:\n        list: A list of groups to which the user belongs\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.list_groups foo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_service_port(service, port):\n    '''\n    \n    '''\n    if service not in get_services(permanent=True):\n        raise CommandExecutionError('The service does not exist.')\n\n    cmd = '--permanent --service={0} --add-port={1}'.format(service, port)\n    return __firewall_cmd(cmd)", "output": "Add a new port to the specified service.\n\n    .. versionadded:: 2016.11.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' firewalld.add_service_port zone 80", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_instance(name, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The show_instance action must be called with -a or --action.'\n        )\n\n    nodes = list_nodes_full()\n    # Find under which cloud service the name is listed, if any\n    if name not in nodes:\n        return {}\n    __utils__['cloud.cache_node'](nodes[name], __active_provider_name__, __opts__)\n    return nodes[name]", "output": "Show the details from the provider concerning an instance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def call_brightness(*args, **kwargs):\n    '''\n    \n    '''\n    res = dict()\n\n    if 'value' not in kwargs:\n        raise CommandExecutionError(\"Parameter 'value' is missing\")\n\n    try:\n        brightness = max(min(int(kwargs['value']), 244), 1)\n    except Exception as err:\n        raise CommandExecutionError(\"Parameter 'value' does not contains an integer\")\n\n    try:\n        transition = max(min(int(kwargs['transition']), 200), 0)\n    except Exception as err:\n        transition = 0\n\n    devices = _get_lights()\n    for dev_id in 'id' not in kwargs and sorted(devices.keys()) or _get_devices(kwargs):\n        res[dev_id] = _set(dev_id, {\"bri\": brightness, \"transitiontime\": transition})\n\n    return res", "output": "Set an effect to the lamp.\n\n    Arguments:\n\n    * **value**: 0~255 brightness of the lamp.\n\n    Options:\n\n    * **id**: Specifies a device ID. Can be a comma-separated values. All, if omitted.\n    * **transition**: Transition 0~200. Default 0.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' hue.brightness value=100\n        salt '*' hue.brightness id=1 value=150\n        salt '*' hue.brightness id=1,2,3 value=255", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def confirmation_option(*param_decls, **attrs):\n    \"\"\"\n    \"\"\"\n    def decorator(f):\n        def callback(ctx, param, value):\n            if not value:\n                ctx.abort()\n        attrs.setdefault('is_flag', True)\n        attrs.setdefault('callback', callback)\n        attrs.setdefault('expose_value', False)\n        attrs.setdefault('prompt', 'Do you want to continue?')\n        attrs.setdefault('help', 'Confirm the action without prompting.')\n        return option(*(param_decls or ('--yes',)), **attrs)(f)\n    return decorator", "output": "Shortcut for confirmation prompts that can be ignored by passing\n    ``--yes`` as parameter.\n\n    This is equivalent to decorating a function with :func:`option` with\n    the following parameters::\n\n        def callback(ctx, param, value):\n            if not value:\n                ctx.abort()\n\n        @click.command()\n        @click.option('--yes', is_flag=True, callback=callback,\n                      expose_value=False, prompt='Do you want to continue?')\n        def dropdb():\n            pass", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _merge_pool_kwargs(self, override):\n        \"\"\"\n        \n        \"\"\"\n        base_pool_kwargs = self.connection_pool_kw.copy()\n        if override:\n            for key, value in override.items():\n                if value is None:\n                    try:\n                        del base_pool_kwargs[key]\n                    except KeyError:\n                        pass\n                else:\n                    base_pool_kwargs[key] = value\n        return base_pool_kwargs", "output": "Merge a dictionary of override values for self.connection_pool_kw.\n\n        This does not modify self.connection_pool_kw and returns a new dict.\n        Any keys in the override dictionary with a value of ``None`` are\n        removed from the merged dictionary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _human_score_map(human_consensus, methods_attrs):\n    \"\"\" \n    \"\"\"\n\n    v = 1 - min(np.sum(np.abs(methods_attrs - human_consensus)) / (np.abs(human_consensus).sum() + 1), 1.0)\n    return v", "output": "Converts human agreement differences to numerical scores for coloring.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def request(self, path, api='public', method='GET', params={}, headers=None, body=None):\n        \"\"\"\"\"\"\n        return self.fetch2(path, api, method, params, headers, body)", "output": "Exchange.request is the entry point for all generated methods", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _clear_cache(tgt=None,\n                 tgt_type='glob',\n                 clear_pillar_flag=False,\n                 clear_grains_flag=False,\n                 clear_mine_flag=False,\n                 clear_mine_func_flag=None):\n    '''\n    \n    '''\n    if tgt is None:\n        return False\n    pillar_util = salt.utils.master.MasterPillarUtil(tgt, tgt_type,\n                                                     use_cached_grains=True,\n                                                     grains_fallback=False,\n                                                     use_cached_pillar=True,\n                                                     pillar_fallback=False,\n                                                     opts=__opts__)\n    return pillar_util.clear_cached_minion_data(clear_pillar=clear_pillar_flag,\n                                                clear_grains=clear_grains_flag,\n                                                clear_mine=clear_mine_flag,\n                                                clear_mine_func=clear_mine_func_flag)", "output": "Clear the cached data/files for the targeted minions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mod_watch(name, **kwargs):\n    '''\n    \n\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': False,\n           'comment': ''}\n\n    if kwargs['sfun'] == 'watch':\n        for p in ['sfun', '__reqs__']:\n            del kwargs[p]\n        kwargs['name'] = name\n        ret = present(**kwargs)\n\n    return ret", "output": "The at watcher, called to invoke the watch command.\n\n    .. note::\n        This state exists to support special handling of the ``watch``\n        :ref:`requisite <requisites>`. It should not be called directly.\n\n        Parameters for this function should be set by the state being triggered.\n\n    name\n        The name of the atjob", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def key_ring_path(cls, project, location, key_ring):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/locations/{location}/keyRings/{key_ring}\",\n            project=project,\n            location=location,\n            key_ring=key_ring,\n        )", "output": "Return a fully-qualified key_ring string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def call(fun, **kwargs):\n    '''\n    \n    '''\n    args = kwargs.get('args', [])\n    dirs = kwargs.get('dirs', [])\n\n    funcs = LazyLoader(\n        [os.path.join(SALT_BASE_PATH, 'modules')] + dirs,\n        None,\n        tag='modules',\n        virtual_enable=False,\n    )\n    return funcs[fun](*args)", "output": "Directly call a function inside a loader directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_epoch_end(self, last_metrics, **kwargs):\n        \"\"\n        return add_metrics(last_metrics, [s.smooth for k,s in self.smootheners.items()])", "output": "Put the various losses in the recorder.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_data(limit=0, split=0.8):\n    \"\"\"\"\"\"\n    # Partition off part of the train data for evaluation\n    train_data, _ = thinc.extra.datasets.imdb()\n    random.shuffle(train_data)\n    train_data = train_data[-limit:]\n    texts, labels = zip(*train_data)\n    cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in labels]\n    split = int(len(train_data) * split)\n    return (texts[:split], cats[:split]), (texts[split:], cats[split:])", "output": "Load data from the IMDB dataset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def between(self, left, right, inclusive=True):\n        \"\"\"\n        \n        \"\"\"\n        if inclusive:\n            lmask = self >= left\n            rmask = self <= right\n        else:\n            lmask = self > left\n            rmask = self < right\n\n        return lmask & rmask", "output": "Return boolean Series equivalent to left <= series <= right.\n\n        This function returns a boolean vector containing `True` wherever the\n        corresponding Series element is between the boundary values `left` and\n        `right`. NA values are treated as `False`.\n\n        Parameters\n        ----------\n        left : scalar\n            Left boundary.\n        right : scalar\n            Right boundary.\n        inclusive : bool, default True\n            Include boundaries.\n\n        Returns\n        -------\n        Series\n            Series representing whether each element is between left and\n            right (inclusive).\n\n        See Also\n        --------\n        Series.gt : Greater than of series and other.\n        Series.lt : Less than of series and other.\n\n        Notes\n        -----\n        This function is equivalent to ``(left <= ser) & (ser <= right)``\n\n        Examples\n        --------\n        >>> s = pd.Series([2, 0, 4, 8, np.nan])\n\n        Boundary values are included by default:\n\n        >>> s.between(1, 4)\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        dtype: bool\n\n        With `inclusive` set to ``False`` boundary values are excluded:\n\n        >>> s.between(1, 4, inclusive=False)\n        0     True\n        1    False\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        `left` and `right` can be any scalar value:\n\n        >>> s = pd.Series(['Alice', 'Bob', 'Carol', 'Eve'])\n        >>> s.between('Anna', 'Daniel')\n        0    False\n        1     True\n        2     True\n        3    False\n        dtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def increment(key, delta=1, host=DEFAULT_HOST, port=DEFAULT_PORT):\n    '''\n    \n    '''\n    conn = _connect(host, port)\n    _check_stats(conn)\n    cur = get(key)\n\n    if cur is None:\n        raise CommandExecutionError('Key \\'{0}\\' does not exist'.format(key))\n    elif not isinstance(cur, six.integer_types):\n        raise CommandExecutionError(\n            'Value for key \\'{0}\\' must be an integer to be '\n            'incremented'.format(key)\n        )\n\n    try:\n        return conn.incr(key, delta)\n    except ValueError:\n        raise SaltInvocationError('Delta value must be an integer')", "output": "Increment the value of a key\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' memcached.increment <key>\n        salt '*' memcached.increment <key> 2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def EnumValueName(self, enum, value):\n    \"\"\"\n    \"\"\"\n    return self.enum_types_by_name[enum].values_by_number[value].name", "output": "Returns the string name of an enum value.\n\n    This is just a small helper method to simplify a common operation.\n\n    Args:\n      enum: string name of the Enum.\n      value: int, value of the enum.\n\n    Returns:\n      string name of the enum value.\n\n    Raises:\n      KeyError if either the Enum doesn't exist or the value is not a valid\n        value for the enum.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_weight(self, weight):\n        \"\"\"\n        \"\"\"\n        if weight is not None and np.all(weight == 1):\n            weight = None\n        self.weight = weight\n        if self.handle is not None and weight is not None:\n            weight = list_to_1d_numpy(weight, name='weight')\n            self.set_field('weight', weight)\n        return self", "output": "Set weight of each instance.\n\n        Parameters\n        ----------\n        weight : list, numpy 1-D array, pandas Series or None\n            Weight to be set for each data point.\n\n        Returns\n        -------\n        self : Dataset\n            Dataset with set weight.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extensions(self):\n        \"\"\"\n        \n        \"\"\"\n        encoded_extensions = []\n        for ext in self._extension_files:\n            file_ = open(ext, 'rb')\n            # Should not use base64.encodestring() which inserts newlines every\n            # 76 characters (per RFC 1521).  Chromedriver has to remove those\n            # unnecessary newlines before decoding, causing performance hit.\n            encoded_extensions.append(base64.b64encode(file_.read()).decode('UTF-8'))\n\n            file_.close()\n        return encoded_extensions + self._extensions", "output": ":Returns: A list of encoded extensions that will be loaded into chrome", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_resource_id_from_stack(cfn_client, stack_name, logical_id):\n        \"\"\"\n        \n        \"\"\"\n\n        LOG.debug(\"Getting resource's PhysicalId from AWS CloudFormation stack. StackName=%s, LogicalId=%s\",\n                  stack_name, logical_id)\n\n        try:\n            response = cfn_client.describe_stack_resource(StackName=stack_name, LogicalResourceId=logical_id)\n\n            LOG.debug(\"Response from AWS CloudFormation %s\", response)\n            return response[\"StackResourceDetail\"][\"PhysicalResourceId\"]\n\n        except botocore.exceptions.ClientError as ex:\n            LOG.debug(\"Unable to fetch resource name from CloudFormation Stack: \"\n                      \"StackName=%s, ResourceLogicalId=%s, Response=%s\", stack_name, logical_id, ex.response)\n\n            # The exception message already has a well formatted error message that we can surface to user\n            raise UserException(str(ex))", "output": "Given the LogicalID of a resource, call AWS CloudFormation to get physical ID of the resource within\n        the specified stack.\n\n        Parameters\n        ----------\n        cfn_client\n            CloudFormation client provided by AWS SDK\n\n        stack_name : str\n            Name of the stack to query\n\n        logical_id : str\n            LogicalId of the resource\n\n        Returns\n        -------\n        str\n            Physical ID of the resource\n\n        Raises\n        ------\n        samcli.commands.exceptions.UserException\n            If the stack or resource does not exist", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _block_shape(values, ndim=1, shape=None):\n    \"\"\"  \"\"\"\n    if values.ndim < ndim:\n        if shape is None:\n            shape = values.shape\n        if not is_extension_array_dtype(values):\n            # TODO: https://github.com/pandas-dev/pandas/issues/23023\n            # block.shape is incorrect for \"2D\" ExtensionArrays\n            # We can't, and don't need to, reshape.\n            values = values.reshape(tuple((1, ) + shape))\n    return values", "output": "guarantee the shape of the values to be at least 1 d", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_repo(name, basedir=None, **kwargs):  # pylint: disable=W0613\n    '''\n    \n    '''\n    repos = list_repos(basedir)\n\n    # Find out what file the repo lives in\n    repofile = ''\n    for repo in repos:\n        if repo == name:\n            repofile = repos[repo]['file']\n\n    if repofile:\n        # Return just one repo\n        filerepos = _parse_repo_file(repofile)[1]\n        return filerepos[name]\n    return {}", "output": "Display a repo from <basedir> (default basedir: all dirs in ``reposdir``\n    yum option).\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' pkg.get_repo myrepo\n        salt '*' pkg.get_repo myrepo basedir=/path/to/dir\n        salt '*' pkg.get_repo myrepo basedir=/path/to/dir,/path/to/another/dir", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(attributes, properties):\n    \"\"\"\"\"\"\n    if isinstance(attributes, basestring):\n        attributes = [attributes]\n    assert is_iterable_typed(attributes, basestring)\n    assert is_iterable_typed(properties, basestring)\n    result = []\n    for e in properties:\n        attributes_new = feature.attributes(get_grist(e))\n        has_common_features = 0\n        for a in attributes_new:\n            if a in attributes:\n                has_common_features = 1\n                break\n\n        if not has_common_features:\n            result += e\n\n    return result", "output": "Returns a property sets which include all the elements\n    in 'properties' that do not have attributes listed in 'attributes'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _do_perspective_warp(c:FlowField, targ_pts:Points, invert=False):\n    \"\"\n    if invert: return _apply_perspective(c, _find_coeffs(targ_pts, _orig_pts))\n    return _apply_perspective(c, _find_coeffs(_orig_pts, targ_pts))", "output": "Apply warp to `targ_pts` from `_orig_pts` to `c` `FlowField`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _determine_timeout(default_timeout, specified_timeout, retry):\n    \"\"\"\n    \"\"\"\n    if specified_timeout is DEFAULT:\n        specified_timeout = default_timeout\n\n    if specified_timeout is default_timeout:\n        # If timeout is the default and the default timeout is exponential and\n        # a non-default retry is specified, make sure the timeout's deadline\n        # matches the retry's. This handles the case where the user leaves\n        # the timeout default but specifies a lower deadline via the retry.\n        if (\n            retry\n            and retry is not DEFAULT\n            and isinstance(default_timeout, timeout.ExponentialTimeout)\n        ):\n            return default_timeout.with_deadline(retry._deadline)\n        else:\n            return default_timeout\n\n    # If timeout is specified as a number instead of a Timeout instance,\n    # convert it to a ConstantTimeout.\n    if isinstance(specified_timeout, (int, float)):\n        return timeout.ConstantTimeout(specified_timeout)\n    else:\n        return specified_timeout", "output": "Determines how timeout should be applied to a wrapped method.\n\n    Args:\n        default_timeout (Optional[Timeout]): The default timeout specified\n            at method creation time.\n        specified_timeout (Optional[Timeout]): The timeout specified at\n            invocation time. If :attr:`DEFAULT`, this will be set to\n            the ``default_timeout``.\n        retry (Optional[Retry]): The retry specified at invocation time.\n\n    Returns:\n        Optional[Timeout]: The timeout to apply to the method or ``None``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_filter(self, filter_values):\n        \"\"\"\n        \n        \"\"\"\n        if not filter_values:\n            return\n\n        f = self.get_value_filter(filter_values[0])\n        for v in filter_values[1:]:\n            f |= self.get_value_filter(v)\n        return f", "output": "Construct a filter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def squeeze(name, x, factor=2, reverse=True):\n  \"\"\"\n  \"\"\"\n  with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n    shape = common_layers.shape_list(x)\n    if factor == 1:\n      return x\n    height = int(shape[1])\n    width = int(shape[2])\n    n_channels = int(shape[3])\n\n    if not reverse:\n      assert height % factor == 0 and width % factor == 0\n      x = tf.reshape(x, [-1, height//factor, factor,\n                         width//factor, factor, n_channels])\n      x = tf.transpose(x, [0, 1, 3, 5, 2, 4])\n      x = tf.reshape(x, [-1, height//factor, width //\n                         factor, n_channels*factor*factor])\n    else:\n      x = tf.reshape(\n          x, (-1, height, width, int(n_channels/factor**2), factor, factor))\n      x = tf.transpose(x, [0, 1, 4, 2, 5, 3])\n      x = tf.reshape(x, (-1, int(height*factor),\n                         int(width*factor), int(n_channels/factor**2)))\n    return x", "output": "Block-wise spatial squeezing of x to increase the number of channels.\n\n  Args:\n    name: Used for variable scoping.\n    x: 4-D Tensor of shape (batch_size X H X W X C)\n    factor: Factor by which the spatial dimensions should be squeezed.\n    reverse: Squueze or unsqueeze operation.\n\n  Returns:\n    x: 4-D Tensor of shape (batch_size X (H//factor) X (W//factor) X\n       (cXfactor^2). If reverse is True, then it is factor = (1 / factor)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def destroy_dns_records(fqdn):\n    '''\n    \n    '''\n    domain = '.'.join(fqdn.split('.')[-2:])\n    hostname = '.'.join(fqdn.split('.')[:-2])\n    # TODO: remove this when the todo on 754 is available\n    try:\n        response = query(method='domains', droplet_id=domain, command='records')\n    except SaltCloudSystemExit:\n        log.debug('Failed to find domains.')\n        return False\n    log.debug(\"found DNS records: %s\", pprint.pformat(response))\n    records = response['domain_records']\n\n    if records:\n        record_ids = [r['id'] for r in records if r['name'].decode() == hostname]\n        log.debug(\"deleting DNS record IDs: %s\", record_ids)\n        for id_ in record_ids:\n            try:\n                log.info('deleting DNS record %s', id_)\n                ret = query(\n                    method='domains',\n                    droplet_id=domain,\n                    command='records/{0}'.format(id_),\n                    http_method='delete'\n                )\n            except SaltCloudSystemExit:\n                log.error('failed to delete DNS domain %s record ID %s.', domain, hostname)\n            log.debug('DNS deletion REST call returned: %s', pprint.pformat(ret))\n\n    return False", "output": "Deletes DNS records for the given hostname if the domain is managed with DO.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def choose_jira_assignee(issue, asf_jira):\n    \"\"\"\n    \n    \"\"\"\n    while True:\n        try:\n            reporter = issue.fields.reporter\n            commentors = map(lambda x: x.author, issue.fields.comment.comments)\n            candidates = set(commentors)\n            candidates.add(reporter)\n            candidates = list(candidates)\n            print(\"JIRA is unassigned, choose assignee\")\n            for idx, author in enumerate(candidates):\n                if author.key == \"apachespark\":\n                    continue\n                annotations = [\"Reporter\"] if author == reporter else []\n                if author in commentors:\n                    annotations.append(\"Commentor\")\n                print(\"[%d] %s (%s)\" % (idx, author.displayName, \",\".join(annotations)))\n            raw_assignee = input(\n                \"Enter number of user, or userid, to assign to (blank to leave unassigned):\")\n            if raw_assignee == \"\":\n                return None\n            else:\n                try:\n                    id = int(raw_assignee)\n                    assignee = candidates[id]\n                except:\n                    # assume it's a user id, and try to assign (might fail, we just prompt again)\n                    assignee = asf_jira.user(raw_assignee)\n                asf_jira.assign_issue(issue.key, assignee.key)\n                return assignee\n        except KeyboardInterrupt:\n            raise\n        except:\n            traceback.print_exc()\n            print(\"Error assigning JIRA, try again (or leave blank and fix manually)\")", "output": "Prompt the user to choose who to assign the issue to in jira, given a list of candidates,\n    including the original reporter and all commentors", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def name(self):\n        \"\"\"\n        \"\"\"\n        project = self._instance._client.project\n        instance_id = self._instance.instance_id\n        table_client = self._instance._client.table_data_client\n        return table_client.table_path(\n            project=project, instance=instance_id, table=self.table_id\n        )", "output": "Table name used in requests.\n\n        For example:\n\n        .. literalinclude:: snippets_table.py\n            :start-after: [START bigtable_table_name]\n            :end-before: [END bigtable_table_name]\n\n        .. note::\n\n          This property will not change if ``table_id`` does not, but the\n          return value is not cached.\n\n        The table name is of the form\n\n            ``\"projects/../instances/../tables/{table_id}\"``\n\n        :rtype: str\n        :returns: The table name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def js_extractor(response):\r\n    \"\"\"\"\"\"\r\n    # Extract .js files\r\n    matches = rscript.findall(response)\r\n    for match in matches:\r\n        match = match[2].replace('\\'', '').replace('\"', '')\r\n        verb('JS file', match)\r\n        bad_scripts.add(match)", "output": "Extract js files from the response body", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_param(self, param_name, layer_index, blob_index):\n        \"\"\"\"\"\"\n        blobs = self.layers[layer_index].blobs\n        self.dict_param[param_name] = mx.nd.array(caffe.io.blobproto_to_array(blobs[blob_index]))", "output": "Add a param to the .params file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def status(name, sig=None):\n    '''\n    \n    '''\n    if sig:\n        return bool(__salt__['status.pid'](sig))\n\n    contains_globbing = bool(re.search(r'\\*|\\?|\\[.+\\]', name))\n    if contains_globbing:\n        services = fnmatch.filter(get_all(), name)\n    else:\n        services = [name]\n    results = {}\n    for service in services:\n        cmd = _service_cmd(service, 'status')\n        results[service] = not _ret_code(cmd, ignore_retcode=True)\n    if contains_globbing:\n        return results\n    return results[name]", "output": "Return the status for a service.\n    If the name contains globbing, a dict mapping service name to True/False\n    values is returned.\n\n    .. versionchanged:: 2018.3.0\n        The service name can now be a glob (e.g. ``salt*``)\n\n    Args:\n        name (str): The name of the service to check\n        sig (str): Signature to use to find the service via ps\n\n    Returns:\n        bool: True if running, False otherwise\n        dict: Maps service name to True if running, False otherwise\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.status <service name> [service signature]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_upsample(self, name, scaling_factor_h, scaling_factor_w, input_name, output_name, mode = 'NN'):\n        \"\"\"\n        \n        \"\"\"\n        spec = self.spec\n        nn_spec = self.nn_spec\n\n        # Add a new inner-product layer\n        spec_layer = nn_spec.layers.add()\n        spec_layer.name = name\n        spec_layer.input.append(input_name)\n        spec_layer.output.append(output_name)\n        spec_layer_params = spec_layer.upsample\n        spec_layer_params.scalingFactor.append(scaling_factor_h)\n        spec_layer_params.scalingFactor.append(scaling_factor_w)\n        if mode == 'NN':\n            spec_layer_params.mode = _NeuralNetwork_pb2.UpsampleLayerParams.InterpolationMode.Value('NN')\n        elif mode == 'BILINEAR':\n            spec_layer_params.mode = _NeuralNetwork_pb2.UpsampleLayerParams.InterpolationMode.Value('BILINEAR')\n        else:\n            raise ValueError(\"Unsupported upsampling mode %s\" % mode)", "output": "Add upsample layer to the model.\n\n        Parameters\n        ----------\n        name: str\n            The name of this layer.\n        scaling_factor_h: int\n            Scaling factor on the vertical direction.\n        scaling_factor_w: int\n            Scaling factor on the horizontal direction.\n        input_name: str\n            The input blob name of this layer.\n        output_name: str\n            The output blob name of this layer.\n        mode: str\n            Following values are supported:\n            'NN': nearest neighbour\n            'BILINEAR' : bilinear interpolation\n\n        See Also\n        --------\n        add_sequence_repeat, add_elementwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _reconstruct_flattened_structure(structure, flattened):\n    \"\"\"\n    \"\"\"\n    if isinstance(structure, list):\n        return list(_reconstruct_flattened_structure(x, flattened) for x in structure)\n    elif isinstance(structure, tuple):\n        return tuple(_reconstruct_flattened_structure(x, flattened) for x in structure)\n    elif isinstance(structure, dict):\n        return {k: _reconstruct_flattened_structure(v, flattened) for k, v in structure.items()}\n    elif isinstance(structure, int):\n        return flattened[structure]\n    else:\n        raise NotImplementedError", "output": "Reconstruct the flattened list back to (possibly) nested structure.\n\n    Parameters\n    ----------\n    structure : An integer or a nested container with integers.\n        The extracted structure of the container of `data`.\n    flattened : list or None\n        The container thats holds flattened result.\n    Returns\n    -------\n    data : A single NDArray/Symbol or nested container with NDArrays/Symbol.\n        The nested container that was flattened.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop(name=None):\n    '''\n    \n\n    '''\n    pids = __salt__['ps.pgrep'](pattern='syslog-ng')\n\n    if not pids:\n        return _format_state_result(name,\n                                    result=False,\n                                    comment='Syslog-ng is not running')\n\n    if __opts__.get('test', False):\n        comment = 'Syslog_ng state module will kill {0} pids'\n        return _format_state_result(name, result=None, comment=comment)\n\n    res = __salt__['ps.pkill']('syslog-ng')\n    killed_pids = res['killed']\n\n    if killed_pids == pids:\n        changes = {'old': killed_pids, 'new': []}\n        return _format_state_result(name, result=True, changes=changes)\n    else:\n        return _format_state_result(name, result=False)", "output": "Kills syslog-ng. This function is intended to be used from the state module.\n\n    Users shouldn't use this function, if the service module is available on\n    their system.  If :mod:`syslog_ng.set_config_file\n    <salt.modules.syslog_ng.set_binary_path>` is called before, this function\n    will use the set binary path.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' syslog_ng.stop", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resume(jid, state_id=None):\n    '''\n    \n    '''\n    jid = six.text_type(jid)\n    if state_id is None:\n        state_id = '__all__'\n    data, pause_path = _get_pause(jid, state_id)\n    if state_id in data:\n        data.pop(state_id)\n    if state_id == '__all__':\n        data = {}\n    with salt.utils.files.fopen(pause_path, 'wb') as fp_:\n        fp_.write(salt.utils.msgpack.dumps(data))", "output": "Remove a pause from a jid, allowing it to continue. If the state_id is\n    not specified then the a general pause will be resumed.\n\n    The given state_id is the id got a given state execution, so given a state\n    that looks like this:\n\n    .. code-block:: yaml\n\n        vim:\n          pkg.installed: []\n\n    The state_id to pass to `rm_pause` is `vim`\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' state.resume 20171130110407769519\n        salt '*' state.resume 20171130110407769519 vim", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exclude_from_stock_ip_list(exclude_ip_list):\n    \"\"\" \n    \"\"\"\n    for exc in exclude_ip_list:\n        if exc in stock_ip_list:\n            stock_ip_list.remove(exc)\n\n    # \u6269\u5c55\u5e02\u573a\n    for exc in exclude_ip_list:\n        if exc in future_ip_list:\n            future_ip_list.remove(exc)", "output": "\u4ecestock_ip_list\u5220\u9664\u5217\u8868exclude_ip_list\u4e2d\u7684ip\n    \u4ecestock_ip_list\u5220\u9664\u5217\u8868future_ip_list\u4e2d\u7684ip\n\n    :param exclude_ip_list:  \u9700\u8981\u5220\u9664\u7684ip_list\n    :return: None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def qapplication(translate=True, test_time=3):\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    if running_in_mac_app():\r\n        SpyderApplication = MacApplication\r\n    else:\r\n        SpyderApplication = QApplication\r\n    \r\n    app = SpyderApplication.instance()\r\n    if app is None:\r\n        # Set Application name for Gnome 3\r\n        # https://groups.google.com/forum/#!topic/pyside/24qxvwfrRDs\r\n        app = SpyderApplication(['Spyder'])\r\n\r\n        # Set application name for KDE (See issue 2207)\r\n        app.setApplicationName('Spyder')\r\n    if translate:\r\n        install_translator(app)\r\n\r\n    test_ci = os.environ.get('TEST_CI_WIDGETS', None)\r\n    if test_ci is not None:\r\n        timer_shutdown = QTimer(app)\r\n        timer_shutdown.timeout.connect(app.quit)\r\n        timer_shutdown.start(test_time*1000)\r\n    return app", "output": "Return QApplication instance\r\n    Creates it if it doesn't already exist\r\n    \r\n    test_time: Time to maintain open the application when testing. It's given\r\n    in seconds", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def commit(self):\n        \"\"\"\n        \"\"\"\n        # Set the status to \"starting\" synchronously, to ensure that\n        # this batch will necessarily not accept new messages.\n        with self._state_lock:\n            if self._status == base.BatchStatus.ACCEPTING_MESSAGES:\n                self._status = base.BatchStatus.STARTING\n            else:\n                return\n\n        # Start a new thread to actually handle the commit.\n        commit_thread = threading.Thread(\n            name=\"Thread-CommitBatchPublisher\", target=self._commit\n        )\n        commit_thread.start()", "output": "Actually publish all of the messages on the active batch.\n\n        .. note::\n\n            This method is non-blocking. It opens a new thread, which calls\n            :meth:`_commit`, which does block.\n\n        This synchronously sets the batch status to \"starting\", and then opens\n        a new thread, which handles actually sending the messages to Pub/Sub.\n\n        If the current batch is **not** accepting messages, this method\n        does nothing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(name, remove=False, force=False):\n    '''\n    \n    '''\n    if salt.utils.data.is_true(force):\n        log.warning(\n            'userdel does not support force-deleting user while user is '\n            'logged in'\n        )\n    cmd = ['userdel']\n    if remove:\n        cmd.append('-r')\n    cmd.append(name)\n    return __salt__['cmd.retcode'](cmd, python_shell=False) == 0", "output": "Remove a user from the minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.delete name remove=True force=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dataset(self, dataset_id, project=None):\n        \"\"\"\n        \"\"\"\n        if project is None:\n            project = self.project\n\n        return DatasetReference(project, dataset_id)", "output": "Construct a reference to a dataset.\n\n        :type dataset_id: str\n        :param dataset_id: ID of the dataset.\n\n        :type project: str\n        :param project: (Optional) project ID for the dataset (defaults to\n                        the project of the client).\n\n        :rtype: :class:`google.cloud.bigquery.dataset.DatasetReference`\n        :returns: a new ``DatasetReference`` instance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_state_op_colocation_error(graph, reported_tags=None):\n  \"\"\"\"\"\"\n  state_op_types = list_registered_stateful_ops_without_inputs()\n  state_op_map = {op.name: op for op in graph.get_operations()\n                  if op.type in state_op_types}\n  for op in state_op_map.values():\n    for colocation_group in op.colocation_groups():\n      if not (colocation_group.startswith(tf.compat.as_bytes(\"loc:@\")) and\n              tf.compat.as_str_any(colocation_group[5:]) in state_op_map):\n        tags_prefix = (\"\" if reported_tags is None else\n                       \"in the graph for tags %s, \" % reported_tags)\n        return (\n            \"A state-holding node x of a module's graph (e.g., a Variable op) \"\n            \"must not be subject to a tf.colocate_with(y) constraint \"\n            \"unless y is also a state-holding node.\\n\"\n            \"Details: %snode '%s' has op '%s', which counts as state-holding, \"\n            \"but Operation.colocation_groups() == %s. \" %\n            (tags_prefix, op.name, op.type, op.colocation_groups()))\n  return None", "output": "Returns error message for colocation of state ops, or None if ok.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close(self):\n        \"\"\"\n        \n        \"\"\"\n        self._lock.acquire()\n        try:\n            self._closed = True\n            self._cv.notifyAll()\n            if self._event is not None:\n                self._event.set()\n        finally:\n            self._lock.release()", "output": "Close this pipe object.  Future calls to `read` after the buffer\n        has been emptied will return immediately with an empty string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_publish(self, package, _):\n        '''\n        \n        '''\n        try:\n            self.publisher.publish(package)\n            return package\n        # Add an extra fallback in case a forked process leeks through\n        except Exception:\n            log.critical('Unexpected error while polling master events',\n                         exc_info=True)\n            return None", "output": "Get something from epull, publish it out epub, and return the package (or None)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def newCDataBlock(self, content, len):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlNewCDataBlock(self._o, content, len)\n        if ret is None:raise treeError('xmlNewCDataBlock() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Creation of a new node containing a CDATA block.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe_api_stages(restApiId, deploymentId, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        stages = conn.get_stages(restApiId=restApiId, deploymentId=deploymentId)\n        return {'stages': [_convert_datetime_str(stage) for stage in stages['item']]}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "Get all API stages for a given apiID and deploymentID\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_apigateway.describe_api_stages restApiId deploymentId", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_state(state='stop', profile_process='worker'):\n    \"\"\"\n    \"\"\"\n    state2int = {'stop': 0, 'run': 1}\n    profile_process2int = {'worker': 0, 'server': 1}\n    check_call(_LIB.MXSetProcessProfilerState(ctypes.c_int(state2int[state]),\n                                              profile_process2int[profile_process],\n                                              profiler_kvstore_handle))", "output": "Set up the profiler state to 'run' or 'stop'.\n\n    Parameters\n    ----------\n    state : string, optional\n        Indicates whether to run the profiler, can\n        be 'stop' or 'run'. Default is `stop`.\n    profile_process : string\n        whether to profile kvstore `server` or `worker`.\n        server can only be profiled when kvstore is of type dist.\n        if this is not passed, defaults to `worker`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_supported():\n    '''\n    \n    '''\n    # Check for supported platforms\n    # NOTE: ZFS on Windows is in development\n    # NOTE: ZFS on NetBSD is in development\n    on_supported_platform = False\n    if salt.utils.platform.is_sunos():\n        on_supported_platform = True\n    elif salt.utils.platform.is_freebsd() and _check_retcode('kldstat -q -m zfs'):\n        on_supported_platform = True\n    elif salt.utils.platform.is_linux() and os.path.exists('/sys/module/zfs'):\n        on_supported_platform = True\n    elif salt.utils.platform.is_linux() and salt.utils.path.which('zfs-fuse'):\n        on_supported_platform = True\n    elif salt.utils.platform.is_darwin() and \\\n         os.path.exists('/Library/Extensions/zfs.kext') and \\\n         os.path.exists('/dev/zfs'):\n        on_supported_platform = True\n\n    # Additional check for the zpool command\n    return (salt.utils.path.which('zpool') and on_supported_platform) is True", "output": "Check the system for ZFS support", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_docker_cache(tag, docker_registry) -> None:\n    \"\"\"\"\"\"\n    if docker_registry:\n        # noinspection PyBroadException\n        try:\n            import docker_cache\n            logging.info('Docker cache download is enabled from registry %s', docker_registry)\n            docker_cache.load_docker_cache(registry=docker_registry, docker_tag=tag)\n        except Exception:\n            logging.exception('Unable to retrieve Docker cache. Continue without...')\n    else:\n        logging.info('Distributed docker cache disabled')", "output": "Imports tagged container from the given docker registry", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def db_remove(name, user=None, password=None, host=None, port=None, authdb=None):\n    '''\n    \n    '''\n    conn = _connect(user, password, host, port, authdb=authdb)\n    if not conn:\n        return 'Failed to connect to mongo database'\n\n    try:\n        log.info('Removing database %s', name)\n        conn.drop_database(name)\n    except pymongo.errors.PyMongoError as err:\n        log.error('Removing database %s failed with error: %s', name, err)\n        return six.text_type(err)\n\n    return True", "output": "Remove a MongoDB database\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mongodb.db_remove <name> <user> <password> <host> <port>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def function_application(func):\n    \"\"\"\n    \n    \"\"\"\n    if func not in NUMEXPR_MATH_FUNCS:\n        raise ValueError(\"Unsupported mathematical function '%s'\" % func)\n\n    @with_doc(func)\n    @with_name(func)\n    def mathfunc(self):\n        if isinstance(self, NumericalExpression):\n            return NumExprFactor(\n                \"{func}({expr})\".format(func=func, expr=self._expr),\n                self.inputs,\n                dtype=float64_dtype,\n            )\n        else:\n            return NumExprFactor(\n                \"{func}(x_0)\".format(func=func),\n                (self,),\n                dtype=float64_dtype,\n            )\n    return mathfunc", "output": "Factory function for producing function application methods for Factor\n    subclasses.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install_pyenv(name, user=None):\n    '''\n    \n    '''\n    ret = {'name': name, 'result': None, 'comment': '', 'changes': {}}\n\n    if __opts__['test']:\n        ret['comment'] = 'pyenv is set to be installed'\n        return ret\n\n    return _check_and_install_python(ret, user)", "output": "Install pyenv if not installed. Allows you to require pyenv be installed\n    prior to installing the plugins. Useful if you want to install pyenv\n    plugins via the git or file modules and need them installed before\n    installing any rubies.\n\n    Use the pyenv.root configuration option to set the path for pyenv if you\n    want a system wide install that is not in a user home dir.\n\n    user: None\n        The user to run pyenv as.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def total_bytes_processed(self):\n        \"\"\"\n        \"\"\"\n        result = self._job_statistics().get(\"totalBytesProcessed\")\n        if result is not None:\n            result = int(result)\n        return result", "output": "Return total bytes processed from job statistics, if present.\n\n        See:\n        https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#statistics.query.totalBytesProcessed\n\n        :rtype: int or None\n        :returns: total bytes processed by the job, or None if job is not\n                  yet complete.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def query(database, query, **client_args):\n    '''\n    \n    '''\n    client = _client(**client_args)\n    _result = client.query(query, database=database)\n\n    if isinstance(_result, collections.Sequence):\n        return [_pull_query_results(_query_result) for _query_result in _result if _query_result]\n    return [_pull_query_results(_result) if _result else {}]", "output": "Execute a query.\n\n    database\n        Name of the database to query on.\n\n    query\n        InfluxQL query string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def inherit(prop, name, **kwargs):\n    '''\n    \n\n    '''\n    ## Configure command\n    # NOTE: initialize the defaults\n    flags = []\n\n    # NOTE: set extra config from kwargs\n    if kwargs.get('recursive', False):\n        flags.append('-r')\n    if kwargs.get('revert', False):\n        flags.append('-S')\n\n    ## Inherit property\n    res = __salt__['cmd.run_all'](\n        __utils__['zfs.zfs_command'](\n            command='inherit',\n            flags=flags,\n            property_name=prop,\n            target=name,\n        ),\n        python_shell=False,\n    )\n\n    return __utils__['zfs.parse_command_result'](res, 'inherited')", "output": "Clears the specified property\n\n    prop : string\n        name of property\n    name : string\n        name of the filesystem, volume, or snapshot\n    recursive : boolean\n        recursively inherit the given property for all children.\n    revert : boolean\n        revert the property to the received value if one exists; otherwise\n        operate as if the -S option was not specified.\n\n    .. versionadded:: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zfs.inherit canmount myzpool/mydataset [recursive=True|False]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_value(self, series, key):\n        \"\"\"  \"\"\"\n        if not is_scalar(key):\n            raise InvalidIndexError\n\n        k = com.values_from_object(key)\n        loc = self.get_loc(k)\n        new_values = com.values_from_object(series)[loc]\n\n        return new_values", "output": "we always want to get an index value, never a value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def conv2d(x_input, w_matrix):\n    \"\"\"\"\"\"\n    return tf.nn.conv2d(x_input, w_matrix, strides=[1, 1, 1, 1], padding='SAME')", "output": "conv2d returns a 2d convolution layer with full stride.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def description(self):\n        \"\"\"\"\"\"\n        try:\n            return self.__cog_cleaned_doc__\n        except AttributeError:\n            self.__cog_cleaned_doc__ = cleaned = inspect.getdoc(self)\n            return cleaned", "output": ":class:`str`: Returns the cog's description, typically the cleaned docstring.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _tag(val, tag):\n        \"\"\"\"\"\"\n        if isinstance(val, str):\n            val = bytes(val, 'utf-8')\n        return (bytes('<' + tag + '>', 'utf-8') + val +\n                bytes('</' + tag + '>', 'utf-8'))", "output": "Surround val with <tag></tag>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _mutation(candidate, rate=0.1):\n        \"\"\"\n        \"\"\"\n        sample_index = np.random.choice(len(candidate))\n        sample = candidate[sample_index]\n        idx_list = []\n        for i in range(int(max(len(sample) * rate, 1))):\n            idx = np.random.choice(len(sample))\n            idx_list.append(idx)\n\n            field = sample[idx]  # one-hot encoding\n            field[np.argmax(field)] = 0\n            bit = np.random.choice(field.shape[0])\n            field[bit] = 1\n\n        logger.info(LOGGING_PREFIX + \"Perform mutation on %sth at index=%s\",\n                    sample_index, str(idx_list))\n        return sample", "output": "Perform mutation action to candidates.\n\n        For example, randomly change 10% of original sample\n\n        Args:\n            candidate: List of candidate genes (encodings).\n            rate: Percentage of mutation bits\n\n        Examples:\n            >>> # Genes that represent 3 parameters\n            >>> gene1 = np.array([[0, 0, 1], [0, 1], [1, 0]])\n            >>> new_gene = _mutation([gene1])\n            >>> # new_gene could be the gene1 with the 3rd parameter changed\n            >>> #   new_gene[0] = gene1[0]\n            >>> #   new_gene[1] = gene1[1]\n            >>> #   new_gene[2] = [0, 1] != gene1[2]\n\n        Returns:\n            New gene (encoding)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def renamed(self, source, dest):\r\n        \"\"\"\"\"\"\r\n        filename = osp.abspath(to_text_string(source))\r\n        index = self.editorstacks[0].has_filename(filename)\r\n        if index is not None:\r\n            for editorstack in self.editorstacks:\r\n                editorstack.rename_in_data(filename,\r\n                                           new_filename=to_text_string(dest))", "output": "File was renamed in file explorer widget or in project explorer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_number(col, d):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.format_number(_to_java_column(col), d))", "output": "Formats the number X to a format like '#,--#,--#.--', rounded to d decimal places\n    with HALF_EVEN round mode, and returns the result as a string.\n\n    :param col: the column name of the numeric value to be formatted\n    :param d: the N decimal places\n\n    >>> spark.createDataFrame([(5,)], ['a']).select(format_number('a', 4).alias('v')).collect()\n    [Row(v=u'5.0000')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_config(lines):\n    '''\n    \n    '''\n    if not isinstance(lines, list):\n        lines = [lines]\n    try:\n        enable()\n        configure_terminal()\n        for line in lines:\n            sendline(line)\n\n        configure_terminal_exit()\n        disable()\n    except TerminalException as e:\n        log.error(e)\n        return False\n    return True", "output": "Add one or more config lines to the switch running config\n\n    .. code-block:: bash\n\n        salt '*' onyx.cmd add_config 'snmp-server community TESTSTRINGHERE rw'\n\n    .. note::\n        For more than one config added per command, lines should be a list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_user(name, password=None, runas=None):\n    '''\n    \n    '''\n    clear_pw = False\n\n    if password is None:\n        # Generate a random, temporary password. RabbitMQ requires one.\n        clear_pw = True\n        password = ''.join(random.SystemRandom().choice(\n            string.ascii_uppercase + string.digits) for x in range(15))\n\n    if runas is None and not salt.utils.platform.is_windows():\n        runas = salt.utils.user.get_user()\n\n    if salt.utils.platform.is_windows():\n        # On Windows, if the password contains a special character\n        # such as '|', normal execution will fail. For example:\n        # cmd: rabbitmq.add_user abc \"asdf|def\"\n        # stderr: 'def' is not recognized as an internal or external\n        #         command,\\r\\noperable program or batch file.\n        # Work around this by using a shell and a quoted command.\n        python_shell = True\n        cmd = '\"{0}\" add_user \"{1}\" \"{2}\"'.format(\n            RABBITMQCTL, name, password\n        )\n    else:\n        python_shell = False\n        cmd = [RABBITMQCTL, 'add_user', name, password]\n\n    res = __salt__['cmd.run_all'](\n        cmd,\n        reset_system_locale=False,\n        output_loglevel='quiet',\n        runas=runas,\n        python_shell=python_shell)\n\n    if clear_pw:\n        # Now, Clear the random password from the account, if necessary\n        try:\n            clear_password(name, runas)\n        except Exception:\n            # Clearing the password failed. We should try to cleanup\n            # and rerun and error.\n            delete_user(name, runas)\n            raise\n\n    msg = 'Added'\n    return _format_response(res, msg)", "output": "Add a rabbitMQ user via rabbitmqctl user_add <user> <password>\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rabbitmq.add_user rabbit_user password", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_cmp(cls, attrs=None):\n    \"\"\"\n    \n    \"\"\"\n    if attrs is None:\n        attrs = cls.__attrs_attrs__\n\n    cls.__eq__, cls.__ne__, cls.__lt__, cls.__le__, cls.__gt__, cls.__ge__ = _make_cmp(  # noqa\n        attrs\n    )\n\n    return cls", "output": "Add comparison methods to *cls*.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_docstring_for_shortcut(self):\r\n        \"\"\"\"\"\"\r\n        # cursor placed below function definition\r\n        result = self.get_function_definition_from_below_last_line()\r\n        if result is not None:\r\n            __, number_of_lines_of_function = result\r\n            cursor = self.code_editor.textCursor()\r\n            for __ in range(number_of_lines_of_function):\r\n                cursor.movePosition(QTextCursor.PreviousBlock)\r\n\r\n            self.code_editor.setTextCursor(cursor)\r\n\r\n        cursor = self.code_editor.textCursor()\r\n        self.line_number_cursor = cursor.blockNumber() + 1\r\n\r\n        self.write_docstring_at_first_line_of_function()", "output": "Write docstring to editor by shortcut of code editor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_namespace(name, **kwargs):\n    '''\n    \n    '''\n    cfg = _setup_conn(**kwargs)\n    try:\n        api_instance = kubernetes.client.CoreV1Api()\n        api_response = api_instance.read_namespace(name)\n\n        return api_response.to_dict()\n    except (ApiException, HTTPError) as exc:\n        if isinstance(exc, ApiException) and exc.status == 404:\n            return None\n        else:\n            log.exception(\n                'Exception when calling '\n                'CoreV1Api->read_namespace'\n            )\n            raise CommandExecutionError(exc)\n    finally:\n        _cleanup(**cfg)", "output": "Return information for a given namespace defined by the specified name\n\n    CLI Examples::\n\n        salt '*' kubernetes.show_namespace kube-system", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def assign_funcs(modname, service, module=None,\n                get_conn_funcname='_get_conn', cache_id_funcname='_cache_id',\n                exactly_one_funcname='_exactly_one'):\n    '''\n    \n    '''\n    mod = sys.modules[modname]\n    setattr(mod, get_conn_funcname, get_connection_func(service, module=module))\n    setattr(mod, cache_id_funcname, cache_id_func(service))\n\n    # TODO: Remove this and import salt.utils.data.exactly_one into boto_* modules instead\n    # Leaving this way for now so boto modules can be back ported\n    if exactly_one_funcname is not None:\n        setattr(mod, exactly_one_funcname, exactly_one)", "output": "Assign _get_conn and _cache_id functions to the named module.\n\n    .. code-block:: python\n\n        _utils__['boto.assign_partials'](__name__, 'ec2')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _try_resolve_sam_resource_refs(self, input, supported_resource_refs):\n        \"\"\"\n        \n        \"\"\"\n        if not self._is_intrinsic_dict(input):\n            return input\n\n        function_type = list(input.keys())[0]\n        return self.supported_intrinsics[function_type].resolve_resource_refs(input, supported_resource_refs)", "output": "Try to resolve SAM resource references on the given template. If the given object looks like one of the\n        supported intrinsics, it calls the appropriate resolution on it. If not, this method returns the original input\n        unmodified.\n\n        :param dict input: Dictionary that may represent an intrinsic function\n        :param SupportedResourceReferences supported_resource_refs: Object containing information about available\n            resource references and the values they resolve to.\n        :return: Modified input dictionary with references resolved", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _database_string(self):\n        \"\"\"\n        \"\"\"\n        if self._database_string_internal is None:\n            # NOTE: database_root_path() is a classmethod, so we don't use\n            #       self._firestore_api (it isn't necessary).\n            db_str = firestore_client.FirestoreClient.database_root_path(\n                self.project, self._database\n            )\n            self._database_string_internal = db_str\n\n        return self._database_string_internal", "output": "The database string corresponding to this client's project.\n\n        This value is lazy-loaded and cached.\n\n        Will be of the form\n\n            ``projects/{project_id}/databases/{database_id}``\n\n        but ``database_id == '(default)'`` for the time being.\n\n        Returns:\n            str: The fully-qualified database string for the current\n            project. (The default database is also in this string.)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def full(shape, val, ctx=None, dtype=mx_real_t, out=None):\n    \"\"\"\n    \"\"\"\n    out = empty(shape, ctx, dtype) if out is None else out\n    out[:] = val\n    return out", "output": "Returns a new array of given shape and type, filled with the given value `val`.\n\n    Parameters\n    --------\n    shape : int or tuple of int\n        The shape of the new array.\n    val : scalar\n        Fill value.\n    ctx : Context, optional\n        Device context (default is the current default context).\n    dtype : `str` or `numpy.dtype`, optional\n        The data type of the returned `NDArray`. The default datatype is `float32`.\n    out : NDArray, optional\n        The output NDArray (default is `None`).\n\n    Returns\n    -------\n    NDArray\n        `NDArray` filled with `val`, with the given shape, ctx, and dtype.\n\n    Examples\n    --------\n    >>> mx.nd.full(1, 2.0).asnumpy()\n    array([ 2.], dtype=float32)\n    >>> mx.nd.full((1, 2), 2.0, mx.gpu(0))\n    <NDArray 1x2 @gpu(0)>\n    >>> mx.nd.full((1, 2), 2.0, dtype='float16').asnumpy()\n    array([[ 2.,  2.]], dtype=float16)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __get_job_status(self):\n        \"\"\"\"\"\"\n        # Figure out status and return it\n        job = self.__get_job()\n\n        if \"succeeded\" in job.obj[\"status\"] and job.obj[\"status\"][\"succeeded\"] > 0:\n            job.scale(replicas=0)\n            if self.print_pod_logs_on_exit:\n                self.__print_pod_logs()\n            if self.delete_on_success:\n                self.__delete_job_cascade(job)\n            return \"SUCCEEDED\"\n\n        if \"failed\" in job.obj[\"status\"]:\n            failed_cnt = job.obj[\"status\"][\"failed\"]\n            self.__logger.debug(\"Kubernetes job \" + self.uu_name\n                                + \" status.failed: \" + str(failed_cnt))\n            if self.print_pod_logs_on_exit:\n                self.__print_pod_logs()\n            if failed_cnt > self.max_retrials:\n                job.scale(replicas=0)  # avoid more retrials\n                return \"FAILED\"\n        return \"RUNNING\"", "output": "Return the Kubernetes job status", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cygcheck(args, cyg_arch='x86_64'):\n    '''\n    \n    '''\n    cmd = ' '.join([\n        os.sep.join(['c:', _get_cyg_dir(cyg_arch), 'bin', 'cygcheck']),\n        '-c', args])\n\n    ret = __salt__['cmd.run_all'](cmd)\n\n    if ret['retcode'] == 0:\n        return ret['stdout']\n    else:\n        return False", "output": "Run the cygcheck executable.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_rule(self, name, callable_):\n        \"\"\"\"\"\"\n        assert isinstance(name, basestring)\n        assert callable(callable_)\n        self.project_rules_.add_rule(name, callable_)", "output": "Makes rule 'name' available to all subsequently loaded Jamfiles.\n\n        Calling that rule wil relay to 'callable'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def market_buy(self, security, amount, ttype=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        self._switch_left_menus([\"\u5e02\u4ef7\u59d4\u6258\", \"\u4e70\u5165\"])\n\n        return self.market_trade(security, amount, ttype)", "output": "\u5e02\u4ef7\u4e70\u5165\n        :param security: \u516d\u4f4d\u8bc1\u5238\u4ee3\u7801\n        :param amount: \u4ea4\u6613\u6570\u91cf\n        :param ttype: \u5e02\u4ef7\u59d4\u6258\u7c7b\u578b\uff0c\u9ed8\u8ba4\u5ba2\u6237\u7aef\u9ed8\u8ba4\u9009\u62e9\uff0c\n                     \u6df1\u5e02\u53ef\u9009 ['\u5bf9\u624b\u65b9\u6700\u4f18\u4ef7\u683c', '\u672c\u65b9\u6700\u4f18\u4ef7\u683c', '\u5373\u65f6\u6210\u4ea4\u5269\u4f59\u64a4\u9500', '\u6700\u4f18\u4e94\u6863\u5373\u65f6\u6210\u4ea4\u5269\u4f59 '\u5168\u989d\u6210\u4ea4\u6216\u64a4\u9500']\n                     \u6caa\u5e02\u53ef\u9009 ['\u6700\u4f18\u4e94\u6863\u6210\u4ea4\u5269\u4f59\u64a4\u9500', '\u6700\u4f18\u4e94\u6863\u6210\u4ea4\u5269\u4f59\u8f6c\u9650\u4ef7']\n\n        :return: {'entrust_no': '\u59d4\u6258\u5355\u53f7'}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_df(cls, path, df:DataFrame, dep_var:str, valid_idx:Collection[int], procs:OptTabTfms=None,\n                cat_names:OptStrList=None, cont_names:OptStrList=None, classes:Collection=None, \n                test_df=None, bs:int=64, val_bs:int=None, num_workers:int=defaults.cpus, dl_tfms:Optional[Collection[Callable]]=None, \n                device:torch.device=None, collate_fn:Callable=data_collate, no_check:bool=False)->DataBunch:\n        \"\"\n        cat_names = ifnone(cat_names, []).copy()\n        cont_names = ifnone(cont_names, list(set(df)-set(cat_names)-{dep_var}))\n        procs = listify(procs)\n        src = (TabularList.from_df(df, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs)\n                           .split_by_idx(valid_idx))\n        src = src.label_from_df(cols=dep_var) if classes is None else src.label_from_df(cols=dep_var, classes=classes)\n        if test_df is not None: src.add_test(TabularList.from_df(test_df, cat_names=cat_names, cont_names=cont_names,\n                                                                 processor = src.train.x.processor))\n        return src.databunch(path=path, bs=bs, val_bs=val_bs, num_workers=num_workers, device=device, \n                             collate_fn=collate_fn, no_check=no_check)", "output": "Create a `DataBunch` from `df` and `valid_idx` with `dep_var`. `kwargs` are passed to `DataBunch.create`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def key_pair_paths(key_name):\n    \"\"\"\"\"\"\n    public_key_path = os.path.expanduser(\"~/.ssh/{}.pub\".format(key_name))\n    private_key_path = os.path.expanduser(\"~/.ssh/{}.pem\".format(key_name))\n    return public_key_path, private_key_path", "output": "Returns public and private key paths for a given key_name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_in_pythonpath(self, dirname):\r\n        \"\"\"\"\"\"\r\n        return fixpath(dirname) in [fixpath(_p) for _p in self.pythonpath]", "output": "Return True if dirname is in project's PYTHONPATH", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_and_validate_dir(data_dir):\n    '''\n    '''\n    if data_dir != \"\":\n        if not os.path.exists(data_dir):\n            try:\n                logging.info('create directory %s', data_dir)\n                os.makedirs(data_dir)\n            except OSError as exc:\n                if exc.errno != errno.EEXIST:\n                    raise OSError('failed to create ' + data_dir)", "output": "Creates/Validates dir", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def project_list(profile=None, **connection_args):\n    '''\n    \n    '''\n    auth(profile, **connection_args)\n\n    if _OS_IDENTITY_API_VERSION > 2:\n        return tenant_list(profile, **connection_args)\n    else:\n        return False", "output": "Return a list of available projects (keystone projects-list).\n    Overrides keystone tenants-list form api V2.\n    For keystone api V3 only.\n\n    .. versionadded:: 2016.11.0\n\n    profile\n        Configuration profile - if configuration for multiple openstack accounts required.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystone.project_list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def safe_expandvars(value):\n    \"\"\"\n    \"\"\"\n    if isinstance(value, six.string_types):\n        return os.path.expandvars(value)\n    return value", "output": "Call os.path.expandvars if value is a string, otherwise do nothing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download(name):\n    '''\n    \n    '''\n    if not update_available(name):\n        raise SaltInvocationError('Update not available: {0}'.format(name))\n\n    if name in list_downloads():\n        return True\n\n    cmd = ['softwareupdate', '--download', name]\n    salt.utils.mac_utils.execute_return_success(cmd)\n\n    return name in list_downloads()", "output": "Download a named update so that it can be installed later with the\n    ``update`` or ``update_all`` functions\n\n    :param str name: The update to download.\n\n    :return: True if successful, otherwise False\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n       salt '*' softwareupdate.download <update name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def contains(bank, key):\n    '''\n    \n    '''\n    if key is None:\n        return True  # any key could be a branch and a leaf at the same time in Consul\n    else:\n        try:\n            c_key = '{0}/{1}'.format(bank, key)\n            _, value = api.kv.get(c_key)\n        except Exception as exc:\n            raise SaltCacheError(\n                'There was an error getting the key, {0}: {1}'.format(\n                    c_key, exc\n                )\n            )\n        return value is not None", "output": "Checks if the specified bank contains the specified key.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_and_close_enable(self, top_left, bottom_right):\r\n        \"\"\"\"\"\"\r\n        self.btn_save_and_close.setEnabled(True)\r\n        self.btn_save_and_close.setAutoDefault(True)\r\n        self.btn_save_and_close.setDefault(True)", "output": "Handle the data change event to enable the save and close button.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def eventFilter(self, widget, event):\n        \"\"\"\"\"\"\n\n        # ---- Zooming\n        if event.type() == QEvent.Wheel:\n            modifiers = QApplication.keyboardModifiers()\n            if modifiers == Qt.ControlModifier:\n                if event.angleDelta().y() > 0:\n                    self.zoom_in()\n                else:\n                    self.zoom_out()\n                return True\n            else:\n                return False\n\n        # ---- Panning\n        # Set ClosedHandCursor:\n        elif event.type() == QEvent.MouseButtonPress:\n            if event.button() == Qt.LeftButton:\n                QApplication.setOverrideCursor(Qt.ClosedHandCursor)\n                self._ispanning = True\n                self.xclick = event.globalX()\n                self.yclick = event.globalY()\n\n        # Reset Cursor:\n        elif event.type() == QEvent.MouseButtonRelease:\n            QApplication.restoreOverrideCursor()\n            self._ispanning = False\n\n        # Move  ScrollBar:\n        elif event.type() == QEvent.MouseMove:\n            if self._ispanning:\n                dx = self.xclick - event.globalX()\n                self.xclick = event.globalX()\n\n                dy = self.yclick - event.globalY()\n                self.yclick = event.globalY()\n\n                scrollBarH = self.horizontalScrollBar()\n                scrollBarH.setValue(scrollBarH.value() + dx)\n\n                scrollBarV = self.verticalScrollBar()\n                scrollBarV.setValue(scrollBarV.value() + dy)\n\n        return QWidget.eventFilter(self, widget, event)", "output": "A filter to control the zooming and panning of the figure canvas.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def recursive_copy(source, dest):\n    '''\n    \n    '''\n    for root, _, files in salt.utils.path.os_walk(source):\n        path_from_source = root.replace(source, '').lstrip(os.sep)\n        target_directory = os.path.join(dest, path_from_source)\n        if not os.path.exists(target_directory):\n            os.makedirs(target_directory)\n        for name in files:\n            file_path_from_source = os.path.join(source, path_from_source, name)\n            target_path = os.path.join(target_directory, name)\n            shutil.copyfile(file_path_from_source, target_path)", "output": "Recursively copy the source directory to the destination,\n    leaving files with the source does not explicitly overwrite.\n\n    (identical to cp -r on a unix machine)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(name):\n    '''\n    \n    '''\n    contextkey = 'docker.exists.{0}'.format(name)\n    if contextkey in __context__:\n        return __context__[contextkey]\n    try:\n        c_info = _client_wrapper('inspect_container',\n                                 name,\n                                 catch_api_errors=False)\n    except docker.errors.APIError:\n        __context__[contextkey] = False\n    else:\n        __context__[contextkey] = True\n    return __context__[contextkey]", "output": "Check if a given container exists\n\n    name\n        Container name or ID\n\n\n    **RETURN DATA**\n\n    A boolean (``True`` if the container exists, otherwise ``False``)\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion docker.exists mycontainer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_params(self,\n                   theta=1.,\n                   gamma=1.,\n                   clip_min=0.,\n                   clip_max=1.,\n                   y_target=None,\n                   symbolic_impl=True,\n                   **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    self.theta = theta\n    self.gamma = gamma\n    self.clip_min = clip_min\n    self.clip_max = clip_max\n    self.y_target = y_target\n    self.symbolic_impl = symbolic_impl\n\n    if len(kwargs.keys()) > 0:\n      warnings.warn(\"kwargs is unused and will be removed on or after \"\n                    \"2019-04-26.\")\n\n    return True", "output": "Take in a dictionary of parameters and applies attack-specific checks\n    before saving them as attributes.\n\n    Attack-specific parameters:\n\n    :param theta: (optional float) Perturbation introduced to modified\n                  components (can be positive or negative)\n    :param gamma: (optional float) Maximum percentage of perturbed features\n    :param clip_min: (optional float) Minimum component value for clipping\n    :param clip_max: (optional float) Maximum component value for clipping\n    :param y_target: (optional) Target tensor if the attack is targeted", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def system_properties_present(server=None, **kwargs):\n    '''\n    \n    '''\n    ret = {'name': '', 'result': None, 'comment': None, 'changes': {}}\n\n    del kwargs['name']\n    try:\n        data = __salt__['glassfish.get_system_properties'](server=server)\n    except requests.ConnectionError as error:\n        if __opts__['test']:\n            ret['changes'] = kwargs\n            ret['result'] = None\n            return ret\n        else:\n            ret['error'] = \"Can't connect to the server\"\n            return ret\n\n    ret['changes'] = {'data': data, 'kwargs': kwargs}\n    if not data == kwargs:\n        data.update(kwargs)\n        if not __opts__['test']:\n            try:\n                __salt__['glassfish.update_system_properties'](data, server=server)\n                ret['changes'] = kwargs\n                ret['result'] = True\n                ret['comment'] = 'System properties updated'\n            except CommandExecutionError as error:\n                ret['comment'] = error\n                ret['result'] = False\n        else:\n            ret['result'] = None\n            ret['changes'] = kwargs\n            ret['coment'] = 'System properties would have been updated'\n    else:\n        ret['changes'] = {}\n        ret['result'] = True\n        ret['comment'] = 'System properties are already up-to-date'\n    return ret", "output": "Ensures that the system properties are present\n\n    properties\n        The system properties", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _process_arguments(arguments):\n    ''' \n\n    '''\n    if arguments is None: return \"\"\n    result = \"\"\n    for key, value in arguments.items():\n        if not key.startswith(\"bokeh-\"):\n            result += \"&{}={}\".format(quote_plus(str(key)), quote_plus(str(value)))\n    return result", "output": "Return user-supplied HTML arguments to add to a Bokeh server URL.\n\n    Args:\n        arguments (dict[str, object]) :\n            Key/value pairs to add to the URL\n\n    Returns:\n        str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def combined_loss_given_predictions(log_probab_actions_new,\n                                    log_probab_actions_old,\n                                    value_prediction,\n                                    padded_actions,\n                                    padded_rewards,\n                                    reward_mask,\n                                    gamma=0.99,\n                                    lambda_=0.95,\n                                    epsilon=0.2,\n                                    c1=1.0,\n                                    c2=0.01):\n  \"\"\"\"\"\"\n  loss_value = value_loss_given_predictions(\n      value_prediction, padded_rewards, reward_mask, gamma=gamma)\n  loss_ppo = ppo_loss_given_predictions(log_probab_actions_new,\n                                        log_probab_actions_old,\n                                        value_prediction,\n                                        padded_actions,\n                                        padded_rewards,\n                                        reward_mask,\n                                        gamma=gamma,\n                                        lambda_=lambda_,\n                                        epsilon=epsilon)\n  # TODO(afrozm): Add the entropy bonus, but since we don't do that in T2T\n  # we'll skip if for now.\n  entropy_bonus = 0.0\n  return (loss_ppo + (c1 * loss_value) - (c2 * entropy_bonus), loss_ppo,\n          loss_value, entropy_bonus)", "output": "Computes the combined (clipped loss + value loss) given predictions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def determine_ip_address():\n    \"\"\"\"\"\"\n    addrs = [\n        x.address for k, v in psutil.net_if_addrs().items() if k[0] == \"e\"\n        for x in v if x.family == AddressFamily.AF_INET\n    ]\n    return addrs[0]", "output": "Return the first IP address for an ethernet interface on the system.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ixs(self, i, axis=0):\n        \"\"\"\n        \n        \"\"\"\n        try:\n\n            # dispatch to the values if we need\n            values = self._values\n            if isinstance(values, np.ndarray):\n                return libindex.get_value_at(values, i)\n            else:\n                return values[i]\n        except IndexError:\n            raise\n        except Exception:\n            if isinstance(i, slice):\n                indexer = self.index._convert_slice_indexer(i, kind='iloc')\n                return self._get_values(indexer)\n            else:\n                label = self.index[i]\n                if isinstance(label, Index):\n                    return self.take(i, axis=axis, convert=True)\n                else:\n                    return libindex.get_value_at(self, i)", "output": "Return the i-th value or values in the Series by location.\n\n        Parameters\n        ----------\n        i : int, slice, or sequence of integers\n\n        Returns\n        -------\n        scalar (int) or Series (slice, sequence)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_namespaced_network_policy(self, name, namespace, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_network_policy_with_http_info(name, namespace, **kwargs)\n        else:\n            (data) = self.read_namespaced_network_policy_with_http_info(name, namespace, **kwargs)\n            return data", "output": "read the specified NetworkPolicy\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_network_policy(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the NetworkPolicy (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'. Deprecated. Planned for removal in 1.18.\n        :param bool export: Should this value be exported.  Export strips fields that a user can not specify. Deprecated. Planned for removal in 1.18.\n        :return: V1beta1NetworkPolicy\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tokenMap(func, *args):\n    \"\"\"\n    \"\"\"\n    def pa(s,l,t):\n        return [func(tokn, *args) for tokn in t]\n\n    try:\n        func_name = getattr(func, '__name__',\n                            getattr(func, '__class__').__name__)\n    except Exception:\n        func_name = str(func)\n    pa.__name__ = func_name\n\n    return pa", "output": "Helper to define a parse action by mapping a function to all\n    elements of a ParseResults list. If any additional args are passed,\n    they are forwarded to the given function as additional arguments\n    after the token, as in\n    ``hex_integer = Word(hexnums).setParseAction(tokenMap(int, 16))``,\n    which will convert the parsed data to an integer using base 16.\n\n    Example (compare the last to example in :class:`ParserElement.transformString`::\n\n        hex_ints = OneOrMore(Word(hexnums)).setParseAction(tokenMap(int, 16))\n        hex_ints.runTests('''\n            00 11 22 aa FF 0a 0d 1a\n            ''')\n\n        upperword = Word(alphas).setParseAction(tokenMap(str.upper))\n        OneOrMore(upperword).runTests('''\n            my kingdom for a horse\n            ''')\n\n        wd = Word(alphas).setParseAction(tokenMap(str.title))\n        OneOrMore(wd).setParseAction(' '.join).runTests('''\n            now is the winter of our discontent made glorious summer by this sun of york\n            ''')\n\n    prints::\n\n        00 11 22 aa FF 0a 0d 1a\n        [0, 17, 34, 170, 255, 10, 13, 26]\n\n        my kingdom for a horse\n        ['MY', 'KINGDOM', 'FOR', 'A', 'HORSE']\n\n        now is the winter of our discontent made glorious summer by this sun of york\n        ['Now Is The Winter Of Our Discontent Made Glorious Summer By This Sun Of York']", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(self, context):\n    \"\"\"\n    \"\"\"\n    try:\n      # pylint: disable=g-import-not-at-top,unused-import\n      import tensorflow\n    except ImportError:\n      return\n    # pylint: disable=g-import-not-at-top\n    from tensorboard.plugins.hparams.hparams_plugin import HParamsPlugin\n    return HParamsPlugin(context)", "output": "Returns the plugin, if possible.\n\n    Args:\n      context: The TBContext flags.\n\n    Returns:\n      A HParamsPlugin instance or None if it couldn't be loaded.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def drift_color(base_color, factor=110):\n    \"\"\"\n    \n    \"\"\"\n    base_color = QColor(base_color)\n    if base_color.lightness() > 128:\n        return base_color.darker(factor)\n    else:\n        if base_color == QColor('#000000'):\n            return drift_color(QColor('#101010'), factor + 20)\n        else:\n            return base_color.lighter(factor + 10)", "output": "Return color that is lighter or darker than the base color.\n\n    If base_color.lightness is higher than 128, the returned color is darker\n    otherwise is is lighter.\n\n    :param base_color: The base color to drift from\n    ;:param factor: drift factor (%)\n    :return A lighter or darker color.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def detach_lb(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The detach_lb function must be called with -f or --function.'\n        )\n\n    if not kwargs or 'name' not in kwargs:\n        log.error(\n            'A load-balancer name must be specified.'\n        )\n        return False\n    if 'member' not in kwargs:\n        log.error(\n            'A node name name must be specified.'\n        )\n        return False\n\n    conn = get_conn()\n    lb_conn = get_lb_conn(conn)\n    lb = lb_conn.get_balancer(kwargs['name'])\n\n    member_list = lb_conn.balancer_list_members(lb)\n    remove_member = None\n    for member in member_list:\n        if member.id == kwargs['member']:\n            remove_member = member\n            break\n\n    if not remove_member:\n        log.error(\n            'The specified member %s was not a member of LB %s.',\n            kwargs['member'], kwargs['name']\n        )\n        return False\n\n    __utils__['cloud.fire_event'](\n        'event',\n        'detach load_balancer',\n        'salt/cloud/loadbalancer/detaching',\n        args=kwargs,\n        sock_dir=__opts__['sock_dir'],\n        transport=__opts__['transport']\n    )\n\n    result = lb_conn.balancer_detach_member(lb, remove_member)\n\n    __utils__['cloud.fire_event'](\n        'event',\n        'detached load_balancer',\n        'salt/cloud/loadbalancer/detached',\n        args=kwargs,\n        sock_dir=__opts__['sock_dir'],\n        transport=__opts__['transport']\n    )\n    return result", "output": "Remove an existing node/member from an existing load-balancer configuration.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f detach_lb gce name=lb member=myinstance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def export_module_spec_with_checkpoint(module_spec,\n                                       checkpoint_path,\n                                       export_path,\n                                       scope_prefix=\"\"):\n  \"\"\"\"\"\"\n\n  # The main requirement is that it is possible to know how to map from\n  # module variable name to checkpoint variable name.\n  # This is trivial if the original code used variable scopes,\n  # but can be messy if the variables to export are interwined\n  # with variables not export.\n  with tf.Graph().as_default():\n    m = hub.Module(module_spec)\n    assign_map = {\n        scope_prefix + name: value for name, value in m.variable_map.items()\n    }\n    tf.train.init_from_checkpoint(checkpoint_path, assign_map)\n    init_op = tf.initializers.global_variables()\n    with tf.Session() as session:\n      session.run(init_op)\n      m.export(export_path, session)", "output": "Exports given checkpoint as tfhub module with given spec.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_nodes_min(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The list_nodes_min function must be called with -f or --function.'\n        )\n\n    ret = {}\n    nodes = _query('linode', 'list')['DATA']\n\n    for node in nodes:\n        name = node['LABEL']\n        this_node = {\n            'id': six.text_type(node['LINODEID']),\n            'state': _get_status_descr_by_id(int(node['STATUS']))\n        }\n\n        ret[name] = this_node\n\n    return ret", "output": "Return a list of the VMs that are on the provider. Only a list of VM names and\n    their state is returned. This is the minimum amount of information needed to\n    check for existing VMs.\n\n    .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f list_nodes_min my-linode-config\n        salt-cloud --function list_nodes_min my-linode-config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expr_to_tree(ind, pset):\n    \"\"\"\n\n    \"\"\"\n    def prim_to_list(prim, args):\n        if isinstance(prim, deap.gp.Terminal):\n            if prim.name in pset.context:\n                return pset.context[prim.name]\n            else:\n                return prim.value\n\n        return [prim.name] + args\n\n    tree = []\n    stack = []\n    for node in ind:\n        stack.append((node, []))\n        while len(stack[-1][1]) == stack[-1][0].arity:\n            prim, args = stack.pop()\n            tree = prim_to_list(prim, args)\n            if len(stack) == 0:\n                break   # If stack is empty, all nodes should have been seen\n            stack[-1][1].append(tree)\n\n    return tree", "output": "Convert the unstructured DEAP pipeline into a tree data-structure.\n\n    Parameters\n    ----------\n    ind: deap.creator.Individual\n       The pipeline that is being exported\n\n    Returns\n    -------\n    pipeline_tree: list\n       List of operators in the current optimized pipeline\n\n    EXAMPLE:\n        pipeline:\n            \"DecisionTreeClassifier(input_matrix, 28.0)\"\n        pipeline_tree:\n            ['DecisionTreeClassifier', 'input_matrix', 28.0]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _int_size_to_type(size):\n    \"\"\"\n    \n    \"\"\"\n    if size <= 8:\n        return ByteType\n    if size <= 16:\n        return ShortType\n    if size <= 32:\n        return IntegerType\n    if size <= 64:\n        return LongType", "output": "Return the Catalyst datatype from the size of integers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(s):\n        \"\"\"\n        \"\"\"\n        if s.find('(') == -1 and s.find('[') != -1:\n            return DenseVector.parse(s)\n        elif s.find('(') != -1:\n            return SparseVector.parse(s)\n        else:\n            raise ValueError(\n                \"Cannot find tokens '[' or '(' from the input string.\")", "output": "Parse a string representation back into the Vector.\n\n        >>> Vectors.parse('[2,1,2 ]')\n        DenseVector([2.0, 1.0, 2.0])\n        >>> Vectors.parse(' ( 100,  [0],  [2])')\n        SparseVector(100, {0: 2.0})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def beacon(config):\n    '''\n    \n\n    '''\n\n    _config = {}\n    list(map(_config.update, config))\n\n    log.debug('telegram_bot_msg beacon starting')\n    ret = []\n    output = {}\n    output['msgs'] = []\n\n    bot = telegram.Bot(_config['token'])\n    updates = bot.get_updates(limit=100, timeout=0, network_delay=10)\n\n    log.debug('Num updates: %d', len(updates))\n    if not updates:\n        log.debug('Telegram Bot beacon has no new messages')\n        return ret\n\n    latest_update_id = 0\n    for update in updates:\n\n        if update.message:\n            message = update.message\n        else:\n            message = update.edited_message\n\n        if update.update_id > latest_update_id:\n            latest_update_id = update.update_id\n\n        if message.chat.username in _config['accept_from']:\n            output['msgs'].append(message.to_dict())\n\n    # mark in the server that previous messages are processed\n    bot.get_updates(offset=latest_update_id + 1)\n\n    log.debug('Emitting %d messages.', len(output['msgs']))\n    if output['msgs']:\n        ret.append(output)\n    return ret", "output": "Emit a dict with a key \"msgs\" whose value is a list of messages\n    sent to the configured bot by one of the allowed usernames.\n\n    .. code-block:: yaml\n\n        beacons:\n          telegram_bot_msg:\n            - token: \"<bot access token>\"\n            - accept_from:\n              - \"<valid username>\"\n            - interval: 10", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def center_loss(embedding, label, num_classes, alpha=0.1, scope=\"center_loss\"):\n    \n    \"\"\"\n    nrof_features = embedding.get_shape()[1]\n    centers = tf.get_variable('centers', [num_classes, nrof_features], dtype=tf.float32,\n                              initializer=tf.constant_initializer(0), trainable=False)\n    label = tf.reshape(label, [-1])\n    centers_batch = tf.gather(centers, label)\n    diff = (1 - alpha) * (centers_batch - embedding)\n    centers = tf.scatter_sub(centers, label, diff)\n    loss = tf.reduce_mean(tf.square(embedding - centers_batch), name=scope)\n    return loss", "output": "r\"\"\"Center-Loss as described in the paper\n    `A Discriminative Feature Learning Approach for Deep Face Recognition`\n    <http://ydwen.github.io/papers/WenECCV16.pdf> by Wen et al.\n\n    Args:\n        embedding (tf.Tensor): features produced by the network\n        label (tf.Tensor): ground-truth label for each feature\n        num_classes (int): number of different classes\n        alpha (float): learning rate for updating the centers\n\n    Returns:\n        tf.Tensor: center loss", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def version(user=None, password=None, host=None, port=None, database='admin', authdb=None):\n    '''\n    \n    '''\n    conn = _connect(user, password, host, port, authdb=authdb)\n    if not conn:\n        err_msg = \"Failed to connect to MongoDB database {0}:{1}\".format(host, port)\n        log.error(err_msg)\n        return (False, err_msg)\n\n    try:\n        mdb = pymongo.database.Database(conn, database)\n        return _version(mdb)\n    except pymongo.errors.PyMongoError as err:\n        log.error('Listing users failed with error: %s', err)\n        return six.text_type(err)", "output": "Get MongoDB instance version\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mongodb.version <user> <password> <host> <port> <database>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def new_set(set=None, set_type=None, family='ipv4', comment=False, **kwargs):\n    '''\n    \n    '''\n\n    ipset_family = _IPSET_FAMILIES[family]\n    if not set:\n        return 'Error: Set needs to be specified'\n\n    if not set_type:\n        return 'Error: Set Type needs to be specified'\n\n    if set_type not in _IPSET_SET_TYPES:\n        return 'Error: Set Type is invalid'\n\n    # Check for required arguments\n    for item in _CREATE_OPTIONS_REQUIRED[set_type]:\n        if item not in kwargs:\n            return 'Error: {0} is a required argument'.format(item)\n\n    cmd = '{0} create {1} {2}'.format(_ipset_cmd(), set, set_type)\n\n    for item in _CREATE_OPTIONS[set_type]:\n        if item in kwargs:\n            if item in _CREATE_OPTIONS_WITHOUT_VALUE:\n                cmd = '{0} {1} '.format(cmd, item)\n            else:\n                cmd = '{0} {1} {2} '.format(cmd, item, kwargs[item])\n\n    # Family only valid for certain set types\n    if 'family' in _CREATE_OPTIONS[set_type]:\n        cmd = '{0} family {1}'.format(cmd, ipset_family)\n\n    if comment:\n        cmd = '{0} comment'.format(cmd)\n\n    out = __salt__['cmd.run'](cmd, python_shell=False)\n\n    if not out:\n        out = True\n    return out", "output": ".. versionadded:: 2014.7.0\n\n    Create new custom set\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ipset.new_set custom_set list:set\n\n        salt '*' ipset.new_set custom_set list:set comment=True\n\n        IPv6:\n        salt '*' ipset.new_set custom_set list:set family=ipv6", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _resolve_relative_to(path, original_root, new_root):\n    \"\"\"\n    \n    \"\"\"\n\n    if not isinstance(path, six.string_types) \\\n            or path.startswith(\"s3://\") \\\n            or os.path.isabs(path):\n        # Value is definitely NOT a relative path. It is either a S3 URi or Absolute path or not a string at all\n        return None\n\n    # Value is definitely a relative path. Change it relative to the destination directory\n    return os.path.relpath(\n        os.path.normpath(os.path.join(original_root, path)),  # Absolute original path w.r.t ``original_root``\n        new_root)", "output": "If the given ``path`` is a relative path, then assume it is relative to ``original_root``. This method will\n    update the path to be resolve it relative to ``new_root`` and return.\n\n    Examples\n    -------\n        # Assume a file called template.txt at location /tmp/original/root/template.txt expressed as relative path\n        # We are trying to update it to be relative to /tmp/new/root instead of the /tmp/original/root\n        >>> result = _resolve_relative_to(\"template.txt\",  \\\n                                          \"/tmp/original/root\", \\\n                                          \"/tmp/new/root\")\n        >>> result\n        ../../original/root/template.txt\n\n    Returns\n    -------\n    Updated path if the given path is a relative path. None, if the path is not a relative path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def members(name, members_list, root=None):\n    '''\n    \n    '''\n    cmd = 'chgrpmem -m = {0} {1}'.format(members_list, name)\n    retcode = __salt__['cmd.retcode'](cmd, python_shell=False)\n\n    return not retcode", "output": "Replaces members of the group with a provided list.\n\n    CLI Example:\n\n        salt '*' group.members foo 'user1,user2,user3,...'\n\n    Replaces a membership list for a local group 'foo'.\n        foo:x:1234:user1,user2,user3,...", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_vm_status(vmid=None, name=None):\n    '''\n    \n    '''\n    if vmid is not None:\n        log.debug('get_vm_status: VMID %s', vmid)\n        vmobj = _get_vm_by_id(vmid)\n    elif name is not None:\n        log.debug('get_vm_status: name %s', name)\n        vmobj = _get_vm_by_name(name)\n    else:\n        log.debug(\"get_vm_status: No ID or NAME given\")\n        raise SaltCloudExecutionFailure\n\n    log.debug('VM found: %s', vmobj)\n\n    if vmobj is not None and 'node' in vmobj:\n        log.debug(\"VM_STATUS: Has desired info. Retrieving.. (%s)\",\n                  vmobj['name'])\n        data = query('get', 'nodes/{0}/{1}/{2}/status/current'.format(\n            vmobj['node'], vmobj['type'], vmobj['vmid']))\n        return data\n\n    log.error('VM or requested status not found..')\n    return False", "output": "Get the status for a VM, either via the ID or the hostname", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def store(bank, key, data):\n    '''\n    \n    '''\n    c_key = '{0}/{1}'.format(bank, key)\n    try:\n        c_data = __context__['serial'].dumps(data)\n        api.kv.put(c_key, c_data)\n    except Exception as exc:\n        raise SaltCacheError(\n            'There was an error writing the key, {0}: {1}'.format(\n                c_key, exc\n            )\n        )", "output": "Store a key value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def poisson(lam=1, shape=_Null, dtype=_Null, **kwargs):\n    \"\"\"\n    \"\"\"\n    return _random_helper(_internal._random_poisson, _internal._sample_poisson,\n                          [lam], shape, dtype, kwargs)", "output": "Draw random samples from a Poisson distribution.\n\n    Samples are distributed according to a Poisson distribution parametrized\n    by *lambda* (rate). Samples will always be returned as a floating point data type.\n\n    Parameters\n    ----------\n    lam : float or Symbol, optional\n        Expectation of interval, should be >= 0.\n    shape : int or tuple of ints, optional\n        The number of samples to draw. If shape is, e.g., `(m, n)` and `lam` is\n        a scalar, output shape will be `(m, n)`. If `lam`\n        is an Symbol with shape, e.g., `(x, y)`, then output will have shape\n        `(x, y, m, n)`, where `m*n` samples are drawn for each entry in `lam`.\n    dtype : {'float16', 'float32', 'float64'}, optional\n        Data type of output samples. Default is 'float32'\n\n    Returns\n    -------\n    Symbol\n        If input `shape` has dimensions, e.g., `(m, n)`, and `lam` is\n        a scalar, output shape will be `(m, n)`. If `lam`\n        is an Symbol with shape, e.g., `(x, y)`, then output will have shape\n        `(x, y, m, n)`, where `m*n` samples are drawn for each entry in `lam`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tz_convert(self, tz, axis=0, level=None, copy=True):\n        \"\"\"\n        \n        \"\"\"\n        axis = self._get_axis_number(axis)\n        ax = self._get_axis(axis)\n\n        def _tz_convert(ax, tz):\n            if not hasattr(ax, 'tz_convert'):\n                if len(ax) > 0:\n                    ax_name = self._get_axis_name(axis)\n                    raise TypeError('%s is not a valid DatetimeIndex or '\n                                    'PeriodIndex' % ax_name)\n                else:\n                    ax = DatetimeIndex([], tz=tz)\n            else:\n                ax = ax.tz_convert(tz)\n            return ax\n\n        # if a level is given it must be a MultiIndex level or\n        # equivalent to the axis name\n        if isinstance(ax, MultiIndex):\n            level = ax._get_level_number(level)\n            new_level = _tz_convert(ax.levels[level], tz)\n            ax = ax.set_levels(new_level, level=level)\n        else:\n            if level not in (None, 0, ax.name):\n                raise ValueError(\"The level {0} is not valid\".format(level))\n            ax = _tz_convert(ax, tz)\n\n        result = self._constructor(self._data, copy=copy)\n        result = result.set_axis(ax, axis=axis, inplace=False)\n        return result.__finalize__(self)", "output": "Convert tz-aware axis to target time zone.\n\n        Parameters\n        ----------\n        tz : string or pytz.timezone object\n        axis : the axis to convert\n        level : int, str, default None\n            If axis ia a MultiIndex, convert a specific level. Otherwise\n            must be None\n        copy : boolean, default True\n            Also make a copy of the underlying data\n\n        Returns\n        -------\n\n        Raises\n        ------\n        TypeError\n            If the axis is tz-naive.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hailstone(n):\n  \"\"\"\n  \"\"\"\n\n  sequence = [n]\n  while n > 1:\n    if n%2 != 0:\n      n = 3*n + 1\n    else: \n      n = int(n/2)\n    sequence.append(n)\n  return sequence", "output": "Return the 'hailstone sequence' from n to 1\n     n: The starting point of the hailstone sequence", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def launch(url, wait=False, locate=False):\n    \"\"\"\n    \"\"\"\n    from ._termui_impl import open_url\n    return open_url(url, wait=wait, locate=locate)", "output": "This function launches the given URL (or filename) in the default\n    viewer application for this file type.  If this is an executable, it\n    might launch the executable in a new session.  The return value is\n    the exit code of the launched application.  Usually, ``0`` indicates\n    success.\n\n    Examples::\n\n        click.launch('https://click.palletsprojects.com/')\n        click.launch('/my/downloaded/file', locate=True)\n\n    .. versionadded:: 2.0\n\n    :param url: URL or filename of the thing to launch.\n    :param wait: waits for the program to stop.\n    :param locate: if this is set to `True` then instead of launching the\n                   application associated with the URL it will attempt to\n                   launch a file manager with the file located.  This\n                   might have weird effects if the URL does not point to\n                   the filesystem.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _reset_bind(self):\n        \"\"\"\"\"\"\n        self.binded = False\n        self._buckets = {}\n        self._curr_module = None\n        self._curr_bucket_key = None", "output": "Internal utility function to reset binding.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(key, profile=None):\n    '''\n    \n    '''\n    if not profile:\n        return False\n    redis_kwargs = profile.copy()\n    redis_kwargs.pop('driver')\n    redis_conn = redis.StrictRedis(**redis_kwargs)\n    return redis_conn.get(key)", "output": "Get a value from the Redis SDB.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def spec_check(self, auth_list, fun, args, form):\n        '''\n        \n        '''\n        if not auth_list:\n            return False\n        if form != 'cloud':\n            comps = fun.split('.')\n            if len(comps) != 2:\n                # Hint at a syntax error when command is passed improperly,\n                # rather than returning an authentication error of some kind.\n                # See Issue #21969 for more information.\n                return {'error': {'name': 'SaltInvocationError',\n                                  'message': 'A command invocation error occurred: Check syntax.'}}\n            mod_name = comps[0]\n            fun_name = comps[1]\n        else:\n            fun_name = mod_name = fun\n        for ind in auth_list:\n            if isinstance(ind, six.string_types):\n                if ind[0] == '@':\n                    if ind[1:] == mod_name or ind[1:] == form or ind == '@{0}s'.format(form):\n                        return True\n            elif isinstance(ind, dict):\n                if len(ind) != 1:\n                    continue\n                valid = next(six.iterkeys(ind))\n                if valid[0] == '@':\n                    if valid[1:] == mod_name:\n                        if self.__fun_check(ind[valid], fun_name, args.get('arg'), args.get('kwarg')):\n                            return True\n                    if valid[1:] == form or valid == '@{0}s'.format(form):\n                        if self.__fun_check(ind[valid], fun, args.get('arg'), args.get('kwarg')):\n                            return True\n        return False", "output": "Check special API permissions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remote(self) -> Optional[str]:\n        \"\"\"\n        \"\"\"\n        if isinstance(self._transport_peername, (list, tuple)):\n            return self._transport_peername[0]\n        else:\n            return self._transport_peername", "output": "Remote IP of client initiated HTTP request.\n\n        The IP is resolved in this order:\n\n        - overridden value by .clone(remote=new_remote) call.\n        - peername of opened socket", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def partition(a, sz): \n    \"\"\"\"\"\"\n    return [a[i:i+sz] for i in range(0, len(a), sz)]", "output": "splits iterables a in equal parts of size sz", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def locale(self) -> tornado.locale.Locale:\n        \"\"\"\n        \"\"\"\n        if not hasattr(self, \"_locale\"):\n            loc = self.get_user_locale()\n            if loc is not None:\n                self._locale = loc\n            else:\n                self._locale = self.get_browser_locale()\n                assert self._locale\n        return self._locale", "output": "The locale for the current session.\n\n        Determined by either `get_user_locale`, which you can override to\n        set the locale based on, e.g., a user preference stored in a\n        database, or `get_browser_locale`, which uses the ``Accept-Language``\n        header.\n\n        .. versionchanged: 4.1\n           Added a property setter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def begin_state(self, batch_size=0, func=ndarray.zeros, **kwargs):\n        \"\"\"\n        \"\"\"\n        assert not self._modified, \\\n            \"After applying modifier cells (e.g. ZoneoutCell) the base \" \\\n            \"cell cannot be called directly. Call the modifier cell instead.\"\n        states = []\n        for info in self.state_info(batch_size):\n            self._init_counter += 1\n            if info is not None:\n                info.update(kwargs)\n            else:\n                info = kwargs\n            state = func(name='%sbegin_state_%d'%(self._prefix, self._init_counter),\n                         **info)\n            states.append(state)\n        return states", "output": "Initial state for this cell.\n\n        Parameters\n        ----------\n        func : callable, default symbol.zeros\n            Function for creating initial state.\n\n            For Symbol API, func can be `symbol.zeros`, `symbol.uniform`,\n            `symbol.var etc`. Use `symbol.var` if you want to directly\n            feed input as states.\n\n            For NDArray API, func can be `ndarray.zeros`, `ndarray.ones`, etc.\n        batch_size: int, default 0\n            Only required for NDArray API. Size of the batch ('N' in layout)\n            dimension of input.\n\n        **kwargs :\n            Additional keyword arguments passed to func. For example\n            `mean`, `std`, `dtype`, etc.\n\n        Returns\n        -------\n        states : nested list of Symbol\n            Starting states for the first RNN step.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read(self, x):\n    \"\"\"\n    \"\"\"\n    access_logits = self._address_content(x)\n    weights = tf.nn.softmax(access_logits)\n    retrieved_mem = tf.reduce_sum(\n        tf.multiply(tf.expand_dims(weights, 3),\n                    tf.expand_dims(self.mem_vals, axis=1)), axis=2)\n    return access_logits, retrieved_mem", "output": "Read from the memory.\n\n    An external component can use the results via a simple MLP,\n    e.g., fn(x W_x + retrieved_mem W_m).\n\n    Args:\n      x: a tensor in the shape of [batch_size, length, depth].\n    Returns:\n      access_logits: the logits for accessing the memory in shape of\n          [batch_size, length, memory_size].\n      retrieved_mem: the retrieved results in the shape of\n          [batch_size, length, val_depth].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _SetAllFieldTypes(self, package, desc_proto, scope):\n    \"\"\"\n    \"\"\"\n\n    package = _PrefixWithDot(package)\n\n    main_desc = self._GetTypeFromScope(package, desc_proto.name, scope)\n\n    if package == '.':\n      nested_package = _PrefixWithDot(desc_proto.name)\n    else:\n      nested_package = '.'.join([package, desc_proto.name])\n\n    for field_proto, field_desc in zip(desc_proto.field, main_desc.fields):\n      self._SetFieldType(field_proto, field_desc, nested_package, scope)\n\n    for extension_proto, extension_desc in (\n        zip(desc_proto.extension, main_desc.extensions)):\n      extension_desc.containing_type = self._GetTypeFromScope(\n          nested_package, extension_proto.extendee, scope)\n      self._SetFieldType(extension_proto, extension_desc, nested_package, scope)\n\n    for nested_type in desc_proto.nested_type:\n      self._SetAllFieldTypes(nested_package, nested_type, scope)", "output": "Sets all the descriptor's fields's types.\n\n    This method also sets the containing types on any extensions.\n\n    Args:\n      package: The current package of desc_proto.\n      desc_proto: The message descriptor to update.\n      scope: Enclosing scope of available types.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_pool(lb, name):\n    '''\n    \n    '''\n    if __opts__['load_balancers'].get(lb, None):\n        (username, password) = list(__opts__['load_balancers'][lb].values())\n    else:\n        raise Exception('Unable to find `{0}` load balancer'.format(lb))\n    F5 = F5Mgmt(lb, username, password)\n    return F5.check_pool(name)", "output": "Check to see if a pool exists\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-run f5.check_pool load_balancer pool_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def settlement_method(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            return self.__dict__[\"settlement_method\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'settlement_method' \".format(self.order_book_id)\n            )", "output": "[str] \u4ea4\u5272\u65b9\u5f0f\uff0c\u2019CashSettlementRequired\u2019 - \u73b0\u91d1\u4ea4\u5272, \u2018PhysicalSettlementRequired\u2019 - \u5b9e\u7269\u4ea4\u5272\uff08\u671f\u8d27\u4e13\u7528\uff09", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_image(data, epoch, image_size, batch_size, output_dir, padding=2):\n    \"\"\"  \"\"\"\n    data = data.asnumpy().transpose((0, 2, 3, 1))\n    datanp = np.clip(\n        (data - np.min(data))*(255.0/(np.max(data) - np.min(data))), 0, 255).astype(np.uint8)\n    x_dim = min(8, batch_size)\n    y_dim = int(math.ceil(float(batch_size) / x_dim))\n    height, width = int(image_size + padding), int(image_size + padding)\n    grid = np.zeros((height * y_dim + 1 + padding // 2, width *\n                     x_dim + 1 + padding // 2, 3), dtype=np.uint8)\n    k = 0\n    for y in range(y_dim):\n        for x in range(x_dim):\n            if k >= batch_size:\n                break\n            start_y = y * height + 1 + padding // 2\n            end_y = start_y + height - padding\n            start_x = x * width + 1 + padding // 2\n            end_x = start_x + width - padding\n            np.copyto(grid[start_y:end_y, start_x:end_x, :], datanp[k])\n            k += 1\n    imageio.imwrite(\n        '{}/fake_samples_epoch_{}.png'.format(output_dir, epoch), grid)", "output": "save image", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cmd_subset(\n            self,\n            tgt,\n            fun,\n            arg=(),\n            timeout=None,\n            tgt_type='glob',\n            ret='',\n            kwarg=None,\n            sub=3,\n            **kwargs):\n        '''\n        \n        '''\n        minion_ret = self.cmd(tgt,\n                              'sys.list_functions',\n                              tgt_type=tgt_type,\n                              **kwargs)\n        minions = list(minion_ret)\n        random.shuffle(minions)\n        f_tgt = []\n        for minion in minions:\n            if fun in minion_ret[minion]['return']:\n                f_tgt.append(minion)\n            if len(f_tgt) >= sub:\n                break\n        return self.cmd_iter(f_tgt, fun, arg, timeout, tgt_type='list', ret=ret, kwarg=kwarg, **kwargs)", "output": "Execute a command on a random subset of the targeted systems\n\n        The function signature is the same as :py:meth:`cmd` with the\n        following exceptions.\n\n        :param sub: The number of systems to execute on\n\n        .. code-block:: python\n\n            >>> import salt.client.ssh.client\n            >>> sshclient= salt.client.ssh.client.SSHClient()\n            >>> sshclient.cmd_subset('*', 'test.ping', sub=1)\n            {'jerry': True}\n\n        .. versionadded:: 2017.7.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kill_plasma_store(self, check_alive=True):\n        \"\"\"\n        \"\"\"\n        self._kill_process_type(\n            ray_constants.PROCESS_TYPE_PLASMA_STORE, check_alive=check_alive)", "output": "Kill the plasma store.\n\n        Args:\n            check_alive (bool): Raise an exception if the process was already\n                dead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dict_func(self, func, axis, *args, **kwargs):\n        \"\"\"\n        \"\"\"\n        if \"axis\" not in kwargs:\n            kwargs[\"axis\"] = axis\n\n        if axis == 0:\n            index = self.columns\n        else:\n            index = self.index\n        func = {idx: func[key] for key in func for idx in index.get_indexer_for([key])}\n\n        def dict_apply_builder(df, func_dict={}):\n            # Sometimes `apply` can return a `Series`, but we require that internally\n            # all objects are `DataFrame`s.\n            return pandas.DataFrame(df.apply(func_dict, *args, **kwargs))\n\n        result_data = self.data.apply_func_to_select_indices_along_full_axis(\n            axis, dict_apply_builder, func, keep_remaining=False\n        )\n        full_result = self._post_process_apply(result_data, axis)\n        return full_result", "output": "Apply function to certain indices across given axis.\n\n        Args:\n            func: The function to apply.\n            axis: Target axis to apply the function along.\n\n        Returns:\n            A new PandasQueryCompiler.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_connection(service, module=None, region=None, key=None, keyid=None,\n                   profile=None):\n    '''\n    \n    '''\n\n    module = module or service\n\n    cxkey, region, key, keyid = _get_profile(service, region, key,\n                                             keyid, profile)\n    cxkey = cxkey + ':conn3'\n\n    if cxkey in __context__:\n        return __context__[cxkey]\n\n    try:\n        session = boto3.session.Session(aws_access_key_id=keyid,\n                          aws_secret_access_key=key,\n                          region_name=region)\n        if session is None:\n            raise SaltInvocationError('Region \"{0}\" is not '\n                                      'valid.'.format(region))\n        conn = session.client(module)\n        if conn is None:\n            raise SaltInvocationError('Region \"{0}\" is not '\n                                      'valid.'.format(region))\n    except boto.exception.NoAuthHandlerFound:\n        raise SaltInvocationError('No authentication credentials found when '\n                                  'attempting to make boto {0} connection to '\n                                  'region \"{1}\".'.format(service, region))\n    __context__[cxkey] = conn\n    return conn", "output": "Return a boto connection for the service.\n\n    .. code-block:: python\n\n        conn = __utils__['boto.get_connection']('ec2', profile='custom_profile')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def split_v2(ary, indices_or_sections, axis=0, squeeze_axis=False):\n    \"\"\"\n\n    \"\"\"\n    indices = []\n    axis_size = ary.shape[axis]\n    if isinstance(indices_or_sections, int):\n        sections = indices_or_sections\n        if axis_size % sections:\n            raise ValueError('array split does not result in an equal division')\n        section_size = int(axis_size / sections)\n        indices = [i * section_size for i in range(sections)]\n    elif isinstance(indices_or_sections, tuple):\n        indices = [0] + list(indices_or_sections)\n    else:\n        raise ValueError('indices_or_sections must either int or tuple of ints')\n    return _internal._split_v2(ary, indices, axis, squeeze_axis)", "output": "Split an array into multiple sub-arrays.\n\n    Parameters\n    ----------\n    ary : NDArray\n        Array to be divided into sub-arrays.\n    indices_or_sections : int or tuple of ints\n        If `indices_or_sections` is an integer, N, the array will be divided\n        into N equal arrays along `axis`.  If such a split is not possible,\n        an error is raised.\n        If `indices_or_sections` is a 1-D array of sorted integers, the entries\n        indicate where along `axis` the array is split.  For example,\n        ``[2, 3]`` would, for ``axis=0``, result in\n        - ary[:2]\n        - ary[2:3]\n        - ary[3:]\n        If an index exceeds the dimension of the array along `axis`,\n        an empty sub-array is returned correspondingly.\n    axis : int, optional\n        The axis along which to split, default is 0.\n    squeeze_axis: boolean, optional\n        Whether to squeeze the axis of sub-arrays or not, only useful when size\n        of the sub-arrays are 1 on the `axis`. Default is False.\n\n    Returns\n    -------\n    NDArray\n        A created array.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_floating_ip(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        log.error(\n            'The create_floating_ip function must be called with -f or --function.'\n        )\n        return False\n\n    if not kwargs:\n        kwargs = {}\n\n    if 'droplet_id' in kwargs:\n        result = query(method='floating_ips',\n                           args={'droplet_id': kwargs['droplet_id']},\n                           http_method='post')\n\n        return result\n\n    elif 'region' in kwargs:\n        result = query(method='floating_ips',\n                           args={'region': kwargs['region']},\n                           http_method='post')\n\n        return result\n\n    else:\n        log.error('A droplet_id or region is required.')\n        return False", "output": "Create a new floating IP\n\n    .. versionadded:: 2016.3.0\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-cloud -f create_floating_ip my-digitalocean-config region='NYC2'\n\n        salt-cloud -f create_floating_ip my-digitalocean-config droplet_id='1234567'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def soft_kill(jid, state_id=None):\n    '''\n    \n    '''\n    minion = salt.minion.MasterMinion(__opts__)\n    minion.functions['state.soft_kill'](jid, state_id)", "output": "Set up a state run to die before executing the given state id,\n    this instructs a running state to safely exit at a given\n    state id. This needs to pass in the jid of the running state.\n    If a state_id is not passed then the jid referenced will be safely exited\n    at the beginning of the next state run.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _merge_assets_key_collection(saved_model_proto, path):\n  \"\"\"\n  \"\"\"\n  for meta_graph in saved_model_proto.meta_graphs:\n    node_asset_map = {}\n    if tf_v1.saved_model.constants.ASSETS_KEY in meta_graph.collection_def:\n      assets_any_proto = meta_graph.collection_def[\n          tf_v1.saved_model.constants.ASSETS_KEY].any_list.value\n      for asset_any_proto in assets_any_proto:\n        asset_proto = meta_graph_pb2.AssetFileDef()\n        asset_any_proto.Unpack(asset_proto)\n        asset_filename = _get_asset_filename(path, asset_proto.filename)\n        node_asset_map[_get_node_name_from_tensor(\n            asset_proto.tensor_info.name)] = asset_filename\n      del meta_graph.collection_def[tf_v1.saved_model.constants.ASSETS_KEY]\n\n    for node in meta_graph.graph_def.node:\n      asset_filepath = node_asset_map.get(node.name)\n      if asset_filepath:\n        _check_asset_node_def(node)\n        node.attr[\"value\"].tensor.string_val[0] = asset_filepath", "output": "Merges the ASSETS_KEY collection into the GraphDefs in saved_model_proto.\n\n  Removes the ASSETS_KEY collection from the GraphDefs in the SavedModel and\n  modifies nodes with the assets filenames to point to the assets in `path`.\n  After this transformation, the SavedModel GraphDefs can be used without\n  feeding asset tensors.\n\n  Args:\n    saved_model_proto: SavedModel proto to be modified.\n    path: path where the SavedModel is being loaded from.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_device(device=None, device_class=None, collector='localhost', prod_state=1000):\n    '''\n    \n    '''\n\n    if not device:\n        device = __salt__['grains.get']('fqdn')\n\n    if not device_class:\n        device_class = _determine_device_class()\n    log.info('Adding device %s to zenoss', device)\n    data = dict(deviceName=device, deviceClass=device_class, model=True, collector=collector, productionState=prod_state)\n    response = _router_request('DeviceRouter', 'addDevice', data=[data])\n    return response", "output": "A function to connect to a zenoss server and add a new device entry.\n\n    Parameters:\n        device:         (Optional) Will use the grain 'fqdn' by default.\n        device_class:   (Optional) The device class to use. If none, will determine based on kernel grain.\n        collector:      (Optional) The collector to use for this device. Defaults to 'localhost'.\n        prod_state:     (Optional) The prodState to set on the device. If none, defaults to 1000 ( production )\n\n    CLI Example:\n        salt '*' zenoss.add_device", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_labels(self, label, crop_box, height, width):\n        \"\"\"\"\"\"\n        xmin = float(crop_box[0]) / width\n        ymin = float(crop_box[1]) / height\n        w = float(crop_box[2]) / width\n        h = float(crop_box[3]) / height\n        out = label.copy()\n        out[:, (1, 3)] -= xmin\n        out[:, (2, 4)] -= ymin\n        out[:, (1, 3)] /= w\n        out[:, (2, 4)] /= h\n        out[:, 1:5] = np.maximum(0, out[:, 1:5])\n        out[:, 1:5] = np.minimum(1, out[:, 1:5])\n        coverage = self._calculate_areas(out[:, 1:]) * w * h / self._calculate_areas(label[:, 1:])\n        valid = np.logical_and(out[:, 3] > out[:, 1], out[:, 4] > out[:, 2])\n        valid = np.logical_and(valid, coverage > self.min_eject_coverage)\n        valid = np.where(valid)[0]\n        if valid.size < 1:\n            return None\n        out = out[valid, :]\n        return out", "output": "Convert labels according to crop box", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __apf_cmd(cmd):\n    '''\n    \n    '''\n    apf_cmd = '{0} {1}'.format(salt.utils.path.which('apf'), cmd)\n    out = __salt__['cmd.run_all'](apf_cmd)\n\n    if out['retcode'] != 0:\n        if not out['stderr']:\n            msg = out['stdout']\n        else:\n            msg = out['stderr']\n        raise CommandExecutionError(\n            'apf failed: {0}'.format(msg)\n        )\n    return out['stdout']", "output": "Return the apf location", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_column(self, key, column, **kwargs):\n        \"\"\"\n        \n\n        \"\"\"\n        return self.get_storer(key).read_column(column=column, **kwargs)", "output": "return a single column from the table. This is generally only useful to\n        select an indexable\n\n        Parameters\n        ----------\n        key : object\n        column: the column of interest\n\n        Exceptions\n        ----------\n        raises KeyError if the column is not found (or key is not a valid\n            store)\n        raises ValueError if the column can not be extracted individually (it\n            is part of a data block)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_table(self, table, fields, retry=DEFAULT_RETRY):\n        \"\"\"\n        \"\"\"\n        partial = table._build_resource(fields)\n        if table.etag is not None:\n            headers = {\"If-Match\": table.etag}\n        else:\n            headers = None\n        api_response = self._call_api(\n            retry, method=\"PATCH\", path=table.path, data=partial, headers=headers\n        )\n        return Table.from_api_repr(api_response)", "output": "Change some fields of a table.\n\n        Use ``fields`` to specify which fields to update. At least one field\n        must be provided. If a field is listed in ``fields`` and is ``None``\n        in ``table``, it will be deleted.\n\n        If ``table.etag`` is not ``None``, the update will only succeed if\n        the table on the server has the same ETag. Thus reading a table with\n        ``get_table``, changing its fields, and then passing it to\n        ``update_table`` will ensure that the changes will only be saved if\n        no modifications to the table occurred since the read.\n\n        Args:\n            table (google.cloud.bigquery.table.Table): The table to update.\n            fields (Sequence[str]):\n                The fields of ``table`` to change, spelled as the Table\n                properties (e.g. \"friendly_name\").\n            retry (google.api_core.retry.Retry):\n                (Optional) A description of how to retry the API call.\n\n        Returns:\n            google.cloud.bigquery.table.Table:\n                The table resource returned from the API call.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render_pep440_pre(pieces):\n    \"\"\"\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \".post.dev%d\" % pieces[\"distance\"]\n    else:\n        # exception #1\n        rendered = \"0.post.dev%d\" % pieces[\"distance\"]\n    return rendered", "output": "TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_socket(addr, type=socket.SOCK_STREAM, proto=0):\n    '''\n    \n    '''\n\n    version = ipaddress.ip_address(addr).version\n    if version == 4:\n        family = socket.AF_INET\n    elif version == 6:\n        family = socket.AF_INET6\n    return socket.socket(family, type, proto)", "output": "Return a socket object for the addr\n    IP-version agnostic", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_datacenters_via_proxy(datacenter_names=None, service_instance=None):\n    '''\n    \n    '''\n    if not datacenter_names:\n        dc_refs = salt.utils.vmware.get_datacenters(service_instance,\n                                                    get_all_datacenters=True)\n    else:\n        dc_refs = salt.utils.vmware.get_datacenters(service_instance,\n                                                    datacenter_names)\n\n    return [{'name': salt.utils.vmware.get_managed_object_name(dc_ref)}\n            for dc_ref in dc_refs]", "output": "Returns a list of dict representations of VMware datacenters.\n    Connection is done via the proxy details.\n\n    Supported proxies: esxdatacenter\n\n    datacenter_names\n        List of datacenter names.\n        Default is None.\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.list_datacenters_via_proxy\n\n        salt '*' vsphere.list_datacenters_via_proxy dc1\n\n        salt '*' vsphere.list_datacenters_via_proxy dc1,dc2\n\n        salt '*' vsphere.list_datacenters_via_proxy datacenter_names=[dc1, dc2]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def data_fetch(self, url, task):\n        ''''''\n        self.on_fetch('data', task)\n        result = {}\n        result['orig_url'] = url\n        result['content'] = dataurl.decode(url)\n        result['headers'] = {}\n        result['status_code'] = 200\n        result['url'] = url\n        result['cookies'] = {}\n        result['time'] = 0\n        result['save'] = task.get('fetch', {}).get('save')\n        if len(result['content']) < 70:\n            logger.info(\"[200] %s:%s %s 0s\", task.get('project'), task.get('taskid'), url)\n        else:\n            logger.info(\n                \"[200] %s:%s data:,%s...[content:%d] 0s\",\n                task.get('project'), task.get('taskid'),\n                result['content'][:70],\n                len(result['content'])\n            )\n\n        return result", "output": "A fake fetcher for dataurl", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_value_label(self, byteorder, encoding):\n        \"\"\"\n        \n        \"\"\"\n\n        self._encoding = encoding\n        bio = BytesIO()\n        null_string = '\\x00'\n        null_byte = b'\\x00'\n\n        # len\n        bio.write(struct.pack(byteorder + 'i', self.len))\n\n        # labname\n        labname = self._encode(_pad_bytes(self.labname[:32], 33))\n        bio.write(labname)\n\n        # padding - 3 bytes\n        for i in range(3):\n            bio.write(struct.pack('c', null_byte))\n\n        # value_label_table\n        # n - int32\n        bio.write(struct.pack(byteorder + 'i', self.n))\n\n        # textlen  - int32\n        bio.write(struct.pack(byteorder + 'i', self.text_len))\n\n        # off - int32 array (n elements)\n        for offset in self.off:\n            bio.write(struct.pack(byteorder + 'i', offset))\n\n        # val - int32 array (n elements)\n        for value in self.val:\n            bio.write(struct.pack(byteorder + 'i', value))\n\n        # txt - Text labels, null terminated\n        for text in self.txt:\n            bio.write(self._encode(text + null_string))\n\n        bio.seek(0)\n        return bio.read()", "output": "Parameters\n        ----------\n        byteorder : str\n            Byte order of the output\n        encoding : str\n            File encoding\n\n        Returns\n        -------\n        value_label : bytes\n            Bytes containing the formatted value label", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_link_map(self, nslave):\n        \"\"\"\n        \n        \"\"\"\n        tree_map, parent_map = self.get_tree(nslave)\n        ring_map = self.get_ring(tree_map, parent_map)\n        rmap = {0 : 0}\n        k = 0\n        for i in range(nslave - 1):\n            k = ring_map[k][1]\n            rmap[k] = i + 1\n\n        ring_map_ = {}\n        tree_map_ = {}\n        parent_map_ ={}\n        for k, v in ring_map.items():\n            ring_map_[rmap[k]] = (rmap[v[0]], rmap[v[1]])\n        for k, v in tree_map.items():\n            tree_map_[rmap[k]] = [rmap[x] for x in v]\n        for k, v in parent_map.items():\n            if k != 0:\n                parent_map_[rmap[k]] = rmap[v]\n            else:\n                parent_map_[rmap[k]] = -1\n        return tree_map_, parent_map_, ring_map_", "output": "get the link map, this is a bit hacky, call for better algorithm\n        to place similar nodes together", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_extension(self, name):\n        \"\"\"\n        \"\"\"\n\n        if name in self.__extensions:\n            raise errors.ExtensionAlreadyLoaded(name)\n\n        try:\n            lib = importlib.import_module(name)\n        except ImportError as e:\n            raise errors.ExtensionNotFound(name, e) from e\n        else:\n            self._load_from_module_spec(lib, name)", "output": "Loads an extension.\n\n        An extension is a python module that contains commands, cogs, or\n        listeners.\n\n        An extension must have a global function, ``setup`` defined as\n        the entry point on what to do when the extension is loaded. This entry\n        point must have a single argument, the ``bot``.\n\n        Parameters\n        ------------\n        name: :class:`str`\n            The extension name to load. It must be dot separated like\n            regular Python imports if accessing a sub-module. e.g.\n            ``foo.test`` if you want to import ``foo/test.py``.\n\n        Raises\n        --------\n        ExtensionNotFound\n            The extension could not be imported.\n        ExtensionAlreadyLoaded\n            The extension is already loaded.\n        NoEntryPointError\n            The extension does not have a setup function.\n        ExtensionFailed\n            The extension setup function had an execution error.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_cache(app: Flask, cache_config) -> Optional[Cache]:\n    \"\"\"\"\"\"\n    if cache_config and cache_config.get('CACHE_TYPE') != 'null':\n        return Cache(app, config=cache_config)\n\n    return None", "output": "Setup the flask-cache on a flask app", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def total_timer(msg):\n    \"\"\"  \"\"\"\n    start = timer()\n    yield\n    t = timer() - start\n    _TOTAL_TIMER_DATA[msg].feed(t)", "output": "A context which add the time spent inside to TotalTimer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _linux_stp(br, state):\n    '''\n    \n    '''\n    brctl = _tool_path('brctl')\n    return __salt__['cmd.run']('{0} stp {1} {2}'.format(brctl, br, state),\n                               python_shell=False)", "output": "Internal, sets STP state", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_cli(ctx, template, semantic_version):\n    \"\"\"\"\"\"\n    try:\n        template_data = get_template_data(template)\n    except ValueError as ex:\n        click.secho(\"Publish Failed\", fg='red')\n        raise UserException(str(ex))\n\n    # Override SemanticVersion in template metadata when provided in command input\n    if semantic_version and SERVERLESS_REPO_APPLICATION in template_data.get(METADATA, {}):\n        template_data.get(METADATA).get(SERVERLESS_REPO_APPLICATION)[SEMANTIC_VERSION] = semantic_version\n\n    try:\n        publish_output = publish_application(template_data)\n        click.secho(\"Publish Succeeded\", fg=\"green\")\n        click.secho(_gen_success_message(publish_output))\n    except InvalidS3UriError:\n        click.secho(\"Publish Failed\", fg='red')\n        raise UserException(\n            \"Your SAM template contains invalid S3 URIs. Please make sure that you have uploaded application \"\n            \"artifacts to S3 by packaging the template. See more details in {}\".format(SAM_PACKAGE_DOC))\n    except ServerlessRepoError as ex:\n        click.secho(\"Publish Failed\", fg='red')\n        LOG.debug(\"Failed to publish application to serverlessrepo\", exc_info=True)\n        error_msg = '{}\\nPlease follow the instructions in {}'.format(str(ex), SAM_PUBLISH_DOC)\n        raise UserException(error_msg)\n\n    application_id = publish_output.get('application_id')\n    _print_console_link(ctx.region, application_id)", "output": "Publish the application based on command line inputs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_variable(self, name):\n        \"\"\"\n        \n        \"\"\"\n        name = get_op_tensor_name(name)[1]\n        if len(self.vs_name):\n            name_with_vs = self.vs_name + \"/\" + name\n        else:\n            name_with_vs = name\n        return get_op_or_tensor_by_name(name_with_vs)", "output": "Get a variable used in this tower.\n        The name should not contain the variable scope prefix of the tower.\n\n        When the tower has the same variable scope and name scope, this is equivalent to\n        :meth:`get_tensor`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_minlength(self, min_length, field, value):\n        \"\"\"  \"\"\"\n        if isinstance(value, Iterable) and len(value) < min_length:\n            self._error(field, errors.MIN_LENGTH, len(value))", "output": "{'type': 'integer'}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_wake_on_modem(enabled):\n    '''\n    \n    '''\n    state = salt.utils.mac_utils.validate_enabled(enabled)\n    cmd = 'systemsetup -setwakeonmodem {0}'.format(state)\n    salt.utils.mac_utils.execute_return_success(cmd)\n\n    return salt.utils.mac_utils.confirm_updated(\n        state,\n        get_wake_on_modem,\n    )", "output": "Set whether or not the computer will wake from sleep when modem activity is\n    detected.\n\n    :param bool enabled: True to enable, False to disable. \"On\" and \"Off\" are\n        also acceptable values. Additionally you can pass 1 and 0 to represent\n        True and False respectively\n\n    :return: True if successful, False if not\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' power.set_wake_on_modem True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fold(self):\n        \"\"\"\"\"\"\n        start, end = self.get_range()\n        TextBlockHelper.set_collapsed(self._trigger, True)\n        block = self._trigger.next()\n        while block.blockNumber() <= end and block.isValid():\n            block.setVisible(False)\n            block = block.next()", "output": "Folds the region.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset(self, indices=None):\n    \"\"\"\n    \"\"\"\n    return tf.cond(\n        tf.cast(tf.reduce_sum(indices + 1), tf.bool),\n        lambda: self._reset_non_empty(indices),\n        lambda: tf.cast(0, self.observ_dtype))", "output": "Reset the batch of environments.\n\n    Args:\n      indices: The batch indices of the environments to reset.\n\n    Returns:\n      Batch tensor of the new observations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_instancenorm(node, **kwargs):\n    \"\"\"\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    eps = float(attrs.get(\"eps\", 0.001))\n\n    node = onnx.helper.make_node(\n        'InstanceNormalization',\n        inputs=input_nodes,\n        outputs=[name],\n        name=name,\n        epsilon=eps)\n\n    return [node]", "output": "Map MXNet's InstanceNorm operator attributes to onnx's InstanceNormalization operator\n    based on the input node's attributes and return the created node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_extract_method(path):\n  \"\"\"\"\"\"\n  info_path = _get_info_path(path)\n  info = _read_info(info_path)\n  fname = info.get('original_fname', path) if info else path\n  return _guess_extract_method(fname)", "output": "Returns `ExtractMethod` to use on resource at path. Cannot be None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def headless(self, value):\n        \"\"\"\n        \n        \"\"\"\n        if value is True:\n            self._arguments.append('-headless')\n        elif '-headless' in self._arguments:\n            self._arguments.remove('-headless')", "output": "Sets the headless argument\n\n        Args:\n          value: boolean value indicating to set the headless option", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _AddSerializeToStringMethod(message_descriptor, cls):\n  \"\"\"\"\"\"\n\n  def SerializeToString(self):\n    # Check if the message has all of its required fields set.\n    errors = []\n    if not self.IsInitialized():\n      raise message_mod.EncodeError(\n          'Message %s is missing required fields: %s' % (\n          self.DESCRIPTOR.full_name, ','.join(self.FindInitializationErrors())))\n    return self.SerializePartialToString()\n  cls.SerializeToString = SerializeToString", "output": "Helper for _AddMessageMethods().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(self, path=None, format=None, schema=None, **options):\n        \"\"\"\n        \"\"\"\n        if format is not None:\n            self.format(format)\n        if schema is not None:\n            self.schema(schema)\n        self.options(**options)\n        if isinstance(path, basestring):\n            return self._df(self._jreader.load(path))\n        elif path is not None:\n            if type(path) != list:\n                path = [path]\n            return self._df(self._jreader.load(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n        else:\n            return self._df(self._jreader.load())", "output": "Loads data from a data source and returns it as a :class`DataFrame`.\n\n        :param path: optional string or a list of string for file-system backed data sources.\n        :param format: optional string for format of the data source. Default to 'parquet'.\n        :param schema: optional :class:`pyspark.sql.types.StructType` for the input schema\n                       or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n        :param options: all other string options\n\n        >>> df = spark.read.format(\"parquet\").load('python/test_support/sql/parquet_partitioned',\n        ...     opt1=True, opt2=1, opt3='str')\n        >>> df.dtypes\n        [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n\n        >>> df = spark.read.format('json').load(['python/test_support/sql/people.json',\n        ...     'python/test_support/sql/people1.json'])\n        >>> df.dtypes\n        [('age', 'bigint'), ('aka', 'string'), ('name', 'string')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _my_top_k(x, k):\n  \"\"\"\n  \"\"\"\n  if k > 10:\n    return tf.nn.top_k(x, k)\n  values = []\n  indices = []\n  depth = tf.shape(x)[1]\n  for i in range(k):\n    values.append(tf.reduce_max(x, 1))\n    argmax = tf.argmax(x, 1)\n    indices.append(argmax)\n    if i + 1 < k:\n      x += tf.one_hot(argmax, depth, -1e9)\n  return tf.stack(values, axis=1), tf.to_int32(tf.stack(indices, axis=1))", "output": "GPU-compatible version of top-k that works for very small constant k.\n\n  Calls argmax repeatedly.\n\n  tf.nn.top_k is implemented for GPU, but the gradient, sparse_to_dense,\n  seems not to be, so if we use tf.nn.top_k, then both the top_k and its\n  gradient go on cpu.  Once this is not an issue, this function becomes\n  obsolete and should be replaced by tf.nn.top_k.\n\n  Args:\n    x: a 2d Tensor.\n    k: a small integer.\n\n  Returns:\n    values: a Tensor of shape [batch_size, k]\n    indices: a int32 Tensor of shape [batch_size, k]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _new_alloc_handle(stype, shape, ctx, delay_alloc, dtype, aux_types, aux_shapes=None):\n    \"\"\"\n    \"\"\"\n    hdl = NDArrayHandle()\n    for aux_t in aux_types:\n        if np.dtype(aux_t) != np.dtype(\"int64\"):\n            raise NotImplementedError(\"only int64 is supported for aux types\")\n    aux_type_ids = [int(_DTYPE_NP_TO_MX[np.dtype(aux_t).type]) for aux_t in aux_types]\n    aux_shapes = [(0,) for aux_t in aux_types] if aux_shapes is None else aux_shapes\n    aux_shape_lens = [len(aux_shape) for aux_shape in aux_shapes]\n    aux_shapes = py_sum(aux_shapes, ())\n    num_aux = mx_uint(len(aux_types))\n    check_call(_LIB.MXNDArrayCreateSparseEx(\n        ctypes.c_int(int(_STORAGE_TYPE_STR_TO_ID[stype])),\n        c_array_buf(mx_uint, native_array('I', shape)),\n        mx_uint(len(shape)),\n        ctypes.c_int(ctx.device_typeid),\n        ctypes.c_int(ctx.device_id),\n        ctypes.c_int(int(delay_alloc)),\n        ctypes.c_int(int(_DTYPE_NP_TO_MX[np.dtype(dtype).type])),\n        num_aux,\n        c_array_buf(ctypes.c_int, native_array('i', aux_type_ids)),\n        c_array_buf(mx_uint, native_array('I', aux_shape_lens)),\n        c_array_buf(mx_uint, native_array('I', aux_shapes)),\n        ctypes.byref(hdl)))\n    return hdl", "output": "Return a new handle with specified storage type, shape, dtype and context.\n\n    Empty handle is only used to hold results\n\n    Returns\n    -------\n    handle\n        A new empty ndarray handle", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(name=None, pkgs=None, **kwargs):\n    '''\n    \n    '''\n    pkg_params = __salt__['pkg_resource.parse_targets'](name,\n                                                        pkgs,\n                                                        **kwargs)[0]\n    old = list_pkgs()\n    targets = [x for x in pkg_params if x in old]\n    if not targets:\n        return {}\n\n    cmd = ['port', 'uninstall']\n    cmd.extend(targets)\n\n    err_message = ''\n    try:\n        salt.utils.mac_utils.execute_return_success(cmd)\n    except CommandExecutionError as exc:\n        err_message = exc.strerror\n\n    __context__.pop('pkg.list_pkgs', None)\n    new = list_pkgs()\n    ret = salt.utils.data.compare_dicts(old, new)\n\n    if err_message:\n        raise CommandExecutionError(\n            'Problem encountered removing package(s)',\n            info={'errors': err_message, 'changes': ret})\n\n    return ret", "output": "Removes packages with ``port uninstall``.\n\n    name\n        The name of the package to be deleted.\n\n\n    Multiple Package Options:\n\n    pkgs\n        A list of packages to delete. Must be passed as a python list. The\n        ``name`` parameter will be ignored if this option is passed.\n\n    .. versionadded:: 0.16.0\n\n\n    Returns a dict containing the changes.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.remove <package name>\n        salt '*' pkg.remove <package1>,<package2>,<package3>\n        salt '*' pkg.remove pkgs='[\"foo\", \"bar\"]'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _proxy_process(proxyname, test):\n    '''\n    \n    '''\n    changes_old = []\n    changes_new = []\n    if not _is_proxy_running(proxyname):\n        if not test:\n            __salt__['cmd.run_all'](\n                'salt-proxy --proxyid={0} -l info -d'.format(salt.ext.six.moves.shlex_quote(proxyname)),\n                timeout=5)\n            changes_new.append('Salt Proxy: Started proxy process for {0}'\n                               .format(proxyname))\n        else:\n            changes_new.append('Salt Proxy: process {0} will be started'\n                               .format(proxyname))\n    else:\n        changes_old.append('Salt Proxy: already running for {0}'\n                           .format(proxyname))\n    return True, changes_new, changes_old", "output": "Check and execute proxy process", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply(self, func, axis, *args, **kwargs):\n        \"\"\"\n        \"\"\"\n        if callable(func):\n            return self._callable_func(func, axis, *args, **kwargs)\n        elif isinstance(func, dict):\n            return self._dict_func(func, axis, *args, **kwargs)\n        elif is_list_like(func):\n            return self._list_like_func(func, axis, *args, **kwargs)\n        else:\n            pass", "output": "Apply func across given axis.\n\n        Args:\n            func: The function to apply.\n            axis: Target axis to apply the function along.\n\n        Returns:\n            A new PandasQueryCompiler.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_docstring(self, doc_type, quote):\r\n        \"\"\"\"\"\"\r\n        docstring = None\r\n\r\n        self.quote3 = quote * 3\r\n        if quote == '\"':\r\n            self.quote3_other = \"'''\"\r\n        else:\r\n            self.quote3_other = '\"\"\"'\r\n\r\n        result = self.get_function_definition_from_below_last_line()\r\n\r\n        if result:\r\n            func_def, __ = result\r\n            func_info = FunctionInfo()\r\n            func_info.parse_def(func_def)\r\n\r\n            if func_info.has_info:\r\n                func_body = self.get_function_body(func_info.func_indent)\r\n                if func_body:\r\n                    func_info.parse_body(func_body)\r\n\r\n                if doc_type == 'Numpydoc':\r\n                    docstring = self._generate_numpy_doc(func_info)\r\n                elif doc_type == 'Googledoc':\r\n                    docstring = self._generate_google_doc(func_info)\r\n\r\n        return docstring", "output": "Generate docstring.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_undone_from_datastore(self, shard_id=None, num_shards=None):\n    \"\"\"\n    \"\"\"\n    if shard_id is not None:\n      shards_list = [(i + shard_id) % num_shards for i in range(num_shards)]\n    else:\n      shards_list = []\n    shards_list.append(None)\n    for shard in shards_list:\n      self._read_undone_shard_from_datastore(shard)\n      if self._work:\n        return shard\n    return None", "output": "Reads undone work from the datastore.\n\n    If shard_id and num_shards are specified then this method will attempt\n    to read undone work for shard with id shard_id. If no undone work was found\n    then it will try to read shard (shard_id+1) and so on until either found\n    shard with undone work or all shards are read.\n\n    Args:\n      shard_id: Id of the start shard\n      num_shards: total number of shards\n\n    Returns:\n      id of the shard with undone work which was read. None means that work\n      from all datastore was read.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pty_make_controlling_tty(tty_fd):\n    ''' '''\n\n    child_name = os.ttyname(tty_fd)\n\n    # Disconnect from controlling tty, if any.  Raises OSError of ENXIO\n    # if there was no controlling tty to begin with, such as when\n    # executed by a cron(1) job.\n    try:\n        fd = os.open(\"/dev/tty\", os.O_RDWR | os.O_NOCTTY)\n        os.close(fd)\n    except OSError as err:\n        if err.errno != errno.ENXIO:\n            raise\n\n    os.setsid()\n\n    # Verify we are disconnected from controlling tty by attempting to open\n    # it again.  We expect that OSError of ENXIO should always be raised.\n    try:\n        fd = os.open(\"/dev/tty\", os.O_RDWR | os.O_NOCTTY)\n        os.close(fd)\n        raise PtyProcessError(\"OSError of errno.ENXIO should be raised.\")\n    except OSError as err:\n        if err.errno != errno.ENXIO:\n            raise\n\n    # Verify we can open child pty.\n    fd = os.open(child_name, os.O_RDWR)\n    os.close(fd)\n\n    # Verify we now have a controlling tty.\n    fd = os.open(\"/dev/tty\", os.O_WRONLY)\n    os.close(fd)", "output": "This makes the pseudo-terminal the controlling tty. This should be\n    more portable than the pty.fork() function. Specifically, this should\n    work on Solaris.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def metrics_api(self):\n        \"\"\"\n        \"\"\"\n        if self._metrics_api is None:\n            if self._use_grpc:\n                self._metrics_api = _gapic.make_metrics_api(self)\n            else:\n                self._metrics_api = JSONMetricsAPI(self)\n        return self._metrics_api", "output": "Helper for log metric-related API calls.\n\n        See\n        https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.metrics", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_label(self, label):\n        \"\"\"\n        \"\"\"\n        self.label = label\n        if self.handle is not None:\n            label = list_to_1d_numpy(_label_from_pandas(label), name='label')\n            self.set_field('label', label)\n        return self", "output": "Set label of Dataset.\n\n        Parameters\n        ----------\n        label : list, numpy 1-D array, pandas Series / one-column DataFrame or None\n            The label information to be set into Dataset.\n\n        Returns\n        -------\n        self : Dataset\n            Dataset with set label.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fit(self, data):\n        \"\"\"\n        \n        \"\"\"\n        if not self._transformers:\n            return\n\n        transformed_data = self._preprocess(data)\n        final_step = self._transformers[-1]\n        final_step[1].fit(transformed_data)", "output": "Fits a transformer using the SFrame `data`.\n\n        Parameters\n        ----------\n        data : SFrame\n            The data used to fit the transformer.\n\n        Returns\n        -------\n        self (A fitted object)\n\n        See Also\n        --------\n        transform, fit_transform\n\n        Examples\n        --------\n        .. sourcecode:: python\n\n          >> chain = chain.fit(sf)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _combine(n, rs):\n    \"\"\"\n    \"\"\"\n    try:\n        r, rs = peek(rs)\n    except StopIteration:\n        yield n\n        return\n\n    if overlap(n, r):\n        yield merge(n, r)\n        next(rs)\n        for r in rs:\n            yield r\n    else:\n        yield n\n        for r in rs:\n            yield r", "output": "helper for ``_group_ranges``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cpu():\n    '''\n    \n    '''\n\n    # Test data\n    max_primes = [500, 1000, 2500, 5000]\n\n    # Initializing the test variables\n    test_command = 'sysbench --test=cpu --cpu-max-prime={0} run'\n    result = None\n    ret_val = {}\n\n    # Test beings!\n    for primes in max_primes:\n        key = 'Prime numbers limit: {0}'.format(primes)\n        run_command = test_command.format(primes)\n        result = __salt__['cmd.run'](run_command)\n        ret_val[key] = _parser(result)\n\n    return ret_val", "output": "Tests for the CPU performance of minions.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' sysbench.cpu", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fg_box_logits(self):\n        \"\"\"  \"\"\"\n        return tf.gather(self.box_logits, self.proposals.fg_inds(), name='fg_box_logits')", "output": "Returns: #fg x ? x 4", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def maybe_copy_file_to_directory(source_filepath, target_directory):\n  \"\"\"\n  \"\"\"\n  if not tf.gfile.Exists(target_directory):\n    tf.logging.info(\"Creating directory %s\" % target_directory)\n    os.mkdir(target_directory)\n  target_filepath = os.path.join(target_directory,\n                                 os.path.basename(source_filepath))\n  if not tf.gfile.Exists(target_filepath):\n    tf.logging.info(\"Copying %s to %s\" % (source_filepath, target_filepath))\n    tf.gfile.Copy(source_filepath, target_filepath)\n    statinfo = os.stat(target_filepath)\n    tf.logging.info(\"Successfully copied %s, %s bytes.\" % (target_filepath,\n                                                           statinfo.st_size))\n  else:\n    tf.logging.info(\"Not copying, file already found: %s\" % target_filepath)\n  return target_filepath", "output": "Copy a file to a directory if it is not already there.\n\n  Returns the target filepath.\n\n  Args:\n    source_filepath: a string\n    target_directory: a string\n\n  Returns:\n    a string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save(self, filename, format=None):\n        \"\"\"\n        \n        \"\"\"\n        from .sframe import SFrame as _SFrame\n\n        if format is None:\n            if filename.endswith(('.csv', '.csv.gz', 'txt')):\n                format = 'text'\n            else:\n                format = 'binary'\n        if format == 'binary':\n            with cython_context():\n                self.__proxy__.save(_make_internal_url(filename))\n        elif format == 'text' or format == 'csv':\n            sf = _SFrame({'X1':self})\n            with cython_context():\n                sf.__proxy__.save_as_csv(_make_internal_url(filename), {'header':False})\n        else:\n            raise ValueError(\"Unsupported format: {}\".format(format))", "output": "Saves the SArray to file.\n\n        The saved SArray will be in a directory named with the `targetfile`\n        parameter.\n\n        Parameters\n        ----------\n        filename : string\n            A local path or a remote URL.  If format is 'text', it will be\n            saved as a text file. If format is 'binary', a directory will be\n            created at the location which will contain the SArray.\n\n        format : {'binary', 'text', 'csv'}, optional\n            Format in which to save the SFrame. Binary saved SArrays can be\n            loaded much faster and without any format conversion losses.\n            'text' and 'csv' are synonymous: Each SArray row will be written\n            as a single line in an output text file. If not\n            given, will try to infer the format from filename given. If file\n            name ends with 'csv', 'txt' or '.csv.gz', then save as 'csv' format,\n            otherwise save as 'binary' format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def accept(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.driver.w3c:\n            self.driver.execute(Command.W3C_ACCEPT_ALERT)\n        else:\n            self.driver.execute(Command.ACCEPT_ALERT)", "output": "Accepts the alert available.\n\n        Usage::\n        Alert(driver).accept() # Confirm a alert dialog.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetch_task_output(self):\n        \"\"\"\n        \n        \"\"\"\n        # \n        if os.path.isfile(os.path.join(self.tmp_dir, \"job.out\")):\n            with open(os.path.join(self.tmp_dir, \"job.out\"), \"r\") as f_out:\n                outputs = f_out.readlines()\n        else:\n            outputs = ''\n        return outputs", "output": "Read in the output file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def precompute_edge_matrices(adjacency, hparams):\n  \"\"\"\n  \"\"\"\n  batch_size, num_nodes, _, edge_dim = common_layers.shape_list(adjacency)\n\n  # build the edge_network for incoming edges\n  with tf.variable_scope(\"edge_network\"):\n    x = tf.reshape(\n        adjacency, [batch_size * num_nodes * num_nodes, edge_dim],\n        name=\"adj_reshape_in\")\n\n    for ip_layer in range(hparams.edge_network_layers):\n      name = \"edge_network_layer_%d\"%ip_layer\n      x = tf.layers.dense(common_layers.layer_preprocess(x, hparams),\n                          hparams.edge_network_hidden_size,\n                          activation=tf.nn.relu,\n                          name=name)\n    x = tf.layers.dense(common_layers.layer_preprocess(x, hparams),\n                        hparams.hidden_size**2,\n                        activation=None,\n                        name=\"edge_network_output\")\n\n  # x = [batch * l * l, d *d]\n  edge_matrices_flat = tf.reshape(x, [batch_size, num_nodes,\n                                      num_nodes, hparams.hidden_size,\n                                      hparams.hidden_size])\n\n  # reshape to [batch, l * d, l *d]\n  edge_matrices = tf.reshape(\n      tf.transpose(edge_matrices_flat, [0, 1, 3, 2, 4]), [\n          -1, num_nodes * hparams.hidden_size,\n          num_nodes * hparams.hidden_size\n      ],\n      name=\"edge_matrices\")\n\n  return edge_matrices", "output": "Precompute the a_in and a_out tensors.\n\n  (we don't want to add to the graph everytime _fprop is called)\n  Args:\n    adjacency: placeholder of real valued vectors of shape [B, L, L, E]\n    hparams: HParams object\n  Returns:\n    edge_matrices: [batch, L * D, L * D] the dense matrix for message passing\n    viewed as a block matrix (L,L) blocks of size (D,D). Each plot is a function\n    of the edge vector of the adjacency matrix at that spot.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_categories(self, new_categories, inplace=False):\n        \"\"\"\n        \n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        if not is_list_like(new_categories):\n            new_categories = [new_categories]\n        already_included = set(new_categories) & set(self.dtype.categories)\n        if len(already_included) != 0:\n            msg = (\"new categories must not include old categories: \"\n                   \"{already_included!s}\")\n            raise ValueError(msg.format(already_included=already_included))\n        new_categories = list(self.dtype.categories) + list(new_categories)\n        new_dtype = CategoricalDtype(new_categories, self.ordered)\n\n        cat = self if inplace else self.copy()\n        cat._dtype = new_dtype\n        cat._codes = coerce_indexer_dtype(cat._codes, new_dtype.categories)\n        if not inplace:\n            return cat", "output": "Add new categories.\n\n        `new_categories` will be included at the last/highest place in the\n        categories and will be unused directly after this call.\n\n        Parameters\n        ----------\n        new_categories : category or list-like of category\n           The new categories to be included.\n        inplace : bool, default False\n           Whether or not to add the categories inplace or return a copy of\n           this categorical with added categories.\n\n        Returns\n        -------\n        cat : Categorical with new categories added or None if inplace.\n\n        Raises\n        ------\n        ValueError\n            If the new categories include old categories or do not validate as\n            categories\n\n        See Also\n        --------\n        rename_categories\n        reorder_categories\n        remove_categories\n        remove_unused_categories\n        set_categories", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve(self):\n        \"\"\"\n        \n        \"\"\"\n        module = __import__(self.module_name, fromlist=['__name__'], level=0)\n        try:\n            return functools.reduce(getattr, self.attrs, module)\n        except AttributeError as exc:\n            raise ImportError(str(exc))", "output": "Resolve the entry point from its module and attrs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def snapshot(self, mode):\n        '''\n        \n        '''\n        self._init_env()\n\n        self._save_cfg_packages(self._get_changed_cfg_pkgs(self._get_cfg_pkgs()))\n        self._save_payload(*self._scan_payload())", "output": "Take a snapshot of the system.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def model_summary(m:Learner, n:int=70):\n    \"\"\n    info = layers_info(m)\n    header = [\"Layer (type)\", \"Output Shape\", \"Param #\", \"Trainable\"]\n    res = \"=\" * n + \"\\n\"\n    res += f\"{header[0]:<20} {header[1]:<20} {header[2]:<10} {header[3]:<10}\\n\"\n    res += \"=\" * n + \"\\n\"\n    total_params = 0\n    total_trainable_params = 0\n    for layer, size, params, trainable in info:\n        if size is None: continue\n        total_params += int(params)\n        total_trainable_params += int(params) * trainable\n        size, trainable = str(list(size)), str(trainable)\n        res += f\"{layer:<20} {size:<20} {int(params):<10,} {trainable:<10}\\n\"\n        res += \"_\" * n + \"\\n\"\n    res += f\"\\nTotal params: {total_params:,}\\n\"\n    res += f\"Total trainable params: {total_trainable_params:,}\\n\"\n    res += f\"Total non-trainable params: {total_params - total_trainable_params:,}\\n\"\n    return PrettyString(res)", "output": "Print a summary of `m` using a output text width of `n` chars", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _rows_page_start(iterator, page, response):\n    \"\"\"\n    \"\"\"\n    total_rows = response.get(\"totalRows\")\n    if total_rows is not None:\n        total_rows = int(total_rows)\n    iterator._total_rows = total_rows", "output": "Grab total rows when :class:`~google.cloud.iterator.Page` starts.\n\n    :type iterator: :class:`~google.api_core.page_iterator.Iterator`\n    :param iterator: The iterator that is currently in use.\n\n    :type page: :class:`~google.api_core.page_iterator.Page`\n    :param page: The page that was just created.\n\n    :type response: dict\n    :param response: The JSON API response for a page of rows in a table.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_calendar_events(self, calendar_id, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if calendar_id in SKIP_IN_PATH:\n            raise ValueError(\n                \"Empty value passed for a required argument 'calendar_id'.\"\n            )\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_ml\", \"calendars\", calendar_id, \"events\"), params=params\n        )", "output": "`<>`_\n\n        :arg calendar_id: The ID of the calendar containing the events\n        :arg end: Get events before this time\n        :arg from_: Skips a number of events\n        :arg job_id: Get events for the job. When this option is used\n            calendar_id must be '_all'\n        :arg size: Specifies a max number of events to get\n        :arg start: Get events after this time", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _assert_no_error(error, exception_class=None):\n    \"\"\"\n    \n    \"\"\"\n    if error == 0:\n        return\n\n    cf_error_string = Security.SecCopyErrorMessageString(error, None)\n    output = _cf_string_to_unicode(cf_error_string)\n    CoreFoundation.CFRelease(cf_error_string)\n\n    if output is None or output == u'':\n        output = u'OSStatus %s' % error\n\n    if exception_class is None:\n        exception_class = ssl.SSLError\n\n    raise exception_class(output)", "output": "Checks the return code and throws an exception if there is an error to\n    report", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def symlink_list(self, load):\n        '''\n        \n        '''\n        if 'env' in load:\n            # \"env\" is not supported; Use \"saltenv\".\n            load.pop('env')\n\n        if not salt.utils.stringutils.is_hex(load['saltenv']) \\\n                and load['saltenv'] not in self.envs():\n            return {}\n        if 'prefix' in load:\n            prefix = load['prefix'].strip('/')\n        else:\n            prefix = ''\n        symlinks = self._file_lists(load, 'symlinks')\n        return dict([(key, val)\n                     for key, val in six.iteritems(symlinks)\n                     if key.startswith(prefix)])", "output": "Return a dict of all symlinks based on a given path in the repo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_indicator_ROC(DataFrame, N=12, M=6):\n    ''\n    C = DataFrame['close']\n    roc = 100 * (C - REF(C, N)) / REF(C, N)\n    ROCMA = MA(roc, M)\n    DICT = {'ROC': roc, 'ROCMA': ROCMA}\n\n    return pd.DataFrame(DICT)", "output": "\u53d8\u52a8\u7387\u6307\u6807", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show(name):\n    '''\n    \n    '''\n    ret = {}\n    ret['enabled'] = False\n    ret['disabled'] = True\n    ret['running'] = False\n    ret['service_path'] = None\n    ret['autostart'] = False\n    ret['command_path'] = None\n\n    ret['available'] = available(name)\n    if not ret['available']:\n        return ret\n\n    ret['enabled'] = enabled(name)\n    ret['disabled'] = not ret['enabled']\n    ret['running'] = status(name)\n    ret['autostart'] = status_autostart(name)\n    ret['service_path'] = _get_svc_path(name)[0]\n    if ret['service_path']:\n        ret['command_path'] = os.path.join(ret['service_path'], 'run')\n\n    # XXX provide info about alias ?\n\n    return ret", "output": "Show properties of one or more units/jobs or the manager\n\n    name\n        the service's name\n\n    CLI Example:\n\n        salt '*' service.show <service name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def option(*param_decls, **attrs):\n    \"\"\"\n    \"\"\"\n    def decorator(f):\n        # Issue 926, copy attrs, so pre-defined options can re-use the same cls=\n        option_attrs = attrs.copy()\n\n        if 'help' in option_attrs:\n            option_attrs['help'] = inspect.cleandoc(option_attrs['help'])\n        OptionClass = option_attrs.pop('cls', Option)\n        _param_memo(f, OptionClass(param_decls, **option_attrs))\n        return f\n    return decorator", "output": "Attaches an option to the command.  All positional arguments are\n    passed as parameter declarations to :class:`Option`; all keyword\n    arguments are forwarded unchanged (except ``cls``).\n    This is equivalent to creating an :class:`Option` instance manually\n    and attaching it to the :attr:`Command.params` list.\n\n    :param cls: the option class to instantiate.  This defaults to\n                :class:`Option`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_input_names(symbol, names, typename, throw):\n    \"\"\"\"\"\"\n    args = symbol.list_arguments()\n    for name in names:\n        if name in args:\n            continue\n        candidates = [arg for arg in args if\n                      not arg.endswith('_weight') and\n                      not arg.endswith('_bias') and\n                      not arg.endswith('_gamma') and\n                      not arg.endswith('_beta')]\n        msg = \"\\033[91mYou created Module with Module(..., %s_names=%s) but \" \\\n              \"input with name '%s' is not found in symbol.list_arguments(). \" \\\n              \"Did you mean one of:\\n\\t%s\\033[0m\"%(\n                  typename, str(names), name, '\\n\\t'.join(candidates))\n        if throw:\n            raise ValueError(msg)\n        else:\n            warnings.warn(msg)", "output": "Check that all input names are in symbol's arguments.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _selection(candidate):\n        \"\"\"\n        \"\"\"\n        sample_index1 = np.random.choice(len(candidate))\n        sample_index2 = np.random.choice(len(candidate))\n        sample_1 = candidate[sample_index1]\n        sample_2 = candidate[sample_index2]\n        select_index = np.random.choice(len(sample_1))\n        logger.info(\n            LOGGING_PREFIX + \"Perform selection from %sth to %sth at index=%s\",\n            sample_index2, sample_index1, select_index)\n\n        next_gen = []\n        for i in range(len(sample_1)):\n            if i is select_index:\n                next_gen.append(sample_2[i])\n            else:\n                next_gen.append(sample_1[i])\n        return next_gen", "output": "Perform selection action to candidates.\n\n        For example, new gene = sample_1 + the 5th bit of sample2.\n\n        Args:\n            candidate: List of candidate genes (encodings).\n\n        Examples:\n            >>> # Genes that represent 3 parameters\n            >>> gene1 = np.array([[0, 0, 1], [0, 1], [1, 0]])\n            >>> gene2 = np.array([[0, 1, 0], [1, 0], [0, 1]])\n            >>> new_gene = _selection([gene1, gene2])\n            >>> # new_gene could be gene1 overwritten with the\n            >>> # 2nd parameter of gene2\n            >>> # in which case:\n            >>> #   new_gene[0] = gene1[0]\n            >>> #   new_gene[1] = gene2[1]\n            >>> #   new_gene[2] = gene1[0]\n\n        Returns:\n            New gene (encoding)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_files(suffix=\"\"):\n  \"\"\"\n  \n  \"\"\"\n\n  cleverhans_path = os.path.abspath(cleverhans.__path__[0])\n  # In some environments cleverhans_path does not point to a real directory.\n  # In such case return empty list.\n  if not os.path.isdir(cleverhans_path):\n    return []\n  repo_path = os.path.abspath(os.path.join(cleverhans_path, os.pardir))\n  file_list = _list_files(cleverhans_path, suffix)\n\n  extra_dirs = ['cleverhans_tutorials', 'examples', 'scripts', 'tests_tf', 'tests_pytorch']\n\n  for extra_dir in extra_dirs:\n    extra_path = os.path.join(repo_path, extra_dir)\n    if os.path.isdir(extra_path):\n      extra_files = _list_files(extra_path, suffix)\n      extra_files = [os.path.join(os.pardir, path) for path in extra_files]\n      file_list = file_list + extra_files\n\n  return file_list", "output": "Returns a list of all files in CleverHans with the given suffix.\n\n  Parameters\n  ----------\n  suffix : str\n\n  Returns\n  -------\n\n  file_list : list\n      A list of all files in CleverHans whose filepath ends with `suffix`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_headers(self) -> None:\n        \"\"\"\n        \"\"\"\n        self.set_header(\"Accept-Ranges\", \"bytes\")\n        self.set_etag_header()\n\n        if self.modified is not None:\n            self.set_header(\"Last-Modified\", self.modified)\n\n        content_type = self.get_content_type()\n        if content_type:\n            self.set_header(\"Content-Type\", content_type)\n\n        cache_time = self.get_cache_time(self.path, self.modified, content_type)\n        if cache_time > 0:\n            self.set_header(\n                \"Expires\",\n                datetime.datetime.utcnow() + datetime.timedelta(seconds=cache_time),\n            )\n            self.set_header(\"Cache-Control\", \"max-age=\" + str(cache_time))\n\n        self.set_extra_headers(self.path)", "output": "Sets the content and caching headers on the response.\n\n        .. versionadded:: 3.1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def probe(self, params, lazy=True):\n        \"\"\"\"\"\"\n        if lazy:\n            self._queue.add(params)\n        else:\n            self._space.probe(params)\n            self.dispatch(Events.OPTMIZATION_STEP)", "output": "Probe target of x", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _simple_cmd(cmd, app, url='http://localhost:8080/manager', timeout=180):\n    '''\n    \n    '''\n\n    try:\n        opts = {\n            'path': app,\n            'version': ls(url)[app]['version']\n        }\n        return '\\n'.join(_wget(cmd, opts, url, timeout=timeout)['msg'])\n    except Exception:\n        return 'FAIL - No context exists for path {0}'.format(app)", "output": "Simple command wrapper to commands that need only a path option", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def squeezenet1_0(num_classes=1000, pretrained='imagenet'):\n    \n    \"\"\"\n    model = models.squeezenet1_0(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['squeezenet1_0'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_squeezenets(model)\n    return model", "output": "r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n    accuracy with 50x fewer parameters and <0.5MB model size\"\n    <https://arxiv.org/abs/1602.07360>`_ paper.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def raw_role_mentions(self):\n        \"\"\"\n        \"\"\"\n        return [int(x) for x in re.findall(r'<@&([0-9]+)>', self.content)]", "output": "A property that returns an array of role IDs matched with\n        the syntax of <@&role_id> in the message content.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vm_netstats(vm_=None):\n    '''\n    \n    '''\n    with _get_xapi_session() as xapi:\n        def _info(vm_):\n            ret = {}\n            vm_rec = _get_record_by_label(xapi, 'VM', vm_)\n            if vm_rec is False:\n                return False\n            for vif in vm_rec['VIFs']:\n                vif_rec = _get_record(xapi, 'VIF', vif)\n                ret[vif_rec['device']] = _get_metrics_record(xapi, 'VIF',\n                                                             vif_rec)\n                del ret[vif_rec['device']]['last_updated']\n\n            return ret\n\n        info = {}\n        if vm_:\n            info[vm_] = _info(vm_)\n        else:\n            for vm_ in list_domains():\n                info[vm_] = _info(vm_)\n        return info", "output": "Return combined network counters used by the vms on this hyper in a\n    list of dicts:\n\n    .. code-block:: python\n\n        [\n            'your-vm': {\n                'io_read_kbs'           : 0,\n                'io_total_read_kbs'     : 0,\n                'io_total_write_kbs'    : 0,\n                'io_write_kbs'          : 0\n                },\n            ...\n            ]\n\n    If you pass a VM name in as an argument then it will return info\n    for just the named VM, otherwise it will return all VMs.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' virt.vm_netstats", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_spark_home():\n    \"\"\"\"\"\"\n    # If the environment has SPARK_HOME set trust it.\n    if \"SPARK_HOME\" in os.environ:\n        return os.environ[\"SPARK_HOME\"]\n\n    def is_spark_home(path):\n        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and\n                (os.path.isdir(os.path.join(path, \"jars\")) or\n                 os.path.isdir(os.path.join(path, \"assembly\"))))\n\n    paths = [\"../\", os.path.dirname(os.path.realpath(__file__))]\n\n    # Add the path of the PySpark module if it exists\n    if sys.version < \"3\":\n        import imp\n        try:\n            module_home = imp.find_module(\"pyspark\")[1]\n            paths.append(module_home)\n            # If we are installed in edit mode also look two dirs up\n            paths.append(os.path.join(module_home, \"../../\"))\n        except ImportError:\n            # Not pip installed no worries\n            pass\n    else:\n        from importlib.util import find_spec\n        try:\n            module_home = os.path.dirname(find_spec(\"pyspark\").origin)\n            paths.append(module_home)\n            # If we are installed in edit mode also look two dirs up\n            paths.append(os.path.join(module_home, \"../../\"))\n        except ImportError:\n            # Not pip installed no worries\n            pass\n\n    # Normalize the paths\n    paths = [os.path.abspath(p) for p in paths]\n\n    try:\n        return next(path for path in paths if is_spark_home(path))\n    except StopIteration:\n        print(\"Could not find valid SPARK_HOME while searching {0}\".format(paths), file=sys.stderr)\n        sys.exit(-1)", "output": "Find the SPARK_HOME.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def min_ver(ver1, ver2):\n    \"\"\"\n    \"\"\"\n    cmp_res = compare(ver1, ver2)\n    if cmp_res == 0 or cmp_res == -1:\n        return ver1\n    else:\n        return ver2", "output": "Returns the smaller version of two versions\n\n    :param ver1: version string 1\n    :param ver2: version string 2\n    :return: the smaller version of the two\n    :rtype: :class:`VersionInfo`\n\n    >>> import semver\n    >>> semver.min_ver(\"1.0.0\", \"2.0.0\")\n    '1.0.0'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _prepare_xml(options=None, state=None):\n    '''\n    \n    '''\n\n    if state:\n        _state = '0'\n    else:\n        _state = '2'\n\n    xml = \"<?xml version='1.0'?>\\n<checkresults>\\n\"\n\n    # No service defined then we set the status of the hostname\n    if 'service' in options and options['service'] != '':\n        xml += \"<checkresult type='service' checktype='\" + six.text_type(options['checktype'])+\"'>\"\n        xml += \"<hostname>\"+cgi.escape(options['hostname'], True)+\"</hostname>\"\n        xml += \"<servicename>\"+cgi.escape(options['service'], True)+\"</servicename>\"\n    else:\n        xml += \"<checkresult type='host' checktype='\" + six.text_type(options['checktype'])+\"'>\"\n        xml += \"<hostname>\"+cgi.escape(options['hostname'], True)+\"</hostname>\"\n\n    xml += \"<state>\"+_state+\"</state>\"\n\n    if 'output' in options:\n        xml += \"<output>\"+cgi.escape(options['output'], True)+\"</output>\"\n\n    xml += \"</checkresult>\"\n\n    xml += \"\\n</checkresults>\"\n\n    return xml", "output": "Get the requests options from salt.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_stock_basic_info_tushare(collections=DATABASE.stock_info_tushare):\n    '''\n    \n    '''\n    '\u83b7\u53d6\u80a1\u7968\u57fa\u672c\u4fe1\u606f'\n    items = [item for item in collections.find()]\n    # \ud83d\udee0todo  \u8f6c\u53d8\u6210 dataframe \u7c7b\u578b\u6570\u636e\n    return items", "output": "purpose:\n        tushare \u80a1\u7968\u5217\u8868\u6570\u636e\u5e93\n\n        code,\u4ee3\u7801\n        name,\u540d\u79f0\n        industry,\u6240\u5c5e\u884c\u4e1a\n        area,\u5730\u533a\n        pe,\u5e02\u76c8\u7387\n        outstanding,\u6d41\u901a\u80a1\u672c(\u4ebf)\n        totals,\u603b\u80a1\u672c(\u4ebf)\n        totalAssets,\u603b\u8d44\u4ea7(\u4e07)\n        liquidAssets,\u6d41\u52a8\u8d44\u4ea7\n        fixedAssets,\u56fa\u5b9a\u8d44\u4ea7\n        reserved,\u516c\u79ef\u91d1\n        reservedPerShare,\u6bcf\u80a1\u516c\u79ef\u91d1\n        esp,\u6bcf\u80a1\u6536\u76ca\n        bvps,\u6bcf\u80a1\u51c0\u8d44\n        pb,\u5e02\u51c0\u7387\n        timeToMarket,\u4e0a\u5e02\u65e5\u671f\n        undp,\u672a\u5206\u5229\u6da6\n        perundp, \u6bcf\u80a1\u672a\u5206\u914d\n        rev,\u6536\u5165\u540c\u6bd4(%)\n        profit,\u5229\u6da6\u540c\u6bd4(%)\n        gpr,\u6bdb\u5229\u7387(%)\n        npr,\u51c0\u5229\u6da6\u7387(%)\n        holders,\u80a1\u4e1c\u4eba\u6570\n\n        add by tauruswang,\n\n    :param collections: stock_info_tushare \u96c6\u5408\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_all(jail=None):\n    '''\n    \n    '''\n    ret = []\n    service = _cmd(jail)\n    for srv in __salt__['cmd.run']('{0} -l'.format(service)).splitlines():\n        if not srv.isupper():\n            ret.append(srv)\n    return sorted(ret)", "output": "Return a list of all available services\n\n    .. versionchanged:: 2016.3.4\n\n    jail: optional jid or jail name\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.get_all", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getOrCreate(cls, checkpointPath, setupFunc):\n        \"\"\"\n        \n        \"\"\"\n        cls._ensure_initialized()\n        gw = SparkContext._gateway\n\n        # Check whether valid checkpoint information exists in the given path\n        ssc_option = gw.jvm.StreamingContextPythonHelper().tryRecoverFromCheckpoint(checkpointPath)\n        if ssc_option.isEmpty():\n            ssc = setupFunc()\n            ssc.checkpoint(checkpointPath)\n            return ssc\n\n        jssc = gw.jvm.JavaStreamingContext(ssc_option.get())\n\n        # If there is already an active instance of Python SparkContext use it, or create a new one\n        if not SparkContext._active_spark_context:\n            jsc = jssc.sparkContext()\n            conf = SparkConf(_jconf=jsc.getConf())\n            SparkContext(conf=conf, gateway=gw, jsc=jsc)\n\n        sc = SparkContext._active_spark_context\n\n        # update ctx in serializer\n        cls._transformerSerializer.ctx = sc\n        return StreamingContext(sc, None, jssc)", "output": "Either recreate a StreamingContext from checkpoint data or create a new StreamingContext.\n        If checkpoint data exists in the provided `checkpointPath`, then StreamingContext will be\n        recreated from the checkpoint data. If the data does not exist, then the provided setupFunc\n        will be used to create a new context.\n\n        @param checkpointPath: Checkpoint directory used in an earlier streaming program\n        @param setupFunc:      Function to create a new context and setup DStreams", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def confirm_updated(value, check_fun, normalize_ret=False, wait=5):\n    '''\n    \n    '''\n    for i in range(wait):\n        state = validate_enabled(check_fun()) if normalize_ret else check_fun()\n        if value in state:\n            return True\n        time.sleep(1)\n    return False", "output": "Wait up to ``wait`` seconds for a system parameter to be changed before\n    deciding it hasn't changed.\n\n    :param str value: The value indicating a successful change\n\n    :param function check_fun: The function whose return is compared with\n        ``value``\n\n    :param bool normalize_ret: Whether to normalize the return from\n        ``check_fun`` with ``validate_enabled``\n\n    :param int wait: The maximum amount of seconds to wait for a system\n        parameter to change", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vector_to_word(vector):\n    \"\"\"\n    \n    \"\"\"\n    word = \"\"\n    for vec in vector:\n        word = word + int2char(vec)\n    return word", "output": "Convert integer vectors to character vectors.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def headerData(self, section, orientation, role=Qt.DisplayRole):\n        \"\"\"\"\"\"\n        if role == Qt.TextAlignmentRole:\n            if orientation == Qt.Horizontal:\n                return to_qvariant(int(Qt.AlignHCenter | Qt.AlignVCenter))\n            return to_qvariant(int(Qt.AlignRight | Qt.AlignVCenter))\n        if role != Qt.DisplayRole:\n            return to_qvariant()\n        if orientation == Qt.Horizontal:\n            if section == LANGUAGE:\n                return to_qvariant(_(\"Language\"))\n            elif section == ADDR:\n                return to_qvariant(_(\"Address\"))\n            elif section == CMD:\n                return to_qvariant(_(\"Command to execute\"))\n        return to_qvariant()", "output": "Qt Override.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setExecutorEnv(self, key=None, value=None, pairs=None):\n        \"\"\"\"\"\"\n        if (key is not None and pairs is not None) or (key is None and pairs is None):\n            raise Exception(\"Either pass one key-value pair or a list of pairs\")\n        elif key is not None:\n            self.set(\"spark.executorEnv.\" + key, value)\n        elif pairs is not None:\n            for (k, v) in pairs:\n                self.set(\"spark.executorEnv.\" + k, v)\n        return self", "output": "Set an environment variable to be passed to executors.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def random_uniform(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    try:\n        from onnx.mapping import TENSOR_TYPE_TO_NP_TYPE\n    except ImportError:\n        raise ImportError(\"Onnx and protobuf need to be installed. \"\n                          \"Instructions to install - https://github.com/onnx/onnx\")\n    new_attrs = translation_utils._remove_attributes(attrs, ['seed'])\n    new_attrs['dtype'] = TENSOR_TYPE_TO_NP_TYPE[int(new_attrs.get('dtype', 1))]\n    return 'random_uniform', new_attrs, inputs", "output": "Draw random samples from a uniform distribtuion.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _api_model_patch_replace(conn, restApiId, modelName, path, value):\n    '''\n    \n    '''\n    response = conn.update_model(restApiId=restApiId, modelName=modelName,\n                                 patchOperations=[{'op': 'replace', 'path': path, 'value': value}])\n    return response", "output": "the replace patch operation on a Model resource", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_read_exception(exc, path, ignore=None):\n    '''\n    \n    '''\n    if ignore is not None:\n        if isinstance(ignore, six.integer_types):\n            ignore = (ignore,)\n    else:\n        ignore = ()\n\n    if exc.errno in ignore:\n        return\n\n    if exc.errno == errno.ENOENT:\n        raise CommandExecutionError('{0} does not exist'.format(path))\n    elif exc.errno == errno.EACCES:\n        raise CommandExecutionError(\n            'Permission denied reading from {0}'.format(path)\n        )\n    else:\n        raise CommandExecutionError(\n            'Error {0} encountered reading from {1}: {2}'.format(\n                exc.errno, path, exc.strerror\n            )\n        )", "output": "Common code for raising exceptions when reading a file fails\n\n    The ignore argument can be an iterable of integer error codes (or a single\n    integer error code) that should be ignored.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def channel(self):\n        \"\"\"\n        \"\"\"\n        guild = self.guild\n        return guild and guild.get_channel(self.channel_id)", "output": "Optional[:class:`TextChannel`]: The text channel this webhook belongs to.\n\n        If this is a partial webhook, then this will always return ``None``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _AddByteSizeMethod(message_descriptor, cls):\n  \"\"\"\"\"\"\n\n  def ByteSize(self):\n    if not self._cached_byte_size_dirty:\n      return self._cached_byte_size\n\n    size = 0\n    for field_descriptor, field_value in self.ListFields():\n      size += field_descriptor._sizer(field_value)\n\n    for tag_bytes, value_bytes in self._unknown_fields:\n      size += len(tag_bytes) + len(value_bytes)\n\n    self._cached_byte_size = size\n    self._cached_byte_size_dirty = False\n    self._listener_for_children.dirty = False\n    return size\n\n  cls.ByteSize = ByteSize", "output": "Helper for _AddMessageMethods().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _item_to_bucket(iterator, item):\n    \"\"\"\n    \"\"\"\n    name = item.get(\"name\")\n    bucket = Bucket(iterator.client, name)\n    bucket._set_properties(item)\n    return bucket", "output": "Convert a JSON bucket to the native object.\n\n    :type iterator: :class:`~google.api_core.page_iterator.Iterator`\n    :param iterator: The iterator that has retrieved the item.\n\n    :type item: dict\n    :param item: An item to be converted to a bucket.\n\n    :rtype: :class:`.Bucket`\n    :returns: The next bucket in the page.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keyPressEvent(self, event):\n        \"\"\"\"\"\"\n        key = event.key()\n        if key in [Qt.Key_Enter, Qt.Key_Return]:\n            self.show_editor()\n        elif key in [Qt.Key_Backtab]:\n            self.parent().reset_btn.setFocus()\n        elif key in [Qt.Key_Up, Qt.Key_Down, Qt.Key_Left, Qt.Key_Right]:\n            super(LSPServerTable, self).keyPressEvent(event)\n        elif key in [Qt.Key_Escape]:\n            self.finder.keyPressEvent(event)\n        else:\n            super(LSPServerTable, self).keyPressEvent(event)", "output": "Qt Override.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_learning_rate(self, lr):\n        \"\"\"\n        \"\"\"\n        if not isinstance(self._optimizer, opt.Optimizer):\n            raise UserWarning(\"Optimizer has to be defined before its learning \"\n                              \"rate is mutated.\")\n        else:\n            self._optimizer.set_learning_rate(lr)", "output": "Sets a new learning rate of the optimizer.\n\n        Parameters\n        ----------\n        lr : float\n            The new learning rate of the optimizer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_attribute(self, key, value):\n        \"\"\"\n        \"\"\"\n        if not isinstance(key, str) or not isinstance(value, str):\n            raise ValueError(\"The arguments 'key' and 'value' must both be \"\n                             \"strings. Instead they are {} and {}.\".format(\n                                 key, value))\n        self.extra_data[key] = value", "output": "Add a key-value pair to the extra_data dict.\n\n        This can be used to add attributes that are not available when\n        ray.profile was called.\n\n        Args:\n            key: The attribute name.\n            value: The attribute value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_reg(cls):\n        \"\"\"\n        \"\"\"\n        # We have to do this on-demand in case task names have changed later\n        reg = dict()\n        for task_cls in cls._reg:\n            if not task_cls._visible_in_registry:\n                continue\n\n            name = task_cls.get_task_family()\n            if name in reg and \\\n                    (reg[name] == Register.AMBIGUOUS_CLASS or  # Check so issubclass doesn't crash\n                     not issubclass(task_cls, reg[name])):\n                # Registering two different classes - this means we can't instantiate them by name\n                # The only exception is if one class is a subclass of the other. In that case, we\n                # instantiate the most-derived class (this fixes some issues with decorator wrappers).\n                reg[name] = Register.AMBIGUOUS_CLASS\n            else:\n                reg[name] = task_cls\n\n        return reg", "output": "Return all of the registered classes.\n\n        :return:  an ``dict`` of task_family -> class", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def top_prior(name, z_shape, learn_prior=\"normal\", temperature=1.0):\n  \"\"\"\n  \"\"\"\n  with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n    h = tf.zeros(z_shape, dtype=tf.float32)\n    if learn_prior == \"normal\":\n      prior_dist = tfp.distributions.Normal(h, tf.exp(h))\n    elif learn_prior == \"single_conv\":\n      prior_dist = single_conv_dist(\"top_learn_prior\", h)\n    else:\n      raise ValueError(\"Expected learn_prior to be normal or single_conv \"\n                       \"got %s\" % learn_prior)\n    return TemperedNormal(prior_dist.loc, prior_dist.scale, temperature)", "output": "Unconditional prior distribution.\n\n  Args:\n    name: variable scope\n    z_shape: Shape of the mean / scale of the prior distribution.\n    learn_prior: Possible options are \"normal\" and \"single_conv\".\n                 If set to \"single_conv\", the gaussian is parametrized by a\n                 single convolutional layer whose input are an array of zeros\n                 and initialized such that the mean and std are zero and one.\n                 If set to \"normal\", the prior is just a Gaussian with zero\n                 mean and unit variance.\n    temperature: Temperature with which to sample from the Gaussian.\n  Returns:\n    objective: 1-D Tensor shape=(batch_size,) summed across spatial components.\n  Raises:\n    ValueError: If learn_prior not in \"normal\" or \"single_conv\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _restart_on_unavailable(restart):\n    \"\"\"\n    \"\"\"\n    resume_token = b\"\"\n    item_buffer = []\n    iterator = restart()\n    while True:\n        try:\n            for item in iterator:\n                item_buffer.append(item)\n                if item.resume_token:\n                    resume_token = item.resume_token\n                    break\n        except ServiceUnavailable:\n            del item_buffer[:]\n            iterator = restart(resume_token=resume_token)\n            continue\n\n        if len(item_buffer) == 0:\n            break\n\n        for item in item_buffer:\n            yield item\n\n        del item_buffer[:]", "output": "Restart iteration after :exc:`.ServiceUnavailable`.\n\n    :type restart: callable\n    :param restart: curried function returning iterator", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_df(cls, df:DataFrame, path:PathOrStr='.', cols:IntsOrStrs=0, processor:PreProcessors=None, **kwargs)->'ItemList':\n        \"\"\n        inputs = df.iloc[:,df_names_to_idx(cols, df)]\n        assert inputs.isna().sum().sum() == 0, f\"You have NaN values in column(s) {cols} of your dataframe, please fix it.\"\n        res = cls(items=_maybe_squeeze(inputs.values), path=path, inner_df=df, processor=processor, **kwargs)\n        return res", "output": "Create an `ItemList` in `path` from the inputs in the `cols` of `df`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge(\n    left,\n    right,\n    how=\"inner\",\n    on=None,\n    left_on=None,\n    right_on=None,\n    left_index=False,\n    right_index=False,\n    sort=False,\n    suffixes=(\"_x\", \"_y\"),\n    copy=True,\n    indicator=False,\n    validate=None,\n):\n    \"\"\"\n        \"\"\"\n    if not isinstance(left, DataFrame):\n        raise ValueError(\n            \"can not merge DataFrame with instance of type {}\".format(type(right))\n        )\n\n    return left.merge(\n        right,\n        how=how,\n        on=on,\n        left_on=left_on,\n        right_on=right_on,\n        left_index=left_index,\n        right_index=right_index,\n        sort=sort,\n        suffixes=suffixes,\n        copy=copy,\n        indicator=indicator,\n        validate=validate,\n    )", "output": "Database style join, where common columns in \"on\" are merged.\n\n    Args:\n        left: DataFrame.\n        right: DataFrame.\n        how: What type of join to use.\n        on: The common column name(s) to join on. If None, and left_on and\n            right_on  are also None, will default to all commonly named\n            columns.\n        left_on: The column(s) on the left to use for the join.\n        right_on: The column(s) on the right to use for the join.\n        left_index: Use the index from the left as the join keys.\n        right_index: Use the index from the right as the join keys.\n        sort: Sort the join keys lexicographically in the result.\n        suffixes: Add this suffix to the common names not in the \"on\".\n        copy: Does nothing in our implementation\n        indicator: Adds a column named _merge to the DataFrame with\n            metadata from the merge about each row.\n        validate: Checks if merge is a specific type.\n\n    Returns:\n         A merged Dataframe", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def highlight(self, message, *values, **colors):\n        '''\n        \n        '''\n\n        m_color = colors.get('_main', self._default_color)\n        h_color = colors.get('_highlight', self._default_hl_color)\n\n        _values = []\n        for value in values:\n            _values.append('{p}{c}{r}'.format(p=self._colors[colors.get(value, h_color)],\n                                              c=value, r=self._colors[m_color]))\n        self._device.write('{s}{m}{e}'.format(s=self._colors[m_color],\n                                              m=message.format(*_values), e=self._colors['ENDC']))\n        self._device.write(os.linesep)\n        self._device.flush()", "output": "Highlighter works the way that message parameter is a template,\n        the \"values\" is a list of arguments going one after another as values there.\n        And so the \"colors\" should designate either highlight color or alternate for each.\n\n        Example:\n\n           highlight('Hello {}, there! It is {}.', 'user', 'daytime', _main='GREEN', _highlight='RED')\n           highlight('Hello {}, there! It is {}.', 'user', 'daytime', _main='GREEN', _highlight='RED', 'daytime'='YELLOW')\n\n        First example will highlight all the values in the template with the red color.\n        Second example will highlight the second value with the yellow color.\n\n        Usage:\n\n            colors:\n              _main: Sets the main color (or default is used)\n              _highlight: Sets the alternative color for everything\n              'any phrase' that is the same in the \"values\" can override color.\n\n        :param message:\n        :param formatted:\n        :param colors:\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expected_gradients(model, data):\n    \"\"\" \n    \"\"\"\n    if isinstance(model, KerasWrap):\n        model = model.model\n    explainer = GradientExplainer(model, data)\n    def f(X):\n        phi = explainer.shap_values(X)\n        if type(phi) is list and len(phi) == 1:\n            return phi[0]\n        else:\n            return phi\n    \n    return f", "output": "Expected Gradients", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_backup(path, backup_id):\n    '''\n    \n    '''\n    path = os.path.expanduser(path)\n\n    ret = {'result': False,\n           'comment': 'Invalid backup_id \\'{0}\\''.format(backup_id)}\n    try:\n        if len(six.text_type(backup_id)) == len(six.text_type(int(backup_id))):\n            backup = list_backups(path)[int(backup_id)]\n        else:\n            return ret\n    except ValueError:\n        return ret\n    except KeyError:\n        ret['comment'] = 'backup_id \\'{0}\\' does not exist for ' \\\n                         '{1}'.format(backup_id, path)\n        return ret\n\n    try:\n        os.remove(backup['Location'])\n    except IOError as exc:\n        ret['comment'] = 'Unable to remove {0}: {1}'.format(backup['Location'],\n                                                            exc)\n    else:\n        ret['result'] = True\n        ret['comment'] = 'Successfully removed {0}'.format(backup['Location'])\n\n    return ret", "output": ".. versionadded:: 0.17.0\n\n    Delete a previous version of a file that was backed up using Salt's\n    :ref:`file state backup <file-state-backups>` system.\n\n    path\n        The path on the minion to check for backups\n    backup_id\n        The numeric id for the backup you wish to delete, as found using\n        :mod:`file.list_backups <salt.modules.file.list_backups>`\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.delete_backup /var/cache/salt/minion/file_backup/home/foo/bar/baz.txt 0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validatePopElement(self, ctxt, elem, qname):\n        \"\"\" \"\"\"\n        if ctxt is None: ctxt__o = None\n        else: ctxt__o = ctxt._o\n        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        ret = libxml2mod.xmlValidatePopElement(ctxt__o, self._o, elem__o, qname)\n        return ret", "output": "Pop the element end from the validation stack.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_instrinsic(input):\n    \"\"\"\n    \n    \"\"\"\n\n    if input is not None \\\n            and isinstance(input, dict) \\\n            and len(input) == 1:\n\n        key = list(input.keys())[0]\n        return key == \"Ref\" or key == \"Condition\" or key.startswith(\"Fn::\")\n\n    return False", "output": "Checks if the given input is an intrinsic function dictionary. Intrinsic function is a dictionary with single\n    key that is the name of the intrinsics.\n\n    :param input: Input value to check if it is an intrinsic\n    :return: True, if yes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check(jail=None,\n          chroot=None,\n          root=None,\n          depends=False,\n          recompute=False,\n          checksum=False):\n    '''\n    \n    '''\n    if not any((depends, recompute, checksum)):\n        return 'One of depends, recompute, or checksum must be set to True'\n\n    opts = ''\n    if depends:\n        opts += 'dy'\n    if recompute:\n        opts += 'r'\n    if checksum:\n        opts += 's'\n\n    cmd = _pkg(jail, chroot, root)\n    cmd.append('check')\n    if opts:\n        cmd.append('-' + opts)\n    return __salt__['cmd.run'](\n        cmd,\n        output_loglevel='trace',\n        python_shell=False\n    )", "output": "Sanity checks installed packages\n\n    jail\n        Perform the sanity check in the specified jail\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.check jail=<jail name or id>\n\n    chroot\n        Perform the sanity check in the specified chroot (ignored if ``jail``\n        is specified)\n\n    root\n        Perform the sanity check in the specified root (ignored if ``jail``\n        is specified)\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.check chroot=/path/to/chroot\n\n\n    Of the below, at least one must be set to ``True``.\n\n    depends\n        Check for and install missing dependencies.\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.check recompute=True\n\n    recompute\n        Recompute sizes and checksums of installed packages.\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.check depends=True\n\n    checksum\n        Find invalid checksums for installed packages.\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.check checksum=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def method(*args, **kwargs):\n    \"\"\"\n    \"\"\"\n    assert len(args) == 0\n    assert len(kwargs) == 1\n    assert \"num_return_vals\" in kwargs\n    num_return_vals = kwargs[\"num_return_vals\"]\n\n    def annotate_method(method):\n        method.__ray_num_return_vals__ = num_return_vals\n        return method\n\n    return annotate_method", "output": "Annotate an actor method.\n\n    .. code-block:: python\n\n        @ray.remote\n        class Foo(object):\n            @ray.method(num_return_vals=2)\n            def bar(self):\n                return 1, 2\n\n        f = Foo.remote()\n\n        _, _ = f.bar.remote()\n\n    Args:\n        num_return_vals: The number of object IDs that should be returned by\n            invocations of this actor method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_url():\n    '''\n    \n    '''\n    protocol = __salt__['config.option']('kapacitor.protocol', 'http')\n    host = __salt__['config.option']('kapacitor.host', 'localhost')\n    port = __salt__['config.option']('kapacitor.port', 9092)\n\n    return '{0}://{1}:{2}'.format(protocol, host, port)", "output": "Get the kapacitor URL.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def commit(name, **kwargs):\n    '''\n    \n    '''\n    ret = {'name': name, 'changes': {}, 'result': True, 'comment': ''}\n    ret['changes'] = __salt__['junos.commit'](**kwargs)\n    return ret", "output": "Commits the changes loaded into the candidate configuration.\n\n    .. code-block:: yaml\n\n            commit the changes:\n              junos:\n                - commit\n                - confirm: 10\n\n\n    Parameters:\n      Optional\n        * kwargs: Keyworded arguments which can be provided like-\n            * timeout:\n              Set NETCONF RPC timeout. Can be used for commands which take a \\\n              while to execute. (default = 30 seconds)\n            * comment:\n              Provide a comment to the commit. (default = None)\n            * confirm:\n              Provide time in minutes for commit confirmation. If this option \\\n              is specified, the commit will be rollbacked in the given time \\\n              unless the commit is confirmed.\n            * sync:\n              On dual control plane systems, requests that the candidate\\\n              configuration on one control plane be copied to the other \\\n              control plane,checked for correct syntax, and committed on \\\n              both Routing Engines. (default = False)\n            * force_sync:\n              On dual control plane systems, force the candidate configuration\n              on one control plane to be copied to the other control plane.\n            * full:\n              When set to True requires all the daemons to check and evaluate \\\n              the new configuration.\n            * detail:\n              When true return commit detail.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def batch_face_locations(images, number_of_times_to_upsample=1, batch_size=128):\n    \"\"\"\n    \n    \"\"\"\n    def convert_cnn_detections_to_css(detections):\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), images[0].shape) for face in detections]\n\n    raw_detections_batched = _raw_face_locations_batched(images, number_of_times_to_upsample, batch_size)\n\n    return list(map(convert_cnn_detections_to_css, raw_detections_batched))", "output": "Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector\n    If you are using a GPU, this can give you much faster results since the GPU\n    can process batches of images at once. If you aren't using a GPU, you don't need this function.\n\n    :param img: A list of images (each as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param batch_size: How many images to include in each GPU processing batch.\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_content(\n        cls, abspath: str, start: int = None, end: int = None\n    ) -> Generator[bytes, None, None]:\n        \"\"\"\n        \"\"\"\n        with open(abspath, \"rb\") as file:\n            if start is not None:\n                file.seek(start)\n            if end is not None:\n                remaining = end - (start or 0)  # type: Optional[int]\n            else:\n                remaining = None\n            while True:\n                chunk_size = 64 * 1024\n                if remaining is not None and remaining < chunk_size:\n                    chunk_size = remaining\n                chunk = file.read(chunk_size)\n                if chunk:\n                    if remaining is not None:\n                        remaining -= len(chunk)\n                    yield chunk\n                else:\n                    if remaining is not None:\n                        assert remaining == 0\n                    return", "output": "Retrieve the content of the requested resource which is located\n        at the given absolute path.\n\n        This class method may be overridden by subclasses.  Note that its\n        signature is different from other overridable class methods\n        (no ``settings`` argument); this is deliberate to ensure that\n        ``abspath`` is able to stand on its own as a cache key.\n\n        This method should either return a byte string or an iterator\n        of byte strings.  The latter is preferred for large files\n        as it helps reduce memory fragmentation.\n\n        .. versionadded:: 3.1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ssh(self, *args, **kwargs):\n        '''\n        \n        '''\n        ssh_client = salt.client.ssh.client.SSHClient(mopts=self.opts,\n                                                      disable_custom_roster=True)\n        return ssh_client.cmd_sync(kwargs)", "output": "Run salt-ssh commands synchronously\n\n        Wraps :py:meth:`salt.client.ssh.client.SSHClient.cmd_sync`.\n\n        :return: Returns the result from the salt-ssh command", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _string_to_record_type(string):\n    '''\n    \n    '''\n    string = string.upper()\n    record_type = getattr(RecordType, string)\n    return record_type", "output": "Return a string representation of a DNS record type to a\n    libcloud RecordType ENUM.\n\n    :param string: A record type, e.g. A, TXT, NS\n    :type  string: ``str``\n\n    :rtype: :class:`RecordType`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_dns_info():\n    '''\n    \n    '''\n    dns_list = []\n    try:\n        with salt.utils.files.fopen('/etc/resolv.conf', 'r+') as dns_info:\n            lines = dns_info.readlines()\n            for line in lines:\n                if 'nameserver' in line:\n                    dns = line.split()[1].strip()\n                    if dns not in dns_list:\n                        dns_list.append(dns)\n    except IOError:\n        log.warning('Could not get domain\\n')\n    return dns_list", "output": "return dns list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(event, reactors, saltenv='base', test=None):\n    '''\n    \n    '''\n    if isinstance(reactors, string_types):\n        reactors = [reactors]\n\n    sevent = salt.utils.event.get_event(\n            'master',\n            __opts__['sock_dir'],\n            __opts__['transport'],\n            opts=__opts__,\n            listen=True)\n\n    master_key = salt.utils.master.get_master_key('root', __opts__)\n\n    __jid_event__.fire_event({'event': event,\n                              'reactors': reactors,\n                              'key': master_key},\n                             'salt/reactors/manage/add')\n\n    res = sevent.get_event(wait=30, tag='salt/reactors/manage/add-complete')\n    return res['result']", "output": "Add a new reactor\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run reactor.add 'salt/cloud/*/destroyed' reactors='/srv/reactor/destroy/*.sls'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getfo(self, remotepath, fl, callback=None):\n        \"\"\"\n        \n        \"\"\"\n        file_size = self.stat(remotepath).st_size\n        with self.open(remotepath, \"rb\") as fr:\n            fr.prefetch(file_size)\n            return self._transfer_with_callback(\n                reader=fr, writer=fl, file_size=file_size, callback=callback\n            )", "output": "Copy a remote file (``remotepath``) from the SFTP server and write to\n        an open file or file-like object, ``fl``.  Any exception raised by\n        operations will be passed through.  This method is primarily provided\n        as a convenience.\n\n        :param object remotepath: opened file or file-like object to copy to\n        :param str fl:\n            the destination path on the local host or open file object\n        :param callable callback:\n            optional callback function (form: ``func(int, int)``) that accepts\n            the bytes transferred so far and the total bytes to be transferred\n        :return: the `number <int>` of bytes written to the opened file object\n\n        .. versionadded:: 1.10", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_images(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_images function must be called with '\n            '-f or --function, or with the --list-images option'\n        )\n\n    ret = {}\n    conn = get_conn()\n\n    for item in conn.list_images()['items']:\n        image = {'id': item['id']}\n        image.update(item['properties'])\n        ret[image['name']] = image\n\n    return ret", "output": "Return a list of the images that are on the provider", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_from_datastore(self):\n    \"\"\"\"\"\"\n    self._data = {}\n    client = self._datastore_client\n    for entity in client.query_fetch(kind=KIND_CLASSIFICATION_BATCH):\n      class_batch_id = entity.key.flat_path[-1]\n      self.data[class_batch_id] = dict(entity)", "output": "Initializes data by reading it from the datastore.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _wait_threads(self):\n        \"\"\"\n        \n        \"\"\"\n        # Note that you need two loops, since you can't say which\n        # thread will get each sentinel\n        for t in self._threads:\n            self._to_fetch.put(None)    # sentinel\n        for t in self._threads:\n            t.join()\n        self._threads = []", "output": "Tell all the threads to terminate (by sending a sentinel value) and\n        wait for them to do so.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def raw_html(self) -> _RawHTML:\n        \"\"\"\n        \"\"\"\n        if self._html:\n            return self._html\n        else:\n            return etree.tostring(self.element, encoding='unicode').strip().encode(self.encoding)", "output": "Bytes representation of the HTML content.\n        (`learn more <http://www.diveintopython3.net/strings.html>`_).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_config_filename(args):\n    ''''''\n    experiment_id = check_experiment_id(args)\n    if experiment_id is None:\n        print_error('Please set the experiment id!')\n        exit(1)\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    return experiment_dict[experiment_id]['fileName']", "output": "get the file name of config file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _srvmgr(cmd, return_json=False):\n    '''\n    \n    '''\n    if isinstance(cmd, list):\n        cmd = ' '.join(cmd)\n\n    if return_json:\n        cmd = 'ConvertTo-Json -Compress -Depth 4 -InputObject @({0})' \\\n              ''.format(cmd)\n\n    cmd = 'Import-Module WebAdministration; {0}'.format(cmd)\n\n    ret = __salt__['cmd.run_all'](cmd, shell='powershell', python_shell=True)\n\n    if ret['retcode'] != 0:\n        msg = 'Unable to execute command: {0}\\nError: {1}' \\\n              ''.format(cmd, ret['stderr'])\n        log.error(msg)\n\n    return ret", "output": "Execute a powershell command from the WebAdministration PS module.\n\n    Args:\n        cmd (list): The command to execute in a list\n        return_json (bool): True formats the return in JSON, False just returns\n            the output of the command.\n\n    Returns:\n        str: The output from the command", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(self, client=None):\n        \"\"\"\n        \"\"\"\n        if self.notification_id is None:\n            raise ValueError(\"Notification not intialized by server\")\n\n        client = self._require_client(client)\n\n        query_params = {}\n        if self.bucket.user_project is not None:\n            query_params[\"userProject\"] = self.bucket.user_project\n\n        client._connection.api_request(\n            method=\"DELETE\", path=self.path, query_params=query_params\n        )", "output": "Delete this notification.\n\n        See:\n        https://cloud.google.com/storage/docs/json_api/v1/notifications/delete\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: Optional. The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :raises: :class:`google.api_core.exceptions.NotFound`:\n            if the notification does not exist.\n        :raises ValueError: if the notification has no ID.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def http_range(self) -> slice:\n        \"\"\"\n\n        \"\"\"\n        rng = self._headers.get(hdrs.RANGE)\n        start, end = None, None\n        if rng is not None:\n            try:\n                pattern = r'^bytes=(\\d*)-(\\d*)$'\n                start, end = re.findall(pattern, rng)[0]\n            except IndexError:  # pattern was not found in header\n                raise ValueError(\"range not in acceptable format\")\n\n            end = int(end) if end else None\n            start = int(start) if start else None\n\n            if start is None and end is not None:\n                # end with no start is to return tail of content\n                start = -end\n                end = None\n\n            if start is not None and end is not None:\n                # end is inclusive in range header, exclusive for slice\n                end += 1\n\n                if start >= end:\n                    raise ValueError('start cannot be after end')\n\n            if start is end is None:  # No valid range supplied\n                raise ValueError('No start or end of range specified')\n\n        return slice(start, end, 1)", "output": "The content of Range HTTP header.\n\n        Return a slice instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def phi4(gold_clustering, predicted_clustering):\n        \"\"\"\n        \n        \"\"\"\n        return 2 * len([mention for mention in gold_clustering if mention in predicted_clustering]) \\\n               / float(len(gold_clustering) + len(predicted_clustering))", "output": "Subroutine for ceafe. Computes the mention F measure between gold and\n        predicted mentions in a cluster.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def history_table_min(self):\n        ''\n        if len(self.history_min) > 0:\n            lens = len(self.history_min[0])\n        else:\n            lens = len(self._history_headers)\n\n        return pd.DataFrame(\n            data=self.history_min,\n            columns=self._history_headers[:lens]\n        ).sort_index()", "output": "\u533a\u95f4\u4ea4\u6613\u5386\u53f2\u7684table", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def open_file(filename, mode='r', encoding=None, errors='strict',\n              lazy=False, atomic=False):\n    \"\"\"\n    \"\"\"\n    if lazy:\n        return LazyFile(filename, mode, encoding, errors, atomic=atomic)\n    f, should_close = open_stream(filename, mode, encoding, errors,\n                                  atomic=atomic)\n    if not should_close:\n        f = KeepOpenFile(f)\n    return f", "output": "This is similar to how the :class:`File` works but for manual\n    usage.  Files are opened non lazy by default.  This can open regular\n    files as well as stdin/stdout if ``'-'`` is passed.\n\n    If stdin/stdout is returned the stream is wrapped so that the context\n    manager will not close the stream accidentally.  This makes it possible\n    to always use the function like this without having to worry to\n    accidentally close a standard stream::\n\n        with open_file(filename) as f:\n            ...\n\n    .. versionadded:: 3.0\n\n    :param filename: the name of the file to open (or ``'-'`` for stdin/stdout).\n    :param mode: the mode in which to open the file.\n    :param encoding: the encoding to use.\n    :param errors: the error handling for this file.\n    :param lazy: can be flipped to true to open the file lazily.\n    :param atomic: in atomic mode writes go into a temporary file and it's\n                   moved on close.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_port(zone, port, permanent=True):\n    '''\n    \n    '''\n    cmd = '--zone={0} --add-port={1}'.format(zone, port)\n\n    if permanent:\n        cmd += ' --permanent'\n\n    return __firewall_cmd(cmd)", "output": "Allow specific ports in a zone.\n\n    .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' firewalld.add_port internal 443/tcp", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _diff_cache_subnet_group(current, desired):\n    '''\n    \n    '''\n    modifiable = {\n        'CacheSubnetGroupDescription': 'CacheSubnetGroupDescription',\n        'SubnetIds': 'SubnetIds'\n    }\n\n    need_update = {}\n    for m, o in modifiable.items():\n        if m in desired:\n            if not o:\n                # Always pass these through - let AWS do the math...\n                need_update[m] = desired[m]\n            else:\n                if m in current:\n                    # Equivalence testing works fine for current simple type comparisons\n                    # This might need enhancement if more complex structures enter the picture\n                    if current[m] != desired[m]:\n                        need_update[m] = desired[m]\n    return need_update", "output": "If you need to enhance what modify_cache_subnet_group() considers when deciding what is to be\n    (or can be) updated, add it to 'modifiable' below.  It's a dict mapping the param as used\n    in modify_cache_subnet_group() to that in describe_cache_subnet_group().  Any data fiddlery that\n    needs to be done to make the mappings meaningful should be done in the munging section\n    below as well.\n\n    This function will ONLY touch settings that are explicitly called out in 'desired' - any\n    settings which might have previously been changed from their 'default' values will not be\n    changed back simply by leaving them out of 'desired'.  This is both intentional, and\n    much, much easier to code :)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dispatch(intent_request):\n    \"\"\"\n    \n    \"\"\"\n\n    logger.debug('dispatch userId={}, intentName={}'.format(intent_request['userId'], intent_request['currentIntent']['name']))\n\n    intent_name = intent_request['currentIntent']['name']\n\n    # Dispatch to your bot's intent handlers\n    if intent_name == 'MakeAppointment':\n        return make_appointment(intent_request)\n    raise Exception('Intent with name ' + intent_name + ' not supported')", "output": "Called when the user specifies an intent for this bot.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _safe_sparse_mask(tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    \n    \"\"\"\n    # pylint: disable=protected-access\n    try:\n        return tensor.sparse_mask(mask)\n    except AttributeError:\n        # TODO(joelgrus): remove this and/or warn at some point\n        return tensor._sparse_mask(mask)", "output": "In PyTorch 1.0, Tensor._sparse_mask was changed to Tensor.sparse_mask.\n    This wrapper allows AllenNLP to (temporarily) work with both 1.0 and 0.4.1.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_on_cooldown(self, ctx):\n        \"\"\"\n        \"\"\"\n        if not self._buckets.valid:\n            return False\n\n        bucket = self._buckets.get_bucket(ctx.message)\n        return bucket.get_tokens() == 0", "output": "Checks whether the command is currently on cooldown.\n\n        Parameters\n        -----------\n        ctx: :class:`.Context.`\n            The invocation context to use when checking the commands cooldown status.\n\n        Returns\n        --------\n        :class:`bool`\n            A boolean indicating if the command is on cooldown.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def profile(profile_name):\n    '''\n    \n    '''\n\n    # run tuned-adm with the profile specified\n    result = __salt__['cmd.retcode']('tuned-adm profile {0}'.format(profile_name))\n    if int(result) != 0:\n        return False\n    return '{0}'.format(profile_name)", "output": "Activate specified profile\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' tuned.profile virtual-guest", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def combine_initial_dims(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    \n    \"\"\"\n    if tensor.dim() <= 2:\n        return tensor\n    else:\n        return tensor.view(-1, tensor.size(-1))", "output": "Given a (possibly higher order) tensor of ids with shape\n    (d1, ..., dn, sequence_length)\n    Return a view that's (d1 * ... * dn, sequence_length).\n    If original tensor is 1-d or 2-d, return it as is.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_static_info(interface):\n    '''\n    \n    '''\n    data = {\n        'connectionid': interface.name,\n        'label': interface.name,\n        'hwaddr': interface.hwaddr[:-1],\n        'up': False,\n        'ipv4': {\n            'supportedrequestmodes': ['static', 'dhcp_linklocal', 'disabled'],\n            'requestmode': 'static'\n        },\n        'wireless': False\n    }\n    hwaddr_section_number = ''.join(data['hwaddr'].split(':'))\n    if os.path.exists(INTERFACES_CONFIG):\n        information = _load_config(hwaddr_section_number, ['IPv4', 'Nameservers'], filename=INTERFACES_CONFIG)\n        if information['IPv4'] != '':\n            ipv4_information = information['IPv4'].split('/')\n            data['ipv4']['address'] = ipv4_information[0]\n            data['ipv4']['dns'] = information['Nameservers'].split(',')\n            data['ipv4']['netmask'] = ipv4_information[1]\n            data['ipv4']['gateway'] = ipv4_information[2]\n    return data", "output": "Return information about an interface from config file.\n\n    :param interface: interface label", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flush_finished_tasks_unsafe():\n    \"\"\"\n    \"\"\"\n    ray.worker.global_worker.check_connected()\n\n    for shard_index in range(len(ray.global_state.redis_clients)):\n        _flush_finished_tasks_unsafe_shard(shard_index)", "output": "This removes some critical state from the Redis shards.\n\n    In a multitenant environment, this will flush metadata for all jobs, which\n    may be undesirable.\n\n    This removes all of the metadata for finished tasks. This can be used to\n    try to address out-of-memory errors caused by the accumulation of metadata\n    in Redis. However, after running this command, fault tolerance will most\n    likely not work.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_clonespec(config_spec, object_ref, reloc_spec, template):\n    '''\n    \n    '''\n    if reloc_spec.diskMoveType == QUICK_LINKED_CLONE:\n        return vim.vm.CloneSpec(\n            template=template,\n            location=reloc_spec,\n            config=config_spec,\n            snapshot=object_ref.snapshot.currentSnapshot\n        )\n\n    return vim.vm.CloneSpec(\n        template=template,\n        location=reloc_spec,\n        config=config_spec\n    )", "output": "Returns the clone spec", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def variables(self):\n    \"\"\"\"\"\"\n    result = []\n    for _, value in sorted(self.variable_map.items()):\n      if isinstance(value, list):\n        result.extend(value)\n      else:\n        result.append(value)\n    return result", "output": "Returns the list of all tf.Variables created by module instantiation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_account_policy_data_value(name, key):\n    '''\n    \n    '''\n    cmd = 'dscl . -readpl /Users/{0} accountPolicyData {1}'.format(name, key)\n    try:\n        ret = salt.utils.mac_utils.execute_return_result(cmd)\n    except CommandExecutionError as exc:\n        if 'eDSUnknownNodeName' in exc.strerror:\n            raise CommandExecutionError('User not found: {0}'.format(name))\n        raise CommandExecutionError('Unknown error: {0}'.format(exc.strerror))\n\n    return ret", "output": "Return the value for a key in the accountPolicy section of the user's plist\n    file. For use by this module only\n\n    :param str name: The username\n    :param str key: The accountPolicy key\n\n    :return: The value contained within the key\n    :rtype: str\n\n    :raises: CommandExecutionError on user not found or any other unknown error", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _table_arg_to_table(value, default_project=None):\n    \"\"\"\n    \"\"\"\n    if isinstance(value, six.string_types):\n        value = TableReference.from_string(value, default_project=default_project)\n    if isinstance(value, TableReference):\n        value = Table(value)\n    if isinstance(value, TableListItem):\n        newvalue = Table(value.reference)\n        newvalue._properties = value._properties\n        value = newvalue\n\n    return value", "output": "Helper to convert a string or TableReference to a Table.\n\n    This function keeps Table and other kinds of objects unchanged.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show(self, obj=None, browser=None, new=\"tab\"):\n        ''' \n\n        '''\n        if obj and obj not in self.document.roots:\n            self.document.add_root(obj)\n        show_session(session=self, browser=browser, new=new)", "output": "Open a browser displaying this session.\n\n        Args:\n            obj (LayoutDOM object, optional) : a Layout (Row/Column),\n                Plot or Widget object to display. The object will be added\n                to the session's document.\n\n            browser (str, optional) : browser to show with (default: None)\n                For systems that support it, the **browser** argument allows\n                specifying which browser to display in, e.g. \"safari\", \"firefox\",\n                \"opera\", \"windows-default\" (see the ``webbrowser`` module\n                documentation in the standard lib for more details).\n\n            new (str, optional) : new file output mode (default: \"tab\")\n                For file-based output, opens or raises the browser window\n                showing the current output file.  If **new** is 'tab', then\n                opens a new tab. If **new** is 'window', then opens a new window.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filter_values(column, default=None):\n    \"\"\" \n    \"\"\"\n    form_data = json.loads(request.form.get('form_data', '{}'))\n    return_val = []\n    for filter_type in ['filters', 'extra_filters']:\n        if filter_type not in form_data:\n            continue\n\n        for f in form_data[filter_type]:\n            if f['col'] == column:\n                for v in f['val']:\n                    return_val.append(v)\n\n    if return_val:\n        return return_val\n\n    if default:\n        return [default]\n    else:\n        return []", "output": "Gets a values for a particular filter as a list\n\n    This is useful if:\n        - you want to use a filter box to filter a query where the name of filter box\n          column doesn't match the one in the select statement\n        - you want to have the ability for filter inside the main query for speed purposes\n\n    This searches for \"filters\" and \"extra_filters\" in form_data for a match\n\n    Usage example:\n        SELECT action, count(*) as times\n        FROM logs\n        WHERE action in ( {{ \"'\" + \"','\".join(filter_values('action_type')) + \"'\" }} )\n        GROUP BY 1\n\n    :param column: column/filter name to lookup\n    :type column: str\n    :param default: default value to return if there's no matching columns\n    :type default: str\n    :return: returns a list of filter values\n    :type: list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_categories(self, removals, inplace=False):\n        \"\"\"\n        \n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        if not is_list_like(removals):\n            removals = [removals]\n\n        removal_set = set(list(removals))\n        not_included = removal_set - set(self.dtype.categories)\n        new_categories = [c for c in self.dtype.categories\n                          if c not in removal_set]\n\n        # GH 10156\n        if any(isna(removals)):\n            not_included = [x for x in not_included if notna(x)]\n            new_categories = [x for x in new_categories if notna(x)]\n\n        if len(not_included) != 0:\n            msg = \"removals must all be in old categories: {not_included!s}\"\n            raise ValueError(msg.format(not_included=not_included))\n\n        return self.set_categories(new_categories, ordered=self.ordered,\n                                   rename=False, inplace=inplace)", "output": "Remove the specified categories.\n\n        `removals` must be included in the old categories. Values which were in\n        the removed categories will be set to NaN\n\n        Parameters\n        ----------\n        removals : category or list of categories\n           The categories which should be removed.\n        inplace : bool, default False\n           Whether or not to remove the categories inplace or return a copy of\n           this categorical with removed categories.\n\n        Returns\n        -------\n        cat : Categorical with removed categories or None if inplace.\n\n        Raises\n        ------\n        ValueError\n            If the removals are not contained in the categories\n\n        See Also\n        --------\n        rename_categories\n        reorder_categories\n        add_categories\n        remove_unused_categories\n        set_categories", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prlsrvctl(sub_cmd, args=None, runas=None):\n    '''\n    \n    '''\n    if not salt.utils.path.which('prlsrvctl'):\n        raise CommandExecutionError('prlsrvctl utility not available')\n\n    # Construct command\n    cmd = ['prlsrvctl', sub_cmd]\n    if args:\n        cmd.extend(_normalize_args(args))\n\n    # Execute command and return output\n    return __salt__['cmd.run'](cmd, runas=runas)", "output": "Execute a prlsrvctl command\n\n    .. versionadded:: 2016.11.0\n\n    :param str sub_cmd:\n        prlsrvctl subcommand to execute\n\n    :param str args:\n        The arguments supplied to ``prlsrvctl <sub_cmd>``\n\n    :param str runas:\n        The user that the prlsrvctl command will be run as\n\n    Example:\n\n    .. code-block:: bash\n\n        salt '*' parallels.prlsrvctl info runas=macdev\n        salt '*' parallels.prlsrvctl usb list runas=macdev\n        salt -- '*' parallels.prlsrvctl set '--mem-limit auto' runas=macdev", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def open(self, mode):\n        \"\"\"\n        \n        \"\"\"\n        if mode not in ('r', 'w'):\n            raise ValueError(\"Unsupported open mode '%s'\" % mode)\n        if mode == 'r':\n            return self.format.pipe_reader(ReadableAzureBlobFile(self.container, self.blob, self.client, self.download_when_reading, **self.azure_blob_options))\n        else:\n            return self.format.pipe_writer(AtomicAzureBlobFile(self.container, self.blob, self.client, **self.azure_blob_options))", "output": "Open the target for reading or writing\n\n        :param char mode:\n            'r' for reading and 'w' for writing.\n\n            'b' is not supported and will be stripped if used. For binary mode, use `format`\n        :return:\n            * :class:`.ReadableAzureBlobFile` if 'r'\n            * :class:`.AtomicAzureBlobFile` if 'w'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def scatter_plot(data, x, y, by=None, ax=None, figsize=None, grid=False,\n                 **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    kwargs.setdefault('edgecolors', 'none')\n\n    def plot_group(group, ax):\n        xvals = group[x].values\n        yvals = group[y].values\n        ax.scatter(xvals, yvals, **kwargs)\n        ax.grid(grid)\n\n    if by is not None:\n        fig = _grouped_plot(plot_group, data, by=by, figsize=figsize, ax=ax)\n    else:\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n        else:\n            fig = ax.get_figure()\n        plot_group(data, ax)\n        ax.set_ylabel(pprint_thing(y))\n        ax.set_xlabel(pprint_thing(x))\n\n        ax.grid(grid)\n\n    return fig", "output": "Make a scatter plot from two DataFrame columns\n\n    Parameters\n    ----------\n    data : DataFrame\n    x : Column name for the x-axis values\n    y : Column name for the y-axis values\n    ax : Matplotlib axis object\n    figsize : A tuple (width, height) in inches\n    grid : Setting this to True will show the grid\n    kwargs : other plotting keyword arguments\n        To be passed to scatter function\n\n    Returns\n    -------\n    matplotlib.Figure", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def not_equal(lhs, rhs):\n    \"\"\"\n    \"\"\"\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_not_equal,\n        lambda x, y: 1 if x != y else 0,\n        _internal._not_equal_scalar,\n        None)", "output": "Returns the result of element-wise **not equal to** (!=) comparison operation\n    with broadcasting.\n\n    For each element in input arrays, return 1(true) if corresponding elements are different,\n    otherwise return 0(false).\n\n    Equivalent to ``lhs != rhs`` and ``mx.nd.broadcast_not_equal(lhs, rhs)``.\n\n    .. note::\n\n       If the corresponding dimensions of two arrays have the same size or one of them has size 1,\n       then the arrays are broadcastable to a common shape.\n\n    Parameters\n    ----------\n    lhs : scalar or mxnet.ndarray.array\n        First array to be compared.\n    rhs : scalar or mxnet.ndarray.array\n         Second array to be compared. If ``lhs.shape != rhs.shape``, they must be\n        broadcastable to a common shape.\n\n    Returns\n    -------\n    NDArray\n        Output array of boolean values.\n\n    Examples\n    --------\n    >>> x = mx.nd.ones((2,3))\n    >>> y = mx.nd.arange(2).reshape((2,1))\n    >>> z = mx.nd.arange(2).reshape((1,2))\n    >>> x.asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> y.asnumpy()\n    array([[ 0.],\n           [ 1.]], dtype=float32)\n    >>> z.asnumpy()\n    array([[ 0.,  1.]], dtype=float32)\n    >>> (z == y).asnumpy()\n    array([[ 1.,  0.],\n           [ 0.,  1.]], dtype=float32)\n    >>> (x != 1).asnumpy()\n    array([[ 0.,  0.,  0.],\n           [ 0.,  0.,  0.]], dtype=float32)\n    >>> (x != y).asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 0.,  0.,  0.]], dtype=float32)\n    >>> mx.nd.not_equal(x, y).asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 0.,  0.,  0.]], dtype=float32)\n    >>> (z != y).asnumpy()\n    array([[ 0.,  1.],\n           [ 1.,  0.]], dtype=float32)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send_msg(name, recipient, profile):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': None,\n           'comment': ''}\n    if __opts__['test']:\n        ret['comment'] = 'Need to send message to {0}: {1}'.format(\n            recipient,\n            name,\n        )\n        return ret\n    __salt__['xmpp.send_msg_multi'](\n        message=name,\n        recipients=[recipient],\n        profile=profile,\n    )\n    ret['result'] = True\n    ret['comment'] = 'Sent message to {0}: {1}'.format(recipient, name)\n    return ret", "output": "Send a message to an XMPP user\n\n    .. code-block:: yaml\n\n        server-warning-message:\n          xmpp.send_msg:\n            - name: 'This is a server warning message'\n            - profile: my-xmpp-account\n            - recipient: admins@xmpp.example.com/salt\n\n    name\n        The message to send to the XMPP user", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_arrow_type(dt):\n    \"\"\" \n    \"\"\"\n    import pyarrow as pa\n    if type(dt) == BooleanType:\n        arrow_type = pa.bool_()\n    elif type(dt) == ByteType:\n        arrow_type = pa.int8()\n    elif type(dt) == ShortType:\n        arrow_type = pa.int16()\n    elif type(dt) == IntegerType:\n        arrow_type = pa.int32()\n    elif type(dt) == LongType:\n        arrow_type = pa.int64()\n    elif type(dt) == FloatType:\n        arrow_type = pa.float32()\n    elif type(dt) == DoubleType:\n        arrow_type = pa.float64()\n    elif type(dt) == DecimalType:\n        arrow_type = pa.decimal128(dt.precision, dt.scale)\n    elif type(dt) == StringType:\n        arrow_type = pa.string()\n    elif type(dt) == BinaryType:\n        arrow_type = pa.binary()\n    elif type(dt) == DateType:\n        arrow_type = pa.date32()\n    elif type(dt) == TimestampType:\n        # Timestamps should be in UTC, JVM Arrow timestamps require a timezone to be read\n        arrow_type = pa.timestamp('us', tz='UTC')\n    elif type(dt) == ArrayType:\n        if type(dt.elementType) in [StructType, TimestampType]:\n            raise TypeError(\"Unsupported type in conversion to Arrow: \" + str(dt))\n        arrow_type = pa.list_(to_arrow_type(dt.elementType))\n    elif type(dt) == StructType:\n        if any(type(field.dataType) == StructType for field in dt):\n            raise TypeError(\"Nested StructType not supported in conversion to Arrow\")\n        fields = [pa.field(field.name, to_arrow_type(field.dataType), nullable=field.nullable)\n                  for field in dt]\n        arrow_type = pa.struct(fields)\n    else:\n        raise TypeError(\"Unsupported type in conversion to Arrow: \" + str(dt))\n    return arrow_type", "output": "Convert Spark data type to pyarrow type", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(self, environment, name, globals=None):\n        \"\"\"\n        \"\"\"\n        code = None\n        if globals is None:\n            globals = {}\n\n        # first we try to get the source for this template together\n        # with the filename and the uptodate function.\n        source, filename, uptodate = self.get_source(environment, name)\n\n        # try to load the code from the bytecode cache if there is a\n        # bytecode cache configured.\n        bcc = environment.bytecode_cache\n        if bcc is not None:\n            bucket = bcc.get_bucket(environment, name, filename, source)\n            code = bucket.code\n\n        # if we don't have code so far (not cached, no longer up to\n        # date) etc. we compile the template\n        if code is None:\n            code = environment.compile(source, name, filename)\n\n        # if the bytecode cache is available and the bucket doesn't\n        # have a code so far, we give the bucket the new code and put\n        # it back to the bytecode cache.\n        if bcc is not None and bucket.code is None:\n            bucket.code = code\n            bcc.set_bucket(bucket)\n\n        return environment.template_class.from_code(environment, code,\n                                                    globals, uptodate)", "output": "Loads a template.  This method looks up the template in the cache\n        or loads one by calling :meth:`get_source`.  Subclasses should not\n        override this method as loaders working on collections of other\n        loaders (such as :class:`PrefixLoader` or :class:`ChoiceLoader`)\n        will not call this method but `get_source` directly.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _extract_variable_parts(variable_key, variable):\n  \"\"\"\n  \"\"\"\n  name, offset, partitioned = None, None, False\n  # pylint: disable=protected-access\n  if variable._save_slice_info:\n    name = variable_key[:variable_key.rfind(\"/\")]\n    if not variable._save_slice_info.full_name.endswith(name):\n      raise RuntimeError(\"Unexpected handling of partitioned variable.\")\n    offset = variable._save_slice_info.var_offset[0]\n    partitioned = True\n  # pylint: enable=protected-access\n  return partitioned, name, offset", "output": "Matches a variable to individual parts.\n\n  Args:\n    variable_key: String identifier of the variable in the module scope.\n    variable: Variable tensor.\n\n  Returns:\n    partitioned: Whether the variable is partitioned.\n    name: Name of the variable up to the partitioning.\n    offset: Offset of the variable into the full variable.\n\n  Raises:\n    RuntimeError: In case of unexpected variable format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_xgb_params(self):\n        \"\"\"\"\"\"\n        xgb_params = self.get_params()\n        random_state = xgb_params.pop('random_state')\n        if 'seed' in xgb_params and xgb_params['seed'] is not None:\n            warnings.warn('The seed parameter is deprecated as of version .6.'\n                          'Please use random_state instead.'\n                          'seed is deprecated.', DeprecationWarning)\n        else:\n            xgb_params['seed'] = random_state\n        n_jobs = xgb_params.pop('n_jobs')\n        if 'nthread' in xgb_params and xgb_params['nthread'] is not None:\n            warnings.warn('The nthread parameter is deprecated as of version .6.'\n                          'Please use n_jobs instead.'\n                          'nthread is deprecated.', DeprecationWarning)\n        else:\n            xgb_params['nthread'] = n_jobs\n\n        if 'silent' in xgb_params and xgb_params['silent'] is not None:\n            warnings.warn('The silent parameter is deprecated.'\n                          'Please use verbosity instead.'\n                          'silent is depreated', DeprecationWarning)\n            # TODO(canonizer): set verbosity explicitly if silent is removed from xgboost,\n            # but remains in this API\n        else:\n            # silent=None shouldn't be passed to xgboost\n            xgb_params.pop('silent', None)\n\n        if xgb_params['nthread'] <= 0:\n            xgb_params.pop('nthread', None)\n        return xgb_params", "output": "Get xgboost type parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, key, default=None):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            return self[key]\n        except (KeyError, ValueError, IndexError):\n            return default", "output": "Get item from object for given key (DataFrame column, Panel slice,\n        etc.). Returns default value if not found.\n\n        Parameters\n        ----------\n        key : object\n\n        Returns\n        -------\n        value : same type as items contained in object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_builder_configs():\n  \"\"\"\n  \"\"\"\n  config_list = []\n  for corruption in _CORRUPTIONS:\n    for severity in range(1, 6):\n      config_list.append(\n          Cifar10CorruptedConfig(\n              name=corruption + '_' + str(severity),\n              version='0.0.1',\n              description='Corruption method: ' + corruption +\n              ', severity level: ' + str(severity),\n              corruption_type=corruption,\n              severity=severity,\n          ))\n  return config_list", "output": "Construct a list of BuilderConfigs.\n\n  Construct a list of 75 Cifar10CorruptedConfig objects, corresponding to\n  the 15 corruption types and 5 severities.\n\n  Returns:\n    A list of 75 Cifar10CorruptedConfig objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def processPath(self, path, objectType):\n        '''\n        \n        '''\n        if objectType == win32security.SE_REGISTRY_KEY:\n            splt = path.split(\"\\\\\")\n            hive = self.getSecurityHkey(splt.pop(0).upper())\n            splt.insert(0, hive)\n            path = r'\\\\'.join(splt)\n        else:\n            path = os.path.expandvars(path)\n        return path", "output": "processes a path/object type combo and returns:\n            registry types with the correct HKEY text representation\n            files/directories with environment variables expanded", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def preprocess(img):\n    \"\"\"\"\"\"\n    # Crop the image.\n    img = img[35:195]\n    # Downsample by factor of 2.\n    img = img[::2, ::2, 0]\n    # Erase background (background type 1).\n    img[img == 144] = 0\n    # Erase background (background type 2).\n    img[img == 109] = 0\n    # Set everything else (paddles, ball) to 1.\n    img[img != 0] = 1\n    return img.astype(np.float).ravel()", "output": "Preprocess 210x160x3 uint8 frame into 6400 (80x80) 1D float vector.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def have_correct_lambda_package_version(self, package_name, package_version):\n        \"\"\"\n        \n        \"\"\"\n        lambda_package_details = lambda_packages.get(package_name, {}).get(self.runtime)\n\n        if lambda_package_details is None:\n            return False\n\n        # Binaries can be compiled for different package versions\n        # Related: https://github.com/Miserlou/Zappa/issues/800\n        if package_version != lambda_package_details['version']:\n            return False\n\n        return True", "output": "Checks if a given package version binary should be copied over from lambda packages.\n        package_name should be lower-cased version of package name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def public_ip_address_delete(name, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    result = False\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        pub_ip = netconn.public_ip_addresses.delete(\n            public_ip_address_name=name,\n            resource_group_name=resource_group\n        )\n        pub_ip.wait()\n        result = True\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Delete a public IP address.\n\n    :param name: The name of the public IP address to delete.\n\n    :param resource_group: The resource group name assigned to the\n        public IP address.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.public_ip_address_delete test-pub-ip testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cmd_implementation(self, events_lib, top_level_cmd_name, subcmd_name, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        event = events_lib.generate_event(top_level_cmd_name, subcmd_name, kwargs)\n        click.echo(event)\n        return event", "output": "calls for value substitution in the event json and returns the\n        customized json as a string\n\n        Parameters\n        ----------\n        events_lib\n        top_level_cmd_name: string\n            the name of the service\n        subcmd_name: string\n            the name of the event under the service\n        args: tuple\n            any arguments passed in before kwargs\n        kwargs: dict\n            the keys and values for substitution in the json\n        Returns\n        -------\n        event: string\n            returns the customized event json as a string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_tagging(Bucket,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.delete_bucket_tagging(Bucket=Bucket)\n        return {'deleted': True, 'name': Bucket}\n    except ClientError as e:\n        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Delete the tags from the given bucket\n\n    Returns {deleted: true} if tags were deleted and returns\n    {deleted: False} if tags were not deleted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.delete_tagging my_bucket", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_event(self, event):\n        \"\"\"\n        \n        \"\"\"\n        self._lock.acquire()\n        try:\n            self._event = event\n            # Make sure the event starts in `set` state if we appear to already\n            # be closed; otherwise, if we start in `clear` state & are closed,\n            # nothing will ever call `.feed` and the event (& OS pipe, if we're\n            # wrapping one - see `Channel.fileno`) will permanently stay in\n            # `clear`, causing deadlock if e.g. `select`ed upon.\n            if self._closed or len(self._buffer) > 0:\n                event.set()\n            else:\n                event.clear()\n        finally:\n            self._lock.release()", "output": "Set an event on this buffer.  When data is ready to be read (or the\n        buffer has been closed), the event will be set.  When no data is\n        ready, the event will be cleared.\n\n        :param threading.Event event: the event to set/clear", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def astype(self, dtype, copy=True):\n        \"\"\"\n        \n        \"\"\"\n\n        # if we are astyping to an existing IntegerDtype we can fastpath\n        if isinstance(dtype, _IntegerDtype):\n            result = self._data.astype(dtype.numpy_dtype, copy=False)\n            return type(self)(result, mask=self._mask, copy=False)\n\n        # coerce\n        data = self._coerce_to_ndarray()\n        return astype_nansafe(data, dtype, copy=None)", "output": "Cast to a NumPy array or IntegerArray with 'dtype'.\n\n        Parameters\n        ----------\n        dtype : str or dtype\n            Typecode or data-type to which the array is cast.\n        copy : bool, default True\n            Whether to copy the data, even if not necessary. If False,\n            a copy is made only if the old dtype does not match the\n            new dtype.\n\n        Returns\n        -------\n        array : ndarray or IntegerArray\n            NumPy ndarray or IntergerArray with 'dtype' for its dtype.\n\n        Raises\n        ------\n        TypeError\n            if incompatible type with an IntegerDtype, equivalent of same_kind\n            casting", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_model(PARAMS):\n    ''''''\n    model = SVC()\n    model.C = PARAMS.get('C')\n    model.keral = PARAMS.get('keral')\n    model.degree = PARAMS.get('degree')\n    model.gamma = PARAMS.get('gamma')\n    model.coef0 = PARAMS.get('coef0')\n    \n    return model", "output": "Get model according to parameters", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _upstart_enable(name):\n    '''\n    \n    '''\n    if _upstart_is_enabled(name):\n        return _upstart_is_enabled(name)\n    override = '/etc/init/{0}.override'.format(name)\n    files = ['/etc/init/{0}.conf'.format(name), override]\n    for file_name in filter(os.path.isfile, files):\n        with salt.utils.files.fopen(file_name, 'r+') as fp_:\n            new_text = re.sub(r'^\\s*manual\\n?',\n                              '',\n                              salt.utils.stringutils.to_unicode(fp_.read()),\n                              0,\n                              re.MULTILINE)\n            fp_.seek(0)\n            fp_.write(\n                salt.utils.stringutils.to_str(\n                    new_text\n                )\n            )\n            fp_.truncate()\n    if os.access(override, os.R_OK) and os.path.getsize(override) == 0:\n        os.unlink(override)\n    return _upstart_is_enabled(name)", "output": "Enable an Upstart service.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _raw_face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n    \"\"\"\n    \n    \"\"\"\n    if model == \"cnn\":\n        return cnn_face_detector(img, number_of_times_to_upsample)\n    else:\n        return face_detector(img, number_of_times_to_upsample)", "output": "Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of dlib 'rect' objects of found face locations", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def annealing_exp(start:Number, end:Number, pct:float)->Number:\n    \"\"\n    return start * (end/start) ** pct", "output": "Exponentially anneal from `start` to `end` as pct goes from 0.0 to 1.0.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sub_period_array(self, other):\n        \"\"\"\n        \n        \"\"\"\n        if not is_period_dtype(self):\n            raise TypeError(\"cannot subtract {dtype}-dtype from {cls}\"\n                            .format(dtype=other.dtype,\n                                    cls=type(self).__name__))\n\n        if len(self) != len(other):\n            raise ValueError(\"cannot subtract arrays/indices of \"\n                             \"unequal length\")\n        if self.freq != other.freq:\n            msg = DIFFERENT_FREQ.format(cls=type(self).__name__,\n                                        own_freq=self.freqstr,\n                                        other_freq=other.freqstr)\n            raise IncompatibleFrequency(msg)\n\n        new_values = checked_add_with_arr(self.asi8, -other.asi8,\n                                          arr_mask=self._isnan,\n                                          b_mask=other._isnan)\n\n        new_values = np.array([self.freq.base * x for x in new_values])\n        if self._hasnans or other._hasnans:\n            mask = (self._isnan) | (other._isnan)\n            new_values[mask] = NaT\n        return new_values", "output": "Subtract a Period Array/Index from self.  This is only valid if self\n        is itself a Period Array/Index, raises otherwise.  Both objects must\n        have the same frequency.\n\n        Parameters\n        ----------\n        other : PeriodIndex or PeriodArray\n\n        Returns\n        -------\n        result : np.ndarray[object]\n            Array of DateOffset objects; nulls represented by NaT.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def combine_tensors_and_multiply(combination: str,\n                                 tensors: List[torch.Tensor],\n                                 weights: torch.nn.Parameter) -> torch.Tensor:\n    \"\"\"\n    \n    \"\"\"\n    if len(tensors) > 9:\n        raise ConfigurationError(\"Double-digit tensor lists not currently supported\")\n    combination = combination.replace('x', '1').replace('y', '2')\n    pieces = combination.split(',')\n    tensor_dims = [tensor.size(-1) for tensor in tensors]\n    combination_dims = [_get_combination_dim(piece, tensor_dims) for piece in pieces]\n    dims_so_far = 0\n    to_sum = []\n    for piece, combination_dim in zip(pieces, combination_dims):\n        weight = weights[dims_so_far:(dims_so_far + combination_dim)]\n        dims_so_far += combination_dim\n        to_sum.append(_get_combination_and_multiply(piece, tensors, weight))\n    result = to_sum[0]\n    for result_piece in to_sum[1:]:\n        result = result + result_piece\n    return result", "output": "Like :func:`combine_tensors`, but does a weighted (linear) multiplication while combining.\n    This is a separate function from ``combine_tensors`` because we try to avoid instantiating\n    large intermediate tensors during the combination, which is possible because we know that we're\n    going to be multiplying by a weight vector in the end.\n\n    Parameters\n    ----------\n    combination : ``str``\n        Same as in :func:`combine_tensors`\n    tensors : ``List[torch.Tensor]``\n        A list of tensors to combine, where the integers in the ``combination`` are (1-indexed)\n        positions in this list of tensors.  These tensors are all expected to have either three or\n        four dimensions, with the final dimension being an embedding.  If there are four\n        dimensions, one of them must have length 1.\n    weights : ``torch.nn.Parameter``\n        A vector of weights to use for the combinations.  This should have shape (combined_dim,),\n        as calculated by :func:`get_combined_dim`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def virtual_machine_convert_to_managed_disks(name, resource_group, **kwargs):  # pylint: disable=invalid-name\n    '''\n    \n\n    '''\n    compconn = __utils__['azurearm.get_client']('compute', **kwargs)\n    try:\n        # pylint: disable=invalid-name\n        vm = compconn.virtual_machines.convert_to_managed_disks(\n            resource_group_name=resource_group,\n            vm_name=name\n        )\n        vm.wait()\n        vm_result = vm.result()\n        result = vm_result.as_dict()\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('compute', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Converts virtual machine disks from blob-based to managed disks. Virtual\n    machine must be stop-deallocated before invoking this operation.\n\n    :param name: The name of the virtual machine to convert.\n\n    :param resource_group: The resource group name assigned to the\n        virtual machine.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_compute.virtual_machine_convert_to_managed_disks testvm testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def formatDump(self, f, format):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlDocFormatDump(f, self._o, format)\n        return ret", "output": "Dump an XML document to an open FILE.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def servicegroup_server_exists(sg_name, s_name, s_port=None, **connection_args):\n    '''\n    \n    '''\n    return _servicegroup_get_server(sg_name, s_name, s_port, **connection_args) is not None", "output": "Check if a server:port combination is a member of a servicegroup\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' netscaler.servicegroup_server_exists 'serviceGroupName' 'serverName' 'serverPort'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def StructField(name, field_type):  # pylint: disable=invalid-name\n    \"\"\"\n    \"\"\"\n    return type_pb2.StructType.Field(name=name, type=field_type)", "output": "Construct a field description protobuf.\n\n    :type name: str\n    :param name: the name of the field\n\n    :type field_type: :class:`type_pb2.Type`\n    :param field_type: the type of the field\n\n    :rtype: :class:`type_pb2.StructType.Field`\n    :returns: the appropriate struct-field-type protobuf", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getitem_block(self, slicer, new_mgr_locs=None):\n        \"\"\"\n        \n        \"\"\"\n        if new_mgr_locs is None:\n            if isinstance(slicer, tuple):\n                axis0_slicer = slicer[0]\n            else:\n                axis0_slicer = slicer\n            new_mgr_locs = self.mgr_locs[axis0_slicer]\n\n        new_values = self._slice(slicer)\n\n        if self._validate_ndim and new_values.ndim != self.ndim:\n            raise ValueError(\"Only same dim slicing is allowed\")\n\n        return self.make_block_same_class(new_values, new_mgr_locs)", "output": "Perform __getitem__-like, return result as block.\n\n        As of now, only supports slices that preserve dimensionality.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_keep_score(source_counts, prediction_counts, target_counts):\n  \"\"\"\"\"\"\n  source_and_prediction_counts = source_counts & prediction_counts\n  source_and_target_counts = source_counts & target_counts\n  true_positives = sum((source_and_prediction_counts &\n                        source_and_target_counts).values())\n  selected = sum(source_and_prediction_counts.values())\n  relevant = sum(source_and_target_counts.values())\n  return _get_fbeta_score(true_positives, selected, relevant)", "output": "Compute the keep score (Equation 5 in the paper).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_urlize(eval_ctx, value, trim_url_limit=None, nofollow=False,\n              target=None, rel=None):\n    \"\"\"\n    \"\"\"\n    policies = eval_ctx.environment.policies\n    rel = set((rel or '').split() or [])\n    if nofollow:\n        rel.add('nofollow')\n    rel.update((policies['urlize.rel'] or '').split())\n    if target is None:\n        target = policies['urlize.target']\n    rel = ' '.join(sorted(rel)) or None\n    rv = urlize(value, trim_url_limit, rel=rel, target=target)\n    if eval_ctx.autoescape:\n        rv = Markup(rv)\n    return rv", "output": "Converts URLs in plain text into clickable links.\n\n    If you pass the filter an additional integer it will shorten the urls\n    to that number. Also a third argument exists that makes the urls\n    \"nofollow\":\n\n    .. sourcecode:: jinja\n\n        {{ mytext|urlize(40, true) }}\n            links are shortened to 40 chars and defined with rel=\"nofollow\"\n\n    If *target* is specified, the ``target`` attribute will be added to the\n    ``<a>`` tag:\n\n    .. sourcecode:: jinja\n\n       {{ mytext|urlize(40, target='_blank') }}\n\n    .. versionchanged:: 2.8+\n       The *target* parameter was added.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put_acl(Bucket,\n           ACL=None,\n           AccessControlPolicy=None,\n           GrantFullControl=None,\n           GrantRead=None,\n           GrantReadACP=None,\n           GrantWrite=None,\n           GrantWriteACP=None,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        kwargs = {}\n        if AccessControlPolicy is not None:\n            if isinstance(AccessControlPolicy, six.string_types):\n                AccessControlPolicy = salt.utils.json.loads(AccessControlPolicy)\n            kwargs['AccessControlPolicy'] = AccessControlPolicy\n        for arg in ('ACL',\n                    'GrantFullControl',\n                    'GrantRead', 'GrantReadACP',\n                    'GrantWrite', 'GrantWriteACP'):\n            if locals()[arg] is not None:\n                kwargs[arg] = str(locals()[arg])  # future lint: disable=blacklisted-function\n        conn.put_bucket_acl(Bucket=Bucket, **kwargs)\n        return {'updated': True, 'name': Bucket}\n    except ClientError as e:\n        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Given a valid config, update the ACL for a bucket.\n\n    Returns {updated: true} if the ACL was updated and returns\n    {updated: False} if the ACL was not updated.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.put_acl my_bucket 'public' \\\\\n                         GrantFullControl='emailaddress=example@example.com' \\\\\n                         GrantRead='uri=\"http://acs.amazonaws.com/groups/global/AllUsers\"' \\\\\n                         GrantReadACP='emailaddress=\"exampl@example.com\",id=\"2345678909876432\"'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decompile_pyc(bin_pyc, output=sys.stdout):\n    '''\n    \n    '''\n    \n    from turicreate.meta.asttools import python_source\n    \n    bin = bin_pyc.read()\n    \n    code = marshal.loads(bin[8:])\n    \n    mod_ast = make_module(code)\n    \n    python_source(mod_ast, file=output)", "output": "decompile apython pyc or pyo binary file.\n    \n    :param bin_pyc: input file objects\n    :param output: output file objects", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, obj, matches=None, mt=None, lt=None, eq=None):\n        '''\n        \n        '''\n        objects = []\n        with gzip.open(os.path.join(self.db_path, obj._TABLE), 'rb') as table:\n            header = None\n            for data in csv.reader(table):\n                if not header:\n                    header = data\n                    continue\n                _obj = obj()\n                for t_attr, t_data in zip(header, data):\n                    t_attr, t_type = t_attr.split(':')\n                    setattr(_obj, t_attr, self._to_type(t_data, t_type))\n                if self.__criteria(_obj, matches=matches, mt=mt, lt=lt, eq=eq):\n                    objects.append(_obj)\n        return objects", "output": "Get objects from the table.\n\n        :param table_name:\n        :param matches: Regexp.\n        :param mt: More than.\n        :param lt: Less than.\n        :param eq: Equals.\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def consume(topic, conf):\n    \"\"\"\n        \n    \"\"\"\n    from confluent_kafka.avro import AvroConsumer\n    from confluent_kafka.avro.serializer import SerializerError\n\n    print(\"Consuming user records from topic {} with group {}. ^c to exit.\".format(topic, conf[\"group.id\"]))\n\n    c = AvroConsumer(conf, reader_value_schema=record_schema)\n    c.subscribe([topic])\n\n    while True:\n        try:\n            msg = c.poll(1)\n\n            # There were no messages on the queue, continue polling\n            if msg is None:\n                continue\n\n            if msg.error():\n                print(\"Consumer error: {}\".format(msg.error()))\n                continue\n\n            record = User(msg.value())\n            print(\"name: {}\\n\\tfavorite_number: {}\\n\\tfavorite_color: {}\\n\".format(\n                record.name, record.favorite_number, record.favorite_color))\n        except SerializerError as e:\n            # Report malformed record, discard results, continue polling\n            print(\"Message deserialization failed {}\".format(e))\n            continue\n        except KeyboardInterrupt:\n            break\n\n    print(\"Shutting down consumer..\")\n    c.close()", "output": "Consume User records", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _gather_pillar(pillarenv, pillar_override):\n    '''\n    \n    '''\n    pillar = salt.pillar.get_pillar(\n        __opts__,\n        __grains__,\n        __opts__['id'],\n        __opts__['saltenv'],\n        pillar_override=pillar_override,\n        pillarenv=pillarenv\n    )\n    ret = pillar.compile_pillar()\n    if pillar_override and isinstance(pillar_override, dict):\n        ret.update(pillar_override)\n    return ret", "output": "Whenever a state run starts, gather the pillar data fresh", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset(name, runas=None):\n    '''\n    \n    '''\n    return prlctl('reset', salt.utils.data.decode(name), runas=runas)", "output": "Reset a VM by performing a hard shutdown and then a restart\n\n    :param str name:\n        Name/ID of VM to reset\n\n    :param str runas:\n        The user that the prlctl command will be run as\n\n    Example:\n\n    .. code-block:: bash\n\n        salt '*' parallels.reset macvm runas=macdev", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render_chart_to_file(self, template_name: str, chart: Any, path: str):\n        \"\"\"\n        \n        \"\"\"\n        tpl = self.env.get_template(template_name)\n        html = tpl.render(chart=self.generate_js_link(chart))\n        write_utf8_html_file(path, self._reg_replace(html))", "output": "Render a chart or page to local html files.\n\n        :param chart: A Chart or Page object\n        :param path: The destination file which the html code write to\n        :param template_name: The name of template file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_SU_save_stock_xdxr(client=DATABASE, ui_log=None, ui_progress=None):\n    \"\"\"\n    \"\"\"\n    stock_list = QA_fetch_get_stock_list().code.unique().tolist()\n    # client.drop_collection('stock_xdxr')\n    try:\n\n        coll = client.stock_xdxr\n        coll.create_index(\n            [('code',\n              pymongo.ASCENDING),\n             ('date',\n              pymongo.ASCENDING)],\n            unique=True\n        )\n    except:\n        client.drop_collection('stock_xdxr')\n        coll = client.stock_xdxr\n        coll.create_index(\n            [('code',\n              pymongo.ASCENDING),\n             ('date',\n              pymongo.ASCENDING)],\n            unique=True\n        )\n    err = []\n\n    def __saving_work(code, coll):\n        QA_util_log_info(\n            '##JOB02 Now Saving XDXR INFO ==== {}'.format(str(code)),\n            ui_log=ui_log\n        )\n        try:\n            coll.insert_many(\n                QA_util_to_json_from_pandas(QA_fetch_get_stock_xdxr(str(code))),\n                ordered=False\n            )\n\n        except:\n\n            err.append(str(code))\n\n    for i_ in range(len(stock_list)):\n        QA_util_log_info(\n            'The {} of Total {}'.format(i_,\n                                        len(stock_list)),\n            ui_log=ui_log\n        )\n        strLogInfo = 'DOWNLOAD PROGRESS {} '.format(\n            str(float(i_ / len(stock_list) * 100))[0:4] + '%'\n        )\n        intLogProgress = int(float(i_ / len(stock_list) * 100))\n        QA_util_log_info(\n            strLogInfo,\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=intLogProgress\n        )\n        __saving_work(stock_list[i_], coll)", "output": "[summary]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install_ruby(ruby, runas=None):\n    '''\n    \n    '''\n    ruby = re.sub(r'^ruby-', '', ruby)\n\n    env = None\n    env_list = []\n\n    if __grains__['os'] in ('FreeBSD', 'NetBSD', 'OpenBSD'):\n        env_list.append('MAKE=gmake')\n\n    if __salt__['config.get']('rbenv:build_env'):\n        env_list.append(__salt__['config.get']('rbenv:build_env'))\n    elif __salt__['config.option']('rbenv.build_env'):\n        env_list.append(__salt__['config.option']('rbenv.build_env'))\n\n    if env_list:\n        env = ' '.join(env_list)\n\n    ret = {}\n    ret = _rbenv_exec(['install', ruby], env=env, runas=runas, ret=ret)\n    if ret is not False and ret['retcode'] == 0:\n        rehash(runas=runas)\n        return ret['stderr']\n    else:\n        # Cleanup the failed installation so it doesn't list as installed\n        uninstall_ruby(ruby, runas=runas)\n        return False", "output": "Install a ruby implementation.\n\n    ruby\n        The version of Ruby to install, should match one of the\n        versions listed by :py:func:`rbenv.list <salt.modules.rbenv.list>`\n\n    runas\n        The user under which to run rbenv. If not specified, then rbenv will be\n        run as the user under which Salt is running.\n\n    Additional environment variables can be configured in pillar /\n    grains / master:\n\n    .. code-block:: yaml\n\n        rbenv:\n          build_env: 'CONFIGURE_OPTS=\"--no-tcmalloc\" CFLAGS=\"-fno-tree-dce\"'\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rbenv.install_ruby 2.0.0-p0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathNextPrecedingSibling(self, cur):\n        \"\"\" \"\"\"\n        if cur is None: cur__o = None\n        else: cur__o = cur._o\n        ret = libxml2mod.xmlXPathNextPrecedingSibling(self._o, cur__o)\n        if ret is None:raise xpathError('xmlXPathNextPrecedingSibling() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Traversal function for the \"preceding-sibling\" direction\n          The preceding-sibling axis contains the preceding siblings\n          of the context node in reverse document order; the first\n          preceding sibling is first on the axis; the sibling\n           preceding that node is the second on the axis and so on.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gain_focus(self):\r\n        \"\"\"\"\"\"\r\n        if (self.is_running and self.any_has_focus() and \r\n            not self.setting_data and self.hidden):\r\n            self.unhide_tips()", "output": "Confirm if the tour regains focus and unhides the tips.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def multilabel_accuracy_matchk(predictions,\n                               labels,\n                               k,\n                               weights_fn=common_layers.weights_nonzero):\n  \"\"\"\n\n  \"\"\"\n  predictions = tf.to_int32(tf.argmax(predictions, axis=-1))\n  scores = tf.to_float(tf.equal(predictions, labels))\n  # those label == 0 do not count\n  weights = weights_fn(labels)\n  scores *= weights\n  scores = tf.reduce_sum(scores, axis=[1, 2, 3])\n  scores = tf.minimum(scores / tf.to_float(k), 1)\n  # every sample count\n  weights = tf.ones(tf.shape(scores), dtype=tf.float32)\n\n  return scores, weights", "output": "Used to evaluate the VQA accuracy.\n\n  Let n be the times that predictions appear in labels, then final score\n  is min(n/k, 1).\n  Refer to https://arxiv.org/pdf/1505.00468.pdf.\n\n  Args:\n    predictions: A tensor with shape [batch_size, 1, 1, 1, vocab_size].\n    labels: A tensor with shape [batch_size, length, 1, 1].\n    k: A tensor constant.\n    weights_fn: weight function.\n  Returns:\n    scores: min(n/k, 1).\n    weights: returns all ones.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_history(self, start, end):\n        \"\"\"\n        \"\"\"\n        return self.history_table.set_index(\n            'datetime',\n            drop=False\n        ).loc[slice(pd.Timestamp(start),\n                    pd.Timestamp(end))]", "output": "\u8fd4\u56de\u5386\u53f2\u6210\u4ea4\n\n        Arguments:\n            start {str} -- [description]\n            end {str]} -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def train(self, conversation):\n        \"\"\"\n        \n        \"\"\"\n        previous_statement_text = None\n        previous_statement_search_text = ''\n\n        statements_to_create = []\n\n        for conversation_count, text in enumerate(conversation):\n            if self.show_training_progress:\n                utils.print_progress_bar(\n                    'List Trainer',\n                    conversation_count + 1, len(conversation)\n                )\n\n            statement_search_text = self.chatbot.storage.tagger.get_bigram_pair_string(text)\n\n            statement = self.get_preprocessed_statement(\n                Statement(\n                    text=text,\n                    search_text=statement_search_text,\n                    in_response_to=previous_statement_text,\n                    search_in_response_to=previous_statement_search_text,\n                    conversation='training'\n                )\n            )\n\n            previous_statement_text = statement.text\n            previous_statement_search_text = statement_search_text\n\n            statements_to_create.append(statement)\n\n        self.chatbot.storage.create_many(statements_to_create)", "output": "Train the chat bot based on the provided list of\n        statements that represents a single conversation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_sizes(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_sizes function must be called with '\n            '-f or --function, or with the --list-sizes option'\n        )\n\n    compconn = get_conn(client_type='compute')\n\n    ret = {}\n    location = get_location()\n\n    try:\n        sizes = compconn.virtual_machine_sizes.list(\n            location=location\n        )\n        for size_obj in sizes:\n            size = size_obj.as_dict()\n            ret[size['name']] = size\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('compute', exc.message)\n        ret = {'Error': exc.message}\n\n    return ret", "output": "Return a list of sizes available from the provider", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install_cygwin(name, install_args=None, override_args=False):\n    '''\n    \n    '''\n    return install(name,\n                   source='cygwin',\n                   install_args=install_args,\n                   override_args=override_args)", "output": "Instructs Chocolatey to install a package via Cygwin.\n\n    name\n        The name of the package to be installed. Only accepts a single argument.\n\n    install_args\n        A list of install arguments you want to pass to the installation process\n        i.e product key or feature list\n\n    override_args\n        Set to true if you want to override the original install arguments (for\n        the native installer) in the package and use your own. When this is set\n        to False install_args will be appended to the end of the default\n        arguments\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' chocolatey.install_cygwin <package name>\n        salt '*' chocolatey.install_cygwin <package name> install_args=<args> override_args=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_version(name):\n    '''\n    \n\n    '''\n    line = '@version: {0}'.format(name)\n    try:\n        if os.path.exists(__SYSLOG_NG_CONFIG_FILE):\n            log.debug(\n                'Removing previous configuration file: %s',\n                __SYSLOG_NG_CONFIG_FILE\n            )\n            os.remove(__SYSLOG_NG_CONFIG_FILE)\n            log.debug('Configuration file successfully removed')\n\n        header = _format_generated_config_header()\n        _write_config(config=header, newlines=1)\n        _write_config(config=line, newlines=2)\n\n        return _format_state_result(name, result=True)\n    except OSError as err:\n        log.error(\n            'Failed to remove previous configuration file \\'%s\\': %s',\n            __SYSLOG_NG_CONFIG_FILE, err\n        )\n        return _format_state_result(name, result=False)", "output": "Removes the previous configuration file, then creates a new one and writes\n    the name line. This function is intended to be used from states.\n\n    If :mod:`syslog_ng.set_config_file\n    <salt.modules.syslog_ng.set_config_file>`, is called before, this function\n    will use the set config file.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' syslog_ng.write_version name=\"3.6\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_dense_weight(layer):\n    '''\n    '''\n    units = layer.units\n    weight = np.eye(units)\n    bias = np.zeros(units)\n    layer.set_weights(\n        (add_noise(weight, np.array([0, 1])), add_noise(bias, np.array([0, 1])))\n    )", "output": "initilize dense layer weight.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_nonblocking(self, size=1, timeout=None):\n        \"\"\"\n        \"\"\"\n\n        try:\n            s = os.read(self.child_fd, size)\n        except OSError as err:\n            if err.args[0] == errno.EIO:\n                # Linux-style EOF\n                self.flag_eof = True\n                raise EOF('End Of File (EOF). Exception style platform.')\n            raise\n        if s == b'':\n            # BSD-style EOF\n            self.flag_eof = True\n            raise EOF('End Of File (EOF). Empty string style platform.')\n\n        s = self._decoder.decode(s, final=False)\n        self._log(s, 'read')\n        return s", "output": "This reads data from the file descriptor.\n\n        This is a simple implementation suitable for a regular file. Subclasses using ptys or pipes should override it.\n\n        The timeout parameter is ignored.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def groupby(self, values):\n        \"\"\"\n        \n        \"\"\"\n\n        # TODO: if we are a MultiIndex, we can do better\n        # that converting to tuples\n        if isinstance(values, ABCMultiIndex):\n            values = values.values\n        values = ensure_categorical(values)\n        result = values._reverse_indexer()\n\n        # map to the label\n        result = {k: self.take(v) for k, v in result.items()}\n\n        return result", "output": "Group the index labels by a given array of values.\n\n        Parameters\n        ----------\n        values : array\n            Values used to determine the groups.\n\n        Returns\n        -------\n        groups : dict\n            {group name -> group labels}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_timestamp(timestamp):\n    \"\"\"\"\"\"\n    dt = dateutil.parser.parse(timestamp)\n    return dt.astimezone(dateutil.tz.tzutc())", "output": "Parse ISO8601 timestamps given by github API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _date_time_match(cron, **kwargs):\n    '''\n    \n    '''\n    return all([kwargs.get(x) is None or cron[x] == six.text_type(kwargs[x])\n                or (six.text_type(kwargs[x]).lower() == 'random' and cron[x] != '*')\n                for x in ('minute', 'hour', 'daymonth', 'month', 'dayweek')])", "output": "Returns true if the minute, hour, etc. params match their counterparts from\n    the dict returned from list_tab().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def await_idle(self, parent_id, timeout):\n        \"\"\"\"\"\"\n        while True:\n            # Get a message from the kernel iopub channel\n            msg = self.get_message(timeout=timeout, stream='iopub') # raises Empty on timeout!\n\n            if msg['parent_header'].get('msg_id') != parent_id:\n                continue\n            if msg['msg_type'] == 'status':\n                if msg['content']['execution_state'] == 'idle':\n                    break", "output": "Poll the iopub stream until an idle message is received for the given parent ID", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ip_int_from_string(self, ip_str):\n        \"\"\"\n\n        \"\"\"\n        if not ip_str:\n            raise AddressValueError('Address cannot be empty')\n\n        octets = ip_str.split('.')\n        if len(octets) != 4:\n            raise AddressValueError(\"Expected 4 octets in %r\" % ip_str)\n\n        try:\n            return _int_from_bytes(map(self._parse_octet, octets), 'big')\n        except ValueError as exc:\n            raise AddressValueError(\"%s in %r\" % (exc, ip_str))", "output": "Turn the given IP string into an integer for comparison.\n\n        Args:\n            ip_str: A string, the IP ip_str.\n\n        Returns:\n            The IP ip_str as an integer.\n\n        Raises:\n            AddressValueError: if ip_str isn't a valid IPv4 Address.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(self, string, evaluate_result=True):\n        '''\n        '''\n        m = self._match_re.match(string)\n        if m is None:\n            return None\n\n        if evaluate_result:\n            return self.evaluate_result(m)\n        else:\n            return Match(self, m)", "output": "Match my format to the string exactly.\n\n        Return a Result or Match instance or None if there's no match.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_config_tree(name, configuration):\n    '''\n    \n    '''\n    type_, id_, options = _get_type_id_options(name, configuration)\n    global _INDENT, _current_statement\n    _INDENT = ''\n    if type_ == 'config':\n        _current_statement = GivenStatement(options)\n    elif type_ == 'log':\n        _current_statement = UnnamedStatement(type='log')\n        _parse_log_statement(options)\n    else:\n        if _is_statement_unnamed(type_):\n            _current_statement = UnnamedStatement(type=type_)\n        else:\n            _current_statement = NamedStatement(type=type_, id=id_)\n        _parse_statement(options)", "output": "Build the configuration tree.\n\n    The root object is _current_statement.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def held(name):\n    '''\n    \n    '''\n    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''}\n    state = __salt__['pkg.get_selections'](\n        pattern=name,\n    )\n    if not state:\n        ret.update(comment='Package {0} does not have a state'.format(name))\n    elif not salt.utils.data.is_true(state.get('hold', False)):\n        if not __opts__['test']:\n            result = __salt__['pkg.set_selections'](\n                selection={'hold': [name]}\n            )\n            ret.update(changes=result[name],\n                       result=True,\n                       comment='Package {0} is now being held'.format(name))\n        else:\n            ret.update(result=None,\n                       comment='Package {0} is set to be held'.format(name))\n    else:\n        ret.update(result=True,\n                   comment='Package {0} is already held'.format(name))\n\n    return ret", "output": "Set package in 'hold' state, meaning it will not be upgraded.\n\n    name\n        The name of the package, e.g., 'tmux'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _section(cls, opts):\n        \"\"\"\"\"\"\n        if isinstance(cls.config, LuigiConfigParser):\n            return False\n        try:\n            logging_config = cls.config['logging']\n        except (TypeError, KeyError, NoSectionError):\n            return False\n        logging.config.dictConfig(logging_config)\n        return True", "output": "Get logging settings from config file section \"logging\".", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_args(argv=None):\n    \"\"\"\n    \"\"\"\n    parent_parser = get_parent_parser()\n\n    # Main parser\n    desc = \"Data Version Control\"\n    parser = DvcParser(\n        prog=\"dvc\",\n        description=desc,\n        parents=[parent_parser],\n        formatter_class=argparse.RawTextHelpFormatter,\n    )\n\n    # NOTE: On some python versions action='version' prints to stderr\n    # instead of stdout https://bugs.python.org/issue18920\n    parser.add_argument(\n        \"-V\",\n        \"--version\",\n        action=VersionAction,\n        nargs=0,\n        help=\"Show program's version.\",\n    )\n\n    # Sub commands\n    subparsers = parser.add_subparsers(\n        title=\"Available Commands\",\n        metavar=\"COMMAND\",\n        dest=\"cmd\",\n        help=\"Use dvc COMMAND --help for command-specific help.\",\n    )\n\n    fix_subparsers(subparsers)\n\n    for cmd in COMMANDS:\n        cmd.add_parser(subparsers, parent_parser)\n\n    args = parser.parse_args(argv)\n\n    return args", "output": "Parses CLI arguments.\n\n    Args:\n        argv: optional list of arguments to parse. sys.argv is used by default.\n\n    Raises:\n        dvc.exceptions.DvcParserError: raised for argument parsing errors.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next(self):\n        \"\"\"\"\"\"\n        if not self.iter_next():\n            raise StopIteration\n        data = self.getdata()\n        label = self.getlabel()\n        # iter should stop when last batch is not complete\n        if data[0].shape[0] != self.batch_size:\n        # in this case, cache it for next epoch\n            self._cache_data = data\n            self._cache_label = label\n            raise StopIteration\n        return DataBatch(data=data, label=label, \\\n            pad=self.getpad(), index=None)", "output": "Returns the next batch of data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _AddSerializePartialToStringMethod(message_descriptor, cls):\n  \"\"\"\"\"\"\n\n  def SerializePartialToString(self):\n    out = BytesIO()\n    self._InternalSerialize(out.write)\n    return out.getvalue()\n  cls.SerializePartialToString = SerializePartialToString\n\n  def InternalSerialize(self, write_bytes):\n    for field_descriptor, field_value in self.ListFields():\n      field_descriptor._encoder(write_bytes, field_value)\n    for tag_bytes, value_bytes in self._unknown_fields:\n      write_bytes(tag_bytes)\n      write_bytes(value_bytes)\n  cls._InternalSerialize = InternalSerialize", "output": "Helper for _AddMessageMethods().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_cost(self, labels, logits):\n    \"\"\"\n    \n    \"\"\"\n    op = logits.op\n    if \"softmax\" in str(op).lower():\n      logits, = op.inputs\n\n    with tf.variable_scope('costs'):\n      xent = tf.nn.softmax_cross_entropy_with_logits(\n          logits=logits, labels=labels)\n      cost = tf.reduce_mean(xent, name='xent')\n      cost += self._decay()\n      cost = cost\n\n    return cost", "output": "Build the graph for cost from the logits if logits are provided.\n    If predictions are provided, logits are extracted from the operation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_images(call=None):\n    '''\n    \n\n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_images function must be called with '\n            '-f or --function, or with the --list-images option'\n        )\n\n    server, user, password = _get_xml_rpc()\n    auth = ':'.join([user, password])\n\n    image_pool = server.one.imagepool.info(auth, -2, -1, -1)[1]\n\n    images = {}\n    for image in _get_xml(image_pool):\n        images[image.find('NAME').text] = _xml_to_dict(image)\n\n    return images", "output": "Return available OpenNebula images.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud --list-images opennebula\n        salt-cloud --function avail_images opennebula\n        salt-cloud -f avail_images opennebula", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forward(self, agent_qs, states):\n        \"\"\"\n        \"\"\"\n        bs = agent_qs.size(0)\n        states = states.reshape(-1, self.state_dim)\n        agent_qs = agent_qs.view(-1, 1, self.n_agents)\n        # First layer\n        w1 = th.abs(self.hyper_w_1(states))\n        b1 = self.hyper_b_1(states)\n        w1 = w1.view(-1, self.n_agents, self.embed_dim)\n        b1 = b1.view(-1, 1, self.embed_dim)\n        hidden = F.elu(th.bmm(agent_qs, w1) + b1)\n        # Second layer\n        w_final = th.abs(self.hyper_w_final(states))\n        w_final = w_final.view(-1, self.embed_dim, 1)\n        # State-dependent bias\n        v = self.V(states).view(-1, 1, 1)\n        # Compute final output\n        y = th.bmm(hidden, w_final) + v\n        # Reshape and return\n        q_tot = y.view(bs, -1, 1)\n        return q_tot", "output": "Forward pass for the mixer.\n\n        Arguments:\n            agent_qs: Tensor of shape [B, T, n_agents, n_actions]\n            states: Tensor of shape [B, T, state_dim]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_current_deployment_id(self):\n        '''\n        \n        '''\n        deploymentId = ''\n        stage = __salt__['boto_apigateway.describe_api_stage'](restApiId=self.restApiId,\n                                                               stageName=self._stage_name,\n                                                               **self._common_aws_args).get('stage')\n        if stage:\n            deploymentId = stage.get('deploymentId')\n        return deploymentId", "output": "Helper method to find the deployment id that the stage name is currently assocaited with.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def random_search_max_confidence_recipe(sess, model, x, y, eps,\n                                        clip_min, clip_max,\n                                        report_path, batch_size=BATCH_SIZE,\n                                        num_noise_points=10000):\n  \"\"\"\n  \"\"\"\n  noise_attack = Noise(model, sess)\n  threat_params = {\"eps\": eps, \"clip_min\": clip_min, \"clip_max\": clip_max}\n  noise_attack_config = AttackConfig(noise_attack, threat_params)\n  attack_configs = [noise_attack_config]\n  assert batch_size % num_devices == 0\n  new_work_goal = {noise_attack_config: num_noise_points}\n  goals = [MaxConfidence(t=1., new_work_goal=new_work_goal)]\n  bundle_attacks(sess, model, x, y, attack_configs, goals, report_path)", "output": "Max confidence using random search.\n\n  References:\n  https://openreview.net/forum?id=H1g0piA9tQ\n    Describes the max_confidence procedure used for the bundling in this recipe\n  https://arxiv.org/abs/1802.00420\n    Describes using random search with 1e5 or more random points to avoid\n    gradient masking.\n\n  :param sess: tf.Session\n  :param model: cleverhans.model.Model\n  :param x: numpy array containing clean example inputs to attack\n  :param y: numpy array containing true labels\n  :param nb_classes: int, number of classes\n  :param eps: float, maximum size of perturbation (measured by max norm)\n  :param eps_iter: float, step size for one version of PGD attacks\n    (will also run another version with 25X smaller step size)\n  :param nb_iter: int, number of iterations for one version of PGD attacks\n    (will also run another version with 25X more iterations)\n  :param report_path: str, the path that the report will be saved to.\n  :batch_size: int, the total number of examples to run simultaneously", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_user(uid, name, password, channel=14, callback=False,\n                link_auth=True, ipmi_msg=True, privilege_level='administrator', **kwargs):\n    '''\n    \n    '''\n    with _IpmiCommand(**kwargs) as c:\n        return c.create_user(uid, name, password, channel, callback,\n                             link_auth, ipmi_msg, privilege_level)", "output": "create/ensure a user is created with provided settings.\n\n    :param privilege_level:\n        User Privilege Limit. (Determines the maximum privilege level that\n        the user is allowed to switch to on the specified channel.)\n        * callback\n        * user\n        * operator\n        * administrator\n        * proprietary\n        * no_access\n    :param kwargs:\n        - api_host=127.0.0.1\n        - api_user=admin\n        - api_pass=example\n        - api_port=623\n        - api_kg=None\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-call ipmi.create_user uid=2 name=steverweber api_host=172.168.0.7 api_pass=nevertell", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _pack_output(self, init_parameter):\n        \"\"\"\n        \"\"\"\n        output = {}\n        for i, param in enumerate(init_parameter):\n            output[self.key_order[i]] = param\n        return output", "output": "Pack the output\n\n        Parameters\n        ----------\n        init_parameter : dict\n\n        Returns\n        -------\n        output : dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _push_render(self):\n        \"\"\"\n        \"\"\"\n        bokeh.io.push_notebook(handle=self.handle)\n        self.last_update = time.time()", "output": "Render the plot with bokeh.io and push to notebook.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_optimizer_states(self, fname):\n        \"\"\"\n        \"\"\"\n        assert self.optimizer_initialized\n\n        if self._update_on_kvstore:\n            self._kvstore.save_optimizer_states(fname)\n        else:\n            with open(fname, 'wb') as fout:\n                fout.write(self._updater.get_states())", "output": "Saves optimizer (updater) state to a file.\n\n        Parameters\n        ----------\n        fname : str\n            Path to output states file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attach(zpool, device, new_device, force=False):\n    '''\n    \n\n    '''\n    ## Configure pool\n    # NOTE: initialize the defaults\n    flags = []\n    target = []\n\n    # NOTE: set extra config\n    if force:\n        flags.append('-f')\n\n    # NOTE: append the pool name and specifications\n    target.append(zpool)\n    target.append(device)\n    target.append(new_device)\n\n    ## Update storage pool\n    res = __salt__['cmd.run_all'](\n        __utils__['zfs.zpool_command'](\n            command='attach',\n            flags=flags,\n            target=target,\n        ),\n        python_shell=False,\n    )\n\n    ret = __utils__['zfs.parse_command_result'](res, 'attached')\n    if ret['attached']:\n        ## NOTE: lookup zpool status for vdev config\n        ret['vdevs'] = _clean_vdev_config(\n            __salt__['zpool.status'](zpool=zpool)[zpool]['config'][zpool],\n        )\n\n    return ret", "output": "Attach specified device to zpool\n\n    zpool : string\n        Name of storage pool\n\n    device : string\n        Existing device name too\n\n    new_device : string\n        New device name (to be attached to ``device``)\n\n    force : boolean\n        Forces use of device\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zpool.attach myzpool /path/to/vdev1 /path/to/vdev2 [...]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def edit(self, pk):\n        \"\"\"\"\"\"\n        resp = super(TableModelView, self).edit(pk)\n        if isinstance(resp, str):\n            return resp\n        return redirect('/superset/explore/table/{}/'.format(pk))", "output": "Simple hack to redirect to explore view after saving", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reindex_axis(self, new_index, axis, method=None, limit=None,\n                     fill_value=None, copy=True):\n        \"\"\"\n        \n        \"\"\"\n        new_index = ensure_index(new_index)\n        new_index, indexer = self.axes[axis].reindex(new_index, method=method,\n                                                     limit=limit)\n\n        return self.reindex_indexer(new_index, indexer, axis=axis,\n                                    fill_value=fill_value, copy=copy)", "output": "Conform block manager to new index.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_backward_end(self, iteration:int, **kwargs)->None:\n        \"\"\n        if iteration == 0: return\n        self._update_batches_if_needed()\n        #TODO:  This could perhaps be implemented as queues of requests instead but that seemed like overkill. \n        # But I'm not the biggest fan of maintaining these boolean flags either... Review pls.\n        if iteration % self.stats_iters == 0: self.gen_stats_updated, self.crit_stats_updated = False, False\n        if not (self.gen_stats_updated and self.crit_stats_updated): self._write_model_stats(iteration=iteration)", "output": "Callback function that writes backward end appropriate data to Tensorboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def print_markdown(data, title=None):\n    \"\"\"\n    \"\"\"\n    markdown = []\n    for key, value in data.items():\n        if isinstance(value, basestring_) and Path(value).exists():\n            continue\n        markdown.append(\"* **{}:** {}\".format(key, unicode_(value)))\n    if title:\n        print(\"\\n## {}\".format(title))\n    print(\"\\n{}\\n\".format(\"\\n\".join(markdown)))", "output": "Print data in GitHub-flavoured Markdown format for issues etc.\n\n    data (dict or list of tuples): Label/value pairs.\n    title (unicode or None): Title, will be rendered as headline 2.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_snapshots(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The list_snapshots function must be called with '\n            '-f or --function.'\n        )\n\n    ret = {}\n    vm_properties = [\n        \"name\",\n        \"rootSnapshot\",\n        \"snapshot\"\n    ]\n\n    vm_list = salt.utils.vmware.get_mors_with_properties(_get_si(), vim.VirtualMachine, vm_properties)\n\n    for vm in vm_list:\n        if vm[\"rootSnapshot\"]:\n            if kwargs and kwargs.get('name') == vm[\"name\"]:\n                return {vm[\"name\"]: _get_snapshots(vm[\"snapshot\"].rootSnapshotList)}\n            else:\n                ret[vm[\"name\"]] = _get_snapshots(vm[\"snapshot\"].rootSnapshotList)\n        else:\n            if kwargs and kwargs.get('name') == vm[\"name\"]:\n                return {}\n\n    return ret", "output": "List snapshots either for all VMs and templates or for a specific VM/template\n    in this VMware environment\n\n    To list snapshots for all VMs and templates:\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f list_snapshots my-vmware-config\n\n    To list snapshots for a specific VM/template:\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f list_snapshots my-vmware-config name=\"vmname\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wer_cer_batch(originals, results):\n    \n    \"\"\"\n    # The WER is calculated on word (and NOT on character) level.\n    # Therefore we split the strings into words first\n    assert len(originals) == len(results)\n\n    total_cer = 0.0\n    total_char_length = 0.0\n\n    total_wer = 0.0\n    total_word_length = 0.0\n\n    for original, result in zip(originals, results):\n        total_cer += levenshtein(original, result)\n        total_char_length += len(original)\n\n        total_wer += levenshtein(original.split(), result.split())\n        total_word_length += len(original.split())\n\n    return total_wer / total_word_length, total_cer / total_char_length", "output": "r\"\"\"\n    The WER is defined as the editing/Levenshtein distance on word level\n    divided by the amount of words in the original text.\n    In case of the original having more words (N) than the result and both\n    being totally different (all N words resulting in 1 edit operation each),\n    the WER will always be 1 (N / N = 1).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def preserve_shape(func):\n    \"\"\"\"\"\"\n    @wraps(func)\n    def wrapped_function(img, *args, **kwargs):\n        shape = img.shape\n        result = func(img, *args, **kwargs)\n        result = result.reshape(shape)\n        return result\n\n    return wrapped_function", "output": "Preserve shape of the image.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process(self, xp:PreProcessor=None, yp:PreProcessor=None, name:str=None):\n        \"\"\n        self.y.process(yp)\n        if getattr(self.y, 'filter_missing_y', False):\n            filt = array([o is None for o in self.y.items])\n            if filt.sum()>0:\n                #Warnings are given later since progress_bar might make them disappear.\n                self.warn = f\"You are labelling your items with {self.y.__class__.__name__}.\\n\"\n                self.warn += f\"Your {name} set contained the following unknown labels, the corresponding items have been discarded.\\n\"\n                for p in self.y.processor:\n                    if len(getattr(p, 'warns', [])) > 0:\n                        warnings = list(set(p.warns))\n                        self.warn += ', '.join(warnings[:5])\n                        if len(warnings) > 5: self.warn += \"...\"\n                    p.warns = []\n                self.x,self.y = self.x[~filt],self.y[~filt]\n        self.x.process(xp)\n        return self", "output": "Launch the processing on `self.x` and `self.y` with `xp` and `yp`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_highstate(queue=False, **kwargs):\n    '''\n    \n    '''\n    conflict = _check_queue(queue, kwargs)\n    if conflict is not None:\n        return conflict\n    pillar_override = kwargs.get('pillar')\n    pillar_enc = kwargs.get('pillar_enc')\n    if pillar_enc is None \\\n            and pillar_override is not None \\\n            and not isinstance(pillar_override, dict):\n        raise SaltInvocationError(\n            'Pillar data must be formatted as a dictionary, unless pillar_enc '\n            'is specified.'\n        )\n\n    opts = salt.utils.state.get_sls_opts(__opts__, **kwargs)\n    try:\n        st_ = salt.state.HighState(opts,\n                                   pillar_override,\n                                   pillar_enc=pillar_enc,\n                                   proxy=__proxy__,\n                                   initial_pillar=_get_initial_pillar(opts))\n    except NameError:\n        st_ = salt.state.HighState(opts,\n                                   pillar_override,\n                                   pillar_enc=pillar_enc,\n                                   initial_pillar=_get_initial_pillar(opts))\n\n    errors = _get_pillar_errors(kwargs, pillar=st_.opts['pillar'])\n    if errors:\n        __context__['retcode'] = salt.defaults.exitcodes.EX_PILLAR_FAILURE\n        raise CommandExecutionError('Pillar failed to render', info=errors)\n\n    st_.push_active()\n    try:\n        ret = st_.compile_highstate()\n    finally:\n        st_.pop_active()\n    _set_retcode(ret)\n    return ret", "output": "Retrieve the highstate data from the salt master and display it\n\n    Custom Pillar data can be passed with the ``pillar`` kwarg.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.show_highstate", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unique(self):\n        \"\"\"\n        \n        \"\"\"\n        from pandas import unique\n\n        uniques = unique(self.astype(object))\n        return self._from_sequence(uniques, dtype=self.dtype)", "output": "Compute the ExtensionArray of unique values.\n\n        Returns\n        -------\n        uniques : ExtensionArray", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def file_list(self, load):\n        '''\n        \n        '''\n        if 'env' in load:\n            # \"env\" is not supported; Use \"saltenv\".\n            load.pop('env')\n\n        ret = set()\n        if 'saltenv' not in load:\n            return []\n        if not isinstance(load['saltenv'], six.string_types):\n            load['saltenv'] = six.text_type(load['saltenv'])\n\n        for fsb in self.backends(load.pop('fsbackend', None)):\n            fstr = '{0}.file_list'.format(fsb)\n            if fstr in self.servers:\n                ret.update(self.servers[fstr](load))\n        # some *fs do not handle prefix. Ensure it is filtered\n        prefix = load.get('prefix', '').strip('/')\n        if prefix != '':\n            ret = [f for f in ret if f.startswith(prefix)]\n        return sorted(ret)", "output": "Return a list of files from the dominant environment", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_script(fname):\r\n    \"\"\"\"\"\"\r\n    text = os.linesep.join([\"# -*- coding: utf-8 -*-\", \"\", \"\"])\r\n    try:\r\n        encoding.write(to_text_string(text), fname, 'utf-8')\r\n    except EnvironmentError as error:\r\n        QMessageBox.critical(_(\"Save Error\"),\r\n                             _(\"<b>Unable to save file '%s'</b>\"\r\n                               \"<br><br>Error message:<br>%s\"\r\n                               ) % (osp.basename(fname), str(error)))", "output": "Create a new Python script", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode(self,\n             dataset_split=None,\n             decode_from_file=False,\n             checkpoint_path=None):\n    \"\"\"\"\"\"\n    if decode_from_file:\n      decoding.decode_from_file(self._estimator,\n                                self._decode_hparams.decode_from_file,\n                                self._hparams,\n                                self._decode_hparams,\n                                self._decode_hparams.decode_to_file)\n    else:\n      decoding.decode_from_dataset(\n          self._estimator,\n          self._hparams.problem.name,\n          self._hparams,\n          self._decode_hparams,\n          dataset_split=dataset_split,\n          checkpoint_path=checkpoint_path)", "output": "Decodes from dataset or file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find(max_depth=3):\n        \"\"\"\"\"\"\n        i = 0\n        for c, d, f in walk_up(os.getcwd()):\n            i += 1\n\n            if i < max_depth:\n                if 'Pipfile':\n                    p = os.path.join(c, 'Pipfile')\n                    if os.path.isfile(p):\n                        return p\n        raise RuntimeError('No Pipfile found!')", "output": "Returns the path of a Pipfile in parent directories.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_util_send_mail(msg, title, from_user, from_password, to_addr, smtp):\n    \"\"\"\n    \"\"\"\n\n    msg = MIMEText(msg, 'plain', 'utf-8')\n    msg['Subject'] = Header(title, 'utf-8').encode()\n\n    server = smtplib.SMTP(smtp, 25)  # SMTP\u534f\u8bae\u9ed8\u8ba4\u7aef\u53e3\u662f25\n    server.set_debuglevel(1)\n    server.login(from_user, from_password)\n    server.sendmail(from_user, [to_addr], msg.as_string())", "output": "\u90ae\u4ef6\u53d1\u9001\n    \n    Arguments:\n        msg {[type]} -- [description]\n        title {[type]} -- [description]\n        from_user {[type]} -- [description]\n        from_password {[type]} -- [description]\n        to_addr {[type]} -- [description]\n        smtp {[type]} -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def icon(self, icontype_or_qfileinfo):\r\n        \"\"\"\"\"\"\r\n        if isinstance(icontype_or_qfileinfo, QFileIconProvider.IconType):\r\n            return super(IconProvider, self).icon(icontype_or_qfileinfo)\r\n        else:\r\n            qfileinfo = icontype_or_qfileinfo\r\n            fname = osp.normpath(to_text_string(qfileinfo.absoluteFilePath()))\r\n            return ima.get_icon_by_extension(fname, scale_factor=1.0)", "output": "Reimplement Qt method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def focusInEvent(self, event):\n        \"\"\"\"\"\"\n        self.focus_changed.emit()\n        return super(PageControlWidget, self).focusInEvent(event)", "output": "Reimplement Qt method to send focus change notification", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n    \"\"\"\n    \n    \"\"\"\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        intersection = ((label == 1) & (pred == 1)).sum()\n        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n        if not union:\n            iou = EMPTY\n        else:\n            iou = float(intersection) / union\n        ious.append(iou)\n    iou = mean(ious)    # mean accross images if per_image\n    return 100 * iou", "output": "IoU for foreground class\n    binary: 1 foreground, 0 background", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def readMemory(buffer, size, URL, encoding, options):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlReadMemory(buffer, size, URL, encoding, options)\n    if ret is None:raise treeError('xmlReadMemory() failed')\n    return xmlDoc(_obj=ret)", "output": "parse an XML in-memory document and build a tree.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def isdicom(fn):\n    ''''''\n    fn = str(fn)\n    if fn.endswith('.dcm'):\n        return True\n    # Dicom signature from the dicom spec.\n    with open(fn,'rb') as fh:\n        fh.seek(0x80)\n        return fh.read(4)==b'DICM'", "output": "True if the fn points to a DICOM image", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_directory(self):\r\n        \"\"\"\"\"\"\r\n        self.__redirect_stdio_emit(False)\r\n        directory = getexistingdirectory(\r\n                self, _(\"\"), self.path)\r\n        if directory:\r\n            directory = to_unicode_from_fs(osp.abspath(directory))\r\n        self.__redirect_stdio_emit(True)\r\n        return directory", "output": "Select directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encoded(self):\n        \"\"\"\n        \n        \"\"\"\n        self.update_preferences()\n        fp = BytesIO()\n        zipped = zipfile.ZipFile(fp, 'w', zipfile.ZIP_DEFLATED)\n        path_root = len(self.path) + 1  # account for trailing slash\n        for base, dirs, files in os.walk(self.path):\n            for fyle in files:\n                filename = os.path.join(base, fyle)\n                zipped.write(filename, filename[path_root:])\n        zipped.close()\n        return base64.b64encode(fp.getvalue()).decode('UTF-8')", "output": "A zipped, base64 encoded string of profile directory\n        for use with remote WebDriver JSON wire protocol", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def volume_present(name, volume_size, sparse=False, create_parent=False, properties=None, cloned_from=None):\n    '''\n    \n\n    '''\n    return _dataset_present(\n        'volume',\n        name,\n        volume_size,\n        sparse=sparse,\n        create_parent=create_parent,\n        properties=properties,\n        cloned_from=cloned_from,\n    )", "output": "ensure volume exists and has properties set\n\n    name : string\n        name of volume\n    volume_size : string\n        size of volume\n    sparse : boolean\n        create sparse volume\n    create_parent : boolean\n        creates all the non-existing parent datasets.\n        any property specified on the command line using the -o option is ignored.\n    cloned_from : string\n        name of snapshot to clone\n    properties : dict\n        additional zfs properties (-o)\n\n    .. note::\n        ``cloned_from`` is only use if the volume does not exist yet,\n        when ``cloned_from`` is set after the volume exists it will be ignored.\n\n    .. note::\n        Properties do not get cloned, if you specify the properties in the state file\n        they will be applied on a subsequent run.\n\n        ``volume_size`` is considered a property, so the volume's size will be\n        corrected when the properties get updated if it differs from the\n        original volume.\n\n        The sparse parameter is ignored when using ``cloned_from``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_allow(allow):\n    '''\n    \n    '''\n    # input=> tcp:53,tcp:80,tcp:443,icmp,tcp:4201,udp:53\n    # output<= [\n    #     {\"IPProtocol\": \"tcp\", \"ports\": [\"53\",\"80\",\"443\",\"4201\"]},\n    #     {\"IPProtocol\": \"icmp\"},\n    #     {\"IPProtocol\": \"udp\", \"ports\": [\"53\"]},\n    # ]\n    seen_protos = {}\n    allow_dict = []\n    protocols = allow.split(',')\n    for p in protocols:\n        pairs = p.split(':')\n        if pairs[0].lower() not in ['tcp', 'udp', 'icmp']:\n            raise SaltCloudSystemExit(\n                'Unsupported protocol {0}. Must be tcp, udp, or icmp.'.format(\n                    pairs[0]\n                )\n            )\n        if len(pairs) == 1 or pairs[0].lower() == 'icmp':\n            seen_protos[pairs[0]] = []\n        else:\n            if pairs[0] not in seen_protos:\n                seen_protos[pairs[0]] = [pairs[1]]\n            else:\n                seen_protos[pairs[0]].append(pairs[1])\n    for k in seen_protos:\n        d = {'IPProtocol': k}\n        if seen_protos[k]:\n            d['ports'] = seen_protos[k]\n        allow_dict.append(d)\n    log.debug(\"firewall allowed protocols/ports: %s\", allow_dict)\n    return allow_dict", "output": "Convert firewall rule allowed user-string to specified REST API format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def search(connect_spec, base, scope='subtree', filterstr='(objectClass=*)',\n           attrlist=None, attrsonly=0):\n    '''\n    '''\n    l = connect(connect_spec)\n    scope = getattr(ldap, 'SCOPE_' + scope.upper())\n    try:\n        results = l.c.search_s(base, scope, filterstr, attrlist, attrsonly)\n    except ldap.NO_SUCH_OBJECT:\n        results = []\n    except ldap.LDAPError as e:\n        _convert_exception(e)\n    return dict(results)", "output": "Search an LDAP database.\n\n    :param connect_spec:\n        See the documentation for the ``connect_spec`` parameter for\n        :py:func:`connect`.\n\n    :param base:\n        Distinguished name of the entry at which to start the search.\n\n    :param scope:\n        One of the following:\n\n        * ``'subtree'``\n            Search the base and all of its descendants.\n\n        * ``'base'``\n            Search only the base itself.\n\n        * ``'onelevel'``\n            Search only the base's immediate children.\n\n    :param filterstr:\n        String representation of the filter to apply in the search.\n\n    :param attrlist:\n        Limit the returned attributes to those in the specified list.\n        If ``None``, all attributes of each entry are returned.\n\n    :param attrsonly:\n        If non-zero, don't return any attribute values.\n\n    :returns:\n        a dict of results.  The dict is empty if there are no results.\n        The dict maps each returned entry's distinguished name to a\n        dict that maps each of the matching attribute names to a list\n        of its values.\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt '*' ldap3.search \"{\n            'url': 'ldaps://ldap.example.com/',\n            'bind': {\n                'method': 'simple',\n                'dn': 'cn=admin,dc=example,dc=com',\n                'password': 'secret',\n            },\n        }\" \"base='dc=example,dc=com'\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _import_plugin(module_name, plugin_path, modnames, modlist):\r\n    \"\"\"\r\n    \"\"\"\r\n    if module_name in modnames:\r\n        return\r\n    try:\r\n        # First add a mock module with the LOCALEPATH attribute so that the\r\n        # helper method can find the locale on import\r\n        mock = _ModuleMock()\r\n        mock.LOCALEPATH = osp.join(plugin_path, module_name, 'locale')\r\n        sys.modules[module_name] = mock\r\n\r\n        if osp.isdir(osp.join(plugin_path, module_name)):\r\n            module = _import_module_from_path(module_name, plugin_path)\r\n        else:\r\n            module = None\r\n\r\n        # Then restore the actual loaded module instead of the mock\r\n        if module and getattr(module, 'PLUGIN_CLASS', False):\r\n            sys.modules[module_name] = module\r\n            modlist.append(module)\r\n            modnames.append(module_name)\r\n    except Exception:\r\n        sys.stderr.write(\"ERROR: 3rd party plugin import failed for \"\r\n                         \"`{0}`\\n\".format(module_name))\r\n        traceback.print_exc(file=sys.stderr)", "output": "Import the plugin `module_name` from `plugin_path`, add it to `modlist`\r\n    and adds its name to `modnames`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_style_colors(self, colors, kwds, col_num, label):\n        \"\"\"\n        \n        \"\"\"\n        style = None\n        if self.style is not None:\n            if isinstance(self.style, list):\n                try:\n                    style = self.style[col_num]\n                except IndexError:\n                    pass\n            elif isinstance(self.style, dict):\n                style = self.style.get(label, style)\n            else:\n                style = self.style\n\n        has_color = 'color' in kwds or self.colormap is not None\n        nocolor_style = style is None or re.match('[a-z]+', style) is None\n        if (has_color or self.subplots) and nocolor_style:\n            kwds['color'] = colors[col_num % len(colors)]\n        return style, kwds", "output": "Manage style and color based on column number and its label.\n        Returns tuple of appropriate style and kwds which \"color\" may be added.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def newer(self, source, target):\n        \"\"\"\n        \"\"\"\n        if not os.path.exists(source):\n            raise DistlibException(\"file '%r' does not exist\" %\n                                   os.path.abspath(source))\n        if not os.path.exists(target):\n            return True\n\n        return os.stat(source).st_mtime > os.stat(target).st_mtime", "output": "Tell if the target is newer than the source.\n\n        Returns true if 'source' exists and is more recently modified than\n        'target', or if 'source' exists and 'target' doesn't.\n\n        Returns false if both exist and 'target' is the same age or younger\n        than 'source'. Raise PackagingFileError if 'source' does not exist.\n\n        Note that this test is not very accurate: files created in the same\n        second will have the same \"age\".", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def concat(self, arrs:Collection[Tensor])->Tensor:\n        \"\"\n        return [torch.cat([l[si] for l in arrs], dim=1) for si in range_of(arrs[0])]", "output": "Concatenate the `arrs` along the batch dimension.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def penalize_boundary_complexity(shp, w=20, mask=None, C=0.5):\n  \"\"\"\n  \"\"\"\n  def inner(T):\n    arr = T(\"input\")\n\n    # print shp\n    if mask is None:\n      mask_ = np.ones(shp)\n      mask_[:, w:-w, w:-w] = 0\n    else:\n      mask_ = mask\n\n    blur = _tf_blur(arr, w=5)\n    diffs = (blur-arr)**2\n    diffs += 0.8*(arr-C)**2\n\n    return -tf.reduce_sum(diffs*mask_)\n  return inner", "output": "Encourage the boundaries of an image to have less variation and of color C.\n\n  Args:\n    shp: shape of T(\"input\") because this may not be known.\n    w: width of boundary to penalize. Ignored if mask is set.\n    mask: mask describing what area should be penalized.\n\n  Returns:\n    Objective.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_dict_key_value(\n        in_dict,\n        keys,\n        value,\n        delimiter=DEFAULT_TARGET_DELIM,\n        ordered_dict=False):\n    '''\n    \n    '''\n    dict_pointer, last_key = _dict_rpartition(in_dict,\n                                              keys,\n                                              delimiter=delimiter,\n                                              ordered_dict=ordered_dict)\n    if last_key not in dict_pointer or dict_pointer[last_key] is None:\n        dict_pointer[last_key] = OrderedDict() if ordered_dict else {}\n    try:\n        dict_pointer[last_key].update(value)\n    except AttributeError:\n        raise SaltInvocationError('The last key contains a {}, which cannot update.'\n                                  ''.format(type(dict_pointer[last_key])))\n    except (ValueError, TypeError):\n        raise SaltInvocationError('Cannot update {} with a {}.'\n                                  ''.format(type(dict_pointer[last_key]), type(value)))\n    return in_dict", "output": "Ensures that in_dict contains the series of recursive keys defined in keys.\n    Also updates the dict, that is at the end of `in_dict` traversed with `keys`,\n    with `value`.\n\n    :param dict in_dict: The dictionary to work with\n    :param str keys: The delimited string with one or more keys.\n    :param any value: The value to update the nested dict-key with.\n    :param str delimiter: The delimiter to use in `keys`. Defaults to ':'.\n    :param bool ordered_dict: Create OrderedDicts if keys are missing.\n                              Default: create regular dicts.\n\n    :return dict: Though it updates in_dict in-place.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_licenses(service_instance=None):\n    '''\n    \n    '''\n    log.trace('Retrieving all licenses')\n    licenses = salt.utils.vmware.get_licenses(service_instance)\n    ret_dict = [{'key': l.licenseKey,\n                 'name': l.name,\n                 'description': l.labels[0].value if l.labels else None,\n                 # VMware handles unlimited capacity as 0\n                 'capacity': l.total if l.total > 0 else sys.maxsize,\n                 'used': l.used if l.used else 0}\n                 for l in licenses]\n    return ret_dict", "output": "Lists all licenses on a vCenter.\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter/ESXi host.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.list_licenses", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _reconnect(self):\n        \"\"\"\"\"\"\n        self._wrapped = self._client.read_rows(\n            _copy_stream_position(self._position), **self._read_rows_kwargs\n        )", "output": "Reconnect to the ReadRows stream using the most recent offset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_floatingip(self, floatingip_id):\n        '''\n        \n        '''\n        ret = self.network_conn.delete_floatingip(floatingip_id)\n        return ret if ret else True", "output": "Deletes the specified floatingip", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_interval(self, interval):\n        \"\"\"\"\"\"\n        self._interval = interval\n        if self.timer is not None:\n            self.timer.setInterval(interval)", "output": "Set timer interval (ms).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait_for_compute_zone_operation(compute, project_name, operation, zone):\n    \"\"\"\"\"\"\n    logger.info(\"wait_for_compute_zone_operation: \"\n                \"Waiting for operation {} to finish...\".format(\n                    operation[\"name\"]))\n\n    for _ in range(MAX_POLLS):\n        result = compute.zoneOperations().get(\n            project=project_name, operation=operation[\"name\"],\n            zone=zone).execute()\n        if \"error\" in result:\n            raise Exception(result[\"error\"])\n\n        if result[\"status\"] == \"DONE\":\n            logger.info(\"wait_for_compute_zone_operation: \"\n                        \"Operation {} finished.\".format(operation[\"name\"]))\n            break\n\n        time.sleep(POLL_INTERVAL)\n\n    return result", "output": "Poll for compute zone operation until finished.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __get_location(conn, vm_):\n    '''\n    \n    '''\n    location = config.get_cloud_config_value(\n        'location', vm_, __opts__)\n    return conn.ex_get_zone(location)", "output": "Need to override libcloud to find the zone.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_chrdev(name):\n    '''\n    \n    '''\n    name = os.path.expanduser(name)\n\n    stat_structure = None\n    try:\n        stat_structure = os.stat(name)\n    except OSError as exc:\n        if exc.errno == errno.ENOENT:\n            # If the character device does not exist in the first place\n            return False\n        else:\n            raise\n    return stat.S_ISCHR(stat_structure.st_mode)", "output": "Check if a file exists and is a character device.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n       salt '*' file.is_chrdev /dev/chr", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(name, runas=None):\n    '''\n    \n    '''\n    return prlctl('start', salt.utils.data.decode(name), runas=runas)", "output": "Start a VM\n\n    :param str name:\n        Name/ID of VM to start\n\n    :param str runas:\n        The user that the prlctl command will be run as\n\n    Example:\n\n    .. code-block:: bash\n\n        salt '*' parallels.start macvm runas=macdev", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_args():\n  \"\"\"\"\"\"\n  parser = argparse.ArgumentParser(\n      description='Tool to run attacks and defenses.')\n  parser.add_argument('--attacks_dir', required=True,\n                      help='Location of all attacks.')\n  parser.add_argument('--targeted_attacks_dir', required=True,\n                      help='Location of all targeted attacks.')\n  parser.add_argument('--defenses_dir', required=True,\n                      help='Location of all defenses.')\n  parser.add_argument('--dataset_dir', required=True,\n                      help='Location of the dataset.')\n  parser.add_argument('--dataset_metadata', required=True,\n                      help='Location of the dataset metadata.')\n  parser.add_argument('--intermediate_results_dir', required=True,\n                      help='Directory to store intermediate results.')\n  parser.add_argument('--output_dir', required=True,\n                      help=('Output directory.'))\n  parser.add_argument('--epsilon', required=False, type=int, default=16,\n                      help='Maximum allowed size of adversarial perturbation')\n  parser.add_argument('--gpu', dest='use_gpu', action='store_true')\n  parser.add_argument('--nogpu', dest='use_gpu', action='store_false')\n  parser.set_defaults(use_gpu=False)\n  parser.add_argument('--save_all_classification',\n                      dest='save_all_classification', action='store_true')\n  parser.add_argument('--nosave_all_classification',\n                      dest='save_all_classification', action='store_false')\n  parser.set_defaults(save_all_classification=False)\n  return parser.parse_args()", "output": "Parses command line arguments.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send(self, load, tries=3, timeout=60, raw=False):\n        '''\n        \n        '''\n        if self.crypt == 'clear':\n            ret = yield self._uncrypted_transfer(load, tries=tries, timeout=timeout)\n        else:\n            ret = yield self._crypted_transfer(load, tries=tries, timeout=timeout, raw=raw)\n        raise tornado.gen.Return(ret)", "output": "Send a request, return a future which will complete when we send the message", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_dhcp_options(dhcp_options_id=None, dhcp_options_name=None,\n                        region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    return _delete_resource(resource='dhcp_options',\n                            name=dhcp_options_name,\n                            resource_id=dhcp_options_id,\n                            region=region, key=key,\n                            keyid=keyid, profile=profile)", "output": "Delete dhcp options by id or name.\n\n    .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_vpc.delete_dhcp_options 'dopt-b6a247df'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_image(kwargs, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The show_image function must be called with -f or --function.'\n        )\n\n    items = query(action='template', command=kwargs['image'])\n    if 'error' in items:\n        return items['error']\n\n    ret = {}\n    for item in items:\n        ret.update({item.attrib['name']: item.attrib})\n\n    return ret", "output": "Show the details from Parallels concerning an image", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_run(command, args, three=None, python=False, pypi_mirror=None):\n    \"\"\"\n    \"\"\"\n    from .cmdparse import ScriptEmptyError\n\n    # Ensure that virtualenv is available.\n    ensure_project(\n        three=three, python=python, validate=False, pypi_mirror=pypi_mirror,\n    )\n\n    load_dot_env()\n\n    previous_pip_shims_module = os.environ.pop(\"PIP_SHIMS_BASE_MODULE\", None)\n\n    # Activate virtualenv under the current interpreter's environment\n    inline_activate_virtual_environment()\n\n    # Set an environment variable, so we know we're in the environment.\n    # Only set PIPENV_ACTIVE after finishing reading virtualenv_location\n    # such as in inline_activate_virtual_environment\n    # otherwise its value will be changed\n    previous_pipenv_active_value = os.environ.get(\"PIPENV_ACTIVE\")\n    os.environ[\"PIPENV_ACTIVE\"] = vistir.misc.fs_str(\"1\")\n\n    try:\n        script = project.build_script(command, args)\n        cmd_string = ' '.join([script.command] + script.args)\n        if environments.is_verbose():\n            click.echo(crayons.normal(\"$ {0}\".format(cmd_string)), err=True)\n    except ScriptEmptyError:\n        click.echo(\"Can't run script {0!r}-it's empty?\", err=True)\n    run_args = [script]\n    run_kwargs = {}\n    if os.name == \"nt\":\n        run_fn = do_run_nt\n    else:\n        run_fn = do_run_posix\n        run_kwargs = {\"command\": command}\n    try:\n        run_fn(*run_args, **run_kwargs)\n    finally:\n        os.environ.pop(\"PIPENV_ACTIVE\", None)\n        if previous_pipenv_active_value is not None:\n            os.environ[\"PIPENV_ACTIVE\"] = previous_pipenv_active_value\n        if previous_pip_shims_module is not None:\n            os.environ[\"PIP_SHIMS_BASE_MODULE\"] = previous_pip_shims_module", "output": "Attempt to run command either pulling from project or interpreting as executable.\n\n    Args are appended to the command in [scripts] section of project if found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe_alias(FunctionName, Name, region=None, key=None,\n                   keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        alias = _find_alias(FunctionName, Name,\n                            region=region, key=key, keyid=keyid, profile=profile)\n        if alias:\n            keys = ('AliasArn', 'Name', 'FunctionVersion', 'Description')\n            return {'alias': dict([(k, alias.get(k)) for k in keys])}\n        else:\n            return {'alias': None}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "Given a function name and alias name describe the properties of the alias.\n\n    Returns a dictionary of interesting properties.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_lambda.describe_alias myalias", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sync_beacons(saltenv=None, refresh=True, extmod_whitelist=None, extmod_blacklist=None):\n    '''\n    \n    '''\n    ret = _sync('beacons', saltenv, extmod_whitelist, extmod_blacklist)\n    if refresh:\n        refresh_beacons()\n    return ret", "output": ".. versionadded:: 2015.5.1\n\n    Sync beacons from ``salt://_beacons`` to the minion\n\n    saltenv\n        The fileserver environment from which to sync. To sync from more than\n        one environment, pass a comma-separated list.\n\n        If not passed, then all environments configured in the :ref:`top files\n        <states-top>` will be checked for beacons to sync. If no top files are\n        found, then the ``base`` environment will be synced.\n\n    refresh : True\n        If ``True``, refresh the available beacons on the minion. This refresh\n        will be performed even if no new beacons are synced. Set to ``False``\n        to prevent this refresh.\n\n    extmod_whitelist : None\n        comma-seperated list of modules to sync\n\n    extmod_blacklist : None\n        comma-seperated list of modules to blacklist based on type\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.sync_beacons\n        salt '*' saltutil.sync_beacons saltenv=dev\n        salt '*' saltutil.sync_beacons saltenv=base,dev", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_hparams_from_args(args):\n  \"\"\"\"\"\"\n  if not args:\n    return\n\n  hp_prefix = \"--hp_\"\n  tf.logging.info(\"Found unparsed command-line arguments. Checking if any \"\n                  \"start with %s and interpreting those as hparams \"\n                  \"settings.\", hp_prefix)\n\n  pairs = []\n  i = 0\n  while i < len(args):\n    arg = args[i]\n    if arg.startswith(hp_prefix):\n      pairs.append((arg[len(hp_prefix):], args[i+1]))\n      i += 2\n    else:\n      tf.logging.warn(\"Found unknown flag: %s\", arg)\n      i += 1\n\n  as_hparams = \",\".join([\"%s=%s\" % (key, val) for key, val in pairs])\n  if FLAGS.hparams:\n    as_hparams = \",\" + as_hparams\n  FLAGS.hparams += as_hparams", "output": "Set hparams overrides from unparsed args list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def export(self, path, _sentinel=None,  # pylint: disable=invalid-name\n             checkpoint_path=None, name_transform_fn=None):\n    \"\"\"\n    \"\"\"\n    from tensorflow_hub.module import export_module_spec  # pylint: disable=g-import-not-at-top\n    if not checkpoint_path:\n      raise ValueError(\"Missing mandatory `checkpoint_path` parameter\")\n    name_transform_fn = name_transform_fn or (lambda x: x)\n    export_module_spec(self, path, checkpoint_path, name_transform_fn)", "output": "Exports a ModuleSpec with weights taken from a checkpoint.\n\n    This is an helper to export modules directly from a ModuleSpec\n    without having to create a session and set the variables to the\n    intended values.\n\n    Example usage:\n\n    ```python\n    spec = hub.create_module_spec(module_fn)\n    spec.export(\"/path/to/export_module\",\n                checkpoint_path=\"/path/to/training_model\")\n    ```\n\n    In some cases, the variable name in the checkpoint does not match\n    the variable name in the module. It is possible to work around that\n    by providing a checkpoint_map_fn that performs the variable mapping.\n    For example with: `name_transform_fn = lambda x: \"extra_scope/\" + x`.\n\n    Args:\n      path: path where to export the module to.\n      _sentinel: used to prevent positional arguments besides `path`.\n      checkpoint_path: path where to load the weights for the module.\n        Mandatory parameter and must be passed by name.\n      name_transform_fn: optional function to provide mapping between\n        variable name in the module and the variable name in the checkpoint.\n\n    Raises:\n      ValueError: if missing mandatory `checkpoint_path` parameter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def AddBookkeepingOperators(model):\n    \"\"\"\n    \"\"\"\n    # Print basically prints out the content of the blob. to_file=1 routes the\n    # printed output to a file. The file is going to be stored under\n    #     root_folder/[blob name]\n    model.Print('accuracy', [], to_file=1)\n    model.Print('loss', [], to_file=1)\n    # Summarizes the parameters. Different from Print, Summarize gives some\n    # statistics of the parameter, such as mean, std, min and max.\n    for param in model.params:\n        model.Summarize(param, [], to_file=1)\n        model.Summarize(model.param_to_grad[param], [], to_file=1)", "output": "This adds a few bookkeeping operators that we can inspect later.\n\n    These operators do not affect the training procedure: they only collect\n    statistics and prints them to file or to logs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fn_with_diet_vars(params):\n  \"\"\"\"\"\"\n  params = copy.copy(params)\n\n  def dec(fn):\n\n    def wrapped(*args):\n      return _fn_with_diet_vars(fn, args, params)\n\n    return wrapped\n\n  return dec", "output": "Decorator for graph-building function to use diet variables.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_zipfile(base_name, base_dir, verbose=0, dry_run=0, logger=None):\n    \"\"\"\n    \"\"\"\n    zip_filename = base_name + \".zip\"\n    archive_dir = os.path.dirname(base_name)\n\n    if not os.path.exists(archive_dir):\n        if logger is not None:\n            logger.info(\"creating %s\", archive_dir)\n        if not dry_run:\n            os.makedirs(archive_dir)\n\n    # If zipfile module is not available, try spawning an external 'zip'\n    # command.\n    try:\n        import zipfile\n    except ImportError:\n        zipfile = None\n\n    if zipfile is None:\n        _call_external_zip(base_dir, zip_filename, verbose, dry_run)\n    else:\n        if logger is not None:\n            logger.info(\"creating '%s' and adding '%s' to it\",\n                        zip_filename, base_dir)\n\n        if not dry_run:\n            zip = zipfile.ZipFile(zip_filename, \"w\",\n                                  compression=zipfile.ZIP_DEFLATED)\n\n            for dirpath, dirnames, filenames in os.walk(base_dir):\n                for name in filenames:\n                    path = os.path.normpath(os.path.join(dirpath, name))\n                    if os.path.isfile(path):\n                        zip.write(path, path)\n                        if logger is not None:\n                            logger.info(\"adding '%s'\", path)\n            zip.close()\n\n    return zip_filename", "output": "Create a zip file from all the files under 'base_dir'.\n\n    The output zip file will be named 'base_name' + \".zip\".  Uses either the\n    \"zipfile\" Python module (if available) or the InfoZIP \"zip\" utility\n    (if installed and found on the default search path).  If neither tool is\n    available, raises ExecError.  Returns the name of the output zip\n    file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hover(self):\n        ''' \n\n        '''\n        hovers = [obj for obj in self.tools if isinstance(obj, HoverTool)]\n        return _list_attr_splat(hovers)", "output": "Splattable list of :class:`~bokeh.models.tools.HoverTool` objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def recursive_walk(rootdir):\n    \"\"\"\n    \n    \"\"\"\n    for r, dirs, files in os.walk(rootdir):\n        for f in files:\n            yield os.path.join(r, f)", "output": "Yields:\n        str: All files in rootdir, recursively.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def go_to_next_cell(self):\r\n        \"\"\"\"\"\"\r\n        cursor = self.textCursor()\r\n        cursor.movePosition(QTextCursor.NextBlock)\r\n        cur_pos = prev_pos = cursor.position()\r\n        while not self.is_cell_separator(cursor):\r\n            # Moving to the next code cell\r\n            cursor.movePosition(QTextCursor.NextBlock)\r\n            prev_pos = cur_pos\r\n            cur_pos = cursor.position()\r\n            if cur_pos == prev_pos:\r\n                return\r\n        self.setTextCursor(cursor)", "output": "Go to the next cell of lines", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def retention_policy_effective_time(self):\n        \"\"\"\n        \"\"\"\n        policy = self._properties.get(\"retentionPolicy\")\n        if policy is not None:\n            timestamp = policy.get(\"effectiveTime\")\n            if timestamp is not None:\n                return _rfc3339_to_datetime(timestamp)", "output": "Retrieve the effective time of the bucket's retention policy.\n\n        :rtype: datetime.datetime or ``NoneType``\n        :returns: point-in time at which the bucket's retention policy is\n                  effective, or ``None`` if the property is not\n                  set locally.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render_variable(env, raw, cookiecutter_dict):\n    \"\"\"\n    \"\"\"\n    if raw is None:\n        return None\n    elif isinstance(raw, dict):\n        return {\n            render_variable(env, k, cookiecutter_dict):\n                render_variable(env, v, cookiecutter_dict)\n            for k, v in raw.items()\n        }\n    elif isinstance(raw, list):\n        return [\n            render_variable(env, v, cookiecutter_dict)\n            for v in raw\n        ]\n    elif not isinstance(raw, basestring):\n        raw = str(raw)\n\n    template = env.from_string(raw)\n\n    rendered_template = template.render(cookiecutter=cookiecutter_dict)\n    return rendered_template", "output": "Inside the prompting taken from the cookiecutter.json file, this renders\n    the next variable. For example, if a project_name is \"Peanut Butter\n    Cookie\", the repo_name could be be rendered with:\n\n        `{{ cookiecutter.project_name.replace(\" \", \"_\") }}`.\n\n    This is then presented to the user as the default.\n\n    :param Environment env: A Jinja2 Environment object.\n    :param str raw: The next value to be prompted for by the user.\n    :param dict cookiecutter_dict: The current context as it's gradually\n        being populated with variables.\n    :return: The rendered value for the default variable.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _raw_hex_id(obj):\n    \"\"\"\"\"\"\n    # interpret as a pointer since that's what really what id returns\n    packed = struct.pack('@P', id(obj))\n    return ''.join(map(_replacer, packed))", "output": "Return the padded hexadecimal id of ``obj``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_plugin_settings(self, options):\r\n        \"\"\"\"\"\"\r\n        color_scheme_n = 'color_scheme_name'\r\n        color_scheme_o = self.get_color_scheme()\r\n        connect_n = 'connect_to_oi'\r\n        wrap_n = 'wrap'\r\n        wrap_o = self.get_option(wrap_n)\r\n        self.wrap_action.setChecked(wrap_o)\r\n        math_n = 'math'\r\n        math_o = self.get_option(math_n)\r\n\r\n        if color_scheme_n in options:\r\n            self.set_plain_text_color_scheme(color_scheme_o)\r\n        if wrap_n in options:\r\n            self.toggle_wrap_mode(wrap_o)\r\n        if math_n in options:\r\n            self.toggle_math_mode(math_o)\r\n\r\n        # To make auto-connection changes take place instantly\r\n        self.main.editor.apply_plugin_settings(options=[connect_n])\r\n        self.main.ipyconsole.apply_plugin_settings(options=[connect_n])", "output": "Apply configuration file's plugin settings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        '''\n        \n        '''\n        context = zmq.Context()\n        # the socket for outgoing timer events\n        socket = context.socket(zmq.PUB)\n        socket.setsockopt(zmq.LINGER, 100)\n        socket.bind('ipc://' + self.timer_sock)\n\n        count = 0\n        log.debug('ConCache-Timer started')\n        while not self.stopped.wait(1):\n            socket.send(self.serial.dumps(count))\n\n            count += 1\n            if count >= 60:\n                count = 0", "output": "main loop that fires the event every second", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_etag_header(self) -> bool:\n        \"\"\"\n        \"\"\"\n        computed_etag = utf8(self._headers.get(\"Etag\", \"\"))\n        # Find all weak and strong etag values from If-None-Match header\n        # because RFC 7232 allows multiple etag values in a single header.\n        etags = re.findall(\n            br'\\*|(?:W/)?\"[^\"]*\"', utf8(self.request.headers.get(\"If-None-Match\", \"\"))\n        )\n        if not computed_etag or not etags:\n            return False\n\n        match = False\n        if etags[0] == b\"*\":\n            match = True\n        else:\n            # Use a weak comparison when comparing entity-tags.\n            def val(x: bytes) -> bytes:\n                return x[2:] if x.startswith(b\"W/\") else x\n\n            for etag in etags:\n                if val(etag) == val(computed_etag):\n                    match = True\n                    break\n        return match", "output": "Checks the ``Etag`` header against requests's ``If-None-Match``.\n\n        Returns ``True`` if the request's Etag matches and a 304 should be\n        returned. For example::\n\n            self.set_etag_header()\n            if self.check_etag_header():\n                self.set_status(304)\n                return\n\n        This method is called automatically when the request is finished,\n        but may be called earlier for applications that override\n        `compute_etag` and want to do an early check for ``If-None-Match``\n        before completing the request.  The ``Etag`` header should be set\n        (perhaps with `set_etag_header`) before calling this method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _property(methode, zone, key, value):\n    '''\n    \n\n    '''\n    ret = {'status': True}\n\n    # generate update script\n    cfg_file = None\n    if methode not in ['set', 'clear']:\n        ret['status'] = False\n        ret['message'] = 'unkown methode {0}!'.format(methode)\n    else:\n        cfg_file = salt.utils.files.mkstemp()\n        with salt.utils.files.fpopen(cfg_file, 'w+', mode=0o600) as fp_:\n            if methode == 'set':\n                if isinstance(value, dict) or isinstance(value, list):\n                    value = _sanitize_value(value)\n                value = six.text_type(value).lower() if isinstance(value, bool) else six.text_type(value)\n                fp_.write(\"{0} {1}={2}\\n\".format(methode, key, _sanitize_value(value)))\n            elif methode == 'clear':\n                fp_.write(\"{0} {1}\\n\".format(methode, key))\n\n    # update property\n    if cfg_file:\n        _dump_cfg(cfg_file)\n        res = __salt__['cmd.run_all']('zonecfg -z {zone} -f {path}'.format(\n            zone=zone,\n            path=cfg_file,\n        ))\n        ret['status'] = res['retcode'] == 0\n        ret['message'] = res['stdout'] if ret['status'] else res['stderr']\n        if ret['message'] == '':\n            del ret['message']\n        else:\n            ret['message'] = _clean_message(ret['message'])\n\n        # cleanup config file\n        if __salt__['file.file_exists'](cfg_file):\n            __salt__['file.remove'](cfg_file)\n\n    return ret", "output": "internal handler for set and clear_property\n\n    methode : string\n        either set, add, or clear\n    zone : string\n        name of zone\n    key : string\n        name of property\n    value : string\n        value of property", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def instance_config_path(cls, project, instance_config):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/instanceConfigs/{instance_config}\",\n            project=project,\n            instance_config=instance_config,\n        )", "output": "Return a fully-qualified instance_config string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _localectl_set(locale=''):\n    '''\n    \n    '''\n    locale_params = _parse_dbus_locale() if dbus is not None else _localectl_status().get('system_locale', {})\n    locale_params['LANG'] = six.text_type(locale)\n    args = ' '.join(['{0}=\"{1}\"'.format(k, v) for k, v in six.iteritems(locale_params) if v is not None])\n    return not __salt__['cmd.retcode']('localectl set-locale {0}'.format(args), python_shell=False)", "output": "Use systemd's localectl command to set the LANG locale parameter, making\n    sure not to trample on other params that have been set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def listdir(self, path):\n        \"\"\"\n        \n        \"\"\"\n        bucket, obj = self._path_to_bucket_and_key(path)\n\n        obj_prefix = self._add_path_delimiter(obj)\n        if self._is_root(obj_prefix):\n            obj_prefix = ''\n\n        obj_prefix_len = len(obj_prefix)\n        for it in self._list_iter(bucket, obj_prefix):\n            yield self._add_path_delimiter(path) + it['name'][obj_prefix_len:]", "output": "Get an iterable with GCS folder contents.\n        Iterable contains paths relative to queried path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def takes_kwargs(obj) -> bool:\n    \"\"\"\n    \n    \"\"\"\n    if inspect.isclass(obj):\n        signature = inspect.signature(obj.__init__)\n    elif inspect.ismethod(obj) or inspect.isfunction(obj):\n        signature = inspect.signature(obj)\n    else:\n        raise ConfigurationError(f\"object {obj} is not callable\")\n    return bool(any([p.kind == inspect.Parameter.VAR_KEYWORD  # type: ignore\n                     for p in signature.parameters.values()]))", "output": "Checks whether a provided object takes in any positional arguments.\n    Similar to takes_arg, we do this for both the __init__ function of\n    the class or a function / method\n    Otherwise, we raise an error", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _import_status(data, item, repo_name, repo_tag):\n    '''\n    \n    '''\n    status = item['status']\n    try:\n        if 'Downloading from' in status:\n            return\n        elif all(x in string.hexdigits for x in status):\n            # Status is an image ID\n            data['Image'] = '{0}:{1}'.format(repo_name, repo_tag)\n            data['Id'] = status\n    except (AttributeError, TypeError):\n        pass", "output": "Process a status update from docker import, updating the data structure", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clean_pipeline_string(self, individual):\n        \"\"\"\n\n        \"\"\"\n        dirty_string = str(individual)\n        # There are many parameter prefixes in the pipeline strings, used solely for\n        # making the terminal name unique, eg. LinearSVC__.\n        parameter_prefixes = [(m.start(), m.end()) for m in re.finditer(', [\\w]+__', dirty_string)]\n        # We handle them in reverse so we do not mess up indices\n        pretty = dirty_string\n        for (start, end) in reversed(parameter_prefixes):\n            pretty = pretty[:start + 2] + pretty[end:]\n\n        return pretty", "output": "Provide a string of the individual without the parameter prefixes.\n\n        Parameters\n        ----------\n        individual: individual\n            Individual which should be represented by a pretty string\n\n        Returns\n        -------\n        A string like str(individual), but with parameter prefixes removed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _try_inject_s3_credentials(url):\n    \"\"\"\n    \n    \"\"\"\n    assert url.startswith('s3://')\n    path = url[5:]\n    # Check if the path already contains credentials\n    tokens = path.split(':')\n    # If there are two ':', its possible that we have already injected credentials\n    if len(tokens) == 3:\n        # Edge case: there are exactly two ':'s in the object key which is a false alarm.\n        # We prevent this by checking that '/' is not in the assumed key and id.\n        if ('/' not in tokens[0]) and ('/' not in tokens[1]):\n            return url\n\n    # S3 url does not contain secret key/id pair, query the environment variables\n    (k, v) = _get_aws_credentials()\n    return 's3://' + k + ':' + v + ':' + path", "output": "Inject aws credentials into s3 url as s3://[aws_id]:[aws_key]:[bucket/][objectkey]\n\n    If s3 url already contains secret key/id pairs, just return as is.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keep_alive(self, val: bool) -> None:\n        \"\"\"\n        \"\"\"\n        self._keepalive = val\n        if self._keepalive_handle:\n            self._keepalive_handle.cancel()\n            self._keepalive_handle = None", "output": "Set keep-alive connection mode.\n\n        :param bool val: new state.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace(zpool, old_device, new_device=None, force=False):\n    '''\n    \n\n    '''\n    ## Configure pool\n    # NOTE: initialize the defaults\n    flags = []\n    target = []\n\n    # NOTE: set extra config\n    if force:\n        flags.append('-f')\n\n    # NOTE: append the pool name and specifications\n    target.append(zpool)\n    target.append(old_device)\n    if new_device:\n        target.append(new_device)\n\n    ## Replace device\n    res = __salt__['cmd.run_all'](\n        __utils__['zfs.zpool_command'](\n            command='replace',\n            flags=flags,\n            target=target,\n        ),\n        python_shell=False,\n    )\n\n    ret = __utils__['zfs.parse_command_result'](res, 'replaced')\n    if ret['replaced']:\n        ## NOTE: lookup zpool status for vdev config\n        ret['vdevs'] = _clean_vdev_config(\n            __salt__['zpool.status'](zpool=zpool)[zpool]['config'][zpool],\n        )\n\n    return ret", "output": "Replaces ``old_device`` with ``new_device``\n\n    .. note::\n\n        This is equivalent to attaching ``new_device``,\n        waiting for it to resilver, and then detaching ``old_device``.\n\n        The size of ``new_device`` must be greater than or equal to the minimum\n        size of all the devices in a mirror or raidz configuration.\n\n    zpool : string\n        Name of storage pool\n\n    old_device : string\n        Old device to replace\n\n    new_device : string\n        Optional new device\n\n    force : boolean\n        Forces use of new_device, even if its appears to be in use.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zpool.replace myzpool /path/to/vdev1 /path/to/vdev2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def split_input(img):\n    \"\"\"\n    \n    \"\"\"\n    # split the image into left + right pairs\n    s = img.shape[0]\n    assert img.shape[1] == 2 * s\n    input, output = img[:, :s, :], img[:, s:, :]\n    if args.mode == 'BtoA':\n        input, output = output, input\n    if IN_CH == 1:\n        input = cv2.cvtColor(input, cv2.COLOR_RGB2GRAY)[:, :, np.newaxis]\n    if OUT_CH == 1:\n        output = cv2.cvtColor(output, cv2.COLOR_RGB2GRAY)[:, :, np.newaxis]\n    return [input, output]", "output": "img: an RGB image of shape (s, 2s, 3).\n    :return: [input, output]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _key_tab(self):\r\n        \"\"\"\"\"\"\r\n        if self.is_cursor_on_last_line():\r\n            empty_line = not self.get_current_line_to_cursor().strip()\r\n            if empty_line:\r\n                self.stdkey_tab()\r\n            else:\r\n                self.show_code_completion()", "output": "Action for TAB key", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_char_input(data, char_dict, max_char_length):\n    '''\n    \n    '''\n    batch_size = len(data)\n    sequence_length = max(len(d) for d in data)\n    char_id = np.zeros((max_char_length, sequence_length,\n                        batch_size), dtype=np.int32)\n    char_lengths = np.zeros((sequence_length, batch_size), dtype=np.float32)\n    for batch_idx in range(0, min(len(data), batch_size)):\n        batch_data = data[batch_idx]\n        for sample_idx in range(0, min(len(batch_data), sequence_length)):\n            word = batch_data[sample_idx]['word']\n            char_lengths[sample_idx, batch_idx] = min(\n                len(word), max_char_length)\n            for i in range(0, min(len(word), max_char_length)):\n                char_id[i, sample_idx, batch_idx] = get_id(char_dict, word[i])\n    return char_id, char_lengths", "output": "Get char input.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def runner():\n    '''\n    \n    '''\n    client = salt.runner.RunnerClient(__opts__)\n    ret = client.get_docs()\n    return ret", "output": "Return all inline documentation for runner modules\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run doc.runner", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_model(name):\n    \"\"\"\n    \"\"\"\n    if name not in models_map:\n        candidates = filter(lambda key: name in key, models_map.keys())\n        candidates_string = \", \".join(candidates)\n        raise ValueError(\n            \"No network named {}. Did you mean one of {}?\".format(\n                name, candidates_string\n            )\n        )\n\n    model_class = models_map[name]\n    model = model_class()\n    return model", "output": "Returns a model instance such as `model = vision_models.InceptionV1()`.\n    In the future may be expanded to filter by additional criteria, such as\n    architecture, dataset, and task the model was trained on.\n    Args:\n      name: The name of the model, as given by the class name in vision_models.\n    Returns:\n      An instantiated Model class with the requested model. Users still need to\n      manually `load_graphdef` on the return value, and manually import this\n      model's graph into their current graph.\n    Raises:\n      ValueError: If network `name` is not recognized.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self, other_info, graph, metric_value, model_id):\n        \"\"\" \n        \"\"\"\n        father_id = other_info\n        self.bo.fit([graph.extract_descriptor()], [metric_value])\n        self.bo.add_child(father_id, model_id)", "output": "Update the controller with evaluation result of a neural architecture.\n\n        Parameters\n        ----------\n        other_info: any object\n            In our case it is the father ID in the search tree.\n        graph: Graph\n            An instance of Graph. The trained neural architecture.\n        metric_value: float\n            The final evaluated metric value.\n        model_id: int", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def with_base_config(base_config, extra_config):\n    \"\"\"\"\"\"\n\n    config = copy.deepcopy(base_config)\n    config.update(extra_config)\n    return config", "output": "Returns the given config dict merged with a base agent conf.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _split_generators(self, dl_manager):\n    \"\"\"\"\"\"\n    # Download the full MNIST Database\n    filenames = {\n        \"train_data\": _MNIST_TRAIN_DATA_FILENAME,\n        \"train_labels\": _MNIST_TRAIN_LABELS_FILENAME,\n        \"test_data\": _MNIST_TEST_DATA_FILENAME,\n        \"test_labels\": _MNIST_TEST_LABELS_FILENAME,\n    }\n    mnist_files = dl_manager.download_and_extract(\n        {k: urllib.parse.urljoin(self.URL, v) for k, v in filenames.items()})\n\n    # MNIST provides TRAIN and TEST splits, not a VALIDATION split, so we only\n    # write the TRAIN and TEST splits to disk.\n    return [\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TRAIN,\n            num_shards=10,\n            gen_kwargs=dict(\n                num_examples=_TRAIN_EXAMPLES,\n                data_path=mnist_files[\"train_data\"],\n                label_path=mnist_files[\"train_labels\"],\n            )),\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TEST,\n            num_shards=1,\n            gen_kwargs=dict(\n                num_examples=_TEST_EXAMPLES,\n                data_path=mnist_files[\"test_data\"],\n                label_path=mnist_files[\"test_labels\"],\n            )),\n    ]", "output": "Returns SplitGenerators.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def label_empty(self, **kwargs):\n        \"\"\n        kwargs['label_cls'] = EmptyLabelList\n        return self.label_from_func(func=lambda o: 0., **kwargs)", "output": "Label every item with an `EmptyLabel`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def help_option(*param_decls, **attrs):\n    \"\"\"\n    \"\"\"\n    def decorator(f):\n        def callback(ctx, param, value):\n            if value and not ctx.resilient_parsing:\n                echo(ctx.get_help(), color=ctx.color)\n                ctx.exit()\n        attrs.setdefault('is_flag', True)\n        attrs.setdefault('expose_value', False)\n        attrs.setdefault('help', 'Show this message and exit.')\n        attrs.setdefault('is_eager', True)\n        attrs['callback'] = callback\n        return option(*(param_decls or ('--help',)), **attrs)(f)\n    return decorator", "output": "Adds a ``--help`` option which immediately ends the program\n    printing out the help page.  This is usually unnecessary to add as\n    this is added by default to all commands unless suppressed.\n\n    Like :func:`version_option`, this is implemented as eager option that\n    prints in the callback and exits.\n\n    All arguments are forwarded to :func:`option`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _IsPresent(item):\n  \"\"\"\"\"\"\n\n  if item[0].label == _FieldDescriptor.LABEL_REPEATED:\n    return bool(item[1])\n  elif item[0].cpp_type == _FieldDescriptor.CPPTYPE_MESSAGE:\n    return item[1]._is_present_in_parent\n  else:\n    return True", "output": "Given a (FieldDescriptor, value) tuple from _fields, return true if the\n  value should be included in the list returned by ListFields().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_forceescape(value):\n    \"\"\"\"\"\"\n    if hasattr(value, '__html__'):\n        value = value.__html__()\n    return escape(text_type(value))", "output": "Enforce HTML escaping.  This will probably double escape variables.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_str(value):\n    \"\"\"\"\"\"\n    if isinstance(value, bytes):\n        try:\n            return value.decode(get_filesystem_encoding())\n        except UnicodeError:\n            return value.decode('utf-8', 'replace')\n    return text_type(value)", "output": "Converts a value into a valid string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dropDuplicates(self, subset=None):\n        \"\"\"\n        \"\"\"\n        if subset is None:\n            jdf = self._jdf.dropDuplicates()\n        else:\n            jdf = self._jdf.dropDuplicates(self._jseq(subset))\n        return DataFrame(jdf, self.sql_ctx)", "output": "Return a new :class:`DataFrame` with duplicate rows removed,\n        optionally only considering certain columns.\n\n        For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n        :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n        duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n        be and system will accordingly limit the state. In addition, too late data older than\n        watermark will be dropped to avoid any possibility of duplicates.\n\n        :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n\n        >>> from pyspark.sql import Row\n        >>> df = sc.parallelize([ \\\\\n        ...     Row(name='Alice', age=5, height=80), \\\\\n        ...     Row(name='Alice', age=5, height=80), \\\\\n        ...     Row(name='Alice', age=10, height=80)]).toDF()\n        >>> df.dropDuplicates().show()\n        +---+------+-----+\n        |age|height| name|\n        +---+------+-----+\n        |  5|    80|Alice|\n        | 10|    80|Alice|\n        +---+------+-----+\n\n        >>> df.dropDuplicates(['name', 'height']).show()\n        +---+------+-----+\n        |age|height| name|\n        +---+------+-----+\n        |  5|    80|Alice|\n        +---+------+-----+", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _split_generators(self, dl_manager):\n    \"\"\"\n    \"\"\"\n    path = dl_manager.download_and_extract(_DOWNLOAD_URL)\n    return [\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TEST,\n            num_shards=1,\n            gen_kwargs={'data_dir': os.path.join(path, _DIRNAME)})\n    ]", "output": "Return the test split of Cifar10.\n\n    Args:\n      dl_manager: download manager object.\n\n    Returns:\n      test split.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keyPressEvent(self, event):\n        \"\"\"\"\"\"\n        event, text, key, ctrl, shift = restore_keyevent(event)\n        if key == Qt.Key_ParenLeft and not self.has_selected_text() \\\n          and self.help_enabled and not self.parent()._reading:\n            self._key_paren_left(text)\n        else:\n            # Let the parent widget handle the key press event\n            QTextEdit.keyPressEvent(self, event)", "output": "Reimplement Qt Method - Basic keypress event handler", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_nodes_min(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The list_nodes_min function must be called '\n            'with -f or --function.'\n        )\n\n    ret = {}\n    vm_properties = [\"name\"]\n\n    vm_list = salt.utils.vmware.get_mors_with_properties(_get_si(), vim.VirtualMachine, vm_properties)\n\n    for vm in vm_list:\n        ret[vm['name']] = {'state': 'Running', 'id': vm['name']}\n\n    return ret", "output": "Return a list of all VMs and templates that are on the specified provider, with no details\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f list_nodes_min my-vmware-config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_proper_name(self, name):\n        \"\"\"\"\"\"\n        with self.proper_names_db_path.open(\"a\") as f:\n            f.write(u\"{0}\\n\".format(name))", "output": "Registers a proper name to the database.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def combination(n, r):\n    \"\"\"\"\"\"\n    if n == r or r == 0:\n        return 1\n    else:\n        return combination(n-1, r-1) + combination(n-1, r)", "output": "This function calculates nCr.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_lineno(self, lineno, override=False):\n        \"\"\"\"\"\"\n        todo = deque([self])\n        while todo:\n            node = todo.popleft()\n            if 'lineno' in node.attributes:\n                if node.lineno is None or override:\n                    node.lineno = lineno\n            todo.extend(node.iter_child_nodes())\n        return self", "output": "Set the line numbers of the node and children.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_summary_metadata(hparams_plugin_data_pb):\n  \"\"\"\n  \"\"\"\n  if not isinstance(hparams_plugin_data_pb, plugin_data_pb2.HParamsPluginData):\n    raise TypeError('Needed an instance of plugin_data_pb2.HParamsPluginData.'\n                    ' Got: %s' % type(hparams_plugin_data_pb))\n  content = plugin_data_pb2.HParamsPluginData()\n  content.CopyFrom(hparams_plugin_data_pb)\n  content.version = PLUGIN_DATA_VERSION\n  return tf.compat.v1.SummaryMetadata(\n      plugin_data=tf.compat.v1.SummaryMetadata.PluginData(\n          plugin_name=PLUGIN_NAME, content=content.SerializeToString()))", "output": "Returns a summary metadata for the HParams plugin.\n\n  Returns a summary_pb2.SummaryMetadata holding a copy of the given\n  HParamsPluginData message in its plugin_data.content field.\n  Sets the version field of the hparams_plugin_data_pb copy to\n  PLUGIN_DATA_VERSION.\n\n  Args:\n    hparams_plugin_data_pb: the HParamsPluginData protobuffer to use.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        \"\"\"\n        \n        \"\"\"\n        with self.output().open('w') as output:\n            for _ in range(1000):\n                output.write('{} {} {}\\n'.format(\n                    random.randint(0, 999),\n                    random.randint(0, 999),\n                    random.randint(0, 999)))", "output": "Generates bogus data and writes it into the :py:meth:`~.Streams.output` target.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_locations(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_images function must be called with '\n            '-f or --function, or with the --list-locations option'\n        )\n\n    ret = {}\n    conn = get_conn()\n\n    for item in conn.list_locations()['items']:\n        reg, loc = item['id'].split('/')\n        location = {'id': item['id']}\n\n        if reg not in ret:\n            ret[reg] = {}\n\n        ret[reg][loc] = location\n    return ret", "output": "Return a dict of all available VM locations on the cloud provider with\n    relevant data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_min_leverage(self, min_leverage, grace_period):\n        \"\"\"\n        \"\"\"\n        deadline = self.sim_params.start_session + grace_period\n        control = MinLeverage(min_leverage, deadline)\n        self.register_account_control(control)", "output": "Set a limit on the minimum leverage of the algorithm.\n\n        Parameters\n        ----------\n        min_leverage : float\n            The minimum leverage for the algorithm.\n        grace_period : pd.Timedelta\n            The offset from the start date used to enforce a minimum leverage.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_actor(name):\n    \"\"\"\n    \"\"\"\n    actor_name = _calculate_key(name)\n    pickled_state = _internal_kv_get(actor_name)\n    if pickled_state is None:\n        raise ValueError(\"The actor with name={} doesn't exist\".format(name))\n    handle = pickle.loads(pickled_state)\n    return handle", "output": "Get a named actor which was previously created.\n\n    If the actor doesn't exist, an exception will be raised.\n\n    Args:\n        name: The name of the named actor.\n\n    Returns:\n        The ActorHandle object corresponding to the name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render_sphinx_doc(self, doc, context=None, css_path=CSS_PATH):\r\n        \"\"\"\"\"\"\r\n        # Math rendering option could have changed\r\n        if self.main.editor is not None:\r\n            fname = self.main.editor.get_current_filename()\r\n            dname = osp.dirname(fname)\r\n        else:\r\n            dname = ''\r\n        self._sphinx_thread.render(doc, context, self.get_option('math'),\r\n                                   dname, css_path=self.css_path)", "output": "Transform doc string dictionary to HTML and show it", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check(modname):\r\n    \"\"\"\"\"\"\r\n    for dependency in DEPENDENCIES:\r\n        if dependency.modname == modname:\r\n            return dependency.check()\r\n    else:\r\n        raise RuntimeError(\"Unkwown dependency %s\" % modname)", "output": "Check if required dependency is installed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _groupname():\n    '''\n    \n    '''\n    if grp:\n        try:\n            groupname = grp.getgrgid(os.getgid()).gr_name\n        except KeyError:\n            groupname = ''\n    else:\n        groupname = ''\n\n    return groupname", "output": "Grain for the minion groupname", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __space_delimited_list(value):\n    ''''''\n    if isinstance(value, six.string_types):\n        value = value.strip().split()\n\n    if hasattr(value, '__iter__') and value != []:\n        return (True, value, 'space-delimited string')\n    else:\n        return (False, value, '{0} is not a valid space-delimited value.\\n'.format(value))", "output": "validate that a value contains one or more space-delimited values", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_notebook_hook(notebook_type, action, *args, **kw):\n    ''' \n\n    '''\n    if notebook_type not in _HOOKS:\n        raise RuntimeError(\"no display hook installed for notebook type %r\" % notebook_type)\n    if _HOOKS[notebook_type][action] is None:\n        raise RuntimeError(\"notebook hook for %r did not install %r action\" % notebook_type, action)\n    return _HOOKS[notebook_type][action](*args, **kw)", "output": "Run an installed notebook hook with supplied arguments.\n\n    Args:\n        noteboook_type (str) :\n            Name of an existing installed notebook hook\n\n        actions (str) :\n            Name of the hook action to execute, ``'doc'`` or ``'app'``\n\n    All other arguments and keyword arguments are passed to the hook action\n    exactly as supplied.\n\n    Returns:\n        Result of the hook action, as-is\n\n    Raises:\n        RuntimeError\n            If the hook or specific action is not installed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mapValues(self, f):\n        \"\"\"\n        \n        \"\"\"\n        map_values_fn = lambda kv: (kv[0], f(kv[1]))\n        return self.map(map_values_fn, preservesPartitioning=True)", "output": "Pass each value in the key-value pair RDD through a map function\n        without changing the keys; this also retains the original RDD's\n        partitioning.\n\n        >>> x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n        >>> def f(x): return len(x)\n        >>> x.mapValues(f).collect()\n        [('a', 3), ('b', 1)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def done(self, retry=DEFAULT_RETRY):\n        \"\"\"\n        \"\"\"\n        # Do not refresh is the state is already done, as the job will not\n        # change once complete.\n        if self.state != _DONE_STATE:\n            self.reload(retry=retry)\n        return self.state == _DONE_STATE", "output": "Refresh the job and checks if it is complete.\n\n        :type retry: :class:`google.api_core.retry.Retry`\n        :param retry: (Optional) How to retry the RPC.\n\n        :rtype: bool\n        :returns: True if the job is complete, False otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_index(self, filename):\r\n        \"\"\"\"\"\"\r\n        index = self.fsmodel.index(filename)\r\n        if index.isValid() and index.model() is self.fsmodel:\r\n            return self.proxymodel.mapFromSource(index)", "output": "Return index associated with filename", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __enforce_only_strings_dict(dictionary):\n    '''\n    \n    '''\n    ret = {}\n\n    for key, value in iteritems(dictionary):\n        ret[six.text_type(key)] = six.text_type(value)\n\n    return ret", "output": "Returns a dictionary that has string keys and values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _key(self):\n        \"\"\"\n        \"\"\"\n        return (self.start_key, self.start_inclusive, self.end_key, self.end_inclusive)", "output": "A tuple key that uniquely describes this field.\n\n        Used to compute this instance's hashcode and evaluate equality.\n\n        Returns:\n            Tuple[str]: The contents of this :class:`.RowRange`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_systemd_services(root):\n    '''\n    \n    '''\n    ret = set()\n    for path in SYSTEM_CONFIG_PATHS + (LOCAL_CONFIG_PATH,):\n        # Make sure user has access to the path, and if the path is a\n        # link it's likely that another entry in SYSTEM_CONFIG_PATHS\n        # or LOCAL_CONFIG_PATH points to it, so we can ignore it.\n        path = _root(path, root)\n        if os.access(path, os.R_OK) and not os.path.islink(path):\n            for fullname in os.listdir(path):\n                try:\n                    unit_name, unit_type = fullname.rsplit('.', 1)\n                except ValueError:\n                    continue\n                if unit_type in VALID_UNIT_TYPES:\n                    ret.add(unit_name if unit_type == 'service' else fullname)\n    return ret", "output": "Use os.listdir() to get all the unit files", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def available():\n    '''\n    \n    '''\n    ret = []\n\n    mod_dir = os.path.join('/lib/modules/', os.uname()[2])\n\n    built_in_file = os.path.join(mod_dir, 'modules.builtin')\n    if os.path.exists(built_in_file):\n        with salt.utils.files.fopen(built_in_file, 'r') as f:\n            for line in f:\n                # Strip .ko from the basename\n                ret.append(os.path.basename(line)[:-4])\n\n    for root, dirs, files in salt.utils.path.os_walk(mod_dir):\n        for fn_ in files:\n            if '.ko' in fn_:\n                ret.append(fn_[:fn_.index('.ko')].replace('-', '_'))\n\n    if 'Arch' in __grains__['os_family']:\n        # Sadly this path is relative to kernel major version but ignores minor version\n        mod_dir_arch = '/lib/modules/extramodules-' + os.uname()[2][0:3] + '-ARCH'\n        for root, dirs, files in salt.utils.path.os_walk(mod_dir_arch):\n            for fn_ in files:\n                if '.ko' in fn_:\n                    ret.append(fn_[:fn_.index('.ko')].replace('-', '_'))\n\n    return sorted(list(ret))", "output": "Return a list of all available kernel modules\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' kmod.available", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _initialize_connection(api_key, app_key):\n    '''\n    \n    '''\n    if api_key is None:\n        raise SaltInvocationError('api_key must be specified')\n    if app_key is None:\n        raise SaltInvocationError('app_key must be specified')\n    options = {\n        'api_key': api_key,\n        'app_key': app_key\n    }\n    datadog.initialize(**options)", "output": "Initialize Datadog connection", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate(self, qstr, editing=True):\r\n        \"\"\"\"\"\"\r\n        if self.selected_text == qstr and qstr != '':\r\n            self.valid.emit(True, True)\r\n            return\r\n\r\n        valid = self.is_valid(qstr)\r\n        if editing:\r\n            if valid:\r\n                self.valid.emit(True, False)\r\n            else:\r\n                self.valid.emit(False, False)", "output": "Validate entered path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_painter(self, painter, light_color):\n        \"\"\"\"\"\"\n        painter.setPen(QColor(light_color).darker(120))\n        painter.setBrush(QBrush(QColor(light_color)))", "output": "Set scroll flag area painter pen and brush colors", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop_button_click_handler(self):\r\n        \"\"\"\"\"\"\r\n        self.stop_button.setDisabled(True)\r\n        # Interrupt computations or stop debugging\r\n        if not self.shellwidget._reading:\r\n            self.interrupt_kernel()\r\n        else:\r\n            self.shellwidget.write_to_stdin('exit')", "output": "Method to handle what to do when the stop button is pressed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_stack(self, name, wait=False):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            stack = self.cf_client.describe_stacks(StackName=name)['Stacks'][0]\n        except: # pragma: no cover\n            print('No Zappa stack named {0}'.format(name))\n            return False\n\n        tags = {x['Key']:x['Value'] for x in stack['Tags']}\n        if tags.get('ZappaProject') == name:\n            self.cf_client.delete_stack(StackName=name)\n            if wait:\n                waiter = self.cf_client.get_waiter('stack_delete_complete')\n                print('Waiting for stack {0} to be deleted..'.format(name))\n                waiter.wait(StackName=name)\n            return True\n        else:\n            print('ZappaProject tag not found on {0}, doing nothing'.format(name))\n            return False", "output": "Delete the CF stack managed by Zappa.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_tcp_keepalive(zmq_socket, opts):\n    '''\n    \n    '''\n    if hasattr(zmq, 'TCP_KEEPALIVE') and opts:\n        if 'tcp_keepalive' in opts:\n            zmq_socket.setsockopt(\n                zmq.TCP_KEEPALIVE, opts['tcp_keepalive']\n            )\n        if 'tcp_keepalive_idle' in opts:\n            zmq_socket.setsockopt(\n                zmq.TCP_KEEPALIVE_IDLE, opts['tcp_keepalive_idle']\n            )\n        if 'tcp_keepalive_cnt' in opts:\n            zmq_socket.setsockopt(\n                zmq.TCP_KEEPALIVE_CNT, opts['tcp_keepalive_cnt']\n            )\n        if 'tcp_keepalive_intvl' in opts:\n            zmq_socket.setsockopt(\n                zmq.TCP_KEEPALIVE_INTVL, opts['tcp_keepalive_intvl']\n            )", "output": "Ensure that TCP keepalives are set as specified in \"opts\".\n\n    Warning: Failure to set TCP keepalives on the salt-master can result in\n    not detecting the loss of a minion when the connection is lost or when\n    it's host has been terminated without first closing the socket.\n    Salt's Presence System depends on this connection status to know if a minion\n    is \"present\".\n\n    Warning: Failure to set TCP keepalives on minions can result in frequent or\n    unexpected disconnects!", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def export_trial_if_needed(self, trial):\n        \"\"\"\n        \"\"\"\n        if trial.export_formats and len(trial.export_formats) > 0:\n            return ray.get(\n                trial.runner.export_model.remote(trial.export_formats))\n        return {}", "output": "Exports model of this trial based on trial.export_formats.\n\n        Return:\n            A dict that maps ExportFormats to successfully exported models.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def discard(self, key):\n        \"\"\"\n        \n        \"\"\"\n        if key in self:\n            i = self.map[key]\n            del self.items[i]\n            del self.map[key]\n            for k, v in self.map.items():\n                if v >= i:\n                    self.map[k] = v - 1", "output": "Remove an element.  Do not raise an exception if absent.\n\n        The MutableSet mixin uses this to implement the .remove() method, which\n        *does* raise an error when asked to remove a non-existent item.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_idx_to_vec_by_embeddings(self, token_embeddings, vocab_len, vocab_idx_to_token):\n        \"\"\"\n        \"\"\"\n\n        new_vec_len = sum(embed.vec_len for embed in token_embeddings)\n        new_idx_to_vec = nd.zeros(shape=(vocab_len, new_vec_len))\n\n        col_start = 0\n        # Concatenate all the embedding vectors in token_embeddings.\n        for embed in token_embeddings:\n            col_end = col_start + embed.vec_len\n            # Cancatenate vectors of the unknown token.\n            new_idx_to_vec[0, col_start:col_end] = embed.idx_to_vec[0]\n            new_idx_to_vec[1:, col_start:col_end] = embed.get_vecs_by_tokens(vocab_idx_to_token[1:])\n            col_start = col_end\n\n        self._vec_len = new_vec_len\n        self._idx_to_vec = new_idx_to_vec", "output": "Sets the mapping between token indices and token embedding vectors.\n\n\n        Parameters\n        ----------\n        token_embeddings : instance or list `mxnet.contrib.text.embedding._TokenEmbedding`\n            One or multiple pre-trained token embeddings to load. If it is a list of multiple\n            embeddings, these embedding vectors will be concatenated for each token.\n        vocab_len : int\n            Length of vocabulary whose tokens are indexed in the token embedding.\n        vocab_idx_to_token: list of str\n            A list of indexed tokens in the vocabulary. These tokens are indexed in the token\n            embedding.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_timing_signal(length,\n                      min_timescale=1,\n                      max_timescale=1e4,\n                      num_timescales=16):\n  \"\"\"\n  \"\"\"\n  positions = to_float(tf.range(length))\n  log_timescale_increment = (\n      math.log(max_timescale / min_timescale) / (num_timescales - 1))\n  inv_timescales = min_timescale * tf.exp(\n      to_float(tf.range(num_timescales)) * -log_timescale_increment)\n  scaled_time = tf.expand_dims(positions, 1) * tf.expand_dims(inv_timescales, 0)\n  return tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)", "output": "Create Tensor of sinusoids of different frequencies.\n\n  Args:\n    length: Length of the Tensor to create, i.e. Number of steps.\n    min_timescale: a float\n    max_timescale: a float\n    num_timescales: an int\n\n  Returns:\n    Tensor of shape (length, 2*num_timescales)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def catch_error(func):\n    \"\"\"\"\"\"\n    import amqp\n    try:\n        import pika.exceptions\n        connect_exceptions = (\n            pika.exceptions.ConnectionClosed,\n            pika.exceptions.AMQPConnectionError,\n        )\n    except ImportError:\n        connect_exceptions = ()\n\n    connect_exceptions += (\n        select.error,\n        socket.error,\n        amqp.ConnectionError\n    )\n\n    def wrap(self, *args, **kwargs):\n        try:\n            return func(self, *args, **kwargs)\n        except connect_exceptions as e:\n            logging.error('RabbitMQ error: %r, reconnect.', e)\n            self.reconnect()\n            return func(self, *args, **kwargs)\n    return wrap", "output": "Catch errors of rabbitmq then reconnect", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bgrewriteaof(host=None, port=None, db=None, password=None):\n    '''\n    \n    '''\n    server = _connect(host, port, db, password)\n    return server.bgrewriteaof()", "output": "Asynchronously rewrite the append-only file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.bgrewriteaof", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(self, decorations):\n        \"\"\"\n        \n        \"\"\"\n        added = 0\n        if isinstance(decorations, list):\n            not_repeated = set(decorations) - set(self._decorations)\n            self._decorations.extend(list(not_repeated))\n            added = len(not_repeated)\n        elif decorations not in self._decorations:\n            self._decorations.append(decorations)\n            added = 1\n\n        if added > 0:\n            self._order_decorations()\n            self.update()\n        return added", "output": "Add text decorations on a CodeEditor instance.\n\n        Don't add duplicated decorations, and order decorations according\n        draw_order and the size of the selection.\n\n        Args:\n            decorations (sourcecode.api.TextDecoration) (could be a list)\n        Returns:\n            int: Amount of decorations added.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def response(self, request, exception):\n        \"\"\"\n        \"\"\"\n        handler = self.lookup(exception)\n        response = None\n        try:\n            if handler:\n                response = handler(request, exception)\n            if response is None:\n                response = self.default(request, exception)\n        except Exception:\n            self.log(format_exc())\n            try:\n                url = repr(request.url)\n            except AttributeError:\n                url = \"unknown\"\n            response_message = (\n                \"Exception raised in exception handler \" '\"%s\" for uri: %s'\n            )\n            logger.exception(response_message, handler.__name__, url)\n\n            if self.debug:\n                return text(response_message % (handler.__name__, url), 500)\n            else:\n                return text(\"An error occurred while handling an error\", 500)\n        return response", "output": "Fetches and executes an exception handler and returns a response\n        object\n\n        :param request: Instance of :class:`sanic.request.Request`\n        :param exception: Exception to handle\n\n        :type request: :class:`sanic.request.Request`\n        :type exception: :class:`sanic.exceptions.SanicException` or\n            :class:`Exception`\n\n        :return: Wrap the return value obtained from :func:`default`\n            or registered handler for that type of exception.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def LAST(COND, N1, N2):\n    \"\"\"\n    \"\"\"\n    N2 = 1 if N2 == 0 else N2\n    assert N2 > 0\n    assert N1 > N2\n    return COND.iloc[-N1:-N2].all()", "output": "\u8868\u8fbe\u6301\u7eed\u6027\n    \u4ece\u524dN1\u65e5\u5230\u524dN2\u65e5\u4e00\u76f4\u6ee1\u8db3COND\u6761\u4ef6\n\n    Arguments:\n        COND {[type]} -- [description]\n        N1 {[type]} -- [description]\n        N2 {[type]} -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _loadlib(lib='standard'):\n    \"\"\"\"\"\"\n    global _LIB\n    if _LIB is not None:\n        warnings.warn('rabit.int call was ignored because it has'\\\n                          ' already been initialized', level=2)\n        return\n    if lib == 'standard':\n        _LIB = ctypes.cdll.LoadLibrary(WRAPPER_PATH % '')\n    elif lib == 'mock':\n        _LIB = ctypes.cdll.LoadLibrary(WRAPPER_PATH % '_mock')\n    elif lib == 'mpi':\n        _LIB = ctypes.cdll.LoadLibrary(WRAPPER_PATH % '_mpi')\n    else:\n        raise Exception('unknown rabit lib %s, can be standard, mock, mpi' % lib)\n    _LIB.RabitGetRank.restype = ctypes.c_int\n    _LIB.RabitGetWorldSize.restype = ctypes.c_int\n    _LIB.RabitVersionNumber.restype = ctypes.c_int", "output": "Load rabit library.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def in_subnet(cidr, addr=None):\n    '''\n    \n    '''\n    try:\n        cidr = ipaddress.ip_network(cidr)\n    except ValueError:\n        log.error('Invalid CIDR \\'%s\\'', cidr)\n        return False\n\n    if addr is None:\n        addr = ip_addrs()\n        addr.extend(ip_addrs6())\n    elif not isinstance(addr, (list, tuple)):\n        addr = (addr,)\n\n    return any(ipaddress.ip_address(item) in cidr for item in addr)", "output": "Returns True if host or (any of) addrs is within specified subnet, otherwise False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def multiply(self, matrix):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(matrix, DenseMatrix):\n            raise ValueError(\"Only multiplication with DenseMatrix \"\n                             \"is supported.\")\n        j_model = self._java_matrix_wrapper.call(\"multiply\", matrix)\n        return RowMatrix(j_model)", "output": "Multiply this matrix by a local dense matrix on the right.\n\n        :param matrix: a local dense matrix whose number of rows must match the number of columns\n                       of this matrix\n        :returns: :py:class:`RowMatrix`\n\n        >>> rm = RowMatrix(sc.parallelize([[0, 1], [2, 3]]))\n        >>> rm.multiply(DenseMatrix(2, 2, [0, 2, 1, 3])).rows.collect()\n        [DenseVector([2.0, 3.0]), DenseVector([6.0, 11.0])]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_key(self, path):\n        \"\"\"\n        \n        \"\"\"\n        (bucket, key) = self._path_to_bucket_and_key(path)\n\n        if self._exists(bucket, key):\n            return self.s3.ObjectSummary(bucket, key)", "output": "Returns the object summary at the path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_worker(q_out, fname, working_dir):\n    \"\"\"\n    \"\"\"\n    pre_time = time.time()\n    count = 0\n    fname = os.path.basename(fname)\n    fname_rec = os.path.splitext(fname)[0] + '.rec'\n    fname_idx = os.path.splitext(fname)[0] + '.idx'\n    record = mx.recordio.MXIndexedRecordIO(os.path.join(working_dir, fname_idx),\n                                           os.path.join(working_dir, fname_rec), 'w')\n    buf = {}\n    more = True\n    while more:\n        deq = q_out.get()\n        if deq is not None:\n            i, s, item = deq\n            buf[i] = (s, item)\n        else:\n            more = False\n        while count in buf:\n            s, item = buf[count]\n            del buf[count]\n            if s is not None:\n                record.write_idx(item[0], s)\n\n            if count % 1000 == 0:\n                cur_time = time.time()\n                print('time:', cur_time - pre_time, ' count:', count)\n                pre_time = cur_time\n            count += 1", "output": "Function that will be spawned to fetch processed image\n    from the output queue and write to the .rec file.\n    Parameters\n    ----------\n    q_out: queue\n    fname: string\n    working_dir: string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def output_scores(self, name=None):\n        \"\"\" \"\"\"\n        return tf.nn.softmax(self.label_logits, name=name)", "output": "Returns: N x #class scores, summed to one for each box.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __dict_to_deployment_spec(spec):\n    '''\n    \n    '''\n    spec_obj = AppsV1beta1DeploymentSpec(template=spec.get('template', ''))\n    for key, value in iteritems(spec):\n        if hasattr(spec_obj, key):\n            setattr(spec_obj, key, value)\n\n    return spec_obj", "output": "Converts a dictionary into kubernetes AppsV1beta1DeploymentSpec instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clean_expired_tokens(opts):\n    '''\n    \n    '''\n    loadauth = salt.auth.LoadAuth(opts)\n    for tok in loadauth.list_tokens():\n        token_data = loadauth.get_tok(tok)\n        if 'expire' not in token_data or token_data.get('expire', 0) < time.time():\n            loadauth.rm_token(tok)", "output": "Clean expired tokens from the master", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _load(formula):\n    '''\n    \n    '''\n\n    # Compute possibilities\n    _mk_client()\n    paths = []\n    for ext in ('yaml', 'json'):\n        source_url = salt.utils.url.create(formula + '/defaults.' + ext)\n        paths.append(source_url)\n    # Fetch files from master\n    defaults_files = __context__['cp.fileclient'].cache_files(paths)\n\n    for file_ in defaults_files:\n        if not file_:\n            # Skip empty string returned by cp.fileclient.cache_files.\n            continue\n\n        suffix = file_.rsplit('.', 1)[-1]\n        if suffix == 'yaml':\n            loader = salt.utils.yaml.safe_load\n        elif suffix == 'json':\n            loader = salt.utils.json.load\n        else:\n            log.debug(\"Failed to determine loader for %r\", file_)\n            continue\n\n        if os.path.exists(file_):\n            log.debug(\"Reading defaults from %r\", file_)\n            with salt.utils.files.fopen(file_) as fhr:\n                defaults = loader(fhr)\n                log.debug(\"Read defaults %r\", defaults)\n\n            return defaults or {}", "output": "Generates a list of salt://<formula>/defaults.(json|yaml) files\n    and fetches them from the Salt master.\n\n    Returns first defaults file as python dict.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathNextDescendantOrSelf(self, ctxt):\n        \"\"\" \"\"\"\n        if ctxt is None: ctxt__o = None\n        else: ctxt__o = ctxt._o\n        ret = libxml2mod.xmlXPathNextDescendantOrSelf(ctxt__o, self._o)\n        if ret is None:raise xpathError('xmlXPathNextDescendantOrSelf() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Traversal function for the \"descendant-or-self\" direction\n          the descendant-or-self axis contains the context node and\n          the descendants of the context node in document order; thus\n          the context node is the first node on the axis, and the\n          first child of the context node is the second node on the\n           axis", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mask_args_value(data, mask):\n    '''\n    \n    '''\n    if not mask:\n        return data\n\n    out = []\n    for line in data.split(os.linesep):\n        if fnmatch.fnmatch(line.strip(), mask) and ':' in line:\n            key, value = line.split(':', 1)\n            out.append('{}: {}'.format(salt.utils.stringutils.to_unicode(key.strip()), '** hidden **'))\n        else:\n            out.append(line)\n\n    return '\\n'.join(out)", "output": "Mask a line in the data, which matches \"mask\".\n\n    This can be used for cases where values in your roster file may contain\n    sensitive data such as IP addresses, passwords, user names, etc.\n\n    Note that this works only when ``data`` is a single string (i.e. when the\n    data in the roster is formatted as ``key: value`` pairs in YAML syntax).\n\n    :param data: String data, already rendered.\n    :param mask: Mask that matches a single line\n\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait_until(name, state, timeout=300):\n    '''\n    \n    '''\n    start_time = time.time()\n    node = show_instance(name, call='action')\n    while True:\n        if node['state'] == state:\n            return True\n        time.sleep(1)\n        if time.time() - start_time > timeout:\n            return False\n        node = show_instance(name, call='action')", "output": "Wait until a specific state has been reached on  a node", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_event(self):\n        \"\"\"\n        \n        \"\"\"\n        event_bus = Environment.get_instance().event_bus\n        event_bus.prepend_listener(EVENT.PRE_BEFORE_TRADING, self._pre_before_trading)\n        event_bus.prepend_listener(EVENT.POST_SETTLEMENT, self._post_settlement)", "output": "\u6ce8\u518c\u4e8b\u4ef6", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xor(*variables):\n    '''\n    \n    '''\n    sum_ = False\n    for value in variables:\n        sum_ = sum_ ^ bool(value)\n    return sum_", "output": "XOR definition for multiple variables", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _serial_sanitizer(instr):\n    ''''''\n    length = len(instr)\n    index = int(math.floor(length * .75))\n    return '{0}{1}'.format(instr[:index], 'X' * (length - index))", "output": "Replaces the last 1/4 of a string with X's", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_template(path, option_key):\n    '''\n    \n    '''\n    with salt.utils.files.fopen(path, 'r') as template_f:\n        template = deserialize(template_f)\n        info = (option_key, template.get('description', ''), template)\n    return info", "output": "Get the contents of a template file and provide it as a module type\n\n    :param path: path to the template.yml file\n    :type  path: ``str``\n\n    :param option_key: The unique key of this template\n    :type  option_key: ``str``\n\n    :returns: Details about the template\n    :rtype: ``tuple``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_encoder_from_vocab(vocab_filepath):\n  \"\"\"\n  \"\"\"\n  if not tf.gfile.Exists(vocab_filepath):\n    raise ValueError(\"Vocab file does not exist: {}.\".format(vocab_filepath))\n\n  tf.logging.info(\"Found vocab file: %s\", vocab_filepath)\n  encoder = text_encoder.SubwordTextEncoder(vocab_filepath)\n  return encoder", "output": "Get encoder from vocab file.\n\n  If vocab is not found in output dir, it will be copied there by\n  copy_vocab_to_output_dir to clarify the vocab used to generate the data.\n\n  Args:\n    vocab_filepath: path to vocab, either local or cns\n\n  Returns:\n    A SubwordTextEncoder vocabulary object. None if the output_parallel_text\n    is set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def equals(self, other):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(other, RangeIndex):\n            ls = len(self)\n            lo = len(other)\n            return (ls == lo == 0 or\n                    ls == lo == 1 and\n                    self._start == other._start or\n                    ls == lo and\n                    self._start == other._start and\n                    self._step == other._step)\n\n        return super().equals(other)", "output": "Determines if two Index objects contain the same elements.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_new_bracket(self):\n        \"\"\"\"\"\"\n        logger.debug(\n            'start to create a new SuccessiveHalving iteration, self.curr_s=%d', self.curr_s)\n        if self.curr_s < 0:\n            logger.info(\"s < 0, Finish this round of Hyperband in BOHB. Generate new round\")\n            self.curr_s = self.s_max\n        self.brackets[self.curr_s] = Bracket(s=self.curr_s, s_max=self.s_max, eta=self.eta,\n                                    max_budget=self.max_budget, optimize_mode=self.optimize_mode)\n        next_n, next_r = self.brackets[self.curr_s].get_n_r()\n        logger.debug(\n            'new SuccessiveHalving iteration, next_n=%d, next_r=%d', next_n, next_r)\n        # rewrite with TPE\n        generated_hyper_configs = self.brackets[self.curr_s].get_hyperparameter_configurations(\n            next_n, next_r, self.cg)\n        self.generated_hyper_configs = generated_hyper_configs.copy()", "output": "generate a new bracket", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def loss(params, batch, model_predict, rng):\n  \"\"\"\"\"\"\n  inputs, targets = batch\n  predictions = model_predict(inputs, params, rng=rng)\n  predictions, targets = _make_list(predictions, targets)\n  xent = []\n  for (pred, target) in zip(predictions, targets):\n    xent.append(np.sum(pred * layers.one_hot(target, pred.shape[-1]), axis=-1))\n  return - masked_mean(xent, targets)", "output": "Calculate loss.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Load(self):\n    \"\"\"\n    \"\"\"\n    try:\n      for event in self._LoadInternal():\n        yield event\n    except tf.errors.OpError:\n      if not tf.io.gfile.exists(self._directory):\n        raise DirectoryDeletedError(\n            'Directory %s has been permanently deleted' % self._directory)", "output": "Loads new values.\n\n    The watcher will load from one path at a time; as soon as that path stops\n    yielding events, it will move on to the next path. We assume that old paths\n    are never modified after a newer path has been written. As a result, Load()\n    can be called multiple times in a row without losing events that have not\n    been yielded yet. In other words, we guarantee that every event will be\n    yielded exactly once.\n\n    Yields:\n      All values that have not been yielded yet.\n\n    Raises:\n      DirectoryDeletedError: If the directory has been permanently deleted\n        (as opposed to being temporarily unavailable).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compute_uncertainty_reward(logits, predictions):\n  \"\"\"\"\"\"\n  # TODO(rsepassi): Add support for L1/L2 loss models. Current code only\n  # works for softmax models.\n  vocab_size = logits.shape[-1]\n  assert vocab_size > 1\n  log_probs = common_layers.log_prob_from_logits(logits)\n  max_log_probs = common_layers.index_last_dim_with_indices(log_probs,\n                                                            predictions)\n  # Threshold\n  neg_log_prob = tf.nn.relu(-max_log_probs - 0.02)\n  # Sum across all but the batch dimension\n  reduce_dims = list(range(len(neg_log_prob.shape)))[1:]\n  summed = tf.reduce_sum(neg_log_prob, axis=reduce_dims)\n  return summed / 10", "output": "Uncertainty reward based on logits.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_peering_connection_ids(name, conn):\n    '''\n    \n    '''\n    filters = [{\n        'Name': 'tag:Name',\n        'Values': [name],\n    }, {\n        'Name': 'status-code',\n        'Values': [ACTIVE, PENDING_ACCEPTANCE, PROVISIONING],\n    }]\n\n    peerings = conn.describe_vpc_peering_connections(\n        Filters=filters).get('VpcPeeringConnections',\n                             [])\n    return [x['VpcPeeringConnectionId'] for x in peerings]", "output": ":param name: The name of the VPC peering connection.\n    :type name: String\n    :param conn: The boto aws ec2 connection.\n    :return: The id associated with this peering connection\n\n    Returns the VPC peering connection ids\n    given the VPC peering connection name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def caller(self, fun, **kwargs):\n        '''\n        \n        '''\n        self.client_cache['caller'].cmd(fun, *kwargs['arg'], **kwargs['kwarg'])", "output": "Wrap LocalCaller to execute remote exec functions locally on the Minion", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def refresh_plugin(self):\r\n        \"\"\"\"\"\"\r\n        if self.tabwidget.count():\r\n            editor = self.tabwidget.currentWidget()\r\n        else:\r\n            editor = None\r\n        self.find_widget.set_editor(editor)", "output": "Refresh tabwidget", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def markers():\n    ''' \n    '''\n    print(\"Available markers: \\n\\n - \" + \"\\n - \".join(list(MarkerType)))\n    print()\n    print(\"Shortcuts: \\n\\n\" + \"\\n\".join(\" %r: %s\" % item for item in _MARKER_SHORTCUTS.items()))", "output": "Prints a list of valid marker types for scatter()\n\n    Returns:\n        None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_lockfile(self, content):\n        \"\"\"\n        \"\"\"\n        s = self._lockfile_encoder.encode(content)\n        open_kwargs = {\"newline\": self._lockfile_newlines, \"encoding\": \"utf-8\"}\n        with vistir.contextmanagers.atomic_open_for_write(\n            self.lockfile_location, **open_kwargs\n        ) as f:\n            f.write(s)\n            # Write newline at end of document. GH-319.\n            # Only need '\\n' here; the file object handles the rest.\n            if not s.endswith(u\"\\n\"):\n                f.write(u\"\\n\")", "output": "Write out the lockfile.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_episode_end(self, episode, logs={}):\n        \"\"\" \"\"\"\n        for callback in self.callbacks:\n            # Check if callback supports the more appropriate `on_episode_end` callback.\n            # If not, fall back to `on_epoch_end` to be compatible with built-in Keras callbacks.\n            if callable(getattr(callback, 'on_episode_end', None)):\n                callback.on_episode_end(episode, logs=logs)\n            else:\n                callback.on_epoch_end(episode, logs=logs)", "output": "Called at end of each episode for each callback in callbackList", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shutdown(self, exitcode=0, exitmsg=None):\n        '''\n        \n        '''\n        if hasattr(self, 'minion') and 'proxymodule' in self.minion.opts:\n            proxy_fn = self.minion.opts['proxymodule'].loaded_base_name + '.shutdown'\n            self.minion.opts['proxymodule'][proxy_fn](self.minion.opts)\n        self.action_log_info('Shutting down')\n        super(ProxyMinion, self).shutdown(\n            exitcode, ('The Salt {0} is shutdown. {1}'.format(\n                self.__class__.__name__, (exitmsg or '')).strip()))", "output": "If sub-classed, run any shutdown operations on this method.\n\n        :param exitcode\n        :param exitmsg", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_input_files(headerDir, sourceDir, containers=['vector', 'list', 'set', 'map'],\n                      seqType='both', verbose=False):\n    \"\"\"\"\"\"\n    # Check the input files for containers in their variadic form.\n    result1 = False\n    if seqType == \"both\" or seqType == \"variadic\":\n        if verbose:\n            print \"Check if input files for pre-processing Boost.MPL variadic containers need fixing.\"\n        result1 = check_input_files_for_variadic_seq(headerDir, sourceDir)\n        if verbose:\n            if result1:\n                print \"  At least one input file needs fixing!\"\n            else:\n                print \"  No input file needs fixing!\"\n    # Check the input files for containers in their numbered form.\n    result2 = False\n    result3 = False\n    if seqType == \"both\" or seqType == \"numbered\":\n        if verbose:\n            print \"Check input files for pre-processing Boost.MPL numbered containers.\"\n        result2 = check_input_files_for_numbered_seq(headerDir, \".hpp\", containers)\n        result3 = check_input_files_for_numbered_seq(sourceDir, \".cpp\", containers)\n        if verbose:\n            if result2 or result3:\n                print \"  At least one input file needs fixing!\"\n            else:\n                print \"  No input file needs fixing!\"\n    # Return result.\n    return result1 or result2 or result3", "output": "Checks if source- and header-files, used as input when pre-processing MPL-containers, need fixing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_get(user_id=None, name=None, profile=None, **connection_args):\n    '''\n    \n    '''\n    kstone = auth(profile, **connection_args)\n    ret = {}\n    if name:\n        for user in kstone.users.list():\n            if user.name == name:\n                user_id = user.id\n                break\n    if not user_id:\n        return {'Error': 'Unable to resolve user id'}\n    try:\n        user = kstone.users.get(user_id)\n    except keystoneclient.exceptions.NotFound:\n        msg = 'Could not find user \\'{0}\\''.format(user_id)\n        log.error(msg)\n        return {'Error': msg}\n\n    ret[user.name] = dict((value, getattr(user, value, None)) for value in dir(user)\n                          if not value.startswith('_') and\n                          isinstance(getattr(user, value, None), (six.string_types, dict, bool)))\n\n    tenant_id = getattr(user, 'tenantId', None)\n    if tenant_id:\n        ret[user.name]['tenant_id'] = tenant_id\n    return ret", "output": "Return a specific users (keystone user-get)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.user_get c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.user_get user_id=c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.user_get name=nova", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_output_layers(self):\n        \"\"\"\n        \n        \"\"\"\n        # TODO\n        # use successors == 0 as the criteria for output layer\n        # will fail when some intermediate layers also generate output.\n        # However, because the possibility of having inserted layers,\n        # it's more difficult to tell which layer is the output layer.\n        # Once possible way is to keep track of newly added layers...\n        self.output_layers = []\n        for layer in self.layer_list:\n            if len(self.get_successors(layer)) == 0:\n                self.output_layers.append(layer)", "output": "Extract the ordering of output layers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _process_image_file(fobj, session, filename):\n  \"\"\"\"\"\"\n  # We need to read the image files and convert them to JPEG, since some files\n  # actually contain GIF, PNG or BMP data (despite having a .jpg extension) and\n  # some encoding options that will make TF crash in general.\n  image = _decode_image(fobj, session, filename=filename)\n  return _encode_jpeg(image)", "output": "Process image files from the dataset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def newNodeEatName(self, name):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlNewNodeEatName(self._o, name)\n        if ret is None:raise treeError('xmlNewNodeEatName() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Creation of a new node element. @ns is optional (None).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_blob(call=None, kwargs=None):  # pylint: disable=unused-argument\n    '''\n    \n    '''\n    if kwargs is None:\n        kwargs = {}\n\n    if 'container' not in kwargs:\n        raise SaltCloudSystemExit(\n            'A container must be specified'\n        )\n\n    if 'blob' not in kwargs:\n        raise SaltCloudSystemExit(\n            'A blob must be specified'\n        )\n\n    storageservice = _get_block_blob_service(kwargs)\n\n    storageservice.delete_blob(kwargs['container'], kwargs['blob'])\n    return True", "output": "Delete a blob from a container.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_mail(window):\n    \"\"\"\n    \n    \"\"\"\n    mail = imaplib.IMAP4_SSL(IMAP_SERVER)\n\n    (retcode, capabilities) = mail.login(LOGIN_EMAIL, LOGIN_PASSWORD)\n    mail.list()\n    typ, data = mail.select('Inbox')\n    n = 0\n    now = datetime.now()\n    # get messages from today\n    search_string = '(SENTON {}-{}-{})'.format(now.day, calendar.month_abbr[now.month], now.year)\n    (retcode, messages) = mail.search(None, search_string)\n    if retcode == 'OK':\n        msg_list = messages[0].split()  # message numbers are separated by spaces, turn into list\n        msg_list.sort(reverse=True)  # sort messages descending\n        for n, message in enumerate(msg_list):\n            if n >= MAX_EMAILS:\n                break\n            from_elem = window.FindElement('{}from'.format(n))\n            date_elem = window.FindElement('{}date'.format(n))\n            from_elem.Update('')  # erase them so you know they're changing\n            date_elem.Update('')\n            window.Refresh()\n            typ, data = mail.fetch(message, '(RFC822)')\n            for response_part in data:\n                if isinstance(response_part, tuple):\n                    original = email.message_from_bytes(response_part[1])\n                    date_str = original['Date'][:22]\n                    from_elem.Update(original['From'])\n                    date_elem.Update(date_str)\n                    window.Refresh()", "output": "Reads late emails from IMAP server and displays them in the Window\n    :param window: window to display emails in\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_sdist(source_dir, sdist_dir, config_settings=None):\n    \"\"\"\n    \"\"\"\n    if config_settings is None:\n        config_settings = {}\n    requires, backend = _load_pyproject(source_dir)\n    hooks = Pep517HookCaller(source_dir, backend)\n\n    with BuildEnvironment() as env:\n        env.pip_install(requires)\n        reqs = hooks.get_requires_for_build_sdist(config_settings)\n        env.pip_install(reqs)\n        return hooks.build_sdist(sdist_dir, config_settings)", "output": "Build an sdist from a source directory using PEP 517 hooks.\n\n    :param str source_dir: Source directory containing pyproject.toml\n    :param str sdist_dir: Target directory to place sdist in\n    :param dict config_settings: Options to pass to build backend\n\n    This is a blocking function which will run pip in a subprocess to install\n    build requirements.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def price_diff(self):\n        ''\n        res = self.price.groupby(level=1).apply(lambda x: x.diff(1))\n        res.name = 'price_diff'\n        return res", "output": "\u8fd4\u56deDataStruct.price\u7684\u4e00\u9636\u5dee\u5206", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _block(self, count):\n        \"\"\"\n        \"\"\"\n        blocks, remainder = divmod(count, BLOCKSIZE)\n        if remainder:\n            blocks += 1\n        return blocks * BLOCKSIZE", "output": "Round up a byte count by BLOCKSIZE and return it,\n           e.g. _block(834) => 1024.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _compute_hparam_infos(self):\n    \"\"\"\n    \"\"\"\n    run_to_tag_to_content = self.multiplexer.PluginRunToTagToContent(\n        metadata.PLUGIN_NAME)\n    # Construct a dict mapping an hparam name to its list of values.\n    hparams = collections.defaultdict(list)\n    for tag_to_content in run_to_tag_to_content.values():\n      if metadata.SESSION_START_INFO_TAG not in tag_to_content:\n        continue\n      start_info = metadata.parse_session_start_info_plugin_data(\n          tag_to_content[metadata.SESSION_START_INFO_TAG])\n      for (name, value) in six.iteritems(start_info.hparams):\n        hparams[name].append(value)\n\n    # Try to construct an HParamInfo for each hparam from its name and list\n    # of values.\n    result = []\n    for (name, values) in six.iteritems(hparams):\n      hparam_info = self._compute_hparam_info_from_values(name, values)\n      if hparam_info is not None:\n        result.append(hparam_info)\n    return result", "output": "Computes a list of api_pb2.HParamInfo from the current run, tag info.\n\n    Finds all the SessionStartInfo messages and collects the hparams values\n    appearing in each one. For each hparam attempts to deduce a type that fits\n    all its values. Finally, sets the 'domain' of the resulting HParamInfo\n    to be discrete if the type is string and the number of distinct values is\n    small enough.\n\n    Returns:\n      A list of api_pb2.HParamInfo messages.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _maybe_localize_point(ts, is_none, is_not_none, freq, tz):\n    \"\"\"\n    \n    \"\"\"\n    # Make sure start and end are timezone localized if:\n    # 1) freq = a Timedelta-like frequency (Tick)\n    # 2) freq = None i.e. generating a linspaced range\n    if isinstance(freq, Tick) or freq is None:\n        localize_args = {'tz': tz, 'ambiguous': False}\n    else:\n        localize_args = {'tz': None}\n    if is_none is None and is_not_none is not None:\n        ts = ts.tz_localize(**localize_args)\n    return ts", "output": "Localize a start or end Timestamp to the timezone of the corresponding\n    start or end Timestamp\n\n    Parameters\n    ----------\n    ts : start or end Timestamp to potentially localize\n    is_none : argument that should be None\n    is_not_none : argument that should not be None\n    freq : Tick, DateOffset, or None\n    tz : str, timezone object or None\n\n    Returns\n    -------\n    ts : Timestamp", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _getpwnam(name, root=None):\n    '''\n    \n    '''\n    root = '/' if not root else root\n    passwd = os.path.join(root, 'etc/passwd')\n    with salt.utils.files.fopen(passwd) as fp_:\n        for line in fp_:\n            line = salt.utils.stringutils.to_unicode(line)\n            comps = line.strip().split(':')\n            if comps[0] == name:\n                # Generate a getpwnam compatible output\n                comps[2], comps[3] = int(comps[2]), int(comps[3])\n                return pwd.struct_passwd(comps)\n    raise KeyError", "output": "Alternative implementation for getpwnam, that use only /etc/passwd", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filter(self, run_counts, criteria):\n    \"\"\"\n    \n    \"\"\"\n    correctness = criteria['correctness']\n    assert correctness.dtype == np.bool\n    filtered_counts = deep_copy(run_counts)\n    for key in filtered_counts:\n      filtered_counts[key] = filtered_counts[key][correctness]\n    return filtered_counts", "output": "Return run counts only for examples that are still correctly classified", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def weights_prepend_inputs_to_targets(labels):\n  \"\"\"\n  \"\"\"\n  past_first_zero = tf.cumsum(to_float(tf.equal(labels, 0)), axis=1)\n  nonzero = to_float(labels)\n  return to_float(tf.not_equal(past_first_zero * nonzero, 0))", "output": "Assign weight 1.0 to only the \"targets\" portion of the labels.\n\n  Weight 1.0 is assigned to all nonzero labels past the first zero.\n  See prepend_mode in common_hparams.py\n\n  Args:\n    labels: A Tensor of int32s.\n\n  Returns:\n    A Tensor of floats.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keyPressEvent(self, event):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        if event.key() in [Qt.Key_Enter, Qt.Key_Return]:\r\n            QTableWidget.keyPressEvent(self, event)\r\n            # To avoid having to enter one final tab\r\n            self.setDisabled(True)\r\n            self.setDisabled(False)\r\n            self._parent.keyPressEvent(event)\r\n        else:\r\n            QTableWidget.keyPressEvent(self, event)", "output": "Qt override.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def revert(self):\r\n        \"\"\"\"\"\"\r\n        index = self.get_stack_index()\r\n        finfo = self.data[index]\r\n        filename = finfo.filename\r\n        if finfo.editor.document().isModified():\r\n            self.msgbox = QMessageBox(\r\n                    QMessageBox.Warning,\r\n                    self.title,\r\n                    _(\"All changes to <b>%s</b> will be lost.\"\r\n                      \"<br>Do you want to revert file from disk?\"\r\n                      ) % osp.basename(filename),\r\n                    QMessageBox.Yes | QMessageBox.No,\r\n                    self)\r\n            answer = self.msgbox.exec_()\r\n            if answer != QMessageBox.Yes:\r\n                return\r\n        self.reload(index)", "output": "Revert file from disk", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_a(name=None, ipv4addr=None, allow_array=False, **api_opts):\n    '''\n    \n    '''\n    r = get_a(name, ipv4addr, allow_array=False, **api_opts)\n    if not r:\n        return True\n    if len(r) > 1 and not allow_array:\n        raise Exception('More than one result, use allow_array to override')\n    ret = []\n    for ri in r:\n        ret.append(delete_object(ri['_ref'], **api_opts))\n    return ret", "output": "Delete A record\n\n    If the A record is used as a round robin you can set ``allow_array=True`` to\n    delete all records for the hostname.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-call infoblox.delete_a name=abc.example.com\n        salt-call infoblox.delete_a ipv4addr=192.168.3.5\n        salt-call infoblox.delete_a name=acname.example.com allow_array=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_args(args):\n    \"\"\"\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Imports GramVaani data for Deep Speech\"\n    )\n    parser.add_argument(\n        \"--version\",\n        action=\"version\",\n        version=\"GramVaaniImporter {ver}\".format(ver=__version__),\n    )\n    parser.add_argument(\n        \"-v\",\n        \"--verbose\",\n        action=\"store_const\",\n        required=False,\n        help=\"set loglevel to INFO\",\n        dest=\"loglevel\",\n        const=logging.INFO,\n    )\n    parser.add_argument(\n        \"-vv\",\n        \"--very-verbose\",\n        action=\"store_const\",\n        required=False,\n        help=\"set loglevel to DEBUG\",\n        dest=\"loglevel\",\n        const=logging.DEBUG,\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--csv_filename\",\n        required=True,\n        help=\"Path to the GramVaani csv\",\n        dest=\"csv_filename\",\n    )\n    parser.add_argument(\n        \"-t\",\n        \"--target_dir\",\n        required=True,\n        help=\"Directory in which to save the importer GramVaani data\",\n        dest=\"target_dir\",\n    )\n    return parser.parse_args(args)", "output": "Parse command line parameters\n    Args:\n      args ([str]): Command line parameters as list of strings\n    Returns:\n      :obj:`argparse.Namespace`: command line parameters namespace", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_plugin(self):\r\n        \"\"\"\"\"\"\r\n        self.redirect_stdio.connect(self.main.redirect_internalshell_stdio)\r\n        self.main.console.shell.refresh.connect(self.refresh_plugin)\r\n        iconsize = 24 \r\n        self.toolbar.setIconSize(QSize(iconsize, iconsize))\r\n        self.main.addToolBar(self.toolbar)", "output": "Register plugin in Spyder's main window", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_from_json(self, obj, json, models=None, setter=None):\n        '''\n\n        '''\n        self._internal_set(obj, json, setter=setter)", "output": "Sets the value of this property from a JSON value.\n\n        Args:\n            obj: (HasProps) : instance to set the property value on\n\n            json: (JSON-value) : value to set to the attribute to\n\n            models (dict or None, optional) :\n                Mapping of model ids to models (default: None)\n\n                This is needed in cases where the attributes to update also\n                have values that have references.\n\n            setter (ClientSession or ServerSession or None, optional) :\n                This is used to prevent \"boomerang\" updates to Bokeh apps.\n                (default: None)\n\n                In the context of a Bokeh server application, incoming updates\n                to properties will be annotated with the session that is\n                doing the updating. This value is propagated through any\n                subsequent change notifications that the update triggers.\n                The session can compare the event setter to itself, and\n                suppress any updates that originate from itself.\n\n        Returns:\n            None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_data_batch(self, data_batch):\n        \"\"\"\"\"\"\n        if self.sym_gen is not None:\n            key = data_batch.bucket_key\n            if key not in self.execgrp_bucket:\n                # create new bucket entry\n                symbol = self.sym_gen(key)\n                execgrp = DataParallelExecutorGroup(symbol, self.arg_names,\n                                                    self.param_names, self.ctx,\n                                                    self.slices, data_batch,\n                                                    shared_group=self.execgrp)\n                self.execgrp_bucket[key] = execgrp\n\n            self.curr_execgrp = self.execgrp_bucket[key]\n        else:\n            self.curr_execgrp = self.execgrp\n\n        self.curr_execgrp.load_data_batch(data_batch)", "output": "Load data and labels into arrays.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def intranges_contain(int_, ranges):\n    \"\"\"\"\"\"\n    tuple_ = _encode_range(int_, 0)\n    pos = bisect.bisect_left(ranges, tuple_)\n    # we could be immediately ahead of a tuple (start, end)\n    # with start < int_ <= end\n    if pos > 0:\n        left, right = _decode_range(ranges[pos-1])\n        if left <= int_ < right:\n            return True\n    # or we could be immediately behind a tuple (int_, end)\n    if pos < len(ranges):\n        left, _ = _decode_range(ranges[pos])\n        if left == int_:\n            return True\n    return False", "output": "Determine if `int_` falls into one of the ranges in `ranges`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def commit(self, client=None):\n        \"\"\"\n        \"\"\"\n        if client is None:\n            client = self.client\n\n        kwargs = {\"logger_name\": self.logger.full_name}\n\n        if self.resource is not None:\n            kwargs[\"resource\"] = self.resource._to_dict()\n\n        if self.logger.labels is not None:\n            kwargs[\"labels\"] = self.logger.labels\n\n        entries = [entry.to_api_repr() for entry in self.entries]\n\n        client.logging_api.write_entries(entries, **kwargs)\n        del self.entries[:]", "output": "Send saved log entries as a single API call.\n\n        :type client: :class:`~google.cloud.logging.client.Client` or\n                      ``NoneType``\n        :param client: the client to use.  If not passed, falls back to the\n                       ``client`` stored on the current batch.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_deletemedia(mediaids, **kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'user.deletemedia'\n\n            if not isinstance(mediaids, list):\n                mediaids = [mediaids]\n            params = mediaids\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result']['mediaids']\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": "Delete media by id.\n\n    .. versionadded:: 2016.3.0\n\n    :param mediaids: IDs of the media to delete\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    :return: IDs of the deleted media, False on failure.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.user_deletemedia 27", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def import_data(self, data):\n        \"\"\"\n        \"\"\"\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(\"Importing data, current processing progress %s / %s\" %(_completed_num, len(data)))\n            _completed_num += 1\n            assert \"parameter\" in trial_info\n            _params = trial_info[\"parameter\"]\n            assert \"value\" in trial_info\n            _value = trial_info['value']\n            if not _value:\n                logger.info(\"Useless trial data, value is %s, skip this trial data.\" %_value)\n                continue\n            _params_tuple = convert_dict2tuple(_params)\n            self.supplement_data[_params_tuple] = True\n        logger.info(\"Successfully import data to grid search tuner.\")", "output": "Import additional data for tuning\n\n        Parameters\n        ----------\n        data:\n            a list of dictionarys, each of which has at least two keys, 'parameter' and 'value'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_hook(hook_name, hooks_dir='hooks'):\n    \"\"\"\n    \"\"\"\n    logger.debug('hooks_dir is {}'.format(os.path.abspath(hooks_dir)))\n\n    if not os.path.isdir(hooks_dir):\n        logger.debug('No hooks/ dir in template_dir')\n        return None\n\n    for hook_file in os.listdir(hooks_dir):\n        if valid_hook(hook_file, hook_name):\n            return os.path.abspath(os.path.join(hooks_dir, hook_file))\n\n    return None", "output": "Return a dict of all hook scripts provided.\n\n    Must be called with the project template as the current working directory.\n    Dict's key will be the hook/script's name, without extension, while values\n    will be the absolute path to the script. Missing scripts will not be\n    included in the returned dict.\n\n    :param hook_name: The hook to find\n    :param hooks_dir: The hook directory in the template\n    :return: The absolute path to the hook script or None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_domain_name(self,\n                           domain_name,\n                           certificate_name,\n                           certificate_body=None,\n                           certificate_private_key=None,\n                           certificate_chain=None,\n                           certificate_arn=None,\n                           lambda_name=None,\n                           stage=None,\n                           base_path=None):\n        \"\"\"\n        \n        \"\"\"\n\n        # This is a Let's Encrypt or custom certificate\n        if not certificate_arn:\n            agw_response = self.apigateway_client.create_domain_name(\n                domainName=domain_name,\n                certificateName=certificate_name,\n                certificateBody=certificate_body,\n                certificatePrivateKey=certificate_private_key,\n                certificateChain=certificate_chain\n            )\n        # This is an AWS ACM-hosted Certificate\n        else:\n            agw_response = self.apigateway_client.create_domain_name(\n                domainName=domain_name,\n                certificateName=certificate_name,\n                certificateArn=certificate_arn\n            )\n\n        api_id = self.get_api_id(lambda_name)\n        if not api_id:\n            raise LookupError(\"No API URL to certify found - did you deploy?\")\n\n        self.apigateway_client.create_base_path_mapping(\n            domainName=domain_name,\n            basePath='' if base_path is None else base_path,\n            restApiId=api_id,\n            stage=stage\n        )\n\n        return agw_response['distributionDomainName']", "output": "Creates the API GW domain and returns the resulting DNS name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_list(self, xs):\n        \"\"\"\n        \n        \"\"\"\n        if not self._is_batchable():\n            raise NotImplementedError('No batch method found')\n        elif not xs:\n            raise ValueError('Empty parameter list passed to parse_list')\n        else:\n            return self._batch_method(map(self.parse, xs))", "output": "Parse a list of values from the scheduler.\n\n        Only possible if this is_batchable() is True. This will combine the list into a single\n        parameter value using batch method. This should never need to be overridden.\n\n        :param xs: list of values to parse and combine\n        :return: the combined parsed values", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self, module, post_check):\n        ''' \n\n        '''\n        try:\n            # Simulate the sys.path behaviour decribed here:\n            #\n            # https://docs.python.org/2/library/sys.html#sys.path\n            _cwd = os.getcwd()\n            _sys_path = list(sys.path)\n            _sys_argv = list(sys.argv)\n            sys.path.insert(0, os.path.dirname(self._path))\n            sys.argv = [os.path.basename(self._path)] + self._argv\n\n            exec(self._code, module.__dict__)\n            post_check()\n\n        except Exception as e:\n            self._failed = True\n            self._error_detail = traceback.format_exc()\n\n            _exc_type, _exc_value, exc_traceback = sys.exc_info()\n            filename, line_number, func, txt = traceback.extract_tb(exc_traceback)[-1]\n\n            self._error = \"%s\\nFile \\\"%s\\\", line %d, in %s:\\n%s\" % (str(e), os.path.basename(filename), line_number, func, txt)\n\n        finally:\n            # undo sys.path, CWD fixups\n            os.chdir(_cwd)\n            sys.path = _sys_path\n            sys.argv = _sys_argv\n            self.ran = True", "output": "Execute the configured source code in a module and run any post\n        checks.\n\n        Args:\n            module (Module) : a module to execute the configured code in.\n\n            post_check(callable) : a function that can raise an exception\n                if expected post-conditions are not met after code execution.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fill_zeros(result, x, y, name, fill):\n    \"\"\"\n    \n    \"\"\"\n    if fill is None or is_float_dtype(result):\n        return result\n\n    if name.startswith(('r', '__r')):\n        x, y = y, x\n\n    is_variable_type = (hasattr(y, 'dtype') or hasattr(y, 'type'))\n    is_scalar_type = is_scalar(y)\n\n    if not is_variable_type and not is_scalar_type:\n        return result\n\n    if is_scalar_type:\n        y = np.array(y)\n\n    if is_integer_dtype(y):\n\n        if (y == 0).any():\n\n            # GH 7325, mask and nans must be broadcastable (also: PR 9308)\n            # Raveling and then reshaping makes np.putmask faster\n            mask = ((y == 0) & ~np.isnan(result)).ravel()\n\n            shape = result.shape\n            result = result.astype('float64', copy=False).ravel()\n\n            np.putmask(result, mask, fill)\n\n            # if we have a fill of inf, then sign it correctly\n            # (GH 6178 and PR 9308)\n            if np.isinf(fill):\n                signs = y if name.startswith(('r', '__r')) else x\n                signs = np.sign(signs.astype('float', copy=False))\n                negative_inf_mask = (signs.ravel() < 0) & mask\n                np.putmask(result, negative_inf_mask, -fill)\n\n            if \"floordiv\" in name:  # (PR 9308)\n                nan_mask = ((y == 0) & (x == 0)).ravel()\n                np.putmask(result, nan_mask, np.nan)\n\n            result = result.reshape(shape)\n\n    return result", "output": "If this is a reversed op, then flip x,y\n\n    If we have an integer value (or array in y)\n    and we have 0's, fill them with the fill,\n    return the result.\n\n    Mask the nan's from x.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def c_array(ctype, values):\n    \"\"\"\n    \"\"\"\n    out = (ctype * len(values))()\n    out[:] = values\n    return out", "output": "Create ctypes array from a Python array.\n\n    Parameters\n    ----------\n    ctype : ctypes data type\n        Data type of the array we want to convert to, such as mx_float.\n\n    values : tuple or list\n        Data content.\n\n    Returns\n    -------\n    out : ctypes array\n        Created ctypes array.\n\n    Examples\n    --------\n    >>> x = mx.base.c_array(mx.base.mx_float, [1, 2, 3])\n    >>> print len(x)\n    3\n    >>> x[1]\n    2.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_type_id_options(name, configuration):\n    '''\n    \n    '''\n    # it's in a form of source.name\n    if '.' in name:\n        type_, sep, id_ = name.partition('.')\n        options = configuration\n    else:\n        type_ = next(six.iterkeys(configuration))\n        id_ = name\n        options = configuration[type_]\n\n    return type_, id_, options", "output": "Returns the type, id and option of a configuration object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _netstat_linux():\n    '''\n    \n    '''\n    ret = []\n    cmd = 'netstat -tulpnea'\n    out = __salt__['cmd.run'](cmd)\n    for line in out.splitlines():\n        comps = line.split()\n        if line.startswith('tcp'):\n            ret.append({\n                'proto': comps[0],\n                'recv-q': comps[1],\n                'send-q': comps[2],\n                'local-address': comps[3],\n                'remote-address': comps[4],\n                'state': comps[5],\n                'user': comps[6],\n                'inode': comps[7],\n                'program': comps[8]})\n        if line.startswith('udp'):\n            ret.append({\n                'proto': comps[0],\n                'recv-q': comps[1],\n                'send-q': comps[2],\n                'local-address': comps[3],\n                'remote-address': comps[4],\n                'user': comps[5],\n                'inode': comps[6],\n                'program': comps[7]})\n    return ret", "output": "Return netstat information for Linux distros", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_kexgss_error(self, m):\n        \"\"\"\n        \n        \"\"\"\n        maj_status = m.get_int()\n        min_status = m.get_int()\n        err_msg = m.get_string()\n        m.get_string()  # we don't care about the language (lang_tag)!\n        raise SSHException(\n            \"\"\"GSS-API Error:\nMajor Status: {}\nMinor Status: {}\nError Message: {}\n\"\"\".format(\n                maj_status, min_status, err_msg\n            )\n        )", "output": "Parse the SSH2_MSG_KEXGSS_ERROR message (client mode).\n        The server may send a GSS-API error message. if it does, we display\n        the error by throwing an exception (client mode).\n\n        :param `Message` m:  The content of the SSH2_MSG_KEXGSS_ERROR message\n        :raise SSHException: Contains GSS-API major and minor status as well as\n                             the error message and the language tag of the\n                             message", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def for_app(*app_names, **kwargs):\n    \"\"\"\"\"\"\n    def _for_app(fn, command):\n        if is_app(command, *app_names, **kwargs):\n            return fn(command)\n        else:\n            return False\n\n    return decorator(_for_app)", "output": "Specifies that matching script is for on of app names.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sync_outlineexplorer_file_order(self):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        if self.outlineexplorer is not None:\r\n            self.outlineexplorer.treewidget.set_editor_ids_order(\r\n                [finfo.editor.get_document_id() for finfo in self.data])", "output": "Order the root file items of the outline explorer as in the tabbar\r\n        of the current EditorStack.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extract_queries(self, args, kwargs):\n        '''\n            \n        '''\n        return super(POSTGRESExtPillar, self).extract_queries(args, kwargs)", "output": "This function normalizes the config block into a set of queries we\n            can use.  The return is a list of consistently laid out dicts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_grad_processors(opt, gradprocs):\n    \"\"\"\n    \n    \"\"\"\n    assert isinstance(gradprocs, (list, tuple)), gradprocs\n    for gp in gradprocs:\n        assert isinstance(gp, GradientProcessor), gp\n\n    class _ApplyGradientProcessor(ProxyOptimizer):\n        def __init__(self, opt, gradprocs):\n            self._gradprocs = gradprocs[:]\n            super(_ApplyGradientProcessor, self).__init__(opt)\n\n        def apply_gradients(self, grads_and_vars,\n                            global_step=None, name=None):\n            g = self._apply(grads_and_vars)\n            return self._opt.apply_gradients(g, global_step, name)\n\n        def _apply(self, g):\n            for proc in self._gradprocs:\n                g = proc.process(g)\n            return g\n\n    return _ApplyGradientProcessor(opt, gradprocs)", "output": "Wrapper around optimizers to apply gradient processors.\n\n    Args:\n        opt (tf.train.Optimizer):\n        gradprocs (list[GradientProcessor]): gradient processors to add to the\n            optimizer.\n\n    Returns:\n        a :class:`tf.train.Optimizer` instance which runs the gradient\n        processors before updating the variables.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_launch_configuration(name, region=None, key=None, keyid=None,\n                                profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    retries = 30\n    while True:\n        try:\n            conn.delete_launch_configuration(name)\n            log.info('Deleted LC %s', name)\n            return True\n        except boto.exception.BotoServerError as e:\n            if retries and e.code == 'Throttling':\n                log.debug('Throttled by AWS API, retrying in 5 seconds...')\n                time.sleep(5)\n                retries -= 1\n                continue\n            log.error(e)\n            msg = 'Failed to delete LC {0}'.format(name)\n            log.error(msg)\n            return False", "output": "Delete a launch configuration.\n\n    CLI example::\n\n        salt myminion boto_asg.delete_launch_configuration mylc", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_lock(clear_func, role, remote=None, lock_type='update'):\n    '''\n    \n    '''\n    msg = 'Clearing {0} lock for {1} remotes'.format(lock_type, role)\n    if remote:\n        msg += ' matching {0}'.format(remote)\n    log.debug(msg)\n    return clear_func(remote=remote, lock_type=lock_type)", "output": "Function to allow non-fileserver functions to clear update locks\n\n    clear_func\n        A function reference. This function will be run (with the ``remote``\n        param as an argument) to clear the lock, and must return a 2-tuple of\n        lists, one containing messages describing successfully cleared locks,\n        and one containing messages describing errors encountered.\n\n    role\n        What type of lock is being cleared (gitfs, git_pillar, etc.). Used\n        solely for logging purposes.\n\n    remote\n        Optional string which should be used in ``func`` to pattern match so\n        that a subset of remotes can be targeted.\n\n    lock_type : update\n        Which type of lock to clear\n\n    Returns the return data from ``clear_func``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put_task(self, dp, callback=None):\n        \"\"\"\n        \n        \"\"\"\n        f = Future()\n        if callback is not None:\n            f.add_done_callback(callback)\n        self.input_queue.put((dp, f))\n        return f", "output": "Same as in :meth:`AsyncPredictorBase.put_task`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def readfiles():\n    \"\"\"  \"\"\"\n    tests = list(filter(lambda x: x.endswith('.py'), os.listdir(TESTPATH)))\n    tests.sort()\n    files = []\n\n    for test in tests:\n        text = open(TESTPATH + test, 'r').read()\n\n        try:\n            class_, desc, cause, workaround, code = [x.rstrip() for x in \\\n                                                    list(filter(None, re.split(SPLIT, text)))]\n            output = Output(test, class_, desc, cause, workaround, code, '', '', '')\n            files.append(output)\n        except IndexError:\n            print('Incorrect format in file ' + TESTPATH + test)\n\n    return files", "output": "Reads test files", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unpack_thin(thin_path):\n    '''\n    \n    '''\n    tfile = tarfile.TarFile.gzopen(thin_path)\n    old_umask = os.umask(0o077)  # pylint: disable=blacklisted-function\n    tfile.extractall(path=OPTIONS.saltdir)\n    tfile.close()\n    os.umask(old_umask)  # pylint: disable=blacklisted-function\n    try:\n        os.unlink(thin_path)\n    except OSError:\n        pass\n    reset_time(OPTIONS.saltdir)", "output": "Unpack the Salt thin archive.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def RegisterMessage(self, message):\n    \"\"\"\n    \"\"\"\n\n    desc = message.DESCRIPTOR\n    self._classes[desc.full_name] = message\n    self.pool.AddDescriptor(desc)\n    return message", "output": "Registers the given message type in the local database.\n\n    Calls to GetSymbol() and GetMessages() will return messages registered here.\n\n    Args:\n      message: a message.Message, to be registered.\n\n    Returns:\n      The provided message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def market_trade(self, security, amount, ttype=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        self._set_market_trade_params(security, amount)\n        if ttype is not None:\n            self._set_market_trade_type(ttype)\n        self._submit_trade()\n\n        return self._handle_pop_dialogs(\n            handler_class=pop_dialog_handler.TradePopDialogHandler\n        )", "output": "\u5e02\u4ef7\u4ea4\u6613\n        :param security: \u516d\u4f4d\u8bc1\u5238\u4ee3\u7801\n        :param amount: \u4ea4\u6613\u6570\u91cf\n        :param ttype: \u5e02\u4ef7\u59d4\u6258\u7c7b\u578b\uff0c\u9ed8\u8ba4\u5ba2\u6237\u7aef\u9ed8\u8ba4\u9009\u62e9\uff0c\n                     \u6df1\u5e02\u53ef\u9009 ['\u5bf9\u624b\u65b9\u6700\u4f18\u4ef7\u683c', '\u672c\u65b9\u6700\u4f18\u4ef7\u683c', '\u5373\u65f6\u6210\u4ea4\u5269\u4f59\u64a4\u9500', '\u6700\u4f18\u4e94\u6863\u5373\u65f6\u6210\u4ea4\u5269\u4f59 '\u5168\u989d\u6210\u4ea4\u6216\u64a4\u9500']\n                     \u6caa\u5e02\u53ef\u9009 ['\u6700\u4f18\u4e94\u6863\u6210\u4ea4\u5269\u4f59\u64a4\u9500', '\u6700\u4f18\u4e94\u6863\u6210\u4ea4\u5269\u4f59\u8f6c\u9650\u4ef7']\n\n        :return: {'entrust_no': '\u59d4\u6258\u5355\u53f7'}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ReraiseTypeErrorWithFieldName(message_name, field_name):\n  \"\"\"\"\"\"\n  exc = sys.exc_info()[1]\n  if len(exc.args) == 1 and type(exc) is TypeError:\n    # simple TypeError; add field name to exception message\n    exc = TypeError('%s for field %s.%s' % (str(exc), message_name, field_name))\n\n  # re-raise possibly-amended exception with original traceback:\n  six.reraise(type(exc), exc, sys.exc_info()[2])", "output": "Re-raise the currently-handled TypeError with the field name added.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def should_generate_summaries():\n  \"\"\"\n  \"\"\"\n  name_scope = tf.contrib.framework.get_name_scope()\n  if name_scope and \"while/\" in name_scope:\n    # Summaries don't work well within tf.while_loop()\n    return False\n  if tf.get_variable_scope().reuse:\n    # Avoid generating separate summaries for different data shards\n    return False\n  return True", "output": "Is this an appropriate context to generate summaries.\n\n  Returns:\n    a boolean", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copyDoc(self, recursive):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlCopyDoc(self._o, recursive)\n        if ret is None:raise treeError('xmlCopyDoc() failed')\n        __tmp = xmlDoc(_obj=ret)\n        return __tmp", "output": "Do a copy of the document info. If recursive, the content\n          tree will be copied too as well as DTD, namespaces and\n           entities.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parquet(self, *paths):\n        \"\"\"\n        \"\"\"\n        return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))", "output": "Loads Parquet files, returning the result as a :class:`DataFrame`.\n\n        You can set the following Parquet-specific option(s) for reading Parquet files:\n            * ``mergeSchema``: sets whether we should merge schemas collected from all \\\n                Parquet part-files. This will override ``spark.sql.parquet.mergeSchema``. \\\n                The default value is specified in ``spark.sql.parquet.mergeSchema``.\n\n        >>> df = spark.read.parquet('python/test_support/sql/parquet_partitioned')\n        >>> df.dtypes\n        [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetch_errors_from(self, path):\n        \"\"\" \n        \"\"\"\n        node = self.fetch_node_from(path)\n        if node is not None:\n            return node.errors\n        else:\n            return ErrorList()", "output": "Returns all errors for a particular path.\n\n        :param path: :class:`tuple` of :term:`hashable` s.\n        :rtype: :class:`~cerberus.errors.ErrorList`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def service_delete(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.delete_service(**kwargs)", "output": "Delete a service\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystoneng.service_delete name=glance\n        salt '*' keystoneng.service_delete name=39cc1327cdf744ab815331554430e8ec", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _track_tasks(task_ids, cluster):\n    \"\"\"\"\"\"\n    while True:\n        statuses = _get_task_statuses(task_ids, cluster)\n        if all([status == 'STOPPED' for status in statuses]):\n            logger.info('ECS tasks {0} STOPPED'.format(','.join(task_ids)))\n            break\n        time.sleep(POLL_TIME)\n        logger.debug('ECS task status for tasks {0}: {1}'.format(task_ids, statuses))", "output": "Poll task status until STOPPED", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_match(self, command):\n        \"\"\"\n\n        \"\"\"\n        if command.output is None and self.requires_output:\n            return False\n\n        try:\n            with logs.debug_time(u'Trying rule: {};'.format(self.name)):\n                if self.match(command):\n                    return True\n        except Exception:\n            logs.rule_failed(self, sys.exc_info())", "output": "Returns `True` if rule matches the command.\n\n        :type command: Command\n        :rtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def date_trunc(format, timestamp):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.date_trunc(format, _to_java_column(timestamp)))", "output": "Returns timestamp truncated to the unit specified by the format.\n\n    :param format: 'year', 'yyyy', 'yy', 'month', 'mon', 'mm',\n        'day', 'dd', 'hour', 'minute', 'second', 'week', 'quarter'\n\n    >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n    >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n    [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n    >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n    [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_std_icon(name, size=None):\n    \"\"\"\"\"\"\n    if not name.startswith('SP_'):\n        name = 'SP_' + name\n    icon = QWidget().style().standardIcon(getattr(QStyle, name))\n    if size is None:\n        return icon\n    else:\n        return QIcon(icon.pixmap(size, size))", "output": "Get standard platform icon\n    Call 'show_std_icons()' for details", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_new_layer(layer, n_dim):\n    ''' \n    '''\n\n    input_shape = layer.output.shape\n    dense_deeper_classes = [StubDense, get_dropout_class(n_dim), StubReLU]\n    conv_deeper_classes = [get_conv_class(n_dim), get_batch_norm_class(n_dim), StubReLU]\n    if is_layer(layer, \"ReLU\"):\n        conv_deeper_classes = [get_conv_class(n_dim), get_batch_norm_class(n_dim)]\n        dense_deeper_classes = [StubDense, get_dropout_class(n_dim)]\n    elif is_layer(layer, \"Dropout\"):\n        dense_deeper_classes = [StubDense, StubReLU]\n    elif is_layer(layer, \"BatchNormalization\"):\n        conv_deeper_classes = [get_conv_class(n_dim), StubReLU]\n\n    layer_class = None\n    if len(input_shape) == 1:\n        # It is in the dense layer part.\n        layer_class = sample(dense_deeper_classes, 1)[0]\n    else:\n        # It is in the conv layer part.\n        layer_class = sample(conv_deeper_classes, 1)[0]\n\n    if layer_class == StubDense:\n        new_layer = StubDense(input_shape[0], input_shape[0])\n\n    elif layer_class == get_dropout_class(n_dim):\n        new_layer = layer_class(Constant.DENSE_DROPOUT_RATE)\n\n    elif layer_class == get_conv_class(n_dim):\n        new_layer = layer_class(\n            input_shape[-1], input_shape[-1], sample((1, 3, 5), 1)[0], stride=1\n        )\n\n    elif layer_class == get_batch_norm_class(n_dim):\n        new_layer = layer_class(input_shape[-1])\n\n    elif layer_class == get_pooling_class(n_dim):\n        new_layer = layer_class(sample((1, 3, 5), 1)[0])\n\n    else:\n        new_layer = layer_class()\n\n    return new_layer", "output": "create  new layer for the graph", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _serve_image_metadata(self, request):\n    \"\"\"\n    \"\"\"\n    tag = request.args.get('tag')\n    run = request.args.get('run')\n    sample = int(request.args.get('sample', 0))\n    response = self._image_response_for_run(run, tag, sample)\n    return http_util.Respond(request, response, 'application/json')", "output": "Given a tag and list of runs, serve a list of metadata for images.\n\n    Note that the images themselves are not sent; instead, we respond with URLs\n    to the images. The frontend should treat these URLs as opaque and should not\n    try to parse information about them or generate them itself, as the format\n    may change.\n\n    Args:\n      request: A werkzeug.wrappers.Request object.\n\n    Returns:\n      A werkzeug.Response application.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_gen_model_stats(self, iteration:int)->None:\n        \"\"\n        generator = self.learn.gan_trainer.generator\n        self.stats_writer.write(model=generator, iteration=iteration, tbwriter=self.tbwriter, name='gen_model_stats')\n        self.gen_stats_updated = True", "output": "Writes gradient statistics for generator to Tensorboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_image_path(image_lists, label_name, index, image_dir, category):\n  \"\"\"\n\n  \"\"\"\n  if label_name not in image_lists:\n    tf.logging.fatal('Label does not exist %s.', label_name)\n  label_lists = image_lists[label_name]\n  if category not in label_lists:\n    tf.logging.fatal('Category does not exist %s.', category)\n  category_list = label_lists[category]\n  if not category_list:\n    tf.logging.fatal('Label %s has no images in the category %s.',\n                     label_name, category)\n  mod_index = index % len(category_list)\n  base_name = category_list[mod_index]\n  sub_dir = label_lists['dir']\n  full_path = os.path.join(image_dir, sub_dir, base_name)\n  return full_path", "output": "Returns a path to an image for a label at the given index.\n\n  Args:\n    image_lists: OrderedDict of training images for each label.\n    label_name: Label string we want to get an image for.\n    index: Int offset of the image we want. This will be moduloed by the\n    available number of images for the label, so it can be arbitrarily large.\n    image_dir: Root folder string of the subfolders containing the training\n    images.\n    category: Name string of set to pull images from - training, testing, or\n    validation.\n\n  Returns:\n    File system path string to an image that meets the requested parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_locations(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_locations function must be called with '\n            '-f or --function, or with the --list-locations option'\n        )\n\n    ret = {}\n    for key in JOYENT_LOCATIONS:\n        ret[key] = {\n            'name': key,\n            'region': JOYENT_LOCATIONS[key]\n        }\n\n    # this can be enabled when the bug in the joyent get data centers call is\n    # corrected, currently only the European dc (new api) returns the correct\n    # values\n    # ret = {}\n    # rcode, datacenters = query(\n    #     command='my/datacenters', location=DEFAULT_LOCATION, method='GET'\n    # )\n    # if rcode in VALID_RESPONSE_CODES and isinstance(datacenters, dict):\n    #     for key in datacenters:\n    #     ret[key] = {\n    #         'name': key,\n    #         'url': datacenters[key]\n    #     }\n    return ret", "output": "List all available locations", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def commit(self):\n        \"\"\"\n        \"\"\"\n        if self._status != self._IN_PROGRESS:\n            raise ValueError(\"Batch must be in progress to commit()\")\n\n        try:\n            self._commit()\n        finally:\n            self._status = self._FINISHED", "output": "Commits the batch.\n\n        This is called automatically upon exiting a with statement,\n        however it can be called explicitly if you don't want to use a\n        context manager.\n\n        :raises: :class:`~exceptions.ValueError` if the batch is not\n                 in progress.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def slice(self, begin, end):\n        \"\"\"\n        \n        \"\"\"\n        jrdds = self._jdstream.slice(self._jtime(begin), self._jtime(end))\n        return [RDD(jrdd, self._sc, self._jrdd_deserializer) for jrdd in jrdds]", "output": "Return all the RDDs between 'begin' to 'end' (both included)\n\n        `begin`, `end` could be datetime.datetime() or unix_timestamp", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop(self):\n        '''\n        \n        '''\n        # avoid getting called twice\n        self.cleanup()\n        if self.running:\n            self.running = False\n            self.timer_stop.set()\n            self.timer.join()", "output": "shutdown cache process", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def all_row_ids(data_batch):\n    \"\"\"  \"\"\"\n    all_users = mx.nd.arange(0, MOVIELENS['max_user'], dtype='int64')\n    all_movies = mx.nd.arange(0, MOVIELENS['max_movie'], dtype='int64')\n    return {'user_weight': all_users, 'item_weight': all_movies}", "output": "Generate row ids for all rows", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def current_ioloop(io_loop):\n    '''\n    \n    '''\n    orig_loop = tornado.ioloop.IOLoop.current()\n    io_loop.make_current()\n    try:\n        yield\n    finally:\n        orig_loop.make_current()", "output": "A context manager that will set the current ioloop to io_loop for the context", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_native_types(self, slicer=None, na_rep='', quoting=None, **kwargs):\n        \"\"\"  \"\"\"\n\n        values = self.values\n        if slicer is not None:\n            # Categorical is always one dimension\n            values = values[slicer]\n        mask = isna(values)\n        values = np.array(values, dtype='object')\n        values[mask] = na_rep\n\n        # we are expected to return a 2-d ndarray\n        return values.reshape(1, len(values))", "output": "convert to our native types format, slicing if desired", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_code_completion(self):\r\n        \"\"\"\"\"\"\r\n        # Note: unicode conversion is needed only for ExternalShellBase\r\n        text = to_text_string(self.get_current_line_to_cursor())\r\n        last_obj = self.get_last_obj()\r\n        if not text:\r\n            return\r\n\r\n        obj_dir = self.get_dir(last_obj)\r\n        if last_obj and obj_dir and text.endswith('.'):\r\n            self.show_completion_list(obj_dir)\r\n            return\r\n        \r\n        # Builtins and globals\r\n        if not text.endswith('.') and last_obj \\\r\n           and re.match(r'[a-zA-Z_0-9]*$', last_obj):\r\n            b_k_g = dir(builtins)+self.get_globals_keys()+keyword.kwlist\r\n            for objname in b_k_g:\r\n                if objname.startswith(last_obj) and objname != last_obj:\r\n                    self.show_completion_list(b_k_g, completion_text=last_obj)\r\n                    return\r\n            else:\r\n                return\r\n        \r\n        # Looking for an incomplete completion\r\n        if last_obj is None:\r\n            last_obj = text\r\n        dot_pos = last_obj.rfind('.')\r\n        if dot_pos != -1:\r\n            if dot_pos == len(last_obj)-1:\r\n                completion_text = \"\"\r\n            else:\r\n                completion_text = last_obj[dot_pos+1:]\r\n                last_obj = last_obj[:dot_pos]\r\n            completions = self.get_dir(last_obj)\r\n            if completions is not None:\r\n                self.show_completion_list(completions,\r\n                                          completion_text=completion_text)\r\n                return\r\n        \r\n        # Looking for ' or \": filename completion\r\n        q_pos = max([text.rfind(\"'\"), text.rfind('\"')])\r\n        if q_pos != -1:\r\n            completions = self.get_cdlistdir()\r\n            if completions:\r\n                self.show_completion_list(completions,\r\n                                          completion_text=text[q_pos+1:])\r\n            return", "output": "Display a completion list based on the current line", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def CurrentDoc(self):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlTextReaderCurrentDoc(self._o)\n        if ret is None:raise treeError('xmlTextReaderCurrentDoc() failed')\n        __tmp = xmlDoc(_obj=ret)\n        return __tmp", "output": "Hacking interface allowing to get the xmlDocPtr\n          correponding to the current document being accessed by the\n          xmlTextReader. NOTE: as a result of this call, the reader\n          will not destroy the associated XML document and calling\n          xmlFreeDoc() on the result is needed once the reader\n           parsing has finished.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_filter_pillar(filter_name,\n                      pillar_key='acl',\n                      pillarenv=None,\n                      saltenv=None):\n    '''\n    \n    '''\n    pillar_cfg = _get_pillar_cfg(pillar_key,\n                                 pillarenv=pillarenv,\n                                 saltenv=saltenv)\n    return _lookup_element(pillar_cfg, filter_name)", "output": "Helper that can be used inside a state SLS,\n    in order to get the filter configuration given its name.\n\n    filter_name\n        The name of the filter.\n\n    pillar_key\n        The root key of the whole policy config.\n\n    pillarenv\n        Query the master to generate fresh pillar data on the fly,\n        specifically from the requested pillar environment.\n\n    saltenv\n        Included only for compatibility with\n        :conf_minion:`pillarenv_from_saltenv`, and is otherwise ignored.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def date_range(cls,start_time,end_time,freq):\n        '''\n        \n       '''\n\n        if not isinstance(start_time,datetime.datetime):\n            raise TypeError(\"The ``start_time`` argument must be from type datetime.datetime.\")\n\n        if not isinstance(end_time,datetime.datetime):\n            raise TypeError(\"The ``end_time`` argument must be from type datetime.datetime.\")\n\n        if not isinstance(freq,datetime.timedelta):\n            raise TypeError(\"The ``freq`` argument must be from type datetime.timedelta.\")\n\n        from .. import extensions\n        return extensions.date_range(start_time,end_time,freq.total_seconds())", "output": "Returns a new SArray that represents a fixed frequency datetime index.\n\n        Parameters\n        ----------\n        start_time : datetime.datetime\n          Left bound for generating dates.\n\n        end_time : datetime.datetime\n          Right bound for generating dates.\n\n        freq : datetime.timedelta\n          Fixed frequency between two consecutive data points.\n\n        Returns\n        -------\n        out : SArray\n\n        Examples\n        --------\n        >>> import datetime as dt\n        >>> start = dt.datetime(2013, 5, 7, 10, 4, 10)\n        >>> end = dt.datetime(2013, 5, 10, 10, 4, 10)\n        >>> sa = tc.SArray.date_range(start,end,dt.timedelta(1))\n        >>> print sa\n        dtype: datetime\n        Rows: 4\n        [datetime.datetime(2013, 5, 7, 10, 4, 10),\n         datetime.datetime(2013, 5, 8, 10, 4, 10),\n         datetime.datetime(2013, 5, 9, 10, 4, 10),\n         datetime.datetime(2013, 5, 10, 10, 4, 10)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_runtime_class(self, name, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_runtime_class_with_http_info(name, body, **kwargs)\n        else:\n            (data) = self.replace_runtime_class_with_http_info(name, body, **kwargs)\n            return data", "output": "replace the specified RuntimeClass\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_runtime_class(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the RuntimeClass (required)\n        :param V1beta1RuntimeClass body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1beta1RuntimeClass\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_element(raw_element: str) -> List[Element]:\n    \"\"\"\n    \n    \"\"\"\n    elements = [regex.match(\"^(([a-zA-Z]+)\\(([^;]+),List\\(([^;]*)\\)\\))$\",\n                            elem.lstrip().rstrip())\n                for elem\n                in raw_element.split(';')]\n    return [interpret_element(*elem.groups()[1:])\n            for elem in elements\n            if elem]", "output": "Parse a raw element into text and indices (integers).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def random_inputs(\n    num_devices,\n    input_shape=gin.REQUIRED, input_dtype=np.int32, input_range=(0, 255),\n    output_shape=gin.REQUIRED, output_dtype=np.int32, output_range=(0, 9)):\n  \"\"\"\n  \"\"\"\n  if input_shape[0] % num_devices != 0:\n    tf.logging.fatal(\n        \"num_devices[%d] should divide the first dimension of input_shape[%s]\",\n        num_devices, input_shape)\n  if output_shape[0] % num_devices != 0:\n    tf.logging.fatal(\n        \"num_devices[%d] should divide the first dimension of output_shape[%s]\",\n        num_devices, output_shape)\n\n  def random_minibatches():\n    \"\"\"Generate a stream of random mini-batches.\"\"\"\n    if input_dtype in [np.float16, np.float32, np.float64]:\n      rand = np.random.uniform\n    else:\n      rand = np.random.random_integers\n    while True:\n      inp = rand(input_range[0], input_range[1], input_shape)\n      inp = inp.astype(input_dtype)\n      out = rand(output_range[0], output_range[1], output_shape)\n      out = out.astype(output_dtype)\n      yield inp, out\n\n  input_shape_without_batch = list(input_shape)[1:]\n  return Inputs(train_stream=random_minibatches,\n                train_eval_stream=random_minibatches,\n                eval_stream=random_minibatches,\n                input_shape=input_shape_without_batch)", "output": "Make random Inputs for debugging.\n\n  Args:\n    num_devices: how many devices to build the inputs for.\n    input_shape: the shape of inputs (including batch dimension).\n    input_dtype: the type of the inputs (int32 by default).\n    input_range: the range of inputs (defaults to (0, 255)).\n    output_shape: the shape of outputs (including batch dimension).\n    output_dtype: the type of the outputs (int32 by default).\n    output_range: the range of outputs (defaults to (0, 9)).\n\n  Returns:\n    trax.inputs.Inputs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _servicegroup_get_server(sg_name, s_name, s_port=None, **connection_args):\n    '''\n    \n    '''\n    ret = None\n    servers = _servicegroup_get_servers(sg_name, **connection_args)\n    if servers is None:\n        return None\n    for server in servers:\n        if server.get_servername() == s_name:\n            if s_port is not None and s_port != server.get_port():\n                ret = None\n            ret = server\n    return ret", "output": "Returns a member of a service group or None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def minute_frame_to_session_frame(minute_frame, calendar):\n\n    \"\"\"\n    \n    \"\"\"\n    how = OrderedDict((c, _MINUTE_TO_SESSION_OHCLV_HOW[c])\n                      for c in minute_frame.columns)\n    labels = calendar.minute_index_to_session_labels(minute_frame.index)\n    return minute_frame.groupby(labels).agg(how)", "output": "Resample a DataFrame with minute data into the frame expected by a\n    BcolzDailyBarWriter.\n\n    Parameters\n    ----------\n    minute_frame : pd.DataFrame\n        A DataFrame with the columns `open`, `high`, `low`, `close`, `volume`,\n        and `dt` (minute dts)\n    calendar : trading_calendars.trading_calendar.TradingCalendar\n        A TradingCalendar on which session labels to resample from minute\n        to session.\n\n    Return\n    ------\n    session_frame : pd.DataFrame\n        A DataFrame with the columns `open`, `high`, `low`, `close`, `volume`,\n        and `day` (datetime-like).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tree(self, path):\n        '''\n        \n        '''\n        ret = {}\n        try:\n            items = self.read(path)\n        except (etcd.EtcdKeyNotFound, ValueError):\n            return None\n        except etcd.EtcdConnectionFailed:\n            log.error(\"etcd: failed to perform 'tree' operation on path %s due to connection error\", path)\n            return None\n\n        for item in items.children:\n            comps = six.text_type(item.key).split('/')\n            if item.dir is True:\n                if item.key == path:\n                    continue\n                ret[comps[-1]] = self.tree(item.key)\n            else:\n                ret[comps[-1]] = item.value\n        return ret", "output": ".. versionadded:: 2014.7.0\n\n        Recurse through etcd and return all values", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _api_version(profile=None, **connection_args):\n    '''\n    \n    '''\n    global _TENANT_ID\n    global _OS_IDENTITY_API_VERSION\n    try:\n        if float(__salt__['keystone.api_version'](profile=profile, **connection_args).strip('v')) >= 3:\n            _TENANT_ID = 'project_id'\n            _OS_IDENTITY_API_VERSION = 3\n    except KeyError:\n        pass", "output": "Sets global variables _OS_IDENTITY_API_VERSION and _TENANT_ID\n    depending on API version.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_plugin(name, path, user):\n    '''\n    \n    '''\n    ret = {'name': name}\n    resp = __salt__['cmd.shell']((\n        'wp --path={0} plugin status {1}'\n    ).format(path, name), runas=user).split('\\n')\n    for line in resp:\n        if 'Status' in line:\n            ret['status'] = line.split(' ')[-1].lower()\n        elif 'Version' in line:\n            ret['version'] = line.split(' ')[-1].lower()\n    return ret", "output": "Show a plugin in a wordpress install and check if it is installed\n\n    name\n        Wordpress plugin name\n\n    path\n        path to wordpress install location\n\n    user\n        user to run the command as\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' wordpress.show_plugin HyperDB /var/www/html apache", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compare_urls(self, url1, url2):\n        # type: (str, str) -> bool\n        \"\"\"\n        \n        \"\"\"\n        return (self.normalize_url(url1) == self.normalize_url(url2))", "output": "Compare two repo URLs for identity, ignoring incidental differences.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dragEnterEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        source = event.mimeData()\r\n        if source.hasUrls():\r\n            if mimedata2url(source):\r\n                event.acceptProposedAction()\r\n            else:\r\n                event.ignore()\r\n        elif source.hasText():\r\n            event.acceptProposedAction()", "output": "Reimplement Qt method\r\n        Inform Qt about the types of data that the widget accepts", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def produce_csv(input, output):\n    \n    '''\n    output.write('\"model\",\"mean\",\"std\"\\n')\n    for model_data in input:\n        output.write('\"%s\",%f,%f\\n' % (model_data['name'], model_data['mean'], model_data['stddev']))\n    output.flush()\n    output.close()\n    print(\"Wrote as %s\" % output.name)", "output": "r'''\n    Take an input dictionnary and write it to the object-file output.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def frame_generator(self):\n        \"\"\"\"\"\"\n        if self.input_rate == self.RATE_PROCESS:\n            while True:\n                yield self.read()\n        else:\n            while True:\n                yield self.read_resampled()", "output": "Generator that yields all audio frames from microphone.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_trials(self):\n        \"\"\"\n        \"\"\"\n        trials = list(self._trial_generator)\n        if self._shuffle:\n            random.shuffle(trials)\n        self._finished = True\n        return trials", "output": "Provides Trial objects to be queued into the TrialRunner.\n\n        Returns:\n            trials (list): Returns a list of trials.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_installed(pkgname=None,\n                 bin_env=None,\n                 user=None,\n                 cwd=None):\n    '''\n    \n    '''\n    for line in freeze(bin_env=bin_env, user=user, cwd=cwd):\n        if line.startswith('-f') or line.startswith('#'):\n            # ignore -f line as it contains --find-links directory\n            # ignore comment lines\n            continue\n        elif line.startswith('-e hg+not trust'):\n            # ignore hg + not trust problem\n            continue\n        elif line.startswith('-e'):\n            line = line.split('-e ')[1]\n            version_, name = line.split('#egg=')\n        elif len(line.split('===')) >= 2:\n            name = line.split('===')[0]\n            version_ = line.split('===')[1]\n        elif len(line.split('==')) >= 2:\n            name = line.split('==')[0]\n            version_ = line.split('==')[1]\n        else:\n            logger.error('Can\\'t parse line \\'%s\\'', line)\n            continue\n\n        if pkgname:\n            if pkgname == name.lower():\n                return True\n\n    return False", "output": ".. versionadded:: 2018.3.0\n\n    Filter list of installed apps from ``freeze`` and return True or False  if\n    ``pkgname`` exists in the list of packages installed.\n\n    .. note::\n        If the version of pip available is older than 8.0.3, the packages\n        wheel, setuptools, and distribute will not be reported by this function\n        even if they are installed. Unlike :py:func:`pip.freeze\n        <salt.modules.pip.freeze>`, this function always reports the version of\n        pip which is installed.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pip.is_installed salt", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pct_change(self, periods=1, fill_method='pad', limit=None, freq=None):\n        \"\"\"\"\"\"\n        # TODO: Remove this conditional when #23918 is fixed\n        if freq:\n            return self.apply(lambda x: x.pct_change(periods=periods,\n                                                     fill_method=fill_method,\n                                                     limit=limit, freq=freq))\n        filled = getattr(self, fill_method)(limit=limit)\n        fill_grp = filled.groupby(self.grouper.labels)\n        shifted = fill_grp.shift(periods=periods, freq=freq)\n\n        return (filled / shifted) - 1", "output": "Calcuate pct_change of each value to previous entry in group", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_section(file_name, section, separator='='):\n    '''\n    \n    '''\n    inifile = _Ini.get_ini_file(file_name, separator=separator)\n    if section in inifile:\n        section = inifile.pop(section)\n        inifile.flush()\n        ret = {}\n        for key, value in six.iteritems(section):\n            if key[0] != '#':\n                ret.update({key: value})\n        return ret", "output": "Remove a section in an ini file. Returns the removed section as dictionary,\n    or ``None`` if nothing was removed.\n\n    API Example:\n\n    .. code-block:: python\n\n        import salt\n        sc = salt.client.get_local_client()\n        sc.cmd('target', 'ini.remove_section',\n               [path_to_ini_file, section_name])\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ini.remove_section /path/to/ini section_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def safe_walk(top, topdown=True, onerror=None, followlinks=True, _seen=None):\n    '''\n    \n    '''\n    if _seen is None:\n        _seen = set()\n\n    # We may not have read permission for top, in which case we can't\n    # get a list of the files the directory contains.  os.path.walk\n    # always suppressed the exception then, rather than blow up for a\n    # minor reason when (say) a thousand readable directories are still\n    # left to visit.  That logic is copied here.\n    try:\n        # Note that listdir and error are globals in this module due\n        # to earlier import-*.\n        names = os.listdir(top)\n    except os.error as err:\n        if onerror is not None:\n            onerror(err)\n        return\n\n    if followlinks:\n        status = os.stat(top)\n        # st_ino is always 0 on some filesystems (FAT, NTFS); ignore them\n        if status.st_ino != 0:\n            node = (status.st_dev, status.st_ino)\n            if node in _seen:\n                return\n            _seen.add(node)\n\n    dirs, nondirs = [], []\n    for name in names:\n        full_path = os.path.join(top, name)\n        if os.path.isdir(full_path):\n            dirs.append(name)\n        else:\n            nondirs.append(name)\n\n    if topdown:\n        yield top, dirs, nondirs\n    for name in dirs:\n        new_path = os.path.join(top, name)\n        if followlinks or not os.path.islink(new_path):\n            for x in safe_walk(new_path, topdown, onerror, followlinks, _seen):\n                yield x\n    if not topdown:\n        yield top, dirs, nondirs", "output": "A clone of the python os.walk function with some checks for recursive\n    symlinks. Unlike os.walk this follows symlinks by default.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def patch(patch_inspect=True):\n    \"\"\"\n    \n    \"\"\"\n    PATCHED['collections.abc.Generator'] = _collections_abc.Generator = Generator\n    PATCHED['collections.abc.Coroutine'] = _collections_abc.Coroutine = Coroutine\n    PATCHED['collections.abc.Awaitable'] = _collections_abc.Awaitable = Awaitable\n\n    if patch_inspect:\n        import inspect\n        PATCHED['inspect.isawaitable'] = inspect.isawaitable = isawaitable", "output": "Main entry point for patching the ``collections.abc`` and ``inspect``\n    standard library modules.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_export_pipeline_code(pipeline_tree, operators):\n    \"\"\"\n\n    \"\"\"\n    steps = _process_operator(pipeline_tree, operators)\n    # number of steps in a pipeline\n    num_step = len(steps)\n    if num_step > 1:\n        pipeline_text = \"make_pipeline(\\n{STEPS}\\n)\".format(STEPS=_indent(\",\\n\".join(steps), 4))\n    # only one operator (root = True)\n    else:\n        pipeline_text = \"{STEPS}\".format(STEPS=_indent(\",\\n\".join(steps), 0))\n\n    return pipeline_text", "output": "Generate code specific to the construction of the sklearn Pipeline for export_pipeline.\n\n    Parameters\n    ----------\n    pipeline_tree: list\n        List of operators in the current optimized pipeline\n\n    Returns\n    -------\n    Source code for the sklearn pipeline", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_indicator_BOLL(DataFrame, N=20, P=2):\n    ''\n    C = DataFrame['close']\n    boll = MA(C, N)\n    UB = boll + P * STD(C, N)\n    LB = boll - P * STD(C, N)\n    DICT = {'BOLL': boll, 'UB': UB, 'LB': LB}\n\n    return pd.DataFrame(DICT)", "output": "\u5e03\u6797\u7ebf", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close(self, force=True):\n        ''' '''\n        if not self.closed:\n            self.flush()\n            self.fileobj.close() # Closes the file descriptor\n            # Give kernel time to update process status.\n            time.sleep(self.delayafterclose)\n            if self.isalive():\n                if not self.terminate(force):\n                    raise PtyProcessError('Could not terminate the child.')\n            self.fd = -1\n            self.closed = True", "output": "This closes the connection with the child application. Note that\n        calling close() more than once is valid. This emulates standard Python\n        behavior with files. Set force to True if you want to make sure that\n        the child is terminated (SIGKILL is sent if the child ignores SIGHUP\n        and SIGINT).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def container_config_get(name, config_key, remote_addr=None,\n                         cert=None, key=None, verify_cert=True):\n    '''\n    \n    '''\n    container = container_get(\n        name, remote_addr, cert, key, verify_cert, _raw=True\n    )\n    return _get_property_dict_item(container, 'config', config_key)", "output": "Get a container config value\n\n    name :\n        Name of the container\n\n    config_key :\n        The config key to retrieve\n\n    remote_addr :\n        An URL to a remote Server, you also have to give cert and key if\n        you provide remote_addr and its a TCP Address!\n\n        Examples:\n            https://myserver.lan:8443\n            /var/lib/mysocket.sock\n\n    cert :\n        PEM Formatted SSL Certificate.\n\n        Examples:\n            ~/.config/lxc/client.crt\n\n    key :\n        PEM Formatted SSL Key.\n\n        Examples:\n            ~/.config/lxc/client.key\n\n    verify_cert : True\n        Wherever to verify the cert, this is by default True\n        but in the most cases you want to set it off as LXD\n        normaly uses self-signed certificates.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _filters_pb(self):\n        \"\"\"\n        \"\"\"\n        num_filters = len(self._field_filters)\n        if num_filters == 0:\n            return None\n        elif num_filters == 1:\n            return _filter_pb(self._field_filters[0])\n        else:\n            composite_filter = query_pb2.StructuredQuery.CompositeFilter(\n                op=enums.StructuredQuery.CompositeFilter.Operator.AND,\n                filters=[_filter_pb(filter_) for filter_ in self._field_filters],\n            )\n            return query_pb2.StructuredQuery.Filter(composite_filter=composite_filter)", "output": "Convert all the filters into a single generic Filter protobuf.\n\n        This may be a lone field filter or unary filter, may be a composite\n        filter or may be :data:`None`.\n\n        Returns:\n            google.cloud.firestore_v1beta1.types.\\\n            StructuredQuery.Filter: A \"generic\" filter representing the\n            current query's filters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_connection(self, host):\n        \"\"\"\n        \n        \"\"\"\n        self.hosts.append(host)\n        self.set_connections(self.hosts)", "output": "Create a new :class:`~elasticsearch.Connection` instance and add it to the pool.\n\n        :arg host: kwargs that will be used to create the instance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _snake_to_camel_case(value):\n    \"\"\"\"\"\"\n    words = value.split(\"_\")\n    return words[0] + \"\".join(map(str.capitalize, words[1:]))", "output": "Convert snake case string to camel case.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lcs(s1, s2, i, j):\n    \"\"\"\n    \n    \"\"\"\n    if i == 0 or j == 0:\n        return 0\n    elif s1[i - 1] == s2[j - 1]:\n        return 1 + lcs(s1, s2, i - 1, j - 1)\n    else:\n        return max(lcs(s1, s2, i - 1, j), lcs(s1, s2, i, j - 1))", "output": "The length of longest common subsequence among the two given strings s1 and s2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_conn():\n    '''\n    \n    '''\n    if __active_provider_name__ in __context__:\n        return __context__[__active_provider_name__]\n    vm_ = get_configured_provider()\n    profile = vm_.pop('profile', None)\n    if profile is not None:\n        vm_ = __utils__['dictupdate.update'](os_client_config.vendors.get_profile(profile), vm_)\n    conn = shade.openstackcloud.OpenStackCloud(cloud_config=None, **vm_)\n    if __active_provider_name__ is not None:\n        __context__[__active_provider_name__] = conn\n    return conn", "output": "Return a conn object for the passed VM data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tmppath(path=None, include_unix_username=True):\n    \"\"\"\n    \n    \"\"\"\n    addon = \"luigitemp-%08d\" % random.randrange(1e9)\n    temp_dir = '/tmp'  # default tmp dir if none is specified in config\n\n    # 1. Figure out to which temporary directory to place\n    configured_hdfs_tmp_dir = hdfs().tmp_dir\n    if configured_hdfs_tmp_dir is not None:\n        # config is superior\n        base_dir = configured_hdfs_tmp_dir\n    elif path is not None:\n        # need to copy correct schema and network location\n        parsed = urlparse(path)\n        base_dir = urlunparse((parsed.scheme, parsed.netloc, temp_dir, '', '', ''))\n    else:\n        # just system temporary directory\n        base_dir = temp_dir\n\n    # 2. Figure out what to place\n    if path is not None:\n        if path.startswith(temp_dir + '/'):\n            # Not 100%, but some protection from directories like /tmp/tmp/file\n            subdir = path[len(temp_dir):]\n        else:\n            # Protection from /tmp/hdfs:/dir/file\n            parsed = urlparse(path)\n            subdir = parsed.path\n        subdir = subdir.lstrip('/') + '-'\n    else:\n        # just return any random temporary location\n        subdir = ''\n\n    if include_unix_username:\n        subdir = os.path.join(getpass.getuser(), subdir)\n\n    return os.path.join(base_dir, subdir + addon)", "output": "@param path: target path for which it is needed to generate temporary location\n    @type path: str\n    @type include_unix_username: bool\n    @rtype: str\n\n    Note that include_unix_username might work on windows too.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(name, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The start action must be called with -a or --action.'\n        )\n\n    log.info('Starting node %s', name)\n\n    return vm_action(name, kwargs={'action': 'resume'}, call=call)", "output": "Start a VM.\n\n    .. versionadded:: 2016.3.0\n\n    name\n        The name of the VM to start.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a start my-vm", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ensure_object(self, object_type):\n        \"\"\"\n        \"\"\"\n        rv = self.find_object(object_type)\n        if rv is None:\n            self.obj = rv = object_type()\n        return rv", "output": "Like :meth:`find_object` but sets the innermost object to a\n        new instance of `object_type` if it does not exist.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def at(*args, **kwargs):  # pylint: disable=C0103\n    '''\n    \n    '''\n\n    # check args\n    if len(args) < 2:\n        return {'jobs': []}\n\n    # build job\n    if 'tag' in kwargs:\n        stdin = '### SALT: {0}\\n{1}'.format(kwargs['tag'], ' '.join(args[1:]))\n    else:\n        stdin = ' '.join(args[1:])\n\n    cmd_kwargs = {'stdin': stdin, 'python_shell': False}\n    if 'runas' in kwargs:\n        cmd_kwargs['runas'] = kwargs['runas']\n    res = __salt__['cmd.run_all']('at \"{timespec}\"'.format(\n        timespec=args[0]\n    ), **cmd_kwargs)\n\n    # verify job creation\n    if res['retcode'] > 0:\n        if 'bad time specification' in res['stderr']:\n            return {'jobs': [], 'error': 'invalid timespec'}\n        return {'jobs': [], 'error': res['stderr']}\n    else:\n        jobid = res['stderr'].splitlines()[1]\n        jobid = six.text_type(jobid.split()[1])\n        return atq(jobid)", "output": "Add a job to the queue.\n\n    The 'timespec' follows the format documented in the\n    at(1) manpage.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' at.at <timespec> <cmd> [tag=<tag>] [runas=<user>]\n        salt '*' at.at 12:05am '/sbin/reboot' tag=reboot\n        salt '*' at.at '3:05am +3 days' 'bin/myscript' tag=nightly runas=jim", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def subscription_path(cls, project, incident, subscription):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/incidents/{incident}/subscriptions/{subscription}\",\n            project=project,\n            incident=incident,\n            subscription=subscription,\n        )", "output": "Return a fully-qualified subscription string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _to_bytes(value, encoding=\"ascii\"):\n    \"\"\"\n    \"\"\"\n    result = value.encode(encoding) if isinstance(value, six.text_type) else value\n    if isinstance(result, six.binary_type):\n        return result\n    else:\n        raise TypeError(\"%r could not be converted to bytes\" % (value,))", "output": "Converts a string value to bytes, if necessary.\n\n    Unfortunately, ``six.b`` is insufficient for this task since in\n    Python2 it does not modify ``unicode`` objects.\n\n    :type value: str / bytes or unicode\n    :param value: The string/bytes value to be converted.\n\n    :type encoding: str\n    :param encoding: The encoding to use to convert unicode to bytes. Defaults\n                     to \"ascii\", which will not allow any characters from\n                     ordinals larger than 127. Other useful values are\n                     \"latin-1\", which which will only allows byte ordinals\n                     (up to 255) and \"utf-8\", which will encode any unicode\n                     that needs to be.\n\n    :rtype: str / bytes\n    :returns: The original value converted to bytes (if unicode) or as passed\n              in if it started out as bytes.\n    :raises TypeError: if the value could not be converted to bytes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def initialize_hooks(self):\n        \"\"\"\n        \n        \"\"\"\n        hooks = self._callbacks.get_hooks()\n        self.hooked_sess = tfv1.train.MonitoredSession(\n            session_creator=ReuseSessionCreator(self.sess), hooks=hooks)", "output": "Create SessionRunHooks for all callbacks, and hook it onto `self.sess` to create `self.hooked_sess`.\n\n        A new trainer may override this method to create multiple groups of hooks,\n        which can be useful when the training is not done by a single `train_op`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def closed_issues(issues, after):\n    \"\"\"\"\"\"\n    logging.info('finding closed issues after {}...'.format(after))\n    seen = set()\n    for issue in issues:\n        if closed_issue(issue, after) and issue['title'] not in seen:\n            seen.add(issue['title'])\n            yield issue", "output": "Yields closed issues (closed after a given datetime) given a list of issues.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_combined_condition_to_template(self, template_dict, condition_name, conditions_to_combine):\n        \"\"\"\n        \n        \"\"\"\n        # defensive precondition check\n        if not conditions_to_combine or len(conditions_to_combine) < 2:\n            raise ValueError('conditions_to_combine must have at least 2 conditions')\n\n        template_conditions = template_dict.setdefault('Conditions', {})\n        new_template_conditions = make_combined_condition(sorted(list(conditions_to_combine)), condition_name)\n        for name, definition in new_template_conditions.items():\n            template_conditions[name] = definition", "output": "Add top-level template condition that combines the given list of conditions.\n\n        :param dict template_dict: SAM template dictionary\n        :param string condition_name: Name of top-level template condition\n        :param list conditions_to_combine: List of conditions that should be combined (via OR operator) to form\n                                           top-level condition.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imdecode(image_path):\n    \"\"\"\"\"\"\n    import os\n    assert os.path.exists(image_path), image_path + ' not found'\n    im = cv2.imread(image_path)\n    return im", "output": "Return BGR image read by opencv", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def service_response(body, headers, status_code):\n        \"\"\"\n        \n        \"\"\"\n        response = Response(body)\n        response.headers = headers\n        response.status_code = status_code\n        return response", "output": "Constructs a Flask Response from the body, headers, and status_code.\n\n        :param str body: Response body as a string\n        :param dict headers: headers for the response\n        :param int status_code: status_code for response\n        :return: Flask Response", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_apppool(name):\n    '''\n    \n    '''\n    current_apppools = list_apppools()\n    apppool_path = r'IIS:\\AppPools\\{0}'.format(name)\n\n    if name in current_apppools:\n        log.debug(\"Application pool '%s' already present.\", name)\n        return True\n\n    ps_cmd = ['New-Item', '-Path', r\"'{0}'\".format(apppool_path)]\n\n    cmd_ret = _srvmgr(ps_cmd)\n\n    if cmd_ret['retcode'] != 0:\n        msg = 'Unable to create application pool: {0}\\nError: {1}' \\\n              ''.format(name, cmd_ret['stderr'])\n        raise CommandExecutionError(msg)\n\n    log.debug('Application pool created successfully: %s', name)\n    return True", "output": "Create an IIS application pool.\n\n    .. note::\n\n        This function only validates against the application pool name, and will\n        return True even if the application pool already exists with a different\n        configuration. It will not modify the configuration of an existing\n        application pool.\n\n    Args:\n        name (str): The name of the IIS application pool.\n\n    Returns:\n        bool: True if successful, otherwise False\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' win_iis.create_apppool name='MyTestPool'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def RetrievePluginAsset(self, plugin_name, asset_name):\n    \"\"\"\n    \"\"\"\n    return plugin_asset_util.RetrieveAsset(self.path, plugin_name, asset_name)", "output": "Return the contents of a given plugin asset.\n\n    Args:\n      plugin_name: The string name of a plugin.\n      asset_name: The string name of an asset.\n\n    Returns:\n      The string contents of the plugin asset.\n\n    Raises:\n      KeyError: If the asset is not available.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_ipv4_range(start_addr=None, end_addr=None, **api_opts):\n    '''\n    \n    '''\n    r = get_ipv4_range(start_addr, end_addr, **api_opts)\n    if r:\n        return delete_object(r['_ref'], **api_opts)\n    else:\n        return True", "output": "Delete ip range.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call infoblox.delete_ipv4_range start_addr=123.123.122.12", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _inherit_docstrings(parent, excluded=[]):\n    \"\"\"\n    \"\"\"\n\n    def decorator(cls):\n        if parent not in excluded:\n            cls.__doc__ = parent.__doc__\n        for attr, obj in cls.__dict__.items():\n            parent_obj = getattr(parent, attr, None)\n            if parent_obj in excluded or (\n                not callable(parent_obj) and not isinstance(parent_obj, property)\n            ):\n                continue\n            if callable(obj):\n                obj.__doc__ = parent_obj.__doc__\n            elif isinstance(obj, property) and obj.fget is not None:\n                p = property(obj.fget, obj.fset, obj.fdel, parent_obj.__doc__)\n                setattr(cls, attr, p)\n        return cls\n\n    return decorator", "output": "Creates a decorator which overwrites a decorated class' __doc__\n    attribute with parent's __doc__ attribute. Also overwrites __doc__ of\n    methods and properties defined in the class with the __doc__ of matching\n    methods and properties in parent.\n\n    Args:\n        parent (object): Class from which the decorated class inherits __doc__.\n        excluded (list): List of parent objects from which the class does not\n            inherit docstrings.\n\n    Returns:\n        function: decorator which replaces the decorated class' documentation\n            parent's documentation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_function_for_aws_event(self, record):\n        \"\"\"\n        \n        \"\"\"\n        if 's3' in record:\n            if ':' in record['s3']['configurationId']:\n                return record['s3']['configurationId'].split(':')[-1]\n\n        arn = None\n        if 'Sns' in record:\n            try:\n                message = json.loads(record['Sns']['Message'])\n                if message.get('command'):\n                    return message['command']\n            except ValueError:\n                pass\n            arn = record['Sns'].get('TopicArn')\n        elif 'dynamodb' in record or 'kinesis' in record:\n            arn = record.get('eventSourceARN')\n        elif 'eventSource' in record and record.get('eventSource') == 'aws:sqs':\n            arn = record.get('eventSourceARN')\n        elif 's3' in record:\n            arn = record['s3']['bucket']['arn']\n\n        if arn:\n            return self.settings.AWS_EVENT_MAPPING.get(arn)\n\n        return None", "output": "Get the associated function to execute for a triggered AWS event\n\n        Support S3, SNS, DynamoDB, kinesis and SQS events", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sync_state_context(self, state, context):\n        \"\"\"\"\"\"\n        if isinstance(state, NDArray):\n            return state.as_in_context(context)\n        elif isinstance(state, (tuple, list)):\n            synced_state = (self.sync_state_context(i, context) for i in state)\n            if isinstance(state, tuple):\n                return tuple(synced_state)\n            else:\n                return list(synced_state)\n        else:\n            return state", "output": "sync state context.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def count_characters(root, out):\n    \"\"\"\"\"\"\n    if os.path.isfile(root):\n        with open(root, 'rb') as in_f:\n            for line in in_f:\n                for char in line:\n                    if char not in out:\n                        out[char] = 0\n                    out[char] = out[char] + 1\n    elif os.path.isdir(root):\n        for filename in os.listdir(root):\n            count_characters(os.path.join(root, filename), out)", "output": "Count the occurrances of the different characters in the files", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _recall_prec(self, record, count):\n        \"\"\"  \"\"\"\n        record = np.delete(record, np.where(record[:, 1].astype(int) == 0)[0], axis=0)\n        sorted_records = record[record[:,0].argsort()[::-1]]\n        tp = np.cumsum(sorted_records[:, 1].astype(int) == 1)\n        fp = np.cumsum(sorted_records[:, 1].astype(int) == 2)\n        if count <= 0:\n            recall = tp * 0.0\n        else:\n            recall = tp / float(count)\n        prec = tp.astype(float) / (tp + fp)\n        return recall, prec", "output": "get recall and precision from internal records", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sleep(current_sleep, max_sleep=_MAX_SLEEP, multiplier=_MULTIPLIER):\n    \"\"\"\n    \"\"\"\n    actual_sleep = random.uniform(0.0, current_sleep)\n    time.sleep(actual_sleep)\n    return min(multiplier * current_sleep, max_sleep)", "output": "Sleep and produce a new sleep time.\n\n    .. _Exponential Backoff And Jitter: https://www.awsarchitectureblog.com/\\\n                                        2015/03/backoff.html\n\n    Select a duration between zero and ``current_sleep``. It might seem\n    counterintuitive to have so much jitter, but\n    `Exponential Backoff And Jitter`_ argues that \"full jitter\" is\n    the best strategy.\n\n    Args:\n        current_sleep (float): The current \"max\" for sleep interval.\n        max_sleep (Optional[float]): Eventual \"max\" sleep time\n        multiplier (Optional[float]): Multiplier for exponential backoff.\n\n    Returns:\n        float: Newly doubled ``current_sleep`` or ``max_sleep`` (whichever\n        is smaller)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _prepare_connection(**nxos_api_kwargs):\n    '''\n    \n    '''\n    nxos_api_kwargs = clean_kwargs(**nxos_api_kwargs)\n    init_kwargs = {}\n    # Clean up any arguments that are not required\n    for karg, warg in six.iteritems(nxos_api_kwargs):\n        if karg in RPC_INIT_KWARGS:\n            init_kwargs[karg] = warg\n    if 'host' not in init_kwargs:\n        init_kwargs['host'] = 'localhost'\n    if 'transport' not in init_kwargs:\n        init_kwargs['transport'] = 'https'\n    if 'port' not in init_kwargs:\n        init_kwargs['port'] = 80 if init_kwargs['transport'] == 'http' else 443\n    verify = init_kwargs.get('verify', True)\n    if isinstance(verify, bool):\n        init_kwargs['verify_ssl'] = verify\n    else:\n        init_kwargs['ca_bundle'] = verify\n    if 'rpc_version' not in init_kwargs:\n        init_kwargs['rpc_version'] = '2.0'\n    if 'timeout' not in init_kwargs:\n        init_kwargs['timeout'] = 60\n    return init_kwargs", "output": "Prepare the connection with the remote network device, and clean up the key\n    value pairs, removing the args used for the connection init.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def constrained_to(self, initial_sequence: torch.Tensor, keep_beam_details: bool = True) -> 'BeamSearch':\n        \"\"\"\n        \n        \"\"\"\n        return BeamSearch(self._beam_size, self._per_node_beam_size, initial_sequence, keep_beam_details)", "output": "Return a new BeamSearch instance that's like this one but with the specified constraint.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_symbol(num_classes=20, nms_thresh=0.5, force_suppress=False,\n               nms_topk=400, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    net = get_symbol_train(num_classes)\n    cls_preds = net.get_internals()[\"multibox_cls_pred_output\"]\n    loc_preds = net.get_internals()[\"multibox_loc_pred_output\"]\n    anchor_boxes = net.get_internals()[\"multibox_anchors_output\"]\n\n    cls_prob = mx.symbol.softmax(data=cls_preds, axis=1, name='cls_prob')\n    out = mx.symbol.contrib.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \\\n        name=\"detection\", nms_threshold=nms_thresh, force_suppress=force_suppress,\n        variances=(0.1, 0.1, 0.2, 0.2), nms_topk=nms_topk)\n    return out", "output": "Single-shot multi-box detection with VGG 16 layers ConvNet\n    This is a modified version, with fc6/fc7 layers replaced by conv layers\n    And the network is slightly smaller than original VGG 16 network\n    This is the detection network\n\n    Parameters:\n    ----------\n    num_classes: int\n        number of object classes not including background\n    nms_thresh : float\n        threshold of overlap for non-maximum suppression\n    force_suppress : boolean\n        whether suppress different class objects\n    nms_topk : int\n        apply NMS to top K detections\n\n    Returns:\n    ----------\n    mx.Symbol", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_parent_parser():\n    \"\"\"\n    \"\"\"\n    parent_parser = argparse.ArgumentParser(add_help=False)\n\n    log_level_group = parent_parser.add_mutually_exclusive_group()\n    log_level_group.add_argument(\n        \"-q\", \"--quiet\", action=\"store_true\", default=False, help=\"Be quiet.\"\n    )\n    log_level_group.add_argument(\n        \"-v\",\n        \"--verbose\",\n        action=\"store_true\",\n        default=False,\n        help=\"Be verbose.\",\n    )\n\n    return parent_parser", "output": "Create instances of a parser containing common arguments shared among\n    all the commands.\n\n    When overwritting `-q` or `-v`, you need to instantiate a new object\n    in order to prevent some weird behavior.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def trainable_params(m:nn.Module)->ParamList:\n    \"\"\n    res = filter(lambda p: p.requires_grad, m.parameters())\n    return res", "output": "Return list of trainable params in `m`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy(self):\n        \"\"\"\n        \"\"\"\n        other = DirectedGraph()\n        other._vertices = set(self._vertices)\n        other._forwards = {k: set(v) for k, v in self._forwards.items()}\n        other._backwards = {k: set(v) for k, v in self._backwards.items()}\n        return other", "output": "Return a shallow copy of this graph.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _GetDeps(self, dependencies):\n    \"\"\"\n    \"\"\"\n\n    for dependency in dependencies:\n      dep_desc = self.FindFileByName(dependency)\n      yield dep_desc\n      for parent_dep in dep_desc.dependencies:\n        yield parent_dep", "output": "Recursively finds dependencies for file protos.\n\n    Args:\n      dependencies: The names of the files being depended on.\n\n    Yields:\n      Each direct and indirect dependency.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_set(self):\n        \"\"\"\"\"\"\n        lineno = next(self.stream).lineno\n        target = self.parse_assign_target(with_namespace=True)\n        if self.stream.skip_if('assign'):\n            expr = self.parse_tuple()\n            return nodes.Assign(target, expr, lineno=lineno)\n        filter_node = self.parse_filter(None)\n        body = self.parse_statements(('name:endset',),\n                                     drop_needle=True)\n        return nodes.AssignBlock(target, filter_node, body, lineno=lineno)", "output": "Parse an assign statement.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def favstar(self, class_name, obj_id, action):\n        \"\"\"\"\"\"\n        session = db.session()\n        FavStar = models.FavStar  # noqa\n        count = 0\n        favs = session.query(FavStar).filter_by(\n            class_name=class_name, obj_id=obj_id,\n            user_id=g.user.get_id()).all()\n        if action == 'select':\n            if not favs:\n                session.add(\n                    FavStar(\n                        class_name=class_name,\n                        obj_id=obj_id,\n                        user_id=g.user.get_id(),\n                        dttm=datetime.now(),\n                    ),\n                )\n            count = 1\n        elif action == 'unselect':\n            for fav in favs:\n                session.delete(fav)\n        else:\n            count = len(favs)\n        session.commit()\n        return json_success(json.dumps({'count': count}))", "output": "Toggle favorite stars on Slices and Dashboard", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def begin_training(self, get_gold_tuples=None, sgd=None, component_cfg=None, **cfg):\n        \"\"\"\n        \"\"\"\n        if get_gold_tuples is None:\n            get_gold_tuples = lambda: []\n        # Populate vocab\n        else:\n            for _, annots_brackets in get_gold_tuples():\n                for annots, _ in annots_brackets:\n                    for word in annots[1]:\n                        _ = self.vocab[word]  # noqa: F841\n        if cfg.get(\"device\", -1) >= 0:\n            util.use_gpu(cfg[\"device\"])\n            if self.vocab.vectors.data.shape[1] >= 1:\n                self.vocab.vectors.data = Model.ops.asarray(self.vocab.vectors.data)\n        link_vectors_to_models(self.vocab)\n        if self.vocab.vectors.data.shape[1]:\n            cfg[\"pretrained_vectors\"] = self.vocab.vectors.name\n        if sgd is None:\n            sgd = create_default_optimizer(Model.ops)\n        self._optimizer = sgd\n        if component_cfg is None:\n            component_cfg = {}\n        for name, proc in self.pipeline:\n            if hasattr(proc, \"begin_training\"):\n                kwargs = component_cfg.get(name, {})\n                kwargs.update(cfg)\n                proc.begin_training(\n                    get_gold_tuples,\n                    pipeline=self.pipeline,\n                    sgd=self._optimizer,\n                    **kwargs\n                )\n        return self._optimizer", "output": "Allocate models, pre-process training data and acquire a trainer and\n        optimizer. Used as a contextmanager.\n\n        get_gold_tuples (function): Function returning gold data\n        component_cfg (dict): Config parameters for specific components.\n        **cfg: Config parameters.\n        RETURNS: An optimizer.\n\n        DOCS: https://spacy.io/api/language#begin_training", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_groups(self, load):\n        '''\n        \n        '''\n        if 'eauth' not in load:\n            return False\n        fstr = '{0}.groups'.format(load['eauth'])\n        if fstr not in self.auth:\n            return False\n        fcall = salt.utils.args.format_call(\n            self.auth[fstr],\n            load,\n            expected_extra_kws=AUTH_INTERNAL_KEYWORDS)\n        try:\n            return self.auth[fstr](*fcall['args'], **fcall['kwargs'])\n        except IndexError:\n            return False\n        except Exception:\n            return None", "output": "Read in a load and return the groups a user is a member of\n        by asking the appropriate provider", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def double_click(self, on_element=None):\n        \"\"\"\n        \n        \"\"\"\n        if on_element:\n            self.move_to_element(on_element)\n        if self._driver.w3c:\n            self.w3c_actions.pointer_action.double_click()\n            for _ in range(4):\n                self.w3c_actions.key_action.pause()\n        else:\n            self._actions.append(lambda: self._driver.execute(\n                                 Command.DOUBLE_CLICK, {}))\n        return self", "output": "Double-clicks an element.\n\n        :Args:\n         - on_element: The element to double-click.\n           If None, clicks on current mouse position.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parser(result):\n    '''\n    \n    '''\n\n    # regexes to match\n    _total_time = re.compile(r'total time:\\s*(\\d*.\\d*s)')\n    _total_execution = re.compile(r'event execution:\\s*(\\d*.\\d*s?)')\n    _min_response_time = re.compile(r'min:\\s*(\\d*.\\d*ms)')\n    _max_response_time = re.compile(r'max:\\s*(\\d*.\\d*ms)')\n    _avg_response_time = re.compile(r'avg:\\s*(\\d*.\\d*ms)')\n    _per_response_time = re.compile(r'95 percentile:\\s*(\\d*.\\d*ms)')\n\n    # extracting data\n    total_time = re.search(_total_time, result).group(1)\n    total_execution = re.search(_total_execution, result).group(1)\n    min_response_time = re.search(_min_response_time, result).group(1)\n    max_response_time = re.search(_max_response_time, result).group(1)\n    avg_response_time = re.search(_avg_response_time, result).group(1)\n    per_response_time = re.search(_per_response_time, result)\n    if per_response_time is not None:\n        per_response_time = per_response_time.group(1)\n\n    # returning the data as dictionary\n    return {\n        'total time': total_time,\n        'total execution time': total_execution,\n        'minimum response time': min_response_time,\n        'maximum response time': max_response_time,\n        'average response time': avg_response_time,\n        '95 percentile': per_response_time\n    }", "output": "parses the output into a dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def switch_to_aux_top_layer(self):\n        \"\"\"\"\"\"\n        if self.aux_top_layer is None:\n            raise RuntimeError(\"Empty auxiliary top layer in the network.\")\n        saved_top_layer = self.top_layer\n        saved_top_size = self.top_size\n        self.top_layer = self.aux_top_layer\n        self.top_size = self.aux_top_size\n        yield\n        self.aux_top_layer = self.top_layer\n        self.aux_top_size = self.top_size\n        self.top_layer = saved_top_layer\n        self.top_size = saved_top_size", "output": "Context that construct cnn in the auxiliary arm.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_raylet(self, use_valgrind=False, use_profiler=False):\n        \"\"\"\n        \"\"\"\n        stdout_file, stderr_file = self.new_log_files(\"raylet\")\n        process_info = ray.services.start_raylet(\n            self._redis_address,\n            self._node_ip_address,\n            self._raylet_socket_name,\n            self._plasma_store_socket_name,\n            self._ray_params.worker_path,\n            self._temp_dir,\n            self._ray_params.num_cpus,\n            self._ray_params.num_gpus,\n            self._ray_params.resources,\n            self._ray_params.object_manager_port,\n            self._ray_params.node_manager_port,\n            self._ray_params.redis_password,\n            use_valgrind=use_valgrind,\n            use_profiler=use_profiler,\n            stdout_file=stdout_file,\n            stderr_file=stderr_file,\n            config=self._config,\n            include_java=self._ray_params.include_java,\n            java_worker_options=self._ray_params.java_worker_options,\n            load_code_from_local=self._ray_params.load_code_from_local,\n        )\n        assert ray_constants.PROCESS_TYPE_RAYLET not in self.all_processes\n        self.all_processes[ray_constants.PROCESS_TYPE_RAYLET] = [process_info]", "output": "Start the raylet.\n\n        Args:\n            use_valgrind (bool): True if we should start the process in\n                valgrind.\n            use_profiler (bool): True if we should start the process in the\n                valgrind profiler.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def namespace_absent(name, **kwargs):\n    '''\n    \n    '''\n\n    ret = {'name': name,\n           'changes': {},\n           'result': False,\n           'comment': ''}\n\n    namespace = __salt__['kubernetes.show_namespace'](name, **kwargs)\n\n    if namespace is None:\n        ret['result'] = True if not __opts__['test'] else None\n        ret['comment'] = 'The namespace does not exist'\n        return ret\n\n    if __opts__['test']:\n        ret['comment'] = 'The namespace is going to be deleted'\n        ret['result'] = None\n        return ret\n\n    res = __salt__['kubernetes.delete_namespace'](name, **kwargs)\n    if (\n            res['code'] == 200 or\n            (\n                isinstance(res['status'], six.string_types) and\n                'Terminating' in res['status']\n            ) or\n            (\n                isinstance(res['status'], dict) and\n                res['status']['phase'] == 'Terminating'\n            )):\n        ret['result'] = True\n        ret['changes'] = {\n            'kubernetes.namespace': {\n                'new': 'absent', 'old': 'present'}}\n        if res['message']:\n            ret['comment'] = res['message']\n        else:\n            ret['comment'] = 'Terminating'\n    else:\n        ret['comment'] = 'Something went wrong, response: {0}'.format(res)\n\n    return ret", "output": "Ensures that the named namespace is absent.\n\n    name\n        The name of the namespace", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_commission(self, commission):\n        \"\"\"\n        \"\"\"\n        asset = commission['asset']\n        cost = commission['cost']\n\n        self.position_tracker.handle_commission(asset, cost)\n        self._cash_flow(-cost)", "output": "Process the commission.\n\n        Parameters\n        ----------\n        commission : zp.Event\n            The commission being paid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render(\n    docs, style=\"dep\", page=False, minify=False, jupyter=None, options={}, manual=False\n):\n    \"\"\"\n    \"\"\"\n    factories = {\n        \"dep\": (DependencyRenderer, parse_deps),\n        \"ent\": (EntityRenderer, parse_ents),\n    }\n    if style not in factories:\n        raise ValueError(Errors.E087.format(style=style))\n    if isinstance(docs, (Doc, Span, dict)):\n        docs = [docs]\n    docs = [obj if not isinstance(obj, Span) else obj.as_doc() for obj in docs]\n    if not all(isinstance(obj, (Doc, Span, dict)) for obj in docs):\n        raise ValueError(Errors.E096)\n    renderer, converter = factories[style]\n    renderer = renderer(options=options)\n    parsed = [converter(doc, options) for doc in docs] if not manual else docs\n    _html[\"parsed\"] = renderer.render(parsed, page=page, minify=minify).strip()\n    html = _html[\"parsed\"]\n    if RENDER_WRAPPER is not None:\n        html = RENDER_WRAPPER(html)\n    if jupyter or (jupyter is None and is_in_jupyter()):\n        # return HTML rendered by IPython display()\n        from IPython.core.display import display, HTML\n\n        return display(HTML(html))\n    return html", "output": "Render displaCy visualisation.\n\n    docs (list or Doc): Document(s) to visualise.\n    style (unicode): Visualisation style, 'dep' or 'ent'.\n    page (bool): Render markup as full HTML page.\n    minify (bool): Minify HTML markup.\n    jupyter (bool): Override Jupyter auto-detection.\n    options (dict): Visualiser-specific options, e.g. colors.\n    manual (bool): Don't parse `Doc` and instead expect a dict/list of dicts.\n    RETURNS (unicode): Rendered HTML markup.\n\n    DOCS: https://spacy.io/api/top-level#displacy.render\n    USAGE: https://spacy.io/usage/visualizers", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _raise_error_evaluation_metric_is_valid(metric, allowed_metrics):\n    \"\"\"\n    \n    \"\"\"\n\n    err_msg = \"Evaluation metric '%s' not recognized. The supported evaluation\"\n    err_msg += \" metrics are (%s).\"\n\n    if metric not in allowed_metrics:\n      raise ToolkitError(err_msg % (metric,\n                          ', '.join(map(lambda x: \"'%s'\" % x, allowed_metrics))))", "output": "Check if the input is an SFrame. Provide a proper error\n    message otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_mask_from_sequence_lengths(sequence_lengths: torch.Tensor, max_length: int) -> torch.Tensor:\n    \"\"\"\n    \n    \"\"\"\n    # (batch_size, max_length)\n    ones = sequence_lengths.new_ones(sequence_lengths.size(0), max_length)\n    range_tensor = ones.cumsum(dim=1)\n    return (sequence_lengths.unsqueeze(1) >= range_tensor).long()", "output": "Given a variable of shape ``(batch_size,)`` that represents the sequence lengths of each batch\n    element, this function returns a ``(batch_size, max_length)`` mask variable.  For example, if\n    our input was ``[2, 2, 3]``, with a ``max_length`` of 4, we'd return\n    ``[[1, 1, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0]]``.\n\n    We require ``max_length`` here instead of just computing it from the input ``sequence_lengths``\n    because it lets us avoid finding the max, then copying that value from the GPU to the CPU so\n    that we can use it to construct a new tensor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def file_page_generator(my_file, max_page_size=2**28):\n  \"\"\"\n  \"\"\"\n  page_start = \"  <page>\\n\"\n  page_end = \"  </page>\\n\"\n  chunk_size = max_page_size\n  page_start = \"  <page>\\n\"\n  page_end = \"  </page>\\n\"\n  leftovers = \"\"\n  while True:\n    chunk = my_file.read(chunk_size)\n    if not chunk:\n      break\n    chunk = leftovers + chunk\n    current_pos = 0\n    while True:\n      start_pos = chunk.find(page_start, current_pos)\n      if start_pos == -1:\n        break\n      end_pos = chunk.find(page_end, start_pos)\n      if end_pos == -1:\n        if len(chunk) - start_pos > max_page_size:\n          leftovers = \"\"\n        else:\n          leftovers = chunk[start_pos:]\n        break\n      raw_page = chunk[start_pos + len(page_start):end_pos]\n      if len(raw_page) < max_page_size:\n        ret = parse_page(raw_page)\n        if ret:\n          yield ret\n      current_pos = end_pos + len(page_end)", "output": "Read wikipedia pages from a history dump.\n\n  Since some pages can be terabytes in size (with all the revisions),\n  we limit page size to max_page_size bytes.\n\n  Args:\n    my_file: an open file object.\n    max_page_size: an integer\n\n  Yields:\n    strings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ctxtUseOptions(self, options):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlCtxtUseOptions(self._o, options)\n        return ret", "output": "Applies the options to the parser context", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decrement(self, delta=1):\n        \"\"\"\n        \"\"\"\n        check_call(_LIB.MXProfileAdjustCounter(self.handle, -int(delta)))", "output": "Decrement counter value.\n\n        Parameters\n        ----------\n        value_change : int\n            Amount by which to subtract from the counter", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _examples_from_path_handler(self, request):\n    \"\"\"\n    \"\"\"\n    examples_count = int(request.args.get('max_examples'))\n    examples_path = request.args.get('examples_path')\n    sampling_odds = float(request.args.get('sampling_odds'))\n    self.example_class = (tf.train.SequenceExample\n        if request.args.get('sequence_examples') == 'true'\n        else tf.train.Example)\n    try:\n      platform_utils.throw_if_file_access_not_allowed(examples_path,\n                                                      self._logdir,\n                                                      self._has_auth_group)\n      example_strings = platform_utils.example_protos_from_path(\n          examples_path, examples_count, parse_examples=False,\n          sampling_odds=sampling_odds, example_class=self.example_class)\n      self.examples = [\n          self.example_class.FromString(ex) for ex in example_strings]\n      self.generate_sprite(example_strings)\n      json_examples = [\n          json_format.MessageToJson(example) for example in self.examples\n      ]\n      self.updated_example_indices = set(range(len(json_examples)))\n      return http_util.Respond(\n          request,\n          {'examples': json_examples,\n           'sprite': True if self.sprite else False}, 'application/json')\n    except common_utils.InvalidUserInputError as e:\n      return http_util.Respond(request, {'error': e.message},\n                               'application/json', code=400)", "output": "Returns JSON of the specified examples.\n\n    Args:\n      request: A request that should contain 'examples_path' and 'max_examples'.\n\n    Returns:\n      JSON of up to max_examlpes of the examples in the path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def quotas(self):\n        \"\"\"\n        \"\"\"\n        path = \"/projects/%s\" % (self.project,)\n        resp = self._connection.api_request(method=\"GET\", path=path)\n\n        return {\n            key: int(value) for key, value in resp[\"quota\"].items() if key != \"kind\"\n        }", "output": "Return DNS quotas for the project associated with this client.\n\n        See\n        https://cloud.google.com/dns/api/v1/projects/get\n\n        :rtype: mapping\n        :returns: keys for the mapping correspond to those of the ``quota``\n                  sub-mapping of the project resource.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def notification(\n        self,\n        topic_name,\n        topic_project=None,\n        custom_attributes=None,\n        event_types=None,\n        blob_name_prefix=None,\n        payload_format=NONE_PAYLOAD_FORMAT,\n    ):\n        \"\"\"\n        \"\"\"\n        return BucketNotification(\n            self,\n            topic_name,\n            topic_project=topic_project,\n            custom_attributes=custom_attributes,\n            event_types=event_types,\n            blob_name_prefix=blob_name_prefix,\n            payload_format=payload_format,\n        )", "output": "Factory:  create a notification resource for the bucket.\n\n        See: :class:`.BucketNotification` for parameters.\n\n        :rtype: :class:`.BucketNotification`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve_code_path(cwd, codeuri):\n    \"\"\"\n    \n\n    \"\"\"\n    LOG.debug(\"Resolving code path. Cwd=%s, CodeUri=%s\", cwd, codeuri)\n\n    # First, let us figure out the current working directory.\n    # If current working directory is not provided, then default to the directory where the CLI is running from\n    if not cwd or cwd == PRESENT_DIR:\n        cwd = os.getcwd()\n\n    # Make sure cwd is an absolute path\n    cwd = os.path.abspath(cwd)\n\n    # Next, let us get absolute path of function code.\n    # Codepath is always relative to current working directory\n    # If the path is relative, then construct the absolute version\n    if not os.path.isabs(codeuri):\n        codeuri = os.path.normpath(os.path.join(cwd, codeuri))\n\n    return codeuri", "output": "Returns path to the function code resolved based on current working directory.\n\n    Parameters\n    ----------\n    cwd str\n        Current working directory\n    codeuri\n        CodeURI of the function. This should contain the path to the function code\n\n    Returns\n    -------\n    str\n        Absolute path to the function code", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetch_result(self, trial):\n        \"\"\"\"\"\"\n        trial_future = self._find_item(self._running, trial)\n        if not trial_future:\n            raise ValueError(\"Trial was not running.\")\n        self._running.pop(trial_future[0])\n        with warn_if_slow(\"fetch_result\"):\n            result = ray.get(trial_future[0])\n\n        # For local mode\n        if isinstance(result, _LocalWrapper):\n            result = result.unwrap()\n        return result", "output": "Fetches one result of the running trials.\n\n        Returns:\n            Result of the most recent trial training run.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def has_safe_repr(value):\n    \"\"\"\"\"\"\n    if value is None or value is NotImplemented or value is Ellipsis:\n        return True\n    if type(value) in (bool, int, float, complex, range_type, Markup) + string_types:\n        return True\n    if type(value) in (tuple, list, set, frozenset):\n        for item in value:\n            if not has_safe_repr(item):\n                return False\n        return True\n    elif type(value) is dict:\n        for key, value in iteritems(value):\n            if not has_safe_repr(key):\n                return False\n            if not has_safe_repr(value):\n                return False\n        return True\n    return False", "output": "Does the node have a safe representation?", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_zoom_factor(self):\r\n        \"\"\"\"\"\"\r\n        if hasattr(self, 'setZoomFactor'):\r\n            # Assuming Qt >=v4.5\r\n            self.setZoomFactor(self.zoom_factor)\r\n        else:\r\n            # Qt v4.4\r\n            self.setTextSizeMultiplier(self.zoom_factor)", "output": "Apply zoom factor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_custom(net, node, module, builder):\n    \"\"\"\"\"\"\n    input_name, output_name = _get_input_output_name(net, node)\n    name = node['name']\n    param = _get_attr(node)\n    if param['op_type'] == 'special-darknet-maxpool':\n        _add_pooling.add_pooling_with_padding_types(\n            builder=builder,\n            name=name,\n            height=2,\n            width=2,\n            stride_height=1,\n            stride_width=1,\n            layer_type='MAX',\n            padding_type='SAME',\n            is_global=False,\n            same_padding_asymmetry_mode='BOTTOM_RIGHT_HEAVY',\n            input_name=input_name,\n            output_name=output_name\n        )\n    else:\n        raise TypeError(\"MXNet layer of type Custom is not supported.\")", "output": "Convert highly specific ops", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dumps(data):  # type: (_TOMLDocument) -> str\n    \"\"\"\n    \n    \"\"\"\n    if not isinstance(data, _TOMLDocument) and isinstance(data, dict):\n        data = item(data)\n\n    return data.as_string()", "output": "Dumps a TOMLDocument into a string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _prepare_settings(pspath, settings):\n    '''\n    \n    '''\n    prepared_settings = []\n    for setting in settings:\n        match = re.search(r'Collection\\[(\\{.*\\})\\]', setting['name'])\n        if match:\n            name = setting['name'][:match.start(1)-1]\n            match_dict = yaml.load(match.group(1))\n            index = _collection_match_to_index(pspath, setting['filter'], name, match_dict)\n            if index != -1:\n                setting['name'] = setting['name'].replace(match.group(1), str(index))\n                prepared_settings.append(setting)\n        else:\n            prepared_settings.append(setting)\n    return prepared_settings", "output": "Prepare settings before execution with get or set functions.\n    Removes settings with a match parameter when index is not found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install_monitor(self, mon):\n        \"\"\"\"\"\"\n        assert self.binded\n        for module in self._modules:\n            module.install_monitor(mon)", "output": "Installs monitor on all executors.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def disable(iface):\n    '''\n    \n    '''\n    if is_disabled(iface):\n        return True\n    cmd = ['netsh', 'interface', 'set', 'interface',\n           'name={0}'.format(iface),\n           'admin=DISABLED']\n    __salt__['cmd.run'](cmd, python_shell=False)\n    return is_disabled(iface)", "output": "Disable an interface\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt -G 'os_family:Windows' ip.disable 'Local Area Connection #2'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def split_source(source_code):\r\n    '''\r\n    '''\r\n    eol_chars = get_eol_chars(source_code)\r\n    if eol_chars:\r\n        return source_code.split(eol_chars)\r\n    else:\r\n        return [source_code]", "output": "Split source code into lines", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_color_scheme_stack(self, scheme_name):\n        \"\"\"\"\"\"\n        self.set_scheme(scheme_name)\n        widget = self.stack.currentWidget()\n        self.stack.removeWidget(widget)\n        index = self.order.index(scheme_name)\n        self.order.pop(index)", "output": "Remove stack widget by 'scheme_name'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def default_batchify_fn(data):\n    \"\"\"\"\"\"\n    if isinstance(data[0], nd.NDArray):\n        return nd.stack(*data)\n    elif isinstance(data[0], tuple):\n        data = zip(*data)\n        return [default_batchify_fn(i) for i in data]\n    else:\n        data = np.asarray(data)\n        return nd.array(data, dtype=data.dtype)", "output": "Collate data into batch.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_plugin_actions(self):\r\n        \"\"\"\"\"\"\r\n        self.new_project_action = create_action(self,\r\n                                    _(\"New Project...\"),\r\n                                    triggered=self.create_new_project)\r\n        self.open_project_action = create_action(self,\r\n                                    _(\"Open Project...\"),\r\n                                    triggered=lambda v: self.open_project())\r\n        self.close_project_action = create_action(self,\r\n                                    _(\"Close Project\"),\r\n                                    triggered=self.close_project)\r\n        self.delete_project_action = create_action(self,\r\n                                    _(\"Delete Project\"),\r\n                                    triggered=self.delete_project)\r\n        self.clear_recent_projects_action =\\\r\n            create_action(self, _(\"Clear this list\"),\r\n                          triggered=self.clear_recent_projects)\r\n        self.edit_project_preferences_action =\\\r\n            create_action(self, _(\"Project Preferences\"),\r\n                          triggered=self.edit_project_preferences)\r\n        self.recent_project_menu = QMenu(_(\"Recent Projects\"), self)\r\n\r\n        if self.main is not None:\r\n            self.main.projects_menu_actions += [self.new_project_action,\r\n                                                MENU_SEPARATOR,\r\n                                                self.open_project_action,\r\n                                                self.close_project_action,\r\n                                                self.delete_project_action,\r\n                                                MENU_SEPARATOR,\r\n                                                self.recent_project_menu,\r\n                                                self.toggle_view_action]\r\n\r\n        self.setup_menu_actions()\r\n        return []", "output": "Return a list of actions related to plugin", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _py2java(sc, obj):\n    \"\"\"  \"\"\"\n    if isinstance(obj, RDD):\n        obj = _to_java_object_rdd(obj)\n    elif isinstance(obj, DataFrame):\n        obj = obj._jdf\n    elif isinstance(obj, SparkContext):\n        obj = obj._jsc\n    elif isinstance(obj, list):\n        obj = [_py2java(sc, x) for x in obj]\n    elif isinstance(obj, JavaObject):\n        pass\n    elif isinstance(obj, (int, long, float, bool, bytes, unicode)):\n        pass\n    else:\n        data = bytearray(PickleSerializer().dumps(obj))\n        obj = sc._jvm.org.apache.spark.mllib.api.python.SerDe.loads(data)\n    return obj", "output": "Convert Python object into Java", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def values_for_column(self,\n                          column_name,\n                          limit=10000):\n        \"\"\"\"\"\"\n        logging.info(\n            'Getting values for columns [{}] limited to [{}]'\n            .format(column_name, limit))\n        # TODO: Use Lexicographic TopNMetricSpec once supported by PyDruid\n        if self.fetch_values_from:\n            from_dttm = utils.parse_human_datetime(self.fetch_values_from)\n        else:\n            from_dttm = datetime(1970, 1, 1)\n\n        qry = dict(\n            datasource=self.datasource_name,\n            granularity='all',\n            intervals=from_dttm.isoformat() + '/' + datetime.now().isoformat(),\n            aggregations=dict(count=count('count')),\n            dimension=column_name,\n            metric='count',\n            threshold=limit,\n        )\n\n        client = self.cluster.get_pydruid_client()\n        client.topn(**qry)\n        df = client.export_pandas()\n        return [row[column_name] for row in df.to_records(index=False)]", "output": "Retrieve some values for the given column", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _infer_fill_value(val):\n    \"\"\"\n    \n    \"\"\"\n\n    if not is_list_like(val):\n        val = [val]\n    val = np.array(val, copy=False)\n    if is_datetimelike(val):\n        return np.array('NaT', dtype=val.dtype)\n    elif is_object_dtype(val.dtype):\n        dtype = lib.infer_dtype(ensure_object(val), skipna=False)\n        if dtype in ['datetime', 'datetime64']:\n            return np.array('NaT', dtype=_NS_DTYPE)\n        elif dtype in ['timedelta', 'timedelta64']:\n            return np.array('NaT', dtype=_TD_DTYPE)\n    return np.nan", "output": "infer the fill value for the nan/NaT from the provided\n    scalar/ndarray/list-like if we are a NaT, return the correct dtyped\n    element to provide proper block construction", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_to_graph(self, term, parents):\n        \"\"\"\n        \n        \"\"\"\n        if self._frozen:\n            raise ValueError(\n                \"Can't mutate %s after construction.\" % type(self).__name__\n            )\n\n        # If we've seen this node already as a parent of the current traversal,\n        # it means we have an unsatisifiable dependency.  This should only be\n        # possible if the term's inputs are mutated after construction.\n        if term in parents:\n            raise CyclicDependency(term)\n\n        parents.add(term)\n\n        self.graph.add_node(term)\n\n        for dependency in term.dependencies:\n            self._add_to_graph(dependency, parents)\n            self.graph.add_edge(dependency, term)\n\n        parents.remove(term)", "output": "Add a term and all its children to ``graph``.\n\n        ``parents`` is the set of all the parents of ``term` that we've added\n        so far. It is only used to detect dependency cycles.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dispatch(self, receiver):\n        ''' \n\n        '''\n        super(SessionCallbackAdded, self).dispatch(receiver)\n        if hasattr(receiver, '_session_callback_added'):\n            receiver._session_callback_added(self)", "output": "Dispatch handling of this event to a receiver.\n\n        This method will invoke ``receiver._session_callback_added`` if\n        it exists.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install_os(name, **kwargs):\n    '''\n    \n\n    '''\n    ret = {'name': name, 'changes': {}, 'result': True, 'comment': ''}\n    ret['changes'] = __salt__['junos.install_os'](name, **kwargs)\n    return ret", "output": "Installs the given image on the device. After the installation is complete\n    the device is rebooted, if reboot=True is given as a keyworded argument.\n\n    .. code-block:: yaml\n\n            salt://images/junos_image.tgz:\n              junos:\n                - install_os\n                - timeout: 100\n                - reboot: True\n\n    Parameters:\n      Required\n        * path:\n          Path where the image file is present on the pro\\\n          xy minion.\n      Optional\n        * kwargs: keyworded arguments to be given such as timeout, reboot etc\n            * timeout:\n              Set NETCONF RPC timeout. Can be used to RPCs which\n              take a while to execute. (default = 30 seconds)\n            * reboot:\n              Whether to reboot after installation (default = False)\n            * no_copy:\n              When True the software package will not be SCP\u2019d to the device. \\\n              (default = False)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shift(self, periods=1, freq=None, axis='major'):\n        \"\"\"\n        \n        \"\"\"\n        if freq:\n            return self.tshift(periods, freq, axis=axis)\n\n        return super().slice_shift(periods, axis=axis)", "output": "Shift index by desired number of periods with an optional time freq.\n\n        The shifted data will not include the dropped periods and the\n        shifted axis will be smaller than the original. This is different\n        from the behavior of DataFrame.shift()\n\n        Parameters\n        ----------\n        periods : int\n            Number of periods to move, can be positive or negative\n        freq : DateOffset, timedelta, or time rule string, optional\n        axis : {'items', 'major', 'minor'} or {0, 1, 2}\n\n        Returns\n        -------\n        shifted : Panel", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def requester(\n        url,\n        main_url=None,\n        delay=0,\n        cook=None,\n        headers=None,\n        timeout=10,\n        host=None,\n        proxies=[None],\n        user_agents=[None],\n        failed=None,\n        processed=None\n    ):\n    \"\"\"\"\"\"\n    cook = cook or set()\n    headers = headers or set()\n    user_agents = user_agents or ['Photon']\n    failed = failed or set()\n    processed = processed or set()\n    # Mark the URL as crawled\n    processed.add(url)\n    # Pause/sleep the program for specified time\n    time.sleep(delay)\n\n    def make_request(url):\n        \"\"\"Default request\"\"\"\n        final_headers = headers or {\n            'Host': host,\n            # Selecting a random user-agent\n            'User-Agent': random.choice(user_agents),\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n            'Accept-Language': 'en-US,en;q=0.5',\n            'Accept-Encoding': 'gzip',\n            'DNT': '1',\n            'Connection': 'close',\n        }\n        try:\n            response = SESSION.get(\n                url,\n                cookies=cook,\n                headers=final_headers,\n                verify=False,\n                timeout=timeout,\n                stream=True,\n                proxies=random.choice(proxies)\n            )\n        except TooManyRedirects:\n            return 'dummy'\n\n        if 'text/html' in response.headers['content-type'] or \\\n           'text/plain' in response.headers['content-type']:\n            if response.status_code != '404':\n                return response.text\n            else:\n                response.close()\n                failed.add(url)\n                return 'dummy'\n        else:\n            response.close()\n            return 'dummy'\n\n    return make_request(url)", "output": "Handle the requests and return the response body.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def widgetbox(*args, **kwargs):\n    \"\"\" \n    \"\"\"\n\n    sizing_mode = kwargs.pop('sizing_mode', None)\n    children = kwargs.pop('children', None)\n\n    children = _handle_children(*args, children=children)\n\n    col_children = []\n    for item in children:\n        if isinstance(item, LayoutDOM):\n            if sizing_mode is not None and _has_auto_sizing(item):\n                item.sizing_mode = sizing_mode\n            col_children.append(item)\n        else:\n            raise ValueError(\"\"\"Only LayoutDOM items can be inserted into a widget box. Tried to insert: %s of type %s\"\"\" % (item, type(item)))\n    return WidgetBox(children=col_children, sizing_mode=sizing_mode, **kwargs)", "output": "Create a column of bokeh widgets with predefined styling.\n\n    Args:\n        children (list of :class:`~bokeh.models.widgets.widget.Widget`): A list of widgets.\n\n        sizing_mode (``\"fixed\"``, ``\"stretch_both\"``, ``\"scale_width\"``, ``\"scale_height\"``, ``\"scale_both\"`` ): How\n            will the items in the layout resize to fill the available space.\n            Default is ``\"fixed\"``. For more information on the different\n            modes see :attr:`~bokeh.models.layouts.LayoutDOM.sizing_mode`\n            description on :class:`~bokeh.models.layouts.LayoutDOM`.\n\n    Returns:\n        WidgetBox: A column layout of widget instances all with the same ``sizing_mode``.\n\n    Examples:\n\n        >>> widgetbox([button, select])\n        >>> widgetbox(children=[slider], sizing_mode='scale_width')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _query(lamp_id, state, action='', method='GET'):\n    '''\n    \n    '''\n    # Because salt.utils.query is that dreadful... :(\n\n    err = None\n    url = \"{0}/lights{1}\".format(CONFIG['uri'],\n                                 lamp_id and '/{0}'.format(lamp_id) or '') \\\n          + (action and \"/{0}\".format(action) or '')\n    conn = http_client.HTTPConnection(CONFIG['host'])\n    if method == 'PUT':\n        conn.request(method, url, salt.utils.json.dumps(state))\n    else:\n        conn.request(method, url)\n    resp = conn.getresponse()\n\n    if resp.status == http_client.OK:\n        res = salt.utils.json.loads(resp.read())\n    else:\n        err = \"HTTP error: {0}, {1}\".format(resp.status, resp.reason)\n    conn.close()\n    if err:\n        raise CommandExecutionError(err)\n\n    return res", "output": "Query the URI\n\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unicode_is_ascii(u_string):\n    \"\"\"\n    \"\"\"\n    assert isinstance(u_string, str)\n    try:\n        u_string.encode('ascii')\n        return True\n    except UnicodeEncodeError:\n        return False", "output": "Determine if unicode string only contains ASCII characters.\n\n    :param str u_string: unicode string to check. Must be unicode\n        and not Python 2 `str`.\n    :rtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def match(self, request: httputil.HTTPServerRequest) -> Optional[Dict[str, Any]]:\n        \"\"\"\"\"\"\n        raise NotImplementedError()", "output": "Matches current instance against the request.\n\n        :arg httputil.HTTPServerRequest request: current HTTP request\n        :returns: a dict of parameters to be passed to the target handler\n            (for example, ``handler_kwargs``, ``path_args``, ``path_kwargs``\n            can be passed for proper `~.web.RequestHandler` instantiation).\n            An empty dict is a valid (and common) return value to indicate a match\n            when the argument-passing features are not used.\n            ``None`` must be returned to indicate that there is no match.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def move_tab(self, index_from, index_to):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        filename = self.filenames.pop(index_from)\r\n        client = self.clients.pop(index_from)\r\n        self.filenames.insert(index_to, filename)\r\n        self.clients.insert(index_to, client)\r\n        self.update_tabs_text()\r\n        self.sig_update_plugin_title.emit()", "output": "Move tab (tabs themselves have already been moved by the tabwidget)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find(path, options):\n    '''\n    \n    '''\n    finder = Finder(options)\n    for path in finder.find(path):\n        yield path", "output": "WRITEME", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_eng_float_format(accuracy=3, use_eng_prefix=False):\n    \"\"\"\n    \n    \"\"\"\n\n    set_option(\"display.float_format\", EngFormatter(accuracy, use_eng_prefix))\n    set_option(\"display.column_space\", max(12, accuracy + 9))", "output": "Alter default behavior on how float is formatted in DataFrame.\n    Format float in engineering format. By accuracy, we mean the number of\n    decimal digits after the floating point.\n\n    See also EngFormatter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _import_platform_generator(platform):\n    '''\n    \n    '''\n    log.debug('Using platform: %s', platform)\n    for mod_name, mod_obj in inspect.getmembers(capirca.aclgen):\n        if mod_name == platform and inspect.ismodule(mod_obj):\n            for plat_obj_name, plat_obj in inspect.getmembers(mod_obj):  # pylint: disable=unused-variable\n                if inspect.isclass(plat_obj) and issubclass(plat_obj, capirca.lib.aclgenerator.ACLGenerator):\n                    log.debug('Identified Capirca class %s for %s', plat_obj, platform)\n                    return plat_obj\n    log.error('Unable to identify any Capirca plaform class for %s', platform)", "output": "Given a specific platform (under the Capirca conventions),\n    return the generator class.\n    The generator class is identified looking under the <platform> module\n    for a class inheriting the `ACLGenerator` class.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def construct_1d_object_array_from_listlike(values):\n    \"\"\"\n    \n    \"\"\"\n    # numpy will try to interpret nested lists as further dimensions, hence\n    # making a 1D array that contains list-likes is a bit tricky:\n    result = np.empty(len(values), dtype='object')\n    result[:] = values\n    return result", "output": "Transform any list-like object in a 1-dimensional numpy array of object\n    dtype.\n\n    Parameters\n    ----------\n    values : any iterable which has a len()\n\n    Raises\n    ------\n    TypeError\n        * If `values` does not have a len()\n\n    Returns\n    -------\n    1-dimensional numpy array of dtype object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_caffemodel(file_path):\n    \"\"\"\n    \n    \"\"\"\n    f = open(file_path, 'rb')\n    contents = f.read()\n\n    net_param = caffe_pb2.NetParameter()\n    net_param.ParseFromString(contents)\n\n    layers = find_layers(net_param)\n    return layers", "output": "parses the trained .caffemodel file\n\n    filepath: /path/to/trained-model.caffemodel\n\n    returns: layers", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _process_parse_dates_argument(parse_dates):\n    \"\"\"\"\"\"\n    # handle non-list entries for parse_dates gracefully\n    if parse_dates is True or parse_dates is None or parse_dates is False:\n        parse_dates = []\n\n    elif not hasattr(parse_dates, '__iter__'):\n        parse_dates = [parse_dates]\n    return parse_dates", "output": "Process parse_dates argument for read_sql functions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_state_shape_invariants(tensor):\n  \"\"\"\"\"\"\n  shape = tensor.shape.as_list()\n  for i in range(1, len(shape) - 1):\n    shape[i] = None\n  return tf.TensorShape(shape)", "output": "Returns the shape of the tensor but sets middle dims to None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def itermerged(self):\n        \"\"\"\"\"\"\n        for key in self:\n            val = self._container[key.lower()]\n            yield val[0], ', '.join(val[1:])", "output": "Iterate over all headers, merging duplicate ones together.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stretch_cv(x,sr,sc,interpolation=cv2.INTER_AREA):\n    \"\"\"  \"\"\"\n    if sr==0 and sc==0: return x\n    r,c,*_ = x.shape\n    x = cv2.resize(x, None, fx=sr+1, fy=sc+1, interpolation=interpolation)\n    nr,nc,*_ = x.shape\n    cr = (nr-r)//2; cc = (nc-c)//2\n    return x[cr:r+cr, cc:c+cc]", "output": "Stretches image x horizontally by sr+1, and vertically by sc+1 while retaining the original image size and proportion.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_dot(self, f, skip_disconnected=True):\n        \"\"\"\n        \"\"\"\n        disconnected = []\n\n        f.write(\"digraph dependencies {\\n\")\n        for dist, adjs in self.adjacency_list.items():\n            if len(adjs) == 0 and not skip_disconnected:\n                disconnected.append(dist)\n            for other, label in adjs:\n                if not label is None:\n                    f.write('\"%s\" -> \"%s\" [label=\"%s\"]\\n' %\n                            (dist.name, other.name, label))\n                else:\n                    f.write('\"%s\" -> \"%s\"\\n' % (dist.name, other.name))\n        if not skip_disconnected and len(disconnected) > 0:\n            f.write('subgraph disconnected {\\n')\n            f.write('label = \"Disconnected\"\\n')\n            f.write('bgcolor = red\\n')\n\n            for dist in disconnected:\n                f.write('\"%s\"' % dist.name)\n                f.write('\\n')\n            f.write('}\\n')\n        f.write('}\\n')", "output": "Writes a DOT output for the graph to the provided file *f*.\n\n        If *skip_disconnected* is set to ``True``, then all distributions\n        that are not dependent on any other distribution are skipped.\n\n        :type f: has to support ``file``-like operations\n        :type skip_disconnected: ``bool``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pack(o, stream, **kwargs):\n    '''\n    \n    '''\n    msgpack_module = kwargs.pop('_msgpack_module', msgpack)\n    orig_enc_func = kwargs.pop('default', lambda x: x)\n\n    def _enc_func(obj):\n        obj = ThreadLocalProxy.unproxy(obj)\n        return orig_enc_func(obj)\n\n    return msgpack_module.pack(o, stream, default=_enc_func, **kwargs)", "output": ".. versionadded:: 2018.3.4\n\n    Wraps msgpack.pack and ensures that the passed object is unwrapped if it is\n    a proxy.\n\n    By default, this function uses the msgpack module and falls back to\n    msgpack_pure, if the msgpack is not available. You can pass an alternate\n    msgpack module using the _msgpack_module argument.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_to_container_dirs(host_paths_to_convert, host_to_container_path_mapping):\n        \"\"\"\n        \n        \"\"\"\n\n        if not host_paths_to_convert:\n            # Nothing to do\n            return host_paths_to_convert\n\n        # Make sure the key is absolute host path. Relative paths are tricky to work with because two different\n        # relative paths can point to the same directory (\"../foo\", \"../../foo\")\n        mapping = {str(pathlib.Path(p).resolve()): v for p, v in host_to_container_path_mapping.items()}\n\n        result = []\n        for original_path in host_paths_to_convert:\n            abspath = str(pathlib.Path(original_path).resolve())\n\n            if abspath in mapping:\n                result.append(mapping[abspath])\n            else:\n                result.append(original_path)\n                LOG.debug(\"Cannot convert host path '%s' to its equivalent path within the container. \"\n                          \"Host path is not mounted within the container\", abspath)\n\n        return result", "output": "Use this method to convert a list of host paths to a list of equivalent paths within the container\n        where the given host path is mounted. This is necessary when SAM CLI needs to pass path information to\n        the Lambda Builder running within the container.\n\n        If a host path is not mounted within the container, then this method simply passes the path to the result\n        without any changes.\n\n        Ex:\n            [ \"/home/foo\", \"/home/bar\", \"/home/not/mounted\"]  => [\"/tmp/source\", \"/tmp/manifest\", \"/home/not/mounted\"]\n\n        Parameters\n        ----------\n        host_paths_to_convert : list\n            List of paths in host that needs to be converted\n\n        host_to_container_path_mapping : dict\n            Mapping of paths in host to the equivalent paths within the container\n\n        Returns\n        -------\n        list\n            Equivalent paths within the container", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kill_reporter(self, check_alive=True):\n        \"\"\"\n        \"\"\"\n        # reporter is started only in PY3.\n        if PY3:\n            self._kill_process_type(\n                ray_constants.PROCESS_TYPE_REPORTER, check_alive=check_alive)", "output": "Kill the reporter.\n\n        Args:\n            check_alive (bool): Raise an exception if the process was already\n                dead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _from_java(cls, java_stage):\n        \"\"\"\n        \n        \"\"\"\n\n        estimator, epms, evaluator = super(TrainValidationSplit, cls)._from_java_impl(java_stage)\n        trainRatio = java_stage.getTrainRatio()\n        seed = java_stage.getSeed()\n        parallelism = java_stage.getParallelism()\n        collectSubModels = java_stage.getCollectSubModels()\n        # Create a new instance of this stage.\n        py_stage = cls(estimator=estimator, estimatorParamMaps=epms, evaluator=evaluator,\n                       trainRatio=trainRatio, seed=seed, parallelism=parallelism,\n                       collectSubModels=collectSubModels)\n        py_stage._resetUid(java_stage.uid())\n        return py_stage", "output": "Given a Java TrainValidationSplit, create and return a Python wrapper of it.\n        Used for ML persistence.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_basic_op_node(op_name, node, kwargs):\n    \"\"\"\"\"\"\n    name, input_nodes, _ = get_inputs(node, kwargs)\n\n    node = onnx.helper.make_node(\n        op_name,\n        input_nodes,\n        [name],\n        name=name\n    )\n    return [node]", "output": "Helper function to create a basic operator\n    node that doesn't contain op specific attrs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def AddExtensionDescriptor(self, extension):\n    \"\"\"\n    \"\"\"\n    if not (isinstance(extension, descriptor.FieldDescriptor) and\n            extension.is_extension):\n      raise TypeError('Expected an extension descriptor.')\n\n    if extension.extension_scope is None:\n      self._toplevel_extensions[extension.full_name] = extension\n\n    try:\n      existing_desc = self._extensions_by_number[\n          extension.containing_type][extension.number]\n    except KeyError:\n      pass\n    else:\n      if extension is not existing_desc:\n        raise AssertionError(\n            'Extensions \"%s\" and \"%s\" both try to extend message type \"%s\" '\n            'with field number %d.' %\n            (extension.full_name, existing_desc.full_name,\n             extension.containing_type.full_name, extension.number))\n\n    self._extensions_by_number[extension.containing_type][\n        extension.number] = extension\n    self._extensions_by_name[extension.containing_type][\n        extension.full_name] = extension\n\n    # Also register MessageSet extensions with the type name.\n    if _IsMessageSetExtension(extension):\n      self._extensions_by_name[extension.containing_type][\n          extension.message_type.full_name] = extension", "output": "Adds a FieldDescriptor describing an extension to the pool.\n\n    Args:\n      extension: A FieldDescriptor.\n\n    Raises:\n      AssertionError: when another extension with the same number extends the\n        same message.\n      TypeError: when the specified extension is not a\n        descriptor.FieldDescriptor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_blob_names(self):\n        \"\"\"\n        \n        \"\"\"\n        # generate blob names that represent edges in blob_name_map\n        # because of the InputLayers, input blobs are also generated.\n\n        # Generate each layer's input / output blob names\n        for layer in self.layer_list:\n            keras_layer = self.keras_layer_map[layer]\n            # no need to generate InputLayers' blobs\n            if not isinstance(keras_layer, _keras.engine.topology.InputLayer):\n                # layer's input blob names depend on predecessors\n                preds = self.get_predecessors(layer)\n                for pred in preds:\n                    blob_name = pred + '_output'\n                    _insert_to_dict(self.layers_inputs, layer, blob_name)\n                # layer's output blob is just named after itself\n                blob_name = layer + '_output'\n                _insert_to_dict(self.layers_outputs, layer, blob_name)", "output": "Generate blob names for each one of the edge.  At this time, Keras does not\n        support \"fork\" operation (a layer with more than 1 blob output). So we just\n        use names of the src layer to identify a blob.  We also assume all neural\n        networks are singly-connected graphs - which should be the case.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def received_sig_option_changed(self, option, value):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        if option == 'autosave_mapping':\r\n            for editorstack in self.editorstacks:\r\n                if editorstack != self.sender():\r\n                    editorstack.autosave_mapping = value\r\n        self.sig_option_changed.emit(option, value)", "output": "Called when sig_option_changed is received.\r\n\r\n        If option being changed is autosave_mapping, then synchronize new\r\n        mapping with all editor stacks except the sender.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_item_size(self, content):\n        \"\"\"\n        \n        \"\"\"\n        strings = []\n        if content:\n            for rich_text in content:\n                label = QLabel(rich_text)\n                label.setTextFormat(Qt.PlainText)\n                strings.append(label.text())\n                fm = label.fontMetrics()\n\n            return (max([fm.width(s) * 1.3 for s in strings]), fm.height())", "output": "Get the max size (width and height) for the elements of a list of\n        strings as a QLabel.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_mode(path, mode):\n    '''\n    \n    '''\n    path = os.path.expanduser(path)\n\n    mode = six.text_type(mode).lstrip('0Oo')\n    if not mode:\n        mode = '0'\n    if not os.path.exists(path):\n        raise CommandExecutionError('{0}: File not found'.format(path))\n    try:\n        os.chmod(path, int(mode, 8))\n    except Exception:\n        return 'Invalid Mode ' + mode\n    return get_mode(path)", "output": "Set the mode of a file\n\n    path\n        file or directory of which to set the mode\n\n    mode\n        mode to set the path to\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.set_mode /etc/passwd 0644", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_ngram_counter(ids, n):\n  \"\"\"\n  \"\"\"\n  # Remove zero IDs used to pad the sequence.\n  ids = [token_id for token_id in ids if token_id != 0]\n  ngram_list = [tuple(ids[i:i + n]) for i in range(len(ids) + 1 - n)]\n  ngrams = set(ngram_list)\n  counts = collections.Counter()\n  for ngram in ngrams:\n    counts[ngram] = 1\n  return counts", "output": "Get a Counter with the ngrams of the given ID list.\n\n  Args:\n    ids: np.array or a list corresponding to a single sentence\n    n: n-gram size\n\n  Returns:\n    collections.Counter with ID tuples as keys and 1s as values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def on_command_error(self, context, exception):\n        \"\"\"\n        \"\"\"\n        if self.extra_events.get('on_command_error', None):\n            return\n\n        if hasattr(context.command, 'on_error'):\n            return\n\n        cog = context.cog\n        if cog:\n            if Cog._get_overridden_method(cog.cog_command_error) is not None:\n                return\n\n        print('Ignoring exception in command {}:'.format(context.command), file=sys.stderr)\n        traceback.print_exception(type(exception), exception, exception.__traceback__, file=sys.stderr)", "output": "|coro|\n\n        The default command error handler provided by the bot.\n\n        By default this prints to ``sys.stderr`` however it could be\n        overridden to have a different implementation.\n\n        This only fires if you do not specify any listeners for command error.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extra(method, profile, **libcloud_kwargs):\n    '''\n    \n    '''\n    libcloud_kwargs = salt.utils.args.clean_kwargs(**libcloud_kwargs)\n    conn = _get_driver(profile=profile)\n    connection_method = getattr(conn, method)\n    return connection_method(**libcloud_kwargs)", "output": "Call an extended method on the driver\n\n    :param method: Driver's method name\n    :type  method: ``str``\n\n    :param profile: The profile key\n    :type  profile: ``str``\n\n    :param libcloud_kwargs: Extra arguments for the driver's method\n    :type  libcloud_kwargs: ``dict``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion libcloud_loadbalancer.extra ex_get_permissions google container_name=my_container object_name=me.jpg --out=yaml", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def refresh(self, index=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"POST\", _make_path(index, \"_refresh\"), params=params\n        )", "output": "Explicitly refresh one or more index, making all operations performed\n        since the last refresh available for search.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-refresh.html>`_\n\n        :arg index: A comma-separated list of index names; use `_all` or empty\n            string to perform the operation on all indices\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default 'open', valid\n            choices are: 'open', 'closed', 'none', 'all'\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dict_mapping_to_pb(mapping, proto_type):\n    \"\"\"\n    \n    \"\"\"\n    converted_pb = getattr(trace_pb2, proto_type)()\n    ParseDict(mapping, converted_pb)\n    return converted_pb", "output": "Convert a dict to protobuf.\n\n    Args:\n        mapping (dict): A dict that needs to be converted to protobuf.\n        proto_type (str): The type of the Protobuf.\n\n    Returns:\n        An instance of the specified protobuf.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def most_by_mask(self, mask, y, mult):\n        \"\"\" \n        \"\"\"\n        idxs = np.where(mask)[0]\n        cnt = min(4, len(idxs))\n        return idxs[np.argsort(mult * self.probs[idxs,y])[:cnt]]", "output": "Extracts the first 4 most correct/incorrect indexes from the ordered list of probabilities\n\n            Arguments:\n                mask (numpy.ndarray): the mask of probabilities specific to the selected class; a boolean array with shape (num_of_samples,) which contains True where class==selected_class, and False everywhere else\n                y (int): the selected class\n                mult (int): sets the ordering; -1 descending, 1 ascending\n\n            Returns:\n                idxs (ndarray): An array of indexes of length 4", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flavor_absent(name, **kwargs):\n    '''\n    \n    '''\n    dry_run = __opts__['test']\n    ret = {'name': name, 'result': False, 'comment': '', 'changes': {}}\n\n    try:\n        object_list = __salt__['nova.flavor_list'](**kwargs)\n        object_id = object_list[name]['id']\n    except KeyError:\n        object_id = False\n\n    if not object_id:\n        ret['result'] = True\n        ret['comment'] = 'Flavor \"{0}\" does not exist.'.format(name)\n    else:\n        if dry_run:\n            ret['result'] = None\n            ret['comment'] = 'Flavor \"{0}\", id: {1}  would be deleted.'.format(name, object_id)\n            ret['changes'] = {name: {'old': 'Flavor \"{0}\", id: {1}  exists.'.format(name, object_id),\n                                     'new': ret['comment']}}\n        else:\n            flavor_delete = __salt__['nova.flavor_delete'](object_id, **kwargs)\n\n            if flavor_delete:\n                ret['result'] = True\n                ret['comment'] = 'Flavor \"{0}\", id: {1}  deleted.'.format(name, object_id)\n                ret['changes'] = {name: {'old': 'Flavor \"{0}\", id: {1}  existed.'.format(name, object_id),\n                                         'new': ret['comment']}}\n\n    return ret", "output": "Makes flavor to be absent\n\n    :param name: flavor name\n\n    .. code-block:: yaml\n\n        nova-flavor-absent:\n            nova.flavor_absent:\n                - name: flavor_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_include_path():\n    \"\"\"\n    \"\"\"\n    incl_from_env = os.environ.get('MXNET_INCLUDE_PATH')\n    if incl_from_env:\n        if os.path.isdir(incl_from_env):\n            if not os.path.isabs(incl_from_env):\n                logging.warning(\"MXNET_INCLUDE_PATH should be an absolute path, instead of: %s\",\n                                incl_from_env)\n            else:\n                return incl_from_env\n        else:\n            logging.warning(\"MXNET_INCLUDE_PATH '%s' doesn't exist\", incl_from_env)\n\n    curr_path = os.path.dirname(os.path.abspath(os.path.expanduser(__file__)))\n    # include path in pip package\n    pip_incl_path = os.path.join(curr_path, 'include/')\n    if os.path.isdir(pip_incl_path):\n        return pip_incl_path\n    else:\n        # include path if build from source\n        src_incl_path = os.path.join(curr_path, '../../include/')\n        if os.path.isdir(src_incl_path):\n            return src_incl_path\n        else:\n            raise RuntimeError('Cannot find the MXNet include path in either ' + pip_incl_path +\n                               ' or ' + src_incl_path + '\\n')", "output": "Find MXNet included header files.\n\n    Returns\n    -------\n    incl_path : string\n        Path to the header files.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def project_deidentify_template_path(cls, project, deidentify_template):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/deidentifyTemplates/{deidentify_template}\",\n            project=project,\n            deidentify_template=deidentify_template,\n        )", "output": "Return a fully-qualified project_deidentify_template string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _SkipVarint(buffer, pos, end):\n  \"\"\"\"\"\"\n  # Previously ord(buffer[pos]) raised IndexError when pos is out of range.\n  # With this code, ord(b'') raises TypeError.  Both are handled in\n  # python_message.py to generate a 'Truncated message' error.\n  while ord(buffer[pos:pos+1]) & 0x80:\n    pos += 1\n  pos += 1\n  if pos > end:\n    raise _DecodeError('Truncated message.')\n  return pos", "output": "Skip a varint value.  Returns the new position.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_env(name, env_creator):\n    \"\"\"\n    \"\"\"\n\n    if not isinstance(env_creator, FunctionType):\n        raise TypeError(\"Second argument must be a function.\", env_creator)\n    _global_registry.register(ENV_CREATOR, name, env_creator)", "output": "Register a custom environment for use with RLlib.\n\n    Args:\n        name (str): Name to register.\n        env_creator (obj): Function that creates an env.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mousePressEvent(self, event):\n        \"\"\"\n        \"\"\"\n        line_number = self.editor.get_linenumber_from_mouse_event(event)\n        self._pressed = line_number\n        self._released = line_number\n        self.editor.select_lines(self._pressed,\n                                 self._released)", "output": "Override Qt method\n\n        Select line, and starts selection", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_id():\n    ''' \n\n    '''\n    global _simple_id\n\n    if settings.simple_ids(True):\n        with _simple_id_lock:\n            _simple_id += 1\n            return str(_simple_id)\n    else:\n        return make_globally_unique_id()", "output": "Return a new unique ID for a Bokeh object.\n\n    Normally this function will return simple monotonically increasing integer\n    IDs (as strings) for identifying Bokeh objects within a Document. However,\n    if it is desirable to have globally unique for every object, this behavior\n    can be overridden by setting the environment variable ``BOKEH_SIMPLE_IDS=no``.\n\n    Returns:\n        str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_kernel_spec(self, kernel_name):\n        \"\"\"\n        \"\"\"\n        if kernel_name == CURRENT_ENV_KERNEL_NAME:\n            return self.kernel_spec_class(\n                resource_dir=ipykernel.kernelspec.RESOURCES,\n                **ipykernel.kernelspec.get_kernel_dict())\n        else:\n            return super(NbvalKernelspecManager, self).get_kernel_spec(kernel_name)", "output": "Returns a :class:`KernelSpec` instance for the given kernel_name.\n\n        Raises :exc:`NoSuchKernel` if the given kernel name is not found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rand_int(start=1, end=10, seed=None):\n    '''\n    \n    '''\n    if seed is not None:\n        random.seed(seed)\n    return random.randint(start, end)", "output": "Returns a random integer number between the start and end number.\n\n    .. versionadded: 2015.5.3\n\n    start : 1\n        Any valid integer number\n\n    end : 10\n        Any valid integer number\n\n    seed :\n        Optional hashable object\n\n    .. versionchanged:: 2019.2.0\n        Added seed argument. Will return the same result when run with the same seed.\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' random.rand_int 1 10", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def installed(name,\n              cyg_arch='x86_64',\n              mirrors=None):\n    '''\n    \n    '''\n    ret = {'name': name, 'result': None, 'comment': '', 'changes': {}}\n\n    if cyg_arch not in ['x86', 'x86_64']:\n        ret['result'] = False\n        ret['comment'] = 'The \\'cyg_arch\\' argument must\\\n be one of \\'x86\\' or \\'x86_64\\''\n        return ret\n\n    LOG.debug('Installed State: Initial Mirror list: %s', mirrors)\n\n    if not __salt__['cyg.check_valid_package'](name,\n                                               cyg_arch=cyg_arch,\n                                               mirrors=mirrors):\n        ret['result'] = False\n        ret['comment'] = 'Invalid package name.'\n        return ret\n\n    pkgs = __salt__['cyg.list'](name, cyg_arch)\n    if name in pkgs:\n        ret['result'] = True\n        ret['comment'] = 'Package is already installed.'\n        return ret\n\n    if __opts__['test']:\n        ret['comment'] = 'The package {0} would\\\n have been installed'.format(name)\n        return ret\n\n    if __salt__['cyg.install'](name,\n                               cyg_arch=cyg_arch,\n                               mirrors=mirrors):\n        ret['result'] = True\n        ret['changes'][name] = 'Installed'\n        ret['comment'] = 'Package was successfully installed'\n    else:\n        ret['result'] = False\n        ret['comment'] = 'Could not install package.'\n\n    return ret", "output": "Make sure that a package is installed.\n\n    name\n        The name of the package to install\n\n    cyg_arch : x86_64\n        The cygwin architecture to install the package into.\n        Current options are x86 and x86_64\n\n    mirrors : None\n        List of mirrors to check.\n        None will use a default mirror (kernel.org)\n\n    CLI Example:\n\n    .. code-block:: yaml\n\n        rsync:\n          cyg.installed:\n            - mirrors:\n              - http://mirror/without/public/key: \"\"\n              - http://mirror/with/public/key: http://url/of/public/key", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def servicegroup_delete(sg_name, **connection_args):\n    '''\n    \n    '''\n    ret = True\n    sg = _servicegroup_get(sg_name, **connection_args)\n    if sg is None:\n        return False\n    nitro = _connect(**connection_args)\n    if nitro is None:\n        return False\n    try:\n        NSServiceGroup.delete(nitro, sg)\n    except NSNitroError as error:\n        log.debug('netscaler module error - NSServiceGroup.delete() failed: %s', error)\n        ret = False\n    _disconnect(nitro)\n    return ret", "output": "Delete a new service group\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' netscaler.servicegroup_delete 'serviceGroupName'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def verify_webhook(signature, body):\n    '''\n    \n\n    '''\n    # get public key setup\n    public_key = __utils__['http.query']('https://api.travis-ci.org/config')['config']['notifications']['webhook']['public_key']\n    pkey_public_key = OpenSSL.crypto.load_publickey(OpenSSL.crypto.FILETYPE_PEM, public_key)\n    certificate = OpenSSL.crypto.X509()\n    certificate.set_pubkey(pkey_public_key)\n\n    # decode signature\n    signature = base64.b64decode(signature)\n\n    # parse the urlencoded payload from travis\n    payload = salt.utils.json.loads(parse_qs(body)['payload'][0])\n\n    try:\n        OpenSSL.crypto.verify(certificate, signature, payload, six.text_type('sha1'))\n    except OpenSSL.crypto.Error:\n        return False\n    return True", "output": "Verify the webhook signature from travisci\n\n    signature\n        The signature header from the webhook header\n\n    body\n        The full payload body from the webhook post\n\n    .. note:: The body needs to be the urlencoded version of the body.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' travisci.verify_webhook 'M6NucCX5722bxisQs7e...' 'payload=%7B%22id%22%3A183791261%2C%22repository...'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def drop(self):\n        \"\"\"\n        \n        \"\"\"\n        Statement = self.get_model('statement')\n        Tag = self.get_model('tag')\n\n        Statement.objects.all().delete()\n        Tag.objects.all().delete()", "output": "Remove all data from the database.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show(uuid):\n    '''\n    \n    '''\n    ret = {}\n\n    if _is_uuid(uuid) or _is_docker_uuid(uuid):\n        cmd = 'imgadm show {0}'.format(uuid)\n        res = __salt__['cmd.run_all'](cmd, python_shell=False)\n        retcode = res['retcode']\n        if retcode != 0:\n            ret['Error'] = _exit_status(retcode, res['stderr'])\n        else:\n            ret = salt.utils.json.loads(res['stdout'])\n    else:\n        ret['Error'] = \"{} is not a valid uuid.\".format(uuid)\n\n    return ret", "output": "Show manifest of a given image\n\n    uuid : string\n        uuid of image\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' imgadm.show e42f8c84-bbea-11e2-b920-078fab2aab1f\n        salt '*' imgadm.show plexinc/pms-docker:plexpass", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def assert_same_rank(self, other):\n        \"\"\"\n        \"\"\"\n        other = as_shape(other)\n        if self.ndims is not None and other.ndims is not None:\n            if self.ndims != other.ndims:\n                raise ValueError(\n                    \"Shapes %s and %s must have the same rank\" % (self, other)\n                )", "output": "Raises an exception if `self` and `other` do not have convertible ranks.\n\n        Args:\n          other: Another `TensorShape`.\n\n        Raises:\n          ValueError: If `self` and `other` do not represent shapes with the\n            same rank.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_extension_array_dtype(arr_or_dtype):\n    \"\"\"\n    \n    \"\"\"\n    dtype = getattr(arr_or_dtype, 'dtype', arr_or_dtype)\n    return (isinstance(dtype, ExtensionDtype) or\n            registry.find(dtype) is not None)", "output": "Check if an object is a pandas extension array type.\n\n    See the :ref:`Use Guide <extending.extension-types>` for more.\n\n    Parameters\n    ----------\n    arr_or_dtype : object\n        For array-like input, the ``.dtype`` attribute will\n        be extracted.\n\n    Returns\n    -------\n    bool\n        Whether the `arr_or_dtype` is an extension array type.\n\n    Notes\n    -----\n    This checks whether an object implements the pandas extension\n    array interface. In pandas, this includes:\n\n    * Categorical\n    * Sparse\n    * Interval\n    * Period\n    * DatetimeArray\n    * TimedeltaArray\n\n    Third-party libraries may implement arrays or types satisfying\n    this interface as well.\n\n    Examples\n    --------\n    >>> from pandas.api.types import is_extension_array_dtype\n    >>> arr = pd.Categorical(['a', 'b'])\n    >>> is_extension_array_dtype(arr)\n    True\n    >>> is_extension_array_dtype(arr.dtype)\n    True\n\n    >>> arr = np.array(['a', 'b'])\n    >>> is_extension_array_dtype(arr.dtype)\n    False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pop_assign_tracking(self, frame):\n        \"\"\"\n        \"\"\"\n        vars = self._assign_stack.pop()\n        if not frame.toplevel or not vars:\n            return\n        public_names = [x for x in vars if x[:1] != '_']\n        if len(vars) == 1:\n            name = next(iter(vars))\n            ref = frame.symbols.ref(name)\n            self.writeline('context.vars[%r] = %s' % (name, ref))\n        else:\n            self.writeline('context.vars.update({')\n            for idx, name in enumerate(vars):\n                if idx:\n                    self.write(', ')\n                ref = frame.symbols.ref(name)\n                self.write('%r: %s' % (name, ref))\n            self.write('})')\n        if public_names:\n            if len(public_names) == 1:\n                self.writeline('context.exported_vars.add(%r)' %\n                               public_names[0])\n            else:\n                self.writeline('context.exported_vars.update((%s))' %\n                               ', '.join(imap(repr, public_names)))", "output": "Pops the topmost level for assignment tracking and updates the\n        context variables if necessary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_stream_handle(stream=sys.stdout):\n    \"\"\"\n    \n    \"\"\"\n    handle = stream\n    if os.name == \"nt\":\n        from ctypes import windll\n\n        handle_id = WIN_STDOUT_HANDLE_ID\n        handle = windll.kernel32.GetStdHandle(handle_id)\n    return handle", "output": "Get the OS appropriate handle for the corresponding output stream.\n\n    :param str stream: The the stream to get the handle for\n    :return: A handle to the appropriate stream, either a ctypes buffer\n             or **sys.stdout** or **sys.stderr**.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def silent_execute(self, code):\n        \"\"\"\"\"\"\n        try:\n            self.kernel_client.execute(to_text_string(code), silent=True)\n        except AttributeError:\n            pass", "output": "Execute code in the kernel without increasing the prompt", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _shadow_model_variables(shadow_vars):\n        \"\"\"\n        \n        \"\"\"\n        G = tf.get_default_graph()\n        curr_shadow_vars = set([v.name for v in shadow_vars])\n        model_vars = tf.model_variables()\n        shadow_model_vars = []\n        for v in model_vars:\n            assert v.name.startswith('tower'), \"Found some MODEL_VARIABLES created outside of the tower function!\"\n            stripped_op_name, stripped_var_name = get_op_tensor_name(re.sub('^tower[0-9]+/', '', v.name))\n            if stripped_op_name in curr_shadow_vars:\n                continue\n            try:\n                G.get_tensor_by_name(stripped_var_name)\n                logger.warn(\"Model Variable {} also appears in other collections.\".format(stripped_var_name))\n                continue\n            except KeyError:\n                pass\n            new_v = tf.get_variable(stripped_op_name, dtype=v.dtype.base_dtype,\n                                    initializer=v.initial_value,\n                                    trainable=False)\n\n            curr_shadow_vars.add(stripped_op_name)  # avoid duplicated shadow_model_vars\n            shadow_vars.append(new_v)\n            shadow_model_vars.append((new_v, v))  # only need to sync model_var from one tower\n        return shadow_model_vars", "output": "Create shadow vars for model_variables as well, and add to the list of ``shadow_vars``.\n\n        Returns:\n            list of (shadow_model_var, local_model_var) used for syncing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def file_saved_in_editorstack(self, editorstack_id_str,\r\n                                  original_filename, filename):\r\n        \"\"\"\"\"\"\r\n        for editorstack in self.editorstacks:\r\n            if str(id(editorstack)) != editorstack_id_str:\r\n                editorstack.file_saved_in_other_editorstack(original_filename,\r\n                                                            filename)", "output": "A file was saved in editorstack, this notifies others", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_(bank):\n    '''\n    \n    '''\n    try:\n        _, keys = api.kv.get(bank + '/', keys=True, separator='/')\n    except Exception as exc:\n        raise SaltCacheError(\n            'There was an error getting the key \"{0}\": {1}'.format(\n                bank, exc\n            )\n        )\n    if keys is None:\n        keys = []\n    else:\n        # Any key could be a branch and a leaf at the same time in Consul\n        # so we have to return a list of unique names only.\n        out = set()\n        for key in keys:\n            out.add(key[len(bank) + 1:].rstrip('/'))\n        keys = list(out)\n    return keys", "output": "Return an iterable object containing all entries stored in the specified bank.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_comments(group_tasks):\n    \"\"\"\n    \n    \"\"\"\n    comments = {}\n    for status, human in _COMMENTS:\n        num_tasks = _get_number_of_tasks_for(status, group_tasks)\n        if num_tasks:\n            space = \"    \" if status in _PENDING_SUB_STATUSES else \"\"\n            comments[status] = '{space}* {num_tasks} {human}:\\n'.format(\n                space=space,\n                num_tasks=num_tasks,\n                human=human)\n    return comments", "output": "Get the human readable comments and quantities for the task types.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def series2cat(df:DataFrame, *col_names):\n    \"\"\n    for c in listify(col_names): df[c] = df[c].astype('category').cat.as_ordered()", "output": "Categorifies the columns `col_names` in `df`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vn_free_ar(call=None, kwargs=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The vn_free_ar function must be called with -f or --function.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    vn_id = kwargs.get('vn_id', None)\n    vn_name = kwargs.get('vn_name', None)\n    ar_id = kwargs.get('ar_id', None)\n\n    if ar_id is None:\n        raise SaltCloudSystemExit(\n            'The vn_free_ar function requires an \\'rn_id\\' to be provided.'\n        )\n\n    if vn_id:\n        if vn_name:\n            log.warning(\n                'Both the \\'vn_id\\' and \\'vn_name\\' arguments were provided. '\n                '\\'vn_id\\' will take precedence.'\n            )\n    elif vn_name:\n        vn_id = get_vn_id(kwargs={'name': vn_name})\n    else:\n        raise SaltCloudSystemExit(\n            'The vn_free_ar function requires a \\'vn_id\\' or a \\'vn_name\\' to '\n            'be provided.'\n        )\n\n    server, user, password = _get_xml_rpc()\n    auth = ':'.join([user, password])\n    response = server.one.vn.free_ar(auth, int(vn_id), int(ar_id))\n\n    data = {\n        'action': 'vn.free_ar',\n        'ar_freed': response[0],\n        'resource_id': response[1],\n        'error_code': response[2],\n    }\n\n    return data", "output": "Frees a reserved address range from a virtual network.\n\n    .. versionadded:: 2016.3.0\n\n    vn_id\n        The ID of the virtual network from which to free an address range.\n        Can be used instead of ``vn_name``.\n\n    vn_name\n        The name of the virtual network from which to free an address range.\n        Can be used instead of ``vn_id``.\n\n    ar_id\n        The ID of the address range to free.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f vn_free_ar opennebula vn_id=3 ar_id=1\n        salt-cloud -f vn_free_ar opennebula vn_name=my-vn ar_id=1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reload(self, client=None):\n        \"\"\"\n        \"\"\"\n        client = self._require_client(client)\n        data = client.metrics_api.metric_get(self.project, self.name)\n        self.description = data.get(\"description\", \"\")\n        self.filter_ = data[\"filter\"]", "output": "API call:  sync local metric configuration via a GET request\n\n        See\n        https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.metrics/get\n\n        :type client: :class:`~google.cloud.logging.client.Client` or\n                      ``NoneType``\n        :param client: the client to use.  If not passed, falls back to the\n                       ``client`` stored on the current metric.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def project_data_source_path(cls, project, data_source):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/dataSources/{data_source}\",\n            project=project,\n            data_source=data_source,\n        )", "output": "Return a fully-qualified project_data_source string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_zone(zone, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    if region is None:\n        region = 'universal'\n\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    _zone = conn.get_zone(zone)\n\n    if _zone:\n        conn.delete_hosted_zone(_zone.id)\n        return True\n    return False", "output": "Delete a Route53 hosted zone.\n\n    .. versionadded:: 2015.8.0\n\n    CLI Example::\n\n        salt myminion boto_route53.delete_zone example.org", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def connect(self):\n        \"\"\"\"\"\"\n        self._cidmotion = self.canvas.mpl_connect('motion_notify_event',\n                                                  self.onmove)\n        self._ciddraw = self.canvas.mpl_connect('draw_event', self.clear)", "output": "connect events", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def EntryTagName(self, entry):\n        \"\"\"\"\"\"\n        name = \"%s_%s\" % (self._name, entry.Name())\n        return name.upper()", "output": "Creates the name inside an enumeration for distinguishing data\n        types.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_listener(self, func, name=None):\n        \"\"\"\n\n        \"\"\"\n        name = func.__name__ if name is None else name\n\n        if not asyncio.iscoroutinefunction(func):\n            raise TypeError('Listeners must be coroutines')\n\n        if name in self.extra_events:\n            self.extra_events[name].append(func)\n        else:\n            self.extra_events[name] = [func]", "output": "The non decorator alternative to :meth:`.listen`.\n\n        Parameters\n        -----------\n        func: :ref:`coroutine <coroutine>`\n            The function to call.\n        name: Optional[:class:`str`]\n            The name of the event to listen for. Defaults to ``func.__name__``.\n\n        Example\n        --------\n\n        .. code-block:: python3\n\n            async def on_ready(): pass\n            async def my_message(message): pass\n\n            bot.add_listener(on_ready)\n            bot.add_listener(my_message, 'on_message')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forward(*args, **kwargs):\n        \"\"\"\n        \"\"\"\n        self, cmd = args[:2]\n\n        # It's also possible to invoke another command which might or\n        # might not have a callback.\n        if not isinstance(cmd, Command):\n            raise TypeError('Callback is not a command.')\n\n        for param in self.params:\n            if param not in kwargs:\n                kwargs[param] = self.params[param]\n\n        return self.invoke(cmd, **kwargs)", "output": "Similar to :meth:`invoke` but fills in default keyword\n        arguments from the current context if the other command expects\n        it.  This cannot invoke callbacks directly, only other commands.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __boost(self, grad, hess):\n        \"\"\"\n        \"\"\"\n        grad = list_to_1d_numpy(grad, name='gradient')\n        hess = list_to_1d_numpy(hess, name='hessian')\n        assert grad.flags.c_contiguous\n        assert hess.flags.c_contiguous\n        if len(grad) != len(hess):\n            raise ValueError(\"Lengths of gradient({}) and hessian({}) don't match\"\n                             .format(len(grad), len(hess)))\n        is_finished = ctypes.c_int(0)\n        _safe_call(_LIB.LGBM_BoosterUpdateOneIterCustom(\n            self.handle,\n            grad.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n            hess.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n            ctypes.byref(is_finished)))\n        self.__is_predicted_cur_iter = [False for _ in range_(self.__num_dataset)]\n        return is_finished.value == 1", "output": "Boost Booster for one iteration with customized gradient statistics.\n\n        Note\n        ----\n        For multi-class task, the score is group by class_id first, then group by row_id.\n        If you want to get i-th row score in j-th class, the access way is score[j * num_data + i]\n        and you should group grad and hess in this way as well.\n\n        Parameters\n        ----------\n        grad : 1-D numpy array or 1-D list\n            The first order derivative (gradient).\n        hess : 1-D numpy array or 1-D list\n            The second order derivative (Hessian).\n\n        Returns\n        -------\n        is_finished : bool\n            Whether the boost was successfully finished.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pack(self, grads):\n        \"\"\"\n        \n        \"\"\"\n        for i, g in enumerate(grads):\n            assert g.shape == self._shapes[i]\n\n        with cached_name_scope(\"GradientPacker\", top_level=False):\n            concat_grads = tf.concat([tf.reshape(g, [-1]) for g in grads], 0, name='concatenated_grads')\n            # concat_grads = tf.cast(concat_grads, tf.float16)\n            grad_packs = tf.split(concat_grads, self._split_sizes)\n            return grad_packs", "output": "Args:\n            grads (list): list of gradient tensors\n\n        Returns:\n            packed list of gradient tensors to be aggregated.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _bind_for_search(anonymous=False, opts=None):\n    '''\n    \n    '''\n    # Get config params; create connection dictionary\n    connargs = {}\n    # config params (auth.ldap.*)\n    params = {\n        'mandatory': ['uri', 'server', 'port', 'starttls', 'tls',\n                      'no_verify', 'anonymous',\n                      'accountattributename', 'activedirectory'],\n        'additional': ['binddn', 'bindpw', 'filter', 'groupclass',\n                       'auth_by_group_membership_only'],\n    }\n\n    paramvalues = {}\n\n    for param in params['mandatory']:\n        paramvalues[param] = _config(param, opts=opts)\n\n    for param in params['additional']:\n        paramvalues[param] = _config(param, mandatory=False, opts=opts)\n\n    paramvalues['anonymous'] = anonymous\n\n    # Only add binddn/bindpw to the connargs when they're set, as they're not\n    # mandatory for initializing the LDAP object, but if they're provided\n    # initially, a bind attempt will be done during the initialization to\n    # validate them\n    if paramvalues['binddn']:\n        connargs['binddn'] = paramvalues['binddn']\n        if paramvalues['bindpw']:\n            params['mandatory'].append('bindpw')\n\n    for name in params['mandatory']:\n        connargs[name] = paramvalues[name]\n\n    if not paramvalues['anonymous']:\n        if paramvalues['binddn'] and paramvalues['bindpw']:\n            # search for the user's DN to be used for the actual authentication\n            return _LDAPConnection(**connargs).ldap", "output": "Bind with binddn and bindpw only for searching LDAP\n    :param anonymous: Try binding anonymously\n    :param opts: Pass in when __opts__ is not available\n    :return: LDAPConnection object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_config(directory):\n    \"\"\"\n    \n    \"\"\"\n    default_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"config.yml\")\n    target_config_path = os.path.abspath(os.path.join(directory, 'config.yml'))\n    shutil.copy(default_config, target_config_path)\n    six.print_(\"Config file has been generated in\", target_config_path)", "output": "Generate default config file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def post_event(event,\n               channel=None,\n               username=None,\n               api_url=None,\n               hook=None):\n    '''\n    \n    '''\n    if not api_url:\n        api_url = _get_api_url()\n\n    if not hook:\n        hook = _get_hook()\n\n    if not username:\n        username = _get_username()\n\n    if not channel:\n        channel = _get_channel()\n\n    if not event:\n        log.error('message is a required option.')\n\n    log.debug('Event: %s', event)\n    log.debug('Event data: %s', event['data'])\n    message = 'tag: {0}\\r\\n'.format(event['tag'])\n    for key, value in six.iteritems(event['data']):\n        message += '{0}: {1}\\r\\n'.format(key, value)\n    result = post_message(channel,\n                          username,\n                          message,\n                          api_url,\n                          hook)\n    return bool(result)", "output": "Send an event to a Mattermost channel.\n    :param channel:     The channel name, either will work.\n    :param username:    The username of the poster.\n    :param event:       The event to send to the Mattermost channel.\n    :param api_url:     The Mattermost api url, if not specified in the configuration.\n    :param hook:        The Mattermost hook, if not specified in the configuration.\n    :return:            Boolean if message was sent successfully.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unpackage(package_):\n    '''\n    \n    '''\n    return salt.utils.msgpack.loads(package_, use_list=True,\n                                    _msgpack_module=msgpack)", "output": "Unpackages a payload", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_matrix(self, data, index, columns, dtype=None):\n        \"\"\"\n        \n        \"\"\"\n        data = prep_ndarray(data, copy=False)\n        index, columns = self._prep_index(data, index, columns)\n        data = {idx: data[:, i] for i, idx in enumerate(columns)}\n        return self._init_dict(data, index, columns, dtype)", "output": "Init self from ndarray or list of lists.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_updated(old_conf, new_conf):\n    '''\n    \n    '''\n    changed = {}\n\n    # Dirty json hacking to get parameters in the same format\n    new_conf = _json_to_unicode(salt.utils.json.loads(\n        salt.utils.json.dumps(new_conf, ensure_ascii=False)))\n    old_conf = salt.utils.json.loads(salt.utils.json.dumps(old_conf, ensure_ascii=False))\n\n    for key, value in old_conf.items():\n        oldval = six.text_type(value).lower()\n        if key in new_conf:\n            newval = six.text_type(new_conf[key]).lower()\n        if oldval == 'null' or oldval == 'none':\n            oldval = ''\n        if key in new_conf and newval != oldval:\n            changed[key] = {'old': oldval, 'new': newval}\n    return changed", "output": "Compare the API results to the current statefile data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def evaluate(dataloader_eval, metric):\n    \"\"\"\n    \"\"\"\n    metric.reset()\n    for _, seqs in enumerate(dataloader_eval):\n        input_ids, valid_len, type_ids, label = seqs\n        out = model(\n            input_ids.as_in_context(ctx), type_ids.as_in_context(ctx),\n            valid_len.astype('float32').as_in_context(ctx))\n        metric.update([label], [out])\n    metric_nm, metric_val = metric.get()\n    if not isinstance(metric_nm, list):\n        metric_nm = [metric_nm]\n        metric_val = [metric_val]\n    metric_str = 'validation metrics:' + ','.join(\n        [i + ':%.4f' for i in metric_nm])\n    logging.info(metric_str, *metric_val)", "output": "Evaluate the model on validation dataset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_json(cls, json_info):\n        \"\"\"\"\"\"\n        if json_info is None:\n            return None\n        return TrialRecord(\n            trial_id=json_info[\"trial_id\"],\n            job_id=json_info[\"job_id\"],\n            trial_status=json_info[\"status\"],\n            start_time=json_info[\"start_time\"],\n            params=json_info[\"params\"])", "output": "Build a Trial instance from a json string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def freeze(name=None, force=False, **kwargs):\n    '''\n    \n\n    '''\n    states_path = _states_path()\n\n    try:\n        os.makedirs(states_path)\n    except OSError as e:\n        msg = 'Error when trying to create the freezer storage %s: %s'\n        log.error(msg, states_path, e)\n        raise CommandExecutionError(msg % (states_path, e))\n\n    if status(name) and not force:\n        raise CommandExecutionError('The state is already present. Use '\n                                    'force parameter to overwrite.')\n    safe_kwargs = clean_kwargs(**kwargs)\n    pkgs = __salt__['pkg.list_pkgs'](**safe_kwargs)\n    repos = __salt__['pkg.list_repos'](**safe_kwargs)\n    for name, content in zip(_paths(name), (pkgs, repos)):\n        with fopen(name, 'w') as fp:\n            json.dump(content, fp)\n    return True", "output": "Save the list of package and repos in a freeze file.\n\n    As this module is build on top of the pkg module, the user can\n    send extra attributes to the underlying pkg module via kwargs.\n    This function will call ``pkg.list_pkgs`` and ``pkg.list_repos``,\n    and any additional arguments will be passed through to those\n    functions.\n\n    name\n        Name of the frozen state. Optional.\n\n    force\n        If true, overwrite the state. Optional.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' freezer.freeze\n        salt '*' freezer.freeze pre_install\n        salt '*' freezer.freeze force=True root=/chroot", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_results(self, results, trial_id):\n        \"\"\"\n        \"\"\"\n        for result in results:\n            self.logger.debug(\"Appending result: %s\" % result)\n            result[\"trial_id\"] = trial_id\n            result_record = ResultRecord.from_json(result)\n            result_record.save()", "output": "Add a list of results into db.\n\n        Args:\n            results (list): A list of json results.\n            trial_id (str): Id of the trial.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_auth(self, auth: Optional[BasicAuth]) -> None:\n        \"\"\"\"\"\"\n        if auth is None:\n            auth = self.auth\n        if auth is None:\n            return\n\n        if not isinstance(auth, helpers.BasicAuth):\n            raise TypeError('BasicAuth() tuple is required instead')\n\n        self.headers[hdrs.AUTHORIZATION] = auth.encode()", "output": "Set basic auth.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __deserialize_date(self, string):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            from dateutil.parser import parse\n            return parse(string).date()\n        except ImportError:\n            return string\n        except ValueError:\n            raise ApiException(\n                status=0,\n                reason=\"Failed to parse `{0}` into a date object\".format(string)\n            )", "output": "Deserializes string to date.\n\n        :param string: str.\n        :return: date.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def takeOrdered(self, num, key=None):\n        \"\"\"\n        \n        \"\"\"\n\n        def merge(a, b):\n            return heapq.nsmallest(num, a + b, key)\n\n        return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)", "output": "Get the N elements from an RDD ordered in ascending order or as\n        specified by the optional key function.\n\n        .. note:: this method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n        [1, 2, 3, 4, 5, 6]\n        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n        [10, 9, 7, 6, 5, 4]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_obj(fn):\n  \"\"\"\n  \"\"\"\n  position = [np.zeros(3, dtype=np.float32)]\n  normal = [np.zeros(3, dtype=np.float32)]\n  uv = [np.zeros(2, dtype=np.float32)]\n  \n  tuple2idx = OrderedDict()\n  trinagle_indices = []\n  \n  input_file = open(fn) if isinstance(fn, str) else fn\n  for line in input_file:\n    line = line.strip()\n    if not line or line[0] == '#':\n      continue\n    line = line.split(' ', 1)\n    tag = line[0]\n    if len(line) > 1:\n      line = line[1]\n    else:\n      line = ''\n    if tag == 'v':\n      position.append(np.fromstring(line, sep=' '))\n    elif tag == 'vt':\n      uv.append(np.fromstring(line, sep=' '))\n    elif tag == 'vn':\n      normal.append(np.fromstring(line, sep=' '))\n    elif tag == 'f':\n      output_face_indices = []\n      for chunk in line.split():\n        # tuple order: pos_idx, uv_idx, normal_idx\n        vt = _parse_vertex_tuple(chunk)\n        if vt not in tuple2idx:  # create a new output vertex?\n          tuple2idx[vt] = len(tuple2idx)\n        output_face_indices.append(tuple2idx[vt])\n      # generate face triangles\n      for i in range(1, len(output_face_indices)-1):\n        for vi in [0, i, i+1]:\n          trinagle_indices.append(output_face_indices[vi])\n  \n  outputs = {}\n  outputs['face'] = np.int32(trinagle_indices)\n  pos_idx, uv_idx, normal_idx = np.int32(list(tuple2idx)).T\n  if np.any(pos_idx):\n    outputs['position'] = _unify_rows(position)[pos_idx]\n  if np.any(uv_idx):\n    outputs['uv'] = _unify_rows(uv)[uv_idx]\n  if np.any(normal_idx):\n    outputs['normal'] = _unify_rows(normal)[normal_idx]\n  return outputs", "output": "Load 3d mesh form .obj' file.\n  \n  Args:\n    fn: Input file name or file-like object.\n    \n  Returns:\n    dictionary with the following keys (some of which may be missing):\n      position: np.float32, (n, 3) array, vertex positions\n      uv: np.float32, (n, 2) array, vertex uv coordinates\n      normal: np.float32, (n, 3) array, vertex uv normals\n      face: np.int32, (k*3,) traingular face indices", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_api_deployment(restApiId, stageName, stageDescription='', description='', cacheClusterEnabled=False,\n                          cacheClusterSize='0.5', variables=None,\n                          region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    try:\n        variables = dict() if variables is None else variables\n\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        deployment = conn.create_deployment(restApiId=restApiId, stageName=stageName,\n                                            stageDescription=stageDescription, description=description,\n                                            cacheClusterEnabled=cacheClusterEnabled, cacheClusterSize=cacheClusterSize,\n                                            variables=variables)\n        return {'created': True, 'deployment': _convert_datetime_str(deployment)}\n    except ClientError as e:\n        return {'created': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Creates a new API deployment.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_apigateway.create_api_deployent restApiId stagename stageDescription='' \\\\\n        description='' cacheClusterEnabled=True|False cacheClusterSize=0.5 variables='{\"name\": \"value\"}'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sub_nat(self):\n        \"\"\"\n        \n        \"\"\"\n        # GH#19124 Timedelta - datetime is not in general well-defined.\n        # We make an exception for pd.NaT, which in this case quacks\n        # like a timedelta.\n        # For datetime64 dtypes by convention we treat NaT as a datetime, so\n        # this subtraction returns a timedelta64 dtype.\n        # For period dtype, timedelta64 is a close-enough return dtype.\n        result = np.zeros(len(self), dtype=np.int64)\n        result.fill(iNaT)\n        return result.view('timedelta64[ns]')", "output": "Subtract pd.NaT from self", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def collection_set_options(collection_name, options, **kwargs):\n    '''\n    \n    '''\n\n    for option in list(options.keys()):\n        if option not in CREATION_ONLY_OPTION:\n            raise ValueError('Option '+option+' can\\'t be modified after collection creation.')\n\n    options_string = _validate_collection_options(options)\n\n    _query('admin/collections?action=MODIFYCOLLECTION&wt=json&collection='+collection_name+options_string, **kwargs)", "output": "Change collection options\n\n    Additional parameters (kwargs) may be passed, they will be proxied to http.query\n\n    Note that not every parameter can be changed after collection creation\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' solrcloud.collection_set_options collection_name options={\"replicationFactor\":4}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_permissions(self, filename):\n        '''\n        \n        '''\n        if salt.utils.platform.is_windows():\n            return True\n\n        # After we've ascertained we're not on windows\n        groups = salt.utils.user.get_gid_list(self.opts['user'], include_default=False)\n        fmode = os.stat(filename)\n\n        if stat.S_IWOTH & fmode.st_mode:\n            # don't allow others to write to the file\n            return False\n\n        if stat.S_IWGRP & fmode.st_mode:\n            # if the group has write access only allow with permissive_pki_access\n            if not self.opts.get('permissive_pki_access', False):\n                return False\n            elif os.getuid() == 0 and fmode.st_gid not in groups:\n                # if salt is root it has to be in the group that has write access\n                # this gives the group 'permission' to have write access\n                return False\n\n        return True", "output": "Check if the specified filename has correct permissions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cleanup_failed_attacks(self):\n    \"\"\"\"\"\"\n    print_header('Cleaning up failed attacks')\n    attacks_to_replace = {}\n    self.attack_work.read_all_from_datastore()\n    failed_submissions = set()\n    error_msg = set()\n    for k, v in iteritems(self.attack_work.work):\n      if v['error'] is not None:\n        attacks_to_replace[k] = dict(v)\n        failed_submissions.add(v['submission_id'])\n        error_msg.add(v['error'])\n        attacks_to_replace[k].update(\n            {\n                'claimed_worker_id': None,\n                'claimed_worker_start_time': None,\n                'is_completed': False,\n                'error': None,\n                'elapsed_time': None,\n            })\n    self.attack_work.replace_work(attacks_to_replace)\n    print('Affected submissions:')\n    print(' '.join(sorted(failed_submissions)))\n    print('Error messages:')\n    print(' '.join(sorted(error_msg)))\n    print('')\n    inp = input_str('Are you sure? (type \"yes\" without quotes to confirm): ')\n    if inp != 'yes':\n      return\n    self.attack_work.write_all_to_datastore()\n    print('Work cleaned up')", "output": "Cleans up data of failed attacks.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_user(self, username, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if username in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'username'.\")\n        return self.transport.perform_request(\n            \"DELETE\", _make_path(\"_security\", \"user\", username), params=params\n        )", "output": "`<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delete-user.html>`_\n\n        :arg username: username\n        :arg refresh: If `true` (the default) then refresh the affected shards\n            to make this operation visible to search, if `wait_for` then wait\n            for a refresh to make this operation visible to search, if `false`\n            then do nothing with refreshes., valid choices are: 'true', 'false',\n            'wait_for'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait_for_state(vmid, state, timeout=300):\n    '''\n    \n    '''\n    start_time = time.time()\n    node = get_vm_status(vmid=vmid)\n    if not node:\n        log.error('wait_for_state: No VM retrieved based on given criteria.')\n        raise SaltCloudExecutionFailure\n\n    while True:\n        if node['status'] == state:\n            log.debug('Host %s is now in \"%s\" state!', node['name'], state)\n            return True\n        time.sleep(1)\n        if time.time() - start_time > timeout:\n            log.debug('Timeout reached while waiting for %s to become %s',\n                      node['name'], state)\n            return False\n        node = get_vm_status(vmid=vmid)\n        log.debug('State for %s is: \"%s\" instead of \"%s\"',\n                  node['name'], node['status'], state)", "output": "Wait until a specific state has been reached on a node", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_ikepolicy(self, name, **kwargs):\n        '''\n        \n        '''\n        body = {'name': name}\n        if 'phase1_negotiation_mode' in kwargs:\n            body['phase1_negotiation_mode'] = kwargs['phase1_negotiation_mode']\n        if 'auth_algorithm' in kwargs:\n            body['auth_algorithm'] = kwargs['auth_algorithm']\n        if 'encryption_algorithm' in kwargs:\n            body['encryption_algorithm'] = kwargs['encryption_algorithm']\n        if 'pfs' in kwargs:\n            body['pfs'] = kwargs['pfs']\n        if 'ike_version' in kwargs:\n            body['ike_version'] = kwargs['ike_version']\n        if 'units' in kwargs:\n            body['lifetime'] = {'units': kwargs['units']}\n        if 'value' in kwargs:\n            if 'lifetime' not in body:\n                body['lifetime'] = {}\n            body['lifetime']['value'] = kwargs['value']\n        return self.network_conn.create_ikepolicy(body={'ikepolicy': body})", "output": "Creates a new IKEPolicy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe(self):\n        \"\"\"\n        \n        \"\"\"\n        counts = self.value_counts(dropna=False)\n        freqs = counts / float(counts.sum())\n\n        from pandas.core.reshape.concat import concat\n        result = concat([counts, freqs], axis=1)\n        result.columns = ['counts', 'freqs']\n        result.index.name = 'categories'\n\n        return result", "output": "Describes this Categorical\n\n        Returns\n        -------\n        description: `DataFrame`\n            A dataframe with frequency and counts by category.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_certificate_signing_request(self, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_certificate_signing_request_with_http_info(body, **kwargs)\n        else:\n            (data) = self.create_certificate_signing_request_with_http_info(body, **kwargs)\n            return data", "output": "create a CertificateSigningRequest\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_certificate_signing_request(body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param V1beta1CertificateSigningRequest body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1beta1CertificateSigningRequest\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def inter_data_operation(self, axis, func, other):\n        \"\"\"\n        \"\"\"\n        if axis:\n            partitions = self.row_partitions\n            other_partitions = other.row_partitions\n        else:\n            partitions = self.column_partitions\n            other_partitions = other.column_partitions\n        func = self.preprocess_func(func)\n        result = np.array(\n            [\n                partitions[i].apply(\n                    func,\n                    num_splits=self._compute_num_partitions(),\n                    other_axis_partition=other_partitions[i],\n                )\n                for i in range(len(partitions))\n            ]\n        )\n        return self.__constructor__(result) if axis else self.__constructor__(result.T)", "output": "Apply a function that requires two BaseFrameManager objects.\n\n        Args:\n            axis: The axis to apply the function over (0 - rows, 1 - columns)\n            func: The function to apply\n            other: The other BaseFrameManager object to apply func to.\n\n        Returns:\n            A new BaseFrameManager object, the type of object that called this.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _symlink_check(name, target, force, user, group, win_owner):\n    '''\n    \n    '''\n    changes = {}\n    if not os.path.exists(name) and not __salt__['file.is_link'](name):\n        changes['new'] = name\n        return None, 'Symlink {0} to {1} is set for creation'.format(\n            name, target\n        ), changes\n    if __salt__['file.is_link'](name):\n        if __salt__['file.readlink'](name) != target:\n            changes['change'] = name\n            return None, 'Link {0} target is set to be changed to {1}'.format(\n                name, target\n            ), changes\n        else:\n            result = True\n            msg = 'The symlink {0} is present'.format(name)\n            if not _check_symlink_ownership(name, user, group, win_owner):\n                result = None\n                changes['ownership'] = '{0}:{1}'.format(*_get_symlink_ownership(name))\n                msg += (\n                    ', but the ownership of the symlink would be changed '\n                    'from {2}:{3} to {0}:{1}'\n                ).format(user, group, *_get_symlink_ownership(name))\n            return result, msg, changes\n    else:\n        if force:\n            return None, ('The file or directory {0} is set for removal to '\n                          'make way for a new symlink targeting {1}'\n                          .format(name, target)), changes\n        return False, ('File or directory exists where the symlink {0} '\n                       'should be. Did you mean to use force?'.format(name)), changes", "output": "Check the symlink function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_date(df:DataFrame, date_field:str):\n    \"\"\n    field_dtype = df[date_field].dtype\n    if isinstance(field_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        field_dtype = np.datetime64\n    if not np.issubdtype(field_dtype, np.datetime64):\n        df[date_field] = pd.to_datetime(df[date_field], infer_datetime_format=True)", "output": "Make sure `df[field_name]` is of the right date type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def classify(self, dataset, missing_value_action='auto'):\n        \"\"\"\n        \n        \"\"\"\n        if (missing_value_action == 'auto'):\n            missing_value_action = select_default_missing_value_policy(self, 'classify')\n\n        # Low latency path\n        if isinstance(dataset, list):\n            return self.__proxy__.fast_classify(dataset, missing_value_action)\n        if isinstance(dataset, dict):\n            return self.__proxy__.fast_classify([dataset], missing_value_action)\n\n        _raise_error_if_not_sframe(dataset, \"dataset\")\n        return self.__proxy__.classify(dataset, missing_value_action)", "output": "Return predictions for ``dataset``, using the trained supervised_learning\n        model. Predictions are generated as class labels (0 or\n        1).\n\n        Parameters\n        ----------\n        dataset: SFrame\n            Dataset of new observations. Must include columns with the same\n            names as the features used for model training, but does not require\n            a target column. Additional columns are ignored.\n\n        missing_value_action: str, optional\n            Action to perform when missing values are encountered. This can be\n            one of:\n\n            - 'auto': Choose model dependent missing value action\n            - 'impute': Proceed with evaluation by filling in the missing\n              values with the mean of the training data. Missing\n              values are also imputed if an entire column of data is\n              missing during evaluation.\n            - 'error': Do not proceed with prediction and terminate with\n              an error message.\n        Returns\n        -------\n        out : SFrame\n            An SFrame with model predictions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_password(name, password):\n    '''\n    \n    '''\n    if __grains__.get('os', '') == 'FreeBSD':\n        cmd = ['pw', 'user', 'mod', name, '-H', '0']\n        stdin = password\n    else:\n        cmd = ['usermod', '-p', password, name]\n        stdin = None\n    __salt__['cmd.run'](cmd,\n                        stdin=stdin,\n                        output_loglevel='quiet',\n                        python_shell=False)\n    return info(name)['passwd'] == password", "output": "Set the password for a named user. The password must be a properly defined\n    hash. The password hash can be generated with this command:\n\n    ``python -c \"import crypt; print crypt.crypt('password', ciphersalt)\"``\n\n    .. note::\n        When constructing the ``ciphersalt`` string, you must escape any dollar\n        signs, to avoid them being interpolated by the shell.\n\n    ``'password'`` is, of course, the password for which you want to generate\n    a hash.\n\n    ``ciphersalt`` is a combination of a cipher identifier, an optional number\n    of rounds, and the cryptographic salt. The arrangement and format of these\n    fields depends on the cipher and which flavor of BSD you are using. For\n    more information on this, see the manpage for ``crpyt(3)``. On NetBSD,\n    additional information is available in ``passwd.conf(5)``.\n\n    It is important to make sure that a supported cipher is used.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' shadow.set_password someuser '$1$UYCIxa628.9qXjpQCjM4a..'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_building (self, main_target_instance):\n        \"\"\" \n        \"\"\"\n        assert isinstance(main_target_instance, MainTarget)\n        if id(main_target_instance) in self.targets_being_built_:\n            names = []\n            for t in self.targets_being_built_.values() + [main_target_instance]:\n                names.append (t.full_name())\n\n            get_manager().errors()(\"Recursion in main target references\\n\")\n\n        self.targets_being_built_[id(main_target_instance)] = main_target_instance", "output": "Helper rules to detect cycles in main target references.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sort(self, column, order=Qt.DescendingOrder):\r\n        \"\"\"\"\"\"\r\n        if column == 0:\r\n            self.breakpoints.sort(\r\n                key=lambda breakpoint: breakpoint[1])\r\n            self.breakpoints.sort(\r\n                key=lambda breakpoint: osp.basename(breakpoint[0]))\r\n        elif column == 1:\r\n            pass\r\n        elif column == 2:\r\n            pass\r\n        elif column == 3:\r\n            pass\r\n        self.reset()", "output": "Overriding sort method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_default_names(data):\n    \"\"\"\"\"\"\n    if com._all_not_none(*data.index.names):\n        nms = data.index.names\n        if len(nms) == 1 and data.index.name == 'index':\n            warnings.warn(\"Index name of 'index' is not round-trippable\")\n        elif len(nms) > 1 and any(x.startswith('level_') for x in nms):\n            warnings.warn(\"Index names beginning with 'level_' are not \"\n                          \"round-trippable\")\n        return data\n\n    data = data.copy()\n    if data.index.nlevels > 1:\n        names = [name if name is not None else 'level_{}'.format(i)\n                 for i, name in enumerate(data.index.names)]\n        data.index.names = names\n    else:\n        data.index.name = data.index.name or 'index'\n    return data", "output": "Sets index names to 'index' for regular, or 'level_x' for Multi", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close(self, code: int = None, reason: str = None) -> None:\n        \"\"\"\"\"\"\n        if not self.server_terminated:\n            if not self.stream.closed():\n                if code is None and reason is not None:\n                    code = 1000  # \"normal closure\" status code\n                if code is None:\n                    close_data = b\"\"\n                else:\n                    close_data = struct.pack(\">H\", code)\n                if reason is not None:\n                    close_data += utf8(reason)\n                try:\n                    self._write_frame(True, 0x8, close_data)\n                except StreamClosedError:\n                    self._abort()\n            self.server_terminated = True\n        if self.client_terminated:\n            if self._waiting is not None:\n                self.stream.io_loop.remove_timeout(self._waiting)\n                self._waiting = None\n            self.stream.close()\n        elif self._waiting is None:\n            # Give the client a few seconds to complete a clean shutdown,\n            # otherwise just close the connection.\n            self._waiting = self.stream.io_loop.add_timeout(\n                self.stream.io_loop.time() + 5, self._abort\n            )", "output": "Closes the WebSocket connection.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_response_time(chatbot, statement='Hello'):\n    \"\"\"\n    \n    \"\"\"\n    import time\n\n    start_time = time.time()\n\n    chatbot.get_response(statement)\n\n    return time.time() - start_time", "output": "Returns the amount of time taken for a given\n    chat bot to return a response.\n\n    :param chatbot: A chat bot instance.\n    :type chatbot: ChatBot\n\n    :returns: The response time in seconds.\n    :rtype: float", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_idx_rect(index_list):\r\n    \"\"\"\"\"\"\r\n    rows, cols = list(zip(*[(i.row(), i.column()) for i in index_list]))\r\n    return ( min(rows), max(rows), min(cols), max(cols) )", "output": "Extract the boundaries from a list of indexes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _upstart_disable(name):\n    '''\n    \n    '''\n    if _upstart_is_disabled(name):\n        return _upstart_is_disabled(name)\n    override = '/etc/init/{0}.override'.format(name)\n    with salt.utils.files.fopen(override, 'a') as ofile:\n        ofile.write(salt.utils.stringutils.to_str('manual\\n'))\n    return _upstart_is_disabled(name)", "output": "Disable an Upstart service.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _finish(self):\n        \"\"\"\n        \n        \"\"\"\n        if self._process.returncode is None:\n            self._process.stdin.flush()\n            self._process.stdin.close()\n            self._process.wait()\n            self.closed = True", "output": "Closes and waits for subprocess to exit.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_close_function(self, func):\r\n        \"\"\"\"\"\"\r\n        state = func is not None\r\n        if state:\r\n            self.sig_close_tab.connect(func)\r\n        try:\r\n            # Assuming Qt >= 4.5\r\n            QTabWidget.setTabsClosable(self, state)\r\n            self.tabCloseRequested.connect(func)\r\n        except AttributeError:\r\n            # Workaround for Qt < 4.5\r\n            close_button = create_toolbutton(self, triggered=func,\r\n                                             icon=ima.icon('fileclose'),\r\n                                             tip=_(\"Close current tab\"))\r\n            self.setCornerWidget(close_button if state else None)", "output": "Setting Tabs close function\r\n        None -> tabs are not closable", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ssh_file(opts, dest_path, contents=None, kwargs=None, local_file=None):\n    '''\n    \n    '''\n    if opts.get('file_transport', 'sftp') == 'sftp':\n        return sftp_file(dest_path, contents, kwargs, local_file)\n    return scp_file(dest_path, contents, kwargs, local_file)", "output": "Copies a file to the remote SSH target using either sftp or scp, as\n    configured.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_absolute_resample__r2(X, y, model_generator, method_name, num_fcounts=11):\n    \"\"\" \n    \"\"\"\n    return __run_measure(measures.remove_resample, X, y, model_generator, method_name, 0, num_fcounts, sklearn.metrics.r2_score)", "output": "Remove Absolute (resample)\n    xlabel = \"Max fraction of features removed\"\n    ylabel = \"1 - R^2\"\n    transform = \"one_minus\"\n    sort_order = 15", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _compute_signature(parameters, access_key_secret, method, path):\n    '''\n    \n    '''\n    parameters['signature_method'] = 'HmacSHA256'\n\n    string_to_sign = '{0}\\n{1}\\n'.format(method.upper(), path)\n\n    keys = sorted(parameters.keys())\n    pairs = []\n    for key in keys:\n        val = six.text_type(parameters[key]).encode('utf-8')\n        pairs.append(_quote(key, safe='') + '=' + _quote(val, safe='-_~'))\n    qs = '&'.join(pairs)\n    string_to_sign += qs\n\n    h = hmac.new(access_key_secret, digestmod=sha256)\n    h.update(string_to_sign)\n\n    signature = base64.b64encode(h.digest()).strip()\n\n    return signature", "output": "Generate an API request signature. Detailed document can be found at:\n\n    https://docs.qingcloud.com/api/common/signature.html", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_trading_control(self, control):\n        \"\"\"\n        \n        \"\"\"\n        if self.initialized:\n            raise RegisterTradingControlPostInit()\n        self.trading_controls.append(control)", "output": "Register a new TradingControl to be checked prior to order calls.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bbox_rotate(bbox, angle, rows, cols, interpolation):\n    \"\"\"\n    \"\"\"\n    scale = cols / float(rows)\n    x = np.array([bbox[0], bbox[2], bbox[2], bbox[0]])\n    y = np.array([bbox[1], bbox[1], bbox[3], bbox[3]])\n    x = x - 0.5\n    y = y - 0.5\n    angle = np.deg2rad(angle)\n    x_t = (np.cos(angle) * x * scale + np.sin(angle) * y) / scale\n    y_t = (-np.sin(angle) * x * scale + np.cos(angle) * y)\n    x_t = x_t + 0.5\n    y_t = y_t + 0.5\n    return [min(x_t), min(y_t), max(x_t), max(y_t)]", "output": "Rotates a bounding box by angle degrees\n\n    Args:\n        bbox (tuple): A tuple (x_min, y_min, x_max, y_max).\n        angle (int): Angle of rotation in degrees\n        rows (int): Image rows.\n        cols (int): Image cols.\n        interpolation (int): interpolation method.\n\n        return a tuple (x_min, y_min, x_max, y_max)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy_figure(self):\n        \"\"\"\"\"\"\n        if self.fmt in ['image/png', 'image/jpeg']:\n            qpixmap = QPixmap()\n            qpixmap.loadFromData(self.fig, self.fmt.upper())\n            QApplication.clipboard().setImage(qpixmap.toImage())\n        elif self.fmt == 'image/svg+xml':\n            svg_to_clipboard(self.fig)\n        else:\n            return\n\n        self.blink_figure()", "output": "Copy figure to clipboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_full_grads(self, train_data):\n        \"\"\"\n        \"\"\"\n        param_names = self._exec_group.param_names\n        arg, aux = self.get_params()\n        self._mod_aux.set_params(arg_params=arg, aux_params=aux)\n        train_data.reset()\n        nbatch = 0\n        padding = 0\n        for batch in train_data:\n            self._mod_aux.forward(batch, is_train=True)\n            self._mod_aux.backward()\n            nbatch += 1\n            for ctx in range(self._ctx_len):\n                for index, name in enumerate(param_names):\n                    grads = self._mod_aux._exec_group.grad_arrays[index][ctx]\n                    self._param_dict[ctx][name] = mx.nd.broadcast_add(self._param_dict[ctx][name], grads, axis=0)\n            padding = batch.pad\n\n        true_num_batch = nbatch - padding / train_data.batch_size\n        for name in param_names:\n            grad_list = []\n            for i in range(self._ctx_len):\n                self._param_dict[i][name] /= true_num_batch\n                grad_list.append(self._param_dict[i][name])\n            if self._kvstore:\n                # If in distributed mode, push a list of gradients from each worker/device to the KVStore\n                self._accumulate_kvstore(name, grad_list)", "output": "Computes the gradients over all data w.r.t weights of past\n        m epochs. For distributed env, it will accumulate full grads in the kvstore.\n\n        Parameters\n        ----------\n        train_data: DataIter\n            Train data iterator", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_cell(self, column_family_id, column, value, timestamp=None, state=None):\n        \"\"\"\n        \"\"\"\n        column = _to_bytes(column)\n        if isinstance(value, six.integer_types):\n            value = _PACK_I64(value)\n        value = _to_bytes(value)\n        if timestamp is None:\n            # Use -1 for current Bigtable server time.\n            timestamp_micros = -1\n        else:\n            timestamp_micros = _microseconds_from_datetime(timestamp)\n            # Truncate to millisecond granularity.\n            timestamp_micros -= timestamp_micros % 1000\n\n        mutation_val = data_v2_pb2.Mutation.SetCell(\n            family_name=column_family_id,\n            column_qualifier=column,\n            timestamp_micros=timestamp_micros,\n            value=value,\n        )\n        mutation_pb = data_v2_pb2.Mutation(set_cell=mutation_val)\n        self._get_mutations(state).append(mutation_pb)", "output": "Helper for :meth:`set_cell`\n\n        Adds a mutation to set the value in a specific cell.\n\n        ``state`` is unused by :class:`DirectRow` but is used by\n        subclasses.\n\n        :type column_family_id: str\n        :param column_family_id: The column family that contains the column.\n                                 Must be of the form\n                                 ``[_a-zA-Z0-9][-_.a-zA-Z0-9]*``.\n\n        :type column: bytes\n        :param column: The column within the column family where the cell\n                       is located.\n\n        :type value: bytes or :class:`int`\n        :param value: The value to set in the cell. If an integer is used,\n                      will be interpreted as a 64-bit big-endian signed\n                      integer (8 bytes).\n\n        :type timestamp: :class:`datetime.datetime`\n        :param timestamp: (Optional) The timestamp of the operation.\n\n        :type state: bool\n        :param state: (Optional) The state that is passed along to\n                      :meth:`_get_mutations`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def splits_and_paths(self, data_dir):\n    \"\"\"\"\"\"\n    filepath_fns = {\n        problem.DatasetSplit.TRAIN: self.training_filepaths,\n        problem.DatasetSplit.EVAL: self.dev_filepaths,\n        problem.DatasetSplit.TEST: self.test_filepaths,\n    }\n\n    def append_epoch(paths):\n      return [\n          \"{}.{}\".format(path, self.current_epoch)\n          for path in paths\n      ]\n\n    # We set shuffled=True as we don't want to shuffle on disk later.\n    return [\n        (split[\"split\"], append_epoch(filepath_fns[split[\"split\"]](\n            data_dir, split[\"shards\"], shuffled=True\n        )))\n        for split in self.dataset_splits\n    ]", "output": "List of pairs (split, paths) for the current epoch.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mk_token(**load):\n    \n    '''\n    # This will hang if the master daemon is not running.\n    netapi = salt.netapi.NetapiClient(__opts__)\n    if not netapi._is_master_running():\n        raise salt.exceptions.SaltDaemonNotRunning(\n                'Salt Master must be running.')\n\n    auth = salt.auth.Resolver(__opts__)\n    return auth.mk_token(load)", "output": "r'''\n    Create an eauth token using provided credentials\n\n    Non-root users may specify an expiration date -- if allowed via the\n    :conf_master:`token_expire_user_override` setting -- by passing an\n    additional ``token_expire`` param. This overrides the\n    :conf_master:`token_expire` setting of the same name in the Master config\n    and is how long a token should live in seconds.\n\n    CLI Example:\n\n    .. code-block:: shell\n\n        salt-run auth.mk_token username=saltdev password=saltdev eauth=auto\n\n        # Create a token valid for three years.\n        salt-run auth.mk_token username=saltdev password=saltdev eauth=auto \\\n            token_expire=94670856\n\n        # Calculate the number of seconds using expr.\n        salt-run auth.mk_token username=saltdev password=saltdev eauth=auto \\\n            token_expire=$(expr \\( 365 \\* 24 \\* 60 \\* 60 \\) \\* 3)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_template(self, template_dict, original_template_path, built_artifacts):\n        \"\"\"\n        \n        \"\"\"\n\n        original_dir = os.path.dirname(original_template_path)\n\n        for logical_id, resource in template_dict.get(\"Resources\", {}).items():\n\n            if logical_id not in built_artifacts:\n                # this resource was not built. So skip it\n                continue\n\n            # Artifacts are written relative to the template because it makes the template portable\n            #   Ex: A CI/CD pipeline build stage could zip the output folder and pass to a\n            #   package stage running on a different machine\n            artifact_relative_path = os.path.relpath(built_artifacts[logical_id], original_dir)\n\n            resource_type = resource.get(\"Type\")\n            properties = resource.setdefault(\"Properties\", {})\n            if resource_type == \"AWS::Serverless::Function\":\n                properties[\"CodeUri\"] = artifact_relative_path\n\n            if resource_type == \"AWS::Lambda::Function\":\n                properties[\"Code\"] = artifact_relative_path\n\n        return template_dict", "output": "Given the path to built artifacts, update the template to point appropriate resource CodeUris to the artifacts\n        folder\n\n        Parameters\n        ----------\n        template_dict\n        original_template_path : str\n            Path where the template file will be written to\n\n        built_artifacts : dict\n            Map of LogicalId of a resource to the path where the the built artifacts for this resource lives\n\n        Returns\n        -------\n        dict\n            Updated template", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_wheel_modern(ireq, output_dir, finder, wheel_cache, kwargs):\n    \"\"\"\n    \"\"\"\n    kwargs.update({\"progress_bar\": \"off\", \"build_isolation\": False})\n    with pip_shims.RequirementTracker() as req_tracker:\n        if req_tracker:\n            kwargs[\"req_tracker\"] = req_tracker\n        preparer = pip_shims.RequirementPreparer(**kwargs)\n        builder = pip_shims.WheelBuilder(finder, preparer, wheel_cache)\n        return builder._build_one(ireq, output_dir)", "output": "Build a wheel.\n\n    * ireq: The InstallRequirement object to build\n    * output_dir: The directory to build the wheel in.\n    * finder: pip's internal Finder object to find the source out of ireq.\n    * kwargs: Various keyword arguments from `_prepare_wheel_building_kwargs`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def recoverMemory(buffer, size):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlRecoverMemory(buffer, size)\n    if ret is None:raise treeError('xmlRecoverMemory() failed')\n    return xmlDoc(_obj=ret)", "output": "parse an XML in-memory block and build a tree. In the case\n      the document is not Well Formed, an attempt to build a tree\n       is tried anyway", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_namespaced_ingress(self, name, namespace, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_ingress_with_http_info(name, namespace, **kwargs)\n        else:\n            (data) = self.read_namespaced_ingress_with_http_info(name, namespace, **kwargs)\n            return data", "output": "read the specified Ingress\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_ingress(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Ingress (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'. Deprecated. Planned for removal in 1.18.\n        :param bool export: Should this value be exported.  Export strips fields that a user can not specify. Deprecated. Planned for removal in 1.18.\n        :return: NetworkingV1beta1Ingress\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encoder(nef, z_dim, batch_size, no_bias=True, fix_gamma=True, eps=1e-5 + 1e-12):\n    '''\n    '''\n    BatchNorm = mx.sym.BatchNorm\n\n    data = mx.sym.Variable('data')\n\n    e1 = mx.sym.Convolution(data, name='enc1', kernel=(5,5), stride=(2,2), pad=(2,2), num_filter=nef, no_bias=no_bias)\n    ebn1 = BatchNorm(e1, name='encbn1', fix_gamma=fix_gamma, eps=eps)\n    eact1 = mx.sym.LeakyReLU(ebn1, name='encact1', act_type='leaky', slope=0.2)\n\n    e2 = mx.sym.Convolution(eact1, name='enc2', kernel=(5,5), stride=(2,2), pad=(2,2), num_filter=nef*2, no_bias=no_bias)\n    ebn2 = BatchNorm(e2, name='encbn2', fix_gamma=fix_gamma, eps=eps)\n    eact2 = mx.sym.LeakyReLU(ebn2, name='encact2', act_type='leaky', slope=0.2)\n\n    e3 = mx.sym.Convolution(eact2, name='enc3', kernel=(5,5), stride=(2,2), pad=(2,2), num_filter=nef*4, no_bias=no_bias)\n    ebn3 = BatchNorm(e3, name='encbn3', fix_gamma=fix_gamma, eps=eps)\n    eact3 = mx.sym.LeakyReLU(ebn3, name='encact3', act_type='leaky', slope=0.2)\n\n    e4 = mx.sym.Convolution(eact3, name='enc4', kernel=(5,5), stride=(2,2), pad=(2,2), num_filter=nef*8, no_bias=no_bias)\n    ebn4 = BatchNorm(e4, name='encbn4', fix_gamma=fix_gamma, eps=eps)\n    eact4 = mx.sym.LeakyReLU(ebn4, name='encact4', act_type='leaky', slope=0.2)\n\n    eact4 = mx.sym.Flatten(eact4)\n\n    z_mu = mx.sym.FullyConnected(eact4, num_hidden=z_dim, name=\"enc_mu\")\n    z_lv = mx.sym.FullyConnected(eact4, num_hidden=z_dim, name=\"enc_lv\")\n\n    z = z_mu + mx.symbol.broadcast_mul(mx.symbol.exp(0.5*z_lv),mx.symbol.random_normal(loc=0, scale=1,shape=(batch_size,z_dim)))\n\n    return z_mu, z_lv, z", "output": "The encoder is a CNN which takes 32x32 image as input\n    generates the 100 dimensional shape embedding as a sample from normal distribution\n    using predicted meand and variance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def jumpTo(self, bytes):\n        \"\"\"\"\"\"\n        newPosition = self[self.position:].find(bytes)\n        if newPosition > -1:\n            # XXX: This is ugly, but I can't see a nicer way to fix this.\n            if self._position == -1:\n                self._position = 0\n            self._position += (newPosition + len(bytes) - 1)\n            return True\n        else:\n            raise StopIteration", "output": "Look for the next sequence of bytes matching a given sequence. If\n        a match is found advance the position to the last byte of the match", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_lr(self, step, nowarn=False):\n        \"\"\"\n        \n        \"\"\"\n        if self.t_total < 0:\n            return 1.\n        progress = float(step) / self.t_total\n        ret = self.get_lr_(progress)\n        # warning for exceeding t_total (only active with warmup_linear\n        if not nowarn and self.warn_t_total and progress > 1. and progress > self.warned_for_t_total_at_progress:\n            logger.warning(\n                \"Training beyond specified 't_total'. Learning rate multiplier set to {}. Please set 't_total' of {} correctly.\"\n                    .format(ret, self.__class__.__name__))\n            self.warned_for_t_total_at_progress = progress\n        # end warning\n        return ret", "output": ":param step:    which of t_total steps we're on\n        :param nowarn:  set to True to suppress warning regarding training beyond specified 't_total' steps\n        :return:        learning rate multiplier for current update", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _raise_error(self, status_code, raw_data):\n        \"\"\"  \"\"\"\n        error_message = raw_data\n        additional_info = None\n        try:\n            if raw_data:\n                additional_info = json.loads(raw_data)\n                error_message = additional_info.get('error', error_message)\n                if isinstance(error_message, dict) and 'type' in error_message:\n                    error_message = error_message['type']\n        except (ValueError, TypeError) as err:\n            logger.warning('Undecodable raw error response from server: %s', err)\n\n        raise HTTP_EXCEPTIONS.get(status_code, TransportError)(status_code, error_message, additional_info)", "output": "Locate appropriate exception and raise it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_entity(self, entity, default=None):\n        \"\"\"\n        \"\"\"\n        self._ensure_loaded()\n        return self.entities.get(str(entity), default)", "output": "Gets an entity object from the ACL.\n\n        :type entity: :class:`_ACLEntity` or string\n        :param entity: The entity to get lookup in the ACL.\n\n        :type default: anything\n        :param default: This value will be returned if the entity\n                        doesn't exist.\n\n        :rtype: :class:`_ACLEntity`\n        :returns: The corresponding entity or the value provided\n                  to ``default``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_or_edit_conditional_breakpoint(self):\r\n        \"\"\"\"\"\"\r\n        if self.data:\r\n            editor = self.get_current_editor()\r\n            editor.debugger.toogle_breakpoint(edit_condition=True)", "output": "Set conditional breakpoint", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def export(zone, path=None):\n    '''\n    \n    '''\n    ret = {'status': True}\n\n    # export zone\n    res = __salt__['cmd.run_all']('zonecfg -z {zone} export{path}'.format(\n        zone=zone,\n        path=' -f {0}'.format(path) if path else '',\n    ))\n    ret['status'] = res['retcode'] == 0\n    ret['message'] = res['stdout'] if ret['status'] else res['stderr']\n    if ret['message'] == '':\n        del ret['message']\n    else:\n        ret['message'] = _clean_message(ret['message'])\n\n    return ret", "output": "Export the configuration from memory to stable storage.\n\n    zone : string\n        name of zone\n    path : string\n        path of file to export to\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zonecfg.export epyon\n        salt '*' zonecfg.export epyon /zones/epyon.cfg", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def num_feature(self):\n        \"\"\"\n        \"\"\"\n        if self.handle is not None:\n            ret = ctypes.c_int()\n            _safe_call(_LIB.LGBM_DatasetGetNumFeature(self.handle,\n                                                      ctypes.byref(ret)))\n            return ret.value\n        else:\n            raise LightGBMError(\"Cannot get num_feature before construct dataset\")", "output": "Get the number of columns (features) in the Dataset.\n\n        Returns\n        -------\n        number_of_columns : int\n            The number of columns (features) in the Dataset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def name(device, partition, name):\n    '''\n    \n    '''\n    _validate_device(device)\n\n    try:\n        int(partition)\n    except Exception:\n        raise CommandExecutionError(\n            'Invalid partition passed to partition.name'\n        )\n\n    valid = string.ascii_letters + string.digits + ' _-'\n    for letter in name:\n        if letter not in valid:\n            raise CommandExecutionError(\n                'Invalid characters passed to partition.name'\n            )\n\n    cmd = '''parted -m -s {0} name {1} \"'{2}'\"'''.format(device, partition, name)\n    out = __salt__['cmd.run'](cmd).splitlines()\n    return out", "output": "Set the name of partition to name. This option works only on Mac, PC98, and\n    GPT disklabels. The name can be placed in quotes, if necessary.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' partition.name /dev/sda 1 'My Documents'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def consul_fetch(client, path):\n    '''\n    \n    '''\n    # Unless the root path is blank, it needs a trailing slash for\n    # the kv get from Consul to work as expected\n    return client.kv.get('' if not path else path.rstrip('/') + '/', recurse=True)", "output": "Query consul for all keys/values within base path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def diagonal_neural_gpu(inputs, hparams, name=None):\n  \"\"\"\"\"\"\n  with tf.variable_scope(name, \"diagonal_neural_gpu\"):\n\n    def step(state_tup, inp):\n      \"\"\"Single step of the improved Neural GPU.\"\"\"\n      state, _ = state_tup\n      x = state\n      for layer in range(hparams.num_hidden_layers):\n        x, new_loss = common_layers.diagonal_conv_gru(\n            x, (hparams.kernel_height, hparams.kernel_width),\n            hparams.hidden_size,\n            dropout=hparams.dropout,\n            name=\"dcgru_%d\" % layer)\n      # Padding input is zeroed-out in the modality, we check this by summing.\n      padding_inp = tf.less(tf.reduce_sum(tf.abs(inp), axis=[1, 2]), 0.00001)\n      new_state = tf.where(padding_inp, state, x)  # No-op where inp is padding.\n      return new_state, new_loss\n\n    final_state, losses = tf.scan(\n        step,\n        tf.transpose(inputs, [1, 0, 2, 3]),\n        initializer=(inputs, tf.constant(0.0)),\n        parallel_iterations=1,\n        swap_memory=True)\n    return final_state[0, :, :, :, :], 2.0 * tf.reduce_mean(losses)", "output": "Improved Neural GPU as in https://arxiv.org/abs/1702.08727.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_row_sep(self):\r\n        \"\"\"\"\"\"\r\n        if self.eol_btn.isChecked():\r\n            return u\"\\n\"\r\n        return to_text_string(self.line_edt_row.text())", "output": "Return the row separator", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_nni_variable(code):\n    \"\"\"\n    \"\"\"\n    name, call = parse_annotation_function(code, 'variable')\n\n    assert len(call.args) == 1, 'nni.variable contains more than one arguments'\n    arg = call.args[0]\n    assert type(arg) is ast.Call, 'Value of nni.variable is not a function call'\n    assert type(arg.func) is ast.Attribute, 'nni.variable value is not a NNI function'\n    assert type(arg.func.value) is ast.Name, 'nni.variable value is not a NNI function'\n    assert arg.func.value.id == 'nni', 'nni.variable value is not a NNI function'\n\n    name_str = astor.to_source(name).strip()\n    keyword_arg = ast.keyword(arg='name', value=ast.Str(s=name_str))\n    arg.keywords.append(keyword_arg)\n    if arg.func.attr == 'choice':\n        convert_args_to_dict(arg)\n\n    return name, arg", "output": "Parse `nni.variable` expression.\n    Return the name argument and AST node of annotated expression.\n    code: annotation string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def bans(self):\n        \"\"\"\n        \"\"\"\n\n        data = await self._state.http.get_bans(self.id)\n        return [BanEntry(user=User(state=self._state, data=e['user']),\n                         reason=e['reason'])\n                for e in data]", "output": "|coro|\n\n        Retrieves all the users that are banned from the guild.\n\n        This coroutine returns a :class:`list` of BanEntry objects, which is a\n        namedtuple with a ``user`` field to denote the :class:`User`\n        that got banned along with a ``reason`` field specifying\n        why the user was banned that could be set to ``None``.\n\n        You must have the :attr:`~Permissions.ban_members` permission\n        to get this information.\n\n        Raises\n        -------\n        Forbidden\n            You do not have proper permissions to get the information.\n        HTTPException\n            An error occurred while fetching the information.\n\n        Returns\n        --------\n        List[BanEntry]\n            A list of BanEntry objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_ae_base():\n  \"\"\"\"\"\"\n  hparams = transformer_ae_small()\n  hparams.batch_size = 2048\n  hparams.hidden_size = 512\n  hparams.filter_size = 4096\n  hparams.num_hidden_layers = 6\n  return hparams", "output": "Set of hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def profile_add(user, profile):\n    '''\n    \n    '''\n    ret = {}\n\n    ## validate profiles\n    profiles = profile.split(',')\n    known_profiles = profile_list().keys()\n    valid_profiles = [p for p in profiles if p in known_profiles]\n    log.debug(\n        'rbac.profile_add - profiles=%s, known_profiles=%s, valid_profiles=%s',\n        profiles,\n        known_profiles,\n        valid_profiles,\n    )\n\n    ## update user profiles\n    if valid_profiles:\n        res = __salt__['cmd.run_all']('usermod -P \"{profiles}\" {login}'.format(\n            login=user,\n            profiles=','.join(set(profile_get(user) + valid_profiles)),\n        ))\n        if res['retcode'] > 0:\n            ret['Error'] = {\n                'retcode': res['retcode'],\n                'message': res['stderr'] if 'stderr' in res else res['stdout']\n            }\n            return ret\n\n    ## update return value\n    active_profiles = profile_get(user, False)\n    for p in profiles:\n        if p not in valid_profiles:\n            ret[p] = 'Unknown'\n        elif p in active_profiles:\n            ret[p] = 'Added'\n        else:\n            ret[p] = 'Failed'\n\n    return ret", "output": "Add profile to user\n\n    user : string\n        username\n    profile : string\n        profile name\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rbac.profile_add martine 'Primary Administrator'\n        salt '*' rbac.profile_add martine 'User Management,User Security'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def maybe_build_from_corpus(self, corpus_generator, **kwargs):\n    \"\"\"\"\"\"\n    if self._encoder_cls is not text_lib.SubwordTextEncoder:\n      return\n    if self.encoder:\n      return\n\n    vocab_size = self._encoder_config.vocab_size\n    self.encoder = text_lib.SubwordTextEncoder.build_from_corpus(\n        corpus_generator=corpus_generator,\n        target_vocab_size=vocab_size,\n        **kwargs)", "output": "Call SubwordTextEncoder.build_from_corpus is encoder_cls is such.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set(self, locs, values):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            self.values[locs] = values\n        except (ValueError):\n\n            # broadcasting error\n            # see GH6171\n            new_shape = list(values.shape)\n            new_shape[0] = len(self.items)\n            self.values = np.empty(tuple(new_shape), dtype=self.dtype)\n            self.values.fill(np.nan)\n            self.values[locs] = values", "output": "Modify Block in-place with new item value\n\n        Returns\n        -------\n        None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_minions():\n    '''\n    \n    '''\n    log.debug('sdstack_etcd returner <get_minions> called')\n    ret = []\n    client, path = _get_conn(__opts__)\n    items = client.get('/'.join((path, 'minions')))\n    for item in items.children:\n        comps = str(item.key).split('/')\n        ret.append(comps[-1])\n    return ret", "output": "Return a list of minions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(self, source, name=None, filename=None):\n        \"\"\"\n        \"\"\"\n        try:\n            return self._parse(source, name, filename)\n        except TemplateSyntaxError:\n            exc_info = sys.exc_info()\n        self.handle_exception(exc_info, source_hint=source)", "output": "Parse the sourcecode and return the abstract syntax tree.  This\n        tree of nodes is used by the compiler to convert the template into\n        executable source- or bytecode.  This is useful for debugging or to\n        extract information from templates.\n\n        If you are :ref:`developing Jinja2 extensions <writing-extensions>`\n        this gives you a good overview of the node tree generated.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(name='all', user=None, conf_file=None, bin_env=None):\n    '''\n    \n    '''\n    if name.endswith(':*'):\n        name = name[:-1]\n    ret = __salt__['cmd.run_all'](\n        _ctl_cmd('start', name, conf_file, bin_env),\n        runas=user,\n        python_shell=False,\n    )\n    return _get_return(ret)", "output": "Start the named service.\n    Process group names should not include a trailing asterisk.\n\n    user\n        user to run supervisorctl as\n    conf_file\n        path to supervisord config file\n    bin_env\n        path to supervisorctl bin or path to virtualenv with supervisor\n        installed\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' supervisord.start <service>\n        salt '*' supervisord.start <group>:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _connect(self):\n        '''\n        \n        '''\n        try:\n            self.bigIP = f5.BIGIP(hostname=self.lb,\n                                  username=self.username,\n                                  password=self.password,\n                                  fromurl=True,\n                                  wsdls=['LocalLB.VirtualServer',\n                                         'LocalLB.Pool'])\n        except Exception:\n            raise Exception(\n                'Unable to connect to {0}'.format(self.lb)\n            )\n\n        return True", "output": "Connect to F5", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _canonize_input(self, dataset):\n        \"\"\"\n        \n        \"\"\"\n        unpack = lambda x: x\n        if isinstance(dataset, _tc.SArray):\n            dataset = _tc.SFrame({self.feature: dataset})\n        elif isinstance(dataset, _tc.Image):\n            dataset = _tc.SFrame({self.feature: [dataset]})\n            unpack = lambda x: x[0]\n        return dataset, unpack", "output": "Takes input and returns tuple of the input in canonical form (SFrame)\n        along with an unpack callback function that can be applied to\n        prediction results to \"undo\" the canonization.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_netloc_and_auth(self, netloc, scheme):\n        \"\"\"\n        \n        \"\"\"\n        if scheme == 'ssh':\n            # The --username and --password options can't be used for\n            # svn+ssh URLs, so keep the auth information in the URL.\n            return super(Subversion, self).get_netloc_and_auth(\n                netloc, scheme)\n\n        return split_auth_from_netloc(netloc)", "output": "This override allows the auth information to be passed to svn via the\n        --username and --password options instead of via the URL.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_paths():\n    \"\"\"\n    \"\"\"\n    paths = sysconfig.get_paths()\n    return {\n        \"prefix\": sys.prefix,\n        \"data\": paths[\"data\"],\n        \"scripts\": paths[\"scripts\"],\n        \"headers\": paths[\"include\"],\n        \"purelib\": paths[\"purelib\"],\n        \"platlib\": paths[\"platlib\"],\n    }", "output": "Prepare paths for distlib.wheel.Wheel to install into.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_repository(self, repository, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if repository in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'repository'.\")\n        return self.transport.perform_request('DELETE',\n            _make_path('_snapshot', repository), params=params)", "output": "Removes a shared file system repository.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html>`_\n\n        :arg repository: A comma-separated list of repository names\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg timeout: Explicit operation timeout", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reduce_max(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'axes':'axis'})\n    return 'max', new_attrs, inputs", "output": "Reduce the array along a given axis by maximum value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def maskrcnn_upXconv_head(feature, num_category, num_convs, norm=None):\n    \"\"\"\n    \n    \"\"\"\n    assert norm in [None, 'GN'], norm\n    l = feature\n    with argscope([Conv2D, Conv2DTranspose], data_format='channels_first',\n                  kernel_initializer=tf.variance_scaling_initializer(\n                      scale=2.0, mode='fan_out',\n                      distribution='untruncated_normal' if get_tf_version_tuple() >= (1, 12) else 'normal')):\n        # c2's MSRAFill is fan_out\n        for k in range(num_convs):\n            l = Conv2D('fcn{}'.format(k), l, cfg.MRCNN.HEAD_DIM, 3, activation=tf.nn.relu)\n            if norm is not None:\n                l = GroupNorm('gn{}'.format(k), l)\n        l = Conv2DTranspose('deconv', l, cfg.MRCNN.HEAD_DIM, 2, strides=2, activation=tf.nn.relu)\n        l = Conv2D('conv', l, num_category, 1)\n    return l", "output": "Args:\n        feature (NxCx s x s): size is 7 in C4 models and 14 in FPN models.\n        num_category(int):\n        num_convs (int): number of convolution layers\n        norm (str or None): either None or 'GN'\n\n    Returns:\n        mask_logits (N x num_category x 2s x 2s):", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def master_call(self, **kwargs):\n        '''\n        \n        '''\n        load = kwargs\n        load['cmd'] = self.client\n        channel = salt.transport.client.ReqChannel.factory(self.opts,\n                                                           crypt='clear',\n                                                           usage='master_call')\n        try:\n            ret = channel.send(load)\n        finally:\n            channel.close()\n        if isinstance(ret, collections.Mapping):\n            if 'error' in ret:\n                salt.utils.error.raise_error(**ret['error'])\n        return ret", "output": "Execute a function through the master network interface.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def max_min_col_update(self):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        if self.df.shape[0] == 0: # If no rows to compute max/min then return\r\n            return\r\n        self.max_min_col = []\r\n        for dummy, col in self.df.iteritems():\r\n            if col.dtype in REAL_NUMBER_TYPES + COMPLEX_NUMBER_TYPES:\r\n                if col.dtype in REAL_NUMBER_TYPES:\r\n                    vmax = col.max(skipna=True)\r\n                    vmin = col.min(skipna=True)\r\n                else:\r\n                    vmax = col.abs().max(skipna=True)\r\n                    vmin = col.abs().min(skipna=True)\r\n                if vmax != vmin:\r\n                    max_min = [vmax, vmin]\r\n                else:\r\n                    max_min = [vmax, vmin - 1]\r\n            else:\r\n                max_min = None\r\n            self.max_min_col.append(max_min)", "output": "Determines the maximum and minimum number in each column.\r\n\r\n        The result is a list whose k-th entry is [vmax, vmin], where vmax and\r\n        vmin denote the maximum and minimum of the k-th column (ignoring NaN). \r\n        This list is stored in self.max_min_col.\r\n\r\n        If the k-th column has a non-numerical dtype, then the k-th entry\r\n        is set to None. If the dtype is complex, then compute the maximum and\r\n        minimum of the absolute values. If vmax equals vmin, then vmin is \r\n        decreased by one.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _exploit(self, trial_executor, trial, trial_to_clone):\n        \"\"\"\"\"\"\n\n        trial_state = self._trial_state[trial]\n        new_state = self._trial_state[trial_to_clone]\n        if not new_state.last_checkpoint:\n            logger.info(\"[pbt]: no checkpoint for trial.\"\n                        \" Skip exploit for Trial {}\".format(trial))\n            return\n        new_config = explore(trial_to_clone.config, self._hyperparam_mutations,\n                             self._resample_probability,\n                             self._custom_explore_fn)\n        logger.info(\"[exploit] transferring weights from trial \"\n                    \"{} (score {}) -> {} (score {})\".format(\n                        trial_to_clone, new_state.last_score, trial,\n                        trial_state.last_score))\n\n        if self._log_config:\n            self._log_config_on_step(trial_state, new_state, trial,\n                                     trial_to_clone, new_config)\n\n        new_tag = make_experiment_tag(trial_state.orig_tag, new_config,\n                                      self._hyperparam_mutations)\n        reset_successful = trial_executor.reset_trial(trial, new_config,\n                                                      new_tag)\n        if reset_successful:\n            trial_executor.restore(\n                trial, Checkpoint.from_object(new_state.last_checkpoint))\n        else:\n            trial_executor.stop_trial(trial, stop_logger=False)\n            trial.config = new_config\n            trial.experiment_tag = new_tag\n            trial_executor.start_trial(\n                trial, Checkpoint.from_object(new_state.last_checkpoint))\n\n        self._num_perturbations += 1\n        # Transfer over the last perturbation time as well\n        trial_state.last_perturbation_time = new_state.last_perturbation_time", "output": "Transfers perturbed state from trial_to_clone -> trial.\n\n        If specified, also logs the updated hyperparam state.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fpopen(*args, **kwargs):\n    '''\n    \n\n    '''\n    # Remove uid, gid and mode from kwargs if present\n    uid = kwargs.pop('uid', -1)  # -1 means no change to current uid\n    gid = kwargs.pop('gid', -1)  # -1 means no change to current gid\n    mode = kwargs.pop('mode', None)\n    with fopen(*args, **kwargs) as f_handle:\n        path = args[0]\n        d_stat = os.stat(path)\n\n        if hasattr(os, 'chown'):\n            # if uid and gid are both -1 then go ahead with\n            # no changes at all\n            if (d_stat.st_uid != uid or d_stat.st_gid != gid) and \\\n                    [i for i in (uid, gid) if i != -1]:\n                os.chown(path, uid, gid)\n\n        if mode is not None:\n            mode_part = stat.S_IMODE(d_stat.st_mode)\n            if mode_part != mode:\n                os.chmod(path, (d_stat.st_mode ^ mode_part) | mode)\n\n        yield f_handle", "output": "Shortcut for fopen with extra uid, gid, and mode options.\n\n    Supported optional Keyword Arguments:\n\n    mode\n        Explicit mode to set. Mode is anything os.chmod would accept\n        as input for mode. Works only on unix/unix-like systems.\n\n    uid\n        The uid to set, if not set, or it is None or -1 no changes are\n        made. Same applies if the path is already owned by this uid.\n        Must be int. Works only on unix/unix-like systems.\n\n    gid\n        The gid to set, if not set, or it is None or -1 no changes are\n        made. Same applies if the path is already owned by this gid.\n        Must be int. Works only on unix/unix-like systems.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_cell(self, column_family_id, column, value, timestamp=None):\n        \"\"\"\n        \"\"\"\n        self._set_cell(column_family_id, column, value, timestamp=timestamp, state=None)", "output": "Sets a value in this row.\n\n        The cell is determined by the ``row_key`` of this :class:`DirectRow`\n        and the ``column``. The ``column`` must be in an existing\n        :class:`.ColumnFamily` (as determined by ``column_family_id``).\n\n        .. note::\n\n            This method adds a mutation to the accumulated mutations on this\n            row, but does not make an API request. To actually\n            send an API request (with the mutations) to the Google Cloud\n            Bigtable API, call :meth:`commit`.\n\n        For example:\n\n        .. literalinclude:: snippets_table.py\n            :start-after: [START bigtable_row_set_cell]\n            :end-before: [END bigtable_row_set_cell]\n\n        :type column_family_id: str\n        :param column_family_id: The column family that contains the column.\n                                 Must be of the form\n                                 ``[_a-zA-Z0-9][-_.a-zA-Z0-9]*``.\n\n        :type column: bytes\n        :param column: The column within the column family where the cell\n                       is located.\n\n        :type value: bytes or :class:`int`\n        :param value: The value to set in the cell. If an integer is used,\n                      will be interpreted as a 64-bit big-endian signed\n                      integer (8 bytes).\n\n        :type timestamp: :class:`datetime.datetime`\n        :param timestamp: (Optional) The timestamp of the operation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_token():\n    '''\n    \n    '''\n    username = __opts__.get('rallydev', {}).get('username', None)\n    password = __opts__.get('rallydev', {}).get('password', None)\n    path = 'https://rally1.rallydev.com/slm/webservice/v2.0/security/authorize'\n    result = salt.utils.http.query(\n        path,\n        decode=True,\n        decode_type='json',\n        text=True,\n        status=True,\n        username=username,\n        password=password,\n        cookies=True,\n        persist_session=True,\n        opts=__opts__,\n    )\n    if 'dict' not in result:\n        return None\n\n    return result['dict']['OperationResult']['SecurityToken']", "output": "Get an auth token", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_all_cache_subnet_groups(name=None, region=None, key=None,\n                                keyid=None, profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        marker = ''\n        groups = []\n        while marker is not None:\n            ret = conn.describe_cache_subnet_groups(cache_subnet_group_name=name,\n                                                    marker=marker)\n            trimmed = ret.get('DescribeCacheSubnetGroupsResponse',\n                              {}).get('DescribeCacheSubnetGroupsResult', {})\n            groups += trimmed.get('CacheSubnetGroups', [])\n            marker = trimmed.get('Marker', None)\n        if not groups:\n            log.debug('No ElastiCache subnet groups found.')\n        return groups\n    except boto.exception.BotoServerError as e:\n        log.error(e)\n        return []", "output": "Return a list of all cache subnet groups with details\n\n    CLI example::\n\n        salt myminion boto_elasticache.get_all_subnet_groups region=us-east-1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_layer_converter_fn(layer):\n    \"\"\"\n    \"\"\"\n    layer_type = type(layer)\n    if layer_type in _KERAS_LAYER_REGISTRY:\n        return _KERAS_LAYER_REGISTRY[layer_type]\n    else:\n        raise TypeError(\"Keras layer of type %s is not supported.\" % type(layer))", "output": "Get the right converter function for Keras", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_known_inconsistencies(bill_data, bond_data):\n    \"\"\"\n    \n    \"\"\"\n    inconsistent_dates = bill_data.index.sym_diff(bond_data.index)\n    known_inconsistencies = [\n        # bill_data has an entry for 2010-02-15, which bond_data doesn't.\n        # bond_data has an entry for 2006-09-04, which bill_data doesn't.\n        # Both of these dates are bank holidays (Flag Day and Labor Day,\n        # respectively).\n        pd.Timestamp('2006-09-04', tz='UTC'),\n        pd.Timestamp('2010-02-15', tz='UTC'),\n        # 2013-07-25 comes back as \"Not available\" from the bills endpoint.\n        # This date doesn't seem to be a bank holiday, but the previous\n        # calendar implementation dropped this entry, so we drop it as well.\n        # If someone cares deeply about the integrity of the Canadian trading\n        # calendar, they may want to consider forward-filling here rather than\n        # dropping the row.\n        pd.Timestamp('2013-07-25', tz='UTC'),\n    ]\n    unexpected_inconsistences = inconsistent_dates.drop(known_inconsistencies)\n    if len(unexpected_inconsistences):\n        in_bills = bill_data.index.difference(bond_data.index).difference(\n            known_inconsistencies\n        )\n        in_bonds = bond_data.index.difference(bill_data.index).difference(\n            known_inconsistencies\n        )\n        raise ValueError(\n            \"Inconsistent dates for Canadian treasury bills vs bonds. \\n\"\n            \"Dates with bills but not bonds: {in_bills}.\\n\"\n            \"Dates with bonds but not bills: {in_bonds}.\".format(\n                in_bills=in_bills,\n                in_bonds=in_bonds,\n            )\n        )", "output": "There are a couple quirks in the data provided by Bank of Canada.\n    Check that no new quirks have been introduced in the latest download.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def file_copy(src=None, dest=None):\n    '''\n    \n    '''\n    conn = __proxy__['junos.conn']()\n    ret = {}\n    ret['out'] = True\n\n    if src is None:\n        ret['message'] = \\\n            'Please provide the absolute path of the file to be copied.'\n        ret['out'] = False\n        return ret\n    if not os.path.isfile(src):\n        ret['message'] = 'Invalid source file path'\n        ret['out'] = False\n        return ret\n\n    if dest is None:\n        ret['message'] = \\\n            'Please provide the absolute path of the destination where the file is to be copied.'\n        ret['out'] = False\n        return ret\n\n    try:\n        with SCP(conn, progress=True) as scp:\n            scp.put(src, dest)\n        ret['message'] = 'Successfully copied file from {0} to {1}'.format(\n            src, dest)\n    except Exception as exception:\n        ret['message'] = 'Could not copy file : \"{0}\"'.format(exception)\n        ret['out'] = False\n\n    return ret", "output": "Copies the file from the local device to the junos device\n\n    src\n        The source path where the file is kept.\n\n    dest\n        The destination path on the where the file will be copied\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'device_name' junos.file_copy /home/m2/info.txt info_copy.txt", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def free (self):\n        \"\"\" \n        \"\"\"\n        result = [p for p in self.lazy_properties\n                  if not p.feature.incidental and p.feature.free]\n        result.extend(self.free_)\n        return result", "output": "Returns free properties which are not dependency properties.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy(self):\n        \"\"\"\"\"\"\n        ret = super().copy()\n        for cmd in self.commands:\n            ret.add_command(cmd.copy())\n        return ret", "output": "Creates a copy of this :class:`Group`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lxml(self) -> HtmlElement:\n        \"\"\"\n        \"\"\"\n        if self._lxml is None:\n            try:\n                self._lxml = soup_parse(self.html, features='html.parser')\n            except ValueError:\n                self._lxml = lxml.html.fromstring(self.raw_html)\n\n        return self._lxml", "output": "`lxml <http://lxml.de>`_ representation of the\n        :class:`Element <Element>` or :class:`HTML <HTML>`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _copy_id_str_old(self):\n        '''\n        \n        '''\n        if self.passwd:\n            # Using single quotes prevents shell expansion and\n            # passwords containing '$'\n            return \"{0} {1} '{2} -p {3} {4} {5}@{6}'\".format(\n                    'ssh-copy-id',\n                    '-i {0}.pub'.format(self.priv),\n                    self._passwd_opts(),\n                    self.port,\n                    self._ssh_opts(),\n                    self.user,\n                    self.host)\n        return None", "output": "Return the string to execute ssh-copy-id", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def systemctl_reload():\n    '''\n    \n    '''\n    out = __salt__['cmd.run_all'](\n        _systemctl_cmd('--system daemon-reload'),\n        python_shell=False,\n        redirect_stderr=True)\n    if out['retcode'] != 0:\n        raise CommandExecutionError(\n            'Problem performing systemctl daemon-reload: %s' % out['stdout']\n        )\n    _clear_context()\n    return True", "output": ".. versionadded:: 0.15.0\n\n    Reloads systemctl, an action needed whenever unit files are updated.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.systemctl_reload", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cummax(self, axis=None, skipna=True, *args, **kwargs):\r\n        \"\"\"\r\n        \"\"\"\r\n        axis = self._get_axis_number(axis) if axis is not None else 0\r\n        if axis:\r\n            self._validate_dtypes()\r\n        return self.__constructor__(\r\n            query_compiler=self._query_compiler.cummax(\r\n                axis=axis, skipna=skipna, **kwargs\r\n            )\r\n        )", "output": "Perform a cumulative maximum across the DataFrame.\r\n\r\n        Args:\r\n            axis (int): The axis to take maximum on.\r\n            skipna (bool): True to skip NA values, false otherwise.\r\n\r\n        Returns:\r\n            The cumulative maximum of the DataFrame.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_adapter(registry, ob):\n    \"\"\"\"\"\"\n    types = _always_object(inspect.getmro(getattr(ob, '__class__', type(ob))))\n    for t in types:\n        if t in registry:\n            return registry[t]", "output": "Return an adapter factory for `ob` from `registry`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_absolute(self, link):\n        \"\"\"\"\"\"\n\n        # Parse the link with stdlib.\n        parsed = urlparse(link)._asdict()\n\n        # If link is relative, then join it with base_url.\n        if not parsed['netloc']:\n            return urljoin(self.base_url, link)\n\n        # Link is absolute; if it lacks a scheme, add one from base_url.\n        if not parsed['scheme']:\n            parsed['scheme'] = urlparse(self.base_url).scheme\n\n            # Reconstruct the URL to incorporate the new scheme.\n            parsed = (v for v in parsed.values())\n            return urlunparse(parsed)\n\n        # Link is absolute and complete with scheme; nothing to be done here.\n        return link", "output": "Makes a given link absolute.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def launch_instance(instance_name,\n                    command,\n                    existing_ip=None,\n                    cpu=1,\n                    mem=4,\n                    code_dir=None,\n                    setup_command=None):\n  \"\"\"\"\"\"\n  # Create instance\n  ip = existing_ip or create_instance(instance_name, cpu=cpu, mem=mem)\n  tf.logging.info(\"Waiting for SSH %s\", instance_name)\n  ready = wait_for_ssh(ip)\n  if not ready:\n    raise ValueError(\"Instance %s never ready for SSH\" % instance_name)\n\n  # Copy code\n  if code_dir:\n    shell_run_with_retry(COPY_CODE, retries=2,\n                         local_dir=code_dir, instance_name=instance_name)\n\n  # Run setup\n  if setup_command:\n    tf.logging.info(\"Running setup on %s\", instance_name)\n    remote_run(setup_command, instance_name)\n\n  # Run command\n  tf.logging.info(\"Running command on %s\", instance_name)\n  remote_run(command, instance_name, detach=True)", "output": "Launch a GCE instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_metadata(self, handler):\n        \"\"\"  \"\"\"\n        if self.metadata is not None:\n            handler.write_metadata(self.cname, self.metadata)", "output": "set the meta data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_argument_list_from_toolkit_function_name(fn):\n    \"\"\"\n    \n    \"\"\"\n    unity = _get_unity()\n    fnprops = unity.describe_toolkit_function(fn)\n    argnames = fnprops['arguments']\n    return argnames", "output": "Given a toolkit function name, return the argument list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __int(value):\n    ''''''\n    valid, _value = False, value\n    try:\n        _value = int(value)\n        valid = True\n    except ValueError:\n        pass\n    return (valid, _value, 'integer')", "output": "validate an integer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_model(name, dataset_name='wikitext-2', **kwargs):\n    \"\"\"\n    \"\"\"\n    models = {'bert_12_768_12': bert_12_768_12,\n              'bert_24_1024_16': bert_24_1024_16}\n    name = name.lower()\n    if name not in models:\n        raise ValueError(\n            'Model %s is not supported. Available options are\\n\\t%s' % (\n                name, '\\n\\t'.join(sorted(models.keys()))))\n    kwargs['dataset_name'] = dataset_name\n    return models[name](**kwargs)", "output": "Returns a pre-defined model by name.\n\n    Parameters\n    ----------\n    name : str\n        Name of the model.\n    dataset_name : str or None, default 'wikitext-2'.\n        If None, then vocab is required, for specifying embedding weight size, and is directly\n        returned.\n    vocab : gluonnlp.Vocab or None, default None\n        Vocabulary object to be used with the language model.\n        Required when dataset_name is not specified.\n        None Vocabulary object is required with the ELMo model.\n    pretrained : bool, default False\n        Whether to load the pre-trained weights for model.\n    ctx : Context, default CPU\n        The context in which to load the pre-trained weights.\n    root : str, default '~/.mxnet/models'\n        Location for keeping the model parameters.\n\n    Returns\n    -------\n    gluon.Block, gluonnlp.Vocab, (optional) gluonnlp.Vocab", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def go_to_new_line(self):\r\n        \"\"\"\"\"\"\r\n        self.stdkey_end(False, False)\r\n        self.insert_text(self.get_line_separator())", "output": "Go to the end of the current line and create a new line", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_row_table_name(table_name, row):\n    \"\"\"\n    \"\"\"\n    if row.table is not None and row.table.name != table_name:\n        raise TableMismatchError(\n            \"Row %s is a part of %s table. Current table: %s\"\n            % (row.row_key, row.table.name, table_name)\n        )", "output": "Checks that a row belongs to a table.\n\n    :type table_name: str\n    :param table_name: The name of the table.\n\n    :type row: :class:`~google.cloud.bigtable.row.Row`\n    :param row: An instance of :class:`~google.cloud.bigtable.row.Row`\n                subclasses.\n\n    :raises: :exc:`~.table.TableMismatchError` if the row does not belong to\n             the table.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_dvs_capability(dvs_name, dvs_capability):\n    '''\n    \n    '''\n    log.trace('Building the dict of the DVS \\'%s\\' capability', dvs_name)\n    return {'operation_supported': dvs_capability.dvsOperationSupported,\n            'portgroup_operation_supported':\n            dvs_capability.dvPortGroupOperationSupported,\n            'port_operation_supported': dvs_capability.dvPortOperationSupported}", "output": "Returns the dict representation of the DVS product_info\n\n    dvs_name\n        The name of the DVS\n\n    dvs_capability\n        The DVS capability", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_slice_axis(self, slice_obj, axis=None):\n        \"\"\"  \"\"\"\n        if axis is None:\n            axis = self.axis or 0\n\n        obj = self.obj\n        if not need_slice(slice_obj):\n            return obj.copy(deep=False)\n\n        labels = obj._get_axis(axis)\n        indexer = labels.slice_indexer(slice_obj.start, slice_obj.stop,\n                                       slice_obj.step, kind=self.name)\n\n        if isinstance(indexer, slice):\n            return self._slice(indexer, axis=axis, kind='iloc')\n        else:\n            return self.obj._take(indexer, axis=axis)", "output": "this is pretty simple as we just have to deal with labels", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def thorium(opts, functions, runners):\n    '''\n    \n    '''\n    pack = {'__salt__': functions, '__runner__': runners, '__context__': {}}\n    ret = LazyLoader(_module_dirs(opts, 'thorium'),\n            opts,\n            tag='thorium',\n            pack=pack)\n    ret.pack['__thorium__'] = ret\n    return ret", "output": "Load the thorium runtime modules", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def participant_from_submission_path(submission_path):\n  \"\"\"\n  \"\"\"\n  basename = os.path.basename(submission_path)\n  file_ext = None\n  for e in ALLOWED_EXTENSIONS:\n    if basename.endswith(e):\n      file_ext = e\n      break\n  if not file_ext:\n    raise ValueError('Invalid submission path: ' + submission_path)\n  basename = basename[:-len(file_ext)]\n  if basename.isdigit():\n    return {'team_id': int(basename)}\n  if basename.startswith('baseline_'):\n    return {'baseline_id': basename[len('baseline_'):]}\n  raise ValueError('Invalid submission path: ' + submission_path)", "output": "Parses type of participant based on submission filename.\n\n  Args:\n    submission_path: path to the submission in Google Cloud Storage\n\n  Returns:\n    dict with one element. Element key correspond to type of participant\n    (team, baseline), element value is ID of the participant.\n\n  Raises:\n    ValueError: is participant can't be determined based on submission path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def html(self) -> _BaseHTML:\n        \"\"\"\n        \"\"\"\n        if self._html:\n            return self.raw_html.decode(self.encoding, errors='replace')\n        else:\n            return etree.tostring(self.element, encoding='unicode').strip()", "output": "Unicode representation of the HTML content\n        (`learn more <http://www.diveintopython3.net/strings.html>`_).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def denormalize_bboxes(bboxes, rows, cols):\n    \"\"\"\"\"\"\n    return [denormalize_bbox(bbox, rows, cols) for bbox in bboxes]", "output": "Denormalize a list of bounding boxes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log_params(params, name=\"params\"):\n  \"\"\"\"\"\"\n  for i, param in enumerate(params):\n    if not param:\n      # Empty tuple.\n      continue\n    if not isinstance(param, (list, tuple)):\n      logging.error(\n          \"%s[%d] : (%s) = [%s]\", name, i, param.shape, onp.array(param))\n    else:\n      for j, p in enumerate(param):\n        logging.error(\n            \"\\t%s[%d, %d] = [%s]\", name, i, j, onp.array(p))", "output": "Dumps the params with `logging.error`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(metric, *args, **kwargs):\n    \"\"\"\n    \"\"\"\n    if callable(metric):\n        return CustomMetric(metric, *args, **kwargs)\n    elif isinstance(metric, list):\n        composite_metric = CompositeEvalMetric()\n        for child_metric in metric:\n            composite_metric.add(create(child_metric, *args, **kwargs))\n        return composite_metric\n\n    return _create(metric, *args, **kwargs)", "output": "Creates evaluation metric from metric names or instances of EvalMetric\n    or a custom metric function.\n\n    Parameters\n    ----------\n    metric : str or callable\n        Specifies the metric to create.\n        This argument must be one of the below:\n\n        - Name of a metric.\n        - An instance of `EvalMetric`.\n        - A list, each element of which is a metric or a metric name.\n        - An evaluation function that computes custom metric for a given batch of\n          labels and predictions.\n    *args : list\n        Additional arguments to metric constructor.\n        Only used when metric is str.\n    **kwargs : dict\n        Additional arguments to metric constructor.\n        Only used when metric is str\n\n    Examples\n    --------\n    >>> def custom_metric(label, pred):\n    ...     return np.mean(np.abs(label - pred))\n    ...\n    >>> metric1 = mx.metric.create('acc')\n    >>> metric2 = mx.metric.create(custom_metric)\n    >>> metric3 = mx.metric.create([metric1, metric2, 'rmse'])", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def total_bytes_billed(self):\n        \"\"\"\n        \"\"\"\n        result = self._job_statistics().get(\"totalBytesBilled\")\n        if result is not None:\n            result = int(result)\n        return result", "output": "Return total bytes billed from job statistics, if present.\n\n        See:\n        https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#statistics.query.totalBytesBilled\n\n        :rtype: int or None\n        :returns: total bytes processed by the job, or None if job is not\n                  yet complete.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def endswith(self, suffix):\n        \"\"\"\n        \n        \"\"\"\n        return ArrayPredicate(\n            term=self,\n            op=LabelArray.endswith,\n            opargs=(suffix,),\n        )", "output": "Construct a Filter matching values ending with ``suffix``.\n\n        Parameters\n        ----------\n        suffix : str\n            String suffix against which to compare values produced by ``self``.\n\n        Returns\n        -------\n        matches : Filter\n            Filter returning True for all sid/date pairs for which ``self``\n            produces a string ending with ``prefix``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_submissions_from_directory(dirname, use_gpu):\n  \"\"\"\n  \"\"\"\n  result = []\n  for sub_dir in os.listdir(dirname):\n    submission_path = os.path.join(dirname, sub_dir)\n    try:\n      if not os.path.isdir(submission_path):\n        continue\n      if not os.path.exists(os.path.join(submission_path, 'metadata.json')):\n        continue\n      with open(os.path.join(submission_path, 'metadata.json')) as f:\n        metadata = json.load(f)\n      if use_gpu and ('container_gpu' in metadata):\n        container = metadata['container_gpu']\n      else:\n        container = metadata['container']\n      entry_point = metadata['entry_point']\n      submission_type = metadata['type']\n      if submission_type == 'attack' or submission_type == 'targeted_attack':\n        submission = Attack(submission_path, container, entry_point, use_gpu)\n      elif submission_type == 'defense':\n        submission = Defense(submission_path, container, entry_point, use_gpu)\n      else:\n        raise ValueError('Invalid type of submission: %s' % submission_type)\n      result.append(submission)\n    except (IOError, KeyError, ValueError):\n      print('Failed to read submission from directory ', submission_path)\n  return result", "output": "Scans directory and read all submissions.\n\n  Args:\n    dirname: directory to scan.\n    use_gpu: whether submissions should use GPU. This argument is\n      used to pick proper Docker container for each submission and create\n      instance of Attack or Defense class.\n\n  Returns:\n    List with submissions (subclasses of Submission class).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hybrid_forward(self, F, words1, words2, weight):  # pylint: disable=arguments-differ\n        \"\"\"\n        \"\"\"\n        embeddings_words1 = F.Embedding(words1, weight,\n                                        input_dim=self._vocab_size,\n                                        output_dim=self._embed_size)\n        embeddings_words2 = F.Embedding(words2, weight,\n                                        input_dim=self._vocab_size,\n                                        output_dim=self._embed_size)\n        similarity = self.similarity(embeddings_words1, embeddings_words2)\n        return similarity", "output": "Predict the similarity of words1 and words2.\n\n        Parameters\n        ----------\n        words1 : Symbol or NDArray\n            The indices of the words the we wish to compare to the words in words2.\n        words2 : Symbol or NDArray\n            The indices of the words the we wish to compare to the words in words1.\n\n        Returns\n        -------\n        similarity : Symbol or NDArray\n            The similarity computed by WordEmbeddingSimilarity.similarity_function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def batch_to_embeddings(self, batch: List[List[str]]) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        \n        \"\"\"\n        character_ids = batch_to_ids(batch)\n        if self.cuda_device >= 0:\n            character_ids = character_ids.cuda(device=self.cuda_device)\n\n        bilm_output = self.elmo_bilm(character_ids)\n        layer_activations = bilm_output['activations']\n        mask_with_bos_eos = bilm_output['mask']\n\n        # without_bos_eos is a 3 element list of (activation, mask) tensor pairs,\n        # each with size (batch_size, num_timesteps, dim and (batch_size, num_timesteps)\n        # respectively.\n        without_bos_eos = [remove_sentence_boundaries(layer, mask_with_bos_eos)\n                           for layer in layer_activations]\n        # Converts a list of pairs (activation, mask) tensors to a single tensor of activations.\n        activations = torch.cat([ele[0].unsqueeze(1) for ele in without_bos_eos], dim=1)\n        # The mask is the same for each ELMo vector, so just take the first.\n        mask = without_bos_eos[0][1]\n\n        return activations, mask", "output": "Parameters\n        ----------\n        batch : ``List[List[str]]``, required\n            A list of tokenized sentences.\n\n        Returns\n        -------\n            A tuple of tensors, the first representing activations (batch_size, 3, num_timesteps, 1024) and\n        the second a mask (batch_size, num_timesteps).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_pb(cls, instance_pb, client):\n        \"\"\"\n        \"\"\"\n        match = _INSTANCE_NAME_RE.match(instance_pb.name)\n        if match is None:\n            raise ValueError(\n                \"Instance protobuf name was not in the \" \"expected format.\",\n                instance_pb.name,\n            )\n        if match.group(\"project\") != client.project:\n            raise ValueError(\n                \"Project ID on instance does not match the \" \"project ID on the client\"\n            )\n        instance_id = match.group(\"instance_id\")\n        configuration_name = instance_pb.config\n\n        result = cls(instance_id, client, configuration_name)\n        result._update_from_pb(instance_pb)\n        return result", "output": "Creates an instance from a protobuf.\n\n        :type instance_pb:\n            :class:`google.spanner.v2.spanner_instance_admin_pb2.Instance`\n        :param instance_pb: A instance protobuf object.\n\n        :type client: :class:`~google.cloud.spanner_v1.client.Client`\n        :param client: The client that owns the instance.\n\n        :rtype: :class:`Instance`\n        :returns: The instance parsed from the protobuf response.\n        :raises ValueError:\n            if the instance name does not match\n            ``projects/{project}/instances/{instance_id}`` or if the parsed\n            project ID does not match the project ID on the client.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _merge_flags(new_flags, old_flags=None, conf='any'):\n    '''\n    \n    '''\n    if not old_flags:\n        old_flags = []\n    args = [old_flags, new_flags]\n    if conf == 'accept_keywords':\n        tmp = new_flags + \\\n            [i for i in old_flags if _check_accept_keywords(new_flags, i)]\n    else:\n        tmp = portage.flatten(args)\n    flags = {}\n    for flag in tmp:\n        if flag[0] == '-':\n            flags[flag[1:]] = False\n        else:\n            flags[flag] = True\n    tmp = []\n    for key, val in six.iteritems(flags):\n        if val:\n            tmp.append(key)\n        else:\n            tmp.append('-' + key)\n\n    # Next sort is just aesthetic, can be commented for a small performance\n    # boost\n    tmp.sort(key=lambda x: x.lstrip('-'))\n    return tmp", "output": "Merges multiple lists of flags removing duplicates and resolving conflicts\n    giving priority to lasts lists.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _tasks_status(set_tasks):\n    \"\"\"\n    \n    \"\"\"\n    if set_tasks[\"ever_failed\"]:\n        if not set_tasks[\"failed\"]:\n            return LuigiStatusCode.SUCCESS_WITH_RETRY\n        else:\n            if set_tasks[\"scheduling_error\"]:\n                return LuigiStatusCode.FAILED_AND_SCHEDULING_FAILED\n            return LuigiStatusCode.FAILED\n    elif set_tasks[\"scheduling_error\"]:\n        return LuigiStatusCode.SCHEDULING_FAILED\n    elif set_tasks[\"not_run\"]:\n        return LuigiStatusCode.NOT_RUN\n    elif set_tasks[\"still_pending_ext\"]:\n        return LuigiStatusCode.MISSING_EXT\n    else:\n        return LuigiStatusCode.SUCCESS", "output": "Given a grouped set of tasks, returns a LuigiStatusCode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_output_layers(self):\n        \"\"\"\n        \n        \"\"\"\n        self.output_layers = []\n        # import pytest; pytest.set_trace()\n        if hasattr(self.model, 'output_layers'):\n            # find corresponding output layers in CoreML model\n            # assume output layers are not shared\n            # Helper function to recursively extract output layers\n            # even if the model has a layer which is a nested model\n            def extract_output_layers(keras_model):\n                output_layers = []\n                for layer in keras_model.output_layers:\n                    if hasattr(layer,'output_layers'):\n                        output_layers.extend(extract_output_layers(layer))\n                    else:\n                        output_layers.append(layer)\n                return output_layers\n\n            for kl in extract_output_layers(self.model):\n                coreml_layers = self.get_coreml_layers(kl)\n                if len(coreml_layers) > 0:\n                    for cl in coreml_layers:\n                        self.output_layers.append(cl)\n        elif len(self.model.outputs) > 0:\n            for model_output in self.model.outputs:\n                for l in self.layer_list:\n                    k_layer = self.keras_layer_map[l]\n                    in_nodes = k_layer._inbound_nodes if hasattr(k_layer, '_inbound_nodes') else k_layer.inbound_nodes\n                    for idx in range(len(in_nodes)):\n                        out_tensor = k_layer.get_output_at(idx)\n                        if out_tensor == model_output or (out_tensor.name in model_output.name):\n                            self.output_layers.append(l)\n        if len(self.output_layers) == 0:\n            raise ValueError(\"No outputs can be identified\")", "output": "Extract the ordering of output layers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_subtokens_from_list(self, subtoken_strings, reserved_tokens=None):\n    \"\"\"\n    \"\"\"\n    if reserved_tokens is None:\n      reserved_tokens = []\n\n    if reserved_tokens:\n      self._all_subtoken_strings = reserved_tokens + subtoken_strings\n    else:\n      self._all_subtoken_strings = subtoken_strings\n\n    # we remember the maximum length of any subtoken to avoid having to\n    # check arbitrarily long strings.\n    self._max_subtoken_len = max([len(s) for s in subtoken_strings])\n    self._subtoken_string_to_id = {\n        s: i + len(reserved_tokens)\n        for i, s in enumerate(subtoken_strings) if s\n    }\n    # Initialize the cache to empty.\n    self._cache_size = 2 ** 20\n    self._cache = [(None, None)] * self._cache_size", "output": "Initialize token information from a list of subtoken strings.\n\n    Args:\n      subtoken_strings: a list of subtokens\n      reserved_tokens: List of reserved tokens. We must have `reserved_tokens`\n        as None or the empty list, or else the global variable `RESERVED_TOKENS`\n        must be a prefix of `reserved_tokens`.\n\n    Raises:\n      ValueError: if reserved is not 0 or len(RESERVED_TOKENS). In this case, it\n        is not clear what the space is being reserved for, or when it will be\n        filled in.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_rmse_log(net, X_train, y_train):\n    \"\"\"\"\"\"\n    num_train = X_train.shape[0]\n    clipped_preds = nd.clip(net(X_train), 1, float('inf'))\n    return np.sqrt(2 * nd.sum(square_loss(\n        nd.log(clipped_preds), nd.log(y_train))).asscalar() / num_train)", "output": "Gets root mse between the logarithms of the prediction and the truth.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unstack(self, level=-1, fill_value=None):\n        \"\"\"\n        \n        \"\"\"\n        from pandas.core.reshape.reshape import unstack\n        return unstack(self, level, fill_value)", "output": "Pivot a level of the (necessarily hierarchical) index labels, returning\n        a DataFrame having a new level of column labels whose inner-most level\n        consists of the pivoted index labels.\n\n        If the index is not a MultiIndex, the output will be a Series\n        (the analogue of stack when the columns are not a MultiIndex).\n\n        The level involved will automatically get sorted.\n\n        Parameters\n        ----------\n        level : int, string, or list of these, default -1 (last level)\n            Level(s) of index to unstack, can pass level name\n        fill_value : replace NaN with this value if the unstack produces\n            missing values\n\n            .. versionadded:: 0.18.0\n\n        Returns\n        -------\n        Series or DataFrame\n\n        See Also\n        --------\n        DataFrame.pivot : Pivot a table based on column values.\n        DataFrame.stack : Pivot a level of the column labels (inverse operation\n            from `unstack`).\n\n        Examples\n        --------\n        >>> index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),\n        ...                                    ('two', 'a'), ('two', 'b')])\n        >>> s = pd.Series(np.arange(1.0, 5.0), index=index)\n        >>> s\n        one  a   1.0\n             b   2.0\n        two  a   3.0\n             b   4.0\n        dtype: float64\n\n        >>> s.unstack(level=-1)\n             a   b\n        one  1.0  2.0\n        two  3.0  4.0\n\n        >>> s.unstack(level=0)\n           one  two\n        a  1.0   3.0\n        b  2.0   4.0\n\n        >>> df = s.unstack(level=0)\n        >>> df.unstack()\n        one  a  1.0\n             b  2.0\n        two  a  3.0\n             b  4.0\n        dtype: float64", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def FromDatetime(self, dt):\n    \"\"\"\"\"\"\n    td = dt - datetime(1970, 1, 1)\n    self.seconds = td.seconds + td.days * _SECONDS_PER_DAY\n    self.nanos = td.microseconds * _NANOS_PER_MICROSECOND", "output": "Converts datetime to Timestamp.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_security_group(self, sec_grp):\n        '''\n        \n        '''\n        sec_grp_id = self._find_security_group_id(sec_grp)\n        ret = self.network_conn.delete_security_group(sec_grp_id)\n        return ret if ret else True", "output": "Deletes the specified security group", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def config_(dev=None, **kwargs):\n    '''\n    \n    '''\n    if dev is None:\n        spath = _fspath()\n    else:\n        spath = _bcpath(dev)\n\n    # filter out 'hidden' kwargs added by our favourite orchestration system\n    updates = dict([(key, val) for key, val in kwargs.items() if not key.startswith('__')])\n\n    if updates:\n        endres = 0\n        for key, val in updates.items():\n            endres += _sysfs_attr([spath, key], val,\n                                  'warn', 'Failed to update {0} with {1}'.format(os.path.join(spath, key), val))\n        return endres > 0\n    else:\n        result = {}\n        data = _sysfs_parse(spath, config=True, internals=True, options=True)\n        for key in ('other_ro', 'inter_ro'):\n            if key in data:\n                del data[key]\n\n        for key in data:\n            result.update(data[key])\n\n        return result", "output": "Show or update config of a bcache device.\n\n    If no device is given, operate on the cache set itself.\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt '*' bcache.config\n        salt '*' bcache.config bcache1\n        salt '*' bcache.config errors=panic journal_delay_ms=150\n        salt '*' bcache.config bcache1 cache_mode=writeback writeback_percent=15\n\n    :return: config or True/False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_binary_support(self, api_id, cors=False):\n        \"\"\"\n        \n        \"\"\"\n        response = self.apigateway_client.get_rest_api(\n            restApiId=api_id\n        )\n        if \"binaryMediaTypes\" in response and \"*/*\" in response[\"binaryMediaTypes\"]:\n            self.apigateway_client.update_rest_api(\n                restApiId=api_id,\n                patchOperations=[\n                    {\n                        'op': 'remove',\n                        'path': '/binaryMediaTypes/*~1*'\n                    }\n                ]\n            )\n        if cors:\n            # go through each resource and change the contentHandling type\n            response = self.apigateway_client.get_resources(restApiId=api_id)\n            resource_ids = [\n                item['id'] for item in response['items']\n                if 'OPTIONS' in item.get('resourceMethods', {})\n            ]\n\n            for resource_id in resource_ids:\n                self.apigateway_client.update_integration(\n                    restApiId=api_id,\n                    resourceId=resource_id,\n                    httpMethod='OPTIONS',\n                    patchOperations=[\n                        {\n                            \"op\": \"replace\",\n                            \"path\": \"/contentHandling\",\n                            \"value\": \"\"\n                        }\n                    ]\n                )", "output": "Remove binary support", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_api(name, description=None, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    try:\n        conn_params = dict(region=region, key=key, keyid=keyid, profile=profile)\n        r = _find_apis_by_name(name, description=description, **conn_params)\n        apis = r.get('restapi')\n        if apis:\n            conn = _get_conn(**conn_params)\n            for api in apis:\n                conn.delete_rest_api(restApiId=api['id'])\n            return {'deleted': True, 'count': len(apis)}\n        else:\n            return {'deleted': False}\n    except ClientError as e:\n        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Delete all REST API Service with the given name and an optional API description\n\n    Returns {deleted: True, count: deleted_count} if apis were deleted, and\n    returns {deleted: False} if error or not found.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_apigateway.delete_api myapi_name\n\n        salt myminion boto_apigateway.delete_api myapi_name description='api description'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_io_module():\n    \"\"\"\"\"\"\n    plist = ctypes.POINTER(ctypes.c_void_p)()\n    size = ctypes.c_uint()\n    check_call(_LIB.MXListDataIters(ctypes.byref(size), ctypes.byref(plist)))\n    module_obj = sys.modules[__name__]\n    for i in range(size.value):\n        hdl = ctypes.c_void_p(plist[i])\n        dataiter = _make_io_iterator(hdl)\n        setattr(module_obj, dataiter.__name__, dataiter)", "output": "List and add all the data iterators to current module.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render_embed_js(self, js_embed: Iterable[bytes]) -> bytes:\n        \"\"\"\n        \"\"\"\n        return (\n            b'<script type=\"text/javascript\">\\n//<![CDATA[\\n'\n            + b\"\\n\".join(js_embed)\n            + b\"\\n//]]>\\n</script>\"\n        )", "output": "Default method used to render the final embedded js for the\n        rendered webpage.\n\n        Override this method in a sub-classed controller to change the output.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_cert_bindings(site):\n    '''\n    \n    '''\n    ret = dict()\n    sites = list_sites()\n\n    if site not in sites:\n        log.warning('Site not found: %s', site)\n        return ret\n\n    for binding in sites[site]['bindings']:\n        if sites[site]['bindings'][binding]['certificatehash']:\n            ret[binding] = sites[site]['bindings'][binding]\n\n    if not ret:\n        log.warning('No certificate bindings found for site: %s', site)\n\n    return ret", "output": "List certificate bindings for an IIS site.\n\n    .. versionadded:: 2016.11.0\n\n    Args:\n        site (str): The IIS site name.\n\n    Returns:\n        dict: A dictionary of the binding names and properties.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' win_iis.list_bindings site", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_sfs_idxs(sizes:Sizes) -> List[int]:\n    \"\"\n    feature_szs = [size[-1] for size in sizes]\n    sfs_idxs = list(np.where(np.array(feature_szs[:-1]) != np.array(feature_szs[1:]))[0])\n    if feature_szs[0] != feature_szs[1]: sfs_idxs = [0] + sfs_idxs\n    return sfs_idxs", "output": "Get the indexes of the layers where the size of the activation changes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_active_project_path(self):\r\n        \"\"\"\"\"\"\r\n        active_project_path = None\r\n        if self.current_active_project:\r\n            active_project_path = self.current_active_project.root_path\r\n        return active_project_path", "output": "Get path of the active project", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_command_line(\n        self, args: List[str] = None, final: bool = True\n    ) -> List[str]:\n        \"\"\"\n\n        \"\"\"\n        if args is None:\n            args = sys.argv\n        remaining = []  # type: List[str]\n        for i in range(1, len(args)):\n            # All things after the last option are command line arguments\n            if not args[i].startswith(\"-\"):\n                remaining = args[i:]\n                break\n            if args[i] == \"--\":\n                remaining = args[i + 1 :]\n                break\n            arg = args[i].lstrip(\"-\")\n            name, equals, value = arg.partition(\"=\")\n            name = self._normalize_name(name)\n            if name not in self._options:\n                self.print_help()\n                raise Error(\"Unrecognized command line option: %r\" % name)\n            option = self._options[name]\n            if not equals:\n                if option.type == bool:\n                    value = \"true\"\n                else:\n                    raise Error(\"Option %r requires a value\" % name)\n            option.parse(value)\n\n        if final:\n            self.run_parse_callbacks()\n\n        return remaining", "output": "Parses all options given on the command line (defaults to\n        `sys.argv`).\n\n        Options look like ``--option=value`` and are parsed according\n        to their ``type``. For boolean options, ``--option`` is\n        equivalent to ``--option=true``\n\n        If the option has ``multiple=True``, comma-separated values\n        are accepted. For multi-value integer options, the syntax\n        ``x:y`` is also accepted and equivalent to ``range(x, y)``.\n\n        Note that ``args[0]`` is ignored since it is the program name\n        in `sys.argv`.\n\n        We return a list of all arguments that are not parsed as options.\n\n        If ``final`` is ``False``, parse callbacks will not be run.\n        This is useful for applications that wish to combine configurations\n        from multiple sources.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def all(self, **kwargs):\n        \"\"\"\n        \"\"\"\n        if self._is_transposed:\n            # Pandas ignores on axis=1\n            kwargs[\"bool_only\"] = False\n            kwargs[\"axis\"] = kwargs.get(\"axis\", 0) ^ 1\n            return self.transpose().all(**kwargs)\n        return self._process_all_any(lambda df, **kwargs: df.all(**kwargs), **kwargs)", "output": "Returns whether all the elements are true, potentially over an axis.\n\n        Return:\n            A new QueryCompiler object containing boolean values or boolean.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def colored(text, color=None, on_color=None, attrs=None):\n    \"\"\"\n    \"\"\"\n    if os.getenv(\"ANSI_COLORS_DISABLED\") is None:\n        style = \"NORMAL\"\n        if \"bold\" in attrs:\n            style = \"BRIGHT\"\n            attrs.remove(\"bold\")\n        if color is not None:\n            color = color.upper()\n            text = to_native_string(\"%s%s%s%s%s\") % (\n                to_native_string(getattr(colorama.Fore, color)),\n                to_native_string(getattr(colorama.Style, style)),\n                to_native_string(text),\n                to_native_string(colorama.Fore.RESET),\n                to_native_string(colorama.Style.NORMAL),\n            )\n\n        if on_color is not None:\n            on_color = on_color.upper()\n            text = to_native_string(\"%s%s%s%s\") % (\n                to_native_string(getattr(colorama.Back, on_color)),\n                to_native_string(text),\n                to_native_string(colorama.Back.RESET),\n                to_native_string(colorama.Style.NORMAL),\n            )\n\n        if attrs is not None:\n            fmt_str = to_native_string(\"%s[%%dm%%s%s[9m\") % (chr(27), chr(27))\n            for attr in attrs:\n                text = fmt_str % (ATTRIBUTES[attr], text)\n\n        text += RESET\n    return text", "output": "Colorize text using a reimplementation of the colorizer from\n    https://github.com/pavdmyt/yaspin so that it works on windows.\n\n    Available text colors:\n        red, green, yellow, blue, magenta, cyan, white.\n\n    Available text highlights:\n        on_red, on_green, on_yellow, on_blue, on_magenta, on_cyan, on_white.\n\n    Available attributes:\n        bold, dark, underline, blink, reverse, concealed.\n\n    Example:\n        colored('Hello, World!', 'red', 'on_grey', ['blue', 'blink'])\n        colored('Hello, World!', 'green')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_inplace_setting(self, value):\n        \"\"\"  \"\"\"\n\n        if self._is_mixed_type:\n            if not self._is_numeric_mixed_type:\n\n                # allow an actual np.nan thru\n                try:\n                    if np.isnan(value):\n                        return True\n                except Exception:\n                    pass\n\n                raise TypeError('Cannot do inplace boolean setting on '\n                                'mixed-types with a non np.nan value')\n\n        return True", "output": "check whether we allow in-place setting with this type of value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_msupdate_status():\n    '''\n    \n    '''\n    # To get the status of Microsoft Update we actually have to check the\n    # Microsoft Update Service Manager\n    # Initialize the PyCom system\n    with salt.utils.winapi.Com():\n        # Create a ServiceManager Object\n        obj_sm = win32com.client.Dispatch('Microsoft.Update.ServiceManager')\n\n    # Return a collection of loaded Services\n    col_services = obj_sm.Services\n\n    # Loop through the collection to find the Microsoft Udpate Service\n    # If it exists return True otherwise False\n    for service in col_services:\n        if service.name == 'Microsoft Update':\n            return True\n\n    return False", "output": "Check to see if Microsoft Update is Enabled\n    Return Boolean", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toggle_all(self, checked):\r\n        \"\"\"\"\"\"\r\n        self.parent_widget.sig_option_changed.emit('show_all', checked)\r\n        self.show_all = checked\r\n        self.set_show_all(checked)", "output": "Toggle all files mode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dependencies(requirement, sources):\n    \"\"\"\n    \"\"\"\n    getters = [\n        _get_dependencies_from_cache,\n        _cached(_get_dependencies_from_json, sources=sources),\n        _cached(_get_dependencies_from_pip, sources=sources),\n    ]\n    ireq = requirement.as_ireq()\n    last_exc = None\n    for getter in getters:\n        try:\n            result = getter(ireq)\n        except Exception as e:\n            last_exc = sys.exc_info()\n            continue\n        if result is not None:\n            deps, pyreq = result\n            reqs = [requirementslib.Requirement.from_line(d) for d in deps]\n            return reqs, pyreq\n    if last_exc:\n        six.reraise(*last_exc)\n    raise RuntimeError(\"failed to get dependencies for {}\".format(\n        requirement.as_line(),\n    ))", "output": "Get all dependencies for a given install requirement.\n\n    :param requirement: A requirement\n    :param sources: Pipfile-formatted sources\n    :type sources: list[dict]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _wget(cmd, opts=None, url='http://localhost:8080/manager', timeout=180):\n    '''\n    \n    '''\n\n    ret = {\n        'res': True,\n        'msg': []\n    }\n\n    # prepare authentication\n    auth = _auth(url)\n    if auth is False:\n        ret['res'] = False\n        ret['msg'] = 'missing username and password settings (grain/pillar)'\n        return ret\n\n    # prepare URL\n    if url[-1] != '/':\n        url += '/'\n    url6 = url\n    url += 'text/{0}'.format(cmd)\n    url6 += '{0}'.format(cmd)\n    if opts:\n        url += '?{0}'.format(_urlencode(opts))\n        url6 += '?{0}'.format(_urlencode(opts))\n\n    # Make the HTTP request\n    _install_opener(auth)\n\n    try:\n        # Trying tomcat >= 7 url\n        ret['msg'] = _urlopen(url, timeout=timeout).read().splitlines()\n    except Exception:\n        try:\n            # Trying tomcat6 url\n            ret['msg'] = _urlopen(url6, timeout=timeout).read().splitlines()\n        except Exception:\n            ret['msg'] = 'Failed to create HTTP request'\n\n    if not ret['msg'][0].startswith('OK'):\n        ret['res'] = False\n\n    return ret", "output": "A private function used to issue the command to tomcat via the manager\n    webapp\n\n    cmd\n        the command to execute\n\n    url\n        The URL of the server manager webapp (example:\n        http://localhost:8080/manager)\n\n    opts\n        a dict of arguments\n\n    timeout\n        timeout for HTTP request\n\n    Return value is a dict in the from of::\n\n        {\n            res: [True|False]\n            msg: list of lines we got back from the manager\n        }", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _linux_brshow(br=None):\n    '''\n    \n    '''\n    brctl = _tool_path('brctl')\n\n    if br:\n        cmd = '{0} show {1}'.format(brctl, br)\n    else:\n        cmd = '{0} show'.format(brctl)\n\n    brs = {}\n\n    for line in __salt__['cmd.run'](cmd, python_shell=False).splitlines():\n        # get rid of first line\n        if line.startswith('bridge name'):\n            continue\n        # get rid of ^\\n's\n        vals = line.split()\n        if not vals:\n            continue\n\n        # bridge name bridge id       STP enabled interfaces\n        # br0       8000.e4115bac8ddc   no      eth0\n        #                                       foo0\n        # br1       8000.e4115bac8ddc   no      eth1\n        if len(vals) > 1:\n            brname = vals[0]\n\n            brs[brname] = {\n                'id': vals[1],\n                'stp': vals[2],\n            }\n            if len(vals) > 3:\n                brs[brname]['interfaces'] = [vals[3]]\n\n        if len(vals) == 1 and brname:\n            brs[brname]['interfaces'].append(vals[0])\n\n    if br:\n        try:\n            return brs[br]\n        except KeyError:\n            return None\n    return brs", "output": "Internal, returns bridges and enslaved interfaces (GNU/Linux - brctl)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def count(self):\n        \"\"\"\n        \n        \"\"\"\n        Statement = self.get_model('statement')\n\n        session = self.Session()\n        statement_count = session.query(Statement).count()\n        session.close()\n        return statement_count", "output": "Return the number of entries in the database.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lf (self):\n        '''\n        '''\n\n        old_r = self.cur_r\n        self.cursor_down()\n        if old_r == self.cur_r:\n            self.scroll_up ()\n            self.erase_line()", "output": "This moves the cursor down with scrolling.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_resources(resource, name=None, resource_id=None, tags=None,\n                    region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n\n    if all((resource_id, name)):\n        raise SaltInvocationError('Only one of name or id may be '\n                                  'provided.')\n\n    if not any((resource_id, name, tags)):\n        raise SaltInvocationError('At least one of the following must be '\n                                  'provided: id, name, or tags.')\n\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    f = 'get_all_{0}'.format(resource)\n    if not f.endswith('s'):\n        f = f + 's'\n    get_resources = getattr(conn, f)\n\n    filter_parameters = {}\n    if name:\n        filter_parameters['filters'] = {'tag:Name': name}\n    if resource_id:\n        filter_parameters['{0}_ids'.format(resource)] = resource_id\n    if tags:\n        for tag_name, tag_value in six.iteritems(tags):\n            filter_parameters['filters']['tag:{0}'.format(tag_name)] = tag_value\n\n    try:\n        r = get_resources(**filter_parameters)\n    except BotoServerError as e:\n        if e.code.endswith('.NotFound'):\n            return None\n        raise\n    return r", "output": "Get VPC resources based on resource type and name, id, or tags.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deployment_group(self, function_logical_id):\n        \"\"\"\n        \n        \"\"\"\n        deployment_preference = self.get(function_logical_id)\n\n        deployment_group = CodeDeployDeploymentGroup(self.deployment_group_logical_id(function_logical_id))\n\n        if deployment_preference.alarms is not None:\n            deployment_group.AlarmConfiguration = {'Enabled': True,\n                                                   'Alarms': [{'Name': alarm} for alarm in\n                                                              deployment_preference.alarms]}\n\n        deployment_group.ApplicationName = self.codedeploy_application.get_runtime_attr('name')\n        deployment_group.AutoRollbackConfiguration = {'Enabled': True,\n                                                      'Events': ['DEPLOYMENT_FAILURE',\n                                                                 'DEPLOYMENT_STOP_ON_ALARM',\n                                                                 'DEPLOYMENT_STOP_ON_REQUEST']}\n        deployment_group.DeploymentConfigName = fnSub(\"CodeDeployDefault.Lambda${ConfigName}\",\n                                                      {\"ConfigName\": deployment_preference.deployment_type})\n        deployment_group.DeploymentStyle = {'DeploymentType': 'BLUE_GREEN',\n                                            'DeploymentOption': 'WITH_TRAFFIC_CONTROL'}\n\n        deployment_group.ServiceRoleArn = self.codedeploy_iam_role.get_runtime_attr(\"arn\")\n        if deployment_preference.role:\n            deployment_group.ServiceRoleArn = deployment_preference.role\n\n        return deployment_group", "output": ":param function_logical_id: logical_id of the function this deployment group belongs to\n        :return: CodeDeployDeploymentGroup resource", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cleanup_keys_with_confirmation(self, keys_to_delete):\n    \"\"\"\n    \"\"\"\n    print('Round name: ', self.round_name)\n    print('Number of entities to be deleted: ', len(keys_to_delete))\n    if not keys_to_delete:\n      return\n    if self.verbose:\n      print('Entities to delete:')\n      idx = 0\n      prev_key_prefix = None\n      dots_printed_after_same_prefix = False\n      for k in keys_to_delete:\n        if idx >= 20:\n          print('   ...')\n          print('   ...')\n          break\n        key_prefix = (k.flat_path[0:1]\n                      if k.flat_path[0] in [u'SubmissionType', u'WorkType']\n                      else k.flat_path[0])\n        if prev_key_prefix == key_prefix:\n          if not dots_printed_after_same_prefix:\n            print('   ...')\n          dots_printed_after_same_prefix = True\n        else:\n          print('  ', k)\n          dots_printed_after_same_prefix = False\n          idx += 1\n        prev_key_prefix = key_prefix\n    print()\n    inp = input_str('Are you sure? (type \"yes\" without quotes to confirm): ')\n    if inp != 'yes':\n      return\n    with self.datastore_client.no_transact_batch() as batch:\n      for k in keys_to_delete:\n        batch.delete(k)\n    print('Data deleted')", "output": "Asks confirmation and then deletes entries with keys.\n\n    Args:\n      keys_to_delete: list of datastore keys for which entries should be deleted", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sendall(self, s):\n        \"\"\"\n        \n        \"\"\"\n        while s:\n            sent = self.send(s)\n            s = s[sent:]\n        return None", "output": "Send data to the channel, without allowing partial results.  Unlike\n        `send`, this method continues to send data from the given string until\n        either all data has been sent or an error occurs.  Nothing is returned.\n\n        :param str s: data to send.\n\n        :raises socket.timeout:\n            if sending stalled for longer than the timeout set by `settimeout`.\n        :raises socket.error:\n            if an error occurred before the entire string was sent.\n\n        .. note::\n            If the channel is closed while only part of the data has been\n            sent, there is no way to determine how much data (if any) was sent.\n            This is irritating, but identically follows Python's API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _flatten_entities(self) -> Tuple[List[str], numpy.ndarray]:\n        \"\"\"\n        \n        \"\"\"\n        entities = []\n        linking_scores = []\n        for entity in sorted(self.linked_entities['number']):\n            entities.append(entity)\n            linking_scores.append(self.linked_entities['number'][entity][2])\n\n        for entity in sorted(self.linked_entities['string']):\n            entities.append(entity)\n            linking_scores.append(self.linked_entities['string'][entity][2])\n\n        return entities, numpy.array(linking_scores)", "output": "When we first get the entities and the linking scores in ``_get_linked_entities``\n        we represent as dictionaries for easier updates to the grammar and valid actions.\n        In this method, we flatten them for the model so that the entities are represented as\n        a list, and the linking scores are a 2D numpy array of shape (num_entities, num_utterance_tokens).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_dict(self, get_value=None):\n        \"\"\"\"\"\"\n        result = {}\n        for key, value in iteritems(self):\n            if isinstance(value, BaseCounter):\n                if get_value is not None:\n                    value = getattr(value, get_value)\n                result[key] = value\n            else:\n                result[key] = value.to_dict(get_value)\n        return result", "output": "Dump counters as a dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _protobuf_value_type(value):\n  \"\"\"\n  \"\"\"\n  if value.HasField(\"number_value\"):\n    return api_pb2.DATA_TYPE_FLOAT64\n  if value.HasField(\"string_value\"):\n    return api_pb2.DATA_TYPE_STRING\n  if value.HasField(\"bool_value\"):\n    return api_pb2.DATA_TYPE_BOOL\n  return None", "output": "Returns the type of the google.protobuf.Value message as an api.DataType.\n\n  Returns None if the type of 'value' is not one of the types supported in\n  api_pb2.DataType.\n\n  Args:\n    value: google.protobuf.Value message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def yaml_parse(yamlstr):\n    \"\"\"\"\"\"\n    try:\n        # PyYAML doesn't support json as well as it should, so if the input\n        # is actually just json it is better to parse it with the standard\n        # json parser.\n        return json.loads(yamlstr)\n    except ValueError:\n        yaml.SafeLoader.add_multi_constructor(\"!\", intrinsics_multi_constructor)\n        return yaml.safe_load(yamlstr)", "output": "Parse a yaml string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _akima_interpolate(xi, yi, x, der=0, axis=0):\n    \"\"\"\n    \n\n    \"\"\"\n    from scipy import interpolate\n    try:\n        P = interpolate.Akima1DInterpolator(xi, yi, axis=axis)\n    except TypeError:\n        # Scipy earlier than 0.17.0 missing axis\n        P = interpolate.Akima1DInterpolator(xi, yi)\n    if der == 0:\n        return P(x)\n    elif interpolate._isscalar(der):\n        return P(x, der=der)\n    else:\n        return [P(x, nu) for nu in der]", "output": "Convenience function for akima interpolation.\n    xi and yi are arrays of values used to approximate some function f,\n    with ``yi = f(xi)``.\n\n    See `Akima1DInterpolator` for details.\n\n    Parameters\n    ----------\n    xi : array_like\n        A sorted list of x-coordinates, of length N.\n    yi :  array_like\n        A 1-D array of real values.  `yi`'s length along the interpolation\n        axis must be equal to the length of `xi`. If N-D array, use axis\n        parameter to select correct axis.\n    x : scalar or array_like\n        Of length M.\n    der : int or list, optional\n        How many derivatives to extract; None for all potentially\n        nonzero derivatives (that is a number equal to the number\n        of points), or a list of derivatives to extract. This number\n        includes the function value as 0th derivative.\n    axis : int, optional\n        Axis in the yi array corresponding to the x-coordinate values.\n\n    See Also\n    --------\n    scipy.interpolate.Akima1DInterpolator\n\n    Returns\n    -------\n    y : scalar or array_like\n        The result, of length R or length M or M by R,", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pad_with_zeros(logits, labels):\n  \"\"\"\"\"\"\n  with tf.name_scope(\"pad_with_zeros\", values=[logits, labels]):\n    logits, labels = pad_to_same_length(logits, labels)\n    if len(labels.shape) == 3:  # 2-d labels.\n      logits, labels = pad_to_same_length(logits, labels, axis=2)\n    return logits, labels", "output": "Pad labels on the length dimension to match logits length.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _download_mlu_data(tmp_dir, data_dir):\n  \"\"\"\n  \"\"\"\n  if not tf.gfile.Exists(data_dir):\n    tf.gfile.MakeDirs(data_dir)\n\n  filename = os.path.basename(_URL)\n  file_path = os.path.join(tmp_dir, filename)\n  headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) \"\n                           \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n                           \"Chrome/63.0.3239.132 Safari/537.36\"}\n  resp = requests.get(_URL, headers=headers)\n  with open(file_path, \"wb\") as f:\n    f.write(resp.content)\n\n  with tarfile.open(file_path, \"r:gz\") as tar:\n    tar.extractall(tmp_dir)\n\n  return tmp_dir", "output": "Downloads and extracts the dataset.\n\n  Args:\n    tmp_dir: temp directory to download and extract the dataset\n    data_dir: The base directory where data and vocab files are stored.\n\n  Returns:\n    tmp_dir: temp directory containing the raw data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stats(self):\n        \"\"\"\n        \"\"\"\n        ret = '{name}:\\n' \\\n            '  sample_num={sample_num}, batch_num={batch_num}\\n' \\\n            '  key={bucket_keys}\\n' \\\n            '  cnt={bucket_counts}\\n' \\\n            '  batch_size={bucket_batch_sizes}'\\\n            .format(name=self.__class__.__name__,\n                    sample_num=len(self._lengths),\n                    batch_num=len(self._batch_infos),\n                    bucket_keys=self._bucket_keys,\n                    bucket_counts=[len(sample_ids) for sample_ids in self._bucket_sample_ids],\n                    bucket_batch_sizes=self._bucket_batch_sizes)\n        return ret", "output": "Return a string representing the statistics of the bucketing sampler.\n\n        Returns\n        -------\n        ret : str\n            String representing the statistics of the buckets.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getInstance(self):\n        \"\"\"\n        \n\n        \"\"\"\n        try:\n            return self._instance\n        except AttributeError:\n            self._instance = self._decorated()\n            return self._instance", "output": "Returns the singleton instance. Upon its first call, it creates a\n        new instance of the decorated class and calls its `__init__` method.\n        On all subsequent calls, the already created instance is returned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _inferSchemaFromList(self, data, names=None):\n        \"\"\"\n        \n        \"\"\"\n        if not data:\n            raise ValueError(\"can not infer schema from empty dataset\")\n        first = data[0]\n        if type(first) is dict:\n            warnings.warn(\"inferring schema from dict is deprecated,\"\n                          \"please use pyspark.sql.Row instead\")\n        schema = reduce(_merge_type, (_infer_schema(row, names) for row in data))\n        if _has_nulltype(schema):\n            raise ValueError(\"Some of types cannot be determined after inferring\")\n        return schema", "output": "Infer schema from list of Row or tuple.\n\n        :param data: list of Row or tuple\n        :param names: list of column names\n        :return: :class:`pyspark.sql.types.StructType`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_extension_arg(arg, arg_dict):\n    \"\"\"\n    \n    \"\"\"\n\n    match = re.match(r'^(([^\\d\\W]\\w*)(\\.[^\\d\\W]\\w*)*)=(.*)$', arg)\n    if match is None:\n        raise ValueError(\n            \"invalid extension argument '%s', must be in key=value form\" % arg\n        )\n\n    name = match.group(1)\n    value = match.group(4)\n    arg_dict[name] = value", "output": "Converts argument strings in key=value or key.namespace=value form\n    to dictionary entries\n\n    Parameters\n    ----------\n    arg : str\n        The argument string to parse, which must be in key=value or\n        key.namespace=value form.\n    arg_dict : dict\n        The dictionary into which the key/value pair will be added", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _access_rule(method,\n                ip=None,\n                port=None,\n                proto='tcp',\n                direction='in',\n                port_origin='d',\n                ip_origin='d',\n                comment=''):\n    '''\n    \n    '''\n    if _status_csf():\n        if ip is None:\n            return {'error': 'You must supply an ip address or CIDR.'}\n        if port is None:\n            args = _build_args(method, ip, comment)\n            return __csf_cmd(args)\n        else:\n            if method not in ['allow', 'deny']:\n                return {'error': 'Only allow and deny rules are allowed when specifying a port.'}\n            return _access_rule_with_port(method=method,\n                                            ip=ip,\n                                            port=port,\n                                            proto=proto,\n                                            direction=direction,\n                                            port_origin=port_origin,\n                                            ip_origin=ip_origin,\n                                            comment=comment)", "output": "Handles the cmd execution for allow and deny commands.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_error(self, request, client_address):\n    \"\"\"\"\"\"\n    del request  # unused\n    # Kludge to override a SocketServer.py method so we can get rid of noisy\n    # EPIPE errors. They're kind of a red herring as far as errors go. For\n    # example, `curl -N http://localhost:6006/ | head` will cause an EPIPE.\n    exc_info = sys.exc_info()\n    e = exc_info[1]\n    if isinstance(e, IOError) and e.errno == errno.EPIPE:\n      logger.warn('EPIPE caused by %s in HTTP serving' % str(client_address))\n    else:\n      logger.error('HTTP serving error', exc_info=exc_info)", "output": "Override to get rid of noisy EPIPE errors.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def document(self, document_id=None):\n        \"\"\"\n        \"\"\"\n        if document_id is None:\n            document_id = _auto_id()\n\n        child_path = self._path + (document_id,)\n        return self._client.document(*child_path)", "output": "Create a sub-document underneath the current collection.\n\n        Args:\n            document_id (Optional[str]): The document identifier\n                within the current collection. If not provided, will default\n                to a random 20 character string composed of digits,\n                uppercase and lowercase and letters.\n\n        Returns:\n            ~.firestore_v1beta1.document.DocumentReference: The child\n            document.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_sd_auth(val, sd_auth_pillar_name='serverdensity'):\n    '''\n    \n    '''\n    sd_pillar = __pillar__.get(sd_auth_pillar_name)\n    log.debug('Server Density Pillar: %s', sd_pillar)\n    if not sd_pillar:\n        log.error('Could not load %s pillar', sd_auth_pillar_name)\n        raise CommandExecutionError(\n            '{0} pillar is required for authentication'.format(sd_auth_pillar_name)\n        )\n\n    try:\n        return sd_pillar[val]\n    except KeyError:\n        log.error('Could not find value %s in pillar', val)\n        raise CommandExecutionError('{0} value was not found in pillar'.format(val))", "output": "Returns requested Server Density authentication value from pillar.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' serverdensity_device.get_sd_auth <val>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _obtain_sampled_health_pills(self, run, node_names):\n    \"\"\"\n    \"\"\"\n    runs_to_tags_to_content = self._event_multiplexer.PluginRunToTagToContent(\n        constants.DEBUGGER_PLUGIN_NAME)\n\n    if run not in runs_to_tags_to_content:\n      # The run lacks health pills.\n      return {}\n\n    # This is also a mapping between node name and plugin content because this\n    # plugin tags by node name.\n    tags_to_content = runs_to_tags_to_content[run]\n\n    mapping = {}\n    for node_name in node_names:\n      if node_name not in tags_to_content:\n        # This node lacks health pill data.\n        continue\n\n      health_pills = []\n      for tensor_event in self._event_multiplexer.Tensors(run, node_name):\n        json_string = tags_to_content[node_name]\n        try:\n          content_object = json.loads(tf.compat.as_text(json_string))\n          device_name = content_object['device']\n          output_slot = content_object['outputSlot']\n          health_pills.append(\n              self._tensor_proto_to_health_pill(tensor_event, node_name,\n                                                device_name, output_slot))\n        except (KeyError, ValueError) as e:\n          logger.error('Could not determine device from JSON string '\n                           '%r: %r', json_string, e)\n\n      mapping[node_name] = health_pills\n\n    return mapping", "output": "Obtains the health pills for a run sampled by the event multiplexer.\n\n    This is much faster than the alternative path of reading health pills from\n    disk.\n\n    Args:\n      run: The run to fetch health pills for.\n      node_names: A list of node names for which to retrieve health pills.\n\n    Returns:\n      A dictionary mapping from node name to a list of\n      event_accumulator.HealthPillEvents.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def items(self):\n        \"\"\"\n        \"\"\"\n        for key, index in six.iteritems(self._xxx_field_to_index):\n            yield (key, copy.deepcopy(self._xxx_values[index]))", "output": "Return items as ``(key, value)`` pairs.\n\n        Returns:\n            Iterable[Tuple[str, object]]:\n                The ``(key, value)`` pairs representing this row.\n\n        Examples:\n\n            >>> list(Row(('a', 'b'), {'x': 0, 'y': 1}).items())\n            [('x', 'a'), ('y', 'b')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def request_url(self, request, proxies):\n        \"\"\"\n        \"\"\"\n        proxy = select_proxy(request.url, proxies)\n        scheme = urlparse(request.url).scheme\n\n        is_proxied_http_request = (proxy and scheme != 'https')\n        using_socks_proxy = False\n        if proxy:\n            proxy_scheme = urlparse(proxy).scheme.lower()\n            using_socks_proxy = proxy_scheme.startswith('socks')\n\n        url = request.path_url\n        if is_proxied_http_request and not using_socks_proxy:\n            url = urldefragauth(request.url)\n\n        return url", "output": "Obtain the url to use when making the final request.\n\n        If the message is being sent through a HTTP proxy, the full URL has to\n        be used. Otherwise, we should only use the path portion of the URL.\n\n        This should not be called from user code, and is only exposed for use\n        when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\n        :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs.\n        :rtype: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def column_types(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.__type__ == VERTEX_GFRAME:\n            return self.__graph__.__proxy__.get_vertex_field_types()\n        elif self.__type__ == EDGE_GFRAME:\n            return self.__graph__.__proxy__.get_edge_field_types()", "output": "Returns the column types.\n\n        Returns\n        -------\n        out : list[type]\n            Column types of the SFrame.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_logger(cls, log_level):\n        \"\"\"\"\"\"\n        logger = logging.getLogger(\"AutoMLBoard\")\n        handler = logging.StreamHandler()\n        formatter = logging.Formatter(\"[%(levelname)s %(asctime)s] \"\n                                      \"%(filename)s: %(lineno)d  \"\n                                      \"%(message)s\")\n        handler.setFormatter(formatter)\n        logger.setLevel(log_level)\n        logger.addHandler(handler)\n        return logger", "output": "Initialize logger settings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_random_int(minimum, maximum):\n    \"\"\"\n    \n    \"\"\"\n    min_int = math.ceil(minimum)\n    max_int = math.floor(maximum)\n\n    return random.randint(min_int, max_int - 1)", "output": "Returns a random integer between min (included) and max (excluded)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _priority_from_env(self, val):\n        \"\"\"\"\"\"\n        for part in val.split(':'):\n            try:\n                rule, priority = part.split('=')\n                yield rule, int(priority)\n            except ValueError:\n                continue", "output": "Gets priority pairs from env.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def num_feature(self):\n        \"\"\"\n        \"\"\"\n        out_num_feature = ctypes.c_int(0)\n        _safe_call(_LIB.LGBM_BoosterGetNumFeature(\n            self.handle,\n            ctypes.byref(out_num_feature)))\n        return out_num_feature.value", "output": "Get number of features.\n\n        Returns\n        -------\n        num_feature : int\n            The number of features.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imagetransformer2d_base_8l_8_32_big():\n  \"\"\"\"\"\"\n  hparams = image_transformer2d_base()\n  hparams.num_heads = 16\n  hparams.hidden_size = 1024\n  hparams.filter_size = 2048\n  hparams.num_decoder_layers = 8\n  hparams.batch_size = 1\n  hparams.layer_prepostprocess_dropout = 0.3\n  hparams.query_shape = (8, 16)\n  hparams.memory_flange = (0, 32)\n  hparams.unconditional = int(False)\n  return hparams", "output": "hparams fo 8 layer big 2d model for cifar 10.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _merge_sorted_items(self, index):\n        \"\"\"  \"\"\"\n        def load_partition(j):\n            path = self._get_spill_dir(j)\n            p = os.path.join(path, str(index))\n            with open(p, 'rb', 65536) as f:\n                for v in self.serializer.load_stream(f):\n                    yield v\n\n        disk_items = [load_partition(j) for j in range(self.spills)]\n\n        if self._sorted:\n            # all the partitions are already sorted\n            sorted_items = heapq.merge(disk_items, key=operator.itemgetter(0))\n\n        else:\n            # Flatten the combined values, so it will not consume huge\n            # memory during merging sort.\n            ser = self.flattened_serializer()\n            sorter = ExternalSorter(self.memory_limit, ser)\n            sorted_items = sorter.sorted(itertools.chain(*disk_items),\n                                         key=operator.itemgetter(0))\n        return ((k, vs) for k, vs in GroupByKey(sorted_items))", "output": "load a partition from disk, then sort and group by key", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def predict(self, dataset, missing_value_action='auto'):\n        \"\"\"\n        \n        \"\"\"\n        return super(DecisionTreeRegression, self).predict(dataset, output_type='margin',\n                                                           missing_value_action=missing_value_action)", "output": "Predict the target column of the given dataset.\n\n        The target column is provided during\n        :func:`~turicreate.decision_tree_regression.create`. If the target column is in the\n        `dataset` it will be ignored.\n\n        Parameters\n        ----------\n        dataset : SFrame\n          A dataset that has the same columns that were used during training.\n          If the target column exists in ``dataset`` it will be ignored\n          while making predictions.\n\n        missing_value_action : str, optional\n            Action to perform when missing values are encountered. Can be\n            one of:\n\n            - 'auto': By default the model will treat missing value as is.\n            - 'impute': Proceed with evaluation by filling in the missing\n              values with the mean of the training data. Missing\n              values are also imputed if an entire column of data is\n              missing during evaluation.\n            - 'error': Do not proceed with evaluation and terminate with\n              an error message.\n\n        Returns\n        -------\n        out : SArray\n           Predicted target value for each example (i.e. row) in the dataset.\n\n        See Also\n        ----------\n        create, predict\n\n        Examples\n        --------\n        >>> m.predict(testdata)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sizes(x):\n  \"\"\"\"\"\"\n  def size(x):\n    try:\n      return x.size\n    except Exception:  # pylint: disable=broad-except\n      return 0\n  return nested_map(x, size)", "output": "Get a structure of sizes for a structure of nested arrays.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shake_shake_branch(x, output_filters, stride, rand_forward, rand_backward,\n                       hparams):\n  \"\"\"\"\"\"\n  is_training = hparams.mode == tf.estimator.ModeKeys.TRAIN\n  x = tf.nn.relu(x)\n  x = tf.layers.conv2d(\n      x,\n      output_filters, (3, 3),\n      strides=(stride, stride),\n      padding=\"SAME\",\n      name=\"conv1\")\n  x = tf.layers.batch_normalization(x, training=is_training, name=\"bn1\")\n  x = tf.nn.relu(x)\n  x = tf.layers.conv2d(x, output_filters, (3, 3), padding=\"SAME\", name=\"conv2\")\n  x = tf.layers.batch_normalization(x, training=is_training, name=\"bn2\")\n  if is_training:\n    x = x * rand_backward + tf.stop_gradient(x * rand_forward -\n                                             x * rand_backward)\n  else:\n    x *= 1.0 / hparams.shake_shake_num_branches\n  return x", "output": "Building a 2 branching convnet.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_network(self):\n        '''\n        \n        '''\n        data = dict()\n        data['interfaces'] = salt.utils.network.interfaces()\n        data['subnets'] = salt.utils.network.subnets()\n\n        return data", "output": "Get network configuration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(table_name, throughput=None, global_indexes=None,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    table = Table(table_name, connection=conn)\n    return table.update(throughput=throughput, global_indexes=global_indexes)", "output": "Update a DynamoDB table.\n\n    CLI example::\n\n        salt myminion boto_dynamodb.update table_name region=us-east-1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cancelled(self):\n        \"\"\"\"\"\"\n        self._refresh_and_update()\n        return (\n            self._operation.HasField(\"error\")\n            and self._operation.error.code == code_pb2.CANCELLED\n        )", "output": "True if the operation was cancelled.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unmatched_quotes_in_line(text):\n    \"\"\"\n    \"\"\"\n    # We check \" first, then ', so complex cases with nested quotes will\n    # get the \" to take precedence.\n    text = text.replace(\"\\\\'\", \"\")\n    text = text.replace('\\\\\"', '')\n    if text.count('\"') % 2:\n        return '\"'\n    elif text.count(\"'\") % 2:\n        return \"'\"\n    else:\n        return ''", "output": "Return whether a string has open quotes.\n\n    This simply counts whether the number of quote characters of either\n    type in the string is odd.\n\n    Take from the IPython project (in IPython/core/completer.py in v0.13)\n    Spyder team: Add some changes to deal with escaped quotes\n\n    - Copyright (C) 2008-2011 IPython Development Team\n    - Copyright (C) 2001-2007 Fernando Perez. <fperez@colorado.edu>\n    - Copyright (C) 2001 Python Software Foundation, www.python.org\n\n    Distributed under the terms of the BSD License.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_next(self, ptype, m):\n        \"\"\"\n        \n        \"\"\"\n        if ptype == MSG_KEXGSS_GROUPREQ:\n            return self._parse_kexgss_groupreq(m)\n        elif ptype == MSG_KEXGSS_GROUP:\n            return self._parse_kexgss_group(m)\n        elif ptype == MSG_KEXGSS_INIT:\n            return self._parse_kexgss_gex_init(m)\n        elif ptype == MSG_KEXGSS_HOSTKEY:\n            return self._parse_kexgss_hostkey(m)\n        elif ptype == MSG_KEXGSS_CONTINUE:\n            return self._parse_kexgss_continue(m)\n        elif ptype == MSG_KEXGSS_COMPLETE:\n            return self._parse_kexgss_complete(m)\n        elif ptype == MSG_KEXGSS_ERROR:\n            return self._parse_kexgss_error(m)\n        msg = \"KexGex asked to handle packet type {:d}\"\n        raise SSHException(msg.format(ptype))", "output": "Parse the next packet.\n\n        :param ptype: The (string) type of the incoming packet\n        :param `.Message` m: The paket content", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def openconfig_lacp(device_name=None):\n    '''\n    \n    '''\n    oc_lacp = {}\n    interfaces = get_interfaces(device_name=device_name)\n    for interface in interfaces:\n        if not interface['lag']:\n            continue\n        if_name, if_unit = _if_name_unit(interface['name'])\n        parent_if = interface['lag']['name']\n        if parent_if not in oc_lacp:\n            oc_lacp[parent_if] = {\n                'config': {\n                    'name': parent_if,\n                    'interval': 'SLOW',\n                    'lacp_mode': 'ACTIVE'\n                },\n                'members': {\n                    'member': {}\n                }\n            }\n        oc_lacp[parent_if]['members']['member'][if_name] = {}\n    return {\n        'lacp': {\n            'interfaces': {\n                'interface': oc_lacp\n            }\n        }\n    }", "output": ".. versionadded:: 2019.2.0\n\n    Return a dictionary structured as standardised in the\n    `openconfig-lacp <http://ops.openconfig.net/branches/master/openconfig-lacp.html>`_\n    YANG model, with configuration data for Link Aggregation Control Protocol\n    (LACP) for aggregate interfaces.\n\n    .. note::\n        The ``interval`` and ``lacp_mode`` keys have the values set as ``SLOW``\n        and ``ACTIVE`` respectively, as this data is not currently available\n        in Netbox, therefore defaulting to the values defined in the standard.\n        See `interval <http://ops.openconfig.net/branches/master/docs/openconfig-lacp.html#lacp-interfaces-interface-config-interval>`_\n        and `lacp-mode <http://ops.openconfig.net/branches/master/docs/openconfig-lacp.html#lacp-interfaces-interface-config-lacp-mode>`_\n        for further details.\n\n    device_name: ``None``\n        The name of the device to query the LACP information for. If not provided,\n        will use the Minion ID.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' netbox.openconfig_lacp\n        salt '*' netbox.openconfig_lacp device_name=cr1.thn.lon", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def MessageToString(message,\n                    as_utf8=False,\n                    as_one_line=False,\n                    pointy_brackets=False,\n                    use_index_order=False,\n                    float_format=None,\n                    use_field_number=False,\n                    descriptor_pool=None,\n                    indent=0):\n  \"\"\"\n  \"\"\"\n  out = TextWriter(as_utf8)\n  printer = _Printer(out, indent, as_utf8, as_one_line, pointy_brackets,\n                     use_index_order, float_format, use_field_number,\n                     descriptor_pool)\n  printer.PrintMessage(message)\n  result = out.getvalue()\n  out.close()\n  if as_one_line:\n    return result.rstrip()\n  return result", "output": "Convert protobuf message to text format.\n\n  Floating point values can be formatted compactly with 15 digits of\n  precision (which is the most that IEEE 754 \"double\" can guarantee)\n  using float_format='.15g'. To ensure that converting to text and back to a\n  proto will result in an identical value, float_format='.17g' should be used.\n\n  Args:\n    message: The protocol buffers message.\n    as_utf8: Produce text output in UTF8 format.\n    as_one_line: Don't introduce newlines between fields.\n    pointy_brackets: If True, use angle brackets instead of curly braces for\n      nesting.\n    use_index_order: If True, print fields of a proto message using the order\n      defined in source code instead of the field number. By default, use the\n      field number order.\n    float_format: If set, use this to specify floating point number formatting\n      (per the \"Format Specification Mini-Language\"); otherwise, str() is used.\n    use_field_number: If True, print field numbers instead of names.\n    descriptor_pool: A DescriptorPool used to resolve Any types.\n    indent: The indent level, in terms of spaces, for pretty print.\n\n  Returns:\n    A string of the text formatted protocol buffer message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy(self):\r\n        \"\"\"\"\"\"\r\n        if self.has_selected_text():\r\n            ConsoleBaseWidget.copy(self)\r\n        elif not sys.platform == 'darwin':\r\n            self.interrupt()", "output": "Copy text to clipboard... or keyboard interrupt", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_output_shapes(self):\n        \"\"\"\"\"\"\n        outputs = self.execs[0].outputs\n        shapes = [out.shape for out in outputs]\n\n        concat_shapes = []\n        for key, the_shape, axis in zip(self.symbol.list_outputs(), shapes, self.output_layouts):\n            the_shape = list(the_shape)\n            if axis >= 0:\n                the_shape[axis] = self.batch_size\n            concat_shapes.append((key, tuple(the_shape)))\n        return concat_shapes", "output": "Get the shapes of the outputs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _forward_kernel(self, F, inputs, states, **kwargs):\n        \"\"\" \"\"\"\n        if self._layout == 'NTC':\n            inputs = F.swapaxes(inputs, dim1=0, dim2=1)\n        if self._projection_size is None:\n            params = (kwargs['{}{}_{}_{}'.format(d, l, g, t)].reshape(-1)\n                      for t in ['weight', 'bias']\n                      for l in range(self._num_layers)\n                      for d in ['l', 'r'][:self._dir]\n                      for g in ['i2h', 'h2h'])\n        else:\n            params = (kwargs['{}{}_{}_{}'.format(d, l, g, t)].reshape(-1)\n                      for t in ['weight', 'bias']\n                      for l in range(self._num_layers)\n                      for d in ['l', 'r'][:self._dir]\n                      for g in ['i2h', 'h2h', 'h2r']\n                      if g != 'h2r' or t != 'bias')\n\n        params = F._internal._rnn_param_concat(*params, dim=0)\n\n        rnn = F.RNN(inputs, params, *states, state_size=self._hidden_size,\n                    projection_size=self._projection_size,\n                    num_layers=self._num_layers, bidirectional=self._dir == 2,\n                    p=self._dropout, state_outputs=True, mode=self._mode,\n                    lstm_state_clip_min=self._lstm_state_clip_min,\n                    lstm_state_clip_max=self._lstm_state_clip_max,\n                    lstm_state_clip_nan=self._lstm_state_clip_nan)\n\n        if self._mode == 'lstm':\n            outputs, states = rnn[0], [rnn[1], rnn[2]]\n        else:\n            outputs, states = rnn[0], [rnn[1]]\n\n        if self._layout == 'NTC':\n            outputs = F.swapaxes(outputs, dim1=0, dim2=1)\n\n        return outputs, states", "output": "forward using CUDNN or CPU kenrel", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_deployment(name,\n                       metadata,\n                       spec,\n                       source,\n                       template,\n                       saltenv,\n                       namespace='default',\n                       **kwargs):\n    '''\n    \n    '''\n    body = __create_object_body(\n        kind='Deployment',\n        obj_class=AppsV1beta1Deployment,\n        spec_creator=__dict_to_deployment_spec,\n        name=name,\n        namespace=namespace,\n        metadata=metadata,\n        spec=spec,\n        source=source,\n        template=template,\n        saltenv=saltenv)\n\n    cfg = _setup_conn(**kwargs)\n\n    try:\n        api_instance = kubernetes.client.ExtensionsV1beta1Api()\n        api_response = api_instance.replace_namespaced_deployment(\n            name, namespace, body)\n\n        return api_response.to_dict()\n    except (ApiException, HTTPError) as exc:\n        if isinstance(exc, ApiException) and exc.status == 404:\n            return None\n        else:\n            log.exception(\n                'Exception when calling '\n                'ExtensionsV1beta1Api->replace_namespaced_deployment'\n            )\n            raise CommandExecutionError(exc)\n    finally:\n        _cleanup(**cfg)", "output": "Replaces an existing deployment with a new one defined by name and\n    namespace, having the specificed metadata and spec.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def verify_signature(self, signature_filename, data_filename,\n                         keystore=None):\n        \"\"\"\n        \n        \"\"\"\n        if not self.gpg:\n            raise DistlibException('verification unavailable because gpg '\n                                   'unavailable')\n        cmd = self.get_verify_command(signature_filename, data_filename,\n                                      keystore)\n        rc, stdout, stderr = self.run_command(cmd)\n        if rc not in (0, 1):\n            raise DistlibException('verify command failed with error '\n                             'code %s' % rc)\n        return rc == 0", "output": "Verify a signature for a file.\n\n        :param signature_filename: The pathname to the file containing the\n                                   signature.\n        :param data_filename: The pathname to the file containing the\n                              signed data.\n        :param keystore: The path to a directory which contains the keys\n                         used in verification. If not specified, the\n                         instance's ``gpg_home`` attribute is used instead.\n        :return: True if the signature was verified, else False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _mkdirs_impacket(path, share='C$', conn=None, host=None, username=None, password=None):\n    '''\n    \n    '''\n    if conn is None:\n        conn = get_conn(host, username, password)\n\n    if conn is False:\n        return False\n\n    comps = path.split('/')\n    pos = 1\n    for comp in comps:\n        cwd = '\\\\'.join(comps[0:pos])\n        try:\n            conn.listPath(share, cwd)\n        except (smbSessionError, smb3SessionError):\n            log.exception('Encountered error running conn.listPath')\n            conn.createDirectory(share, cwd)\n        pos += 1", "output": "Recursively create a directory structure on an SMB share\n\n    Paths should be passed in with forward-slash delimiters, and should not\n    start with a forward-slash.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forget_xy(t):\n  \"\"\"\n  \"\"\"\n  shape = (t.shape[0], None, None, t.shape[3])\n  return tf.placeholder_with_default(t, shape)", "output": "Ignore sizes of dimensions (1, 2) of a 4d tensor in shape inference.\n\n  This allows using smaller input sizes, which create an invalid graph at higher\n  layers (for example because a spatial dimension becomes smaller than a conv\n  filter) when we only use early parts of it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_password(name, password):\n    '''\n    \n    '''\n    cmd = \"dscl . -passwd /Users/{0} '{1}'\".format(name, password)\n    try:\n        salt.utils.mac_utils.execute_return_success(cmd)\n    except CommandExecutionError as exc:\n        if 'eDSUnknownNodeName' in exc.strerror:\n            raise CommandExecutionError('User not found: {0}'.format(name))\n        raise CommandExecutionError('Unknown error: {0}'.format(exc.strerror))\n\n    return True", "output": "Set the password for a named user (insecure, the password will be in the\n    process list while the command is running)\n\n    :param str name: The name of the local user, which is assumed to be in the\n        local directory service\n\n    :param str password: The plaintext password to set\n\n    :return: True if successful, otherwise False\n    :rtype: bool\n\n    :raises: CommandExecutionError on user not found or any other unknown error\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mac_shadow.set_password macuser macpassword", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(name=None, pkgs=None, **kwargs):\n    '''\n    \n    '''\n    try:\n        pkg_params, pkg_type = __salt__['pkg_resource.parse_targets'](\n            name, pkgs\n        )\n    except MinionError as exc:\n        raise CommandExecutionError(exc)\n\n    if not pkg_params:\n        return {}\n\n    old = list_pkgs()\n    args = []\n\n    for param in pkg_params:\n        ver = old.get(param, [])\n        if not ver:\n            continue\n        if isinstance(ver, list):\n            args.extend(['{0}-{1}'.format(param, v) for v in ver])\n        else:\n            args.append('{0}-{1}'.format(param, ver))\n\n    if not args:\n        return {}\n\n    pkgin = _check_pkgin()\n    cmd = [pkgin, '-y', 'remove'] if pkgin else ['pkg_remove']\n    cmd.extend(args)\n\n    out = __salt__['cmd.run_all'](cmd, output_loglevel='trace')\n\n    if out['retcode'] != 0 and out['stderr']:\n        errors = [out['stderr']]\n    else:\n        errors = []\n\n    __context__.pop('pkg.list_pkgs', None)\n    new = list_pkgs()\n    ret = salt.utils.data.compare_dicts(old, new)\n\n    if errors:\n        raise CommandExecutionError(\n            'Problem encountered removing package(s)',\n            info={'errors': errors, 'changes': ret}\n        )\n\n    return ret", "output": "name\n        The name of the package to be deleted.\n\n\n    Multiple Package Options:\n\n    pkgs\n        A list of packages to delete. Must be passed as a python list. The\n        ``name`` parameter will be ignored if this option is passed.\n\n    .. versionadded:: 0.16.0\n\n\n    Returns a list containing the removed packages.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.remove <package name>\n        salt '*' pkg.remove <package1>,<package2>,<package3>\n        salt '*' pkg.remove pkgs='[\"foo\", \"bar\"]'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shrink(self, index, target, body=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        for param in (index, target):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"PUT\", _make_path(index, \"_shrink\", target), params=params, body=body\n        )", "output": "The shrink index API allows you to shrink an existing index into a new\n        index with fewer primary shards. The number of primary shards in the\n        target index must be a factor of the shards in the source index. For\n        example an index with 8 primary shards can be shrunk into 4, 2 or 1\n        primary shards or an index with 15 primary shards can be shrunk into 5,\n        3 or 1. If the number of shards in the index is a prime number it can\n        only be shrunk into a single primary shard. Before shrinking, a\n        (primary or replica) copy of every shard in the index must be present\n        on the same node.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-shrink-index.html>`_\n\n        :arg index: The name of the source index to shrink\n        :arg target: The name of the target index to shrink into\n        :arg body: The configuration for the target index (`settings` and\n            `aliases`)\n        :arg master_timeout: Specify timeout for connection to master\n        :arg request_timeout: Explicit operation timeout\n        :arg wait_for_active_shards: Set the number of active shards to wait for\n            on the shrunken index before the operation returns.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _decode_messages(self, messages):\n        '''\n        \n        '''\n        messages_len = len(messages)\n        # if it was one message, then its old style\n        if messages_len == 1:\n            payload = self.serial.loads(messages[0])\n        # 2 includes a header which says who should do it\n        elif messages_len == 2:\n            if (self.opts.get('__role') != 'syndic' and messages[0] not in ('broadcast', self.hexid)) or \\\n                (self.opts.get('__role') == 'syndic' and messages[0] not in ('broadcast', 'syndic')):\n                log.debug('Publish received for not this minion: %s', messages[0])\n                raise tornado.gen.Return(None)\n            payload = self.serial.loads(messages[1])\n        else:\n            raise Exception(('Invalid number of messages ({0}) in zeromq pub'\n                             'message from master').format(len(messages_len)))\n        # Yield control back to the caller. When the payload has been decoded, assign\n        # the decoded payload to 'ret' and resume operation\n        ret = yield self._decode_payload(payload)\n        raise tornado.gen.Return(ret)", "output": "Take the zmq messages, decrypt/decode them into a payload\n\n        :param list messages: A list of messages to be decoded", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_trace_api(client):\n    \"\"\"\n    \n    \"\"\"\n    generated = trace_service_client.TraceServiceClient(\n        credentials=client._credentials, client_info=_CLIENT_INFO\n    )\n    return _TraceAPI(generated, client)", "output": "Create an instance of the gapic Trace API.\n\n    Args:\n        client (~google.cloud.trace.client.Client): The client that holds\n            configuration details.\n\n    Returns:\n        A :class:`~google.cloud.trace._gapic._TraceAPI` instance with the\n        proper configurations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(queue, items):\n    '''\n    \n    '''\n    con = _conn(queue)\n    with con:\n        cur = con.cursor()\n        if isinstance(items, six.string_types):\n            items = _quote_escape(items)\n            cmd = \"\"\"DELETE FROM {0} WHERE name = '{1}'\"\"\".format(queue, items)\n            log.debug('SQL Query: %s', cmd)\n            cur.execute(cmd)\n            return True\n        if isinstance(items, list):\n            items = [_quote_escape(el) for el in items]\n            cmd = 'DELETE FROM {0} WHERE name = ?'.format(queue)\n            log.debug('SQL Query: %s', cmd)\n            newitems = []\n            for item in items:\n                newitems.append((item,))\n                # we need a list of one item tuples here\n            cur.executemany(cmd, newitems)\n        if isinstance(items, dict):\n            items = salt.utils.json.dumps(items).replace('\"', \"'\")\n            items = _quote_escape(items)\n            cmd = (\"\"\"DELETE FROM {0} WHERE name = '{1}'\"\"\").format(queue, items)  # future lint: disable=blacklisted-function\n            log.debug('SQL Query: %s', cmd)\n            cur.execute(cmd)\n            return True\n        return True", "output": "Delete an item or items from a queue", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def md5(self, path):\n        \"\"\"\n        \n        \"\"\"\n        uname = self.execute(\"uname\").strip()\n\n        command = {\n            \"Darwin\": \"md5 {}\".format(path),\n            \"Linux\": \"md5sum --tag {}\".format(path),\n        }.get(uname)\n\n        if not command:\n            raise DvcException(\n                \"'{uname}' is not supported as a remote\".format(uname=uname)\n            )\n\n        md5 = self.execute(command).split()[-1]\n        assert len(md5) == 32\n        return md5", "output": "Use different md5 commands depending on the OS:\n\n         - Darwin's `md5` returns BSD-style checksums by default\n         - Linux's `md5sum` needs the `--tag` flag for a similar output\n\n         Example:\n              MD5 (foo.txt) = f3d220a856b52aabbf294351e8a24300", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save(self, filename, strip_prefix=''):\n        \"\"\"\n        \"\"\"\n        arg_dict = {}\n        for param in self.values():\n            weight = param._reduce()\n            if not param.name.startswith(strip_prefix):\n                raise ValueError(\n                    \"Prefix '%s' is to be striped before saving, but Parameter's \"\n                    \"name '%s' does not start with '%s'. \"\n                    \"this may be due to your Block shares parameters from other \"\n                    \"Blocks or you forgot to use 'with name_scope()' when creating \"\n                    \"child blocks. For more info on naming, please see \"\n                    \"http://mxnet.incubator.apache.org/tutorials/basic/naming.html\"%(\n                        strip_prefix, param.name, strip_prefix))\n            arg_dict[param.name[len(strip_prefix):]] = weight\n        ndarray.save(filename, arg_dict)", "output": "Save parameters to file.\n\n        Parameters\n        ----------\n        filename : str\n            Path to parameter file.\n        strip_prefix : str, default ''\n            Strip prefix from parameter names before saving.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_path(self, path, method=None):\n        \"\"\"\n        \n        \"\"\"\n        method = self._normalize_method_name(method)\n\n        path_dict = self.paths.setdefault(path, {})\n\n        if not isinstance(path_dict, dict):\n            # Either customers has provided us an invalid Swagger, or this class has messed it somehow\n            raise InvalidDocumentException(\n                [InvalidTemplateException(\"Value of '{}' path must be a dictionary according to Swagger spec.\"\n                                          .format(path))])\n\n        if self._CONDITIONAL_IF in path_dict:\n            path_dict = path_dict[self._CONDITIONAL_IF][1]\n\n        path_dict.setdefault(method, {})", "output": "Adds the path/method combination to the Swagger, if not already present\n\n        :param string path: Path name\n        :param string method: HTTP method\n        :raises ValueError: If the value of `path` in Swagger is not a dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _split_classes_and_methods(folds):\n    \"\"\"\n    \n    \"\"\"\n    classes = []\n    functions = []\n    for fold in folds:\n        if fold.def_type == OED.FUNCTION_TOKEN:\n            functions.append(fold)\n        elif fold.def_type == OED.CLASS_TOKEN:\n            classes.append(fold)\n\n    return classes, functions", "output": "Split out classes and methods into two separate lists.\n\n    Parameters\n    ----------\n    folds : list of :class:`FoldScopeHelper`\n        The result of :func:`_get_fold_levels`.\n\n    Returns\n    -------\n    classes, functions: list of :class:`FoldScopeHelper`\n        Two separate lists of :class:`FoldScopeHelper` objects. The former\n        contains only class definitions while the latter contains only\n        function/method definitions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def UTF8Strsub(utf, start, len):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlUTF8Strsub(utf, start, len)\n    return ret", "output": "Create a substring from a given UTF-8 string Note:\n       positions are given in units of UTF-8 chars", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_partition(rule):\n    '''\n    \n    '''\n    parser = argparse.ArgumentParser()\n    rules = shlex.split(rule)\n    rules.pop(0)\n    parser.add_argument('mntpoint')\n    parser.add_argument('--size', dest='size', action='store')\n    parser.add_argument('--grow', dest='grow', action='store_true')\n    parser.add_argument('--maxsize', dest='maxsize', action='store')\n    parser.add_argument('--noformat', dest='noformat', action='store_true')\n    parser.add_argument('--onpart', '--usepart', dest='onpart', action='store')\n    parser.add_argument('--ondisk', '--ondrive', dest='ondisk', action='store')\n    parser.add_argument('--asprimary', dest='asprimary', action='store_true')\n    parser.add_argument('--fsprofile', dest='fsprofile', action='store')\n    parser.add_argument('--fstype', dest='fstype', action='store')\n    parser.add_argument('--fsoptions', dest='fsoptions', action='store')\n    parser.add_argument('--label', dest='label', action='store')\n    parser.add_argument('--recommended', dest='recommended',\n                        action='store_true')\n    parser.add_argument('--onbiosdisk', dest='onbiosdisk', action='store')\n    parser.add_argument('--encrypted', dest='encrypted', action='store_true')\n    parser.add_argument('--passphrase', dest='passphrase', action='store')\n    parser.add_argument('--escrowcert', dest='escrowcert', action='store')\n    parser.add_argument('--backupphrase', dest='backupphrase', action='store')\n\n    args = clean_args(vars(parser.parse_args(rules)))\n    parser = None\n    return args", "output": "Parse the partition line", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def IdentityBlock(kernel_size, filters):\n  \"\"\"\"\"\"\n  ks = kernel_size\n  filters1, filters2, filters3 = filters\n  main = layers.Serial(\n      layers.Conv(filters1, (1, 1)),\n      layers.BatchNorm(),\n      layers.Relu(),\n      layers.Conv(filters2, (ks, ks), padding='SAME'),\n      layers.BatchNorm(),\n      layers.Relu(),\n      layers.Conv(filters3, (1, 1)),\n      layers.BatchNorm()\n  )\n  return layers.Serial(\n      layers.Branch(),\n      layers.Parallel(main, layers.Identity()),\n      layers.SumBranches(),\n      layers.Relu()\n  )", "output": "ResNet identical size block.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_bucket(self, environment, name, filename, source):\n        \"\"\"\n        \"\"\"\n        key = self.get_cache_key(name, filename)\n        checksum = self.get_source_checksum(source)\n        bucket = Bucket(environment, key, checksum)\n        self.load_bytecode(bucket)\n        return bucket", "output": "Return a cache bucket for the given template.  All arguments are\n        mandatory but filename may be `None`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __flush_eventqueue(self):\r\n        \"\"\"\"\"\"\r\n        while self.eventqueue:\r\n            past_event = self.eventqueue.pop(0)\r\n            self.postprocess_keyevent(past_event)", "output": "Flush keyboard event queue", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def current_portfolio_weights(self):\n        \"\"\"\n        \n        \"\"\"\n        position_values = pd.Series({\n            asset: (\n                    position.last_sale_price *\n                    position.amount *\n                    asset.price_multiplier\n            )\n            for asset, position in self.positions.items()\n        })\n        return position_values / self.portfolio_value", "output": "Compute each asset's weight in the portfolio by calculating its held\n        value divided by the total value of all positions.\n\n        Each equity's value is its price times the number of shares held. Each\n        futures contract's value is its unit price times number of shares held\n        times the multiplier.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_file_name(url):\n  \"\"\"\"\"\"\n  return os.path.basename(urllib.parse.urlparse(url).path) or 'unknown_name'", "output": "Returns file name of file at given url.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def region_path(cls, project, region):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/regions/{region}\", project=project, region=region\n        )", "output": "Return a fully-qualified region string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_filename(self, index):\r\n        \"\"\"\"\"\"\r\n        if index:\r\n            return osp.normpath(to_text_string(self.fsmodel.filePath(index)))", "output": "Return filename associated with *index*", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _invalid_indexer(self, form, key):\n        \"\"\"\n        \n        \"\"\"\n        raise TypeError(\"cannot do {form} indexing on {klass} with these \"\n                        \"indexers [{key}] of {kind}\".format(\n                            form=form, klass=type(self), key=key,\n                            kind=type(key)))", "output": "Consistent invalid indexer message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _format_eval_result(value, show_stdv=True):\n    \"\"\"\"\"\"\n    if len(value) == 4:\n        return '%s\\'s %s: %g' % (value[0], value[1], value[2])\n    elif len(value) == 5:\n        if show_stdv:\n            return '%s\\'s %s: %g + %g' % (value[0], value[1], value[2], value[4])\n        else:\n            return '%s\\'s %s: %g' % (value[0], value[1], value[2])\n    else:\n        raise ValueError(\"Wrong metric value\")", "output": "Format metric string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_sure_path_exists(path):\n    \"\"\"\n    \"\"\"\n    logger.debug('Making sure path exists: {}'.format(path))\n    try:\n        os.makedirs(path)\n        logger.debug('Created directory at: {}'.format(path))\n    except OSError as exception:\n        if exception.errno != errno.EEXIST:\n            return False\n    return True", "output": "Ensure that a directory exists.\n\n    :param path: A directory path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install(self, exe):\n        \"\"\"\n        \"\"\"\n        exe.set_monitor_callback(self.stat_helper, self.monitor_all)\n        self.exes.append(exe)", "output": "install callback to executor.\n        Supports installing to multiple exes.\n\n        Parameters\n        ----------\n        exe : mx.executor.Executor\n            The Executor (returned by symbol.bind) to install to.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def group_list():\n    '''\n    \n    '''\n    ret = {'installed': [],\n           'available': [],\n           'installed environments': [],\n           'available environments': [],\n           'available languages': {}}\n\n    section_map = {\n        'installed groups:': 'installed',\n        'available groups:': 'available',\n        'installed environment groups:': 'installed environments',\n        'available environment groups:': 'available environments',\n        'available language groups:': 'available languages',\n    }\n\n    out = __salt__['cmd.run_stdout'](\n        [_yum(), 'grouplist', 'hidden'],\n        output_loglevel='trace',\n        python_shell=False\n    )\n    key = None\n    for line in salt.utils.itertools.split(out, '\\n'):\n        line_lc = line.lower()\n        if line_lc == 'done':\n            break\n\n        section_lookup = section_map.get(line_lc)\n        if section_lookup is not None and section_lookup != key:\n            key = section_lookup\n            continue\n\n        # Ignore any administrative comments (plugin info, repo info, etc.)\n        if key is None:\n            continue\n\n        line = line.strip()\n        if key != 'available languages':\n            ret[key].append(line)\n        else:\n            match = re.match(r'(.+) \\[(.+)\\]', line)\n            if match:\n                name, lang = match.groups()\n                ret[key][line] = {'name': name, 'language': lang}\n    return ret", "output": ".. versionadded:: 2014.1.0\n\n    Lists all groups known by yum on this system\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.group_list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_cache_path_parts(self, link):\n        # type: (Link) -> List[str]\n        \"\"\"\n        \"\"\"\n\n        # We want to generate an url to use as our cache key, we don't want to\n        # just re-use the URL because it might have other items in the fragment\n        # and we don't care about those.\n        key_parts = [link.url_without_fragment]\n        if link.hash_name is not None and link.hash is not None:\n            key_parts.append(\"=\".join([link.hash_name, link.hash]))\n        key_url = \"#\".join(key_parts)\n\n        # Encode our key url with sha224, we'll use this because it has similar\n        # security properties to sha256, but with a shorter total output (and\n        # thus less secure). However the differences don't make a lot of\n        # difference for our use case here.\n        hashed = hashlib.sha224(key_url.encode()).hexdigest()\n\n        # We want to nest the directories some to prevent having a ton of top\n        # level directories where we might run out of sub directories on some\n        # FS.\n        parts = [hashed[:2], hashed[2:4], hashed[4:6], hashed[6:]]\n\n        return parts", "output": "Get parts of part that must be os.path.joined with cache_dir", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def function_name(self):\n        \"\"\"\n        \n        \"\"\"\n        if self._function_identifier:\n            return self._function_identifier\n\n        # Function Identifier is *not* provided. If there is only one function in the template,\n        # default to it.\n\n        all_functions = [f for f in self._function_provider.get_all()]\n        if len(all_functions) == 1:\n            return all_functions[0].name\n\n        # Get all the available function names to print helpful exception message\n        all_function_names = [f.name for f in all_functions]\n\n        # There are more functions in the template, and function identifier is not provided, hence raise.\n        raise InvokeContextException(\"You must provide a function identifier (function's Logical ID in the template). \"\n                                     \"Possible options in your template: {}\".format(all_function_names))", "output": "Returns name of the function to invoke. If no function identifier is provided, this method will return name of\n        the only function from the template\n\n        :return string: Name of the function\n        :raises InvokeContextException: If function identifier is not provided", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lost_focus(self):\r\n        \"\"\"\"\"\"\r\n        if (self.is_running and not self.any_has_focus() and\r\n            not self.setting_data and not self.hidden):\r\n            self.hide_tips()", "output": "Confirm if the tour loses focus and hides the tips.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mutate_query_from_config(self, sql):\n        \"\"\"\"\"\"\n        SQL_QUERY_MUTATOR = config.get('SQL_QUERY_MUTATOR')\n        if SQL_QUERY_MUTATOR:\n            username = utils.get_username()\n            sql = SQL_QUERY_MUTATOR(sql, username, security_manager, self.database)\n        return sql", "output": "Apply config's SQL_QUERY_MUTATOR\n\n        Typically adds comments to the query with context", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def order_target_value(id_or_ins, cash_amount, price=None, style=None):\n    \"\"\"\n    \n    \"\"\"\n    order_book_id = assure_stock_order_book_id(id_or_ins)\n    account = Environment.get_instance().portfolio.accounts[DEFAULT_ACCOUNT_TYPE.STOCK.name]\n    position = account.positions[order_book_id]\n\n    style = cal_style(price, style)\n    if cash_amount == 0:\n        return _sell_all_stock(order_book_id, position.sellable, style)\n\n    try:\n        market_value = position.market_value\n    except RuntimeError:\n        order_result = order_value(order_book_id, np.nan, style=style)\n        if order_result:\n            raise\n    else:\n        return order_value(order_book_id, cash_amount - market_value, style=style)", "output": "\u4e70\u5165/\u5356\u51fa\u5e76\u4e14\u81ea\u52a8\u8c03\u6574\u8be5\u8bc1\u5238\u7684\u4ed3\u4f4d\u5230\u4e00\u4e2a\u76ee\u6807\u4ef7\u503c\u3002\n    \u52a0\u4ed3\u65f6\uff0ccash_amount \u4ee3\u8868\u73b0\u6709\u6301\u4ed3\u7684\u4ef7\u503c\u52a0\u4e0a\u5373\u5c06\u82b1\u8d39\uff08\u5305\u542b\u7a0e\u8d39\uff09\u7684\u73b0\u91d1\u7684\u603b\u4ef7\u503c\u3002\n    \u51cf\u4ed3\u65f6\uff0ccash_amount \u4ee3\u8868\u8c03\u6574\u4ed3\u4f4d\u7684\u76ee\u6807\u4ef7\u81f3\u3002\n\n    \u9700\u8981\u6ce8\u610f\uff0c\u5982\u679c\u8d44\u91d1\u4e0d\u8db3\uff0c\u8be5API\u5c06\u4e0d\u4f1a\u521b\u5efa\u53d1\u9001\u8ba2\u5355\u3002\n\n    :param id_or_ins: \u4e0b\u5355\u6807\u7684\u7269\n    :type id_or_ins: :class:`~Instrument` object | `str` | List[:class:`~Instrument`] | List[`str`]\n\n    :param float cash_amount: \u6700\u7ec8\u7684\u8be5\u8bc1\u5238\u7684\u4ed3\u4f4d\u76ee\u6807\u4ef7\u503c\u3002\n\n    :param float price: \u4e0b\u5355\u4ef7\u683c\uff0c\u9ed8\u8ba4\u4e3aNone\uff0c\u8868\u793a :class:`~MarketOrder`, \u6b64\u53c2\u6570\u4e3b\u8981\u7528\u4e8e\u7b80\u5316 `style` \u53c2\u6570\u3002\n\n    :param style: \u4e0b\u5355\u7c7b\u578b, \u9ed8\u8ba4\u662f\u5e02\u4ef7\u5355\u3002\u76ee\u524d\u652f\u6301\u7684\u8ba2\u5355\u7c7b\u578b\u6709 :class:`~LimitOrder` \u548c :class:`~MarketOrder`\n    :type style: `OrderStyle` object\n\n    :return: :class:`~Order` object | None\n\n    :example:\n\n    .. code-block:: python\n\n        #\u5982\u679c\u73b0\u5728\u7684\u6295\u8d44\u7ec4\u5408\u4e2d\u6301\u6709\u4ef7\u503c\uffe53000\u7684\u5e73\u5b89\u94f6\u884c\u80a1\u7968\u7684\u4ed3\u4f4d\uff0c\u4ee5\u4e0b\u4ee3\u7801\u8303\u4f8b\u4f1a\u53d1\u9001\u82b1\u8d39 \uffe57000 \u73b0\u91d1\u7684\u5e73\u5b89\u94f6\u884c\u4e70\u5355\u5230\u5e02\u573a\u3002\uff08\u5411\u4e0b\u8c03\u6574\u5230\u6700\u63a5\u8fd1\u6bcf\u624b\u80a1\u6570\u5373100\u7684\u500d\u6570\u7684\u80a1\u6570\uff09\uff1a\n        order_target_value('000001.XSHE', 10000)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def finite_datetimes(self, finite_start, finite_stop):\n        \"\"\"\n        \n        \"\"\"\n        start_date = self._align(finite_start)\n        aligned_stop = self._align(finite_stop)\n        dates = []\n        for m in itertools.count():\n            t = start_date + relativedelta(months=m)\n            if t >= aligned_stop:\n                return dates\n            if t >= finite_start:\n                dates.append(t)", "output": "Simply returns the points in time that correspond to turn of month.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hostinterface_update(interfaceid, **kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'hostinterface.update'\n            params = {\"interfaceid\": interfaceid}\n            params = _params_extend(params, **kwargs)\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result']['interfaceids']\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": ".. versionadded:: 2016.3.0\n\n    Update host interface\n\n    .. note::\n        This function accepts all standard hostinterface: keyword argument\n        names differ depending on your zabbix version, see here__.\n\n        .. __: https://www.zabbix.com/documentation/2.4/manual/api/reference/hostinterface/object#host_interface\n\n    :param interfaceid: ID of the hostinterface to update\n\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    :return: ID of the updated host interface, False on failure.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.hostinterface_update 6 ip_=0.0.0.2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_create_agent(agent_kwargs):\n  \"\"\"\n  \"\"\"\n\n  def create_agent(sess, environment, summary_writer=None):\n    \"\"\"Creates a DQN agent.\n\n    Simplified version of `dopamine.discrete_domains.train.create_agent`\n\n    Args:\n      sess: a session\n      environment: an environment\n      summary_writer: a summary writer.\n\n    Returns:\n      a DQN agent.\n    \"\"\"\n    return BatchDQNAgent(\n        env_batch_size=environment.batch_size,\n        sess=sess,\n        num_actions=environment.action_space.n,\n        summary_writer=summary_writer,\n        tf_device=\"/gpu:*\",\n        **agent_kwargs)\n\n  return create_agent", "output": "Factory for dopamine agent initialization.\n\n  Args:\n    agent_kwargs: dict of BatchDQNAgent parameters\n\n  Returns:\n    Function(sess, environment, summary_writer) -> BatchDQNAgent instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_scripts():\n    \"\"\"\"\"\"\n    proc = Popen(['npm', 'run-script'], stdout=PIPE)\n    should_yeild = False\n    for line in proc.stdout.readlines():\n        line = line.decode()\n        if 'available via `npm run-script`:' in line:\n            should_yeild = True\n            continue\n\n        if should_yeild and re.match(r'^  [^ ]+', line):\n            yield line.strip().split(' ')[0]", "output": "Get custom npm scripts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sinkhorn(inputs, n_iters=20):\n  \"\"\"\n  \"\"\"\n  vocab_size = tf.shape(inputs)[-1]\n  log_alpha = tf.reshape(inputs, [-1, vocab_size, vocab_size])\n\n  for _ in range(n_iters):\n    log_alpha -= tf.reshape(tf.reduce_logsumexp(log_alpha, axis=2),\n                            [-1, vocab_size, 1])\n    log_alpha -= tf.reshape(tf.reduce_logsumexp(log_alpha, axis=1),\n                            [-1, 1, vocab_size])\n  outputs = tf.exp(log_alpha)\n  return outputs", "output": "Performs incomplete Sinkhorn normalization to inputs.\n\n  By a theorem by Sinkhorn and Knopp [1], a sufficiently well-behaved  matrix\n  with positive entries can be turned into a doubly-stochastic matrix\n  (i.e. its rows and columns add up to one) via the succesive row and column\n  normalization.\n  -To ensure positivity, the effective input to sinkhorn has to be\n  exp(inputs) (elementwise).\n  -However, for stability, sinkhorn works in the log-space. It is only at\n   return time that entries are exponentiated.\n\n  Code is adapted from Mena et al. [2].\n\n  [1] Richard Sinkhorn and Paul Knopp. Concerning nonnegative matrices and\n  doubly stochastic matrices. Pacific Journal of Mathematics, 1967.\n\n  [2] Gonzalo Mena, David Belanger, Scott Linderman, Jasper Snoek.\n  Learning latent permutations with Gumbel-Sinkhorn networks. International\n  Conference on Learning Representations, 2018.\n\n  Args:\n    inputs: A `Tensor` with shape `[..., vocab_size, vocab_size]`.\n    n_iters: Number of sinkhorn iterations (in practice, as little as 20\n      iterations are needed to achieve decent convergence for `vocab_size` ~100)\n\n  Returns:\n    outputs: A `Tensor` of close-to-doubly-stochastic matrices with shape\n      `[:, vocab_size, vocab_size]`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_rich_text_font(self, font):\r\n        \"\"\"\"\"\"\r\n        self.rich_text.set_font(font, fixed_font=self.get_plugin_font())", "output": "Set rich text mode font", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_value_to_set_at_point(self, point):\n        \"\"\"\n        \n        \"\"\"\n        laste, lastv = None, None\n        for e, v in self.schedule:\n            if e == point:\n                return v    # meet the exact boundary, return directly\n            if e > point:\n                break\n            laste, lastv = e, v\n        if laste is None or laste == e:\n            # hasn't reached the first scheduled point, or reached the end of all scheduled points\n            return None\n        if self.interp is None:\n            # If no interpolation, nothing to do.\n            return None\n        v = (point - laste) * 1. / (e - laste) * (v - lastv) + lastv\n        return v", "output": "Using schedule, compute the value to be set at a given point.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def key_from_ireq(ireq):\n    \"\"\"\"\"\"\n    if ireq.req is None and ireq.link is not None:\n        return str(ireq.link)\n    else:\n        return key_from_req(ireq.req)", "output": "Get a standardized key for an InstallRequirement.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lastChild(self):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlGetLastChild(self._o)\n        if ret is None:raise treeError('xmlGetLastChild() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Search the last child of a node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def empty(self):\n        \"\"\"\n        \n        \"\"\"\n        return any(len(self._get_axis(a)) == 0 for a in self._AXIS_ORDERS)", "output": "Indicator whether DataFrame is empty.\n\n        True if DataFrame is entirely empty (no items), meaning any of the\n        axes are of length 0.\n\n        Returns\n        -------\n        bool\n            If DataFrame is empty, return True, if not return False.\n\n        See Also\n        --------\n        Series.dropna\n        DataFrame.dropna\n\n        Notes\n        -----\n        If DataFrame contains only NaNs, it is still not considered empty. See\n        the example below.\n\n        Examples\n        --------\n        An example of an actual empty DataFrame. Notice the index is empty:\n\n        >>> df_empty = pd.DataFrame({'A' : []})\n        >>> df_empty\n        Empty DataFrame\n        Columns: [A]\n        Index: []\n        >>> df_empty.empty\n        True\n\n        If we only have NaNs in our DataFrame, it is not considered empty! We\n        will need to drop the NaNs to make the DataFrame empty:\n\n        >>> df = pd.DataFrame({'A' : [np.nan]})\n        >>> df\n            A\n        0 NaN\n        >>> df.empty\n        False\n        >>> df.dropna().empty\n        True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_user_name(uid, return_none_on_error=True, **kwargs):\n    '''\n    \n    '''\n    with _IpmiCommand(**kwargs) as s:\n        return s.get_user_name(uid, return_none_on_error=True)", "output": "Get user name\n\n    :param uid: user number [1:16]\n    :param return_none_on_error: return None on error\n    :param kwargs:\n        - api_host=127.0.0.1\n        - api_user=admin\n        - api_pass=example\n        - api_port=623\n        - api_kg=None\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-call ipmi.get_user_name uid=2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getenforce():\n    '''\n    \n    '''\n    _selinux_fs_path = selinux_fs_path()\n    if _selinux_fs_path is None:\n        return 'Disabled'\n    try:\n        enforce = os.path.join(_selinux_fs_path, 'enforce')\n        with salt.utils.files.fopen(enforce, 'r') as _fp:\n            if salt.utils.stringutils.to_unicode(_fp.readline()).strip() == '0':\n                return 'Permissive'\n            else:\n                return 'Enforcing'\n    except (IOError, OSError, AttributeError):\n        return 'Disabled'", "output": "Return the mode selinux is running in\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' selinux.getenforce", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _canonical_unit_name(name):\n    '''\n    \n    '''\n    if not isinstance(name, six.string_types):\n        name = six.text_type(name)\n    if any(name.endswith(suffix) for suffix in VALID_UNIT_TYPES):\n        return name\n    return '%s.service' % name", "output": "Build a canonical unit name treating unit names without one\n    of the valid suffixes as a service.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def join(self, other, **kwargs):\n        \"\"\"\n        \"\"\"\n        if not isinstance(other, list):\n            other = [other]\n        return self._join_list_of_managers(other, **kwargs)", "output": "Joins a list or two objects together.\n\n        Args:\n            other: The other object(s) to join on.\n\n        Returns:\n            Joined objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_batch_begin(self, train, **kwargs:Any)->None:\n        \"\"\n        if train:\n            self.lrs.append(self.opt.lr)\n            self.moms.append(self.opt.mom)", "output": "Record learning rate and momentum at beginning of batch.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_tooltip(self, title, text, color=_DEFAULT_TITLE_COLOR,\r\n                     at_line=None, at_position=None, at_point=None):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        if text is not None and len(text) != 0:\r\n            # Find position of calltip\r\n            point = self._calculate_position(\r\n                at_line=at_line,\r\n                at_position=at_position,\r\n                at_point=at_point,\r\n            )\r\n\r\n            # Format text\r\n            tiptext = self._format_text(title, text, color, ellide=True)\r\n\r\n            self._update_stylesheet(self.tooltip_widget)\r\n\r\n            # Display tooltip\r\n            self.tooltip_widget.show_tip(point, tiptext)\r\n            self.tooltip_widget.show()", "output": "Show tooltip.\r\n\r\n        Tooltips will disappear if mouse hovers them. They are meant for quick\r\n        inspections.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_exports(self, exports):\n        \"\"\"\n        \n        \"\"\"\n        rf = self.get_distinfo_file(EXPORTS_FILENAME)\n        with open(rf, 'w') as f:\n            write_exports(exports, f)", "output": "Write a dictionary of exports to a file in .ini format.\n        :param exports: A dictionary of exports, mapping an export category to\n                        a list of :class:`ExportEntry` instances describing the\n                        individual export entries.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def round(self, decimals=0, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        nv.validate_round(args, kwargs)\n\n        if is_integer(decimals):\n            result = np.apply_along_axis(np.round, 0, self.values)\n            return self._wrap_result(result, axis=0)\n        raise TypeError(\"decimals must be an integer\")", "output": "Round each value in Panel to a specified number of decimal places.\n\n        .. versionadded:: 0.18.0\n\n        Parameters\n        ----------\n        decimals : int\n            Number of decimal places to round to (default: 0).\n            If decimals is negative, it specifies the number of\n            positions to the left of the decimal point.\n\n        Returns\n        -------\n        Panel object\n\n        See Also\n        --------\n        numpy.around", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_timeout(start_time, timeout):\n    '''\n    \n    '''\n    timeout_milisec = timeout * 60000\n    if timeout_milisec < (int(round(time.time() * 1000)) - start_time):\n        raise salt.exceptions.TimeoutError('Timeout expired.')", "output": "Name of the last installed kernel, for Red Hat based systems.\n\n    Returns:\n            List with name of last installed kernel as it is interpreted in output of `uname -a` command.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next(self, rows: List[Row]) -> List[Row]:\n        \"\"\"\n        \n        \"\"\"\n        if not rows:\n            return []\n        input_row_index = self._get_row_index(rows[0])\n        if input_row_index < len(self.table_data) - 1 and input_row_index != -1:\n            return [self.table_data[input_row_index + 1]]\n        return []", "output": "Takes an expression that evaluates to a single row, and returns the row that occurs after\n        the input row in the original set of rows. If the input row happens to be the last row, we\n        will return an empty list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_dirs(metadata):\n    '''\n    \n    '''\n\n    ret = []\n    found = {}\n\n    for bucket_dict in metadata:\n        for bucket_name, data in six.iteritems(bucket_dict):\n            dirpaths = set()\n            for path in [k['Key'] for k in data]:\n                prefix = ''\n                for part in path.split('/')[:-1]:\n                    directory = prefix + part + '/'\n                    dirpaths.add(directory)\n                    prefix = directory\n            if bucket_name not in found:\n                found[bucket_name] = True\n                ret.append({bucket_name: list(dirpaths)})\n            else:\n                for bucket in ret:\n                    if bucket_name in bucket:\n                        bucket[bucket_name] += list(dirpaths)\n                        bucket[bucket_name] = list(set(bucket[bucket_name]))\n                        break\n    return ret", "output": "Looks for all the directories in the S3 bucket cache metadata.\n\n    Supports trailing '/' keys (as created by S3 console) as well as\n    directories discovered in the path of file keys.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def archives(self):\n        '''\n        \n        '''\n        arc_files = []\n        tmpdir = tempfile.gettempdir()\n        for filename in os.listdir(tmpdir):\n            mtc = re.match(r'\\w+-\\w+-\\d+-\\d+\\.bz2', filename)\n            if mtc and len(filename) == mtc.span()[-1]:\n                arc_files.append(os.path.join(tmpdir, filename))\n\n        return arc_files", "output": "Get list of existing archives.\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def contains(path, text):\n    '''\n    \n    '''\n    path = os.path.expanduser(path)\n\n    if not os.path.exists(path):\n        return False\n\n    stripped_text = six.text_type(text).strip()\n    try:\n        with salt.utils.filebuffer.BufferedReader(path) as breader:\n            for chunk in breader:\n                if stripped_text in chunk:\n                    return True\n        return False\n    except (IOError, OSError):\n        return False", "output": ".. deprecated:: 0.17.0\n       Use :func:`search` instead.\n\n    Return ``True`` if the file at ``path`` contains ``text``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.contains /etc/crontab 'mymaintenance.sh'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main(\n        template, extra_context, no_input, checkout, verbose,\n        replay, overwrite_if_exists, output_dir, config_file,\n        default_config, debug_file):\n    \"\"\"\n    \"\"\"\n    # If you _need_ to support a local template in a directory\n    # called 'help', use a qualified path to the directory.\n    if template == u'help':\n        click.echo(click.get_current_context().get_help())\n        sys.exit(0)\n\n    configure_logger(\n        stream_level='DEBUG' if verbose else 'INFO',\n        debug_file=debug_file,\n    )\n\n    try:\n        cookiecutter(\n            template, checkout, no_input,\n            extra_context=extra_context,\n            replay=replay,\n            overwrite_if_exists=overwrite_if_exists,\n            output_dir=output_dir,\n            config_file=config_file,\n            default_config=default_config,\n            password=os.environ.get('COOKIECUTTER_REPO_PASSWORD')\n        )\n    except (OutputDirExistsException,\n            InvalidModeException,\n            FailedHookException,\n            UnknownExtension,\n            InvalidZipRepository,\n            RepositoryNotFound,\n            RepositoryCloneFailed) as e:\n        click.echo(e)\n        sys.exit(1)\n    except UndefinedVariableInTemplate as undefined_err:\n        click.echo('{}'.format(undefined_err.message))\n        click.echo('Error message: {}'.format(undefined_err.error.message))\n\n        context_str = json.dumps(\n            undefined_err.context,\n            indent=4,\n            sort_keys=True\n        )\n        click.echo('Context: {}'.format(context_str))\n        sys.exit(1)", "output": "Create a project from a Cookiecutter project template (TEMPLATE).\n\n    Cookiecutter is free and open source software, developed and managed by\n    volunteers. If you would like to help out or fund the project, please get\n    in touch at https://github.com/audreyr/cookiecutter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close(self) -> None:\n        \"\"\"\"\"\"\n        self._close = True\n        if self._waiter:\n            self._waiter.cancel()", "output": "Stop accepting new pipelinig messages and close\n        connection when handlers done processing messages", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def domain_list(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.list_domains(**kwargs)", "output": "List domains\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystoneng.domain_list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def layer_prepostprocess(previous_value,\n                         x,\n                         sequence,\n                         dropout_rate,\n                         norm_type,\n                         depth,\n                         epsilon,\n                         default_name,\n                         name=None,\n                         dropout_broadcast_dims=None,\n                         layer_collection=None):\n  \"\"\"\n  \"\"\"\n  with tf.variable_scope(name, default_name=default_name):\n    if sequence == \"none\":\n      return x\n    for c in sequence:\n      if c == \"a\":\n        x += previous_value\n      elif c == \"z\":\n        x = zero_add(previous_value, x)\n      elif c == \"n\":\n        x = apply_norm(\n            x, norm_type, depth, epsilon, layer_collection=layer_collection)\n      else:\n        assert c == \"d\", (\"Unknown sequence step %s\" % c)\n        x = dropout_with_broadcast_dims(\n            x, 1.0 - dropout_rate, broadcast_dims=dropout_broadcast_dims)\n    return x", "output": "Apply a sequence of functions to the input or output of a layer.\n\n  The sequence is specified as a string which may contain the following\n  characters:\n    a: add previous_value\n    n: apply normalization\n    d: apply dropout\n    z: zero add\n\n  For example, if sequence==\"dna\", then the output is\n    previous_value + normalize(dropout(x))\n\n  Args:\n    previous_value: A Tensor, to be added as a residual connection ('a')\n    x: A Tensor to be transformed.\n    sequence: a string.\n    dropout_rate: a float\n    norm_type: a string (see apply_norm())\n    depth: an integer (size of last dimension of x).\n    epsilon: a float (parameter for normalization)\n    default_name: a string\n    name: a string\n    dropout_broadcast_dims:  an optional list of integers less than 3\n      specifying in which dimensions to broadcast the dropout decisions.\n      saves memory.\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the\n      KFAC optimizer. Default is None.\n\n  Returns:\n    a Tensor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def types(cls):\n        \"\"\"  \"\"\"\n        redundant_types = \\\n            set(cls.types_mapping) & set(cls._types_from_methods)\n        if redundant_types:\n            warn(\"These types are defined both with a method and in the\"\n                 \"'types_mapping' property of this validator: %s\"\n                 % redundant_types)\n\n        return tuple(cls.types_mapping) + cls._types_from_methods", "output": "The constraints that can be used for the 'type' rule.\n            Type: A tuple of strings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def isna(obj):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(obj, BasePandasDataset):\n        return obj.isna()\n    else:\n        return pandas.isna(obj)", "output": "Detect missing values for an array-like object.\n    Args:\n        obj: Object to check for null or missing values.\n\n    Returns:\n        bool or array-like of bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extract(self, member, path=\"\", set_attrs=True):\n        \"\"\"\n        \"\"\"\n        self._check(\"r\")\n\n        if isinstance(member, str):\n            tarinfo = self.getmember(member)\n        else:\n            tarinfo = member\n\n        # Prepare the link target for makelink().\n        if tarinfo.islnk():\n            tarinfo._link_target = os.path.join(path, tarinfo.linkname)\n\n        try:\n            self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n                                 set_attrs=set_attrs)\n        except EnvironmentError as e:\n            if self.errorlevel > 0:\n                raise\n            else:\n                if e.filename is None:\n                    self._dbg(1, \"tarfile: %s\" % e.strerror)\n                else:\n                    self._dbg(1, \"tarfile: %s %r\" % (e.strerror, e.filename))\n        except ExtractError as e:\n            if self.errorlevel > 1:\n                raise\n            else:\n                self._dbg(1, \"tarfile: %s\" % e)", "output": "Extract a member from the archive to the current working directory,\n           using its full name. Its file information is extracted as accurately\n           as possible. `member' may be a filename or a TarInfo object. You can\n           specify a different directory using `path'. File attributes (owner,\n           mtime, mode) are set unless `set_attrs' is False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read_buckets_cache_file(cache_file):\n    '''\n    \n    '''\n\n    log.debug('Reading buckets cache file')\n\n    with salt.utils.files.fopen(cache_file, 'rb') as fp_:\n        try:\n            data = pickle.load(fp_)\n        except (pickle.UnpicklingError, AttributeError, EOFError, ImportError,\n                IndexError, KeyError):\n            data = None\n\n    return data", "output": "Return the contents of the buckets cache file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def value_counts(self, dropna=True):\n        \"\"\"\n        \n\n        \"\"\"\n\n        from pandas import Index, Series\n\n        # compute counts on the data with no nans\n        data = self._data[~self._mask]\n        value_counts = Index(data).value_counts()\n        array = value_counts.values\n\n        # TODO(extension)\n        # if we have allow Index to hold an ExtensionArray\n        # this is easier\n        index = value_counts.index.astype(object)\n\n        # if we want nans, count the mask\n        if not dropna:\n\n            # TODO(extension)\n            # appending to an Index *always* infers\n            # w/o passing the dtype\n            array = np.append(array, [self._mask.sum()])\n            index = Index(np.concatenate(\n                [index.values,\n                 np.array([np.nan], dtype=object)]), dtype=object)\n\n        return Series(array, index=index)", "output": "Returns a Series containing counts of each category.\n\n        Every category will have an entry, even those with a count of 0.\n\n        Parameters\n        ----------\n        dropna : boolean, default True\n            Don't include counts of NaN.\n\n        Returns\n        -------\n        counts : Series\n\n        See Also\n        --------\n        Series.value_counts", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pillar(tgt=None, tgt_type='glob', **kwargs):\n    '''\n    \n    '''\n    pillar_util = salt.utils.master.MasterPillarUtil(tgt, tgt_type,\n                                                     use_cached_grains=True,\n                                                     grains_fallback=False,\n                                                     use_cached_pillar=True,\n                                                     pillar_fallback=False,\n                                                     opts=__opts__)\n    cached_pillar = pillar_util.get_minion_pillar()\n    return cached_pillar", "output": ".. versionchanged:: 2017.7.0\n        The ``expr_form`` argument has been renamed to ``tgt_type``, earlier\n        releases must use ``expr_form``.\n\n    Return cached pillars of the targeted minions\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run cache.pillar", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(self, add_gt=True, add_mask=False):\n        \"\"\"\n        \n        \"\"\"\n        if add_mask:\n            assert add_gt\n        with timed_operation('Load Groundtruth Boxes for {}'.format(self.name)):\n            img_ids = self.coco.getImgIds()\n            img_ids.sort()\n            # list of dict, each has keys: height,width,id,file_name\n            imgs = self.coco.loadImgs(img_ids)\n\n            for img in tqdm.tqdm(imgs):\n                img['image_id'] = img.pop('id')\n                self._use_absolute_file_name(img)\n                if add_gt:\n                    self._add_detection_gt(img, add_mask)\n            return imgs", "output": "Args:\n            add_gt: whether to add ground truth bounding box annotations to the dicts\n            add_mask: whether to also add ground truth mask\n\n        Returns:\n            a list of dict, each has keys including:\n                'image_id', 'file_name',\n                and (if add_gt is True) 'boxes', 'class', 'is_crowd', and optionally\n                'segmentation'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def metric_delete(self, project, metric_name):\n        \"\"\"\n        \"\"\"\n        target = \"/projects/%s/metrics/%s\" % (project, metric_name)\n        self.api_request(method=\"DELETE\", path=target)", "output": "API call:  delete a metric resource.\n\n        See\n        https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.metrics/delete\n\n        :type project: str\n        :param project: ID of the project containing the metric.\n\n        :type metric_name: str\n        :param metric_name: the name of the metric.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ppo_base_v1():\n  \"\"\"\"\"\"\n  hparams = common_hparams.basic_params1()\n  hparams.learning_rate_schedule = \"constant\"\n  hparams.learning_rate_constant = 1e-4\n  hparams.clip_grad_norm = 0.5\n  hparams.weight_decay = 0\n  # If set, extends the LR warmup to all epochs except the final one.\n  hparams.add_hparam(\"lr_decay_in_final_epoch\", False)\n  hparams.add_hparam(\"init_mean_factor\", 0.1)\n  hparams.add_hparam(\"init_logstd\", 0.1)\n  hparams.add_hparam(\"policy_layers\", (100, 100))\n  hparams.add_hparam(\"value_layers\", (100, 100))\n  hparams.add_hparam(\"clipping_coef\", 0.2)\n  hparams.add_hparam(\"gae_gamma\", 0.99)\n  hparams.add_hparam(\"gae_lambda\", 0.95)\n  hparams.add_hparam(\"entropy_loss_coef\", 0.01)\n  hparams.add_hparam(\"value_loss_coef\", 1)\n  hparams.add_hparam(\"optimization_epochs\", 15)\n  hparams.add_hparam(\"epoch_length\", 200)\n  hparams.add_hparam(\"epochs_num\", 2000)\n  hparams.add_hparam(\"eval_every_epochs\", 10)\n  hparams.add_hparam(\"save_models_every_epochs\", 30)\n  hparams.add_hparam(\"optimization_batch_size\", 50)\n  hparams.add_hparam(\"intrinsic_reward_scale\", 0.)\n  hparams.add_hparam(\"logits_clip\", 0.0)\n  hparams.add_hparam(\"dropout_ppo\", 0.1)\n  hparams.add_hparam(\"effective_num_agents\", None)\n  # TODO(afrozm): Clean this up, this is used in PPO learner to get modalities.\n  hparams.add_hparam(\"policy_problem_name\", \"dummy_policy_problem\")\n  return hparams", "output": "Set of hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def items(self):\n        \"\"\"  \"\"\"\n        if not self.pdata and not self.spills:\n            return iter(self.data.items())\n        return self._external_items()", "output": "Return all merged items as iterator", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _avro_schema(read_session):\n    \"\"\"\n    \"\"\"\n    json_schema = json.loads(read_session.avro_schema.schema)\n    column_names = tuple((field[\"name\"] for field in json_schema[\"fields\"]))\n    return fastavro.parse_schema(json_schema), column_names", "output": "Extract and parse Avro schema from a read session.\n\n    Args:\n        read_session ( \\\n            ~google.cloud.bigquery_storage_v1beta1.types.ReadSession \\\n        ):\n            The read session associated with this read rows stream. This\n            contains the schema, which is required to parse the data\n            blocks.\n\n    Returns:\n        Tuple[fastavro.schema, Tuple[str]]:\n            A parsed Avro schema, using :func:`fastavro.schema.parse_schema`\n            and the column names for a read session.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def BBI(Series, N1, N2, N3, N4):\n    ''\n\n    bbi = (MA(Series, N1) + MA(Series, N2) +\n           MA(Series, N3) + MA(Series, N4)) / 4\n    DICT = {'BBI': bbi}\n    VAR = pd.DataFrame(DICT)\n    return VAR", "output": "\u591a\u7a7a\u6307\u6807", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(queue, items):\n    '''\n    \n    '''\n    with _conn(commit=True) as cur:\n        if isinstance(items, dict):\n            cmd = str(\"\"\"DELETE FROM {0} WHERE data = '{1}'\"\"\").format(  # future lint: disable=blacklisted-function\n                queue,\n                salt.utils.json.dumps(items))\n            log.debug('SQL Query: %s', cmd)\n            cur.execute(cmd)\n            return True\n        if isinstance(items, list):\n            items = [(salt.utils.json.dumps(el),) for el in items]\n            cmd = 'DELETE FROM {0} WHERE data = %s'.format(queue)\n            log.debug('SQL Query: %s', cmd)\n            cur.executemany(cmd, items)\n    return True", "output": "Delete an item or items from a queue", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nansum(values, axis=None, skipna=True, min_count=0, mask=None):\n    \"\"\"\n    \n    \"\"\"\n    values, mask, dtype, dtype_max, _ = _get_values(values,\n                                                    skipna, 0, mask=mask)\n    dtype_sum = dtype_max\n    if is_float_dtype(dtype):\n        dtype_sum = dtype\n    elif is_timedelta64_dtype(dtype):\n        dtype_sum = np.float64\n    the_sum = values.sum(axis, dtype=dtype_sum)\n    the_sum = _maybe_null_out(the_sum, axis, mask, min_count=min_count)\n\n    return _wrap_results(the_sum, dtype)", "output": "Sum the elements along an axis ignoring NaNs\n\n    Parameters\n    ----------\n    values : ndarray[dtype]\n    axis: int, optional\n    skipna : bool, default True\n    min_count: int, default 0\n    mask : ndarray[bool], optional\n        nan-mask if known\n\n    Returns\n    -------\n    result : dtype\n\n    Examples\n    --------\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1, 2, np.nan])\n    >>> nanops.nansum(s)\n    3.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_server_moduli(filename=None):\n        \"\"\"\n        \n        \"\"\"\n        Transport._modulus_pack = ModulusPack()\n        # places to look for the openssh \"moduli\" file\n        file_list = [\"/etc/ssh/moduli\", \"/usr/local/etc/moduli\"]\n        if filename is not None:\n            file_list.insert(0, filename)\n        for fn in file_list:\n            try:\n                Transport._modulus_pack.read_file(fn)\n                return True\n            except IOError:\n                pass\n        # none succeeded\n        Transport._modulus_pack = None\n        return False", "output": "(optional)\n        Load a file of prime moduli for use in doing group-exchange key\n        negotiation in server mode.  It's a rather obscure option and can be\n        safely ignored.\n\n        In server mode, the remote client may request \"group-exchange\" key\n        negotiation, which asks the server to send a random prime number that\n        fits certain criteria.  These primes are pretty difficult to compute,\n        so they can't be generated on demand.  But many systems contain a file\n        of suitable primes (usually named something like ``/etc/ssh/moduli``).\n        If you call `load_server_moduli` and it returns ``True``, then this\n        file of primes has been loaded and we will support \"group-exchange\" in\n        server mode.  Otherwise server mode will just claim that it doesn't\n        support that method of key negotiation.\n\n        :param str filename:\n            optional path to the moduli file, if you happen to know that it's\n            not in a standard location.\n        :return:\n            True if a moduli file was successfully loaded; False otherwise.\n\n        .. note:: This has no effect when used in client mode.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self, labels, preds):\n        \"\"\"\n        \n        \"\"\"\n        # get generated multi label from network\n        cls_prob = preds[0].asnumpy()\n        loc_loss = preds[1].asnumpy()\n        cls_label = preds[2].asnumpy()\n        valid_count = np.sum(cls_label >= 0)\n        # overall accuracy & object accuracy\n        label = cls_label.flatten()\n        mask = np.where(label >= 0)[0]\n        indices = np.int64(label[mask])\n        prob = cls_prob.transpose((0, 2, 1)).reshape((-1, cls_prob.shape[1]))\n        prob = prob[mask, indices]\n        self.sum_metric[0] += (-np.log(prob + self.eps)).sum()\n        self.num_inst[0] += valid_count\n        # smoothl1loss\n        self.sum_metric[1] += np.sum(loc_loss)\n        self.num_inst[1] += valid_count", "output": "Implementation of updating metrics", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_action(self, node: Node) -> None:\n        \"\"\"\n        \n        \"\"\"\n        if node.expr.name and node.expr.name not in ['ws', 'wsp']:\n            nonterminal = f'{node.expr.name} -> '\n\n            if isinstance(node.expr, Literal):\n                right_hand_side = f'[\"{node.text}\"]'\n\n            else:\n                child_strings = []\n                for child in node.__iter__():\n                    if child.expr.name in ['ws', 'wsp']:\n                        continue\n                    if child.expr.name != '':\n                        child_strings.append(child.expr.name)\n                    else:\n                        child_right_side_string = child.expr._as_rhs().lstrip(\"(\").rstrip(\")\") # pylint: disable=protected-access\n                        child_right_side_list = [tok for tok in\n                                                 WHITESPACE_REGEX.split(child_right_side_string) if tok]\n                        child_right_side_list = [tok.upper() if tok.upper() in\n                                                 self.keywords_to_uppercase else tok\n                                                 for tok in child_right_side_list]\n                        child_strings.extend(child_right_side_list)\n                right_hand_side = \"[\" + \", \".join(child_strings) + \"]\"\n            rule = nonterminal + right_hand_side\n            self.action_sequence = [rule] + self.action_sequence", "output": "For each node, we accumulate the rules that generated its children in a list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def command(name=None, cls=None, **attrs):\n    \"\"\"\n    \"\"\"\n    if cls is None:\n        cls = Command\n\n    def decorator(func):\n        if isinstance(func, Command):\n            raise TypeError('Callback is already a command.')\n        return cls(func, name=name, **attrs)\n\n    return decorator", "output": "A decorator that transforms a function into a :class:`.Command`\n    or if called with :func:`.group`, :class:`.Group`.\n\n    By default the ``help`` attribute is received automatically from the\n    docstring of the function and is cleaned up with the use of\n    ``inspect.cleandoc``. If the docstring is ``bytes``, then it is decoded\n    into :class:`str` using utf-8 encoding.\n\n    All checks added using the :func:`.check` & co. decorators are added into\n    the function. There is no way to supply your own checks through this\n    decorator.\n\n    Parameters\n    -----------\n    name: :class:`str`\n        The name to create the command with. By default this uses the\n        function name unchanged.\n    cls\n        The class to construct with. By default this is :class:`.Command`.\n        You usually do not change this.\n    attrs\n        Keyword arguments to pass into the construction of the class denoted\n        by ``cls``.\n\n    Raises\n    -------\n    TypeError\n        If the function is not a coroutine or is already a command.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_program_installed(basename):\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    for path in os.environ[\"PATH\"].split(os.pathsep):\r\n        abspath = osp.join(path, basename)\r\n        if osp.isfile(abspath):\r\n            return abspath", "output": "Return program absolute path if installed in PATH.\r\n\r\n    Otherwise, return None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fail(self, text=u\"FAIL\", err=False):\n        \"\"\"\"\"\"\n        # Do not display spin text for fail state\n        self._text = None\n\n        _text = text if text else u\"FAIL\"\n        err = err or not self.write_to_stdout\n        self._freeze(_text, err=err)", "output": "Set fail finalizer to a spinner.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def configure_proxy(proxyname, start=True):\n    '''\n    \n    '''\n    changes_new = []\n    changes_old = []\n    status_file = True\n    test = __opts__['test']\n\n    # write the proxy file if necessary\n    proxyfile = '/etc/salt/proxy'\n    status_file, msg_new, msg_old = _proxy_conf_file(proxyfile, test)\n    changes_new.extend(msg_new)\n    changes_old.extend(msg_old)\n    status_proc = False\n\n    # start the proxy process\n    if start:\n        status_proc, msg_new, msg_old = _proxy_process(proxyname, test)\n        changes_old.extend(msg_old)\n        changes_new.extend(msg_new)\n    else:\n        changes_old.append('Start is False, not starting salt-proxy process')\n        log.debug('Process not started')\n\n    return {\n        'result': status_file and status_proc,\n        'changes': {\n            'old': '\\n'.join(changes_old),\n            'new': '\\n'.join(changes_new),\n        },\n    }", "output": "Create the salt proxy file and start the proxy process\n    if required\n\n    Parameters:\n        proxyname:\n            Name to be used for this proxy (should match entries in pillar)\n        start:\n            Boolean indicating if the process should be started\n            default = True\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt deviceminion salt_proxy.configure_proxy p8000", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_using_network_time():\n    '''\n    \n    '''\n    ret = salt.utils.mac_utils.execute_return_result(\n        'systemsetup -getusingnetworktime')\n\n    return salt.utils.mac_utils.validate_enabled(\n        salt.utils.mac_utils.parse_return(ret)) == 'on'", "output": "Display whether network time is on or off\n\n    :return: True if network time is on, False if off\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' timezone.get_using_network_time", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def linux_distribution(self, full_distribution_name=True):\n        \"\"\"\n        \n        \"\"\"\n        return (\n            self.name() if full_distribution_name else self.id(),\n            self.version(),\n            self.codename()\n        )", "output": "Return information about the OS distribution that is compatible\n        with Python's :func:`platform.linux_distribution`, supporting a subset\n        of its parameters.\n\n        For details, see :func:`distro.linux_distribution`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_certificate_signing_request(self, name, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_certificate_signing_request_with_http_info(name, body, **kwargs)\n        else:\n            (data) = self.replace_certificate_signing_request_with_http_info(name, body, **kwargs)\n            return data", "output": "replace the specified CertificateSigningRequest\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_certificate_signing_request(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the CertificateSigningRequest (required)\n        :param V1beta1CertificateSigningRequest body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1beta1CertificateSigningRequest\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_external_path(self, path):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        if not osp.exists(path):\r\n            return\r\n        self.removeItem(self.findText(path))\r\n        self.addItem(path)\r\n        self.setItemData(self.count() - 1, path, Qt.ToolTipRole)\r\n        while self.count() > MAX_PATH_HISTORY + EXTERNAL_PATHS:\r\n            self.removeItem(EXTERNAL_PATHS)", "output": "Adds an external path to the combobox if it exists on the file system.\r\n        If the path is already listed in the combobox, it is removed from its\r\n        current position and added back at the end. If the maximum number of\r\n        paths is reached, the oldest external path is removed from the list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_object_id_by_params(obj, params=None, **kwargs):\n    '''\n    \n    '''\n    if params is None:\n        params = {}\n    res = run_query(obj + '.get', params, **kwargs)\n    if res and len(res) == 1:\n        return six.text_type(res[0][ZABBIX_ID_MAPPER[obj]])\n    else:\n        raise SaltException('Zabbix API: Object does not exist or bad Zabbix user permissions or other unexpected '\n                            'result. Called method {0} with params {1}. '\n                            'Result: {2}'.format(obj + '.get', params, res))", "output": ".. versionadded:: 2017.7\n\n    Get ID of single Zabbix object specified by its name.\n\n    :param obj: Zabbix object type\n    :param params: Parameters by which object is uniquely identified\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    :return: object ID", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def section(self, name):\n        \"\"\"\n        \"\"\"\n        self.write_paragraph()\n        self.write_heading(name)\n        self.indent()\n        try:\n            yield\n        finally:\n            self.dedent()", "output": "Helpful context manager that writes a paragraph, a heading,\n        and the indents.\n\n        :param name: the section name that is written as heading.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_microseconds(value):\n    \"\"\"\n    \"\"\"\n    if not value.tzinfo:\n        value = value.replace(tzinfo=pytz.utc)\n    # Regardless of what timezone is on the value, convert it to UTC.\n    value = value.astimezone(pytz.utc)\n    # Convert the datetime to a microsecond timestamp.\n    return int(calendar.timegm(value.timetuple()) * 1e6) + value.microsecond", "output": "Convert a datetime to microseconds since the unix epoch.\n\n    Args:\n        value (datetime.datetime): The datetime to covert.\n\n    Returns:\n        int: Microseconds since the unix epoch.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def trim(value):\n        '''\n        \n        '''\n        value = (value or '').strip()\n        if not value:\n            raise CommandExecutionError(\"Empty value during sanitation\")\n\n        return six.text_type(value)", "output": "Raise an exception if value is empty. Otherwise strip it down.\n        :param value:\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_access_token(self, code, state=None):\n        '''\n        \n        '''\n        kw = dict(client_id=self._client_id, client_secret=self._client_secret, code=code)\n        if self._redirect_uri:\n            kw['redirect_uri'] = self._redirect_uri\n        if state:\n            kw['state'] = state\n        opener = build_opener(HTTPSHandler)\n        request = Request('https://github.com/login/oauth/access_token', data=_encode_params(kw))\n        request.get_method = _METHOD_MAP['POST']\n        request.add_header('Accept', 'application/json')\n        try:\n            response = opener.open(request, timeout=TIMEOUT)\n            r = _parse_json(response.read())\n            if 'error' in r:\n                raise ApiAuthError(str(r.error))\n            return str(r.access_token)\n        except HTTPError as e:\n            raise ApiAuthError('HTTPError when get access token')", "output": "In callback url: http://host/callback?code=123&state=xyz\n\n        use code and state to get an access token.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def open(self, dbname=None):\n        '''\n        \n        '''\n        databases = self.list()\n        if self.is_closed():\n            self.db_path = os.path.join(self.path, dbname or (databases and databases[0] or self.new()))\n            if not self._opened:\n                self.list_tables()\n                self._opened = True", "output": "Open database from the path with the name or latest.\n        If there are no yet databases, create a new implicitly.\n\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dump(self, msg, fn_):\n        '''\n        \n        '''\n        if six.PY2:\n            fn_.write(self.dumps(msg))\n        else:\n            # When using Python 3, write files in such a way\n            # that the 'bytes' and 'str' types are distinguishable\n            # by using \"use_bin_type=True\".\n            fn_.write(self.dumps(msg, use_bin_type=True))\n        fn_.close()", "output": "Serialize the correct data into the named file object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def calibrate(self, cali_data, cali_labels):\n    \"\"\"\n    \n    \"\"\"\n    self.nb_cali = cali_labels.shape[0]\n    self.cali_activations = self.get_activations(cali_data)\n    self.cali_labels = cali_labels\n\n    print(\"Starting calibration of DkNN.\")\n    cali_knns_ind, cali_knns_labels = self.find_train_knns(\n        self.cali_activations)\n    assert all([v.shape == (self.nb_cali, self.neighbors)\n                for v in cali_knns_ind.itervalues()])\n    assert all([v.shape == (self.nb_cali, self.neighbors)\n                for v in cali_knns_labels.itervalues()])\n\n    cali_knns_not_in_class = self.nonconformity(cali_knns_labels)\n    cali_knns_not_in_l = np.zeros(self.nb_cali, dtype=np.int32)\n    for i in range(self.nb_cali):\n      cali_knns_not_in_l[i] = cali_knns_not_in_class[i, cali_labels[i]]\n    cali_knns_not_in_l_sorted = np.sort(cali_knns_not_in_l)\n    self.cali_nonconformity = np.trim_zeros(cali_knns_not_in_l_sorted, trim='f')\n    self.nb_cali = self.cali_nonconformity.shape[0]\n    self.calibrated = True\n    print(\"DkNN calibration complete.\")", "output": "Runs the DkNN on holdout data to calibrate the credibility metric.\n    :param cali_data: np array of calibration data.\n    :param cali_labels: np vector of calibration labels.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def has_resources(self, resources):\n        \"\"\"\n        \"\"\"\n        if time.time() - self._last_resource_refresh > self._refresh_period:\n            self._update_avail_resources()\n\n        currently_available = Resources.subtract(self._avail_resources,\n                                                 self._committed_resources)\n\n        have_space = (\n            resources.cpu_total() <= currently_available.cpu\n            and resources.gpu_total() <= currently_available.gpu and all(\n                resources.get_res_total(res) <= currently_available.get(res)\n                for res in resources.custom_resources))\n\n        if have_space:\n            return True\n\n        can_overcommit = self._queue_trials\n\n        if (resources.cpu_total() > 0 and currently_available.cpu <= 0) or \\\n           (resources.gpu_total() > 0 and currently_available.gpu <= 0) or \\\n           any((resources.get_res_total(res_name) > 0\n                and currently_available.get(res_name) <= 0)\n               for res_name in resources.custom_resources):\n            can_overcommit = False  # requested resource is already saturated\n\n        if can_overcommit:\n            logger.warning(\n                \"Allowing trial to start even though the \"\n                \"cluster does not have enough free resources. Trial actors \"\n                \"may appear to hang until enough resources are added to the \"\n                \"cluster (e.g., via autoscaling). You can disable this \"\n                \"behavior by specifying `queue_trials=False` in \"\n                \"ray.tune.run().\")\n            return True\n\n        return False", "output": "Returns whether this runner has at least the specified resources.\n\n        This refreshes the Ray cluster resources if the time since last update\n        has exceeded self._refresh_period. This also assumes that the\n        cluster is not resizing very frequently.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_state(self, output: UnityRLOutput) -> (AllBrainInfo, bool):\n        \"\"\"\n        \n        \"\"\"\n        _data = {}\n        global_done = output.global_done\n        for brain_name in output.agentInfos:\n            agent_info_list = output.agentInfos[brain_name].value\n            _data[brain_name] = BrainInfo.from_agent_proto(agent_info_list,\n                                                           self.brains[brain_name])\n        return _data, global_done", "output": "Collects experience information from all external brains in environment at current step.\n        :return: a dictionary of BrainInfo objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_hashes(values):\n    \"\"\"\n    \"\"\"\n    hashes = {}\n    if not values:\n        return hashes\n    for value in values:\n        try:\n            name, value = value.split(\":\", 1)\n        except ValueError:\n            name = \"sha256\"\n        if name not in hashes:\n            hashes[name] = []\n        hashes[name].append(value)\n    return hashes", "output": "Convert Pipfile.lock hash lines into InstallRequirement option format.\n\n    The option format uses a str-list mapping. Keys are hash algorithms, and\n    the list contains all values of that algorithm.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_pretty_examples(examples):\n    \"\"\"\n    \n\n    \"\"\"\n\n    if examples.startswith(\"\\n    Examples:\"):\n        examples = \"\\n\".join(map(lambda u: u[6:], examples.strip().split(\"\\n\")[1:]))\n        return \"**Examples:**\\n\\n```\\n%s\\n```\\n\\n\" % examples", "output": "Makes the examples description pretty and returns a formatted string if `examples`\n    starts with the example prefix. Otherwise, returns None.\n\n    Expected input:\n\n        Examples:\n          > SELECT ...;\n           ...\n          > SELECT ...;\n           ...\n\n    Expected output:\n    **Examples:**\n\n    ```\n    > SELECT ...;\n     ...\n    > SELECT ...;\n     ...\n    ```", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_epoch_end(self, epoch:int, num_batch:int, smooth_loss:Tensor,\n                     last_metrics=MetricsList, **kwargs:Any)->bool:\n        \"\"\n        self.nb_batches.append(num_batch)\n        if last_metrics is not None: self.val_losses.append(last_metrics[0])\n        else: last_metrics = [] if self.no_val else [None]\n        if len(last_metrics) > 1: self.metrics.append(last_metrics[1:])\n        self.format_stats([epoch, smooth_loss] + last_metrics)", "output": "Save epoch info: num_batch, smooth_loss, metrics.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_active(self):\n    \"\"\"\n    \"\"\"\n    return bool(\n        self._grpc_port is not None and\n        self._event_multiplexer and\n        self._event_multiplexer.PluginRunToTagToContent(\n            constants.DEBUGGER_PLUGIN_NAME))", "output": "Determines whether this plugin is active.\n\n    This plugin is active if any health pills information is present for any\n    run.\n\n    Returns:\n      A boolean. Whether this plugin is active.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def previousElementSibling(self):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlPreviousElementSibling(self._o)\n        if ret is None:return None\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Finds the first closest previous sibling of the node which\n          is an element node. Note the handling of entities\n          references is different than in the W3C DOM element\n          traversal spec since we don't have back reference from\n           entities content to entities references.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_expire(name, expire, root=None):\n    '''\n    \n    '''\n    return _set_attrib(name, 'expire', expire, '-E', root=root, validate=False)", "output": ".. versionchanged:: 2014.7.0\n\n    Sets the value for the date the account expires as days since the epoch\n    (January 1, 1970). Using a value of -1 will clear expiration. See man\n    chage.\n\n    name\n        User to modify\n\n    date\n        Date the account expires\n\n    root\n        Directory to chroot into\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' shadow.set_expire username -1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flags_as_args():\n  \"\"\"\"\"\"\n  if hasattr(FLAGS, \"flag_values_dict\"):\n    args_dict = FLAGS.flag_values_dict()\n  else:\n    args_dict = dict(FLAGS.__dict__[\"__flags\"])\n  del args_dict[\"cloud_mlengine\"]\n  # Configured later\n  del args_dict[\"t2t_usr_dir\"]\n  args_dict.pop(\"h\", None)\n  args_dict.pop(\"helpfull\", None)\n  args_dict.pop(\"helpshort\", None)\n  args_dict.pop(\"help\", None)\n  args = []\n  for name, val in args_dict.items():\n    if val is None:\n      continue\n    if name.startswith(\"autotune\"):\n      continue\n    args.extend([\"--%s=%s\" % (name, str(val))])\n  return args", "output": "Convert FLAGS to list of args suitable for passing on cmd line.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _restore(name, fields, value):\n    \"\"\" \"\"\"\n    k = (name, fields)\n    cls = __cls.get(k)\n    if cls is None:\n        cls = collections.namedtuple(name, fields)\n        __cls[k] = cls\n    return cls(*value)", "output": "Restore an object of namedtuple", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_binner(self):\n        \"\"\"\n        \n        \"\"\"\n\n        binner, bins, binlabels = self._get_binner_for_time()\n        bin_grouper = BinGrouper(bins, binlabels, indexer=self.groupby.indexer)\n        return binner, bin_grouper", "output": "Create the BinGrouper, assume that self.set_grouper(obj)\n        has already been called.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def critic(self, real_pred, input):\n        \"\"\n        fake = self.gan_model.generator(input.requires_grad_(False)).requires_grad_(True)\n        fake_pred = self.gan_model.critic(fake)\n        return self.loss_funcC(real_pred, fake_pred)", "output": "Create some `fake_pred` with the generator from `input` and compare them to `real_pred` in `self.loss_funcD`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_transport(cls, t, window_size=None, max_packet_size=None):\n        \"\"\"\n        \n        \"\"\"\n        chan = t.open_session(\n            window_size=window_size, max_packet_size=max_packet_size\n        )\n        if chan is None:\n            return None\n        chan.invoke_subsystem(\"sftp\")\n        return cls(chan)", "output": "Create an SFTP client channel from an open `.Transport`.\n\n        Setting the window and packet sizes might affect the transfer speed.\n        The default settings in the `.Transport` class are the same as in\n        OpenSSH and should work adequately for both files transfers and\n        interactive sessions.\n\n        :param .Transport t: an open `.Transport` which is already\n            authenticated\n        :param int window_size:\n            optional window size for the `.SFTPClient` session.\n        :param int max_packet_size:\n            optional max packet size for the `.SFTPClient` session..\n\n        :return:\n            a new `.SFTPClient` object, referring to an sftp session (channel)\n            across the transport\n\n        .. versionchanged:: 1.15\n            Added the ``window_size`` and ``max_packet_size`` arguments.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def corr(self, col1, col2, method=None):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(col1, basestring):\n            raise ValueError(\"col1 should be a string.\")\n        if not isinstance(col2, basestring):\n            raise ValueError(\"col2 should be a string.\")\n        if not method:\n            method = \"pearson\"\n        if not method == \"pearson\":\n            raise ValueError(\"Currently only the calculation of the Pearson Correlation \" +\n                             \"coefficient is supported.\")\n        return self._jdf.stat().corr(col1, col2, method)", "output": "Calculates the correlation of two columns of a DataFrame as a double value.\n        Currently only supports the Pearson Correlation Coefficient.\n        :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n\n        :param col1: The name of the first column\n        :param col2: The name of the second column\n        :param method: The correlation method. Currently only supports \"pearson\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_backup(name):\n    '''\n    \n    '''\n    if name not in list_backups():\n        log.debug('Backup already removed: %s', name)\n        return True\n\n    ps_cmd = ['Remove-WebConfigurationBackup',\n              '-Name', \"'{0}'\".format(name)]\n\n    cmd_ret = _srvmgr(ps_cmd)\n\n    if cmd_ret['retcode'] != 0:\n        msg = 'Unable to remove web configuration: {0}\\nError: {1}' \\\n              ''.format(name, cmd_ret['stderr'])\n        raise CommandExecutionError(msg)\n\n    return name not in list_backups()", "output": "Remove an IIS Configuration backup from the System.\n\n    .. versionadded:: 2017.7.0\n\n    Args:\n        name (str): The name of the backup to remove\n\n    Returns:\n        bool: True if successful, otherwise False\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' win_iis.remove_backup backup_20170209", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def json(self, *,  # type: ignore\n             loads: Callable[[Any], Any]=json.loads) -> None:\n        \"\"\"\n        \"\"\"\n        return loads(self.data)", "output": "Return parsed JSON data.\n\n        .. versionadded:: 0.22", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shape2d(a):\n    \"\"\"\n    \n    \"\"\"\n    if type(a) == int:\n        return [a, a]\n    if isinstance(a, (list, tuple)):\n        assert len(a) == 2\n        return list(a)\n    raise RuntimeError(\"Illegal shape: {}\".format(a))", "output": "Ensure a 2D shape.\n\n    Args:\n        a: a int or tuple/list of length 2\n\n    Returns:\n        list: of length 2. if ``a`` is a int, return ``[a, a]``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def returner(ret):\n    '''\n    \n    '''\n\n    _options = _get_options(ret)\n\n    webhook = _options.get('webhook', None)\n    show_tasks = _options.get('show_tasks')\n    author_icon = _options.get('author_icon')\n\n    if not webhook or webhook is '':\n        log.error('%s.webhook not defined in salt config', __virtualname__)\n        return\n\n    report = _generate_report(ret, show_tasks)\n\n    if report.get('success'):\n        title = _options.get('success_title')\n    else:\n        title = _options.get('failure_title')\n\n    slack = _post_message(webhook, author_icon, title, report)\n\n    return slack", "output": "Send a slack message with the data through a webhook\n    :param ret: The Salt return\n    :return: The result of the post", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _kname(obj):\n    ''''''\n    if isinstance(obj, dict):\n        return [obj.get(\"metadata\", {}).get(\"name\", \"\")]\n    elif isinstance(obj, (list, tuple)):\n        names = []\n        for i in obj:\n            names.append(i.get(\"metadata\", {}).get(\"name\", \"\"))\n        return names\n    else:\n        return \"Unknown type\"", "output": "Get name or names out of json result from API server", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def updating(name,\n             jail=None,\n             chroot=None,\n             root=None,\n             filedate=None,\n             filename=None):\n    ''''\n    \n    '''\n\n    opts = ''\n    if filedate:\n        opts += 'd {0}'.format(filedate)\n    if filename:\n        opts += 'f {0}'.format(filename)\n\n    cmd = _pkg(jail, chroot, root)\n    cmd.append('updating')\n    if opts:\n        cmd.append('-' + opts)\n    cmd.append(name)\n    return __salt__['cmd.run'](\n        cmd,\n        output_loglevel='trace',\n        python_shell=False\n    )", "output": "Displays UPDATING entries of software packages\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.updating foo\n\n    jail\n        Perform the action in the specified jail\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.updating foo jail=<jail name or id>\n\n    chroot\n        Perform the action in the specified chroot (ignored if ``jail`` is\n        specified)\n\n    root\n        Perform the action in the specified root (ignored if ``jail`` is\n        specified)\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.updating foo chroot=/path/to/chroot\n\n    filedate\n        Only entries newer than date are shown. Use a YYYYMMDD date format.\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.updating foo filedate=20130101\n\n    filename\n        Defines an alternative location of the UPDATING file.\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.updating foo filename=/tmp/UPDATING", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def object_to_dict(obj):\n    '''\n    \n    '''\n    if isinstance(obj, list) or isinstance(obj, tuple):\n        ret = []\n        for item in obj:\n            ret.append(object_to_dict(item))\n    elif hasattr(obj, '__dict__'):\n        ret = {}\n        for item in obj.__dict__:\n            if item.startswith('_'):\n                continue\n            ret[item] = object_to_dict(obj.__dict__[item])\n    else:\n        ret = obj\n    return ret", "output": ".. versionadded:: 2015.8.0\n\n    Convert an object to a dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __catalina_home():\n    '''\n    \n    '''\n    locations = ['/usr/share/tomcat*', '/opt/tomcat']\n    for location in locations:\n        folders = glob.glob(location)\n        if folders:\n            for catalina_home in folders:\n                if os.path.isdir(catalina_home + \"/bin\"):\n                    return catalina_home\n    return False", "output": "Tomcat paths differ depending on packaging", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def json_processor(entity):\n    '''\n    \n    '''\n    if six.PY2:\n        body = entity.fp.read()\n    else:\n        # https://github.com/cherrypy/cherrypy/pull/1572\n        contents = BytesIO()\n        body = entity.fp.read(fp_out=contents)\n        contents.seek(0)\n        body = salt.utils.stringutils.to_unicode(contents.read())\n        del contents\n    try:\n        cherrypy.serving.request.unserialized_data = salt.utils.json.loads(body)\n    except ValueError:\n        raise cherrypy.HTTPError(400, 'Invalid JSON document')\n\n    cherrypy.serving.request.raw_body = body", "output": "Unserialize raw POST data in JSON format to a Python data structure.\n\n    :param entity: raw POST data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def json(\n    body,\n    status=200,\n    headers=None,\n    content_type=\"application/json\",\n    dumps=json_dumps,\n    **kwargs\n):\n    \"\"\"\n    \n    \"\"\"\n    return HTTPResponse(\n        dumps(body, **kwargs),\n        headers=headers,\n        status=status,\n        content_type=content_type,\n    )", "output": "Returns response object with body in json format.\n\n    :param body: Response data to be serialized.\n    :param status: Response code.\n    :param headers: Custom Headers.\n    :param kwargs: Remaining arguments that are passed to the json encoder.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _crop(image, offset_height, offset_width, crop_height, crop_width):\n  \"\"\"\n  \"\"\"\n  original_shape = tf.shape(image)\n\n  rank_assertion = tf.Assert(\n      tf.equal(tf.rank(image), 3), [\"Rank of image must be equal to 3.\"])\n  with tf.control_dependencies([rank_assertion]):\n    cropped_shape = tf.stack([crop_height, crop_width, original_shape[2]])\n\n  size_assertion = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(original_shape[0], crop_height),\n          tf.greater_equal(original_shape[1], crop_width)),\n      [\"Crop size greater than the image size.\"])\n\n  offsets = tf.to_int32(tf.stack([offset_height, offset_width, 0]))\n\n  # Use tf.slice instead of crop_to_bounding box as it accepts tensors to\n  # define the crop size.\n  with tf.control_dependencies([size_assertion]):\n    image = tf.slice(image, offsets, cropped_shape)\n  return tf.reshape(image, cropped_shape)", "output": "Crops the given image using the provided offsets and sizes.\n\n  Note that the method doesn't assume we know the input image size but it does\n  assume we know the input image rank.\n\n  Args:\n    image: `Tensor` image of shape [height, width, channels].\n    offset_height: `Tensor` indicating the height offset.\n    offset_width: `Tensor` indicating the width offset.\n    crop_height: the height of the cropped image.\n    crop_width: the width of the cropped image.\n\n  Returns:\n    the cropped (and resized) image.\n\n  Raises:\n    InvalidArgumentError: if the rank is not 3 or if the image dimensions are\n      less than the crop size.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def win_cmd(command, **kwargs):\n    '''\n    \n    '''\n    logging_command = kwargs.get('logging_command', None)\n\n    try:\n        proc = NonBlockingPopen(\n            command,\n            shell=True,\n            stderr=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stream_stds=kwargs.get('display_ssh_output', True),\n            logging_command=logging_command,\n        )\n\n        if logging_command is None:\n            log.debug(\n                'Executing command(PID %s): \\'%s\\'',\n                proc.pid, command\n            )\n        else:\n            log.debug(\n                'Executing command(PID %s): \\'%s\\'',\n                proc.pid, logging_command\n            )\n\n        proc.poll_and_read_until_finish()\n        proc.communicate()\n        return proc.returncode\n    except Exception as err:\n        log.exception('Failed to execute command \\'%s\\'', logging_command)\n    # Signal an error\n    return 1", "output": "Wrapper for commands to be run against Windows boxes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_apppool(name):\n    # Remove IIS AppPool\n    '''\n    \n    '''\n\n    ret = {'name': name,\n           'changes': {},\n           'result': None,\n           'comment': ''}\n\n    current_apppools = __salt__['win_iis.list_apppools']()\n\n    if name not in current_apppools:\n        ret['comment'] = 'Application pool has already been removed: {0}'.format(name)\n        ret['result'] = True\n    elif __opts__['test']:\n        ret['comment'] = 'Application pool will be removed: {0}'.format(name)\n        ret['changes'] = {'old': name,\n                          'new': None}\n    else:\n        ret['comment'] = 'Removed application pool: {0}'.format(name)\n        ret['changes'] = {'old': name,\n                          'new': None}\n        ret['result'] = __salt__['win_iis.remove_apppool'](name)\n    return ret", "output": "Remove an IIS application pool.\n\n    :param str name: The name of the IIS application pool.\n\n    Usage:\n\n    .. code-block:: yaml\n\n        defaultapppool-remove:\n            win_iis.remove_apppool:\n                - name: DefaultAppPool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reorder_levels(self, order):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(self.index, MultiIndex):  # pragma: no cover\n            raise Exception('Can only reorder levels on a hierarchical axis.')\n\n        result = self.copy()\n        result.index = result.index.reorder_levels(order)\n        return result", "output": "Rearrange index levels using input order.\n\n        May not drop or duplicate levels.\n\n        Parameters\n        ----------\n        order : list of int representing new level order\n               (reference level by number or key)\n\n        Returns\n        -------\n        type of caller (new object)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def slice_sequence(sequence, length, pad_last=False, pad_val=C.PAD_TOKEN, overlap=0):\n    \"\"\"\n\n    \"\"\"\n    if length <= overlap:\n        raise ValueError('length needs to be larger than overlap')\n\n    if pad_last:\n        pad_len = _slice_pad_length(len(sequence), length, overlap)\n        sequence = sequence + [pad_val] * pad_len\n    num_samples = (len(sequence) - length) // (length - overlap) + 1\n\n    return [sequence[i * (length - overlap): ((i + 1) * length - i * overlap)]\n            for i in range(num_samples)]", "output": "Slice a flat sequence of tokens into sequences tokens, with each\n    inner sequence's length equal to the specified `length`, taking into account the requested\n    sequence overlap.\n\n    Parameters\n    ----------\n    sequence : list of object\n        A flat list of tokens.\n    length : int\n        The length of each of the samples.\n    pad_last : bool, default False\n        Whether to pad the last sequence when its length doesn't align. If the last sequence's\n        length doesn't align and ``pad_last`` is False, it will be dropped.\n    pad_val : object, default\n        The padding value to use when the padding of the last sequence is enabled. In general,\n        the type of ``pad_val`` should be the same as the tokens.\n    overlap : int, default 0\n        The extra number of items in current sample that should overlap with the\n        next sample.\n\n    Returns\n    -------\n    List of list of tokens, with the length of each inner list equal to `length`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def contains_python_files_or_subdirs(folder):\n    \"\"\"\n    \n    \"\"\"\n    for root, dirs, files in os.walk(folder):\n        if [filename for filename in files if filename.endswith('.py') or filename.endswith('.pyc')]:\n            return True\n\n        for d in dirs:\n            for _, subdirs, subfiles in os.walk(d):\n                if [filename for filename in subfiles if filename.endswith('.py') or filename.endswith('.pyc')]:\n                    return True\n\n    return False", "output": "Checks (recursively) if the directory contains .py or .pyc files", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _alarms_present(name, alarms, alarms_from_pillar, region, key, keyid, profile):\n    ''''''\n    current = __salt__['config.option'](alarms_from_pillar, {})\n    if alarms:\n        current = salt.utils.dictupdate.update(current, alarms)\n    ret = {'name': name, 'result': True, 'comment': '', 'changes': {}}\n    for _, info in six.iteritems(current):\n        info[\"name\"] = name + \" \" + info[\"name\"]\n        info[\"attributes\"][\"description\"] = name + \" \" + info[\"attributes\"][\"description\"]\n        info[\"attributes\"][\"dimensions\"] = {\"LoadBalancerName\": [name]}\n        kwargs = {\n            \"name\": info[\"name\"],\n            \"attributes\": info[\"attributes\"],\n            \"region\": region,\n            \"key\": key,\n            \"keyid\": keyid,\n            \"profile\": profile,\n        }\n        # No test=False cluase needed since the state handles that itself...\n        results = __states__['boto_cloudwatch_alarm.present'](**kwargs)\n        if not results.get('result'):\n            ret[\"result\"] = results[\"result\"]\n        if results.get(\"changes\", {}) != {}:\n            ret[\"changes\"][info[\"name\"]] = results[\"changes\"]\n        if \"comment\" in results:\n            ret[\"comment\"] += results[\"comment\"]\n    return ret", "output": "helper method for present.  ensure that cloudwatch_alarms are set", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_remote_experiments(experiments, thread_hosts, rate_limit=10):\n    \"\"\" \n    \"\"\"\n\n    global ssh_conn_per_min_limit\n    ssh_conn_per_min_limit = rate_limit\n    \n    # first we kill any remaining workers from previous runs\n    # note we don't check_call because pkill kills our ssh call as well\n    thread_hosts = copy.copy(thread_hosts)\n    random.shuffle(thread_hosts)\n    for host in set(thread_hosts):\n        hostname,_ = host.split(\":\")\n        try:\n            subprocess.run([\"ssh\", hostname, \"pkill -f shap.benchmark.run_experiment\"], timeout=15)\n        except subprocess.TimeoutExpired:\n            print(\"Failed to connect to\", hostname, \"after 15 seconds! Exiting.\")\n            return\n    \n    experiments = copy.copy(list(experiments))\n    random.shuffle(experiments) # this way all the hard experiments don't get put on one machine\n    global nexperiments, total_sent, total_done, total_failed, host_records\n    nexperiments = len(experiments)\n    total_sent = 0\n    total_done = 0\n    total_failed = 0\n    host_records = {}\n\n    q = Queue()\n\n    for host in thread_hosts:\n        worker = Thread(target=__thread_worker, args=(q, host))\n        worker.setDaemon(True)\n        worker.start()\n\n    for experiment in experiments:\n        q.put(experiment)\n\n    q.join()", "output": "Use ssh to run the experiments on remote machines in parallel.\n\n    Parameters\n    ----------\n    experiments : iterable\n        Output of shap.benchmark.experiments(...).\n\n    thread_hosts : list of strings\n        Each host has the format \"host_name:path_to_python_binary\" and can appear multiple times\n        in the list (one for each parallel execution you want on that machine).\n\n    rate_limit : int\n        How many ssh connections we make per minute to each host (to avoid throttling issues).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_user_info(user=None):\n    '''\n    \n    '''\n    if not user:\n        # Get user Salt runnining as\n        user = __salt__['config.option']('user')\n\n    userinfo = __salt__['user.info'](user)\n\n    if not userinfo:\n        if user == 'salt':\n            # Special case with `salt` user:\n            # if it doesn't exist then fall back to user Salt running as\n            userinfo = _get_user_info()\n        else:\n            raise SaltInvocationError('User {0} does not exist'.format(user))\n\n    return userinfo", "output": "Wrapper for user.info Salt function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def random_number_lower_endian(length, base):\n  \"\"\"\"\"\"\n  if length == 1:  # Last digit can be 0 only if length is 1.\n    return [np.random.randint(base)]\n  prefix = [np.random.randint(base) for _ in range(length - 1)]\n  return prefix + [np.random.randint(base - 1) + 1]", "output": "Helper function: generate a random number as a lower-endian digits list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_image(ami_name, instance_id=None, instance_name=None, tags=None, region=None,\n                 key=None, keyid=None, profile=None, description=None, no_reboot=False,\n                 dry_run=False, filters=None):\n    '''\n    \n\n    '''\n\n    instances = find_instances(instance_id=instance_id, name=instance_name, tags=tags,\n                               region=region, key=key, keyid=keyid, profile=profile,\n                               return_objs=True, filters=filters)\n\n    if not instances:\n        log.error('Source instance not found')\n        return False\n    if len(instances) > 1:\n        log.error('Multiple instances found, must match exactly only one instance to create an image from')\n        return False\n\n    instance = instances[0]\n    try:\n        return instance.create_image(ami_name, description=description,\n                                     no_reboot=no_reboot, dry_run=dry_run)\n    except boto.exception.BotoServerError as exc:\n        log.error(exc)\n        return False", "output": "Given instance properties that define exactly one instance, create AMI and return AMI-id.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt myminion boto_ec2.create_image ami_name instance_name=myinstance\n        salt myminion boto_ec2.create_image another_ami_name tags='{\"mytag\": \"value\"}' description='this is my ami'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _auto(direction, name, value, source='auto', convert_to_human=True):\n    '''\n    \n    '''\n    # NOTE: check direction\n    if direction not in ['to', 'from']:\n        return value\n\n    # NOTE: collect property data\n    props = property_data_zpool()\n    if source == 'zfs':\n        props = property_data_zfs()\n    elif source == 'auto':\n        props.update(property_data_zfs())\n\n    # NOTE: figure out the conversion type\n    value_type = props[name]['type'] if name in props else 'str'\n\n    # NOTE: convert\n    if value_type == 'size' and direction == 'to':\n        return globals()['{}_{}'.format(direction, value_type)](value, convert_to_human)\n\n    return globals()['{}_{}'.format(direction, value_type)](value)", "output": "Internal magic for from_auto and to_auto", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def json_query(data, expr):\n    '''\n    \n    '''\n    if jmespath is None:\n        err = 'json_query requires jmespath module installed'\n        log.error(err)\n        raise RuntimeError(err)\n    return jmespath.search(expr, data)", "output": "Query data using JMESPath language (http://jmespath.org).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_strl_names(self):\n        \"\"\"\"\"\"\n        # Update convert_strl if names changed\n        for orig, new in self._converted_names.items():\n            if orig in self._convert_strl:\n                idx = self._convert_strl.index(orig)\n                self._convert_strl[idx] = new", "output": "Update column names for conversion to strl if they might have been\n        changed to comply with Stata naming rules", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait_started(name, path=None, timeout=300):\n    '''\n    \n\n    '''\n    if not exists(name, path=path):\n        raise CommandExecutionError(\n            'Container {0} does does exists'.format(name))\n    if not state(name, path=path) == 'running':\n        raise CommandExecutionError(\n            'Container {0} is not running'.format(name))\n    ret = False\n    if running_systemd(name, path=path):\n        test_started = test_sd_started_state\n        logger = log.error\n    else:\n        test_started = test_bare_started_state\n        logger = log.debug\n    now = time.time()\n    expire = now + timeout\n    now = time.time()\n    started = test_started(name, path=path)\n    while time.time() < expire and not started:\n        time.sleep(0.3)\n        started = test_started(name, path=path)\n    if started is None:\n        logger(\n            'Assuming %s is started, although we failed to detect that'\n            ' is fully started correctly', name)\n        ret = True\n    else:\n        ret = started\n    return ret", "output": "Check that the system has fully inited\n\n    This is actually very important for systemD based containers\n\n    see https://github.com/saltstack/salt/issues/23847\n\n    path\n        path to the container parent\n        default: /var/lib/lxc (system default)\n\n        .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion lxc.wait_started ubuntu", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mysql_to_dict(data, key):\n    '''\n    \n    '''\n    ret = {}\n    headers = ['']\n    for line in data:\n        if not line:\n            continue\n        if line.startswith('+'):\n            continue\n        comps = line.split('|')\n        for comp in range(len(comps)):\n            comps[comp] = comps[comp].strip()\n        if len(headers) > 1:\n            index = len(headers) - 1\n            row = {}\n            for field in range(index):\n                if field < 1:\n                    continue\n                else:\n                    row[headers[field]] = salt.utils.stringutils.to_num(comps[field])\n            ret[row[key]] = row\n        else:\n            headers = comps\n    return ret", "output": "Convert MySQL-style output to a python dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def secgroup_delete(call=None, kwargs=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The secgroup_delete function must be called with -f or --function.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    name = kwargs.get('name', None)\n    secgroup_id = kwargs.get('secgroup_id', None)\n\n    if secgroup_id:\n        if name:\n            log.warning(\n                'Both the \\'secgroup_id\\' and \\'name\\' arguments were provided. '\n                '\\'secgroup_id\\' will take precedence.'\n            )\n    elif name:\n        secgroup_id = get_secgroup_id(kwargs={'name': name})\n    else:\n        raise SaltCloudSystemExit(\n            'The secgroup_delete function requires either a \\'name\\' or a '\n            '\\'secgroup_id\\' to be provided.'\n        )\n\n    server, user, password = _get_xml_rpc()\n    auth = ':'.join([user, password])\n    response = server.one.secgroup.delete(auth, int(secgroup_id))\n\n    data = {\n        'action': 'secgroup.delete',\n        'deleted': response[0],\n        'secgroup_id': response[1],\n        'error_code': response[2],\n    }\n\n    return data", "output": "Deletes the given security group from OpenNebula. Either a name or a secgroup_id\n    must be supplied.\n\n    .. versionadded:: 2016.3.0\n\n    name\n        The name of the security group to delete. Can be used instead of\n        ``secgroup_id``.\n\n    secgroup_id\n        The ID of the security group to delete. Can be used instead of ``name``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f secgroup_delete opennebula name=my-secgroup\n        salt-cloud --function secgroup_delete opennebula secgroup_id=100", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def returner(ret):\n    '''\n    \n    '''\n\n    try:\n        _connect_sentry(_get_message(ret), ret)\n    except Exception as err:\n        log.error('Can\\'t run connect_sentry: %s', err, exc_info=True)", "output": "Log outcome to sentry. The returner tries to identify errors and report\n    them as such. All other messages will be reported at info level.\n    Failed states will be appended as separate list for convenience.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_config():\n    '''\n    \n    '''\n    profiles = {}\n    curr = None\n\n    cmd = ['netsh', 'advfirewall', 'show', 'allprofiles']\n    ret = __salt__['cmd.run_all'](cmd, python_shell=False, ignore_retcode=True)\n    if ret['retcode'] != 0:\n        raise CommandExecutionError(ret['stdout'])\n\n    # There may be some problems with this depending on how `netsh` is localized\n    # It's looking for lines that contain `Profile Settings` or start with\n    # `State` which may be different in different localizations\n    for line in ret['stdout'].splitlines():\n        if not curr:\n            tmp = re.search('(.*) Profile Settings:', line)\n            if tmp:\n                curr = tmp.group(1)\n        elif line.startswith('State'):\n            profiles[curr] = line.split()[1] == 'ON'\n            curr = None\n\n    return profiles", "output": "Get the status of all the firewall profiles\n\n    Returns:\n        dict: A dictionary of all profiles on the system\n\n    Raises:\n        CommandExecutionError: If the command fails\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' firewall.get_config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def inceptionv4(pretrained=True):\n    \n    \"\"\"\n    model = InceptionV4()\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['imagenet']))\n    return model", "output": "r\"\"\"InceptionV4 model architecture from the\n    `\"Inception-v4, Inception-ResNet...\" <https://arxiv.org/abs/1602.07261>`_ paper.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_action_begin(self, action, logs={}):\n        \"\"\" \"\"\"\n        for callback in self.callbacks:\n            if callable(getattr(callback, 'on_action_begin', None)):\n                callback.on_action_begin(action, logs=logs)", "output": "Called at beginning of each action for each callback in callbackList", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hypermedia_in():\n    '''\n    \n    '''\n    # Be liberal in what you accept\n    ct_in_map = {\n        'application/x-www-form-urlencoded': urlencoded_processor,\n        'application/json': json_processor,\n        'application/x-yaml': yaml_processor,\n        'text/yaml': yaml_processor,\n        'text/plain': text_processor,\n    }\n\n    # Do not process the body for POST requests that have specified no content\n    # or have not specified Content-Length\n    if (cherrypy.request.method.upper() == 'POST'\n            and cherrypy.request.headers.get('Content-Length', '0') == '0'):\n        cherrypy.request.process_request_body = False\n        cherrypy.request.unserialized_data = None\n\n    cherrypy.request.body.processors.clear()\n    cherrypy.request.body.default_proc = cherrypy.HTTPError(\n            406, 'Content type not supported')\n    cherrypy.request.body.processors = ct_in_map", "output": "Unserialize POST/PUT data of a specified Content-Type.\n\n    The following custom processors all are intended to format Low State data\n    and will place that data structure into the request object.\n\n    :raises HTTPError: if the request contains a Content-Type that we do not\n        have a processor for", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_metric(self, metric, labels, pre_sliced=False):\n        \"\"\"\"\"\"\n        self.curr_execgrp.update_metric(metric, labels, pre_sliced)", "output": "Update metric with the current executor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_rgb_to_real(prediction, targets):\n  \"\"\"\"\"\"\n  prediction = tf.squeeze(prediction, axis=-1)\n  prediction = common_layers.convert_rgb_to_real(prediction)\n  targets = common_layers.convert_rgb_to_real(targets)\n  return prediction, targets", "output": "Convert prediction and target from rgb to real.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rename_get_variable(mapping):\n    \"\"\"\n    \n    \"\"\"\n    def custom_getter(getter, name, *args, **kwargs):\n        splits = name.split('/')\n        basename = splits[-1]\n        if basename in mapping:\n            basename = mapping[basename]\n            splits[-1] = basename\n            name = '/'.join(splits)\n        return getter(name, *args, **kwargs)\n    return custom_getter_scope(custom_getter)", "output": "Args:\n        mapping(dict): an old -> new mapping for variable basename. e.g. {'kernel': 'W'}\n\n    Returns:\n        A context where the variables are renamed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dilated_conv_stack(name, x, mid_channels, output_channels,\n                       dilation_rates, activation=\"relu\",\n                       dropout=0.0):\n  \"\"\"\n  \"\"\"\n  with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n    output = 0.0\n    for dil_ind, dil_rate in enumerate(dilation_rates):\n      # TODO(mechcoder) try (concat across channels + 1x1) modulo memory issues.\n      curr_out = conv_stack(\"dil_%d\" % dil_ind, x, mid_channels=mid_channels,\n                            output_channels=output_channels, dilations=dil_rate,\n                            activation=activation, dropout=dropout)\n      output += curr_out\n    return output", "output": "Dilated convolutional stack.\n\n  Features at different rates are computed independently using a 3 layer\n  convolutional stack and added.\n\n  Args:\n    name: variable scope.\n    x: 5-D Tensor.\n    mid_channels: Number of output channels of the first layer in the conv\n                  stack.\n    output_channels: Number of output channels of the last layer.\n    dilation_rates: A list of dilation rates.\n    activation: Can be either \"relu\" or \"gatu\"\n    dropout: dropout.\n  Returns:\n    output: 5-D Tensor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_list(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_openstack_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.list_users(**kwargs)", "output": "List users\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystoneng.user_list\n        salt '*' keystoneng.user_list domain_id=b62e76fbeeff4e8fb77073f591cf211e", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_settings_bond_2(opts, iface, bond_def):\n    '''\n    \n    '''\n\n    bond = {'mode': '2'}\n\n    valid = ['list of ips (up to 16)']\n    if 'arp_ip_target' in opts:\n        if isinstance(opts['arp_ip_target'], list):\n            if 1 <= len(opts['arp_ip_target']) <= 16:\n                bond.update({'arp_ip_target': ''})\n                for ip in opts['arp_ip_target']:  # pylint: disable=C0103\n                    if bond['arp_ip_target']:\n                        bond['arp_ip_target'] = bond['arp_ip_target'] + ',' + ip\n                    else:\n                        bond['arp_ip_target'] = ip\n            else:\n                _raise_error_iface(iface, 'arp_ip_target', valid)\n        else:\n            _raise_error_iface(iface, 'arp_ip_target', valid)\n    else:\n        _raise_error_iface(iface, 'arp_ip_target', valid)\n\n    if 'arp_interval' in opts:\n        try:\n            int(opts['arp_interval'])\n            bond.update({'arp_interval': opts['arp_interval']})\n        except ValueError:\n            _raise_error_iface(iface, 'arp_interval', ['integer'])\n    else:\n        _log_default_iface(iface, 'arp_interval', bond_def['arp_interval'])\n        bond.update({'arp_interval': bond_def['arp_interval']})\n\n    if 'hashing-algorithm' in opts:\n        valid = ['layer2', 'layer2+3', 'layer3+4']\n        if opts['hashing-algorithm'] in valid:\n            bond.update({'xmit_hash_policy': opts['hashing-algorithm']})\n        else:\n            _raise_error_iface(iface, 'hashing-algorithm', valid)\n\n    return bond", "output": "Filters given options and outputs valid settings for bond2.\n    If an option has a value that is not expected, this\n    function will log what the Interface, Setting and what it was\n    expecting.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_frame_sampling_stochastic():\n  \"\"\"\"\"\"\n  hparams = basic_deterministic_params.next_frame_sampling()\n  hparams.stochastic_model = True\n  hparams.add_hparam(\"latent_channels\", 1)\n  hparams.add_hparam(\"latent_std_min\", -5.0)\n  hparams.add_hparam(\"num_iterations_1st_stage\", 15000)\n  hparams.add_hparam(\"num_iterations_2nd_stage\", 15000)\n  hparams.add_hparam(\"latent_loss_multiplier\", 1e-3)\n  hparams.add_hparam(\"latent_loss_multiplier_dynamic\", False)\n  hparams.add_hparam(\"latent_loss_multiplier_alpha\", 1e-5)\n  hparams.add_hparam(\"latent_loss_multiplier_epsilon\", 1.0)\n  hparams.add_hparam(\"latent_loss_multiplier_schedule\", \"constant\")\n  hparams.add_hparam(\"latent_num_frames\", 0)  # 0 means use all frames.\n  hparams.add_hparam(\"anneal_end\", 40000)\n  hparams.add_hparam(\"information_capacity\", 0.0)\n  return hparams", "output": "Basic 2-frame conv model with stochastic tower.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_bank_hier(bank, redis_pipe):\n    '''\n    \n    '''\n    bank_list = bank.split('/')\n    parent_bank_path = bank_list[0]\n    for bank_name in bank_list[1:]:\n        prev_bank_redis_key = _get_bank_redis_key(parent_bank_path)\n        redis_pipe.sadd(prev_bank_redis_key, bank_name)\n        log.debug('Adding %s to %s', bank_name, prev_bank_redis_key)\n        parent_bank_path = '{curr_path}/{bank_name}'.format(\n            curr_path=parent_bank_path,\n            bank_name=bank_name\n        )  # this becomes the parent of the next\n    return True", "output": "Build the bank hierarchy from the root of the tree.\n    If already exists, it won't rewrite.\n    It's using the Redis pipeline,\n    so there will be only one interaction with the remote server.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def friends(self):\n        \n        \"\"\"\n        return [r.user for r in self._relationships.values() if r.type is RelationshipType.friend]", "output": "r\"\"\"Returns a :class:`list` of :class:`User`\\s that the user is friends with.\n\n        .. note::\n\n            This only applies to non-bot accounts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _expires(name):\n    '''\n    \n    '''\n    cert_file = _cert_file(name, 'cert')\n    # Use the salt module if available\n    if 'tls.cert_info' in __salt__:\n        expiry = __salt__['tls.cert_info'](cert_file)['not_after']\n    # Cobble it together using the openssl binary\n    else:\n        openssl_cmd = 'openssl x509 -in {0} -noout -enddate'.format(cert_file)\n        # No %e format on my Linux'es here\n        strptime_sux_cmd = 'date --date=\"$({0} | cut -d= -f2)\" +%s'.format(openssl_cmd)\n        expiry = float(__salt__['cmd.shell'](strptime_sux_cmd, output_loglevel='quiet'))\n        # expiry = datetime.datetime.strptime(expiry.split('=', 1)[-1], '%b %e %H:%M:%S %Y %Z')\n\n    return datetime.datetime.fromtimestamp(expiry)", "output": "Return the expiry date of a cert\n\n    :return datetime object of expiry date", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait_for(func, timeout=10, step=1, default=None, func_args=(), func_kwargs=None):\n    '''\n    \n    '''\n    if func_kwargs is None:\n        func_kwargs = dict()\n    max_time = time.time() + timeout\n    # Time moves forward so we might not reenter the loop if we step too long\n    step = min(step or 1, timeout) * BLUR_FACTOR\n\n    ret = default\n    while time.time() <= max_time:\n        call_ret = func(*func_args, **func_kwargs)\n        if call_ret:\n            ret = call_ret\n            break\n        else:\n            time.sleep(step)\n\n            # Don't allow cases of over-stepping the timeout\n            step = min(step, max_time - time.time()) * BLUR_FACTOR\n    if time.time() > max_time:\n        log.warning(\"Exceeded waiting time (%s seconds) to exectute %s\", timeout, func)\n    return ret", "output": "Call `func` at regular intervals and Waits until the given function returns\n    a truthy value within the given timeout and returns that value.\n\n    @param func:\n    @type func: function\n    @param timeout:\n    @type timeout: int | float\n    @param step: Interval at which we should check for the value\n    @type step: int | float\n    @param default: Value that should be returned should `func` not return a truthy value\n    @type default:\n    @param func_args: *args for `func`\n    @type func_args: list | tuple\n    @param func_kwargs: **kwargs for `func`\n    @type func_kwargs: dict\n    @return: `default` or result of `func`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_eval_dataflow(name, shard=0, num_shards=1):\n    \"\"\"\n    \n    \"\"\"\n    roidbs = DetectionDataset().load_inference_roidbs(name)\n\n    num_imgs = len(roidbs)\n    img_per_shard = num_imgs // num_shards\n    img_range = (shard * img_per_shard, (shard + 1) * img_per_shard if shard + 1 < num_shards else num_imgs)\n\n    # no filter for training\n    ds = DataFromListOfDict(roidbs[img_range[0]: img_range[1]], ['file_name', 'image_id'])\n\n    def f(fname):\n        im = cv2.imread(fname, cv2.IMREAD_COLOR)\n        assert im is not None, fname\n        return im\n    ds = MapDataComponent(ds, f, 0)\n    # Evaluation itself may be multi-threaded, therefore don't add prefetch here.\n    return ds", "output": "Args:\n        name (str): name of the dataset to evaluate\n        shard, num_shards: to get subset of evaluation data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_cache(self, template):\n        '''\n        \n        '''\n        if template not in self.cached:\n            self.cache_file(template)\n            self.cached.append(template)", "output": "Cache a file only once", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def can_attend_meetings(intervals):\n    \"\"\"\n    \n    \"\"\"\n    intervals = sorted(intervals, key=lambda x: x.start)\n    for i in range(1, len(intervals)):\n        if intervals[i].start < intervals[i - 1].end:\n            return False\n    return True", "output": ":type intervals: List[Interval]\n    :rtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def export(self, name):\n        '''\n        \n        '''\n\n        self.name = name\n        root = self._create_doc()\n        self._set_description(root)\n        self._set_preferences(root)\n        self._set_repositories(root)\n        self._set_users(root)\n        self._set_packages(root)\n\n        return '\\n'.join([line for line in minidom.parseString(\n            etree.tostring(root, encoding='UTF-8', pretty_print=True)).toprettyxml(indent=\"  \").split(\"\\n\")\n                          if line.strip()])", "output": "Export to the Kiwi config.xml as text.\n\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_local_urls(port):\n    ''''''\n    url_list = []\n    for name, info in psutil.net_if_addrs().items():\n        for addr in info:\n            if AddressFamily.AF_INET == addr.family:\n                url_list.append('http://{}:{}'.format(addr.address, port))\n    return url_list", "output": "get urls of local machine", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def beacons(opts, functions, context=None, proxy=None):\n    '''\n    \n    '''\n    return LazyLoader(\n        _module_dirs(opts, 'beacons'),\n        opts,\n        tag='beacons',\n        pack={'__context__': context, '__salt__': functions, '__proxy__': proxy or {}},\n        virtual_funcs=[],\n    )", "output": "Load the beacon modules\n\n    :param dict opts: The Salt options dictionary\n    :param dict functions: A dictionary of minion modules, with module names as\n                            keys and funcs as values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fold(self, zeroValue, op):\n        \"\"\"\n        \n        \"\"\"\n        op = fail_on_stopiteration(op)\n\n        def func(iterator):\n            acc = zeroValue\n            for obj in iterator:\n                acc = op(acc, obj)\n            yield acc\n        # collecting result of mapPartitions here ensures that the copy of\n        # zeroValue provided to each partition is unique from the one provided\n        # to the final reduce call\n        vals = self.mapPartitions(func).collect()\n        return reduce(op, vals, zeroValue)", "output": "Aggregate the elements of each partition, and then the results for all\n        the partitions, using a given associative function and a neutral \"zero value.\"\n\n        The function C{op(t1, t2)} is allowed to modify C{t1} and return it\n        as its result value to avoid object allocation; however, it should not\n        modify C{t2}.\n\n        This behaves somewhat differently from fold operations implemented\n        for non-distributed collections in functional languages like Scala.\n        This fold operation may be applied to partitions individually, and then\n        fold those results into the final result, rather than apply the fold\n        to each element sequentially in some defined ordering. For functions\n        that are not commutative, the result may differ from that of a fold\n        applied to a non-distributed collection.\n\n        >>> from operator import add\n        >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n        15", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_extension_type(arr):\n    \"\"\"\n    \n    \"\"\"\n\n    if is_categorical(arr):\n        return True\n    elif is_sparse(arr):\n        return True\n    elif is_datetime64tz_dtype(arr):\n        return True\n    return False", "output": "Check whether an array-like is of a pandas extension class instance.\n\n    Extension classes include categoricals, pandas sparse objects (i.e.\n    classes represented within the pandas library and not ones external\n    to it like scipy sparse matrices), and datetime-like arrays.\n\n    Parameters\n    ----------\n    arr : array-like\n        The array-like to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array-like is of a pandas extension class instance.\n\n    Examples\n    --------\n    >>> is_extension_type([1, 2, 3])\n    False\n    >>> is_extension_type(np.array([1, 2, 3]))\n    False\n    >>>\n    >>> cat = pd.Categorical([1, 2, 3])\n    >>>\n    >>> is_extension_type(cat)\n    True\n    >>> is_extension_type(pd.Series(cat))\n    True\n    >>> is_extension_type(pd.SparseArray([1, 2, 3]))\n    True\n    >>> is_extension_type(pd.SparseSeries([1, 2, 3]))\n    True\n    >>>\n    >>> from scipy.sparse import bsr_matrix\n    >>> is_extension_type(bsr_matrix([1, 2, 3]))\n    False\n    >>> is_extension_type(pd.DatetimeIndex([1, 2, 3]))\n    False\n    >>> is_extension_type(pd.DatetimeIndex([1, 2, 3], tz=\"US/Eastern\"))\n    True\n    >>>\n    >>> dtype = DatetimeTZDtype(\"ns\", tz=\"US/Eastern\")\n    >>> s = pd.Series([], dtype=dtype)\n    >>> is_extension_type(s)\n    True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_from_java_class(cls, java_class, *args):\n        \"\"\"\n        \n        \"\"\"\n        java_obj = JavaWrapper._new_java_obj(java_class, *args)\n        return cls(java_obj)", "output": "Construct this object from given Java classname and arguments", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imdecode(str_img, flag=1):\n    \"\"\"\n    \"\"\"\n    hdl = NDArrayHandle()\n    check_call(_LIB.MXCVImdecode(ctypes.c_char_p(str_img),\n                                 mx_uint(len(str_img)),\n                                 flag, ctypes.byref(hdl)))\n    return mx.nd.NDArray(hdl)", "output": "Decode image from str buffer.\n    Wrapper for cv2.imdecode that uses mx.nd.NDArray\n\n    Parameters\n    ----------\n    str_img : str\n        str buffer read from image file\n    flag : int\n        same as flag for cv2.imdecode\n    Returns\n    -------\n    img : NDArray\n        decoded image in (width, height, channels)\n        with BGR color channel order", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def augment_observation(\n    observation, reward, cum_reward, frame_index, bar_color=None,\n    header_height=27\n):\n  \"\"\"\"\"\"\n  img = PIL_Image().new(\n      \"RGB\", (observation.shape[1], header_height,)\n  )\n  draw = PIL_ImageDraw().Draw(img)\n  draw.text(\n      (1, 0), \"c:{:3}, r:{:3}\".format(int(cum_reward), int(reward)),\n      fill=(255, 0, 0)\n  )\n  draw.text(\n      (1, 15), \"f:{:3}\".format(int(frame_index)),\n      fill=(255, 0, 0)\n  )\n  header = np.copy(np.asarray(img))\n  del img\n  if bar_color is not None:\n    header[0, :, :] = bar_color\n  return np.concatenate([header, observation], axis=0)", "output": "Augments an observation with debug info.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def paintEvent(self, event):\n        \"\"\"\"\"\"\n        if self.isVisible() and self.position != self.Position.FLOATING:\n            # fill background\n            self._background_brush = QBrush(QColor(\n                self.editor.sideareas_color))\n            self._foreground_pen = QPen(QColor(\n                self.palette().windowText().color()))\n            painter = QPainter(self)\n            painter.fillRect(event.rect(), self._background_brush)", "output": "Fills the panel background using QPalette.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetch(bank, key):\n    '''\n    \n    '''\n    _init_client()\n    query = \"SELECT data FROM {0} WHERE bank='{1}' AND etcd_key='{2}'\".format(\n        _table_name, bank, key)\n    cur, _ = run_query(client, query)\n    r = cur.fetchone()\n    cur.close()\n    if r is None:\n        return {}\n    return __context__['serial'].loads(r[0])", "output": "Fetch a key value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def teardown_coverage(config, kernel, output_loc=None):\n    \"\"\"\n    \"\"\"\n    language = kernel.language\n    if language.startswith('python'):\n        # Teardown code does not require any input, simply execute:\n        msg_id = kernel.kc.execute(_python_teardown)\n        kernel.await_idle(msg_id, 60)  # A minute should be plenty to write out coverage\n\n        # Ensure we merge our data into parent data of pytest-cov, if possible\n        cov = get_cov(config)\n        _merge_nbval_coverage_data(cov)\n\n    else:\n        # Warnings should be given on setup, or there might be no teardown\n        # for a specific language, so do nothing here\n        pass", "output": "Finish coverage reporting in kernel.\n\n    The coverage should previously have been started with\n    setup_coverage.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def with_rank_at_least(self, rank):\n        \"\"\"\n        \"\"\"\n        if self.ndims is not None and self.ndims < rank:\n            raise ValueError(\"Shape %s must have rank at least %d\" % (self, rank))\n        else:\n            return self", "output": "Returns a shape based on `self` with at least the given rank.\n\n        Args:\n          rank: An integer.\n\n        Returns:\n          A shape that is at least as specific as `self` with at least the given\n          rank.\n\n        Raises:\n          ValueError: If `self` does not represent a shape with at least the given\n            `rank`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_action(parent, text, shortcut=None, icon=None, tip=None,\r\n                  toggled=None, triggered=None, data=None, menurole=None,\r\n                  context=Qt.WindowShortcut):\r\n    \"\"\"\"\"\"\r\n    action = SpyderAction(text, parent)\r\n    if triggered is not None:\r\n        action.triggered.connect(triggered)\r\n    if toggled is not None:\r\n        action.toggled.connect(toggled)\r\n        action.setCheckable(True)\r\n    if icon is not None:\r\n        if is_text_string(icon):\r\n            icon = get_icon(icon)\r\n        action.setIcon(icon)\r\n    if tip is not None:\r\n        action.setToolTip(tip)\r\n        action.setStatusTip(tip)\r\n    if data is not None:\r\n        action.setData(to_qvariant(data))\r\n    if menurole is not None:\r\n        action.setMenuRole(menurole)\r\n\r\n    # Workround for Mac because setting context=Qt.WidgetShortcut\r\n    # there doesn't have any effect\r\n    if sys.platform == 'darwin':\r\n        action._shown_shortcut = None\r\n        if context == Qt.WidgetShortcut:\r\n            if shortcut is not None:\r\n                action._shown_shortcut = shortcut\r\n            else:\r\n                # This is going to be filled by\r\n                # main.register_shortcut\r\n                action._shown_shortcut = 'missing'\r\n        else:\r\n            if shortcut is not None:\r\n                action.setShortcut(shortcut)\r\n            action.setShortcutContext(context)\r\n    else:\r\n        if shortcut is not None:\r\n            action.setShortcut(shortcut)\r\n        action.setShortcutContext(context)\r\n\r\n    return action", "output": "Create a QAction", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_imagenet_iterator(root, batch_size, num_workers, data_shape=224, dtype='float32'):\n    \"\"\"\"\"\"\n    train_dir = os.path.join(root, 'train')\n    train_transform, val_transform = get_imagenet_transforms(data_shape, dtype)\n    logging.info(\"Loading image folder %s, this may take a bit long...\", train_dir)\n    train_dataset = ImageFolderDataset(train_dir, transform=train_transform)\n    train_data = DataLoader(train_dataset, batch_size, shuffle=True,\n                            last_batch='discard', num_workers=num_workers)\n    val_dir = os.path.join(root, 'val')\n    if not os.path.isdir(os.path.expanduser(os.path.join(root, 'val', 'n01440764'))):\n        user_warning = 'Make sure validation images are stored in one subdir per category, a helper script is available at https://git.io/vNQv1'\n        raise ValueError(user_warning)\n    logging.info(\"Loading image folder %s, this may take a bit long...\", val_dir)\n    val_dataset = ImageFolderDataset(val_dir, transform=val_transform)\n    val_data = DataLoader(val_dataset, batch_size, last_batch='keep', num_workers=num_workers)\n    return DataLoaderIter(train_data, dtype), DataLoaderIter(val_data, dtype)", "output": "Dataset loader with preprocessing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_config_modify(dn=None, inconfig=None, hierarchical=False):\n    '''\n    \n    '''\n    ret = {}\n    cookie = logon()\n\n    # Declare if the search contains hierarchical results.\n    h = \"false\"\n    if hierarchical is True:\n        h = \"true\"\n\n    payload = '<configConfMo cookie=\"{0}\" inHierarchical=\"{1}\" dn=\"{2}\">' \\\n              '<inConfig>{3}</inConfig></configConfMo>'.format(cookie, h, dn, inconfig)\n    r = __utils__['http.query'](DETAILS['url'],\n                                data=payload,\n                                method='POST',\n                                decode_type='plain',\n                                decode=True,\n                                verify_ssl=False,\n                                raise_error=True,\n                                status=True,\n                                headers=DETAILS['headers'])\n\n    _validate_response_code(r['status'], cookie)\n\n    answer = re.findall(r'(<[\\s\\S.]*>)', r['text'])[0]\n    items = ET.fromstring(answer)\n    logout(cookie)\n    for item in items:\n        ret[item.tag] = prepare_return(item)\n    return ret", "output": "The configConfMo method configures the specified managed object in a single subtree (for example, DN).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_nodes(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The list_nodes function must be called with -f or --function.'\n        )\n\n    ret = {}\n    nodes = list_nodes_full()\n    if 'error' in nodes:\n        raise SaltCloudSystemExit(\n            'An error occurred while listing nodes: {0}'.format(\n                nodes['error']['Errors']['Error']['Message']\n            )\n        )\n    for node in nodes:\n        ret[node] = {\n            'id': nodes[node]['hostname'],\n            'ram': nodes[node]['maxMemory'],\n            'cpus': nodes[node]['maxCpu'],\n        }\n        if 'primaryIpAddress' in nodes[node]:\n            ret[node]['public_ips'] = nodes[node]['primaryIpAddress']\n        if 'primaryBackendIpAddress' in nodes[node]:\n            ret[node]['private_ips'] = nodes[node]['primaryBackendIpAddress']\n        if 'status' in nodes[node]:\n            ret[node]['state'] = six.text_type(nodes[node]['status']['name'])\n    return ret", "output": "Return a list of the VMs that are on the provider", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_plugin_font(self, rich_text=False):\n        \"\"\"\n        \n        \"\"\"\n\n        if rich_text:\n            option = 'rich_font'\n            font_size_delta = self.RICH_FONT_SIZE_DELTA\n        else:\n            option = 'font'\n            font_size_delta = self.FONT_SIZE_DELTA\n\n        return get_font(option=option, font_size_delta=font_size_delta)", "output": "Return plugin font option.\n\n        All plugins in Spyder use a global font. This is a convenience method\n        in case some plugins will have a delta size based on the default size.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_dirty(dir_path):\n    \"\"\"\"\"\"\n    try:\n        subprocess.check_call([\"git\", \"diff\", \"--quiet\"], cwd=dir_path)\n        return False\n    except subprocess.CalledProcessError:\n        return True", "output": "Check whether a git repository has uncommitted changes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def GetLogdirSubdirectories(path):\n  \"\"\"\n  \"\"\"\n  if not tf.io.gfile.exists(path):\n    # No directory to traverse.\n    return ()\n\n  if not tf.io.gfile.isdir(path):\n    raise ValueError('GetLogdirSubdirectories: path exists and is not a '\n                     'directory, %s' % path)\n\n  if IsCloudPath(path):\n    # Glob-ing for files can be significantly faster than recursively\n    # walking through directories for some file systems.\n    logger.info(\n        'GetLogdirSubdirectories: Starting to list directories via glob-ing.')\n    traversal_method = ListRecursivelyViaGlobbing\n  else:\n    # For other file systems, the glob-ing based method might be slower because\n    # each call to glob could involve performing a recursive walk.\n    logger.info(\n        'GetLogdirSubdirectories: Starting to list directories via walking.')\n    traversal_method = ListRecursivelyViaWalking\n\n  return (\n      subdir\n      for (subdir, files) in traversal_method(path)\n      if any(IsTensorFlowEventsFile(f) for f in files)\n  )", "output": "Obtains all subdirectories with events files.\n\n  The order of the subdirectories returned is unspecified. The internal logic\n  that determines order varies by scenario.\n\n  Args:\n    path: The path to a directory under which to find subdirectories.\n\n  Returns:\n    A tuple of absolute paths of all subdirectories each with at least 1 events\n    file directly within the subdirectory.\n\n  Raises:\n    ValueError: If the path passed to the method exists and is not a directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _send(self, message):\n        \"\"\"\n        \n        \"\"\"\n        message['command'] = 'zappa.asynchronous.route_sns_task'\n        payload = json.dumps(message).encode('utf-8')\n        if len(payload) > LAMBDA_ASYNC_PAYLOAD_LIMIT: # pragma: no cover\n            raise AsyncException(\"Payload too large for SNS\")\n        self.response = self.client.publish(\n                                TargetArn=self.arn,\n                                Message=payload\n                            )\n        self.sent = self.response.get('MessageId')", "output": "Given a message, publish to this topic.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stp(br=None, state='disable', iface=None):\n    '''\n    \n    '''\n    kernel = __grains__['kernel']\n    if kernel == 'Linux':\n        states = {'enable': 'on', 'disable': 'off'}\n        return _os_dispatch('stp', br, states[state])\n    elif kernel in SUPPORTED_BSD_LIKE:\n        states = {'enable': 'stp', 'disable': '-stp'}\n        return _os_dispatch('stp', br, states[state], iface)\n    else:\n        return False", "output": "Sets Spanning Tree Protocol state for a bridge\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bridge.stp br0 enable\n        salt '*' bridge.stp br0 disable\n\n    For BSD-like operating systems, it is required to add the interface on\n    which to enable the STP.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bridge.stp bridge0 enable fxp0\n        salt '*' bridge.stp bridge0 disable fxp0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Pad(self, n):\n        \"\"\"\"\"\"\n        for i in range_func(n):\n            self.Place(0, N.Uint8Flags)", "output": "Pad places zeros at the current offset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _lookup_element(lst, key):\n    '''\n    \n    '''\n    if not lst:\n        return {}\n    for ele in lst:\n        if not ele or not isinstance(ele, dict):\n            continue\n        if ele.keys()[0] == key:\n            return ele.values()[0]\n    return {}", "output": "Find an dictionary in a list of dictionaries, given its main key.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_filename(self, index):\r\n        \"\"\"\"\"\"\r\n        if index:\r\n            path = self.fsmodel.filePath(self.proxymodel.mapToSource(index))\r\n            return osp.normpath(to_text_string(path))", "output": "Return filename from index", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ifacestartswith(cidr):\n    '''\n    \n    '''\n    net_list = interfaces()\n    intfnames = []\n    pattern = six.text_type(cidr)\n    size = len(pattern)\n    for ifname, ifval in six.iteritems(net_list):\n        if 'inet' in ifval:\n            for inet in ifval['inet']:\n                if inet['address'][0:size] == pattern:\n                    if 'label' in inet:\n                        intfnames.append(inet['label'])\n                    else:\n                        intfnames.append(ifname)\n    return intfnames", "output": "Retrieve the interface name from a specific CIDR\n\n    .. versionadded:: 2016.11.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' network.ifacestartswith 10.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathNewParserContext(self, str):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlXPathNewParserContext(str, self._o)\n        if ret is None:raise xpathError('xmlXPathNewParserContext() failed')\n        __tmp = xpathParserContext(_obj=ret)\n        return __tmp", "output": "Create a new xmlXPathParserContext", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def open(cls, rootdir, end_session=None):\n        \"\"\"\n        \n        \"\"\"\n        metadata = BcolzMinuteBarMetadata.read(rootdir)\n        return BcolzMinuteBarWriter(\n            rootdir,\n            metadata.calendar,\n            metadata.start_session,\n            end_session if end_session is not None else metadata.end_session,\n            metadata.minutes_per_day,\n            metadata.default_ohlc_ratio,\n            metadata.ohlc_ratios_per_sid,\n            write_metadata=end_session is not None\n        )", "output": "Open an existing ``rootdir`` for writing.\n\n        Parameters\n        ----------\n        end_session : Timestamp (optional)\n            When appending, the intended new ``end_session``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uninstall(cert_name,\n              keychain=\"/Library/Keychains/System.keychain\",\n              keychain_password=None):\n    '''\n    \n    '''\n    if keychain_password is not None:\n        unlock_keychain(keychain, keychain_password)\n\n    cmd = 'security delete-certificate -c \"{0}\" {1}'.format(cert_name, keychain)\n    return __salt__['cmd.run'](cmd)", "output": "Uninstall a certificate from a keychain\n\n    cert_name\n        The name of the certificate to remove\n\n    keychain\n        The keychain to install the certificate to, this defaults to\n        /Library/Keychains/System.keychain\n\n    keychain_password\n        If your keychain is likely to be locked pass the password and it will be unlocked\n        before running the import\n\n        Note: The password given here will show up as plaintext in the returned job\n        info.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keychain.install test.p12 test123", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rm_rf(path):\n    '''\n    \n    '''\n    def _onerror(func, path, exc_info):\n        '''\n        Error handler for `shutil.rmtree`.\n\n        If the error is due to an access error (read only file)\n        it attempts to add write permission and then retries.\n\n        If the error is for another reason it re-raises the error.\n\n        Usage : `shutil.rmtree(path, onerror=onerror)`\n        '''\n        if salt.utils.platform.is_windows() and not os.access(path, os.W_OK):\n            # Is the error an access error ?\n            os.chmod(path, stat.S_IWUSR)\n            func(path)\n        else:\n            raise  # pylint: disable=E0704\n    if os.path.islink(path) or not os.path.isdir(path):\n        os.remove(path)\n    else:\n        if salt.utils.platform.is_windows():\n            try:\n                path = salt.utils.stringutils.to_unicode(path)\n            except TypeError:\n                pass\n        shutil.rmtree(path, onerror=_onerror)", "output": "Platform-independent recursive delete. Includes code from\n    http://stackoverflow.com/a/2656405", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_column(cls, name):\n        \"\"\"\n        \"\"\"\n        clsdict = vars(cls)\n        try:\n            maybe_column = clsdict[name]\n            if not isinstance(maybe_column, _BoundColumnDescr):\n                raise KeyError(name)\n        except KeyError:\n            raise AttributeError(\n                \"{dset} has no column {colname!r}:\\n\\n\"\n                \"Possible choices are:\\n\"\n                \"{choices}\".format(\n                    dset=cls.qualname,\n                    colname=name,\n                    choices=bulleted_list(\n                        sorted(cls._column_names),\n                        max_count=10,\n                    ),\n                )\n            )\n\n        # Resolve column descriptor into a BoundColumn.\n        return maybe_column.__get__(None, cls)", "output": "Look up a column by name.\n\n        Parameters\n        ----------\n        name : str\n            Name of the column to look up.\n\n        Returns\n        -------\n        column : zipline.pipeline.data.BoundColumn\n            Column with the given name.\n\n        Raises\n        ------\n        AttributeError\n            If no column with the given name exists.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fpn_map_rois_to_levels(boxes):\n    \"\"\"\n    \n    \"\"\"\n    sqrtarea = tf.sqrt(tf_area(boxes))\n    level = tf.cast(tf.floor(\n        4 + tf.log(sqrtarea * (1. / 224) + 1e-6) * (1.0 / np.log(2))), tf.int32)\n\n    # RoI levels range from 2~5 (not 6)\n    level_ids = [\n        tf.where(level <= 2),\n        tf.where(tf.equal(level, 3)),   # == is not supported\n        tf.where(tf.equal(level, 4)),\n        tf.where(level >= 5)]\n    level_ids = [tf.reshape(x, [-1], name='roi_level{}_id'.format(i + 2))\n                 for i, x in enumerate(level_ids)]\n    num_in_levels = [tf.size(x, name='num_roi_level{}'.format(i + 2))\n                     for i, x in enumerate(level_ids)]\n    add_moving_summary(*num_in_levels)\n\n    level_boxes = [tf.gather(boxes, ids) for ids in level_ids]\n    return level_ids, level_boxes", "output": "Assign boxes to level 2~5.\n\n    Args:\n        boxes (nx4):\n\n    Returns:\n        [tf.Tensor]: 4 tensors for level 2-5. Each tensor is a vector of indices of boxes in its level.\n        [tf.Tensor]: 4 tensors, the gathered boxes in each level.\n\n    Be careful that the returned tensor could be empty.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_family(families):\n    \"\"\"\"\"\"\n    if not isinstance(families, list):\n        families = [ families ]\n    for family in families:\n        if font_is_installed(family):\n            return family\n    else:\n        print(\"Warning: None of the following fonts is installed: %r\" % families)  # spyder: test-skip\n        return QFont().family()", "output": "Return the first installed font family in family list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_topic_rule(ruleName,\n            region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.delete_topic_rule(ruleName=ruleName)\n        return {'deleted': True}\n    except ClientError as e:\n        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Given a rule name, delete it.\n\n    Returns {deleted: true} if the rule was deleted and returns\n    {deleted: false} if the rule was not deleted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iot.delete_rule myrule", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_policy_template(self, policy):\n        \"\"\"\n        \n        \"\"\"\n\n        return self._policy_template_processor is not None and \\\n            isinstance(policy, dict) and \\\n            len(policy) == 1 and \\\n            self._policy_template_processor.has(list(policy.keys())[0]) is True", "output": "Is the given policy data a policy template? Policy templates is a dictionary with one key which is the name\n        of the template.\n\n        :param dict policy: Policy data\n        :return: True, if this is a policy template. False if it is not", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def calculate_per_unit_commission(order,\n                                  transaction,\n                                  cost_per_unit,\n                                  initial_commission,\n                                  min_trade_cost):\n    \"\"\"\n    \n    \"\"\"\n    additional_commission = abs(transaction.amount * cost_per_unit)\n\n    if order.commission == 0:\n        # no commission paid yet, pay at least the minimum plus a one-time\n        # exchange fee.\n        return max(min_trade_cost, additional_commission + initial_commission)\n    else:\n        # we've already paid some commission, so figure out how much we\n        # would be paying if we only counted per unit.\n        per_unit_total = \\\n            abs(order.filled * cost_per_unit) + \\\n            additional_commission + \\\n            initial_commission\n\n        if per_unit_total < min_trade_cost:\n            # if we haven't hit the minimum threshold yet, don't pay\n            # additional commission\n            return 0\n        else:\n            # we've exceeded the threshold, so pay more commission.\n            return per_unit_total - order.commission", "output": "If there is a minimum commission:\n        If the order hasn't had a commission paid yet, pay the minimum\n        commission.\n\n        If the order has paid a commission, start paying additional\n        commission once the minimum commission has been reached.\n\n    If there is no minimum commission:\n        Pay commission based on number of units in the transaction.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_active(self):\n    \"\"\"\n    \"\"\"\n    if not self._multiplexer:\n      return False\n\n    if self._index_cached is not None:\n      # If we already have computed the index, use it to determine whether\n      # the plugin should be active, and if so, return immediately.\n      if any(self._index_cached.values()):\n        return True\n\n    if self._multiplexer.PluginRunToTagToContent(metadata.PLUGIN_NAME):\n      # Text data is present in the multiplexer. No need to further check for\n      # data stored via the outdated plugin assets method.\n      return True\n\n    # We haven't conclusively determined if the plugin should be active. Launch\n    # a thread to compute index_impl() and return False to avoid blocking.\n    self._maybe_launch_index_impl_thread()\n\n    return False", "output": "Determines whether this plugin is active.\n\n    This plugin is only active if TensorBoard sampled any text summaries.\n\n    Returns:\n      Whether this plugin is active.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def infer_objects(self):\n        \"\"\"\n        \n        \"\"\"\n        # numeric=False necessary to only soft convert;\n        # python objects will still be converted to\n        # native numpy numeric types\n        return self._constructor(\n            self._data.convert(datetime=True, numeric=False,\n                               timedelta=True, coerce=False,\n                               copy=True)).__finalize__(self)", "output": "Attempt to infer better dtypes for object columns.\n\n        Attempts soft conversion of object-dtyped\n        columns, leaving non-object and unconvertible\n        columns unchanged. The inference rules are the\n        same as during normal Series/DataFrame construction.\n\n        .. versionadded:: 0.21.0\n\n        Returns\n        -------\n        converted : same type as input object\n\n        See Also\n        --------\n        to_datetime : Convert argument to datetime.\n        to_timedelta : Convert argument to timedelta.\n        to_numeric : Convert argument to numeric type.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [\"a\", 1, 2, 3]})\n        >>> df = df.iloc[1:]\n        >>> df\n           A\n        1  1\n        2  2\n        3  3\n\n        >>> df.dtypes\n        A    object\n        dtype: object\n\n        >>> df.infer_objects().dtypes\n        A    int64\n        dtype: object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset(self):\n        \"\"\"\"\"\"\n        # shuffle data\n        if self.is_train:\n            np.random.shuffle(self.idx)\n            self.data = _shuffle(self.data, self.idx)\n            self.label = _shuffle(self.label, self.idx)\n\n        if self.last_batch_handle == 'roll_over' and self.cursor > self.num_data:\n            self.cursor = -self.batch_size + (self.cursor % self.num_data) % self.batch_size\n        else:\n            self.cursor = -self.batch_size", "output": "Reset class MNISTCustomIter(mx.io.NDArrayIter):", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_float_info(self, field, data):\n        \"\"\"\n        \"\"\"\n        _check_call(_LIB.XGDMatrixSetFloatInfo(self.handle,\n                                               c_str(field),\n                                               c_array(ctypes.c_float, data),\n                                               len(data)))", "output": "Set float type property into the DMatrix.\n\n        Parameters\n        ----------\n        field: str\n            The field name of the information\n\n        data: numpy array\n            The array ofdata to be set", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def configure_snmp(community, snmp_port=161, snmp_trapport=161):\n    '''\n    \n    '''\n    _xml = \"\"\"<RIBCL VERSION=\"2.2\">\n                <LOGIN USER_LOGIN=\"x\" PASSWORD=\"y\">\n                  <RIB_INFO mode=\"write\">\n                    <MOD_GLOBAL_SETTINGS>\n                      <SNMP_ACCESS_ENABLED VALUE=\"Yes\"/>\n                      <SNMP_PORT VALUE=\"{0}\"/>\n                      <SNMP_TRAP_PORT VALUE=\"{1}\"/>\n                    </MOD_GLOBAL_SETTINGS>\n\n                   <MOD_SNMP_IM_SETTINGS>\n                     <SNMP_ADDRESS_1 VALUE=\"\"/>\n                     <SNMP_ADDRESS_1_ROCOMMUNITY VALUE=\"{2}\"/>\n                     <SNMP_ADDRESS_1_TRAPCOMMUNITY VERSION=\"\" VALUE=\"\"/>\n                     <RIB_TRAPS VALUE=\"Y\"/>\n                     <OS_TRAPS VALUE=\"Y\"/>\n                     <SNMP_PASSTHROUGH_STATUS VALUE=\"N\"/>\n                  </MOD_SNMP_IM_SETTINGS>\n                </RIB_INFO>\n              </LOGIN>\n           </RIBCL>\"\"\".format(snmp_port, snmp_trapport, community)\n\n    return __execute_cmd('Configure_SNMP', _xml)", "output": "Configure SNMP\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ilo.configure_snmp [COMMUNITY STRING] [SNMP PORT] [SNMP TRAP PORT]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attach_cluster(config_file, start, use_tmux, override_cluster_name, new):\n    \"\"\"\n    \"\"\"\n\n    if use_tmux:\n        if new:\n            cmd = \"tmux new\"\n        else:\n            cmd = \"tmux attach || tmux new\"\n    else:\n        if new:\n            cmd = \"screen -L\"\n        else:\n            cmd = \"screen -L -xRR\"\n\n    exec_cluster(config_file, cmd, False, False, False, False, start,\n                 override_cluster_name, None)", "output": "Attaches to a screen for the specified cluster.\n\n    Arguments:\n        config_file: path to the cluster yaml\n        start: whether to start the cluster if it isn't up\n        use_tmux: whether to use tmux as multiplexer\n        override_cluster_name: set the name of the cluster\n        new: whether to force a new screen", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put(self, entity):\n        \"\"\"\n        \"\"\"\n        if self._options.HasField(\"read_only\"):\n            raise RuntimeError(\"Transaction is read only\")\n        else:\n            super(Transaction, self).put(entity)", "output": "Adds an entity to be committed.\n\n        Ensures the transaction is not marked readonly.\n        Please see documentation at\n        :meth:`~google.cloud.datastore.batch.Batch.put`\n\n        :type entity: :class:`~google.cloud.datastore.entity.Entity`\n        :param entity: the entity to be saved.\n\n        :raises: :class:`RuntimeError` if the transaction\n                 is marked ReadOnly", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bot_has_any_role(*items):\n    \"\"\"\n    \"\"\"\n    def predicate(ctx):\n        ch = ctx.channel\n        if not isinstance(ch, discord.abc.GuildChannel):\n            raise NoPrivateMessage()\n\n        me = ch.guild.me\n        getter = functools.partial(discord.utils.get, me.roles)\n        if any(getter(id=item) is not None if isinstance(item, int) else getter(name=item) is not None for item in items):\n            return True\n        raise BotMissingAnyRole(items)\n    return check(predicate)", "output": "Similar to :func:`.has_any_role` except checks if the bot itself has\n    any of the roles listed.\n\n    This check raises one of two special exceptions, :exc:`.BotMissingAnyRole` if the bot\n    is missing all roles, or :exc:`.NoPrivateMessage` if it is used in a private message.\n    Both inherit from :exc:`.CheckFailure`.\n\n    .. versionchanged:: 1.1.0\n\n        Raise :exc:`.BotMissingAnyRole` or :exc:`.NoPrivateMessage`\n        instead of generic checkfailure", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_graph_to_file(graph_file_name, module_spec, class_count):\n  \"\"\"\"\"\"\n  sess, _, _, _, _, _ = build_eval_session(module_spec, class_count)\n  graph = sess.graph\n\n  output_graph_def = tf.graph_util.convert_variables_to_constants(\n      sess, graph.as_graph_def(), [FLAGS.final_tensor_name])\n\n  with tf.gfile.GFile(graph_file_name, 'wb') as f:\n    f.write(output_graph_def.SerializeToString())", "output": "Saves an graph to file, creating a valid quantized one if necessary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_fmadm_config(output):\n    '''\n    \n    '''\n    result = []\n    output = output.split(\"\\n\")\n\n    # extract header\n    header = [field for field in output[0].lower().split(\" \") if field]\n    del output[0]\n\n    # parse entries\n    for entry in output:\n        entry = [item for item in entry.split(\" \") if item]\n        entry = entry[0:3] + [\" \".join(entry[3:])]\n\n        # prepare component\n        component = OrderedDict()\n        for field in header:\n            component[field] = entry[header.index(field)]\n\n        result.append(component)\n\n    # keying\n    keyed_result = OrderedDict()\n    for component in result:\n        keyed_result[component['module']] = component\n        del keyed_result[component['module']]['module']\n\n    result = keyed_result\n\n    return result", "output": "Parsbb fmdump/fmadm output", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_cert_path(name):\n    '''\n    \n    '''\n    cmd = r\"Test-Path -Path '{0}'\".format(name)\n\n    if not ast.literal_eval(_cmd_run(cmd=cmd)):\n        raise SaltInvocationError(r\"Invalid path specified: {0}\".format(name))", "output": "Ensure that the certificate path, as determind from user input, is valid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def append_all_agent_batch_to_update_buffer(self, key_list=None, batch_size=None, training_length=None):\n        \"\"\"\n        \n        \"\"\"\n        for agent_id in self.keys():\n            self.append_update_buffer(agent_id, key_list, batch_size, training_length)", "output": "Appends the buffer of all agents to the update buffer.\n        :param key_list: The fields that must be added. If None: all fields will be appended.\n        :param batch_size: The number of elements that must be appended. If None: All of them will be.\n        :param training_length: The length of the samples that must be appended. If None: only takes one element.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def AddRunsFromDirectory(self, path, name=None):\n    \"\"\"\n    \"\"\"\n    logger.info('Starting AddRunsFromDirectory: %s (as %s)', path, name)\n    for subdir in io_wrapper.GetLogdirSubdirectories(path):\n      logger.info('Processing directory %s', subdir)\n      if subdir not in self._run_loaders:\n        logger.info('Creating DB loader for directory %s', subdir)\n        names = self._get_exp_and_run_names(path, subdir, name)\n        experiment_name, run_name = names\n        self._run_loaders[subdir] = _RunLoader(\n            subdir=subdir,\n            experiment_name=experiment_name,\n            run_name=run_name)\n    logger.info('Done with AddRunsFromDirectory: %s', path)", "output": "Load runs from a directory; recursively walks subdirectories.\n\n    If path doesn't exist, no-op. This ensures that it is safe to call\n      `AddRunsFromDirectory` multiple times, even before the directory is made.\n\n    Args:\n      path: A string path to a directory to load runs from.\n      name: Optional, specifies a name for the experiment under which the\n        runs from this directory hierarchy will be imported. If omitted, the\n        path will be used as the name.\n\n    Raises:\n      ValueError: If the path exists and isn't a directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deconv2d(\n    input_, output_shape, k_h, k_w, d_h, d_w, stddev=0.02, name=\"deconv2d\"):\n  \"\"\"\"\"\"\n  with tf.variable_scope(name):\n    w = tf.get_variable(\n        \"w\", [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n        initializer=tf.random_normal_initializer(stddev=stddev))\n    deconv = tf.nn.conv2d_transpose(\n        input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1])\n    biases = tf.get_variable(\n        \"biases\", [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n    return tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())", "output": "Deconvolution layer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def three_sum(array):\n    \"\"\"\n    \n    \"\"\"\n    res = set()\n    array.sort()\n    for i in range(len(array) - 2):\n        if i > 0 and array[i] == array[i - 1]:\n            continue\n        l, r = i + 1, len(array) - 1\n        while l < r:\n            s = array[i] + array[l] + array[r]\n            if s > 0:\n                r -= 1\n            elif s < 0:\n                l += 1\n            else:\n                # found three sum\n                res.add((array[i], array[l], array[r]))\n\n                # remove duplicates\n                while l < r and array[l] == array[l + 1]:\n                    l += 1\n\n                while l < r and array[r] == array[r - 1]:\n                    r -= 1\n\n                l += 1\n                r -= 1\n    return res", "output": ":param array: List[int]\n    :return: Set[ Tuple[int, int, int] ]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _result_type_many(*arrays_and_dtypes):\n    \"\"\"  \"\"\"\n    try:\n        return np.result_type(*arrays_and_dtypes)\n    except ValueError:\n        # we have > NPY_MAXARGS terms in our expression\n        return reduce(np.result_type, arrays_and_dtypes)", "output": "wrapper around numpy.result_type which overcomes the NPY_MAXARGS (32)\n    argument limit", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compress_histogram_proto(histo, bps=NORMAL_HISTOGRAM_BPS):\n  \"\"\"\n  \"\"\"\n  # See also: Histogram::Percentile() in core/lib/histogram/histogram.cc\n  if not histo.num:\n    return [CompressedHistogramValue(b, 0.0) for b in bps]\n  bucket = np.array(histo.bucket)\n  bucket_limit = list(histo.bucket_limit)\n  weights = (bucket * bps[-1] / (bucket.sum() or 1.0)).cumsum()\n  values = []\n  j = 0\n  while j < len(bps):\n    i = np.searchsorted(weights, bps[j], side='right')\n    while i < len(weights):\n      cumsum = weights[i]\n      cumsum_prev = weights[i - 1] if i > 0 else 0.0\n      if cumsum == cumsum_prev:  # prevent lerp divide by zero\n        i += 1\n        continue\n      if not i or not cumsum_prev:\n        lhs = histo.min\n      else:\n        lhs = max(bucket_limit[i - 1], histo.min)\n      rhs = min(bucket_limit[i], histo.max)\n      weight = _lerp(bps[j], cumsum_prev, cumsum, lhs, rhs)\n      values.append(CompressedHistogramValue(bps[j], weight))\n      j += 1\n      break\n    else:\n      break\n  while j < len(bps):\n    values.append(CompressedHistogramValue(bps[j], histo.max))\n    j += 1\n  return values", "output": "Creates fixed size histogram by adding compression to accumulated state.\n\n  This routine transforms a histogram at a particular step by interpolating its\n  variable number of buckets to represent their cumulative weight at a constant\n  number of compression points. This significantly reduces the size of the\n  histogram and makes it suitable for a two-dimensional area plot where the\n  output of this routine constitutes the ranges for a single x coordinate.\n\n  Args:\n    histo: A HistogramProto object.\n    bps: Compression points represented in basis points, 1/100ths of a percent.\n        Defaults to normal distribution.\n\n  Returns:\n    List of values for each basis point.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_pb(cls, cell_pb):\n        \"\"\"\n        \"\"\"\n        if cell_pb.labels:\n            return cls(cell_pb.value, cell_pb.timestamp_micros, labels=cell_pb.labels)\n        else:\n            return cls(cell_pb.value, cell_pb.timestamp_micros)", "output": "Create a new cell from a Cell protobuf.\n\n        :type cell_pb: :class:`._generated.data_pb2.Cell`\n        :param cell_pb: The protobuf to convert.\n\n        :rtype: :class:`Cell`\n        :returns: The cell corresponding to the protobuf.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_config(self):\n        \"\"\"\n        \"\"\"\n        config = super(LinearAnnealedPolicy, self).get_config()\n        config['attr'] = self.attr\n        config['value_max'] = self.value_max\n        config['value_min'] = self.value_min\n        config['value_test'] = self.value_test\n        config['nb_steps'] = self.nb_steps\n        config['inner_policy'] = get_object_config(self.inner_policy)\n        return config", "output": "Return configurations of LinearAnnealedPolicy\n\n        # Returns\n            Dict of config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate(self,\n                 _portfolio,\n                 _account,\n                 _algo_datetime,\n                 _algo_current_data):\n        \"\"\"\n        \n        \"\"\"\n        if _account.leverage > self.max_leverage:\n            self.fail()", "output": "Fail if the leverage is greater than the allowed leverage.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_argname_value(self, argname):\n        '''\n        \n        '''\n        # Let's see if there's a private function to get the value\n        argvalue = getattr(self, '__get_{0}__'.format(argname), None)\n        if argvalue is not None and callable(argvalue):\n            argvalue = argvalue()\n        if argvalue is None:\n            # Let's see if the value is defined as a public class variable\n            argvalue = getattr(self, argname, None)\n        if argvalue is None:\n            # Let's see if it's defined as a private class variable\n            argvalue = getattr(self, '__{0}__'.format(argname), None)\n        if argvalue is None:\n            # Let's look for it in the extra dictionary\n            argvalue = self.extra.get(argname, None)\n        return argvalue", "output": "Return the argname value looking up on all possible attributes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imagetransformer_b12l_4h_uncond_dr03_tpu():\n  \"\"\"\"\"\"\n  hparams = imagetransformer_b12l_4h_b256_uncond_dr03_tpu()\n  hparams.learning_rate = 0.2\n  hparams.learning_rate_warmup_steps = 4000\n  hparams.layer_preprocess_sequence = \"none\"\n  hparams.layer_postprocess_sequence = \"dan\"\n  hparams.layer_prepostprocess_dropout = 0.3\n  return hparams", "output": "TPU related small model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_config(variable, value):\n    '''\n    \n    '''\n\n    if _TRAFFICCTL:\n        cmd = _traffic_ctl('config', 'set', variable, value)\n    else:\n        cmd = _traffic_line('-s', variable, '-v', value)\n\n    log.debug('Setting %s to %s', variable, value)\n    return _subprocess(cmd)", "output": "Set the value of a Traffic Server configuration variable.\n\n    variable\n        Name of a Traffic Server configuration variable.\n\n    value\n        The new value to set.\n\n    .. versionadded:: 2016.11.0\n\n    .. code-block:: bash\n\n        salt '*' trafficserver.set_config proxy.config.http.keep_alive_post_out 0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exit_standby(name, instance_ids, should_decrement_desired_capacity=False,\n                  region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    conn = _get_conn_autoscaling_boto3(\n        region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        response = conn.exit_standby(\n            InstanceIds=instance_ids,\n            AutoScalingGroupName=name)\n    except ClientError as e:\n        err = __utils__['boto3.get_error'](e)\n        if e.response.get('Error', {}).get('Code') == 'ResourceNotFoundException':\n            return {'exists': False}\n        return {'error': err}\n    return all(activity['StatusCode'] != 'Failed' for activity in response['Activities'])", "output": "Exit desired instances from StandBy mode\n\n    .. versionadded:: 2016.11.0\n\n    CLI example::\n\n        salt-call boto_asg.exit_standby my_autoscale_group_name '[\"i-xxxxxx\"]'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _determine_notification_info(notification_arn,\n                                 notification_arn_from_pillar,\n                                 notification_types,\n                                 notification_types_from_pillar):\n    '''\n    \n    '''\n    pillar_arn_list = copy.deepcopy(\n        __salt__['config.option'](notification_arn_from_pillar, {})\n    )\n    pillar_arn = None\n    if pillar_arn_list:\n        pillar_arn = pillar_arn_list[0]\n    pillar_notification_types = copy.deepcopy(\n        __salt__['config.option'](notification_types_from_pillar, {})\n    )\n    arn = notification_arn if notification_arn else pillar_arn\n    types = notification_types if notification_types else pillar_notification_types\n    return (arn, types)", "output": "helper method for present.  ensure that notification_configs are set", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_psexec_command(cmd, args, host, username, password, port=445):\n    '''\n    \n    '''\n    if has_winexe() and not HAS_PSEXEC:\n        ret_code = run_winexe_command(cmd, args, host, username, password, port)\n        return None, None, ret_code\n    service_name = 'PS-Exec-{0}'.format(uuid.uuid4())\n    stdout, stderr, ret_code = '', '', None\n    client = Client(host, username, password, port=port, encrypt=False, service_name=service_name)\n    client.connect()\n    try:\n        client.create_service()\n        stdout, stderr, ret_code = client.run_executable(cmd, args)\n    finally:\n        client.remove_service()\n        client.disconnect()\n    return stdout, stderr, ret_code", "output": "Run a command remotly using the psexec protocol", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_default_moe_hparams(hparams):\n  \"\"\"\"\"\"\n  hparams.moe_num_experts = 16\n  hparams.moe_loss_coef = 1e-2\n  hparams.add_hparam(\"moe_gating\", \"top_2\")\n  # Experts have fixed capacity per batch.  We need some extra capacity\n  # in case gating is not perfectly balanced.\n  # moe_capacity_factor_* should be set to a value >=1.\n  hparams.add_hparam(\"moe_capacity_factor_train\", 1.25)\n  hparams.add_hparam(\"moe_capacity_factor_eval\", 2.0)\n  hparams.add_hparam(\"moe_capacity_factor_second_level\", 1.0)\n  # Each expert has a hidden layer with this size.\n  hparams.add_hparam(\"moe_hidden_size\", 4096)\n  # For gating, divide inputs into groups of this size before gating.\n  # Each group sends the same number of inputs to each expert.\n  # Ideally, the group size would be the whole batch, but this is expensive\n  # due to our use of matrix multiplication for reordering.\n  hparams.add_hparam(\"moe_group_size\", 1024)\n  # For top_2 gating, whether to impose an additional loss in order to make\n  # the experts equally used as the second-place expert.\n  hparams.add_hparam(\"moe_use_second_place_loss\", 0)\n  # In top_2 gating, policy for whether to use a second-place expert.\n  # Legal values are:\n  #    \"all\": always\n  #    \"none\": never\n  #    \"threshold\": if gate value > the given threshold\n  #    \"random\": if gate value > threshold*random_uniform(0,1)\n  hparams.add_hparam(\"moe_second_policy_train\", \"random\")\n  hparams.add_hparam(\"moe_second_policy_eval\", \"random\")\n  hparams.add_hparam(\"moe_second_threshold_train\", 0.2)\n  hparams.add_hparam(\"moe_second_threshold_eval\", 0.2)", "output": "Add necessary hyperparameters for mixture-of-experts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_namespaces(apiserver_url, name=\"\"):\n    ''''''\n    # Prepare URL\n    url = \"{0}/api/v1/namespaces/{1}\".format(apiserver_url, name)\n    # Make request\n    ret = http.query(url)\n    if ret.get(\"body\"):\n        return salt.utils.json.loads(ret.get(\"body\"))\n    else:\n        return None", "output": "Get namespace is namespace is defined otherwise return all namespaces", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prefetch(self, file_size=None):\n        \"\"\"\n        \n        \"\"\"\n        if file_size is None:\n            file_size = self.stat().st_size\n\n        # queue up async reads for the rest of the file\n        chunks = []\n        n = self._realpos\n        while n < file_size:\n            chunk = min(self.MAX_REQUEST_SIZE, file_size - n)\n            chunks.append((n, chunk))\n            n += chunk\n        if len(chunks) > 0:\n            self._start_prefetch(chunks)", "output": "Pre-fetch the remaining contents of this file in anticipation of future\n        `.read` calls.  If reading the entire file, pre-fetching can\n        dramatically improve the download speed by avoiding roundtrip latency.\n        The file's contents are incrementally buffered in a background thread.\n\n        The prefetched data is stored in a buffer until read via the `.read`\n        method.  Once data has been read, it's removed from the buffer.  The\n        data may be read in a random order (using `.seek`); chunks of the\n        buffer that haven't been read will continue to be buffered.\n\n        :param int file_size:\n            When this is ``None`` (the default), this method calls `stat` to\n            determine the remote file size. In some situations, doing so can\n            cause exceptions or hangs (see `#562\n            <https://github.com/paramiko/paramiko/pull/562>`_); as a\n            workaround, one may call `stat` explicitly and pass its value in\n            via this parameter.\n\n        .. versionadded:: 1.5.1\n        .. versionchanged:: 1.16.0\n            The ``file_size`` parameter was added (with no default value).\n        .. versionchanged:: 1.16.1\n            The ``file_size`` parameter was made optional for backwards\n            compatibility.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parseDoc(cur):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlParseDoc(cur)\n    if ret is None:raise parserError('xmlParseDoc() failed')\n    return xmlDoc(_obj=ret)", "output": "parse an XML in-memory document and build a tree.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_path(pathname):\n    \"\"\"\n    \"\"\"\n    if os.sep == '/':\n        return pathname\n    if not pathname:\n        return pathname\n    if pathname[0] == '/':\n        raise ValueError(\"path '%s' cannot be absolute\" % pathname)\n    if pathname[-1] == '/':\n        raise ValueError(\"path '%s' cannot end with '/'\" % pathname)\n\n    paths = pathname.split('/')\n    while os.curdir in paths:\n        paths.remove(os.curdir)\n    if not paths:\n        return os.curdir\n    return os.path.join(*paths)", "output": "Return 'pathname' as a name that will work on the native filesystem.\n\n    The path is split on '/' and put back together again using the current\n    directory separator.  Needed because filenames in the setup script are\n    always supplied in Unix style, and have to be converted to the local\n    convention before we can actually use them in the filesystem.  Raises\n    ValueError on non-Unix-ish systems if 'pathname' either starts or\n    ends with a slash.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def allow_port(port, proto='tcp', direction='both'):\n    '''\n    \n    '''\n\n    ports = get_ports(proto=proto, direction=direction)\n    direction = direction.upper()\n    _validate_direction_and_proto(direction, proto)\n    directions = build_directions(direction)\n    results = []\n    for direction in directions:\n        _ports = ports[direction]\n        _ports.append(port)\n        results += allow_ports(_ports, proto=proto, direction=direction)\n    return results", "output": "Like allow_ports, but it will append to the\n    existing entry instead of replacing it.\n    Takes a single port instead of a list of ports.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' csf.allow_port 22 proto='tcp' direction='in'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def purge(name, delete_key=True):\n    '''\n    \n    '''\n    ret = {}\n    client = salt.client.get_local_client(__opts__['conf_file'])\n    data = vm_info(name, quiet=True)\n    if not data:\n        __jid_event__.fire_event({'error': 'Failed to find VM {0} to purge'.format(name)}, 'progress')\n        return 'fail'\n    host = next(six.iterkeys(data))\n    try:\n        cmd_ret = client.cmd_iter(\n                host,\n                'virt.purge',\n                [name, True],\n                timeout=600)\n    except SaltClientError as client_error:\n        return 'Virtual machine {0} could not be purged: {1}'.format(name, client_error)\n\n    for comp in cmd_ret:\n        ret.update(comp)\n\n    if delete_key:\n        log.debug('Deleting key %s', name)\n        skey = salt.key.Key(__opts__)\n        skey.delete_key(name)\n    __jid_event__.fire_event({'message': 'Purged VM {0}'.format(name)}, 'progress')\n    return 'good'", "output": "Destroy the named VM", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_bboxes_to_albumentations(bboxes, source_format, rows, cols, check_validity=False):\n    \"\"\"\n    \"\"\"\n    return [convert_bbox_to_albumentations(bbox, source_format, rows, cols, check_validity) for bbox in bboxes]", "output": "Convert a list bounding boxes from a format specified in `source_format` to the format used by albumentations", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generateLinearInput(intercept, weights, xMean, xVariance,\n                            nPoints, seed, eps):\n        \"\"\"\n        \n        \"\"\"\n        weights = [float(weight) for weight in weights]\n        xMean = [float(mean) for mean in xMean]\n        xVariance = [float(var) for var in xVariance]\n        return list(callMLlibFunc(\n            \"generateLinearInputWrapper\", float(intercept), weights, xMean,\n            xVariance, int(nPoints), int(seed), float(eps)))", "output": ":param: intercept bias factor, the term c in X'w + c\n        :param: weights   feature vector, the term w in X'w + c\n        :param: xMean     Point around which the data X is centered.\n        :param: xVariance Variance of the given data\n        :param: nPoints   Number of points to be generated\n        :param: seed      Random Seed\n        :param: eps       Used to scale the noise. If eps is set high,\n                          the amount of gaussian noise added is more.\n\n        Returns a list of LabeledPoints of length nPoints", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cyclic_dt_features(d:Union[date,datetime], time:bool=True, add_linear:bool=False)->List[float]:\n    \"\"\n    tt,fs = d.timetuple(), [np.cos, np.sin]\n    day_year,days_month = tt.tm_yday, calendar.monthrange(d.year, d.month)[1]\n    days_year = 366 if calendar.isleap(d.year) else 365\n    rs = d.weekday()/7, (d.day-1)/days_month, (d.month-1)/12, (day_year-1)/days_year\n    feats = [f(r * 2 * np.pi) for r in rs for f in fs]\n    if time and isinstance(d, datetime) and type(d) != date:\n        rs = tt.tm_hour/24, tt.tm_hour%12/12, tt.tm_min/60, tt.tm_sec/60\n        feats += [f(r * 2 * np.pi) for r in rs for f in fs]\n    if add_linear:\n        if type(d) == date: feats.append(d.year + rs[-1])\n        else:\n            secs_in_year = (datetime(d.year+1, 1, 1) - datetime(d.year, 1, 1)).total_seconds()\n            feats.append(d.year + ((d - datetime(d.year, 1, 1)).total_seconds() / secs_in_year))\n    return feats", "output": "Calculate the cos and sin of date/time cycles.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cyclic_dt_feat_names(time:bool=True, add_linear:bool=False)->List[str]:\n    \"\"\n    fs = ['cos','sin']\n    attr = [f'{r}_{f}' for r in 'weekday day_month month_year day_year'.split() for f in fs]\n    if time: attr += [f'{r}_{f}' for r in 'hour clock min sec'.split() for f in fs]\n    if add_linear: attr.append('year_lin')\n    return attr", "output": "Return feature names of date/time cycles as produced by `cyclic_dt_features`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def table_exists(client, table_reference):\n    \"\"\"\n    \"\"\"\n    from google.cloud.exceptions import NotFound\n\n    try:\n        client.get_table(table_reference)\n        return True\n    except NotFound:\n        return False", "output": "Return if a table exists.\n\n    Args:\n        client (google.cloud.bigquery.client.Client):\n            A client to connect to the BigQuery API.\n        table_reference (google.cloud.bigquery.table.TableReference):\n            A reference to the table to look for.\n\n    Returns:\n        bool: ``True`` if the table exists, ``False`` otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, resource):\n        \"\"\"\n        \"\"\"\n        name = resource.get(\"name\")\n        instance = cls(name)\n        type_resources = {}\n        types = instance.struct_types\n        for item in resource[\"parameterType\"][\"structTypes\"]:\n            types[item[\"name\"]] = item[\"type\"][\"type\"]\n            type_resources[item[\"name\"]] = item[\"type\"]\n        struct_values = resource[\"parameterValue\"][\"structValues\"]\n        for key, value in struct_values.items():\n            type_ = types[key]\n            converted = None\n            if type_ == \"STRUCT\":\n                struct_resource = {\n                    \"name\": key,\n                    \"parameterType\": type_resources[key],\n                    \"parameterValue\": value,\n                }\n                converted = StructQueryParameter.from_api_repr(struct_resource)\n            elif type_ == \"ARRAY\":\n                struct_resource = {\n                    \"name\": key,\n                    \"parameterType\": type_resources[key],\n                    \"parameterValue\": value,\n                }\n                converted = ArrayQueryParameter.from_api_repr(struct_resource)\n            else:\n                value = value[\"value\"]\n                converted = _QUERY_PARAMS_FROM_JSON[type_](value, None)\n            instance.struct_values[key] = converted\n        return instance", "output": "Factory: construct parameter from JSON resource.\n\n        :type resource: dict\n        :param resource: JSON mapping of parameter\n\n        :rtype: :class:`~google.cloud.bigquery.query.StructQueryParameter`\n        :returns: instance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def strip_doc_string(proto):  # type: (google.protobuf.message.Message) -> None\n    \"\"\"\n    \n    \"\"\"\n    assert isinstance(proto, google.protobuf.message.Message)\n    for descriptor in proto.DESCRIPTOR.fields:\n        if descriptor.name == 'doc_string':\n            proto.ClearField(descriptor.name)\n        elif descriptor.type == descriptor.TYPE_MESSAGE:\n            if descriptor.label == descriptor.LABEL_REPEATED:\n                for x in getattr(proto, descriptor.name):\n                    strip_doc_string(x)\n            elif proto.HasField(descriptor.name):\n                strip_doc_string(getattr(proto, descriptor.name))", "output": "Empties `doc_string` field on any nested protobuf messages", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _page_to_title(page):\n  \"\"\"\n  \"\"\"\n  # print(\"page=%s\" % page)\n  start_tag = u\"<title>\"\n  end_tag = u\"</title>\"\n  start_pos = page.find(start_tag)\n  end_pos = page.find(end_tag)\n  assert start_pos != -1\n  assert end_pos != -1\n  start_pos += len(start_tag)\n  return page[start_pos:end_pos]", "output": "Extract the title from a page.\n\n  Args:\n    page: a unicode string\n  Returns:\n    a unicode string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shape(self):\n        \"\"\"\n        \"\"\"\n        ndim = mx_int()\n        pdata = ctypes.POINTER(mx_int)()\n        check_call(_LIB.MXNDArrayGetShapeEx(\n            self.handle, ctypes.byref(ndim), ctypes.byref(pdata)))\n        if ndim.value == -1:\n            return None\n        else:\n            return tuple(pdata[:ndim.value])", "output": "Tuple of array dimensions.\n\n        Examples\n        --------\n        >>> x = mx.nd.array([1, 2, 3, 4])\n        >>> x.shape\n        (4L,)\n        >>> y = mx.nd.zeros((2, 3, 4))\n        >>> y.shape\n        (2L, 3L, 4L)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_word_level_vocab(self):\n        \"\"\"\n        \"\"\"\n\n        def simple_tokenize(source_str, token_delim=' ', seq_delim='\\n'):\n            return list(filter(None, re.split(token_delim + '|' + seq_delim, source_str)))\n\n        return VocabProvider._create_squad_vocab(simple_tokenize, self._dataset)", "output": "Provides word level vocabulary\n\n        Returns\n        -------\n        Vocab\n            Word level vocabulary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def truncate_to_string(num, precision=0):\n        \"\"\"\"\"\"\n        if precision > 0:\n            parts = ('{0:.%df}' % precision).format(Decimal(num)).split('.')\n            decimal_digits = parts[1][:precision].rstrip('0')\n            decimal_digits = decimal_digits if len(decimal_digits) else '0'\n            return parts[0] + '.' + decimal_digits\n        return ('%d' % num)", "output": "Deprecated, todo: remove references from subclasses", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main_target_default_build (self, specification, project):\n        \"\"\" \n        \"\"\"\n        assert is_iterable_typed(specification, basestring)\n        assert isinstance(project, ProjectTarget)\n        if specification:\n            return property_set.create_with_validation(specification)\n        else:\n            return project.get ('default-build')", "output": "Return the default build value to use when declaring a main target,\n            which is obtained by using specified value if not empty and parent's\n            default build attribute otherwise.\n            specification:  Default build explicitly specified for a main target\n            project:        Project where the main target is to be declared", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def max_element (elements, ordered = None):\n    \"\"\" \n    \"\"\"\n    assert is_iterable(elements)\n    assert callable(ordered) or ordered is None\n    if not ordered: ordered = operator.lt\n\n    max = elements [0]\n    for e in elements [1:]:\n        if ordered (max, e):\n            max = e\n\n    return max", "output": "Returns the maximum number in 'elements'. Uses 'ordered' for comparisons,\n        or '<' is none is provided.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put(self, message):\n    \"\"\"\n    \"\"\"\n    with self._outgoing_lock:\n      self._outgoing.append(message)\n      self._outgoing_counter += 1\n\n      # Check to see if there are pending queues waiting for the item.\n      if self._outgoing_counter in self._outgoing_pending_queues:\n        for q in self._outgoing_pending_queues[self._outgoing_counter]:\n          q.put(message)\n        del self._outgoing_pending_queues[self._outgoing_counter]", "output": "Put a message into the outgoing message stack.\n\n    Outgoing message will be stored indefinitely to support multi-users.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bottom(self, objects: Set[Object]) -> Set[Object]:\n        \"\"\"\n        \n        \"\"\"\n        objects_per_box = self._separate_objects_by_boxes(objects)\n        return_set: Set[Object] = set()\n        for _, box_objects in objects_per_box.items():\n            max_y_loc = max([obj.y_loc for obj in box_objects])\n            return_set.update(set([obj for obj in box_objects if obj.y_loc == max_y_loc]))\n        return return_set", "output": "Return the bottom most objects(i.e. maximum y_loc). The comparison is done separately for\n        each box.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_rollup_caps(self, id=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_rollup\", \"data\", id), params=params\n        )", "output": "`<>`_\n\n        :arg id: The ID of the index to check rollup capabilities on, or left\n            blank for all jobs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def login(self, user=None, password=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        cookies = kwargs.get('cookies')\n        if cookies is None:\n            raise TypeError('\u96ea\u7403\u767b\u9646\u9700\u8981\u8bbe\u7f6e cookies\uff0c \u5177\u4f53\u89c1'\n                            'https://smalltool.github.io/2016/08/02/cookie/')\n        headers = self._generate_headers()\n        self.s.headers.update(headers)\n\n        self.s.get(self.LOGIN_PAGE)\n\n        cookie_dict = helpers.parse_cookies_str(cookies)\n        self.s.cookies.update(cookie_dict)\n\n        log.info('\u767b\u5f55\u6210\u529f')", "output": "\u96ea\u7403\u767b\u9646\uff0c \u9700\u8981\u8bbe\u7f6e cookies\n        :param cookies: \u96ea\u7403\u767b\u9646\u9700\u8981\u8bbe\u7f6e cookies\uff0c \u5177\u4f53\u89c1\n            https://smalltool.github.io/2016/08/02/cookie/\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def _retrieve_guilds_before_strategy(self, retrieve):\n        \"\"\"\"\"\"\n        before = self.before.id if self.before else None\n        data = await self.get_guilds(retrieve, before=before)\n        if len(data):\n            if self.limit is not None:\n                self.limit -= retrieve\n            self.before = Object(id=int(data[-1]['id']))\n        return data", "output": "Retrieve guilds using before parameter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_SU_save_stock_block(client=DATABASE, ui_log=None, ui_progress=None):\n    \"\"\"\n    \"\"\"\n\n    client.drop_collection('stock_block')\n    coll = client.stock_block\n    coll.create_index('code')\n\n    try:\n        QA_util_log_info(\n            '##JOB09 Now Saving STOCK_BlOCK ====',\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=5000\n        )\n        coll.insert_many(\n            QA_util_to_json_from_pandas(QA_fetch_get_stock_block('tdx'))\n        )\n        QA_util_log_info(\n            'tdx Block ====',\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=5000\n        )\n\n        # \ud83d\udee0todo fixhere here \u83b7\u53d6\u540c\u82b1\u987a\u677f\u5757\uff0c \u8fd8\u662f\u8c03\u7528tdx\u7684\n        coll.insert_many(\n            QA_util_to_json_from_pandas(QA_fetch_get_stock_block('ths'))\n        )\n        QA_util_log_info(\n            'ths Block ====',\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=8000\n        )\n\n        QA_util_log_info(\n            '\u5b8c\u6210\u80a1\u7968\u677f\u5757\u83b7\u53d6=',\n            ui_log=ui_log,\n            ui_progress=ui_progress,\n            ui_progress_int_value=10000\n        )\n\n    except Exception as e:\n        QA_util_log_info(e, ui_log=ui_log)\n        print(\" Error save_tdx.QA_SU_save_stock_block exception!\")\n        pass", "output": "save stock_block\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def maybe_inspect_zip(models):\n    \n    '''\n\n    if not(is_zip_file(models)):\n        return models\n\n    if len(models) > 1:\n        return models\n\n    if len(models) < 1:\n        raise AssertionError('No models at all')\n\n    return zipfile.ZipFile(models[0]).namelist()", "output": "r'''\n    Detect if models is a list of protocolbuffer files or a ZIP file.\n    If the latter, then unzip it and return the list of protocolbuffer files\n    that were inside.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_tall_pretrain_lm_tpu_adafactor():\n  \"\"\"\"\"\"\n  hparams = transformer_tall_pretrain_lm()\n  update_hparams_for_tpu(hparams)\n  hparams.max_length = 1024\n  # For multi-problem on TPU we need it in absolute examples.\n  hparams.batch_size = 8\n  hparams.multiproblem_vocab_size = 2**16\n  return hparams", "output": "Hparams for transformer on LM pretraining (with 64k vocab) on TPU.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_open_orders(self, asset=None):\n        \"\"\"\n        \"\"\"\n        if asset is None:\n            return {\n                key: [order.to_api_obj() for order in orders]\n                for key, orders in iteritems(self.blotter.open_orders)\n                if orders\n            }\n        if asset in self.blotter.open_orders:\n            orders = self.blotter.open_orders[asset]\n            return [order.to_api_obj() for order in orders]\n        return []", "output": "Retrieve all of the current open orders.\n\n        Parameters\n        ----------\n        asset : Asset\n            If passed and not None, return only the open orders for the given\n            asset instead of all open orders.\n\n        Returns\n        -------\n        open_orders : dict[list[Order]] or list[Order]\n            If no asset is passed this will return a dict mapping Assets\n            to a list containing all the open orders for the asset.\n            If an asset is passed then this will return a list of the open\n            orders for this asset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def down(removekeys=False, tgt='*', tgt_type='glob', timeout=None, gather_job_timeout=None):\n    '''\n    \n\n    '''\n    ret = status(output=False,\n                 tgt=tgt,\n                 tgt_type=tgt_type,\n                 timeout=timeout,\n                 gather_job_timeout=gather_job_timeout\n    ).get('down', [])\n    for minion in ret:\n        if removekeys:\n            wheel = salt.wheel.Wheel(__opts__)\n            wheel.call_func('key.delete', match=minion)\n    return ret", "output": ".. versionchanged:: 2017.7.0\n        The ``expr_form`` argument has been renamed to ``tgt_type``, earlier\n        releases must use ``expr_form``.\n\n    Print a list of all the down or unresponsive salt minions\n    Optionally remove keys of down minions\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run manage.down\n        salt-run manage.down removekeys=True\n        salt-run manage.down tgt=\"webservers\" tgt_type=\"nodegroup\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_frame_l1():\n  \"\"\"\"\"\"\n  hparams = next_frame_basic_deterministic()\n  hparams.loss[\"targets\"] = modalities.video_l1_loss\n  hparams.top[\"targets\"] = modalities.video_l1_top\n  hparams.video_modality_loss_cutoff = 2.4\n  return hparams", "output": "Basic conv model with L1 modality.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _verify_docker_image_size(self, image_name):\n    \"\"\"\n    \"\"\"\n    shell_call(['docker', 'pull', image_name])\n    try:\n      image_size = subprocess.check_output(\n          ['docker', 'inspect', '--format={{.Size}}', image_name]).strip()\n      image_size = int(image_size)\n    except (ValueError, subprocess.CalledProcessError) as e:\n      logging.error('Failed to determine docker image size: %s', e)\n      return False\n    logging.info('Size of docker image %s is %d', image_name, image_size)\n    if image_size > MAX_DOCKER_IMAGE_SIZE:\n      logging.error('Image size exceeds limit %d', MAX_DOCKER_IMAGE_SIZE)\n    return image_size <= MAX_DOCKER_IMAGE_SIZE", "output": "Verifies size of Docker image.\n\n    Args:\n      image_name: name of the Docker image.\n\n    Returns:\n      True if image size is within the limits, False otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def objects_to_td64ns(data, unit=\"ns\", errors=\"raise\"):\n    \"\"\"\n    \n    \"\"\"\n    # coerce Index to np.ndarray, converting string-dtype if necessary\n    values = np.array(data, dtype=np.object_, copy=False)\n\n    result = array_to_timedelta64(values,\n                                  unit=unit, errors=errors)\n    return result.view('timedelta64[ns]')", "output": "Convert a object-dtyped or string-dtyped array into an\n    timedelta64[ns]-dtyped array.\n\n    Parameters\n    ----------\n    data : ndarray or Index\n    unit : str, default \"ns\"\n        The timedelta unit to treat integers as multiples of.\n    errors : {\"raise\", \"coerce\", \"ignore\"}, default \"raise\"\n        How to handle elements that cannot be converted to timedelta64[ns].\n        See ``pandas.to_timedelta`` for details.\n\n    Returns\n    -------\n    numpy.ndarray : timedelta64[ns] array converted from data\n\n    Raises\n    ------\n    ValueError : Data cannot be converted to timedelta64[ns].\n\n    Notes\n    -----\n    Unlike `pandas.to_timedelta`, if setting `errors=ignore` will not cause\n    errors to be ignored; they are caught and subsequently ignored at a\n    higher level.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_plugin_icon(self):\r\n        \"\"\"\"\"\"\r\n        path = osp.join(self.PLUGIN_PATH, self.IMG_PATH)\r\n        return ima.icon('pylint', icon_path=path)", "output": "Return widget icon", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_encoding_format(self, encoding_format):\n    \"\"\"\"\"\"\n    supported = ENCODE_FN.keys()\n    if encoding_format not in supported:\n      raise ValueError('`encoding_format` must be one of %s.' % supported)\n    self._encoding_format = encoding_format", "output": "Update the encoding format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _override_size(vm_):\n    '''\n    \n    '''\n    vm_size = get_size(vm_)\n\n    if 'cores' in vm_:\n        vm_size['cores'] = vm_['cores']\n\n    if 'ram' in vm_:\n        vm_size['ram'] = vm_['ram']\n\n    return vm_size", "output": "Apply any extra component overrides to VM from the cloud profile.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sysv_services():\n    '''\n    \n    '''\n    _services = []\n    output = __salt__['cmd.run'](['chkconfig', '--list'], python_shell=False)\n    for line in output.splitlines():\n        comps = line.split()\n        try:\n            if comps[1].startswith('0:'):\n                _services.append(comps[0])\n        except IndexError:\n            continue\n    # Return only the services that have an initscript present\n    return [x for x in _services if _service_is_sysv(x)]", "output": "Return list of sysv services.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_imgs(self) -> None:\n        \"\"\n        self._preview_header.value = self._heading\n        self._img_pane.children = tuple()", "output": "Clear the widget's images preview pane.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_patches(installed_only=False):\n    '''\n    \n    '''\n    patches = {}\n\n    cmd = [_yum(), '--quiet', 'updateinfo', 'list', 'all']\n    ret = __salt__['cmd.run_stdout'](\n        cmd,\n        python_shell=False\n    )\n    for line in salt.utils.itertools.split(ret, os.linesep):\n        inst, advisory_id, sev, pkg = re.match(r'([i|\\s]) ([^\\s]+) +([^\\s]+) +([^\\s]+)',\n                                               line).groups()\n        if inst != 'i' and installed_only:\n            continue\n        patches[advisory_id] = {\n            'installed': True if inst == 'i' else False,\n            'summary': pkg\n        }\n    return patches", "output": "List all known patches in repos.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def identify(self):\n        \"\"\"\"\"\"\n        payload = {\n            'op': self.IDENTIFY,\n            'd': {\n                'token': self.token,\n                'properties': {\n                    '$os': sys.platform,\n                    '$browser': 'discord.py',\n                    '$device': 'discord.py',\n                    '$referrer': '',\n                    '$referring_domain': ''\n                },\n                'compress': True,\n                'large_threshold': 250,\n                'v': 3\n            }\n        }\n\n        if not self._connection.is_bot:\n            payload['d']['synced_guilds'] = []\n\n        if self.shard_id is not None and self.shard_count is not None:\n            payload['d']['shard'] = [self.shard_id, self.shard_count]\n\n        state = self._connection\n        if state._activity is not None or state._status is not None:\n            payload['d']['presence'] = {\n                'status': state._status,\n                'game': state._activity,\n                'since': 0,\n                'afk': False\n            }\n\n        await self.send_as_json(payload)\n        log.info('Shard ID %s has sent the IDENTIFY payload.', self.shard_id)", "output": "Sends the IDENTIFY packet.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def iter_installable_versions(self):\n        \"\"\"\n        \"\"\"\n        for name in self._pyenv('install', '--list').out.splitlines():\n            try:\n                version = Version.parse(name.strip())\n            except ValueError:\n                continue\n            yield version", "output": "Iterate through CPython versions available for Pipenv to install.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_potential_multi_index(columns):\n    \"\"\"\n    \n    \"\"\"\n    return (len(columns) and not isinstance(columns, MultiIndex) and\n            all(isinstance(c, tuple) for c in columns))", "output": "Check whether or not the `columns` parameter\n    could be converted into a MultiIndex.\n\n    Parameters\n    ----------\n    columns : array-like\n        Object which may or may not be convertible into a MultiIndex\n\n    Returns\n    -------\n    boolean : Whether or not columns could become a MultiIndex", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def CurrentNode(self):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlTextReaderCurrentNode(self._o)\n        if ret is None:raise treeError('xmlTextReaderCurrentNode() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Hacking interface allowing to get the xmlNodePtr\n          correponding to the current node being accessed by the\n          xmlTextReader. This is dangerous because the underlying\n           node may be destroyed on the next Reads.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def regexp_extract(str, pattern, idx):\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.regexp_extract(_to_java_column(str), pattern, idx)\n    return Column(jc)", "output": "r\"\"\"Extract a specific group matched by a Java regex, from the specified string column.\n    If the regex did not match, or the specified group did not match, an empty string is returned.\n\n    >>> df = spark.createDataFrame([('100-200',)], ['str'])\n    >>> df.select(regexp_extract('str', r'(\\d+)-(\\d+)', 1).alias('d')).collect()\n    [Row(d=u'100')]\n    >>> df = spark.createDataFrame([('foo',)], ['str'])\n    >>> df.select(regexp_extract('str', r'(\\d+)', 1).alias('d')).collect()\n    [Row(d=u'')]\n    >>> df = spark.createDataFrame([('aaaac',)], ['str'])\n    >>> df.select(regexp_extract('str', '(a+)(b)?(c)', 2).alias('d')).collect()\n    [Row(d=u'')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge_bytes(binder_strings):\n    \"\"\"\"\"\"\n    output = None\n    for byte_string in binder_strings:\n        binder = Binder().from_bytes(byte_string)\n        if output is None:\n            output = binder\n        else:\n            output.merge(binder)\n    return output.to_bytes()", "output": "Concatenate multiple serialized binders into one byte string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_ordered(self, value, inplace=False):\n        \"\"\"\n        \n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        new_dtype = CategoricalDtype(self.categories, ordered=value)\n        cat = self if inplace else self.copy()\n        cat._dtype = new_dtype\n        if not inplace:\n            return cat", "output": "Set the ordered attribute to the boolean value.\n\n        Parameters\n        ----------\n        value : bool\n           Set whether this categorical is ordered (True) or not (False).\n        inplace : bool, default False\n           Whether or not to set the ordered attribute in-place or return\n           a copy of this categorical with ordered set to the value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_main():\n  \"\"\"\"\"\"\n  program.setup_environment()\n\n  if getattr(tf, '__version__', 'stub') == 'stub':\n    print(\"TensorFlow installation not found - running with reduced feature set.\",\n          file=sys.stderr)\n\n  tensorboard = program.TensorBoard(default.get_plugins(),\n                                    program.get_default_assets_zip_provider())\n  try:\n    from absl import app\n    # Import this to check that app.run() will accept the flags_parser argument.\n    from absl.flags import argparse_flags\n    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n    raise AssertionError(\"absl.app.run() shouldn't return\")\n  except ImportError:\n    pass\n  except base_plugin.FlagsError as e:\n    print(\"Error: %s\" % e, file=sys.stderr)\n    sys.exit(1)\n\n  tensorboard.configure(sys.argv)\n  sys.exit(tensorboard.main())", "output": "Initializes flags and calls main().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_repr_table_from_sframe(X):\n    \"\"\"\n    \n    \"\"\"\n\n    assert isinstance(X, _SFrame)\n\n    column_names = X.column_names()\n\n    out_data = [ [None]*len(column_names) for i in range(X.num_rows())]\n\n    column_sizes = [len(s) for s in column_names]\n\n    for i, c in enumerate(column_names):\n        for j, e in enumerate(X[c]):\n            out_data[j][i] = str(e)\n            column_sizes[i] = max(column_sizes[i], len(e))\n\n    # now, go through and pad everything.\n    out_data = ([ [cn.ljust(k, ' ') for cn, k in zip(column_names, column_sizes)],\n                  [\"-\"*k for k in column_sizes] ]\n                + [ [e.ljust(k, ' ') for e, k in zip(row, column_sizes)] for row in out_data] )\n\n    return ['  '.join(row) for row in out_data]", "output": "Serializes an SFrame to a list of strings, that, when printed, creates a well-formatted table.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nodeDumpOutput(self, doc, cur, level, format, encoding):\n        \"\"\" \"\"\"\n        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        if cur is None: cur__o = None\n        else: cur__o = cur._o\n        libxml2mod.xmlNodeDumpOutput(self._o, doc__o, cur__o, level, format, encoding)", "output": "Dump an XML node, recursive behaviour, children are printed\n          too. Note that @format = 1 provide node indenting only if\n          xmlIndentTreeOutput = 1 or xmlKeepBlanksDefault(0) was\n           called", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_transform_interface_params(spec, input_features, output_features, are_optional = False):\n    \"\"\" \n    \"\"\"\n    input_features = _fm.process_or_validate_features(input_features)\n    output_features = _fm.process_or_validate_features(output_features)\n\n    # Add input and output features\n    for (fname, ftype) in input_features:\n        input_ = spec.description.input.add()\n        input_.name = fname\n        datatypes._set_datatype(input_.type, ftype)\n        if are_optional:\n            input_.type.isOptional = are_optional\n\n    for (fname, ftype) in output_features:\n        output_ = spec.description.output.add()\n        output_.name = fname\n        datatypes._set_datatype(output_.type, ftype)\n\n    return spec", "output": "Common utilities to set transform interface params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_show_output(cmd_ret):\n    '''\n    \n    '''\n    parsed_data = dict()\n    list_key = None\n\n    for line in cmd_ret.splitlines():\n        # Skip empty lines.\n        if not line.strip():\n            continue\n\n        # If there are indented lines below an existing key, assign them as a list value\n        # for that key.\n        if not salt.utils.stringutils.contains_whitespace(line[0]):\n            list_key = None\n\n        if list_key:\n            list_value = _convert_to_closest_type(line)\n            parsed_data.setdefault(list_key, []).append(list_value)\n            continue\n\n        items = [item.strip() for item in line.split(':', 1)]\n\n        key = items[0].lower()\n        key = ' '.join(key.split()).replace(' ', '_')\n\n        # Track the current key so that we can use it in instances where the values\n        # appear on subsequent lines of the output.\n        list_key = key\n\n        try:\n            value = items[1]\n        except IndexError:\n            # If the line doesn't have the separator or is otherwise invalid, skip it.\n            log.debug('Skipping line: %s', line)\n            continue\n\n        if value:\n            parsed_data[key] = _convert_to_closest_type(value)\n\n    return _convert_parsed_show_output(parsed_data=parsed_data)", "output": "Parse the output of an aptly show command.\n\n    :param str cmd_ret: The text of the command output that needs to be parsed.\n\n    :return: A dictionary containing the configuration data.\n    :rtype: dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def invert(d):\n    \"\"\"\n    \n    \"\"\"\n    out = {}\n    for k, v in iteritems(d):\n        try:\n            out[v].add(k)\n        except KeyError:\n            out[v] = {k}\n    return out", "output": "Invert a dictionary into a dictionary of sets.\n\n    >>> invert({'a': 1, 'b': 2, 'c': 1})  # doctest: +SKIP\n    {1: {'a', 'c'}, 2: {'b'}}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def construct_sls_str(self, node):\n        '''\n        \n        '''\n\n        # Ensure obj is str, not py2 unicode or py3 bytes\n        obj = self.construct_scalar(node)\n        if six.PY2:\n            obj = obj.encode('utf-8')\n        return SLSString(obj)", "output": "Build the SLSString.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_norm(node, **kwargs):\n    \"\"\"\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    mx_axis = attrs.get(\"axis\", None)\n    axes = convert_string_to_list(str(mx_axis)) if mx_axis else None\n\n    keepdims = get_boolean_attribute_value(attrs, \"keepdims\")\n    ord = int(attrs.get(\"ord\", 2))\n\n    onnx_op_name = \"ReduceL1\" if ord == 1 else \"ReduceL2\"\n\n    if axes:\n        reduce_node = onnx.helper.make_node(\n            onnx_op_name,\n            input_nodes,\n            [name],\n            axes=axes,\n            keepdims=keepdims,\n            name=name\n        )\n        return [reduce_node]\n    else:\n        reduce_node = onnx.helper.make_node(\n            onnx_op_name,\n            input_nodes,\n            [name],\n            keepdims=keepdims,\n            name=name\n        )\n        return [reduce_node]", "output": "Map MXNet's norm operator attributes to onnx's ReduceL1 and ReduceL2 operators\n    and return the created node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_initial_state(self):\n        \"\"\"\"\"\"\n        paths = self.paths\n        self.initial_widget = self.get_widget()\n        self.initial_cursors = {}\n\n        for i, editor in enumerate(self.widgets):\n            if editor is self.initial_widget:\n                self.initial_path = paths[i]\n            # This try is needed to make the fileswitcher work with \n            # plugins that does not have a textCursor.\n            try:\n                self.initial_cursors[paths[i]] = editor.textCursor()\n            except AttributeError:\n                pass", "output": "Save initial cursors and initial active widget.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def change_minion_cachedir(\n        minion_id,\n        cachedir,\n        data=None,\n        base=None,\n):\n    '''\n    \n    '''\n    if not isinstance(data, dict):\n        return False\n\n    if base is None:\n        base = __opts__['cachedir']\n\n    fname = '{0}.p'.format(minion_id)\n    path = os.path.join(base, cachedir, fname)\n\n    with salt.utils.files.fopen(path, 'r') as fh_:\n        cache_data = salt.utils.data.decode(\n            salt.utils.msgpack.load(fh_, encoding=MSGPACK_ENCODING))\n\n    cache_data.update(data)\n\n    with salt.utils.files.fopen(path, 'w') as fh_:\n        salt.utils.msgpack.dump(cache_data, fh_, encoding=MSGPACK_ENCODING)", "output": "Changes the info inside a minion's cachedir entry. The type of cachedir\n    must be specified (i.e., 'requested' or 'active'). A dict is also passed in\n    which contains the data to be changed.\n\n    Example:\n\n        change_minion_cachedir(\n            'myminion',\n            'requested',\n            {'fingerprint': '26:5c:8c:de:be:fe:89:c0:02:ed:27:65:0e:bb:be:60'},\n        )", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mkdir_p(newdir, mode=0o777):\n    \"\"\"\n    \"\"\"\n    # http://code.activestate.com/recipes/82465-a-friendly-mkdir/\n\n    newdir = fs_encode(newdir)\n    if os.path.exists(newdir):\n        if not os.path.isdir(newdir):\n            raise OSError(\n                \"a file with the same name as the desired dir, '{0}', already exists.\".format(\n                    fs_decode(newdir)\n                )\n            )\n    else:\n        head, tail = os.path.split(newdir)\n        # Make sure the tail doesn't point to the asame place as the head\n        curdir = fs_encode(\".\")\n        tail_and_head_match = (\n            os.path.relpath(tail, start=os.path.basename(head)) == curdir\n        )\n        if tail and not tail_and_head_match and not os.path.isdir(newdir):\n            target = os.path.join(head, tail)\n            if os.path.exists(target) and os.path.isfile(target):\n                raise OSError(\n                    \"A file with the same name as the desired dir, '{0}', already exists.\".format(\n                        fs_decode(newdir)\n                    )\n                )\n            os.makedirs(os.path.join(head, tail), mode)", "output": "Recursively creates the target directory and all of its parents if they do not\n    already exist.  Fails silently if they do.\n\n    :param str newdir: The directory path to ensure\n    :raises: OSError if a file is encountered along the way", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_minions():\n    '''\n    \n    '''\n    log.debug('sqlite3 returner <get_minions> called')\n    conn = _get_conn(ret=None)\n    cur = conn.cursor()\n    sql = '''SELECT DISTINCT id FROM salt_returns'''\n    cur.execute(sql)\n    data = cur.fetchall()\n    ret = []\n    for minion in data:\n        ret.append(minion[0])\n    _close_conn(conn)\n    return ret", "output": "Return a list of minions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_location(vm_=None):\n    '''\n    \n    '''\n    return __opts__.get(\n        'location',\n        config.get_cloud_config_value(\n            'location',\n            vm_ or get_configured_provider(),\n            __opts__,\n            default=DEFAULT_LOCATION,\n            search_global=False\n        )\n    )", "output": "Return the joyent data center to use, in this order:\n        - CLI parameter\n        - VM parameter\n        - Cloud profile setting", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def combination_memo(n, r):\n    \"\"\"\"\"\"\n    memo = {}\n    def recur(n, r):\n        if n == r or r == 0:\n            return 1\n        if (n, r) not in memo:\n            memo[(n, r)] = recur(n - 1, r - 1) + recur(n - 1, r)\n        return memo[(n, r)]\n    return recur(n, r)", "output": "This function calculates nCr using memoization method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, index, feature=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if index in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, feature), params=params\n        )", "output": "The get index API allows to retrieve information about one or more indexes.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-index.html>`_\n\n        :arg index: A comma-separated list of index names\n        :arg allow_no_indices: Ignore if a wildcard expression resolves to no\n            concrete indices (default: false)\n        :arg expand_wildcards: Whether wildcard expressions should get expanded\n            to open or closed indices (default: open), default 'open', valid\n            choices are: 'open', 'closed', 'none', 'all'\n        :arg flat_settings: Return settings in flat format (default: false)\n        :arg ignore_unavailable: Ignore unavailable indexes (default: false)\n        :arg include_defaults: Whether to return all default setting for each of\n            the indices., default False\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)\n        :arg include_type_name: Specify whether requests and responses should include a\n            type name (default: depends on Elasticsearch version).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def group_diff(name):\n    '''\n    \n    '''\n    pkgtypes = ('mandatory', 'optional', 'default', 'conditional')\n    ret = {}\n    for pkgtype in pkgtypes:\n        ret[pkgtype] = {'installed': [], 'not installed': []}\n\n    pkgs = list_pkgs()\n    group_pkgs = group_info(name, expand=True)\n    for pkgtype in pkgtypes:\n        for member in group_pkgs.get(pkgtype, []):\n            if member in pkgs:\n                ret[pkgtype]['installed'].append(member)\n            else:\n                ret[pkgtype]['not installed'].append(member)\n    return ret", "output": ".. versionadded:: 2014.1.0\n    .. versionchanged:: 2016.3.0,2015.8.4,2015.5.10\n        Environment groups are now supported. The key names have been renamed,\n        similar to the changes made in :py:func:`pkg.group_info\n        <salt.modules.yumpkg.group_info>`.\n\n    Lists which of a group's packages are installed and which are not\n    installed\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.group_diff 'Perl Support'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kernel_push(self, kernel_push_request, **kwargs):  # noqa: E501\n        \"\"\"\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.kernel_push_with_http_info(kernel_push_request, **kwargs)  # noqa: E501\n        else:\n            (data) = self.kernel_push_with_http_info(kernel_push_request, **kwargs)  # noqa: E501\n            return data", "output": "Push a new kernel version.  Can be used to create a new kernel and update an existing one.  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.kernel_push(kernel_push_request, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param KernelPushRequest kernel_push_request: Information for pushing a new kernel version (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, task_id=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request('GET', _make_path('_tasks',\n            task_id), params=params)", "output": "Retrieve information for a particular task.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/tasks.html>`_\n\n        :arg task_id: Return the task with specified id (node_id:task_number)\n        :arg wait_for_completion: Wait for the matching tasks to complete\n            (default: false)\n        :arg timeout: Maximum waiting time for `wait_for_completion`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_capability(capability, image=None, restart=False):\n    '''\n    \n    '''\n    if salt.utils.versions.version_cmp(__grains__['osversion'], '10') == -1:\n        raise NotImplementedError(\n            '`uninstall_capability` is not available on this version of '\n            'Windows: {0}'.format(__grains__['osversion']))\n\n    cmd = ['DISM',\n           '/Quiet',\n           '/Image:{0}'.format(image) if image else '/Online',\n           '/Remove-Capability',\n           '/CapabilityName:{0}'.format(capability)]\n\n    if not restart:\n        cmd.append('/NoRestart')\n\n    return __salt__['cmd.run_all'](cmd)", "output": "Uninstall a capability\n\n    Args:\n        capability(str): The capability to be removed\n        image (Optional[str]): The path to the root directory of an offline\n            Windows image. If `None` is passed, the running operating system is\n            targeted. Default is None.\n        restart (Optional[bool]): Reboot the machine if required by the install\n\n    Raises:\n        NotImplementedError: For all versions of Windows that are not Windows 10\n        and later. Server editions of Windows use ServerManager instead.\n\n    Returns:\n        dict: A dictionary containing the results of the command\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' dism.remove_capability Tools.Graphics.DirectX~~~~0.0.1.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format(self, record):\n        \"\"\"\n        \n        \"\"\"\n        formatted = super(IndentingFormatter, self).format(record)\n        prefix = ''\n        if self.add_timestamp:\n            prefix = self.formatTime(record, \"%Y-%m-%dT%H:%M:%S \")\n        prefix += \" \" * get_indentation()\n        formatted = \"\".join([\n            prefix + line\n            for line in formatted.splitlines(True)\n        ])\n        return formatted", "output": "Calls the standard formatter, but will indent all of the log messages\n        by our current indentation level.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_api_compression(self, api_id, min_compression_size):\n        \"\"\"\n        \n        \"\"\"\n        self.apigateway_client.update_rest_api(\n            restApiId=api_id,\n            patchOperations=[\n                {\n                    'op': 'replace',\n                    'path': '/minimumCompressionSize',\n                    'value': str(min_compression_size)\n                }\n            ]\n        )", "output": "Add Rest API compression", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write(self, obj, data_columns=None, **kwargs):\n        \"\"\"  \"\"\"\n        if not isinstance(obj, DataFrame):\n            name = obj.name or 'values'\n            obj = DataFrame({name: obj}, index=obj.index)\n            obj.columns = [name]\n        return super().write(obj=obj, data_columns=obj.columns.tolist(),\n                             **kwargs)", "output": "we are going to write this as a frame table", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lookup_asset_types(self, sids):\n        \"\"\"\n        \n        \"\"\"\n        found = {}\n        missing = set()\n\n        for sid in sids:\n            try:\n                found[sid] = self._asset_type_cache[sid]\n            except KeyError:\n                missing.add(sid)\n\n        if not missing:\n            return found\n\n        router_cols = self.asset_router.c\n\n        for assets in group_into_chunks(missing):\n            query = sa.select((router_cols.sid, router_cols.asset_type)).where(\n                self.asset_router.c.sid.in_(map(int, assets))\n            )\n            for sid, type_ in query.execute().fetchall():\n                missing.remove(sid)\n                found[sid] = self._asset_type_cache[sid] = type_\n\n            for sid in missing:\n                found[sid] = self._asset_type_cache[sid] = None\n\n        return found", "output": "Retrieve asset types for a list of sids.\n\n        Parameters\n        ----------\n        sids : list[int]\n\n        Returns\n        -------\n        types : dict[sid -> str or None]\n            Asset types for the provided sids.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def database(self, database_id, ddl_statements=(), pool=None):\n        \"\"\"\n        \"\"\"\n        return Database(database_id, self, ddl_statements=ddl_statements, pool=pool)", "output": "Factory to create a database within this instance.\n\n        :type database_id: str\n        :param database_id: The ID of the instance.\n\n        :type ddl_statements: list of string\n        :param ddl_statements: (Optional) DDL statements, excluding the\n                               'CREATE DATABSE' statement.\n\n        :type pool: concrete subclass of\n                    :class:`~google.cloud.spanner_v1.pool.AbstractSessionPool`.\n        :param pool: (Optional) session pool to be used by database.\n\n        :rtype: :class:`~google.cloud.spanner_v1.database.Database`\n        :returns: a database owned by this instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def backbone_scope(freeze):\n    \"\"\"\n    \n    \"\"\"\n    def nonlin(x):\n        x = get_norm()(x)\n        return tf.nn.relu(x)\n\n    with argscope([Conv2D, MaxPooling, BatchNorm], data_format='channels_first'), \\\n            argscope(Conv2D, use_bias=False, activation=nonlin,\n                     kernel_initializer=tf.variance_scaling_initializer(\n                         scale=2.0, mode='fan_out')), \\\n            ExitStack() as stack:\n        if cfg.BACKBONE.NORM in ['FreezeBN', 'SyncBN']:\n            if freeze or cfg.BACKBONE.NORM == 'FreezeBN':\n                stack.enter_context(argscope(BatchNorm, training=False))\n            else:\n                stack.enter_context(argscope(\n                    BatchNorm, sync_statistics='nccl' if cfg.TRAINER == 'replicated' else 'horovod'))\n\n        if freeze:\n            stack.enter_context(freeze_variables(stop_gradient=False, skip_collection=True))\n        else:\n            # the layers are not completely freezed, but we may want to only freeze the affine\n            if cfg.BACKBONE.FREEZE_AFFINE:\n                stack.enter_context(custom_getter_scope(freeze_affine_getter))\n        yield", "output": "Args:\n        freeze (bool): whether to freeze all the variables under the scope", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save(path, ndarray, min_val=None, max_val=None):\n  \"\"\"\n  \n  \"\"\"\n  as_pil(ndarray, min_val, max_val).save(path)", "output": "Save an image, represented as an ndarray, to the filesystem\n  :param path: string, filepath\n  :param ndarray: The image as an ndarray\n  :param min_val: The minimum pixel value in the image format\n  :param max_val: The maximum pixel valie in the image format\n    If min_val and max_val are not specified, attempts to\n    infer whether the image is in any of the common ranges:\n      [0, 1], [-1, 1], [0, 255]\n    This can be ambiguous, so it is better to specify if known.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ping():\n    '''\n    \n    '''\n    if _worker_name() not in DETAILS:\n        init()\n    try:\n        return DETAILS[_worker_name()].conn.isalive()\n    except TerminalException as e:\n        log.error(e)\n        return False", "output": "Ping the device on the other end of the connection\n\n    .. code-block: bash\n\n        salt '*' onyx.cmd ping", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def root (self, set = None):\n        \"\"\" \n        \"\"\"\n        assert isinstance(set, (int, bool, type(None)))\n        if set:\n            self.root_ = True\n        return self.root_", "output": "Sets/gets the 'root' flag. Target is root is it directly correspods to some\n            variant of a main target.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_bottleneck_path(image_lists, label_name, index, bottleneck_dir,\n                        category, module_name):\n  \"\"\"\n  \"\"\"\n  module_name = (module_name.replace('://', '~')  # URL scheme.\n                 .replace('/', '~')  # URL and Unix paths.\n                 .replace(':', '~').replace('\\\\', '~'))  # Windows paths.\n  return get_image_path(image_lists, label_name, index, bottleneck_dir,\n                        category) + '_' + module_name + '.txt'", "output": "Returns a path to a bottleneck file for a label at the given index.\n\n  Args:\n    image_lists: OrderedDict of training images for each label.\n    label_name: Label string we want to get an image for.\n    index: Integer offset of the image we want. This will be moduloed by the\n    available number of images for the label, so it can be arbitrarily large.\n    bottleneck_dir: Folder string holding cached files of bottleneck values.\n    category: Name string of set to pull images from - training, testing, or\n    validation.\n    module_name: The name of the image module being used.\n\n  Returns:\n    File system path string to an image that meets the requested parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render(hjson_data, saltenv='base', sls='', **kws):\n    '''\n    \n    '''\n    if not isinstance(hjson_data, six.string_types):\n        hjson_data = hjson_data.read()\n\n    if hjson_data.startswith('#!'):\n        hjson_data = hjson_data[(hjson_data.find('\\n') + 1):]\n    if not hjson_data.strip():\n        return {}\n    return hjson.loads(hjson_data)", "output": "Accepts HJSON as a string or as a file object and runs it through the HJSON\n    parser.\n\n    :rtype: A Python data structure", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def alias(self, *alias, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n\n        metadata = kwargs.pop('metadata', None)\n        assert not kwargs, 'Unexpected kwargs where passed: %s' % kwargs\n\n        sc = SparkContext._active_spark_context\n        if len(alias) == 1:\n            if metadata:\n                jmeta = sc._jvm.org.apache.spark.sql.types.Metadata.fromJson(\n                    json.dumps(metadata))\n                return Column(getattr(self._jc, \"as\")(alias[0], jmeta))\n            else:\n                return Column(getattr(self._jc, \"as\")(alias[0]))\n        else:\n            if metadata:\n                raise ValueError('metadata can only be provided for a single column')\n            return Column(getattr(self._jc, \"as\")(_to_seq(sc, list(alias))))", "output": "Returns this column aliased with a new name or names (in the case of expressions that\n        return more than one column, such as explode).\n\n        :param alias: strings of desired column names (collects all positional arguments passed)\n        :param metadata: a dict of information to be stored in ``metadata`` attribute of the\n            corresponding :class: `StructField` (optional, keyword only argument)\n\n        .. versionchanged:: 2.2\n           Added optional ``metadata`` argument.\n\n        >>> df.select(df.age.alias(\"age2\")).collect()\n        [Row(age2=2), Row(age2=5)]\n        >>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n        99", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_filter(self, filter_id, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if filter_id in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'filter_id'.\")\n        return self.transport.perform_request(\n            \"DELETE\", _make_path(\"_ml\", \"filters\", filter_id), params=params\n        )", "output": "`<>`_\n\n        :arg filter_id: The ID of the filter to delete", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write(self, text):\n        \"\"\"\"\"\"\n        # similar to tqdm.write()\n        # https://pypi.python.org/pypi/tqdm#writing-messages\n        sys.stdout.write(\"\\r\")\n        self._clear_line()\n\n        _text = to_unicode(text)\n        if PY2:\n            _text = _text.encode(ENCODING)\n\n        # Ensure output is bytes for Py2 and Unicode for Py3\n        assert isinstance(_text, builtin_str)\n\n        sys.stdout.write(\"{0}\\n\".format(_text))", "output": "Write text in the terminal without breaking the spinner.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_name(name):\n    '''\n    \n    '''\n    name = six.text_type(name)\n    name_length = len(name)\n    regex = re.compile(r'^[a-zA-Z0-9][A-Za-z0-9_-]*[a-zA-Z0-9]$')\n\n    if name_length < 3 or name_length > 48:\n        ret = False\n    elif not re.match(regex, name):\n        ret = False\n    else:\n        ret = True\n\n    if ret is False:\n        log.warning(\n            'A Linode label may only contain ASCII letters or numbers, dashes, and '\n            'underscores, must begin and end with letters or numbers, and be at least '\n            'three characters in length.'\n        )\n\n    return ret", "output": "Checks if the provided name fits Linode's labeling parameters.\n\n    .. versionadded:: 2015.5.6\n\n    name\n        The VM name to validate", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def int_to_roman(num):\n    \"\"\"\n    \n    \"\"\"\n    m = [\"\", \"M\", \"MM\", \"MMM\"];\n    c = [\"\", \"C\", \"CC\", \"CCC\", \"CD\", \"D\", \"DC\", \"DCC\", \"DCCC\", \"CM\"];\n    x = [\"\", \"X\", \"XX\", \"XXX\", \"XL\", \"L\", \"LX\", \"LXX\", \"LXXX\", \"XC\"];\n    i = [\"\", \"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\", \"VIII\", \"IX\"];\n    return m[num//1000] + c[(num%1000)//100] + x[(num%100)//10] + i[num%10];", "output": ":type num: int\n    :rtype: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _quantize_wp(wp, nbits, qm, axis=0, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n\n    scale = bias = lut = None\n    # Linear Quantization\n    if qm == _QUANTIZATION_MODE_LINEAR_QUANTIZATION:\n        qw, scale, bias = _quantize_channelwise_linear(wp, nbits, axis)\n    # Lookup tables\n    elif qm == _QUANTIZATION_MODE_LOOKUP_TABLE_KMEANS:\n        lut, qw = _get_kmeans_lookup_table_and_weight(nbits, wp)\n    elif qm == _QUANTIZATION_MODE_CUSTOM_LOOKUP_TABLE:\n        if 'lut_function' not in kwargs.keys():\n            raise Exception('Custom lookup table quantization mode '\n                            'selected but no lookup table function passed')\n        lut_function = kwargs['lut_function']\n        if not callable(lut_function):\n            raise Exception('Argument for Lookup Table passed in but is '\n                            'not callable')\n        try:\n            lut, qw = lut_function(nbits, wp)\n        except Exception as e:\n            raise Exception('{}\\nCall to Lookup Table function failed'\n                            .format(e.message))\n    elif qm == _QUANTIZATION_MODE_LOOKUP_TABLE_LINEAR:\n        lut, qw = _get_linear_lookup_table_and_weight(nbits, wp)\n    else:\n        raise NotImplementedError('Quantization method \"{}\" not supported'.format(qm))\n\n    quantized_wp = _np.uint8(qw)\n    return scale, bias, lut, quantized_wp", "output": "Quantize the weight blob\n\n    :param wp: numpy.array\n        Weight parameters\n    :param nbits: int\n        Number of bits\n    :param qm:\n        Quantization mode\n    :param lut_function: (``callable function``)\n        Python callable representing a look-up table\n\n    Returns\n    -------\n    scale: numpy.array\n        Per-channel scale\n    bias: numpy.array\n        Per-channel bias\n    lut: numpy.array\n        Lookup table\n    quantized_wp: numpy.array\n        Quantized weight of same shape as wp, with dtype numpy.uint8", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def quit(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            RemoteWebDriver.quit(self)\n        except http_client.BadStatusLine:\n            pass\n        finally:\n            self.service.stop()", "output": "Closes the browser and shuts down the WebKitGTKDriver executable\n        that is started when starting the WebKitGTKDriver", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _async_open(self, session_id, proto_version):\n        ''' \n\n        '''\n        try:\n            yield self.application_context.create_session_if_needed(session_id, self.request)\n            session = self.application_context.get_session(session_id)\n\n            protocol = Protocol(proto_version)\n            self.receiver = Receiver(protocol)\n            log.debug(\"Receiver created for %r\", protocol)\n\n            self.handler = ProtocolHandler()\n            log.debug(\"ProtocolHandler created for %r\", protocol)\n\n            self.connection = self.application.new_connection(protocol, self, self.application_context, session)\n            log.info(\"ServerConnection created\")\n\n        except ProtocolError as e:\n            log.error(\"Could not create new server session, reason: %s\", e)\n            self.close()\n            raise e\n\n        msg = self.connection.protocol.create('ACK')\n        yield self.send_message(msg)\n\n        raise gen.Return(None)", "output": "Perform the specific steps needed to open a connection to a Bokeh session\n\n        Specifically, this method coordinates:\n\n        * Getting a session for a session ID (creating a new one if needed)\n        * Creating a protocol receiver and hander\n        * Opening a new ServerConnection and sending it an ACK\n\n        Args:\n            session_id (str) :\n                A session ID to for a session to connect to\n\n                If no session exists with the given ID, a new session is made\n\n            proto_version (str):\n                The protocol version requested by the connecting client.\n\n        Returns:\n            None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def disabled(name,\n            skip_verify=False,\n             **kwargs):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    ret.update(_disable(name, None, skip_verify=skip_verify, **kwargs))\n    return ret", "output": "Ensure that the service is disabled on boot, only use this state if you\n    don't want to manage the running process, remember that if you want to\n    disable a service to use the enable: False option for the running or dead\n    function.\n\n    name\n        The name of the init or rc script used to manage the service\n\n    skip_verify\n        Skip verifying that the service is available before disabling it.\n        ``True`` will skip the verification. The default is ``False``,\n        which will ensure the service is available before disabling it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filter(self, filter_fn):\n        \"\"\"\n        \"\"\"\n        op = Operator(\n            _generate_uuid(),\n            OpType.Filter,\n            \"Filter\",\n            filter_fn,\n            num_instances=self.env.config.parallelism)\n        return self.__register(op)", "output": "Applies a filter to the stream.\n\n        Attributes:\n             filter_fn (function): The user-defined filter function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def installed_capabilities(image=None):\n    '''\n    \n    '''\n    if salt.utils.versions.version_cmp(__grains__['osversion'], '10') == -1:\n        raise NotImplementedError(\n            '`installed_capabilities` is not available on this version of '\n            'Windows: {0}'.format(__grains__['osversion']))\n    return _get_components(\"Capability Identity\", \"Capabilities\", \"Installed\")", "output": "List the capabilities installed on the system\n\n    Args:\n        image (Optional[str]): The path to the root directory of an offline\n            Windows image. If `None` is passed, the running operating system is\n            targeted. Default is None.\n\n    Raises:\n        NotImplementedError: For all versions of Windows that are not Windows 10\n        and later. Server editions of Windows use ServerManager instead.\n\n    Returns:\n        list: A list of installed capabilities\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' dism.installed_capabilities", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kill_redis(self, check_alive=True):\n        \"\"\"\n        \"\"\"\n        self._kill_process_type(\n            ray_constants.PROCESS_TYPE_REDIS_SERVER, check_alive=check_alive)", "output": "Kill the Redis servers.\n\n        Args:\n            check_alive (bool): Raise an exception if any of the processes\n                were already dead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_unpack_format(name, extensions, function, extra_args=None,\n                           description=''):\n    \"\"\"\n    \"\"\"\n    if extra_args is None:\n        extra_args = []\n    _check_unpack_options(extensions, function, extra_args)\n    _UNPACK_FORMATS[name] = extensions, function, extra_args, description", "output": "Registers an unpack format.\n\n    `name` is the name of the format. `extensions` is a list of extensions\n    corresponding to the format.\n\n    `function` is the callable that will be\n    used to unpack archives. The callable will receive archives to unpack.\n    If it's unable to handle an archive, it needs to raise a ReadError\n    exception.\n\n    If provided, `extra_args` is a sequence of\n    (name, value) tuples that will be passed as arguments to the callable.\n    description can be provided to describe the format, and will be returned\n    by the get_unpack_formats() function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _next_datetime_with_utc_hour(table_name, utc_hour):\n    '''\n    \n    '''\n    today = datetime.date.today()\n\n    # The minute and second values generated are deterministic, as we do not want\n    # pipeline definition to change for every run.\n    start_date_time = datetime.datetime(\n        year=today.year,\n        month=today.month,\n        day=today.day,\n        hour=utc_hour,\n        minute=_get_deterministic_value_for_table_name(table_name, 60),\n        second=_get_deterministic_value_for_table_name(table_name, 60)\n    )\n\n    if start_date_time < datetime.datetime.utcnow():\n        one_day = datetime.timedelta(days=1)\n        start_date_time += one_day\n\n    return start_date_time", "output": "Datapipeline API is throttling us, as all the pipelines are started at the same time.\n    We would like to uniformly distribute the startTime over a 60 minute window.\n\n    Return the next future utc datetime where\n        hour == utc_hour\n        minute = A value between 0-59 (depending on table name)\n        second = A value between 0-59 (depending on table name)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def drop(self, index=None, columns=None):\n        \"\"\"\n        \"\"\"\n        if self._is_transposed:\n            return self.transpose().drop(index=columns, columns=index).transpose()\n        if index is None:\n            new_data = self.data\n            new_index = self.index\n        else:\n\n            def delitem(df, internal_indices=[]):\n                return df.drop(index=df.index[internal_indices])\n\n            numeric_indices = list(self.index.get_indexer_for(index))\n            new_data = self.data.apply_func_to_select_indices(\n                1, delitem, numeric_indices, keep_remaining=True\n            )\n            # We can't use self.index.drop with duplicate keys because in Pandas\n            # it throws an error.\n            new_index = self.index[~self.index.isin(index)]\n        if columns is None:\n            new_columns = self.columns\n            new_dtypes = self.dtypes\n        else:\n\n            def delitem(df, internal_indices=[]):\n                return df.drop(columns=df.columns[internal_indices])\n\n            numeric_indices = list(self.columns.get_indexer_for(columns))\n            new_data = new_data.apply_func_to_select_indices(\n                0, delitem, numeric_indices, keep_remaining=True\n            )\n\n            new_columns = self.columns[~self.columns.isin(columns)]\n            new_dtypes = self.dtypes.drop(columns)\n        return self.__constructor__(new_data, new_index, new_columns, new_dtypes)", "output": "Remove row data for target index and columns.\n\n        Args:\n            index: Target index to drop.\n            columns: Target columns to drop.\n\n        Returns:\n            A new QueryCompiler.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def time(host=None, port=None, db=None, password=None):\n    '''\n    \n    '''\n    server = _connect(host, port, db, password)\n    return server.time()[0]", "output": "Return the current server UNIX time in seconds\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.time", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vgextend(vgname, devices):\n    '''\n    \n    '''\n    if not vgname or not devices:\n        return 'Error: vgname and device(s) are both required'\n    if isinstance(devices, six.string_types):\n        devices = devices.split(',')\n\n    cmd = ['vgextend', vgname]\n    for device in devices:\n        cmd.append(device)\n    out = __salt__['cmd.run'](cmd, python_shell=False).splitlines()\n    vgdata = {'Output from vgextend': out[0].strip()}\n    return vgdata", "output": "Add physical volumes to an LVM volume group\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt mymachine lvm.vgextend my_vg /dev/sdb1,/dev/sdb2\n        salt mymachine lvm.vgextend my_vg /dev/sdb1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def collect_cmd(self, args, ret):\n        \"\"\"\"\"\"\n        from dvc.command.daemon import CmdDaemonAnalytics\n\n        assert isinstance(ret, int) or ret is None\n\n        if ret is not None:\n            self.info[self.PARAM_CMD_RETURN_CODE] = ret\n\n        if args is not None and hasattr(args, \"func\"):\n            assert args.func != CmdDaemonAnalytics\n            self.info[self.PARAM_CMD_CLASS] = args.func.__name__", "output": "Collect analytics info from a CLI command.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def restore_from_object(self, obj):\n        \"\"\"\n        \"\"\"\n\n        info = pickle.loads(obj)\n        data = info[\"data\"]\n        tmpdir = tempfile.mkdtemp(\"restore_from_object\", dir=self.logdir)\n        checkpoint_path = os.path.join(tmpdir, info[\"checkpoint_name\"])\n\n        for file_name, file_contents in data.items():\n            with open(os.path.join(tmpdir, file_name), \"wb\") as f:\n                f.write(file_contents)\n\n        self.restore(checkpoint_path)\n        shutil.rmtree(tmpdir)", "output": "Restores training state from a checkpoint object.\n\n        These checkpoints are returned from calls to save_to_object().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chgid(name, gid):\n    '''\n    \n    '''\n    pre_info = info(name)\n    if not pre_info:\n        raise CommandExecutionError(\n            'User \\'{0}\\' does not exist'.format(name)\n        )\n    if gid == pre_info['gid']:\n        return True\n    cmd = ['usermod', '-g', gid, name]\n    __salt__['cmd.run'](cmd, python_shell=False)\n    return info(name).get('gid') == gid", "output": "Change the default group of the user\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.chgid foo 4376", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def double_tap(self, on_element):\n        \"\"\"\n        \n        \"\"\"\n        self._actions.append(lambda: self._driver.execute(\n            Command.DOUBLE_TAP, {'element': on_element.id}))\n        return self", "output": "Double taps on a given element.\n\n        :Args:\n         - on_element: The element to tap.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_get_index_list(ip=None, port=None):\n    \"\"\"\n    \"\"\"\n\n    ip, port = get_mainmarket_ip(ip, port)\n    api = TdxHq_API()\n    with api.connect(ip, port):\n        data = pd.concat(\n            [pd.concat([api.to_df(api.get_security_list(j, i * 1000)).assign(sse='sz' if j == 0 else 'sh').set_index(\n                ['code', 'sse'], drop=False) for i in range(int(api.get_security_count(j) / 1000) + 1)], axis=0) for j\n                in range(2)], axis=0)\n        # data.code = data.code.apply(int)\n        sz = data.query('sse==\"sz\"')\n        sh = data.query('sse==\"sh\"')\n\n        sz = sz.assign(sec=sz.code.apply(for_sz))\n        sh = sh.assign(sec=sh.code.apply(for_sh))\n        return pd.concat([sz, sh]).query('sec==\"index_cn\"').sort_index().assign(\n            name=data['name'].apply(lambda x: str(x)[0:6]))", "output": "\u83b7\u53d6\u6307\u6570\u5217\u8868\n\n    Keyword Arguments:\n        ip {[type]} -- [description] (default: {None})\n        port {[type]} -- [description] (default: {None})\n\n    Returns:\n        [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chocolatey_version():\n    '''\n    \n    '''\n    if 'chocolatey._version' in __context__:\n        return __context__['chocolatey._version']\n\n    cmd = [_find_chocolatey(__context__, __salt__)]\n    cmd.append('-v')\n    out = __salt__['cmd.run'](cmd, python_shell=False)\n    __context__['chocolatey._version'] = out\n\n    return __context__['chocolatey._version']", "output": "Returns the version of Chocolatey installed on the minion.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' chocolatey.chocolatey_version", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _fetch_templates(src):\n    '''\n    \n    '''\n    templates = []\n    log.debug('Listing contents of %s', src)\n    for item in os.listdir(src):\n        s = os.path.join(src, item)\n        if os.path.isdir(s):\n            template_path = os.path.join(s, TEMPLATE_FILE_NAME)\n            if os.path.isfile(template_path):\n                templates.append(_get_template(template_path, item))\n            else:\n                log.debug(\"Directory does not contain %s %s\", template_path,\n                          TEMPLATE_FILE_NAME)\n    return templates", "output": "Fetch all of the templates in the src directory\n\n    :param src: The source path\n    :type  src: ``str``\n\n    :rtype: ``list`` of ``tuple``\n    :returns: ``list`` of ('key', 'description')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tabify_plugins(self, first, second):\r\n        \"\"\"\"\"\"\r\n        self.tabifyDockWidget(first.dockwidget, second.dockwidget)", "output": "Tabify plugin dockwigdets", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def createIntSubset(self, name, ExternalID, SystemID):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlCreateIntSubset(self._o, name, ExternalID, SystemID)\n        if ret is None:raise treeError('xmlCreateIntSubset() failed')\n        __tmp = xmlDtd(_obj=ret)\n        return __tmp", "output": "Create the internal subset of a document", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_environ(env_name, value):\n    \"\"\"\"\"\"\n    value_changed = value is not None\n    if value_changed:\n        old_value = os.environ.get(env_name)\n        os.environ[env_name] = value\n    try:\n        yield\n    finally:\n        if value_changed:\n            if old_value is None:\n                del os.environ[env_name]\n            else:\n                os.environ[env_name] = old_value", "output": "Set the environment variable 'env_name' to 'value'\n\n    Save previous value, yield, and then restore the previous value stored in\n    the environment variable 'env_name'.\n\n    If 'value' is None, do nothing", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ListAssets(logdir, plugin_name):\n  \"\"\"\n  \"\"\"\n  plugin_dir = PluginDirectory(logdir, plugin_name)\n  try:\n    # Strip trailing slashes, which listdir() includes for some filesystems.\n    return [x.rstrip('/') for x in tf.io.gfile.listdir(plugin_dir)]\n  except tf.errors.NotFoundError:\n    return []", "output": "List all the assets that are available for given plugin in a logdir.\n\n  Args:\n    logdir: A directory that was created by a TensorFlow summary.FileWriter.\n    plugin_name: A string name of a plugin to list assets for.\n\n  Returns:\n    A string list of available plugin assets. If the plugin subdirectory does\n    not exist (either because the logdir doesn't exist, or because the plugin\n    didn't register) an empty list is returned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_load(jid, load, minions=None):\n    '''\n    \n    '''\n    with _get_serv(commit=True) as cur:\n\n        sql = '''INSERT INTO `jids` (`jid`, `load`) VALUES (%s, %s)'''\n\n        try:\n            cur.execute(sql, (jid, salt.utils.json.dumps(load)))\n        except MySQLdb.IntegrityError:\n            # https://github.com/saltstack/salt/issues/22171\n            # Without this try/except we get tons of duplicate entry errors\n            # which result in job returns not being stored properly\n            pass", "output": "Save the load to the specified jid id", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_textcat_data(limit=0):\n    \"\"\"\"\"\"\n    # Partition off part of the train data for evaluation\n    train_data, eval_data = thinc.extra.datasets.imdb()\n    random.shuffle(train_data)\n    train_data = train_data[-limit:]\n    texts, labels = zip(*train_data)\n    eval_texts, eval_labels = zip(*eval_data)\n    cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in labels]\n    eval_cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in eval_labels]\n    return (texts, cats), (eval_texts, eval_cats)", "output": "Load data from the IMDB dataset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_graph(self):\n        \"\"\"  \"\"\"\n        all_vars = tfv1.global_variables() + tfv1.local_variables()\n        for v in all_vars:\n            if v.name == self.var_name:\n                self.var = v\n                break\n        else:\n            raise ValueError(\"{} is not a variable in the graph!\".format(self.var_name))", "output": "Will setup the assign operator for that variable.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_vqa_v2_annotations(directory,\n                            annotation_url,\n                            annotation_filename=\"vqa_v2.tar.gz\"):\n  \"\"\"\"\"\"\n  annotation_file = generator_utils.maybe_download_from_drive(\n      directory, annotation_filename, annotation_url)\n  with tarfile.open(annotation_file, \"r:gz\") as annotation_tar:\n    annotation_tar.extractall(directory)", "output": "Extract the VQA V2 annotation files to directory unless it's there.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unix_timestamp(timestamp=None, format='yyyy-MM-dd HH:mm:ss'):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    if timestamp is None:\n        return Column(sc._jvm.functions.unix_timestamp())\n    return Column(sc._jvm.functions.unix_timestamp(_to_java_column(timestamp), format))", "output": "Convert time string with given pattern ('yyyy-MM-dd HH:mm:ss', by default)\n    to Unix time stamp (in seconds), using the default timezone and the default\n    locale, return null if fail.\n\n    if `timestamp` is None, then it returns current timestamp.\n\n    >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n    >>> time_df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n    >>> time_df.select(unix_timestamp('dt', 'yyyy-MM-dd').alias('unix_time')).collect()\n    [Row(unix_time=1428476400)]\n    >>> spark.conf.unset(\"spark.sql.session.timeZone\")", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_template_texts(source_list=None,\n                        template='jinja',\n                        defaults=None,\n                        context=None,\n                        **kwargs):\n    '''\n    \n    '''\n\n    ret = {'name': '_get_template_texts',\n           'changes': {},\n           'result': True,\n           'comment': '',\n           'data': []}\n\n    if source_list is None:\n        return _error(ret,\n                      '_get_template_texts called with empty source_list')\n\n    txtl = []\n\n    for (source, source_hash) in source_list:\n\n        context_dict = defaults if defaults else {}\n        if context:\n            context_dict = salt.utils.dictupdate.merge(context_dict, context)\n        rndrd_templ_fn = __salt__['cp.get_template'](\n            source,\n            '',\n            template=template,\n            saltenv=__env__,\n            context=context_dict,\n            **kwargs\n        )\n        log.debug('cp.get_template returned %s (Called with: %s)',\n                  rndrd_templ_fn, source)\n        if rndrd_templ_fn:\n            tmplines = None\n            with salt.utils.files.fopen(rndrd_templ_fn, 'rb') as fp_:\n                tmplines = fp_.read()\n                tmplines = salt.utils.stringutils.to_unicode(tmplines)\n                tmplines = tmplines.splitlines(True)\n            if not tmplines:\n                msg = 'Failed to read rendered template file {0} ({1})'.format(\n                    rndrd_templ_fn, source\n                )\n                log.debug(msg)\n                ret['name'] = source\n                return _error(ret, msg)\n            txtl.append(''.join(tmplines))\n        else:\n            msg = 'Failed to load template file {0}'.format(source)\n            log.debug(msg)\n            ret['name'] = source\n            return _error(ret, msg)\n\n    ret['data'] = txtl\n    return ret", "output": "Iterate a list of sources and process them as templates.\n    Returns a list of 'chunks' containing the rendered templates.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def local_connect_and_auth(port, auth_secret):\n    \"\"\"\n    \n    \"\"\"\n    sock = None\n    errors = []\n    # Support for both IPv4 and IPv6.\n    # On most of IPv6-ready systems, IPv6 will take precedence.\n    for res in socket.getaddrinfo(\"127.0.0.1\", port, socket.AF_UNSPEC, socket.SOCK_STREAM):\n        af, socktype, proto, _, sa = res\n        try:\n            sock = socket.socket(af, socktype, proto)\n            sock.settimeout(15)\n            sock.connect(sa)\n            sockfile = sock.makefile(\"rwb\", 65536)\n            _do_server_auth(sockfile, auth_secret)\n            return (sockfile, sock)\n        except socket.error as e:\n            emsg = _exception_message(e)\n            errors.append(\"tried to connect to %s, but an error occured: %s\" % (sa, emsg))\n            sock.close()\n            sock = None\n    raise Exception(\"could not open socket: %s\" % errors)", "output": "Connect to local host, authenticate with it, and return a (sockfile,sock) for that connection.\n    Handles IPV4 & IPV6, does some error handling.\n    :param port\n    :param auth_secret\n    :return: a tuple with (sockfile, sock)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def string(name, value, expire=None, expireat=None, **connection_args):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': 'Key already set to defined value'}\n\n    old_key = __salt__['redis.get_key'](name, **connection_args)\n\n    if old_key != value:\n        __salt__['redis.set_key'](name, value, **connection_args)\n        ret['changes'][name] = 'Value updated'\n        ret['comment'] = 'Key updated to new value'\n\n    if expireat:\n        __salt__['redis.expireat'](name, expireat, **connection_args)\n        ret['changes']['expireat'] = 'Key expires at {0}'.format(expireat)\n    elif expire:\n        __salt__['redis.expire'](name, expire, **connection_args)\n        ret['changes']['expire'] = 'TTL set to {0} seconds'.format(expire)\n\n    return ret", "output": "Ensure that the key exists in redis with the value specified\n\n    name\n        Redis key to manage\n\n    value\n        Data to persist in key\n\n    expire\n        Sets time to live for key in seconds\n\n    expireat\n        Sets expiration time for key via UNIX timestamp, overrides `expire`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_output_matrix(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.f.isQuant():\n            raise ValueError(\"Can't get quantized Matrix\")\n        return np.array(self.f.getOutputMatrix())", "output": "Get a copy of the full output matrix of a Model. This only\n        works if the model is not quantized.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def asnumpy(self):\n        \"\"\"\"\"\"\n\n        # Create C variables that will serve as out parameters for TCMPS.\n        data_ptr = _ctypes.POINTER(_ctypes.c_float)()    # float* data_ptr\n        shape_ptr = _ctypes.POINTER(_ctypes.c_size_t)()  # size_t* shape_ptr\n        dim = _ctypes.c_size_t()                         # size_t dim\n\n        # Obtain pointers into memory owned by the C++ object self.handle.\n        # Note that this may trigger synchronization with another thread\n        # producing the data.\n        status_code = self._LIB.TCMPSReadFloatArray(\n            self.handle, _ctypes.byref(data_ptr), _ctypes.byref(shape_ptr),\n            _ctypes.byref(dim))\n        assert status_code == 0, \"Error calling TCMPSReadFloatArray\"\n\n        return _numpy_array_from_ctypes(data_ptr, shape_ptr, dim)", "output": "Copy the data from TCMPS into a new numpy ndarray", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_userprofile_from_registry(user, sid):\n    '''\n    \n    '''\n    profile_dir = __utils__['reg.read_value'](\n        'HKEY_LOCAL_MACHINE',\n        'SOFTWARE\\\\Microsoft\\\\Windows NT\\\\CurrentVersion\\\\ProfileList\\\\{0}'.format(sid),\n        'ProfileImagePath'\n    )['vdata']\n    log.debug(\n        'user %s with sid=%s profile is located at \"%s\"',\n        user, sid, profile_dir\n    )\n    return profile_dir", "output": "In case net user doesn't return the userprofile we can get it from the\n    registry\n\n    Args:\n        user (str): The user name, used in debug message\n\n        sid (str): The sid to lookup in the registry\n\n    Returns:\n        str: Profile directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def evaluate(self, dataset, metric='auto', **kwargs):\n        \"\"\"\n        \n\n        \"\"\"\n        m = self.__proxy__['classifier']\n        target = self.__proxy__['target']\n        f = _BOW_FEATURE_EXTRACTOR\n        test = f(dataset, target)\n        return m.evaluate(test, metric, **kwargs)", "output": "Evaluate the model by making predictions of target values and comparing\n        these to actual values.\n\n        Parameters\n        ----------\n        dataset : SFrame\n            An SFrame having the same feature columns as provided when creating\n            the model.\n\n        metric : str, optional\n            Name of the evaluation metric.  Possible values are:\n\n            - 'auto'             : Returns all available metrics.\n            - 'accuracy'         : Classification accuracy (micro average).\n            - 'auc'              : Area under the ROC curve (macro average)\n            - 'precision'        : Precision score (macro average)\n            - 'recall'           : Recall score (macro average)\n            - 'f1_score'         : F1 score (macro average)\n            - 'log_loss'         : Log loss\n            - 'confusion_matrix' : An SFrame with counts of possible prediction/true label combinations.\n            - 'roc_curve'        : An SFrame containing information needed for an ROC curve\n\n            For more flexibility in calculating evaluation metrics, use the\n            :class:`~turicreate.evaluation` module.\n\n        Returns\n        -------\n        out : dict\n            Dictionary of evaluation results where the key is the name of the\n            evaluation metric (e.g. `accuracy`) and the value is the evaluation\n            score.\n\n        See Also\n        ----------\n        create, predict, classify", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _legacy_grains(grains):\n    '''\n    \n    '''\n    # parse legacy sdc grains\n    if 'mdata' in grains and 'sdc' in grains['mdata']:\n        if 'server_uuid' not in grains['mdata']['sdc'] or 'FAILURE' in grains['mdata']['sdc']['server_uuid']:\n            grains['hypervisor_uuid'] = 'unknown'\n        else:\n            grains['hypervisor_uuid'] = grains['mdata']['sdc']['server_uuid']\n\n        if 'datacenter_name' not in grains['mdata']['sdc'] or 'FAILURE' in grains['mdata']['sdc']['datacenter_name']:\n            grains['datacenter'] = 'unknown'\n        else:\n            grains['datacenter'] = grains['mdata']['sdc']['datacenter_name']\n\n    # parse rules grains\n    if 'mdata' in grains and 'rules' in grains['mdata']:\n        grains['roles'] = grains['mdata']['roles'].split(',')\n\n    return grains", "output": "Grains for backwards compatibility\n    Remove this function in Neon", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_handler(  # noqa: F811\n        self, fd: Union[int, _Selectable], handler: Callable[..., None], events: int\n    ) -> None:\n        \"\"\"\n        \"\"\"\n        raise NotImplementedError()", "output": "Registers the given handler to receive the given events for ``fd``.\n\n        The ``fd`` argument may either be an integer file descriptor or\n        a file-like object with a ``fileno()`` and ``close()`` method.\n\n        The ``events`` argument is a bitwise or of the constants\n        ``IOLoop.READ``, ``IOLoop.WRITE``, and ``IOLoop.ERROR``.\n\n        When an event occurs, ``handler(fd, events)`` will be run.\n\n        .. versionchanged:: 4.0\n           Added the ability to pass file-like objects in addition to\n           raw file descriptors.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def invalid_config_error_message(action, key, val):\n    \"\"\"\"\"\"\n    if action in ('store_true', 'store_false'):\n        return (\"{0} is not a valid value for {1} option, \"\n                \"please specify a boolean value like yes/no, \"\n                \"true/false or 1/0 instead.\").format(val, key)\n\n    return (\"{0} is not a valid value for {1} option, \"\n            \"please specify a numerical value like 1/0 \"\n            \"instead.\").format(val, key)", "output": "Returns a better error message when invalid configuration option\n    is provided.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_valid(self):\n        \"\"\"\n        \n        \"\"\"\n        managed_policy_map = self.managed_policy_loader.load()\n\n        sam_translator = Translator(managed_policy_map=managed_policy_map,\n                                    sam_parser=self.sam_parser,\n                                    plugins=[])\n\n        self._replace_local_codeuri()\n\n        try:\n            template = sam_translator.translate(sam_template=self.sam_template,\n                                                parameter_values={})\n            LOG.debug(\"Translated template is:\\n%s\", yaml_dump(template))\n        except InvalidDocumentException as e:\n            raise InvalidSamDocumentException(\n                functools.reduce(lambda message, error: message + ' ' + str(error), e.causes, str(e)))", "output": "Runs the SAM Translator to determine if the template provided is valid. This is similar to running a\n        ChangeSet in CloudFormation for a SAM Template\n\n        Raises\n        -------\n        InvalidSamDocumentException\n             If the template is not valid, an InvalidSamDocumentException is raised", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def AAAA(host, nameserver=None):\n    '''\n    \n    '''\n    dig = ['dig', '+short', six.text_type(host), 'AAAA']\n\n    if nameserver is not None:\n        dig.append('@{0}'.format(nameserver))\n\n    cmd = __salt__['cmd.run_all'](dig, python_shell=False)\n    # In this case, 0 is not the same as False\n    if cmd['retcode'] != 0:\n        log.warning(\n            'dig returned exit code \\'%s\\'. Returning empty list as fallback.',\n            cmd['retcode']\n        )\n        return []\n\n    # make sure all entries are IPs\n    return [x for x in cmd['stdout'].split('\\n') if check_ip(x)]", "output": "Return the AAAA record for ``host``.\n\n    Always returns a list.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt ns1 dig.AAAA www.google.com", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_page_load_timeout(self, time_to_wait):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            self.execute(Command.SET_TIMEOUTS, {\n                'pageLoad': int(float(time_to_wait) * 1000)})\n        except WebDriverException:\n            self.execute(Command.SET_TIMEOUTS, {\n                'ms': float(time_to_wait) * 1000,\n                'type': 'page load'})", "output": "Set the amount of time to wait for a page load to complete\n           before throwing an error.\n\n        :Args:\n         - time_to_wait: The amount of time to wait\n\n        :Usage:\n            ::\n\n                driver.set_page_load_timeout(30)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_resampler_for_grouping(groupby, rule, how=None, fill_method=None,\n                               limit=None, kind=None, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n\n    # .resample uses 'on' similar to how .groupby uses 'key'\n    kwargs['key'] = kwargs.pop('on', None)\n\n    tg = TimeGrouper(freq=rule, **kwargs)\n    resampler = tg._get_resampler(groupby.obj, kind=kind)\n    r = resampler._get_resampler_for_grouping(groupby=groupby)\n    return _maybe_process_deprecations(r,\n                                       how=how,\n                                       fill_method=fill_method,\n                                       limit=limit)", "output": "Return our appropriate resampler when grouping as well.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ascii_native_(s):\n    '''\n    \n    '''\n    if isinstance(s, text_type):\n        s = s.encode('ascii')\n\n    return str(s, 'ascii', 'strict') if PY3 else s", "output": "Python 3: If ``s`` is an instance of ``text_type``, return\n    ``s.encode('ascii')``, otherwise return ``str(s, 'ascii', 'strict')``\n\n    Python 2: If ``s`` is an instance of ``text_type``, return\n    ``s.encode('ascii')``, otherwise return ``str(s)``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encdec_attention_1d(x,\n                        encoder_output,\n                        encoder_decoder_attention_bias,\n                        hparams):\n  \"\"\"\"\"\"\n  x, x_shape, is_4d = maybe_reshape_4d_to_3d(x)\n  encoder_output, _, _ = maybe_reshape_4d_to_3d(encoder_output)\n  with tf.variable_scope(\"encdec_attention\"):\n    # Encoder Decoder attention\n    y = common_attention.multihead_attention(\n        x,\n        encoder_output,\n        encoder_decoder_attention_bias,\n        hparams.attention_key_channels or hparams.hidden_size,\n        hparams.attention_value_channels or hparams.hidden_size,\n        hparams.hidden_size,\n        hparams.num_heads,\n        hparams.attention_dropout,\n        name=\"encdec_attention\")\n  if is_4d:\n    y = tf.reshape(y, x_shape)\n    y.set_shape([None, None, None, hparams.hidden_size])\n  return y", "output": "Local 1d self attention.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_diff_trees(self, a_ref, b_ref=None):\n        \"\"\"\n        \"\"\"\n        diff_dct = {DIFF_EQUAL: False}\n        trees, commit_refs = self._get_diff_trees(a_ref, b_ref)\n        diff_dct[DIFF_A_REF] = commit_refs[0]\n        diff_dct[DIFF_B_REF] = commit_refs[1]\n        if commit_refs[0] == commit_refs[1]:\n            diff_dct[DIFF_EQUAL] = True\n            return diff_dct\n        diff_dct[DIFF_A_TREE] = trees[DIFF_A_TREE]\n        diff_dct[DIFF_B_TREE] = trees[DIFF_B_TREE]\n        return diff_dct", "output": "Method for getting two repo trees between two git tag commits\n        returns the dvc hash names of changed file/directory\n\n        Args:\n            a_ref(str) - git reference\n            b_ref(str) - optional second git reference, default None\n\n        Returns:\n            dict - dictionary with keys: (a_tree, b_tree, a_ref, b_ref, equal)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _extract_functions(resources):\n        \"\"\"\n        \n        \"\"\"\n\n        result = {}\n\n        for name, resource in resources.items():\n\n            resource_type = resource.get(\"Type\")\n            resource_properties = resource.get(\"Properties\", {})\n\n            if resource_type == SamFunctionProvider._SERVERLESS_FUNCTION:\n                layers = SamFunctionProvider._parse_layer_info(resource_properties.get(\"Layers\", []), resources)\n                result[name] = SamFunctionProvider._convert_sam_function_resource(name, resource_properties, layers)\n\n            elif resource_type == SamFunctionProvider._LAMBDA_FUNCTION:\n                layers = SamFunctionProvider._parse_layer_info(resource_properties.get(\"Layers\", []), resources)\n                result[name] = SamFunctionProvider._convert_lambda_function_resource(name, resource_properties, layers)\n\n            # We don't care about other resource types. Just ignore them\n\n        return result", "output": "Extracts and returns function information from the given dictionary of SAM/CloudFormation resources. This\n        method supports functions defined with AWS::Serverless::Function and AWS::Lambda::Function\n\n        :param dict resources: Dictionary of SAM/CloudFormation resources\n        :return dict(string : samcli.commands.local.lib.provider.Function): Dictionary of function LogicalId to the\n            Function configuration object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def erase_up (self): # <ESC>[1J\n        ''''''\n\n        self.erase_start_of_line ()\n        self.fill_region (self.cur_r-1, 1, 1, self.cols)", "output": "Erases the screen from the current line up to the top of the\n        screen.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _requirement_to_str_lowercase_name(requirement):\n    \"\"\"\n    \n    \"\"\"\n\n    parts = [requirement.name.lower()]\n\n    if requirement.extras:\n        parts.append(\"[{0}]\".format(\",\".join(sorted(requirement.extras))))\n\n    if requirement.specifier:\n        parts.append(str(requirement.specifier))\n\n    if requirement.url:\n        parts.append(\"@ {0}\".format(requirement.url))\n\n    if requirement.marker:\n        parts.append(\"; {0}\".format(requirement.marker))\n\n    return \"\".join(parts)", "output": "Formats a packaging.requirements.Requirement with a lowercase name.\n\n    This is simply a copy of\n    https://github.com/pypa/packaging/blob/16.8/packaging/requirements.py#L109-L124\n    modified to lowercase the dependency name.\n\n    Previously, we were invoking the original Requirement.__str__ method and\n    lower-casing the entire result, which would lowercase the name, *and* other,\n    important stuff that should not be lower-cased (such as the marker). See\n    this issue for more information: https://github.com/pypa/pipenv/issues/2113.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_pool(self, name, method='ROUND_ROBIN'):\n        '''\n        \n        '''\n        lbmethods = self.bigIP.LocalLB.Pool.typefactory.create(\n            'LocalLB.LBMethod'\n        )\n\n        supported_method = [i[0] for i in lbmethods if (\n            i[0].split('_', 2)[-1] == method.upper()\n        )]\n\n        if supported_method and not self.check_pool(name):\n            try:\n                self.bigIP.LocalLB.Pool.create(pool_names=[name],\n                                               lb_methods=[supported_method],\n                                               members=[[]])\n            except Exception as e:\n                raise Exception(\n                    'Unable to create `{0}` pool\\n\\n{1}'.format(name, e)\n                )\n        else:\n            raise Exception('Unsupported method')\n        return True", "output": "Create a pool on the F5 load balancer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flags2segs(flags, window):\n    '''\n    \n    '''\n\n    preFlag = 0\n    cur_flag = 0\n    n_segs = 0\n\n    cur_val = flags[cur_flag]\n    segsList = []\n    classes = []\n    while (cur_flag < len(flags) - 1):\n        stop = 0\n        preFlag = cur_flag\n        preVal = cur_val\n        while (stop == 0):\n            cur_flag = cur_flag + 1\n            tempVal = flags[cur_flag]\n            if ((tempVal != cur_val) | (cur_flag == len(flags) - 1)):  # stop\n                n_segs = n_segs + 1\n                stop = 1\n                cur_seg = cur_val\n                cur_val = flags[cur_flag]\n                segsList.append((cur_flag * window))\n                classes.append(preVal)\n    segs = numpy.zeros((len(segsList), 2))\n\n    for i in range(len(segsList)):\n        if i > 0:\n            segs[i, 0] = segsList[i-1]\n        segs[i, 1] = segsList[i]\n    return (segs, classes)", "output": "ARGUMENTS:\n     - flags:      a sequence of class flags (per time window)\n     - window:     window duration (in seconds)\n\n    RETURNS:\n     - segs:       a sequence of segment's limits: segs[i,0] is start and\n                   segs[i,1] are start and end point of segment i\n     - classes:    a sequence of class flags: class[i] is the class ID of\n                   the i-th segment", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def url_ok(match_tuple: MatchTuple) -> bool:\n    \"\"\"\"\"\"\n    try:\n        result = requests.get(match_tuple.link, timeout=5)\n        return result.ok\n    except (requests.ConnectionError, requests.Timeout):\n        return False", "output": "Check if a URL is reachable.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def follow(\n        self,\n        users,\n        run_id,\n        track_interval=1,\n        trade_cmd_expire_seconds=120,\n        cmd_cache=True,\n        entrust_prop=\"limit\",\n        send_interval=0,\n    ):\n        \"\"\"\n        \"\"\"\n        users = self.warp_list(users)\n        run_ids = self.warp_list(run_id)\n\n        if cmd_cache:\n            self.load_expired_cmd_cache()\n\n        self.start_trader_thread(\n            users, trade_cmd_expire_seconds, entrust_prop, send_interval\n        )\n\n        workers = []\n        for id_ in run_ids:\n            strategy_name = self.extract_strategy_name(id_)\n            strategy_worker = Thread(\n                target=self.track_strategy_worker,\n                args=[id_, strategy_name],\n                kwargs={\"interval\": track_interval},\n            )\n            strategy_worker.start()\n            workers.append(strategy_worker)\n            log.info(\"\u5f00\u59cb\u8ddf\u8e2a\u7b56\u7565: %s\", strategy_name)\n        for worker in workers:\n            worker.join()", "output": "\u8ddf\u8e2aricequant\u5bf9\u5e94\u7684\u6a21\u62df\u4ea4\u6613\uff0c\u652f\u6301\u591a\u7528\u6237\u591a\u7b56\u7565\n        :param users: \u652f\u6301easytrader\u7684\u7528\u6237\u5bf9\u8c61\uff0c\u652f\u6301\u4f7f\u7528 [] \u6307\u5b9a\u591a\u4e2a\u7528\u6237\n        :param run_id: ricequant \u7684\u6a21\u62df\u4ea4\u6613ID\uff0c\u652f\u6301\u4f7f\u7528 [] \u6307\u5b9a\u591a\u4e2a\u6a21\u62df\u4ea4\u6613\n        :param track_interval: \u8f6e\u8bad\u6a21\u62df\u4ea4\u6613\u65f6\u95f4\uff0c\u5355\u4f4d\u4e3a\u79d2\n        :param trade_cmd_expire_seconds: \u4ea4\u6613\u6307\u4ee4\u8fc7\u671f\u65f6\u95f4, \u5355\u4f4d\u4e3a\u79d2\n        :param cmd_cache: \u662f\u5426\u8bfb\u53d6\u5b58\u50a8\u5386\u53f2\u6267\u884c\u8fc7\u7684\u6307\u4ee4\uff0c\u9632\u6b62\u91cd\u542f\u65f6\u91cd\u590d\u6267\u884c\u5df2\u7ecf\u4ea4\u6613\u8fc7\u7684\u6307\u4ee4\n        :param entrust_prop: \u59d4\u6258\u65b9\u5f0f, 'limit' \u4e3a\u9650\u4ef7\uff0c'market' \u4e3a\u5e02\u4ef7, \u4ec5\u5728\u94f6\u6cb3\u5b9e\u73b0\n        :param send_interval: \u4ea4\u6613\u53d1\u9001\u95f4\u9694\uff0c \u9ed8\u8ba4\u4e3a0s\u3002\u8c03\u5927\u53ef\u9632\u6b62\u5356\u51fa\u4e70\u5165\u65f6\u5356\u51fa\u5355\u6ca1\u6709\u53ca\u65f6\u6210\u4ea4\u5bfc\u81f4\u7684\u4e70\u5165\u91d1\u989d\u4e0d\u8db3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def endpoint_search(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.search_endpoints(**kwargs)", "output": "Search endpoints\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystoneng.endpoint_search\n        salt '*' keystoneng.endpoint_search id=02cffaa173b2460f98e40eda3748dae5", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _clean_salt_variables(params, variable_prefix=\"__\"):\n    '''\n    \n    '''\n    list(list(map(params.pop, [k for k in params if k.startswith(variable_prefix)])))\n    return params", "output": "Pops out variables from params which starts with `variable_prefix`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_host(zone, name, ttl, ip, nameserver='127.0.0.1', replace=True,\n             timeout=5, port=53, **kwargs):\n    '''\n    \n    '''\n    res = update(zone, name, ttl, 'A', ip, nameserver, timeout, replace, port,\n                 **kwargs)\n    if res is False:\n        return False\n\n    fqdn = '{0}.{1}.'.format(name, zone)\n    parts = ip.split('.')[::-1]\n    popped = []\n\n    # Iterate over possible reverse zones\n    while len(parts) > 1:\n        p = parts.pop(0)\n        popped.append(p)\n        zone = '{0}.{1}'.format('.'.join(parts), 'in-addr.arpa.')\n        name = '.'.join(popped)\n        ptr = update(zone, name, ttl, 'PTR', fqdn, nameserver, timeout,\n                     replace, port, **kwargs)\n        if ptr:\n            return True\n    return res", "output": "Add, replace, or update the A and PTR (reverse) records for a host.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt ns1 ddns.add_host example.com host1 60 10.1.1.1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def batch_contains_deleted(self):\n        \"\"\n        if not self._duplicates: return False\n        imgs = [self._all_images[:self._batch_size][0][1], self._all_images[:self._batch_size][1][1]]\n        return any(img in self._deleted_fns for img in imgs)", "output": "Check if current batch contains already deleted images.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(self):\n        \"\"\"\n        \"\"\"\n        client = self._instance._client\n        cluster_pb = self._to_pb()\n\n        return client.instance_admin_client.create_cluster(\n            self._instance.name, self.cluster_id, cluster_pb\n        )", "output": "Create this cluster.\n\n        For example:\n\n        .. literalinclude:: snippets.py\n            :start-after: [START bigtable_create_cluster]\n            :end-before: [END bigtable_create_cluster]\n\n        .. note::\n\n            Uses the ``project``, ``instance`` and ``cluster_id`` on the\n            current :class:`Cluster` in addition to the ``serve_nodes``.\n            To change them before creating, reset the values via\n\n            .. code:: python\n\n                cluster.serve_nodes = 8\n                cluster.cluster_id = 'i-changed-my-mind'\n\n            before calling :meth:`create`.\n\n        :rtype: :class:`~google.api_core.operation.Operation`\n        :returns: The long-running operation corresponding to the\n                  create operation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unwrap_to_tensors(*tensors: torch.Tensor):\n        \"\"\"\n        \n        \"\"\"\n        return (x.detach().cpu() if isinstance(x, torch.Tensor) else x for x in tensors)", "output": "If you actually passed gradient-tracking Tensors to a Metric, there will be\n        a huge memory leak, because it will prevent garbage collection for the computation\n        graph. This method ensures that you're using tensors directly and that they are on\n        the CPU.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _on_rpc_done(self, future):\n        \"\"\"\n        \"\"\"\n        _LOGGER.info(\"RPC termination has signaled manager shutdown.\")\n        future = _maybe_wrap_exception(future)\n        thread = threading.Thread(\n            name=_RPC_ERROR_THREAD_NAME, target=self.close, kwargs={\"reason\": future}\n        )\n        thread.daemon = True\n        thread.start()", "output": "Triggered whenever the underlying RPC terminates without recovery.\n\n        This is typically triggered from one of two threads: the background\n        consumer thread (when calling ``recv()`` produces a non-recoverable\n        error) or the grpc management thread (when cancelling the RPC).\n\n        This method is *non-blocking*. It will start another thread to deal\n        with shutting everything down. This is to prevent blocking in the\n        background consumer and preventing it from being ``joined()``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def order(self,\n              asset,\n              amount,\n              limit_price=None,\n              stop_price=None,\n              style=None):\n        \"\"\"\n        \"\"\"\n        if not self._can_order_asset(asset):\n            return None\n\n        amount, style = self._calculate_order(asset, amount,\n                                              limit_price, stop_price, style)\n        return self.blotter.order(asset, amount, style)", "output": "Place an order.\n\n        Parameters\n        ----------\n        asset : Asset\n            The asset that this order is for.\n        amount : int\n            The amount of shares to order. If ``amount`` is positive, this is\n            the number of shares to buy or cover. If ``amount`` is negative,\n            this is the number of shares to sell or short.\n        limit_price : float, optional\n            The limit price for the order.\n        stop_price : float, optional\n            The stop price for the order.\n        style : ExecutionStyle, optional\n            The execution style for the order.\n\n        Returns\n        -------\n        order_id : str or None\n            The unique identifier for this order, or None if no order was\n            placed.\n\n        Notes\n        -----\n        The ``limit_price`` and ``stop_price`` arguments provide shorthands for\n        passing common execution styles. Passing ``limit_price=N`` is\n        equivalent to ``style=LimitOrder(N)``. Similarly, passing\n        ``stop_price=M`` is equivalent to ``style=StopOrder(M)``, and passing\n        ``limit_price=N`` and ``stop_price=M`` is equivalent to\n        ``style=StopLimitOrder(N, M)``. It is an error to pass both a ``style``\n        and ``limit_price`` or ``stop_price``.\n\n        See Also\n        --------\n        :class:`zipline.finance.execution.ExecutionStyle`\n        :func:`zipline.api.order_value`\n        :func:`zipline.api.order_percent`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _connect(self):\n        '''\n        \n        '''\n        while True:\n            if self._closing:\n                break\n            try:\n                kwargs = {}\n                if self.source_ip or self.source_port:\n                    if tornado.version_info >= (4, 5):\n                        ### source_ip and source_port are supported only in Tornado >= 4.5\n                        # See http://www.tornadoweb.org/en/stable/releases/v4.5.0.html\n                        # Otherwise will just ignore these args\n                        kwargs = {'source_ip': self.source_ip,\n                                  'source_port': self.source_port}\n                    else:\n                        log.warning('If you need a certain source IP/port, consider upgrading Tornado >= 4.5')\n                with salt.utils.asynchronous.current_ioloop(self.io_loop):\n                    self._stream = yield self._tcp_client.connect(self.host,\n                                                                  self.port,\n                                                                  ssl_options=self.opts.get('ssl'),\n                                                                  **kwargs)\n                self._connecting_future.set_result(True)\n                break\n            except Exception as e:\n                yield tornado.gen.sleep(1)", "output": "Try to connect for the rest of time!", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _nxapi_request(commands,\n                   method='cli_conf',\n                   **kwargs):\n    '''\n    \n    '''\n    if salt.utils.platform.is_proxy():\n        return __proxy__['nxos._nxapi_request'](commands, method=method, **kwargs)\n    else:\n        api_kwargs = __salt__['config.get']('nxos', {})\n        api_kwargs.update(**kwargs)\n        return __utils__['nxos.nxapi_request'](commands, method=method, **api_kwargs)", "output": "Helper function to send exec and config requests over NX-API.\n\n    commands\n        The exec or config commands to be sent.\n\n    method: ``cli_show``\n        ``cli_show_ascii``: Return raw test or unstructured output.\n        ``cli_show``: Return structured output.\n        ``cli_conf``: Send configuration commands to the device.\n        Defaults to ``cli_conf``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_train_op(self):\n        \"\"\"\"\"\"\n        num_gpus = self.hps.num_gpus if self.hps.num_gpus != 0 else 1\n        # The learning rate schedule is dependent on the number of gpus.\n        boundaries = [int(20000 * i / np.sqrt(num_gpus)) for i in range(2, 5)]\n        values = [0.1, 0.01, 0.001, 0.0001]\n        self.lrn_rate = tf.train.piecewise_constant(self.global_step,\n                                                    boundaries, values)\n        tf.summary.scalar(\"learning rate\", self.lrn_rate)\n\n        if self.hps.optimizer == \"sgd\":\n            optimizer = tf.train.GradientDescentOptimizer(self.lrn_rate)\n        elif self.hps.optimizer == \"mom\":\n            optimizer = tf.train.MomentumOptimizer(self.lrn_rate, 0.9)\n\n        apply_op = optimizer.minimize(self.cost, global_step=self.global_step)\n        train_ops = [apply_op] + self._extra_train_ops\n        self.train_op = tf.group(*train_ops)\n        self.variables = ray.experimental.tf_utils.TensorFlowVariables(\n            self.train_op)", "output": "Build training specific ops for the graph.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _open_browser(self, single_doc_html):\n        \"\"\"\n        \n        \"\"\"\n        url = os.path.join('file://', DOC_PATH, 'build', 'html',\n                           single_doc_html)\n        webbrowser.open(url, new=2)", "output": "Open a browser tab showing single", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_logits(self, x):\n    \"\"\"\n    \n    \"\"\"\n    logits_name = self._get_logits_name()\n    logits_layer = self.get_layer(x, logits_name)\n\n    # Need to deal with the case where softmax is part of the\n    # logits layer\n    if logits_name == self._get_softmax_name():\n      softmax_logit_layer = self.get_layer(x, logits_name)\n\n      # The final op is the softmax. Return its input\n      logits_layer = softmax_logit_layer._op.inputs[0]\n\n    return logits_layer", "output": ":param x: A symbolic representation of the network input.\n    :return: A symbolic representation of the logits", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _periodically_flush_profile_events(self):\n        \"\"\"\"\"\"\n        # Note(rkn): This is run on a background thread in the driver. It uses\n        # the raylet client. This should be ok because it doesn't read\n        # from the raylet client and we have the GIL here. However,\n        # if either of those things changes, then we could run into issues.\n        while True:\n            # Sleep for 1 second. This will be interrupted if\n            # self.threads_stopped is set.\n            self.threads_stopped.wait(timeout=1)\n\n            # Exit if we received a signal that we should stop.\n            if self.threads_stopped.is_set():\n                return\n\n            self.flush_profile_data()", "output": "Drivers run this as a thread to flush profile data in the\n        background.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def configure(self, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        for k in list(self._conns):\n            # try and preserve existing client to keep the persistent connections alive\n            if k in self._kwargs and kwargs.get(k, None) == self._kwargs[k]:\n                continue\n            del self._conns[k]\n        self._kwargs = kwargs", "output": "Configure multiple connections at once, useful for passing in config\n        dictionaries obtained from other sources, like Django's settings or a\n        configuration management tool.\n\n        Example::\n\n            connections.configure(\n                default={'hosts': 'localhost'},\n                dev={'hosts': ['esdev1.example.com:9200'], sniff_on_start=True}\n            )\n\n        Connections will only be constructed lazily when requested through\n        ``get_connection``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def default_if_none(default=NOTHING, factory=None):\n    \"\"\"\n    \n    \"\"\"\n    if default is NOTHING and factory is None:\n        raise TypeError(\"Must pass either `default` or `factory`.\")\n\n    if default is not NOTHING and factory is not None:\n        raise TypeError(\n            \"Must pass either `default` or `factory` but not both.\"\n        )\n\n    if factory is not None:\n        default = Factory(factory)\n\n    if isinstance(default, Factory):\n        if default.takes_self:\n            raise ValueError(\n                \"`takes_self` is not supported by default_if_none.\"\n            )\n\n        def default_if_none_converter(val):\n            if val is not None:\n                return val\n\n            return default.factory()\n\n    else:\n\n        def default_if_none_converter(val):\n            if val is not None:\n                return val\n\n            return default\n\n    return default_if_none_converter", "output": "A converter that allows to replace ``None`` values by *default* or the\n    result of *factory*.\n\n    :param default: Value to be used if ``None`` is passed. Passing an instance\n       of :class:`attr.Factory` is supported, however the ``takes_self`` option\n       is *not*.\n    :param callable factory: A callable that takes not parameters whose result\n       is used if ``None`` is passed.\n\n    :raises TypeError: If **neither** *default* or *factory* is passed.\n    :raises TypeError: If **both** *default* and *factory* are passed.\n    :raises ValueError: If an instance of :class:`attr.Factory` is passed with\n       ``takes_self=True``.\n\n    .. versionadded:: 18.2.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def headerData(self, section, orientation, role):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        if role == Qt.TextAlignmentRole:\r\n            if orientation == Qt.Horizontal:\r\n                return Qt.AlignCenter | Qt.AlignBottom\r\n            else:\r\n                return Qt.AlignRight | Qt.AlignVCenter\r\n        if role != Qt.DisplayRole and role != Qt.ToolTipRole:\r\n            return None\r\n        if self.model.header_shape[0] <= 1 and orientation == Qt.Horizontal:\r\n            if self.model.name(1,section):\r\n                return self.model.name(1,section)\r\n            return _('Index')\r\n        elif self.model.header_shape[0] <= 1:\r\n            return None\r\n        elif self.model.header_shape[1] <= 1 and orientation == Qt.Vertical:\r\n            return None\r\n        return _('Index') + ' ' + to_text_string(section)", "output": "Get the text to put in the header of the levels of the indexes.\r\n\r\n        By default it returns 'Index i', where i is the section in the index", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_epoch_end(self, last_metrics:MetricsList, iteration:int, **kwargs)->None:\n        \"\"\n        self._write_metrics(iteration=iteration, last_metrics=last_metrics)", "output": "Callback function that writes epoch end appropriate data to Tensorboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_noneof(self, definitions, field, value):\n        \"\"\"  \"\"\"\n        valids, _errors = \\\n            self.__validate_logical('noneof', definitions, field, value)\n        if valids > 0:\n            self._error(field, errors.NONEOF, _errors,\n                        valids, len(definitions))", "output": "{'type': 'list', 'logical': 'noneof'}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def train(args, params):\n    '''\n    \n    '''\n    x_train, y_train, x_test, y_test = load_mnist_data(args)\n    model = create_mnist_model(params)\n\n    # nni \n    model.fit(x_train, y_train, batch_size=args.batch_size, epochs=args.epochs, verbose=1,\n        validation_data=(x_test, y_test), callbacks=[SendMetrics(), TensorBoard(log_dir=TENSORBOARD_DIR)])\n\n    _, acc = model.evaluate(x_test, y_test, verbose=0)\n    LOG.debug('Final result is: %d', acc)\n    nni.report_final_result(acc)", "output": "Train model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_identify(on=True, duration=600, **kwargs):\n    '''\n    \n    '''\n    with _IpmiCommand(**kwargs) as s:\n        return s.set_identify(on=on, duration=duration)", "output": "Request identify light\n\n    Request the identify light to turn off, on for a duration,\n    or on indefinitely.  Other than error exceptions,\n\n    :param on: Set to True to force on or False to force off\n    :param duration: Set if wanting to request turn on for a duration\n                    in seconds, None = indefinitely.\n    :param kwargs:\n        - api_host=127.0.0.1\n        - api_user=admin\n        - api_pass=example\n        - api_port=623\n        - api_kg=None\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-call ipmi.set_identify", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xmoe2_tiny():\n  \"\"\"\"\"\"\n  hparams = xmoe2_v1()\n  hparams.decoder_layers = [\n      \"local_att\", \"att\", \"compressed_att\", \"drd\", \"hmoe\"]\n  hparams.d_model = 128\n  hparams.moe_hidden_size = 512\n  hparams.outer_batch_size = 0\n  hparams.batch_size = 2\n  hparams.mesh_shape = \"\"\n  hparams.activation_dtype = \"float32\"\n  return hparams", "output": "Test on local cpu.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def adjust_brightness_contrast(image, brightness=0., contrast=0.):\n    \"\"\"\n    \n    \"\"\"\n    beta = 0\n    # See the OpenCV docs for more info on the `beta` parameter to addWeighted\n    # https://docs.opencv.org/3.4.2/d2/de8/group__core__array.html#gafafb2513349db3bcff51f54ee5592a19\n    return cv2.addWeighted(image,\n                           1 + float(contrast) / 100.,\n                           image,\n                           beta,\n                           float(brightness))", "output": "Adjust the brightness and/or contrast of an image\n\n    :param image: OpenCV BGR image\n    :param contrast: Float, contrast adjustment with 0 meaning no change\n    :param brightness: Float, brightness adjustment with 0 meaning no change", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hard_reset(self):\n        \"\"\"\"\"\"\n        if self.shuffle:\n            self._shuffle_data()\n        self.cursor = -self.batch_size\n        self._cache_data = None\n        self._cache_label = None", "output": "Ignore roll over data and set to start.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_user_prefs(self, user_prefs):\n        \"\"\"\n        \n        \"\"\"\n        with open(self.userPrefs, \"w\") as f:\n            for key, value in user_prefs.items():\n                f.write('user_pref(\"%s\", %s);\\n' % (key, json.dumps(value)))", "output": "writes the current user prefs dictionary to disk", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def print_results(distributions, list_files=False, verbose=False):\n    \"\"\"\n    \n    \"\"\"\n    results_printed = False\n    for i, dist in enumerate(distributions):\n        results_printed = True\n        if i > 0:\n            logger.info(\"---\")\n\n        name = dist.get('name', '')\n        required_by = [\n            pkg.project_name for pkg in pkg_resources.working_set\n            if name in [required.name for required in pkg.requires()]\n        ]\n\n        logger.info(\"Name: %s\", name)\n        logger.info(\"Version: %s\", dist.get('version', ''))\n        logger.info(\"Summary: %s\", dist.get('summary', ''))\n        logger.info(\"Home-page: %s\", dist.get('home-page', ''))\n        logger.info(\"Author: %s\", dist.get('author', ''))\n        logger.info(\"Author-email: %s\", dist.get('author-email', ''))\n        logger.info(\"License: %s\", dist.get('license', ''))\n        logger.info(\"Location: %s\", dist.get('location', ''))\n        logger.info(\"Requires: %s\", ', '.join(dist.get('requires', [])))\n        logger.info(\"Required-by: %s\", ', '.join(required_by))\n\n        if verbose:\n            logger.info(\"Metadata-Version: %s\",\n                        dist.get('metadata-version', ''))\n            logger.info(\"Installer: %s\", dist.get('installer', ''))\n            logger.info(\"Classifiers:\")\n            for classifier in dist.get('classifiers', []):\n                logger.info(\"  %s\", classifier)\n            logger.info(\"Entry-points:\")\n            for entry in dist.get('entry_points', []):\n                logger.info(\"  %s\", entry.strip())\n        if list_files:\n            logger.info(\"Files:\")\n            for line in dist.get('files', []):\n                logger.info(\"  %s\", line.strip())\n            if \"files\" not in dist:\n                logger.info(\"Cannot locate installed-files.txt\")\n    return results_printed", "output": "Print the informations from installed distributions found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_experiment_fn(*args, **kwargs):\n  \"\"\"\"\"\"\n\n  def experiment_fn(run_config, hparams):\n    return create_experiment(run_config, hparams, *args, **kwargs)\n\n  return experiment_fn", "output": "Wrapper for canonical experiment_fn. See create_experiment.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def invoke(self, ctx):\n        \"\"\"\n        \"\"\"\n        if ctx.command is not None:\n            self.dispatch('command', ctx)\n            try:\n                if await self.can_run(ctx, call_once=True):\n                    await ctx.command.invoke(ctx)\n            except errors.CommandError as exc:\n                await ctx.command.dispatch_error(ctx, exc)\n            else:\n                self.dispatch('command_completion', ctx)\n        elif ctx.invoked_with:\n            exc = errors.CommandNotFound('Command \"{}\" is not found'.format(ctx.invoked_with))\n            self.dispatch('command_error', ctx, exc)", "output": "|coro|\n\n        Invokes the command given under the invocation context and\n        handles all the internal event dispatch mechanisms.\n\n        Parameters\n        -----------\n        ctx: :class:`.Context`\n            The invocation context to invoke.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(self, filename):\n        \"\"\"\n        \n        \"\"\"\n        with open(filename, \"r\") as f:\n            for lineno, line in enumerate(f, 1):\n                line = line.strip()\n                if (len(line) == 0) or (line[0] == \"#\"):\n                    continue\n                try:\n                    e = HostKeyEntry.from_line(line, lineno)\n                except SSHException:\n                    continue\n                if e is not None:\n                    _hostnames = e.hostnames\n                    for h in _hostnames:\n                        if self.check(h, e.key):\n                            e.hostnames.remove(h)\n                    if len(e.hostnames):\n                        self._entries.append(e)", "output": "Read a file of known SSH host keys, in the format used by OpenSSH.\n        This type of file unfortunately doesn't exist on Windows, but on\n        posix, it will usually be stored in\n        ``os.path.expanduser(\"~/.ssh/known_hosts\")``.\n\n        If this method is called multiple times, the host keys are merged,\n        not cleared.  So multiple calls to `load` will just call `add`,\n        replacing any existing entries and adding new ones.\n\n        :param str filename: name of the file to read host keys from\n\n        :raises: ``IOError`` -- if there was an error reading the file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _df_index_name(df):\n        ''' \n\n        '''\n        if df.index.name:\n            return df.index.name\n        elif df.index.names:\n            try:\n                return \"_\".join(df.index.names)\n            except TypeError:\n                return \"index\"\n        else:\n            return \"index\"", "output": "Return the Bokeh-appropriate column name for a ``DataFrame`` index\n\n        If there is no named index, then `\"index\" is returned.\n\n        If there is a single named index, then ``df.index.name`` is returned.\n\n        If there is a multi-index, and the index names are all strings, then\n        the names are joined with '_' and the result is returned, e.g. for a\n        multi-index ``['ind1', 'ind2']`` the result will be \"ind1_ind2\".\n        Otherwise if any index name is not a string, the fallback name \"index\"\n        is returned.\n\n        Args:\n            df (DataFrame) : the ``DataFrame`` to find an index name for\n\n        Returns:\n            str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_conf(conf_file=default_conf, name=None):\n    '''\n    \n    '''\n    cfg = _parse_conf(conf_file)\n\n    # filter\n    if name and name in cfg:\n        return {name: cfg[name]}\n    elif name:\n        return {name: 'not found in {}'.format(conf_file)}\n    else:\n        return cfg", "output": "Show configuration\n\n    conf_file : string\n        path to logadm.conf, defaults to /etc/logadm.conf\n    name : string\n        optional show only a single entry\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' logadm.show_conf\n        salt '*' logadm.show_conf name=/var/log/syslog", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def linear_extrapolation_plot(log_prob_adv_array, y, file_name,\n                              min_epsilon=-10, max_epsilon=10,\n                              num_points=21):\n  \"\"\"\n  \"\"\"\n  import matplotlib\n  matplotlib.use('Agg')\n  import matplotlib.pyplot as plt\n\n  figure = plt.figure()\n  figure.canvas.set_window_title('Cleverhans: Linear Extrapolation Plot')\n\n  correct_idx = np.argmax(y, axis=0)\n  fig = plt.figure()\n  plt.xlabel('Epsilon')\n  plt.ylabel('Logits')\n  x_axis = np.linspace(min_epsilon, max_epsilon, num_points)\n  plt.xlim(min_epsilon - 1, max_epsilon + 1)\n  for i in range(y.shape[0]):\n    if i == correct_idx:\n      ls = '-'\n      linewidth = 5\n    else:\n      ls = '--'\n      linewidth = 2\n    plt.plot(\n        x_axis,\n        log_prob_adv_array[:, i],\n        ls=ls,\n        linewidth=linewidth,\n        label='{}'.format(i))\n  plt.legend(loc='best', fontsize=14)\n  plt.show()\n  fig.savefig(file_name)\n  plt.clf()\n  return figure", "output": "Generate linear extrapolation plot.\n\n  Args:\n      log_prob_adv_array: Numpy array containing log probabilities\n      y: Tf placeholder for the labels\n      file_name: Plot filename\n      min_epsilon: Minimum value of epsilon over the interval\n      max_epsilon: Maximum value of epsilon over the interval\n      num_points: Number of points used to interpolate", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_sum(node, **kwargs):\n    \"\"\"\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    mx_axis = attrs.get(\"axis\", None)\n    axes = convert_string_to_list(str(mx_axis)) if mx_axis is not None else None\n\n    keepdims = get_boolean_attribute_value(attrs, \"keepdims\")\n\n    if axes:\n        node = onnx.helper.make_node(\n            'ReduceSum',\n            inputs=input_nodes,\n            outputs=[name],\n            axes=axes,\n            keepdims=keepdims,\n            name=name\n        )\n    else:\n        node = onnx.helper.make_node(\n            'ReduceSum',\n            inputs=input_nodes,\n            outputs=[name],\n            keepdims=keepdims,\n            name=name\n        )\n    return [node]", "output": "Map MXNet's sum operator attributes to onnx's ReduceSum operator\n    and return the created node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _string_in_table(self, candidate: str) -> List[str]:\n        \"\"\"\n        \n        \"\"\"\n        candidate_column_names: List[str] = []\n        # First check if the entire candidate occurs as a cell.\n        if candidate in self._string_column_mapping:\n            candidate_column_names = self._string_column_mapping[candidate]\n        # If not, check if it is a substring pf any cell value.\n        if not candidate_column_names:\n            for cell_value, column_names in self._string_column_mapping.items():\n                if candidate in cell_value:\n                    candidate_column_names.extend(column_names)\n        candidate_column_names = list(set(candidate_column_names))\n        return candidate_column_names", "output": "Checks if the string occurs in the table, and if it does, returns the names of the columns\n        under which it occurs. If it does not, returns an empty list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install_js():\n    ''' \n\n    '''\n    target_jsdir = join(SERVER, 'static', 'js')\n    target_cssdir = join(SERVER, 'static', 'css')\n    target_tslibdir = join(SERVER, 'static', 'lib')\n\n    STATIC_ASSETS = [\n        join(JS,  'bokeh.js'),\n        join(JS,  'bokeh.min.js'),\n        join(CSS, 'bokeh.css'),\n        join(CSS, 'bokeh.min.css'),\n    ]\n    if not all(exists(a) for a in STATIC_ASSETS):\n        print(BOKEHJS_INSTALL_FAIL)\n        sys.exit(1)\n\n    if exists(target_jsdir):\n        shutil.rmtree(target_jsdir)\n    shutil.copytree(JS, target_jsdir)\n\n    if exists(target_cssdir):\n        shutil.rmtree(target_cssdir)\n    shutil.copytree(CSS, target_cssdir)\n\n    if exists(target_tslibdir):\n        shutil.rmtree(target_tslibdir)\n    if exists(TSLIB):\n        # keep in sync with bokehjs/src/compiler/compile.ts\n        lib = {\n            \"lib.es5.d.ts\",\n            \"lib.dom.d.ts\",\n            \"lib.es2015.core.d.ts\",\n            \"lib.es2015.promise.d.ts\",\n            \"lib.es2015.symbol.d.ts\",\n            \"lib.es2015.iterable.d.ts\",\n        }\n        shutil.copytree(TSLIB, target_tslibdir, ignore=lambda _, files: [ f for f in files if f not in lib ])", "output": "Copy built BokehJS files into the Python source tree.\n\n    Returns:\n        None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_nb_metadata(nb_path=None, title=None, summary=None, keywords='fastai', overwrite=True, **kwargs):\n    \"\"\n    nb = read_nb(nb_path)\n    data = {'title': title, 'summary': summary, 'keywords': keywords, **kwargs}\n    data = {k:v for (k,v) in data.items() if v is not None} # remove none values\n    if not data: return\n    nb['metadata']['jekyll'] = data\n    write_nb(nb, nb_path)\n    NotebookNotary().sign(nb)", "output": "Creates jekyll metadata for given notebook path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _mergetreejinja(src, dst, context):\n    '''\n    \n    '''\n    for item in os.listdir(src):\n        s = os.path.join(src, item)\n        d = os.path.join(dst, item)\n        if os.path.isdir(s):\n            log.info(\"Copying folder %s to %s\", s, d)\n            if os.path.exists(d):\n                _mergetreejinja(s, d, context)\n            else:\n                os.mkdir(d)\n                _mergetreejinja(s, d, context)\n        else:\n            if item != TEMPLATE_FILE_NAME:\n                d = Template(d).render(context)\n                log.info(\"Copying file %s to %s\", s, d)\n                with salt.utils.files.fopen(s, 'r') as source_file:\n                    src_contents = salt.utils.stringutils.to_unicode(source_file.read())\n                    dest_contents = Template(src_contents).render(context)\n                with salt.utils.files.fopen(d, 'w') as dest_file:\n                    dest_file.write(salt.utils.stringutils.to_str(dest_contents))", "output": "Merge directory A to directory B, apply Jinja2 templating to both\n    the file/folder names AND to the contents of the files\n\n    :param src: The source path\n    :type  src: ``str``\n\n    :param dst: The destination path\n    :type  dst: ``str``\n\n    :param context: The dictionary to inject into the Jinja template as context\n    :type  context: ``dict``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def acceptNavigationRequest(self, url, navigation_type, isMainFrame):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        if navigation_type == QWebEnginePage.NavigationTypeLinkClicked:\r\n            self.linkClicked.emit(url)\r\n            return False\r\n        return True", "output": "Overloaded method to handle links ourselves", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_get(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_openstack_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.get_user(**kwargs)", "output": "Get a single user\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystoneng.user_get name=user1\n        salt '*' keystoneng.user_get name=user1 domain_id=b62e76fbeeff4e8fb77073f591cf211e\n        salt '*' keystoneng.user_get name=02cffaa173b2460f98e40eda3748dae5", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pipeline_id_from_name(name, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    r = {}\n    result_pipelines = list_pipelines()\n    if 'error' in result_pipelines:\n        return result_pipelines\n\n    for pipeline in result_pipelines['result']:\n        if pipeline['name'] == name:\n            r['result'] = pipeline['id']\n            return r\n    r['error'] = 'No pipeline found with name={0}'.format(name)\n    return r", "output": "Get the pipeline id, if it exists, for the given name.\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt myminion boto_datapipeline.pipeline_id_from_name my_pipeline_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rlmb_base_stochastic_discrete_noresize():\n  \"\"\"\"\"\"\n  hparams = rlmb_base()\n  hparams.generative_model = \"next_frame_basic_stochastic_discrete\"\n  hparams.generative_model_params = \"next_frame_basic_stochastic_discrete\"\n  hparams.resize_height_factor = 1\n  hparams.resize_width_factor = 1\n  return hparams", "output": "Base setting with stochastic discrete model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_input_shape(self, input_shape):\n    batch_size, rows, cols, input_channels = input_shape\n    # assert self.mode == 'train' or self.mode == 'eval'\n    \"\"\"\"\"\"\n    input_shape = list(input_shape)\n    input_shape[0] = 1\n    dummy_batch = tf.zeros(input_shape)\n    dummy_output = self.fprop(dummy_batch)\n    output_shape = [int(e) for e in dummy_output.get_shape()]\n    output_shape[0] = batch_size\n    self.output_shape = tuple(output_shape)", "output": "Build the core model within the graph.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_global_step_var():\n    \"\"\"\n    \n    \"\"\"\n    scope = tfv1.VariableScope(reuse=False, name='')  # the root vs\n    with tfv1.variable_scope(scope):\n        var = tfv1.train.get_or_create_global_step()\n    return var", "output": "Returns:\n        tf.Tensor: the global_step variable in the current graph. Create if doesn't exist.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_api_repr(self):\n        \"\"\"\n        \"\"\"\n        s_types = {}\n        values = {}\n        for name, value in self.struct_values.items():\n            type_ = self.struct_types[name]\n            if type_ in (\"STRUCT\", \"ARRAY\"):\n                repr_ = value.to_api_repr()\n                s_types[name] = {\"name\": name, \"type\": repr_[\"parameterType\"]}\n                values[name] = repr_[\"parameterValue\"]\n            else:\n                s_types[name] = {\"name\": name, \"type\": {\"type\": type_}}\n                converter = _SCALAR_VALUE_TO_JSON_PARAM.get(type_)\n                if converter is not None:\n                    value = converter(value)\n                values[name] = {\"value\": value}\n\n        resource = {\n            \"parameterType\": {\n                \"type\": \"STRUCT\",\n                \"structTypes\": [s_types[key] for key in self.struct_types],\n            },\n            \"parameterValue\": {\"structValues\": values},\n        }\n        if self.name is not None:\n            resource[\"name\"] = self.name\n        return resource", "output": "Construct JSON API representation for the parameter.\n\n        :rtype: dict\n        :returns: JSON mapping", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def any_auth(self, form, auth_list, fun, arg, tgt=None, tgt_type='glob'):\n        '''\n        \n        '''\n        # This function is only called from salt.auth.Authorize(), which is also\n        # deprecated and will be removed in Neon.\n        salt.utils.versions.warn_until(\n            'Neon',\n            'The \\'any_auth\\' function has been deprecated. Support for this '\n            'function will be removed in Salt {version}.'\n        )\n        if form == 'publish':\n            return self.auth_check(\n                    auth_list,\n                    fun,\n                    arg,\n                    tgt,\n                    tgt_type)\n        return self.spec_check(\n                auth_list,\n                fun,\n                arg,\n                form)", "output": "Read in the form and determine which auth check routine to execute", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def edit(self, **fields):\n        \"\"\"\n        \"\"\"\n\n        try:\n            icon_bytes = fields['icon']\n        except KeyError:\n            pass\n        else:\n            if icon_bytes is not None:\n                fields['icon'] = utils._bytes_to_base64_data(icon_bytes)\n\n        data = await self._state.http.edit_group(self.id, **fields)\n        self._update_group(data)", "output": "|coro|\n\n        Edits the group.\n\n        Parameters\n        -----------\n        name: Optional[:class:`str`]\n            The new name to change the group to.\n            Could be ``None`` to remove the name.\n        icon: Optional[:class:`bytes`]\n            A :term:`py:bytes-like object` representing the new icon.\n            Could be ``None`` to remove the icon.\n\n        Raises\n        -------\n        HTTPException\n            Editing the group failed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_version():\n    '''\n    \n    '''\n    version_string = __salt__['cmd.run'](\n        [_check_xbps(), '--version'],\n        output_loglevel='trace')\n    if version_string is None:\n        # Dunno why it would, but...\n        return False\n\n    VERSION_MATCH = re.compile(r'(?:XBPS:[\\s]+)([\\d.]+)(?:[\\s]+.*)')\n    version_match = VERSION_MATCH.search(version_string)\n    if not version_match:\n        return False\n\n    return version_match.group(1).split('.')", "output": "Get the xbps version", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read_embeddings_from_hdf5(embeddings_filename: str,\n                               embedding_dim: int,\n                               vocab: Vocabulary,\n                               namespace: str = \"tokens\") -> torch.FloatTensor:\n    \"\"\"\n    \n    \"\"\"\n    with h5py.File(embeddings_filename, 'r') as fin:\n        embeddings = fin['embedding'][...]\n\n    if list(embeddings.shape) != [vocab.get_vocab_size(namespace), embedding_dim]:\n        raise ConfigurationError(\n                \"Read shape {0} embeddings from the file, but expected {1}\".format(\n                        list(embeddings.shape), [vocab.get_vocab_size(namespace), embedding_dim]))\n\n    return torch.FloatTensor(embeddings)", "output": "Reads from a hdf5 formatted file. The embedding matrix is assumed to\n    be keyed by 'embedding' and of size ``(num_tokens, embedding_dim)``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_static(self, prefix: str, path: PathLike, *,\n                   name: Optional[str]=None,\n                   expect_handler: Optional[_ExpectHandler]=None,\n                   chunk_size: int=256 * 1024,\n                   show_index: bool=False, follow_symlinks: bool=False,\n                   append_version: bool=False) -> AbstractResource:\n        \"\"\"\n\n        \"\"\"\n        assert prefix.startswith('/')\n        if prefix.endswith('/'):\n            prefix = prefix[:-1]\n        resource = StaticResource(prefix, path,\n                                  name=name,\n                                  expect_handler=expect_handler,\n                                  chunk_size=chunk_size,\n                                  show_index=show_index,\n                                  follow_symlinks=follow_symlinks,\n                                  append_version=append_version)\n        self.register_resource(resource)\n        return resource", "output": "Add static files view.\n\n        prefix - url prefix\n        path - folder with files", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_plugin_actions(self):\r\n        \"\"\"\"\"\"\r\n        return [self.rich_text_action, self.plain_text_action,\r\n                self.show_source_action, MENU_SEPARATOR,\r\n                self.auto_import_action]", "output": "Return a list of actions related to plugin", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reconciliateNs(self, tree):\n        \"\"\" \"\"\"\n        if tree is None: tree__o = None\n        else: tree__o = tree._o\n        ret = libxml2mod.xmlReconciliateNs(self._o, tree__o)\n        return ret", "output": "This function checks that all the namespaces declared\n          within the given tree are properly declared. This is needed\n          for example after Copy or Cut and then paste operations.\n          The subtree may still hold pointers to namespace\n          declarations outside the subtree or invalid/masked. As much\n          as possible the function try to reuse the existing\n          namespaces found in the new environment. If not possible\n          the new namespaces are redeclared on @tree at the top of\n           the given subtree.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parent(self):\n        \"\"\"\"\"\"\n        drv = self._drv\n        root = self._root\n        parts = self._parts\n        if len(parts) == 1 and (drv or root):\n            return self\n        return self._from_parsed_parts(drv, root, parts[:-1])", "output": "The logical parent of the path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sampleBy(self, col, fractions, seed=None):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(col, basestring):\n            col = Column(col)\n        elif not isinstance(col, Column):\n            raise ValueError(\"col must be a string or a column, but got %r\" % type(col))\n        if not isinstance(fractions, dict):\n            raise ValueError(\"fractions must be a dict but got %r\" % type(fractions))\n        for k, v in fractions.items():\n            if not isinstance(k, (float, int, long, basestring)):\n                raise ValueError(\"key must be float, int, long, or string, but got %r\" % type(k))\n            fractions[k] = float(v)\n        col = col._jc\n        seed = seed if seed is not None else random.randint(0, sys.maxsize)\n        return DataFrame(self._jdf.stat().sampleBy(col, self._jmap(fractions), seed), self.sql_ctx)", "output": "Returns a stratified sample without replacement based on the\n        fraction given on each stratum.\n\n        :param col: column that defines strata\n        :param fractions:\n            sampling fraction for each stratum. If a stratum is not\n            specified, we treat its fraction as zero.\n        :param seed: random seed\n        :return: a new DataFrame that represents the stratified sample\n\n        >>> from pyspark.sql.functions import col\n        >>> dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n        >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n        >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n        +---+-----+\n        |key|count|\n        +---+-----+\n        |  0|    3|\n        |  1|    6|\n        +---+-----+\n        >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n        33\n\n        .. versionchanged:: 3.0\n           Added sampling by a column of :class:`Column`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def plot(self, tag, mpl_plt, step=None, close_plot=True):\n    \"\"\"\n    \"\"\"\n    if step is None:\n      step = self._step\n    else:\n      self._step = step\n    fig = mpl_plt.get_current_fig_manager()\n    img_w, img_h = fig.canvas.get_width_height()\n    image_buf = io.BytesIO()\n    mpl_plt.savefig(image_buf, format='png')\n    image_summary = Summary.Image(\n        encoded_image_string=image_buf.getvalue(),\n        colorspace=4,  # RGBA\n        height=img_h,\n        width=img_w)\n    summary = Summary(value=[Summary.Value(tag=tag, image=image_summary)])\n    self.add_summary(summary, step)\n    if close_plot:\n      mpl_plt.close()", "output": "Saves matplotlib plot output to summary image.\n\n    Args:\n      tag: str: label for this data\n      mpl_plt: matplotlib stateful pyplot object with prepared plotting state\n      step: int: training step\n      close_plot: bool: automatically closes plot", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_run_by_other_worker(worker):\n    \"\"\"\n    \n    \"\"\"\n    task_sets = _get_external_workers(worker).values()\n    return functools.reduce(lambda a, b: a | b, task_sets, set())", "output": "This returns a set of the tasks that are being run by other worker", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_dvportgroup(portgroup, dvs, service_instance=None):\n    '''\n    \n    '''\n    log.trace('Removing portgroup\\'%s\\' in dvs \\'%s\\'', portgroup, dvs)\n    proxy_type = get_proxy_type()\n    if proxy_type == 'esxdatacenter':\n        datacenter = __salt__['esxdatacenter.get_details']()['datacenter']\n        dc_ref = _get_proxy_target(service_instance)\n    elif proxy_type == 'esxcluster':\n        datacenter = __salt__['esxcluster.get_details']()['datacenter']\n        dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)\n    dvs_refs = salt.utils.vmware.get_dvss(dc_ref, dvs_names=[dvs])\n    if not dvs_refs:\n        raise VMwareObjectRetrievalError('DVS \\'{0}\\' was not '\n                                         'retrieved'.format(dvs))\n    pg_refs = salt.utils.vmware.get_dvportgroups(dvs_refs[0],\n                                                 portgroup_names=[portgroup])\n    if not pg_refs:\n        raise VMwareObjectRetrievalError('Portgroup \\'{0}\\' was not '\n                                         'retrieved'.format(portgroup))\n    salt.utils.vmware.remove_dvportgroup(pg_refs[0])\n    return True", "output": "Removes a distributed virtual portgroup.\n\n    portgroup\n        Name of the portgroup to be removed.\n\n    dvs\n        Name of the DVS containing the portgroups.\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.remove_dvportgroup portgroup=pg1 dvs=dvs1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_device_role(role, color):\n    '''\n    \n    '''\n    nb_role = get_('dcim', 'device-roles', name=role)\n    if nb_role:\n        return False\n    else:\n        payload = {'name': role, 'slug': slugify(role), 'color': color}\n        role = _add('dcim', 'device-roles', payload)\n        if role:\n            return{'dcim': {'device-roles': payload}}\n        else:\n            return False", "output": ".. versionadded:: 2019.2.0\n\n    Create a device role\n\n    role\n        String of device role, e.g., ``router``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion netbox.create_device_role router", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_match_number(self, pattern, case=False, regexp=False):\r\n        \"\"\"\"\"\"\r\n        position = self.textCursor().position()\r\n        source_text = self.get_text(position_from='sof', position_to=position)\r\n        match_number = self.get_number_matches(pattern,\r\n                                               source_text=source_text,\r\n                                               case=case, regexp=regexp)\r\n        return match_number", "output": "Get number of the match for the searched text.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_dist_restriction(options, check_target=False):\n    # type: (Values, bool) -> None\n    \"\"\"\n    \"\"\"\n    dist_restriction_set = any([\n        options.python_version,\n        options.platform,\n        options.abi,\n        options.implementation,\n    ])\n\n    binary_only = FormatControl(set(), {':all:'})\n    sdist_dependencies_allowed = (\n        options.format_control != binary_only and\n        not options.ignore_dependencies\n    )\n\n    # Installations or downloads using dist restrictions must not combine\n    # source distributions and dist-specific wheels, as they are not\n    # gauranteed to be locally compatible.\n    if dist_restriction_set and sdist_dependencies_allowed:\n        raise CommandError(\n            \"When restricting platform and interpreter constraints using \"\n            \"--python-version, --platform, --abi, or --implementation, \"\n            \"either --no-deps must be set, or --only-binary=:all: must be \"\n            \"set and --no-binary must not be set (or must be set to \"\n            \":none:).\"\n        )\n\n    if check_target:\n        if dist_restriction_set and not options.target_dir:\n            raise CommandError(\n                \"Can not use any platform or abi specific options unless \"\n                \"installing via '--target'\"\n            )", "output": "Function for determining if custom platform options are allowed.\n\n    :param options: The OptionParser options.\n    :param check_target: Whether or not to check if --target is being used.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_dividends(self, next_session, asset_finder, adjustment_reader):\n        \"\"\"\n        \"\"\"\n        position_tracker = self.position_tracker\n\n        # Earn dividends whose ex_date is the next trading day. We need to\n        # check if we own any of these stocks so we know to pay them out when\n        # the pay date comes.\n        held_sids = set(position_tracker.positions)\n        if held_sids:\n            cash_dividends = adjustment_reader.get_dividends_with_ex_date(\n                held_sids,\n                next_session,\n                asset_finder\n            )\n            stock_dividends = (\n                adjustment_reader.get_stock_dividends_with_ex_date(\n                    held_sids,\n                    next_session,\n                    asset_finder\n                )\n            )\n\n            # Earning a dividend just marks that we need to get paid out on\n            # the dividend's pay-date. This does not affect our cash yet.\n            position_tracker.earn_dividends(\n                cash_dividends,\n                stock_dividends,\n            )\n\n        # Pay out the dividends whose pay-date is the next session. This does\n        # affect out cash.\n        self._cash_flow(\n            position_tracker.pay_dividends(\n                next_session,\n            ),\n        )", "output": "Process dividends for the next session.\n\n        This will earn us any dividends whose ex-date is the next session as\n        well as paying out any dividends whose pay-date is the next session", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resource_string(self):\n        \"\"\"\"\"\"\n\n        if self._resources_initialized:\n            res_str = \"{} CPUs, {} GPUs\".format(self._avail_resources.cpu,\n                                                self._avail_resources.gpu)\n            if self._avail_resources.custom_resources:\n                custom = \", \".join(\n                    \"{} {}\".format(\n                        self._avail_resources.get_res_total(name), name)\n                    for name in self._avail_resources.custom_resources)\n                res_str += \" ({})\".format(custom)\n            return res_str\n        else:\n            return \"? CPUs, ? GPUs\"", "output": "Returns a string describing the total resources available.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def translate_paths (properties, path):\n    \"\"\" \n        \"\"\"\n    assert is_iterable_typed(properties, Property)\n    result = []\n\n    for p in properties:\n\n        if p.feature.path:\n            values = __re_two_ampersands.split(p.value)\n\n            new_value = \"&&\".join(os.path.normpath(os.path.join(path, v)) for v in values)\n\n            if new_value != p.value:\n                result.append(Property(p.feature, new_value, p.condition))\n            else:\n                result.append(p)\n\n        else:\n            result.append (p)\n\n    return result", "output": "Interpret all path properties in 'properties' as relative to 'path'\n        The property values are assumed to be in system-specific form, and\n        will be translated into normalized form.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def inspect(self):\n        \"\"\"\n        \n        \"\"\"\n        return dedent(\n            \"\"\"\\\n            Adjusted Array ({dtype}):\n\n            Data:\n            {data!r}\n\n            Adjustments:\n            {adjustments}\n            \"\"\"\n        ).format(\n            dtype=self.dtype.name,\n            data=self.data,\n            adjustments=self.adjustments,\n        )", "output": "Return a string representation of the data stored in this array.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def before_invoke(self, coro):\n        \"\"\"\n        \"\"\"\n        if not asyncio.iscoroutinefunction(coro):\n            raise TypeError('The pre-invoke hook must be a coroutine.')\n\n        self._before_invoke = coro\n        return coro", "output": "A decorator that registers a coroutine as a pre-invoke hook.\n\n        A pre-invoke hook is called directly before the command is\n        called. This makes it a useful function to set up database\n        connections or any type of set up required.\n\n        This pre-invoke hook takes a sole parameter, a :class:`.Context`.\n\n        See :meth:`.Bot.before_invoke` for more info.\n\n        Parameters\n        -----------\n        coro: :ref:`coroutine <coroutine>`\n            The coroutine to register as the pre-invoke hook.\n\n        Raises\n        -------\n        TypeError\n            The coroutine passed is not actually a coroutine.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unpack_img(s, iscolor=-1):\n    \"\"\"\n    \"\"\"\n    header, s = unpack(s)\n    img = np.frombuffer(s, dtype=np.uint8)\n    assert cv2 is not None\n    img = cv2.imdecode(img, iscolor)\n    return header, img", "output": "Unpack a MXImageRecord to image.\n\n    Parameters\n    ----------\n    s : str\n        String buffer from ``MXRecordIO.read``.\n    iscolor : int\n        Image format option for ``cv2.imdecode``.\n\n    Returns\n    -------\n    header : IRHeader\n        Header of the image record.\n    img : numpy.ndarray\n        Unpacked image.\n\n    Examples\n    --------\n    >>> record = mx.recordio.MXRecordIO('test.rec', 'r')\n    >>> item = record.read()\n    >>> header, img = mx.recordio.unpack_img(item)\n    >>> header\n    HEADER(flag=0, label=14.0, id=20129312, id2=0)\n    >>> img\n    array([[[ 23,  27,  45],\n            [ 28,  32,  50],\n            ...,\n            [ 36,  40,  59],\n            [ 35,  39,  58]],\n           ...,\n           [[ 91,  92, 113],\n            [ 97,  98, 119],\n            ...,\n            [168, 169, 167],\n            [166, 167, 165]]], dtype=uint8)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _pkg(jail=None, chroot=None, root=None):\n    '''\n    \n    '''\n    ret = ['pkg']\n    if jail:\n        ret.extend(['-j', jail])\n    elif chroot:\n        ret.extend(['-c', chroot])\n    elif root:\n        ret.extend(['-r', root])\n    return ret", "output": "Returns the prefix for a pkg command, using -j if a jail is specified, or\n    -c if chroot is specified.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_info(self, info):\n        \"\"\"  \"\"\"\n\n        for key in self._info_fields:\n\n            value = getattr(self, key, None)\n            idx = _get_info(info, self.name)\n\n            existing_value = idx.get(key)\n            if key in idx and value is not None and existing_value != value:\n\n                # frequency/name just warn\n                if key in ['freq', 'index_name']:\n                    ws = attribute_conflict_doc % (key, existing_value, value)\n                    warnings.warn(ws, AttributeConflictWarning, stacklevel=6)\n\n                    # reset\n                    idx[key] = None\n                    setattr(self, key, None)\n\n                else:\n                    raise ValueError(\n                        \"invalid info for [{name}] for [{key}], \"\n                        \"existing_value [{existing_value}] conflicts with \"\n                        \"new value [{value}]\".format(\n                            name=self.name, key=key,\n                            existing_value=existing_value, value=value))\n            else:\n                if value is not None or existing_value is not None:\n                    idx[key] = value\n\n        return self", "output": "set/update the info for this indexable with the key/value\n            if there is a conflict raise/warn as needed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __init_from_csc(self, csc, params_str, ref_dataset):\n        \"\"\"\"\"\"\n        if len(csc.indices) != len(csc.data):\n            raise ValueError('Length mismatch: {} vs {}'.format(len(csc.indices), len(csc.data)))\n        self.handle = ctypes.c_void_p()\n\n        ptr_indptr, type_ptr_indptr, __ = c_int_array(csc.indptr)\n        ptr_data, type_ptr_data, _ = c_float_array(csc.data)\n\n        assert csc.shape[0] <= MAX_INT32\n        csc.indices = csc.indices.astype(np.int32, copy=False)\n\n        _safe_call(_LIB.LGBM_DatasetCreateFromCSC(\n            ptr_indptr,\n            ctypes.c_int(type_ptr_indptr),\n            csc.indices.ctypes.data_as(ctypes.POINTER(ctypes.c_int32)),\n            ptr_data,\n            ctypes.c_int(type_ptr_data),\n            ctypes.c_int64(len(csc.indptr)),\n            ctypes.c_int64(len(csc.data)),\n            ctypes.c_int64(csc.shape[0]),\n            c_str(params_str),\n            ref_dataset,\n            ctypes.byref(self.handle)))\n        return self", "output": "Initialize data from a CSC matrix.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def seek(self, pos=0):\n        \"\"\"\n        \"\"\"\n        if pos - self.pos >= 0:\n            blocks, remainder = divmod(pos - self.pos, self.bufsize)\n            for i in range(blocks):\n                self.read(self.bufsize)\n            self.read(remainder)\n        else:\n            raise StreamError(\"seeking backwards is not allowed\")\n        return self.pos", "output": "Set the stream's file pointer to pos. Negative seeking\n           is forbidden.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fit(self, validation_data=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        callbacks = kwargs.pop('callbacks', [])\n        if validation_data is not None:\n            # There is no way to guess where users want this callback. So we have to choose one.\n            # MinSaver may need results from this callback,\n            # so we put this callback at first.\n            callbacks.insert(0, InferenceRunner(\n                validation_data, ScalarStats(self._stats_to_inference)))\n        self.trainer.train_with_defaults(callbacks=callbacks, **kwargs)", "output": "Args:\n            validation_data (DataFlow or InputSource): to be used for inference.\n                The inference callback is added as the first in the callback list.\n                If you need to use it in a different order, please write it in the callback list manually.\n            kwargs: same arguments as :meth:`Trainer.train_with_defaults`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rebuild_proxies(self, prepared_request, proxies):\n        \"\"\"\n        \"\"\"\n        proxies = proxies if proxies is not None else {}\n        headers = prepared_request.headers\n        url = prepared_request.url\n        scheme = urlparse(url).scheme\n        new_proxies = proxies.copy()\n        no_proxy = proxies.get('no_proxy')\n\n        bypass_proxy = should_bypass_proxies(url, no_proxy=no_proxy)\n        if self.trust_env and not bypass_proxy:\n            environ_proxies = get_environ_proxies(url, no_proxy=no_proxy)\n\n            proxy = environ_proxies.get(scheme, environ_proxies.get('all'))\n\n            if proxy:\n                new_proxies.setdefault(scheme, proxy)\n\n        if 'Proxy-Authorization' in headers:\n            del headers['Proxy-Authorization']\n\n        try:\n            username, password = get_auth_from_url(new_proxies[scheme])\n        except KeyError:\n            username, password = None, None\n\n        if username and password:\n            headers['Proxy-Authorization'] = _basic_auth_str(username, password)\n\n        return new_proxies", "output": "This method re-evaluates the proxy configuration by considering the\n        environment variables. If we are redirected to a URL covered by\n        NO_PROXY, we strip the proxy configuration. Otherwise, we set missing\n        proxy keys for this URL (in case they were stripped by a previous\n        redirect).\n\n        This method also replaces the Proxy-Authorization header where\n        necessary.\n\n        :rtype: dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_state(cls, path:PathOrStr, state:dict) -> 'LabelList':\n        \"\"\n        x = state['x_cls']([], path=path, processor=state['x_proc'], ignore_empty=True)\n        y = state['y_cls']([], path=path, processor=state['y_proc'], ignore_empty=True)\n        res = cls(x, y, tfms=state['tfms'], tfm_y=state['tfm_y'], **state['tfmargs']).process()\n        if state.get('tfms_y', False):    res.tfms_y    = state['tfms_y']\n        if state.get('tfmargs_y', False): res.tfmargs_y = state['tfmargs_y']\n        if state.get('normalize', False): res.normalize = state['normalize']\n        return res", "output": "Create a `LabelList` from `state`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_get(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.get_image(**kwargs)", "output": "Get a single image\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' glanceng.image_get name=image1\n        salt '*' glanceng.image_get name=0e4febc2a5ab4f2c8f374b054162506d", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def begin(self):\n        \"\"\"\n        \"\"\"\n        if self._status != self._INITIAL:\n            raise ValueError(\"Batch already started previously.\")\n        self._status = self._IN_PROGRESS", "output": "Begins a batch.\n\n        This method is called automatically when entering a with\n        statement, however it can be called explicitly if you don't want\n        to use a context manager.\n\n        Overridden by :class:`google.cloud.datastore.transaction.Transaction`.\n\n        :raises: :class:`ValueError` if the batch has already begun.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_config_lock(name):\n    '''\n    \n\n    '''\n    ret = _default_ret(name)\n\n    ret.update({\n        'changes': __salt__['panos.remove_config_lock'](),\n        'result': True\n    })\n\n    return ret", "output": "Release config lock previously held.\n\n    name: The name of the module function to execute.\n\n    SLS Example:\n\n    .. code-block:: yaml\n\n        panos/takelock:\n            panos.remove_config_lock", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def entry_from_resource(resource, client, loggers):\n    \"\"\"\n    \"\"\"\n    if \"textPayload\" in resource:\n        return TextEntry.from_api_repr(resource, client, loggers)\n\n    if \"jsonPayload\" in resource:\n        return StructEntry.from_api_repr(resource, client, loggers)\n\n    if \"protoPayload\" in resource:\n        return ProtobufEntry.from_api_repr(resource, client, loggers)\n\n    return LogEntry.from_api_repr(resource, client, loggers)", "output": "Detect correct entry type from resource and instantiate.\n\n    :type resource: dict\n    :param resource: One entry resource from API response.\n\n    :type client: :class:`~google.cloud.logging.client.Client`\n    :param client: Client that owns the log entry.\n\n    :type loggers: dict\n    :param loggers:\n        A mapping of logger fullnames -> loggers.  If the logger\n        that owns the entry is not in ``loggers``, the entry\n        will have a newly-created logger.\n\n    :rtype: :class:`~google.cloud.logging.entries._BaseEntry`\n    :returns: The entry instance, constructed via the resource", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_from_json(self, obj, json, models=None, setter=None):\n        ''' \n\n        '''\n        return super(BasicPropertyDescriptor, self).set_from_json(obj,\n                                                        self.property.from_json(json, models),\n                                                        models, setter)", "output": "Sets the value of this property from a JSON value.\n\n        This method first\n\n        Args:\n            obj (HasProps) :\n\n            json (JSON-dict) :\n\n            models(seq[Model], optional) :\n\n            setter (ClientSession or ServerSession or None, optional) :\n                This is used to prevent \"boomerang\" updates to Bokeh apps.\n                (default: None)\n\n                In the context of a Bokeh server application, incoming updates\n                to properties will be annotated with the session that is\n                doing the updating. This value is propagated through any\n                subsequent change notifications that the update triggers.\n                The session can compare the event setter to itself, and\n                suppress any updates that originate from itself.\n\n        Returns:\n            None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _maybe_coerce_values(self, values):\n        \"\"\"\n        \"\"\"\n        if isinstance(values, (ABCIndexClass, ABCSeries)):\n            values = values._values\n        return values", "output": "Unbox to an extension array.\n\n        This will unbox an ExtensionArray stored in an Index or Series.\n        ExtensionArrays pass through. No dtype coercion is done.\n\n        Parameters\n        ----------\n        values : Index, Series, ExtensionArray\n\n        Returns\n        -------\n        ExtensionArray", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avg_grads(tower_grads):\n  \"\"\"\n  \"\"\"\n  if len(tower_grads) == 1:\n    return tower_grads[0]\n  average_grads = []\n  for grad_and_vars in zip(*tower_grads):\n    # Note that each grad_and_vars looks like the following:\n    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n    grads = [g for g, _ in grad_and_vars]\n\n    # Average over the 'tower' dimension.\n    grad = tf.add_n(grads) / len(grads)\n\n    # Keep in mind that the Variables are redundant because they are shared\n    # across towers. So .. we will just return the first tower's pointer to\n    # the Variable.\n    v = grad_and_vars[0][1]\n    assert all(v is grad_and_var[1] for grad_and_var in grad_and_vars)\n    grad_and_var = (grad, v)\n    average_grads.append(grad_and_var)\n  return average_grads", "output": "Calculate the average gradient for each shared variable across all\n  towers.\n  Note that this function provides a synchronization point across all towers.\n  Args:\n    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n      is over individual gradients. The inner list is over the gradient\n      calculation for each tower.\n  Returns:\n     List of pairs of (gradient, variable) where the gradient has been\n     averaged across all towers.\n\n  Modified from this tutorial: https://tinyurl.com/n3jr2vm", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fits(A, B, temp, cos_distance):\n    \"\"\"\n    \"\"\"\n    if cos_distance:\n      distance_matrix = SNNLCrossEntropy.pairwise_cos_distance(A, B)\n    else:\n      distance_matrix = SNNLCrossEntropy.pairwise_euclid_distance(A, B)\n    return tf.exp(-(distance_matrix / temp))", "output": "Exponentiated pairwise distance between each element of A and\n    all those of B.\n    :param A: a matrix.\n    :param B: a matrix.\n    :param temp: Temperature\n    :cos_distance: Boolean for using cosine or Euclidean distance.\n\n    :returns: A tensor for the exponentiated pairwise distance between\n    each element and A and all those of B.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_SU_save_stock_block(engine, client=DATABASE):\n    \"\"\"\n    \"\"\"\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_stock_block(client=client)", "output": "save stock_block\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _coerce_to_ndarray(cls, data):\n        \"\"\"\n        \n        \"\"\"\n\n        if not isinstance(data, (np.ndarray, Index)):\n            if data is None or is_scalar(data):\n                cls._scalar_data_error(data)\n\n            # other iterable of some kind\n            if not isinstance(data, (ABCSeries, list, tuple)):\n                data = list(data)\n            data = np.asarray(data)\n        return data", "output": "Coerces data to ndarray.\n\n        Converts other iterables to list first and then to array.\n        Does not touch ndarrays.\n\n        Raises\n        ------\n        TypeError\n            When the data passed in is a scalar.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attach_network_interface(device_index, name=None, network_interface_id=None,\n                             instance_name=None, instance_id=None,\n                             region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    if not salt.utils.data.exactly_one((name, network_interface_id)):\n        raise SaltInvocationError(\n            \"Exactly one (but not both) of 'name' or 'network_interface_id' \"\n            \"must be provided.\"\n        )\n\n    if not salt.utils.data.exactly_one((instance_name, instance_id)):\n        raise SaltInvocationError(\n            \"Exactly one (but not both) of 'instance_name' or 'instance_id' \"\n            \"must be provided.\"\n        )\n\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    r = {}\n    result = _get_network_interface(conn, name, network_interface_id)\n    if 'error' in result:\n        return result\n    eni = result['result']\n    try:\n        info = _describe_network_interface(eni)\n        network_interface_id = info['id']\n    except KeyError:\n        r['error'] = {'message': 'ID not found for this network interface.'}\n        return r\n\n    if instance_name:\n        try:\n            instance_id = get_id(name=instance_name, region=region, key=key,\n                                 keyid=keyid, profile=profile)\n        except boto.exception.BotoServerError as e:\n            log.error(e)\n            return False\n\n    try:\n        r['result'] = conn.attach_network_interface(\n            network_interface_id, instance_id, device_index\n        )\n    except boto.exception.EC2ResponseError as e:\n        r['error'] = __utils__['boto.get_error'](e)\n    return r", "output": "Attach an Elastic Network Interface.\n\n    .. versionadded:: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_ec2.attach_network_interface my_eni instance_name=salt-master device_index=0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_requirement_info(dist):\n    # type: (Distribution) -> RequirementInfo\n    \"\"\"\n    \n    \"\"\"\n    if not dist_is_editable(dist):\n        return (None, False, [])\n\n    location = os.path.normcase(os.path.abspath(dist.location))\n\n    from pipenv.patched.notpip._internal.vcs import vcs, RemoteNotFoundError\n    vc_type = vcs.get_backend_type(location)\n\n    if not vc_type:\n        req = dist.as_requirement()\n        logger.debug(\n            'No VCS found for editable requirement {!r} in: {!r}', req,\n            location,\n        )\n        comments = [\n            '# Editable install with no version control ({})'.format(req)\n        ]\n        return (location, True, comments)\n\n    try:\n        req = vc_type.get_src_requirement(location, dist.project_name)\n    except RemoteNotFoundError:\n        req = dist.as_requirement()\n        comments = [\n            '# Editable {} install with no remote ({})'.format(\n                vc_type.__name__, req,\n            )\n        ]\n        return (location, True, comments)\n\n    except BadCommand:\n        logger.warning(\n            'cannot determine version of editable source in %s '\n            '(%s command not found in path)',\n            location,\n            vc_type.name,\n        )\n        return (None, True, [])\n\n    except InstallationError as exc:\n        logger.warning(\n            \"Error when trying to get requirement for VCS system %s, \"\n            \"falling back to uneditable format\", exc\n        )\n    else:\n        if req is not None:\n            return (req, True, [])\n\n    logger.warning(\n        'Could not determine repository location of %s', location\n    )\n    comments = ['## !! Could not determine repository location']\n\n    return (None, False, comments)", "output": "Compute and return values (req, editable, comments) for use in\n    FrozenRequirement.from_dist().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _worker_fn(samples, batchify_fn, dataset=None):\n    \"\"\"\"\"\"\n    # pylint: disable=unused-argument\n    # it is required that each worker process has to fork a new MXIndexedRecordIO handle\n    # preserving dataset as global variable can save tons of overhead and is safe in new process\n    global _worker_dataset\n    batch = batchify_fn([_worker_dataset[i] for i in samples])\n    buf = io.BytesIO()\n    ForkingPickler(buf, pickle.HIGHEST_PROTOCOL).dump(batch)\n    return buf.getvalue()", "output": "Function for processing data in worker process.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toString(value):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(value, basestring):\n            return value\n        elif type(value) in [np.string_, np.str_]:\n            return str(value)\n        elif type(value) == np.unicode_:\n            return unicode(value)\n        else:\n            raise TypeError(\"Could not convert %s to string type\" % type(value))", "output": "Convert a value to a string, if possible.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _lstm_unroll_base(num_lstm_layer, seq_len, num_hidden):\n    \"\"\" \"\"\"\n    param_cells = []\n    last_states = []\n    for i in range(num_lstm_layer):\n        param_cells.append(LSTMParam(i2h_weight=mx.sym.Variable(\"l%d_i2h_weight\" % i),\n                                     i2h_bias=mx.sym.Variable(\"l%d_i2h_bias\" % i),\n                                     h2h_weight=mx.sym.Variable(\"l%d_h2h_weight\" % i),\n                                     h2h_bias=mx.sym.Variable(\"l%d_h2h_bias\" % i)))\n        state = LSTMState(c=mx.sym.Variable(\"l%d_init_c\" % i),\n                          h=mx.sym.Variable(\"l%d_init_h\" % i))\n        last_states.append(state)\n    assert len(last_states) == num_lstm_layer\n\n    # embedding layer\n    data = mx.sym.Variable('data')\n    wordvec = mx.sym.SliceChannel(data=data, num_outputs=seq_len, squeeze_axis=1)\n\n    hidden_all = []\n    for seqidx in range(seq_len):\n        hidden = wordvec[seqidx]\n        for i in range(num_lstm_layer):\n            next_state = _lstm(\n                num_hidden=num_hidden,\n                indata=hidden,\n                prev_state=last_states[i],\n                param=param_cells[i],\n                seqidx=seqidx,\n                layeridx=i)\n            hidden = next_state.h\n            last_states[i] = next_state\n        hidden_all.append(hidden)\n\n    hidden_concat = mx.sym.Concat(*hidden_all, dim=0)\n    pred_fc = mx.sym.FullyConnected(data=hidden_concat, num_hidden=11, name=\"pred_fc\")\n    return pred_fc", "output": "Returns symbol for LSTM model up to loss/softmax", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ustr(x):\n    ''''''\n\n    if sys.version_info < (3, 0, 0):\n        from PyQt4.QtCore import QString\n        if type(x) == str:\n            return x.decode(DEFAULT_ENCODING)\n        if type(x) == QString:\n            #https://blog.csdn.net/friendan/article/details/51088476\n            #https://blog.csdn.net/xxm524/article/details/74937308\n            return unicode(x.toUtf8(), DEFAULT_ENCODING, 'ignore')\n        return x\n    else:\n        return x", "output": "py2/py3 unicode helper", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_module_bookmark_actions(parent, bookmarks):\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    actions = []\r\n    for key, url, title in bookmarks:\r\n        # Create actions for scientific distros only if Spyder is installed\r\n        # under them\r\n        create_act = True\r\n        if key == 'winpython':\r\n            if not programs.is_module_installed(key):\r\n                create_act = False\r\n        if create_act:\r\n            act = create_bookmark_action(parent, url, title)\r\n            actions.append(act)\r\n    return actions", "output": "Create bookmark actions depending on module installation:\r\n    bookmarks = ((module_name, url, title), ...)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def template_str(tem, queue=False, **kwargs):\n    '''\n    \n    '''\n    conflict = _check_queue(queue, kwargs)\n    if conflict is not None:\n        return conflict\n\n    opts = salt.utils.state.get_sls_opts(__opts__, **kwargs)\n\n    try:\n        st_ = salt.state.State(opts,\n                               proxy=__proxy__,\n                               initial_pillar=_get_initial_pillar(opts))\n    except NameError:\n        st_ = salt.state.State(opts, initial_pillar=_get_initial_pillar(opts))\n    ret = st_.call_template_str(tem)\n    _set_retcode(ret)\n    return ret", "output": "Execute the information stored in a string from an sls template\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.template_str '<Template String>'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_indicator_MACD(DataFrame, short=12, long=26, mid=9):\n    \"\"\"\n    \n    \"\"\"\n    CLOSE = DataFrame['close']\n\n    DIF = EMA(CLOSE, short)-EMA(CLOSE, long)\n    DEA = EMA(DIF, mid)\n    MACD = (DIF-DEA)*2\n\n    return pd.DataFrame({'DIF': DIF, 'DEA': DEA, 'MACD': MACD})", "output": "MACD CALC", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_period_range_edges(first, last, offset, closed='left', base=0):\n    \"\"\"\n    \n    \"\"\"\n    if not all(isinstance(obj, pd.Period) for obj in [first, last]):\n        raise TypeError(\"'first' and 'last' must be instances of type Period\")\n\n    # GH 23882\n    first = first.to_timestamp()\n    last = last.to_timestamp()\n    adjust_first = not offset.onOffset(first)\n    adjust_last = offset.onOffset(last)\n\n    first, last = _get_timestamp_range_edges(first, last, offset,\n                                             closed=closed, base=base)\n\n    first = (first + adjust_first * offset).to_period(offset)\n    last = (last - adjust_last * offset).to_period(offset)\n    return first, last", "output": "Adjust the provided `first` and `last` Periods to the respective Period of\n    the given offset that encompasses them.\n\n    Parameters\n    ----------\n    first : pd.Period\n        The beginning Period of the range to be adjusted.\n    last : pd.Period\n        The ending Period of the range to be adjusted.\n    offset : pd.DateOffset\n        The dateoffset to which the Periods will be adjusted.\n    closed : {'right', 'left'}, default None\n        Which side of bin interval is closed.\n    base : int, default 0\n        The \"origin\" of the adjusted Periods.\n\n    Returns\n    -------\n    A tuple of length 2, containing the adjusted pd.Period objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _groupby_and_aggregate(self, how, grouper=None, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n\n        if grouper is None:\n            self._set_binner()\n            grouper = self.grouper\n\n        obj = self._selected_obj\n\n        grouped = groupby(obj, by=None, grouper=grouper, axis=self.axis)\n\n        try:\n            if isinstance(obj, ABCDataFrame) and callable(how):\n                # Check if the function is reducing or not.\n                result = grouped._aggregate_item_by_item(how, *args, **kwargs)\n            else:\n                result = grouped.aggregate(how, *args, **kwargs)\n        except Exception:\n\n            # we have a non-reducing function\n            # try to evaluate\n            result = grouped.apply(how, *args, **kwargs)\n\n        result = self._apply_loffset(result)\n        return self._wrap_result(result)", "output": "Re-evaluate the obj with a groupby aggregation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init(images, num_channels, dim='2d', stride=2,\n         kernel_size=7, maxpool=True, training=True, scope='init'):\n  \"\"\"\n  \"\"\"\n  conv = CONFIG[dim]['conv']\n  pool = CONFIG[dim]['max_pool']\n  with tf.variable_scope(scope):\n    net = conv(images, num_channels, kernel_size, strides=stride,\n               padding='SAME', activation=None)\n    net = tf.layers.batch_normalization(net, training=training)\n    net = tf.nn.relu(net)\n    if maxpool:\n      net = pool(net, pool_size=3, strides=stride)\n    x1, x2 = tf.split(net, 2, axis=CONFIG[dim]['split_axis'])\n    return x1, x2", "output": "Standard ResNet initial block used as first RevNet block.\n\n  Args:\n    images: [N, H, W, 3] tensor of input images to the model.\n    num_channels: Output depth of convolutional layer in initial block.\n    dim: '2d' if 2-dimensional, '3d' if 3-dimensional.\n    stride: stride for the convolution and pool layer.\n    kernel_size: Size of the initial convolution filter\n    maxpool: If true, apply a maxpool after the convolution\n    training: True for train phase, False for eval phase.\n    scope: Optional scope for the init block.\n\n  Returns:\n    Two [N, H, W, C] output activations from input images.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe(Name,\n             region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        trails = conn.describe_trails(trailNameList=[Name])\n        if trails and trails.get('trailList'):\n            keys = ('Name', 'S3BucketName', 'S3KeyPrefix',\n                    'SnsTopicName', 'IncludeGlobalServiceEvents',\n                    'IsMultiRegionTrail',\n                    'HomeRegion', 'TrailARN',\n                    'LogFileValidationEnabled', 'CloudWatchLogsLogGroupArn',\n                    'CloudWatchLogsRoleArn', 'KmsKeyId')\n            trail = trails['trailList'].pop()\n            return {'trail': dict([(k, trail.get(k)) for k in keys])}\n        else:\n            return {'trail': None}\n    except ClientError as e:\n        err = __utils__['boto3.get_error'](e)\n        if e.response.get('Error', {}).get('Code') == 'TrailNotFoundException':\n            return {'trail': None}\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "Given a trail name describe its properties.\n\n    Returns a dictionary of interesting properties.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_cloudtrail.describe mytrail", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_sshkey(host, known_hosts=None):\n    '''\n    \n    '''\n    if known_hosts is None:\n        if 'HOME' in os.environ:\n            known_hosts = '{0}/.ssh/known_hosts'.format(os.environ['HOME'])\n        else:\n            try:\n                known_hosts = '{0}/.ssh/known_hosts'.format(\n                    pwd.getpwuid(os.getuid()).pwd_dir\n                )\n            except Exception:\n                pass\n\n    if known_hosts is not None:\n        log.debug(\n            'Removing ssh key for %s from known hosts file %s',\n            host, known_hosts\n        )\n    else:\n        log.debug('Removing ssh key for %s from known hosts file', host)\n\n    cmd = 'ssh-keygen -R {0}'.format(host)\n    subprocess.call(cmd, shell=True)", "output": "Remove a host from the known_hosts file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def total_value(self):\n        \"\"\"\n        \n        \"\"\"\n        return sum(account.total_value for account in six.itervalues(self._accounts))", "output": "[float]\u603b\u6743\u76ca", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save(host=None, port=None, db=None, password=None):\n    '''\n    \n    '''\n    server = _connect(host, port, db, password)\n    return server.save()", "output": "Synchronously save the dataset to disk\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.save", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Load(self):\n    \"\"\"\n    \"\"\"\n    for record in super(EventFileLoader, self).Load():\n      yield event_pb2.Event.FromString(record)", "output": "Loads all new events from disk.\n\n    Calling Load multiple times in a row will not 'drop' events as long as the\n    return value is not iterated over.\n\n    Yields:\n      All events in the file that have not been yielded yet.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_account(self, portfolio_cookie: str, account_cookie: str):\n        \"\"\"\n        \"\"\"\n\n        try:\n            return self.portfolio_list[portfolio_cookie][account_cookie]\n        except:\n            return None", "output": "\u76f4\u63a5\u4ece\u4e8c\u7ea7\u76ee\u5f55\u62ff\u5230account\n\n        Arguments:\n            portfolio_cookie {str} -- [description]\n            account_cookie {str} -- [description]\n\n        Returns:\n            [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_ctp_tick(code, start, end, frequence, format='pd', collections=DATABASE.ctp_tick):\n    \"\"\"\n    \"\"\"\n\n    code = QA_util_code_tolist(code, auto_fill=False)\n    cursor = collections.find({\n        'InstrumentID': {'$in': code}, \"time_stamp\": {\n            \"$gte\": QA_util_time_stamp(start),\n            \"$lte\": QA_util_time_stamp(end)\n        }, 'type': frequence\n    }, {\"_id\": 0}, batch_size=10000)\n\n    hq = pd.DataFrame([data for data in cursor]).replace(1.7976931348623157e+308,\n                                                         numpy.nan).replace('', numpy.nan).dropna(axis=1)\n    p1 = hq.loc[:, ['ActionDay', 'AskPrice1', 'AskVolume1', 'AveragePrice', 'BidPrice1',\n                    'BidVolume1', 'HighestPrice', 'InstrumentID', 'LastPrice',\n                    'OpenInterest', 'TradingDay', 'UpdateMillisec',\n                    'UpdateTime', 'Volume']]\n    p1 = p1.assign(datetime=p1.ActionDay.apply(QA_util_date_int2str)+' '+p1.UpdateTime + (p1.UpdateMillisec/1000000).apply(lambda x: str('%.6f' % x)[1:]),\n                   code=p1.InstrumentID)\n    p1.datetime = pd.to_datetime(p1.datetime)\n    return p1.set_index(p1.datetime)", "output": "\u4ec5\u4f9b\u5b58\u50a8\u7684ctp tick\u4f7f\u7528\n\n    Arguments:\n        code {[type]} -- [description]\n\n    Keyword Arguments:\n        format {str} -- [description] (default: {'pd'})\n        collections {[type]} -- [description] (default: {DATABASE.ctp_tick})\n\n    Returns:\n        [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_folder_names(self, folder_names):\r\n        \"\"\"\"\"\"\r\n        assert self.root_path is not None\r\n        path_list = [osp.join(self.root_path, dirname)\r\n                     for dirname in folder_names]\r\n        self.proxymodel.setup_filter(self.root_path, path_list)", "output": "Set folder names", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pub_ret(self, load, skip_verify=False):\n        '''\n        \n        '''\n        if not skip_verify and any(key not in load for key in ('jid', 'id')):\n            return {}\n        else:\n            auth_cache = os.path.join(\n                    self.opts['cachedir'],\n                    'publish_auth')\n            if not os.path.isdir(auth_cache):\n                os.makedirs(auth_cache)\n            jid_fn = os.path.join(auth_cache, load['jid'])\n            with salt.utils.files.fopen(jid_fn, 'r') as fp_:\n                if not load['id'] == salt.utils.stringutils.to_unicode(fp_.read()):\n                    return {}\n\n            return self.local.get_cache_returns(load['jid'])", "output": "Request the return data from a specific jid, only allowed\n        if the requesting minion also initialted the execution.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def directives():\n    '''\n    \n    '''\n    cmd = '{0} -L'.format(_detect_os())\n    ret = {}\n    out = __salt__['cmd.run'](cmd)\n    out = out.replace('\\n\\t', '\\t')\n    for line in out.splitlines():\n        if not line:\n            continue\n        comps = line.split('\\t')\n        desc = '\\n'.join(comps[1:])\n        ret[comps[0]] = desc\n    return ret", "output": "Return list of directives together with expected arguments\n    and places where the directive is valid (``apachectl -L``)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' apache.directives", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def snapshots_all(container, remote_addr=None, cert=None, key=None, verify_cert=True):\n    '''\n    \n    '''\n    containers = container_get(\n        container, remote_addr, cert, key, verify_cert, _raw=True\n    )\n    if container:\n        containers = [containers]\n    ret = {}\n    for cont in containers:\n        ret.update({cont.name: [{'name': c.name}\n                                for c in cont.snapshots.all()]})\n\n    return ret", "output": "Get all snapshots for a container\n\n    container :\n        The name of the container to get.\n\n    remote_addr :\n        An URL to a remote server. The 'cert' and 'key' fields must also be\n        provided if 'remote_addr' is defined.\n\n        Examples:\n            https://myserver.lan:8443\n            /var/lib/mysocket.sock\n\n    cert :\n        PEM Formatted SSL Certificate.\n\n        Examples:\n            ~/.config/lxc/client.crt\n\n    key :\n        PEM Formatted SSL Key.\n\n        Examples:\n            ~/.config/lxc/client.key\n\n    verify_cert : True\n        Verify the ssl certificate.  Default: True\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        $ salt '*' lxd.snapshots_all test-container", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gcs_files(prefix_filter=None):\n  \"\"\"\"\"\"\n  top_level_xml_str = download_gcs_file(\"\", prefix_filter=prefix_filter)\n  xml_root = ElementTree.fromstring(top_level_xml_str)\n  filenames = [el[0].text for el in xml_root if el.tag.endswith(\"Contents\")]\n  return filenames", "output": "List all files in GCS bucket.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _http_request(url,\n                  headers=None,\n                  data=None):\n    '''\n    \n    '''\n    if not headers:\n        headers = _get_headers()\n    session = requests.session()\n    log.debug('Querying %s', url)\n    req = session.post(url,\n                       headers=headers,\n                       data=salt.utils.json.dumps(data))\n    req_body = req.json()\n    ret = _default_ret()\n    log.debug('Status code: %d', req.status_code)\n    log.debug('Response body:')\n    log.debug(req_body)\n    if req.status_code != 200:\n        if req.status_code == 500:\n            ret['comment'] = req_body.pop('message', '')\n            ret['out'] = req_body\n            return ret\n        ret.update({\n            'comment': req_body.get('error', '')\n        })\n        return ret\n    ret.update({\n        'result': True,\n        'out': req.json()\n    })\n    return ret", "output": "Make the HTTP request and return the body as python object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_csi_driver(self, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_csi_driver_with_http_info(body, **kwargs)\n        else:\n            (data) = self.create_csi_driver_with_http_info(body, **kwargs)\n            return data", "output": "create a CSIDriver\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_csi_driver(body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param V1beta1CSIDriver body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1beta1CSIDriver\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_experiment_tag(self):\n    \"\"\"\n    \"\"\"\n    with self._experiment_from_tag_lock:\n      if self._experiment_from_tag is None:\n        mapping = self.multiplexer.PluginRunToTagToContent(\n            metadata.PLUGIN_NAME)\n        for tag_to_content in mapping.values():\n          if metadata.EXPERIMENT_TAG in tag_to_content:\n            self._experiment_from_tag = metadata.parse_experiment_plugin_data(\n                tag_to_content[metadata.EXPERIMENT_TAG])\n            break\n    return self._experiment_from_tag", "output": "Finds the experiment associcated with the metadata.EXPERIMENT_TAG tag.\n\n    Caches the experiment if it was found.\n\n    Returns:\n      The experiment or None if no such experiment is found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def normalize(self, dt):\n        \"\"\"\n        \n        \"\"\"\n        if dt is None:\n            return None\n\n        dt = self._convert_to_dt(dt)\n\n        dt = dt.replace(microsecond=0)  # remove microseconds, to avoid float rounding issues.\n        delta = (dt - self.start).total_seconds()\n        granularity = (self._timedelta * self.interval).total_seconds()\n        return dt - datetime.timedelta(seconds=delta % granularity)", "output": "Clamp dt to every Nth :py:attr:`~_DatetimeParameterBase.interval` starting at\n        :py:attr:`~_DatetimeParameterBase.start`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dir_list(load):\n    '''\n    \n    '''\n    ret = set()\n    files = file_list(load)\n    for f in files:\n        dirname = f\n        while dirname:\n            dirname = os.path.dirname(dirname)\n            if dirname:\n                ret.add(dirname)\n    return list(ret)", "output": "Return a list of all directories in a specified environment", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_iam_policy(self, policy):\n        \"\"\"\n        \"\"\"\n        instance_admin_client = self._client.instance_admin_client\n        resp = instance_admin_client.set_iam_policy(\n            resource=self.name, policy=policy.to_pb()\n        )\n        return Policy.from_pb(resp)", "output": "Sets the access control policy on an instance resource. Replaces any\n        existing policy.\n\n        For more information about policy, please see documentation of\n        class `google.cloud.bigtable.policy.Policy`\n\n        For example:\n\n        .. literalinclude:: snippets.py\n            :start-after: [START bigtable_set_iam_policy]\n            :end-before: [END bigtable_set_iam_policy]\n\n        :type policy: :class:`google.cloud.bigtable.policy.Policy`\n        :param policy: A new IAM policy to replace the current IAM policy\n                       of this instance\n\n        :rtype: :class:`google.cloud.bigtable.policy.Policy`\n        :returns: The current IAM policy of this instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def collections(self, page_size=None):\n        \"\"\"\n        \"\"\"\n        iterator = self._client._firestore_api.list_collection_ids(\n            self._document_path,\n            page_size=page_size,\n            metadata=self._client._rpc_metadata,\n        )\n        iterator.document = self\n        iterator.item_to_value = _item_to_collection_ref\n        return iterator", "output": "List subcollections of the current document.\n\n        Args:\n            page_size (Optional[int]]): The maximum number of collections\n            in each page of results from this request. Non-positive values\n            are ignored. Defaults to a sensible value set by the API.\n\n        Returns:\n            Sequence[~.firestore_v1beta1.collection.CollectionReference]:\n                iterator of subcollections of the current document. If the\n                document does not exist at the time of `snapshot`, the\n                iterator will be empty", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fit(epochs:int, learn:BasicLearner, callbacks:Optional[CallbackList]=None, metrics:OptMetrics=None)->None:\n    \"\"\n    assert len(learn.data.train_dl) != 0, f\"\"\"Your training dataloader is empty, can't train a model.\n        Use a smaller batch size (batch size={learn.data.train_dl.batch_size} for {len(learn.data.train_dl.dataset)} elements).\"\"\"\n    cb_handler = CallbackHandler(callbacks, metrics)\n    pbar = master_bar(range(epochs))\n    cb_handler.on_train_begin(epochs, pbar=pbar, metrics=metrics)\n\n    exception=False\n    try:\n        for epoch in pbar:\n            learn.model.train()\n            cb_handler.set_dl(learn.data.train_dl)\n            cb_handler.on_epoch_begin()\n            for xb,yb in progress_bar(learn.data.train_dl, parent=pbar):\n                xb, yb = cb_handler.on_batch_begin(xb, yb)\n                loss = loss_batch(learn.model, xb, yb, learn.loss_func, learn.opt, cb_handler)\n                if cb_handler.on_batch_end(loss): break\n\n            if not cb_handler.skip_validate and not learn.data.empty_val:\n                val_loss = validate(learn.model, learn.data.valid_dl, loss_func=learn.loss_func,\n                                       cb_handler=cb_handler, pbar=pbar)\n            else: val_loss=None\n            if cb_handler.on_epoch_end(val_loss): break\n    except Exception as e:\n        exception = e\n        raise\n    finally: cb_handler.on_train_end(exception)", "output": "Fit the `model` on `data` and learn using `loss_func` and `opt`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_auth(name, sock_dir=None, queue=None, timeout=300):\n    '''\n    \n    '''\n    event = salt.utils.event.SaltEvent('master', sock_dir, listen=True)\n    starttime = time.mktime(time.localtime())\n    newtimeout = timeout\n    log.debug('In check_auth, waiting for %s to become available', name)\n    while newtimeout > 0:\n        newtimeout = timeout - (time.mktime(time.localtime()) - starttime)\n        ret = event.get_event(full=True)\n        if ret is None:\n            continue\n        if ret['tag'] == 'salt/minion/{0}/start'.format(name):\n            queue.put(name)\n            newtimeout = 0\n            log.debug('Minion %s is ready to receive commands', name)", "output": "This function is called from a multiprocess instance, to wait for a minion\n    to become available to receive salt commands", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_network(self, network):\n        '''\n        \n        '''\n        net_id = self._find_network_id(network)\n        ret = self.network_conn.delete_network(network=net_id)\n        return ret if ret else True", "output": "Deletes the specified network", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kill_process_children(pid):\n    \"\"\"\n    \"\"\"\n    if sys.platform == \"darwin\":\n        kill_process_children_osx(pid)\n    elif sys.platform == \"linux\":\n        kill_process_children_unix(pid)\n    else:\n        pass", "output": "Find and kill child processes of a process.\n\n    :param pid: PID of parent process (process ID)\n    :return: Nothing", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _output_to_list(cmdoutput):\n    '''\n    \n    '''\n    return [item for line in cmdoutput.splitlines() if _safe_output(line) for item in line.split()]", "output": "Convert rabbitmqctl output to a list of strings (assuming whitespace-delimited output).\n    Ignores output lines that shouldn't be parsed, like warnings.\n    cmdoutput: string output of rabbitmqctl commands", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_model(model_name, epoch_num, data_shapes, label_shapes, label_names, gpus=''):\n    \"\"\"\n    \"\"\"\n    sym, arg_params, aux_params = mx.model.load_checkpoint(model_name, epoch_num)\n\n    mod = create_module(sym, data_shapes, label_shapes, label_names, gpus)\n\n    mod.set_params(\n        arg_params=arg_params,\n        aux_params=aux_params,\n        allow_missing=True\n    )\n\n    return mod", "output": "Returns a module loaded with the provided model.\n\n    Parameters\n    ----------\n    model_name: str\n        Prefix of the MXNet model name as stored on the local directory.\n\n    epoch_num : int\n        Epoch number of model we would like to load.\n\n    input_shape: tuple\n        The shape of the input data in the form of (batch_size, channels, height, width)\n\n    files: list of strings\n        List of URLs pertaining to files that need to be downloaded in order to use the model.\n\n    data_shapes: list of tuples.\n        List of tuples where each tuple is a pair of input variable name and its shape.\n\n    label_shapes: list of (str, tuple)\n        Typically is ``data_iter.provide_label``.\n\n    label_names: list of str\n        Name of the output labels in the MXNet symbolic graph.\n\n    gpus: str\n        Comma separated string of gpu ids on which inferences are executed. E.g. 3,5,6 would refer to GPUs 3, 5 and 6.\n        If empty, we use CPU.\n\n    Returns\n    -------\n    MXNet module", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_domain_workgroup(workgroup):\n    '''\n    \n    '''\n    if six.PY2:\n        workgroup = _to_unicode(workgroup)\n\n    # Initialize COM\n    with salt.utils.winapi.Com():\n        # Grab the first Win32_ComputerSystem object from wmi\n        conn = wmi.WMI()\n        comp = conn.Win32_ComputerSystem()[0]\n\n        # Now we can join the new workgroup\n        res = comp.JoinDomainOrWorkgroup(Name=workgroup.upper())\n\n    return True if not res[0] else False", "output": "Set the domain or workgroup the computer belongs to.\n\n    .. versionadded:: 2019.2.0\n\n    Returns:\n        bool: ``True`` if successful, otherwise ``False``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'minion-id' system.set_domain_workgroup LOCAL", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_member_named(self, name):\n        \"\"\"\n        \"\"\"\n\n        result = None\n        members = self.members\n        if len(name) > 5 and name[-5] == '#':\n            # The 5 length is checking to see if #0000 is in the string,\n            # as a#0000 has a length of 6, the minimum for a potential\n            # discriminator lookup.\n            potential_discriminator = name[-4:]\n\n            # do the actual lookup and return if found\n            # if it isn't found then we'll do a full name lookup below.\n            result = utils.get(members, name=name[:-5], discriminator=potential_discriminator)\n            if result is not None:\n                return result\n\n        def pred(m):\n            return m.nick == name or m.name == name\n\n        return utils.find(pred, members)", "output": "Returns the first member found that matches the name provided.\n\n        The name can have an optional discriminator argument, e.g. \"Jake#0001\"\n        or \"Jake\" will both do the lookup. However the former will give a more\n        precise result. Note that the discriminator must have all 4 digits\n        for this to work.\n\n        If a nickname is passed, then it is looked up via the nickname. Note\n        however, that a nickname + discriminator combo will not lookup the nickname\n        but rather the username + discriminator combo due to nickname + discriminator\n        not being unique.\n\n        If no member is found, ``None`` is returned.\n\n        Parameters\n        -----------\n        name: :class:`str`\n            The name of the member to lookup with an optional discriminator.\n\n        Returns\n        --------\n        :class:`Member`\n            The member in this guild with the associated name. If not found\n            then ``None`` is returned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uniformRDD(sc, size, numPartitions=None, seed=None):\n        \"\"\"\n        \n        \"\"\"\n        return callMLlibFunc(\"uniformRDD\", sc._jsc, size, numPartitions, seed)", "output": "Generates an RDD comprised of i.i.d. samples from the\n        uniform distribution U(0.0, 1.0).\n\n        To transform the distribution in the generated RDD from U(0.0, 1.0)\n        to U(a, b), use\n        C{RandomRDDs.uniformRDD(sc, n, p, seed)\\\n          .map(lambda v: a + (b - a) * v)}\n\n        :param sc: SparkContext used to create the RDD.\n        :param size: Size of the RDD.\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`).\n        :param seed: Random seed (default: a random long integer).\n        :return: RDD of float comprised of i.i.d. samples ~ `U(0.0, 1.0)`.\n\n        >>> x = RandomRDDs.uniformRDD(sc, 100).collect()\n        >>> len(x)\n        100\n        >>> max(x) <= 1.0 and min(x) >= 0.0\n        True\n        >>> RandomRDDs.uniformRDD(sc, 100, 4).getNumPartitions()\n        4\n        >>> parts = RandomRDDs.uniformRDD(sc, 100, seed=4).getNumPartitions()\n        >>> parts == sc.defaultParallelism\n        True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def construct_yaml_omap(self, node):\n        '''\n        \n        '''\n        sls_map = SLSMap()\n        if not isinstance(node, MappingNode):\n            raise ConstructorError(\n                None,\n                None,\n                'expected a mapping node, but found {0}'.format(node.id),\n                node.start_mark)\n\n        self.flatten_mapping(node)\n\n        for key_node, value_node in node.value:\n\n            # !reset instruction applies on document only.\n            # It tells to reset previous decoded value for this present key.\n            reset = key_node.tag == '!reset'\n\n            # even if !aggregate tag apply only to values and not keys\n            # it's a reason to act as a such nazi.\n            if key_node.tag == '!aggregate':\n                log.warning('!aggregate applies on values only, not on keys')\n                value_node.tag = key_node.tag\n                key_node.tag = self.resolve_sls_tag(key_node)[0]\n\n            key = self.construct_object(key_node, deep=False)\n            try:\n                hash(key)\n            except TypeError:\n                err = ('While constructing a mapping {0} found unacceptable '\n                       'key {1}').format(node.start_mark, key_node.start_mark)\n                raise ConstructorError(err)\n            value = self.construct_object(value_node, deep=False)\n            if key in sls_map and not reset:\n                value = merge_recursive(sls_map[key], value)\n            sls_map[key] = value\n        return sls_map", "output": "Build the SLSMap", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def low(data, queue=False, **kwargs):\n    '''\n    \n    '''\n    conflict = _check_queue(queue, kwargs)\n    if conflict is not None:\n        return conflict\n    try:\n        st_ = salt.state.State(__opts__, proxy=__proxy__)\n    except NameError:\n        st_ = salt.state.State(__opts__)\n    err = st_.verify_data(data)\n    if err:\n        __context__['retcode'] = salt.defaults.exitcodes.EX_STATE_COMPILER_ERROR\n        return err\n    ret = st_.call(data)\n    if isinstance(ret, list):\n        __context__['retcode'] = salt.defaults.exitcodes.EX_STATE_COMPILER_ERROR\n    if __utils__['state.check_result'](ret):\n        __context__['retcode'] = salt.defaults.exitcodes.EX_STATE_FAILURE\n    return ret", "output": "Execute a single low data call\n\n    This function is mostly intended for testing the state system and is not\n    likely to be needed in everyday usage.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.low '{\"state\": \"pkg\", \"fun\": \"installed\", \"name\": \"vi\"}'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_delete(self):\n        ''''''\n        now = time.time()\n        for project in list(itervalues(self.projects)):\n            if project.db_status != 'STOP':\n                continue\n            if now - project.updatetime < self.DELETE_TIME:\n                continue\n            if 'delete' not in self.projectdb.split_group(project.group):\n                continue\n\n            logger.warning(\"deleting project: %s!\", project.name)\n            del self.projects[project.name]\n            self.taskdb.drop(project.name)\n            self.projectdb.drop(project.name)\n            if self.resultdb:\n                self.resultdb.drop(project.name)\n            for each in self._cnt.values():\n                del each[project.name]", "output": "Check project delete", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _summarize_accessible_fields(field_descriptions, width=40,\n                                 section_title='Accessible fields'):\n    \"\"\"\n    \n    \"\"\"\n    key_str = \"{:<{}}: {}\"\n\n    items = []\n    items.append(section_title)\n    items.append(\"-\" * len(section_title))\n\n    for field_name, field_desc in field_descriptions.items():\n        items.append(key_str.format(field_name, width, field_desc))\n\n    return \"\\n\".join(items)", "output": "Create a summary string for the accessible fields in a model. Unlike\n    `_toolkit_repr_print`, this function does not look up the values of the\n    fields, it just formats the names and descriptions.\n\n    Parameters\n    ----------\n    field_descriptions : dict{str: str}\n        Name of each field and its description, in a dictionary. Keys and\n        values should be strings.\n\n    width : int, optional\n        Width of the names. This is usually determined and passed by the\n        calling `__repr__` method.\n\n    section_title : str, optional\n        Name of the accessible fields section in the summary string.\n\n    Returns\n    -------\n    out : str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_model():\n    \"\"\"\n    \"\"\"\n    if not os.path.exists(\"checkpoint\"):\n        os.mkdir(\"checkpoint\")\n    return mx.callback.do_checkpoint(\"checkpoint/checkpoint\", args.save_period)", "output": "Save cnn model\n    Returns\n    ----------\n    callback: A callback function that can be passed as epoch_end_callback to fit", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_script_timeout(self, time_to_wait):\n        \"\"\"\n        \n        \"\"\"\n        if self.w3c:\n            self.execute(Command.SET_TIMEOUTS, {\n                'script': int(float(time_to_wait) * 1000)})\n        else:\n            self.execute(Command.SET_SCRIPT_TIMEOUT, {\n                'ms': float(time_to_wait) * 1000})", "output": "Set the amount of time that the script should wait during an\n           execute_async_script call before throwing an error.\n\n        :Args:\n         - time_to_wait: The amount of time to wait (in seconds)\n\n        :Usage:\n            ::\n\n                driver.set_script_timeout(30)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unset_value(self, key):\n        # type: (str) -> None\n        \"\"\"\n        \"\"\"\n        self._ensure_have_load_only()\n\n        if key not in self._config[self.load_only]:\n            raise ConfigurationError(\"No such key - {}\".format(key))\n\n        fname, parser = self._get_parser_to_modify()\n\n        if parser is not None:\n            section, name = _disassemble_key(key)\n\n            # Remove the key in the parser\n            modified_something = False\n            if parser.has_section(section):\n                # Returns whether the option was removed or not\n                modified_something = parser.remove_option(section, name)\n\n            if modified_something:\n                # name removed from parser, section may now be empty\n                section_iter = iter(parser.items(section))\n                try:\n                    val = six.next(section_iter)\n                except StopIteration:\n                    val = None\n\n                if val is None:\n                    parser.remove_section(section)\n\n                self._mark_as_modified(fname, parser)\n            else:\n                raise ConfigurationError(\n                    \"Fatal Internal error [id=1]. Please report as a bug.\"\n                )\n\n        del self._config[self.load_only][key]", "output": "Unset a value in the configuration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def usergroup_list(**kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'usergroup.get'\n            params = {\"output\": \"extend\", }\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result']\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": "Retrieve all enabled user groups.\n\n    .. versionadded:: 2016.3.0\n\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    :return: Array with enabled user groups details, False on failure.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.usergroup_list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def leaveEvent(self, event):\n        \"\"\" \n        \"\"\"\n        super(CallTipWidget, self).leaveEvent(event)\n        self._leave_event_hide()", "output": "Reimplemented to start the hide timer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def order_percent(id_or_ins, percent, price=None, style=None):\n    \"\"\"\n    \n    \"\"\"\n    if percent < -1 or percent > 1:\n        raise RQInvalidArgument(_(u\"percent should between -1 and 1\"))\n\n    style = cal_style(price, style)\n    account = Environment.get_instance().portfolio.accounts[DEFAULT_ACCOUNT_TYPE.STOCK.name]\n    return order_value(id_or_ins, account.total_value * percent, style=style)", "output": "\u53d1\u9001\u4e00\u4e2a\u82b1\u8d39\u4ef7\u503c\u7b49\u4e8e\u76ee\u524d\u6295\u8d44\u7ec4\u5408\uff08\u5e02\u573a\u4ef7\u503c\u548c\u76ee\u524d\u73b0\u91d1\u7684\u603b\u548c\uff09\u4e00\u5b9a\u767e\u5206\u6bd4\u73b0\u91d1\u7684\u4e70/\u5356\u5355\uff0c\u6b63\u6570\u4ee3\u8868\u4e70\uff0c\u8d1f\u6570\u4ee3\u8868\u5356\u3002\u80a1\u7968\u7684\u80a1\u6570\u603b\u662f\u4f1a\u88ab\u8c03\u6574\u6210\u5bf9\u5e94\u7684\u4e00\u624b\u7684\u80a1\u7968\u6570\u7684\u500d\u6570\uff081\u624b\u662f100\u80a1\uff09\u3002\u767e\u5206\u6bd4\u662f\u4e00\u4e2a\u5c0f\u6570\uff0c\u5e76\u4e14\u5c0f\u4e8e\u6216\u7b49\u4e8e1\uff08<=100%\uff09\uff0c0.5\u8868\u793a\u7684\u662f50%.\u9700\u8981\u6ce8\u610f\uff0c\u5982\u679c\u8d44\u91d1\u4e0d\u8db3\uff0c\u8be5API\u5c06\u4e0d\u4f1a\u521b\u5efa\u53d1\u9001\u8ba2\u5355\u3002\n\n    \u9700\u8981\u6ce8\u610f\uff1a\n    \u53d1\u9001\u4e70\u5355\u65f6\uff0cpercent \u4ee3\u8868\u7684\u662f\u671f\u671b\u4e70\u5165\u80a1\u7968\u6d88\u8017\u7684\u91d1\u989d\uff08\u5305\u542b\u7a0e\u8d39\uff09\u5360\u6295\u8d44\u7ec4\u5408\u603b\u6743\u76ca\u7684\u6bd4\u4f8b\u3002\n    \u53d1\u9001\u5356\u5355\u65f6\uff0cpercent \u4ee3\u8868\u7684\u662f\u671f\u671b\u5356\u51fa\u7684\u80a1\u7968\u603b\u4ef7\u503c\u5360\u6295\u8d44\u7ec4\u5408\u603b\u6743\u76ca\u7684\u6bd4\u4f8b\u3002\n\n    :param id_or_ins: \u4e0b\u5355\u6807\u7684\u7269\n    :type id_or_ins: :class:`~Instrument` object | `str`\n\n    :param float percent: \u5360\u6709\u73b0\u6709\u7684\u6295\u8d44\u7ec4\u5408\u4ef7\u503c\u7684\u767e\u5206\u6bd4\u3002\u6b63\u6570\u8868\u793a\u4e70\u5165\uff0c\u8d1f\u6570\u8868\u793a\u5356\u51fa\u3002\n\n    :param float price: \u4e0b\u5355\u4ef7\u683c\uff0c\u9ed8\u8ba4\u4e3aNone\uff0c\u8868\u793a :class:`~MarketOrder`, \u6b64\u53c2\u6570\u4e3b\u8981\u7528\u4e8e\u7b80\u5316 `style` \u53c2\u6570\u3002\n\n    :param style: \u4e0b\u5355\u7c7b\u578b, \u9ed8\u8ba4\u662f\u5e02\u4ef7\u5355\u3002\u76ee\u524d\u652f\u6301\u7684\u8ba2\u5355\u7c7b\u578b\u6709 :class:`~LimitOrder` \u548c :class:`~MarketOrder`\n    :type style: `OrderStyle` object\n\n    :return: :class:`~Order` object | None\n\n    :example:\n\n    .. code-block:: python\n\n        #\u82b1\u8d39\u7b49\u4e8e\u73b0\u6709\u6295\u8d44\u7ec4\u540850%\u4ef7\u503c\u7684\u73b0\u91d1\u4e70\u5165\u5e73\u5b89\u94f6\u884c\u80a1\u7968\uff1a\n        order_percent('000001.XSHG', 0.5)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_customer_gateway(vpn_connection_type, ip_address, bgp_asn,\n                            customer_gateway_name=None, tags=None,\n                            region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    return _create_resource('customer_gateway', customer_gateway_name,\n                            type=vpn_connection_type,\n                            ip_address=ip_address, bgp_asn=bgp_asn,\n                            tags=tags, region=region, key=key,\n                            keyid=keyid, profile=profile)", "output": "Given a valid VPN connection type, a static IP address and a customer\n    gateway\u2019s Border Gateway Protocol (BGP) Autonomous System Number,\n    create a customer gateway.\n\n    Returns the customer gateway id if the customer gateway was created and\n    returns False if the customer gateway was not created.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_vpc.create_customer_gateway 'ipsec.1', '12.1.2.3', 65534", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_pret_embs(self, word_dims=None):\n        \"\"\"\n        \"\"\"\n        assert (self._pret_embeddings is not None), \"No pretrained file provided.\"\n        pret_embeddings = gluonnlp.embedding.create(self._pret_embeddings[0], source=self._pret_embeddings[1])\n        embs = [None] * len(self._id2word)\n        for idx, vec in enumerate(pret_embeddings.idx_to_vec):\n            embs[idx] = vec.asnumpy()\n        if word_dims is None:\n            word_dims = len(pret_embeddings.idx_to_vec[0])\n        for idx, emb in enumerate(embs):\n            if emb is None:\n                embs[idx] = np.zeros(word_dims)\n        pret_embs = np.array(embs, dtype=np.float32)\n        return pret_embs / np.std(pret_embs)", "output": "Read pre-trained embedding file\n\n        Parameters\n        ----------\n        word_dims : int or None\n            vector size. Use `None` for auto-infer\n        Returns\n        -------\n        numpy.ndarray\n            T x C numpy NDArray", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_sequence(self, xs, masks, init, is_left_to_right):\n        '''\n        \n        '''\n        states = []\n        last = init\n        if is_left_to_right:\n            for i, xs_i in enumerate(xs):\n                h = self.build(xs_i, last, masks[i])\n                states.append(h)\n                last = h\n        else:\n            for i in range(len(xs) - 1, -1, -1):\n                h = self.build(xs[i], last, masks[i])\n                states.insert(0, h)\n                last = h\n        return states", "output": "Build GRU sequence.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def scale_to(x, ratio, targ): \n    ''''''\n    return max(math.floor(x*ratio), targ)", "output": "Calculate dimension of an image during scaling with aspect ratio", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(name, **kwargs):\n    '''\n    \n\n    '''\n    ## Configure command\n    # NOTE: initialize the defaults\n    opts = {}\n\n    # NOTE: set extra config from kwargs\n    if kwargs.get('type', False):\n        opts['-t'] = kwargs.get('type')\n\n    ## Check if 'name' of 'type' exists\n    res = __salt__['cmd.run_all'](\n        __utils__['zfs.zfs_command'](\n            command='list',\n            opts=opts,\n            target=name,\n        ),\n        python_shell=False,\n        ignore_retcode=True,\n    )\n\n    return res['retcode'] == 0", "output": "Check if a ZFS filesystem or volume or snapshot exists.\n\n    name : string\n        name of dataset\n    type : string\n        also check if dataset is of a certain type, valid choices are:\n        filesystem, snapshot, volume, bookmark, or all.\n\n    .. versionadded:: 2015.5.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zfs.exists myzpool/mydataset\n        salt '*' zfs.exists myzpool/myvolume type=volume", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def simplex_projection(v, b=1):\n    \n    \"\"\"\n\n    v = np.asarray(v)\n    p = len(v)\n\n    # Sort v into u in descending order\n    v = (v > 0) * v\n    u = np.sort(v)[::-1]\n    sv = np.cumsum(u)\n\n    rho = np.where(u > (sv - b) / np.arange(1, p + 1))[0][-1]\n    theta = np.max([0, (sv[rho] - b) / (rho + 1)])\n    w = (v - theta)\n    w[w < 0] = 0\n    return w", "output": "r\"\"\"Projection vectors to the simplex domain\n\n    Implemented according to the paper: Efficient projections onto the\n    l1-ball for learning in high dimensions, John Duchi, et al. ICML 2008.\n    Implementation Time: 2011 June 17 by Bin@libin AT pmail.ntu.edu.sg\n    Optimization Problem: min_{w}\\| w - v \\|_{2}^{2}\n    s.t. sum_{i=1}^{m}=z, w_{i}\\geq 0\n\n    Input: A vector v \\in R^{m}, and a scalar z > 0 (default=1)\n    Output: Projection vector w\n\n    :Example:\n    >>> proj = simplex_projection([.4 ,.3, -.4, .5])\n    >>> proj  # doctest: +NORMALIZE_WHITESPACE\n    array([ 0.33333333, 0.23333333, 0. , 0.43333333])\n    >>> print(proj.sum())\n    1.0\n\n    Original matlab implementation: John Duchi (jduchi@cs.berkeley.edu)\n    Python-port: Copyright 2013 by Thomas Wiecki (thomas.wiecki@gmail.com).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_data_with_shared_vocab(self, data_dir, tmp_dir, task_id=-1):\n    \"\"\"\"\"\"\n    global_vocab_filename = os.path.join(data_dir, self.vocab_filename)\n    if not tf.gfile.Exists(global_vocab_filename):\n      raise ValueError(\n          'Global vocabulary file: %s does not exist, '\n          'please create one using build_vocab.py' % global_vocab_filename)\n    # Before generating data, we copy the global vocabulary file to the children\n    # locations. Although this is not the most disk efficient strategy, it\n    # imposes the fewest changes to the text-to-text API.\n    for p in self.problems:\n      local_vocab_filename = os.path.join(data_dir, p.vocab_filename)\n      if not tf.gfile.Exists(local_vocab_filename):\n        tf.gfile.Copy(global_vocab_filename, local_vocab_filename)\n      p.generate_data(data_dir, tmp_dir, task_id)", "output": "Generates TF-Records for problems using a global vocabulary file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def scroll(self, scroll_id=None, body=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if scroll_id in SKIP_IN_PATH and body in SKIP_IN_PATH:\n            raise ValueError(\"You need to supply scroll_id or body.\")\n        elif scroll_id and not body:\n            body = {\"scroll_id\": scroll_id}\n        elif scroll_id:\n            params[\"scroll_id\"] = scroll_id\n\n        return self.transport.perform_request(\n            \"GET\", \"/_search/scroll\", params=params, body=body\n        )", "output": "Scroll a search request created by specifying the scroll parameter.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html>`_\n\n        :arg scroll_id: The scroll ID\n        :arg body: The scroll ID if not passed by URL or query parameter.\n        :arg scroll: Specify how long a consistent view of the index should be\n            maintained for scrolled search\n        :arg rest_total_hits_as_int: This parameter is used to restore the total hits as a number\n            in the response. This param is added version 6.x to handle mixed cluster queries where nodes\n            are in multiple versions (7.0 and 6.latest)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_projects(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudException(\n            'The avail_projects function must be called with -f or --function.'\n        )\n\n    vm_ = get_configured_provider()\n    manager = packet.Manager(auth_token=vm_['token'])\n\n    ret = {}\n\n    for project in manager.list_projects():\n        ret[project.name] = project.__dict__\n\n    return ret", "output": "Return available Packet projects.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f avail_projects packet-provider", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pad_to_same_length(x, y, final_length_divisible_by=1, axis=1):\n  \"\"\"\"\"\"\n  if axis not in [1, 2]:\n    raise ValueError(\"Only axis=1 and axis=2 supported for now.\")\n  with tf.name_scope(\"pad_to_same_length\", values=[x, y]):\n    x_length = shape_list(x)[axis]\n    y_length = shape_list(y)[axis]\n    if (isinstance(x_length, int) and isinstance(y_length, int) and\n        x_length == y_length and final_length_divisible_by == 1):\n      return x, y\n    max_length = tf.maximum(x_length, y_length)\n    if final_length_divisible_by > 1:\n      # Find the nearest larger-or-equal integer divisible by given number.\n      max_length += final_length_divisible_by - 1\n      max_length //= final_length_divisible_by\n      max_length *= final_length_divisible_by\n    length_diff1 = max_length - x_length\n    length_diff2 = max_length - y_length\n\n    def padding_list(length_diff, arg):\n      if axis == 1:\n        return [[[0, 0], [0, length_diff]],\n                tf.zeros([tf.rank(arg) - 2, 2], dtype=tf.int32)]\n      return [[[0, 0], [0, 0], [0, length_diff]],\n              tf.zeros([tf.rank(arg) - 3, 2], dtype=tf.int32)]\n\n    paddings1 = tf.concat(padding_list(length_diff1, x), axis=0)\n    paddings2 = tf.concat(padding_list(length_diff2, y), axis=0)\n    res_x = tf.pad(x, paddings1)\n    res_y = tf.pad(y, paddings2)\n    # Static shapes are the same except for axis=1.\n    x_shape = x.shape.as_list()\n    x_shape[axis] = None\n    res_x.set_shape(x_shape)\n    y_shape = y.shape.as_list()\n    y_shape[axis] = None\n    res_y.set_shape(y_shape)\n    return res_x, res_y", "output": "Pad tensors x and y on axis 1 so that they have the same length.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _post_message(user,\n                  device,\n                  message,\n                  title,\n                  priority,\n                  expire,\n                  retry,\n                  sound,\n                  api_version=1,\n                  token=None):\n    '''\n    \n    '''\n\n    user_validate = salt.utils.pushover.validate_user(user, device, token)\n    if not user_validate['result']:\n        return user_validate\n\n    parameters = dict()\n    parameters['user'] = user\n    parameters['device'] = device\n    parameters['token'] = token\n    parameters['title'] = title\n    parameters['priority'] = priority\n    parameters['expire'] = expire\n    parameters['retry'] = retry\n    parameters['message'] = message\n\n    if sound:\n        sound_validate = salt.utils.pushover.validate_sound(sound, token)\n        if sound_validate['res']:\n            parameters['sound'] = sound\n\n    result = salt.utils.pushover.query(function='message',\n                                       method='POST',\n                                       header_dict={'Content-Type': 'application/x-www-form-urlencoded'},\n                                       data=_urlencode(parameters),\n                                       opts=__opts__)\n\n    return result", "output": "Send a message to a Pushover user or group.\n    :param user:        The user or group to send to, must be key of user or group not email address.\n    :param message:     The message to send to the PushOver user or group.\n    :param title:       Specify who the message is from.\n    :param priority     The priority of the message, defaults to 0.\n    :param api_version: The PushOver API version, if not specified in the configuration.\n    :param notify:      Whether to notify the room, default: False.\n    :param token:       The PushOver token, if not specified in the configuration.\n    :return:            Boolean if message was sent successfully.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_info(self, info):\r\n        \"\"\"\"\"\"\r\n        if info['docstring']:\r\n            if info['filename']:\r\n                filename = os.path.basename(info['filename'])\r\n                filename = os.path.splitext(filename)[0]\r\n            else:\r\n                filename = '<module>'\r\n            resp = dict(docstring=info['docstring'],\r\n                        name=filename,\r\n                        note='',\r\n                        argspec='',\r\n                        calltip=None)\r\n            return resp\r\n        else:\r\n            return default_info_response()", "output": "Get a formatted calltip and docstring from Fallback", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_chunk(self, size=None):\n        \"\"\"\n        \n        \"\"\"\n        if size is None:\n            size = self._chunksize\n        return self.read(nrows=size)", "output": "Reads lines from Xport file and returns as dataframe\n\n        Parameters\n        ----------\n        size : int, defaults to None\n            Number of lines to read.  If None, reads whole file.\n\n        Returns\n        -------\n        DataFrame", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compile_expression(self, source, undefined_to_none=True):\n        \"\"\"\n        \"\"\"\n        parser = Parser(self, source, state='variable')\n        exc_info = None\n        try:\n            expr = parser.parse_expression()\n            if not parser.stream.eos:\n                raise TemplateSyntaxError('chunk after expression',\n                                          parser.stream.current.lineno,\n                                          None, None)\n            expr.set_environment(self)\n        except TemplateSyntaxError:\n            exc_info = sys.exc_info()\n        if exc_info is not None:\n            self.handle_exception(exc_info, source_hint=source)\n        body = [nodes.Assign(nodes.Name('result', 'store'), expr, lineno=1)]\n        template = self.from_string(nodes.Template(body, lineno=1))\n        return TemplateExpression(template, undefined_to_none)", "output": "A handy helper method that returns a callable that accepts keyword\n        arguments that appear as variables in the expression.  If called it\n        returns the result of the expression.\n\n        This is useful if applications want to use the same rules as Jinja\n        in template \"configuration files\" or similar situations.\n\n        Example usage:\n\n        >>> env = Environment()\n        >>> expr = env.compile_expression('foo == 42')\n        >>> expr(foo=23)\n        False\n        >>> expr(foo=42)\n        True\n\n        Per default the return value is converted to `None` if the\n        expression returns an undefined value.  This can be changed\n        by setting `undefined_to_none` to `False`.\n\n        >>> env.compile_expression('var')() is None\n        True\n        >>> env.compile_expression('var', undefined_to_none=False)()\n        Undefined\n\n        .. versionadded:: 2.1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_circuit_termination(circuit, interface, device, speed, xconnect_id=None, term_side='A'):\n    '''\n    \n    '''\n\n    nb_device = get_('dcim', 'devices', name=device)\n    nb_interface = get_('dcim', 'interfaces', device_id=nb_device['id'], name=interface)\n    nb_circuit = get_('circuits', 'circuits', cid=circuit)\n    if nb_circuit and nb_device:\n        nb_termination = get_('circuits',\n                             'circuit-terminations',\n                             q=nb_circuit['cid'])\n        if nb_termination:\n            return False\n        payload = {\n            'circuit': nb_circuit['id'],\n            'interface': nb_interface['id'],\n            'site': nb_device['site']['id'],\n            'port_speed': speed,\n            'term_side': term_side\n        }\n        if xconnect_id:\n            payload['xconnect_id'] = xconnect_id\n        circuit_termination = _add('circuits', 'circuit-terminations', payload)\n        if circuit_termination:\n            return {'circuits': {'circuit-terminations': {circuit_termination['id']: payload}}}\n        else:\n            return circuit_termination", "output": ".. versionadded:: 2019.2.0\n\n    Terminate a circuit on an interface\n\n    circuit\n        The name of the circuit\n    interface\n        The name of the interface to terminate on\n    device\n        The name of the device the interface belongs to\n    speed\n        The speed of the circuit, in Kbps\n    xconnect_id\n        The cross-connect identifier\n    term_side\n        The side of the circuit termination\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion netbox.create_circuit_termination NEW_CIRCUIT_01 xe-0/0/1 myminion 10000 xconnect_id=XCON01", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def restorecon(path, recursive=False):\n    '''\n    \n    '''\n    if recursive:\n        cmd = ['restorecon', '-FR', path]\n    else:\n        cmd = ['restorecon', '-F', path]\n    return not __salt__['cmd.retcode'](cmd, python_shell=False)", "output": "Reset the SELinux context on a given path\n\n    CLI Example:\n\n    .. code-block:: bash\n\n         salt '*' file.restorecon /home/user/.ssh/authorized_keys", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _DoubleDecoder():\n  \"\"\"\n  \"\"\"\n\n  local_unpack = struct.unpack\n\n  def InnerDecode(buffer, pos):\n    # We expect a 64-bit value in little-endian byte order.  Bit 1 is the sign\n    # bit, bits 2-12 represent the exponent, and bits 13-64 are the significand.\n    new_pos = pos + 8\n    double_bytes = buffer[pos:new_pos]\n\n    # If this value has all its exponent bits set and at least one significand\n    # bit set, it's not a number.  In Python 2.4, struct.unpack will treat it\n    # as inf or -inf.  To avoid that, we treat it specially.\n    if ((double_bytes[7:8] in b'\\x7F\\xFF')\n        and (double_bytes[6:7] >= b'\\xF0')\n        and (double_bytes[0:7] != b'\\x00\\x00\\x00\\x00\\x00\\x00\\xF0')):\n      return (_NAN, new_pos)\n\n    # Note that we expect someone up-stack to catch struct.error and convert\n    # it to _DecodeError -- this way we don't have to set up exception-\n    # handling blocks every time we parse one value.\n    result = local_unpack('<d', double_bytes)[0]\n    return (result, new_pos)\n  return _SimpleDecoder(wire_format.WIRETYPE_FIXED64, InnerDecode)", "output": "Returns a decoder for a double field.\n\n  This code works around a bug in struct.unpack for not-a-number.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_raw_data(assets,\n                  data_query_cutoff_times,\n                  expr,\n                  odo_kwargs,\n                  checkpoints=None):\n    \"\"\"\n    \n    \"\"\"\n    lower_dt, upper_dt = data_query_cutoff_times[[0, -1]]\n    raw = ffill_query_in_range(\n        expr,\n        lower_dt,\n        upper_dt,\n        checkpoints=checkpoints,\n        odo_kwargs=odo_kwargs,\n    )\n    sids = raw[SID_FIELD_NAME]\n    raw.drop(\n        sids[~sids.isin(assets)].index,\n        inplace=True\n    )\n    return raw", "output": "Given an expression representing data to load, perform normalization and\n    forward-filling and return the data, materialized. Only accepts data with a\n    `sid` field.\n\n    Parameters\n    ----------\n    assets : pd.int64index\n        the assets to load data for.\n    data_query_cutoff_times : pd.DatetimeIndex\n        The datetime when data should no longer be considered available for\n        a session.\n    expr : expr\n        the expression representing the data to load.\n    odo_kwargs : dict\n        extra keyword arguments to pass to odo when executing the expression.\n    checkpoints : expr, optional\n        the expression representing the checkpointed data for `expr`.\n\n    Returns\n    -------\n    raw : pd.dataframe\n        The result of computing expr and materializing the result as a\n        dataframe.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _iter_config_files(self):\n        # type: () -> Iterable[Tuple[Kind, List[str]]]\n        \"\"\"\n        \"\"\"\n        # SMELL: Move the conditions out of this function\n\n        # environment variables have the lowest priority\n        config_file = os.environ.get('PIP_CONFIG_FILE', None)\n        if config_file is not None:\n            yield kinds.ENV, [config_file]\n        else:\n            yield kinds.ENV, []\n\n        # at the base we have any global configuration\n        yield kinds.GLOBAL, list(site_config_files)\n\n        # per-user configuration next\n        should_load_user_config = not self.isolated and not (\n            config_file and os.path.exists(config_file)\n        )\n        if should_load_user_config:\n            # The legacy config file is overridden by the new config file\n            yield kinds.USER, [legacy_config_file, new_config_file]\n\n        # finally virtualenv configuration first trumping others\n        if running_under_virtualenv():\n            yield kinds.VENV, [venv_config_file]", "output": "Yields variant and configuration files associated with it.\n\n        This should be treated like items of a dictionary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_layer_timing_signal_learned_1d(channels, layer, num_layers):\n  \"\"\"\n  \"\"\"\n  shape = [num_layers, 1, 1, channels]\n  layer_embedding = (\n      tf.get_variable(\n          \"layer_embedding\",\n          shape,\n          initializer=tf.random_normal_initializer(0, channels**-0.5)) *\n      (channels**0.5))\n  return layer_embedding[layer, :, :, :]", "output": "get n-dimensional embedding as the layer (vertical) timing signal.\n\n  Adds embeddings to represent the position of the layer in the tower.\n\n  Args:\n    channels: dimension of the timing signal\n    layer: layer num\n    num_layers: total number of layers\n\n  Returns:\n    a Tensor of timing signals [1, 1, channels].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def subscriptions_list_locations(subscription_id=None, **kwargs):\n    '''\n    \n\n    '''\n    result = {}\n\n    if not subscription_id:\n        subscription_id = kwargs.get('subscription_id')\n    elif not kwargs.get('subscription_id'):\n        kwargs['subscription_id'] = subscription_id\n\n    subconn = __utils__['azurearm.get_client']('subscription', **kwargs)\n    try:\n        locations = __utils__['azurearm.paged_object_to_list'](\n            subconn.subscriptions.list_locations(\n                subscription_id=kwargs['subscription_id']\n            )\n        )\n\n        for loc in locations:\n            result[loc['name']] = loc\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('resource', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    List all locations for a subscription.\n\n    :param subscription_id: The ID of the subscription to query.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_resource.subscriptions_list_locations XXXXXXXX", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def slugify(value):\n    ''''\n    \n    '''\n    value = re.sub(r'[^\\w\\s-]', '', value).strip().lower()\n    return re.sub(r'[-\\s]+', '-', value)", "output": "Slugify given value.\n    Credit to Djangoproject https://docs.djangoproject.com/en/2.0/_modules/django/utils/text/#slugify", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathNextAncestorOrSelf(self, ctxt):\n        \"\"\" \"\"\"\n        if ctxt is None: ctxt__o = None\n        else: ctxt__o = ctxt._o\n        ret = libxml2mod.xmlXPathNextAncestorOrSelf(ctxt__o, self._o)\n        if ret is None:raise xpathError('xmlXPathNextAncestorOrSelf() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Traversal function for the \"ancestor-or-self\" direction he\n          ancestor-or-self axis contains the context node and\n          ancestors of the context node in reverse document order;\n          thus the context node is the first node on the axis, and\n          the context node's parent the second; parent here is\n           defined the same as with the parent axis.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_metrics(metrics, all_branches=False, all_tags=False):\n    \"\"\"\n    \n    \"\"\"\n    for branch, val in metrics.items():\n        if all_branches or all_tags:\n            logger.info(\"{branch}:\".format(branch=branch))\n\n        for fname, metric in val.items():\n            lines = metric if type(metric) is list else metric.splitlines()\n\n            if len(lines) > 1:\n                logger.info(\"\\t{fname}:\".format(fname=fname))\n\n                for line in lines:\n                    logger.info(\"\\t\\t{content}\".format(content=line))\n\n            else:\n                logger.info(\"\\t{}: {}\".format(fname, metric))", "output": "Args:\n        metrics (list): Where each element is either a `list`\n            if an xpath was specified, otherwise a `str`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_spyder_breakpoints(self, force=False):\n        \"\"\"\"\"\"\n        if self._reading or force:\n            breakpoints_dict = CONF.get('run', 'breakpoints', {})\n\n            # We need to enclose pickled values in a list to be able to\n            # send them to the kernel in Python 2\n            serialiazed_breakpoints = [pickle.dumps(breakpoints_dict,\n                                                    protocol=PICKLE_PROTOCOL)]\n            breakpoints = to_text_string(serialiazed_breakpoints)\n\n            cmd = u\"!get_ipython().kernel._set_spyder_breakpoints({})\"\n            self.kernel_client.input(cmd.format(breakpoints))", "output": "Set Spyder breakpoints into a debugging session", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def predict(self, dataset, output_type='class'):\n        \"\"\"\n        \n\n        \"\"\"\n        m = self.__proxy__['classifier']\n        target = self.__proxy__['target']\n        f = _BOW_FEATURE_EXTRACTOR\n        return m.predict(f(dataset, target), output_type=output_type)", "output": "Return predictions for ``dataset``, using the trained model.\n\n        Parameters\n        ----------\n        dataset : SFrame\n            dataset of new observations. Must include columns with the same\n            names as the features used for model training, but does not require\n            a target column. Additional columns are ignored.\n\n        output_type : {'class', 'probability_vector'}, optional\n            Form of the predictions which are one of:\n\n            - 'probability_vector': Prediction probability associated with each\n              class as a vector. The probability of the first class (sorted\n              alphanumerically by name of the class in the training set) is in\n              position 0 of the vector, the second in position 1 and so on.\n            - 'class': Class prediction. For multi-class classification, this\n              returns the class with maximum probability.\n\n        Returns\n        -------\n        out : SArray\n            An SArray with model predictions.\n\n        See Also\n        ----------\n        create, evaluate, classify\n\n\n        Examples\n        --------\n        >>> import turicreate as tc\n        >>> dataset = tc.SFrame({'rating': [1, 5], 'text': ['hate it', 'love it']})\n        >>> m = tc.text_classifier.create(dataset, 'rating', features=['text'])\n        >>> m.predict(dataset)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extraction_to_conll(ex: Extraction) -> List[str]:\n    \"\"\"\n    \n    \"\"\"\n    ex = split_predicate(ex)\n    toks = ex.sent.split(' ')\n    ret = ['*'] * len(toks)\n    args = [ex.arg1] + ex.args2\n    rels_and_args = [(\"ARG{}\".format(arg_ind), arg)\n                     for arg_ind, arg in enumerate(args)] + \\\n                         [(rel_part.elem_type, rel_part)\n                          for rel_part\n                          in ex.rel]\n\n    for rel, arg in rels_and_args:\n        # Add brackets\n        cur_start_ind = char_to_word_index(arg.span[0],\n                                           ex.sent)\n        cur_end_ind = char_to_word_index(arg.span[1],\n                                         ex.sent)\n        ret[cur_start_ind] = \"({}{}\".format(rel, ret[cur_start_ind])\n        ret[cur_end_ind] += ')'\n    return ret", "output": "Return a conll representation of a given input Extraction.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_connection(self, alias):\n        \"\"\"\n        \n        \"\"\"\n        errors = 0\n        for d in (self._conns, self._kwargs):\n            try:\n                del d[alias]\n            except KeyError:\n                errors += 1\n\n        if errors == 2:\n            raise KeyError('There is no connection with alias %r.' % alias)", "output": "Remove connection from the registry. Raises ``KeyError`` if connection\n        wasn't found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_current_context(self, device_id):\n        \"\"\"\n        \"\"\"\n        if device_id not in self._all_index_update_counts:\n            self._all_index_update_counts[device_id] = {}\n        self._index_update_count = self._all_index_update_counts[device_id]", "output": "Sets the number of the currently handled device.\n\n        Parameters\n        ----------\n        device_id : int\n            The number of current device.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def CreateString(self, s, encoding='utf-8', errors='strict'):\n        \"\"\"\"\"\"\n\n        self.assertNotNested()\n        ## @cond FLATBUFFERS_INTERNAL\n        self.nested = True\n        ## @endcond\n\n        if isinstance(s, compat.string_types):\n            x = s.encode(encoding, errors)\n        elif isinstance(s, compat.binary_types):\n            x = s\n        else:\n            raise TypeError(\"non-string passed to CreateString\")\n\n        self.Prep(N.UOffsetTFlags.bytewidth, (len(x)+1)*N.Uint8Flags.bytewidth)\n        self.Place(0, N.Uint8Flags)\n\n        l = UOffsetTFlags.py_type(len(s))\n        ## @cond FLATBUFFERS_INTERNAL\n        self.head = UOffsetTFlags.py_type(self.Head() - l)\n        ## @endcond\n        self.Bytes[self.Head():self.Head()+l] = x\n\n        return self.EndVector(len(x))", "output": "CreateString writes a null-terminated byte string as a vector.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def del_cookie(self, name: str, *,\n                   domain: Optional[str]=None,\n                   path: str='/') -> None:\n        \"\"\"\n        \"\"\"\n        # TODO: do we need domain/path here?\n        self._cookies.pop(name, None)\n        self.set_cookie(name, '', max_age=0,\n                        expires=\"Thu, 01 Jan 1970 00:00:00 GMT\",\n                        domain=domain, path=path)", "output": "Delete cookie.\n\n        Creates new empty expired cookie.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _kernel_versions_debian():\n    '''\n    \n    '''\n    kernel_get_selections = __salt__['cmd.run']('dpkg --get-selections linux-image-*')\n    kernels = []\n    kernel_versions = []\n    for line in kernel_get_selections.splitlines():\n        kernels.append(line)\n\n    try:\n        kernel = kernels[-2]\n    except IndexError:\n        kernel = kernels[0]\n\n    kernel = kernel.rstrip('\\t\\tinstall')\n\n    kernel_get_version = __salt__['cmd.run']('apt-cache policy ' + kernel)\n\n    for line in kernel_get_version.splitlines():\n        if line.startswith('  Installed: '):\n            kernel_v = line.strip('  Installed: ')\n            kernel_versions.append(kernel_v)\n            break\n\n    if __grains__['os'] == 'Ubuntu':\n        kernel_v = kernel_versions[0].rsplit('.', 1)\n        kernel_ubuntu_generic = kernel_v[0] + '-generic #' + kernel_v[1]\n        kernel_ubuntu_lowlatency = kernel_v[0] + '-lowlatency #' + kernel_v[1]\n        kernel_versions.extend([kernel_ubuntu_generic, kernel_ubuntu_lowlatency])\n\n    return kernel_versions", "output": "Last installed kernel name, for Debian based systems.\n\n    Returns:\n            List with possible names of last installed kernel\n            as they are probably interpreted in output of `uname -a` command.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_network_settings():\n    '''\n    \n    '''\n    skip_etc_default_networking = (\n        __grains__['osfullname'] == 'Ubuntu' and\n        int(__grains__['osrelease'].split('.')[0]) >= 12)\n\n    if skip_etc_default_networking:\n        settings = {}\n        if __salt__['service.available']('networking'):\n            if __salt__['service.status']('networking'):\n                settings['networking'] = \"yes\"\n            else:\n                settings['networking'] = \"no\"\n        else:\n            settings['networking'] = \"no\"\n\n        hostname = _parse_hostname()\n        domainname = _parse_domainname()\n        searchdomain = _parse_searchdomain()\n\n        settings['hostname'] = hostname\n        settings['domainname'] = domainname\n        settings['searchdomain'] = searchdomain\n\n    else:\n        settings = _parse_current_network_settings()\n\n    try:\n        template = JINJA.get_template('display-network.jinja')\n    except jinja2.exceptions.TemplateNotFound:\n        log.error('Could not load template display-network.jinja')\n        return ''\n\n    network = template.render(settings)\n    return _read_temp(network)", "output": "Return the contents of the global network script.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ip.get_network_settings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uniform_unit_scaling(tensor: torch.Tensor, nonlinearity: str = \"linear\"):\n    \"\"\"\n    \n    \"\"\"\n    size = 1.\n    # Estimate the input size. This won't work perfectly,\n    # but it covers almost all use cases where this initialiser\n    # would be expected to be useful, i.e in large linear and\n    # convolutional layers, as the last dimension will almost\n    # always be the output size.\n    for dimension in list(tensor.size())[:-1]:\n        size *= dimension\n\n    activation_scaling = torch.nn.init.calculate_gain(nonlinearity, tensor)\n    max_value = math.sqrt(3 / size) * activation_scaling\n\n    return tensor.data.uniform_(-max_value, max_value)", "output": "An initaliser which preserves output variance for approximately gaussian\n    distributed inputs. This boils down to initialising layers using a uniform\n    distribution in the range ``(-sqrt(3/dim[0]) * scale, sqrt(3 / dim[0]) * scale)``, where\n    ``dim[0]`` is equal to the input dimension of the parameter and the ``scale``\n    is a constant scaling factor which depends on the non-linearity used.\n\n    See `Random Walk Initialisation for Training Very Deep Feedforward Networks\n    <https://www.semanticscholar.org/paper/Random-Walk-Initialization-for-Training-Very-Deep-Sussillo-Abbott/be9728a0728b6acf7a485225b1e41592176eda0b>`_\n    for more information.\n\n    Parameters\n    ----------\n    tensor : ``torch.Tensor``, required.\n        The tensor to initialise.\n    nonlinearity : ``str``, optional (default = \"linear\")\n        The non-linearity which is performed after the projection that this\n        tensor is involved in. This must be the name of a function contained\n        in the ``torch.nn.functional`` package.\n\n    Returns\n    -------\n    The initialised tensor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read_csv_with_offset_pyarrow_on_ray(\n    fname, num_splits, start, end, kwargs, header\n):  # pragma: no cover\n    \"\"\"\n    \"\"\"\n    bio = open(fname, \"rb\")\n    # The header line for the CSV file\n    first_line = bio.readline()\n    bio.seek(start)\n    to_read = header + first_line + bio.read(end - start)\n    bio.close()\n    table = csv.read_csv(\n        BytesIO(to_read), parse_options=csv.ParseOptions(header_rows=1)\n    )\n    chunksize = get_default_chunksize(table.num_columns, num_splits)\n    chunks = [\n        pa.Table.from_arrays(table.columns[chunksize * i : chunksize * (i + 1)])\n        for i in range(num_splits)\n    ]\n    return chunks + [table.num_rows]", "output": "Use a Ray task to read a chunk of a CSV into a pyarrow Table.\n     Note: Ray functions are not detected by codecov (thus pragma: no cover)\n     Args:\n        fname: The filename of the file to open.\n        num_splits: The number of splits (partitions) to separate the DataFrame into.\n        start: The start byte offset.\n        end: The end byte offset.\n        kwargs: The kwargs for the pyarrow `read_csv` function.\n        header: The header of the file.\n     Returns:\n         A list containing the split pyarrow Tables and the the number of\n         rows of the tables as the last element. This is used to determine\n         the total length of the DataFrame to build a default Index.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_state(self):\n        \"\"\"\n        \"\"\"\n        if self._transaction_id is None:\n            raise ValueError(\"Transaction is not begun\")\n\n        if self.committed is not None:\n            raise ValueError(\"Transaction is already committed\")\n\n        if self._rolled_back:\n            raise ValueError(\"Transaction is already rolled back\")", "output": "Helper for :meth:`commit` et al.\n\n        :raises: :exc:`ValueError` if the object's state is invalid for making\n                 API requests.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_max(self, max_value, field, value):\n        \"\"\"  \"\"\"\n        try:\n            if value > max_value:\n                self._error(field, errors.MAX_VALUE)\n        except TypeError:\n            pass", "output": "{'nullable': False }", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hybrid_forward(self, F, row, col, counts):\n        \"\"\"\n\n        \"\"\"\n\n        emb_in = self.source_embedding(row)\n        emb_out = self.context_embedding(col)\n\n        if self._dropout:\n            emb_in = F.Dropout(emb_in, p=self._dropout)\n            emb_out = F.Dropout(emb_out, p=self._dropout)\n\n        bias_in = self.source_bias(row).squeeze()\n        bias_out = self.context_bias(col).squeeze()\n        dot = F.batch_dot(emb_in.expand_dims(1),\n                          emb_out.expand_dims(2)).squeeze()\n        tmp = dot + bias_in + bias_out - F.log(counts).squeeze()\n        weight = F.clip(((counts / self._x_max)**self._alpha), a_min=0,\n                        a_max=1).squeeze()\n        loss = weight * F.square(tmp)\n        return loss", "output": "Compute embedding of words in batch.\n\n        Parameters\n        ----------\n        row : mxnet.nd.NDArray or mxnet.sym.Symbol\n            Array of token indices for source words. Shape (batch_size, ).\n        row : mxnet.nd.NDArray or mxnet.sym.Symbol\n            Array of token indices for context words. Shape (batch_size, ).\n        counts : mxnet.nd.NDArray or mxnet.sym.Symbol\n            Their co-occurrence counts. Shape (batch_size, ).\n\n        Returns\n        -------\n        mxnet.nd.NDArray or mxnet.sym.Symbol\n            Loss. Shape (batch_size, ).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _try_dump_cnt(self):\n        ''''''\n        now = time.time()\n        if now - self._last_dump_cnt > 60:\n            self._last_dump_cnt = now\n            self._dump_cnt()\n            self._print_counter_log()", "output": "Dump counters every 60 seconds", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __duplicate_line_or_selection(self, after_current_line=True):\r\n        \"\"\"\"\"\"\r\n        cursor = self.textCursor()\r\n        cursor.beginEditBlock()\r\n        start_pos, end_pos = self.__save_selection()\r\n        if to_text_string(cursor.selectedText()):\r\n            cursor.setPosition(end_pos)\r\n            # Check if end_pos is at the start of a block: if so, starting\r\n            # changes from the previous block\r\n            cursor.movePosition(QTextCursor.StartOfBlock,\r\n                                QTextCursor.KeepAnchor)\r\n            if not to_text_string(cursor.selectedText()):\r\n                cursor.movePosition(QTextCursor.PreviousBlock)\r\n                end_pos = cursor.position()\r\n\r\n        cursor.setPosition(start_pos)\r\n        cursor.movePosition(QTextCursor.StartOfBlock)\r\n        while cursor.position() <= end_pos:\r\n            cursor.movePosition(QTextCursor.EndOfBlock, QTextCursor.KeepAnchor)\r\n            if cursor.atEnd():\r\n                cursor_temp = QTextCursor(cursor)\r\n                cursor_temp.clearSelection()\r\n                cursor_temp.insertText(self.get_line_separator())\r\n                break\r\n            cursor.movePosition(QTextCursor.NextBlock, QTextCursor.KeepAnchor)\r\n        text = cursor.selectedText()\r\n        cursor.clearSelection()\r\n\r\n        if not after_current_line:\r\n            # Moving cursor before current line/selected text\r\n            cursor.setPosition(start_pos)\r\n            cursor.movePosition(QTextCursor.StartOfBlock)\r\n            start_pos += len(text)\r\n            end_pos += len(text)\r\n\r\n        cursor.insertText(text)\r\n        cursor.endEditBlock()\r\n        self.setTextCursor(cursor)\r\n        self.__restore_selection(start_pos, end_pos)", "output": "Duplicate current line or selected text", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_stream_handler(self, request):\n        \"\"\" \n        \"\"\"\n        try:\n            handler = self.get(request)[0]\n        except (NotFound, MethodNotSupported):\n            return False\n        if hasattr(handler, \"view_class\") and hasattr(\n            handler.view_class, request.method.lower()\n        ):\n            handler = getattr(handler.view_class, request.method.lower())\n        return hasattr(handler, \"is_stream\")", "output": "Handler for request is stream or not.\n        :param request: Request object\n        :return: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fileserver_update(fileserver):\n    '''\n    \n    '''\n    try:\n        if not fileserver.servers:\n            log.error(\n                'No fileservers loaded, the master will not be able to '\n                'serve files to minions'\n            )\n            raise salt.exceptions.SaltMasterError('No fileserver backends available')\n        fileserver.update()\n    except Exception as exc:\n        log.error(\n            'Exception %s occurred in file server update', exc,\n            exc_info_on_loglevel=logging.DEBUG\n        )", "output": "Update the fileserver backends, requires that a salt.fileserver.Fileserver\n    object be passed in", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mousePressEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        if sys.platform.startswith('linux') and event.button() == Qt.MidButton:\r\n            self.calltip_widget.hide()\r\n            self.setFocus()\r\n            event = QMouseEvent(QEvent.MouseButtonPress, event.pos(),\r\n                                Qt.LeftButton, Qt.LeftButton, Qt.NoModifier)\r\n            QPlainTextEdit.mousePressEvent(self, event)\r\n            QPlainTextEdit.mouseReleaseEvent(self, event)\r\n            # Send selection text to clipboard to be able to use\r\n            # the paste method and avoid the strange Issue 1445\r\n            # NOTE: This issue seems a focusing problem but it\r\n            # seems really hard to track\r\n            mode_clip = QClipboard.Clipboard\r\n            mode_sel = QClipboard.Selection\r\n            text_clip = QApplication.clipboard().text(mode=mode_clip)\r\n            text_sel = QApplication.clipboard().text(mode=mode_sel)\r\n            QApplication.clipboard().setText(text_sel, mode=mode_clip)\r\n            self.paste()\r\n            QApplication.clipboard().setText(text_clip, mode=mode_clip)\r\n        else:\r\n            self.calltip_widget.hide()\r\n            QPlainTextEdit.mousePressEvent(self, event)", "output": "Reimplement Qt method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lock(instance_id, profile=None, **kwargs):\n    '''\n    \n\n    '''\n    conn = _auth(profile, **kwargs)\n    return conn.lock(instance_id)", "output": "Lock an instance\n\n    instance_id\n        ID of the instance to be locked\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' nova.lock 1138", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def commit_config(name):\n    '''\n    \n\n    '''\n    ret = _default_ret(name)\n\n    ret.update({\n        'commit': __salt__['panos.commit'](),\n        'result': True\n    })\n\n    return ret", "output": "Commits the candidate configuration to the running configuration.\n\n    name: The name of the module function to execute.\n\n    SLS Example:\n\n    .. code-block:: yaml\n\n        panos/commit:\n            panos.commit_config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def patch_certificate_signing_request(self, name, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_certificate_signing_request_with_http_info(name, body, **kwargs)\n        else:\n            (data) = self.patch_certificate_signing_request_with_http_info(name, body, **kwargs)\n            return data", "output": "partially update the specified CertificateSigningRequest\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_certificate_signing_request(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the CertificateSigningRequest (required)\n        :param object body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch).\n        :param bool force: Force is going to \\\"force\\\" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests.\n        :return: V1beta1CertificateSigningRequest\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_not_pickle_safe_gl_class(obj_class):\n    \"\"\"\n    \n\n    \"\"\"\n    gl_ds = [_SFrame, _SArray, _SGraph]\n\n    # Object is GLC-DS or GLC-Model\n    return (obj_class in gl_ds) or _is_not_pickle_safe_gl_model_class(obj_class)", "output": "Check if class is a Turi create model.\n\n    The function does it by checking the method resolution order (MRO) of the\n    class and verifies that _Model is the base class.\n\n    Parameters\n    ----------\n    obj_class    : Class to be checked.\n\n    Returns\n    ----------\n    True if the class is a GLC Model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def interpolate_2d(values, method='pad', axis=0, limit=None, fill_value=None,\n                   dtype=None):\n    \"\"\"\n    \n    \"\"\"\n\n    transf = (lambda x: x) if axis == 0 else (lambda x: x.T)\n\n    # reshape a 1 dim if needed\n    ndim = values.ndim\n    if values.ndim == 1:\n        if axis != 0:  # pragma: no cover\n            raise AssertionError(\"cannot interpolate on a ndim == 1 with \"\n                                 \"axis != 0\")\n        values = values.reshape(tuple((1,) + values.shape))\n\n    if fill_value is None:\n        mask = None\n    else:  # todo create faster fill func without masking\n        mask = mask_missing(transf(values), fill_value)\n\n    method = clean_fill_method(method)\n    if method == 'pad':\n        values = transf(pad_2d(\n            transf(values), limit=limit, mask=mask, dtype=dtype))\n    else:\n        values = transf(backfill_2d(\n            transf(values), limit=limit, mask=mask, dtype=dtype))\n\n    # reshape back\n    if ndim == 1:\n        values = values[0]\n\n    return values", "output": "Perform an actual interpolation of values, values will be make 2-d if\n    needed fills inplace, returns the result.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next(self):\n        \"\"\"\"\"\"\n        self._parse_block()\n        if self._remaining > 0:\n            self._remaining -= 1\n        return six.next(self._iter_rows)", "output": "Get the next row in the page.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stats(self, using=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        return self._get_connection(using).indices.stats(index=self._name, **kwargs)", "output": "Retrieve statistics on different operations happening on the index.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.stats`` unchanged.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def multi_perspective_match_pairwise(vector1: torch.Tensor,\n                                     vector2: torch.Tensor,\n                                     weight: torch.Tensor,\n                                     eps: float = 1e-8) -> torch.Tensor:\n    \"\"\"\n    \n    \"\"\"\n    num_perspectives = weight.size(0)\n\n    # (1, num_perspectives, 1, hidden_size)\n    weight = weight.unsqueeze(0).unsqueeze(2)\n\n    # (batch, num_perspectives, seq_len*, hidden_size)\n    vector1 = weight * vector1.unsqueeze(1).expand(-1, num_perspectives, -1, -1)\n    vector2 = weight * vector2.unsqueeze(1).expand(-1, num_perspectives, -1, -1)\n\n    # (batch, num_perspectives, seq_len*, 1)\n    vector1_norm = vector1.norm(p=2, dim=3, keepdim=True)\n    vector2_norm = vector2.norm(p=2, dim=3, keepdim=True)\n\n    # (batch, num_perspectives, seq_len1, seq_len2)\n    mul_result = torch.matmul(vector1, vector2.transpose(2, 3))\n    norm_value = vector1_norm * vector2_norm.transpose(2, 3)\n\n    # (batch, seq_len1, seq_len2, num_perspectives)\n    return (mul_result / norm_value.clamp(min=eps)).permute(0, 2, 3, 1)", "output": "Calculate multi-perspective cosine matching between each time step of\n    one vector and each time step of another vector.\n\n    Parameters\n    ----------\n    vector1 : ``torch.Tensor``\n        A tensor of shape ``(batch, seq_len1, hidden_size)``\n    vector2 : ``torch.Tensor``\n        A tensor of shape ``(batch, seq_len2, hidden_size)``\n    weight : ``torch.Tensor``\n        A tensor of shape ``(num_perspectives, hidden_size)``\n    eps : ``float`` optional, (default = 1e-8)\n        A small value to avoid zero division problem\n\n    Returns\n    -------\n    A tensor of shape (batch, seq_len1, seq_len2, num_perspectives) consisting\n    multi-perspective matching results", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve_resource_refs(self, input_dict, supported_resource_refs):\n        \"\"\"\n        \n        \"\"\"\n\n        if not self.can_handle(input_dict):\n            return input_dict\n\n        ref_value = input_dict[self.intrinsic_name]\n        logical_id, property = self._parse_resource_reference(ref_value)\n\n        # ref_value could not be parsed\n        if not logical_id:\n            return input_dict\n\n        resolved_value = supported_resource_refs.get(logical_id, property)\n        if not resolved_value:\n            return input_dict\n\n        return {\n            self.intrinsic_name: resolved_value\n        }", "output": "Resolves references to some property of a resource. These are runtime properties which can't be converted\n        to a value here. Instead we output another reference that will more actually resolve to the value when\n        executed via CloudFormation\n\n        Example:\n            {\"Ref\": \"LogicalId.Property\"} => {\"Ref\": \"SomeOtherLogicalId\"}\n\n        :param dict input_dict: Dictionary representing the Ref function to be resolved.\n        :param samtranslator.intrinsics.resource_refs.SupportedResourceReferences supported_resource_refs: Instance of\n            an `SupportedResourceReferences` object that contain value of the property.\n        :return dict: Dictionary with resource references resolved.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fillna(self, **kwargs):\n        \"\"\"\n        \"\"\"\n        axis = kwargs.get(\"axis\", 0)\n        value = kwargs.get(\"value\")\n        if isinstance(value, dict):\n            value = kwargs.pop(\"value\")\n\n            if axis == 0:\n                index = self.columns\n            else:\n                index = self.index\n            value = {\n                idx: value[key] for key in value for idx in index.get_indexer_for([key])\n            }\n\n            def fillna_dict_builder(df, func_dict={}):\n                # We do this to ensure that no matter the state of the columns we get\n                # the correct ones.\n                func_dict = {df.columns[idx]: func_dict[idx] for idx in func_dict}\n                return df.fillna(value=func_dict, **kwargs)\n\n            new_data = self.data.apply_func_to_select_indices(\n                axis, fillna_dict_builder, value, keep_remaining=True\n            )\n            return self.__constructor__(new_data, self.index, self.columns)\n        else:\n            func = self._prepare_method(pandas.DataFrame.fillna, **kwargs)\n            new_data = self._map_across_full_axis(axis, func)\n            return self.__constructor__(new_data, self.index, self.columns)", "output": "Replaces NaN values with the method provided.\n\n        Returns:\n            A new QueryCompiler with null values filled.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def document_did_save_notification(self, params):\n        \"\"\"\n        \n        \"\"\"\n        text = None\n        if 'text' in params:\n            text = params['text']\n        params = {\n            'textDocument': {\n                'uri': path_as_uri(params['file'])\n            }\n        }\n        if text is not None:\n            params['text'] = text\n        return params", "output": "Handle the textDocument/didSave message received from an LSP server.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ip_to_host(ip):\n    '''\n    \n    '''\n    try:\n        hostname, aliaslist, ipaddrlist = socket.gethostbyaddr(ip)\n    except Exception as exc:\n        log.debug('salt.utils.network.ip_to_host(%r) failed: %s', ip, exc)\n        hostname = None\n    return hostname", "output": "Returns the hostname of a given IP", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_nan_block_id(partition_class, n_row=1, n_col=1, transpose=False):\n    \"\"\"\n    \"\"\"\n    global _NAN_BLOCKS\n    if transpose:\n        n_row, n_col = n_col, n_row\n    shape = (n_row, n_col)\n    if shape not in _NAN_BLOCKS:\n        arr = np.tile(np.array(np.NaN), shape)\n        # TODO Not use pandas.DataFrame here, but something more general.\n        _NAN_BLOCKS[shape] = partition_class.put(pandas.DataFrame(data=arr))\n    return _NAN_BLOCKS[shape]", "output": "A memory efficient way to get a block of NaNs.\n\n    Args:\n        partition_class (BaseFramePartition): The class to use to put the object\n            in the remote format.\n        n_row(int): The number of rows.\n        n_col(int): The number of columns.\n        transpose(bool): If true, swap rows and columns.\n    Returns:\n        ObjectID of the NaN block.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_flags_from_package_conf(conf, atom):\n    '''\n    \n    '''\n    if conf in SUPPORTED_CONFS:\n        package_file = _get_config_file(conf, atom)\n        if '/' not in atom:\n            atom = _p_to_cp(atom)\n\n        has_wildcard = '*' in atom\n        if has_wildcard:\n            match_list = set(atom)\n        else:\n            try:\n                match_list = set(_porttree().dbapi.xmatch(\"match-all\", atom))\n            except AttributeError:\n                return []\n\n        flags = []\n        try:\n            with salt.utils.files.fopen(package_file) as fp_:\n                for line in fp_:\n                    line = salt.utils.stringutils.to_unicode(line).strip()\n                    line_package = line.split()[0]\n\n                    if has_wildcard:\n                        found_match = line_package == atom\n                    else:\n                        line_list = _porttree().dbapi.xmatch(\"match-all\", line_package)\n                        found_match = match_list.issubset(line_list)\n\n                    if found_match:\n                        f_tmp = [flag for flag in line.strip().split(' ') if flag][1:]\n                        if f_tmp:\n                            flags.extend(f_tmp)\n                        else:\n                            flags.append('~ARCH')\n\n            return _merge_flags(flags)\n        except IOError:\n            return []", "output": "Get flags for a given package or DEPEND atom.\n    Warning: This only works if the configuration files tree is in the correct\n    format (the one enforced by enforce_nice_config)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' portage_config.get_flags_from_package_conf license salt", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zoom_out(self):\n        \"\"\"\"\"\"\n        if self._scalefactor >= self._sfmin:\n            self._scalefactor -= 1\n            self.scale_image()\n            self._adjust_scrollbar(1/self._scalestep)\n            self.sig_zoom_changed.emit(self.get_scaling())", "output": "Scale the image down by one scale step.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_shortcut_ownership(path, user):\n    '''\n    \n    '''\n    try:\n        __salt__['file.lchown'](path, user)\n    except OSError:\n        pass\n    return _check_shortcut_ownership(path, user)", "output": "Set the ownership of a shortcut and return a boolean indicating\n    success/failure", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def char_beam_search(out):\n    \"\"\"\n    \n    \"\"\"\n    out_conv = list()\n    for idx in range(out.shape[0]):\n        probs = out[idx]\n        prob = probs.softmax().asnumpy()\n        line_string_proposals = ctcBeamSearch(prob, ALPHABET, None, k=4, beamWidth=25)\n        out_conv.append(line_string_proposals[0])\n    return out_conv", "output": "Description : apply beam search for prediction result", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(self, term, name, overwrite=False):\n        \"\"\"\n        \n        \"\"\"\n        self.validate_column(name, term)\n\n        columns = self.columns\n        if name in columns:\n            if overwrite:\n                self.remove(name)\n            else:\n                raise KeyError(\"Column '{}' already exists.\".format(name))\n\n        if not isinstance(term, ComputableTerm):\n            raise TypeError(\n                \"{term} is not a valid pipeline column. Did you mean to \"\n                \"append '.latest'?\".format(term=term)\n            )\n\n        self._columns[name] = term", "output": "Add a column.\n\n        The results of computing `term` will show up as a column in the\n        DataFrame produced by running this pipeline.\n\n        Parameters\n        ----------\n        column : zipline.pipeline.Term\n            A Filter, Factor, or Classifier to add to the pipeline.\n        name : str\n            Name of the column to add.\n        overwrite : bool\n            Whether to overwrite the existing entry if we already have a column\n            named `name`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def loadACatalog(filename):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlLoadACatalog(filename)\n    if ret is None:raise treeError('xmlLoadACatalog() failed')\n    return catalog(_obj=ret)", "output": "Load the catalog and build the associated data structures.\n      This can be either an XML Catalog or an SGML Catalog It\n      will recurse in SGML CATALOG entries. On the other hand XML\n       Catalogs are not handled recursively.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(self, event, subscriber):\n        \"\"\"\n        \n        \"\"\"\n        subs = self._subscribers\n        if event not in subs:\n            raise ValueError('No subscribers: %r' % event)\n        subs[event].remove(subscriber)", "output": "Remove a subscriber for an event.\n\n        :param event: The name of an event.\n        :param subscriber: The subscriber to be removed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _fill(self, direction, limit=None):\n        \"\"\"\"\"\"\n        res = super()._fill(direction, limit=limit)\n        output = OrderedDict(\n            (grp.name, grp.grouper) for grp in self.grouper.groupings)\n\n        from pandas import concat\n        return concat((self._wrap_transformed_output(output), res), axis=1)", "output": "Overridden method to join grouped columns in output", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filter(self, fn, skip_na=True, seed=None):\n        \"\"\"\n        \n        \"\"\"\n        assert callable(fn), \"Input must be callable\"\n        if seed is None:\n            seed = abs(hash(\"%0.20f\" % time.time())) % (2 ** 31)\n\n\n        with cython_context():\n            return SArray(_proxy=self.__proxy__.filter(fn, skip_na, seed))", "output": "Filter this SArray by a function.\n\n        Returns a new SArray filtered by this SArray.  If `fn` evaluates an\n        element to true, this element is copied to the new SArray. If not, it\n        isn't. Throws an exception if the return type of `fn` is not castable\n        to a boolean value.\n\n        Parameters\n        ----------\n        fn : function\n            Function that filters the SArray. Must evaluate to bool or int.\n\n        skip_na : bool, optional\n            If True, will not apply fn to any undefined values.\n\n        seed : int, optional\n            Used as the seed if a random number generator is included in fn.\n\n        Returns\n        -------\n        out : SArray\n            The SArray filtered by fn. Each element of the SArray is of\n            type int.\n\n        Examples\n        --------\n        >>> sa = turicreate.SArray([1,2,3])\n        >>> sa.filter(lambda x: x < 3)\n        dtype: int\n        Rows: 2\n        [1, 2]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def report(result_pickle_file_path, target_report_csv_path):\n    \"\"\"\n    \n    \"\"\"\n    import pandas as pd\n    result_dict = pd.read_pickle(result_pickle_file_path)\n\n    from .report import generate_report\n    generate_report(result_dict, target_report_csv_path)", "output": "[sys_analyser] Generate report from backtest output file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_pending_servermanager():\n    '''\n    \n    '''\n    vname = 'CurrentRebootAttempts'\n    key = r'SOFTWARE\\Microsoft\\ServerManager'\n\n    # There are situations where it's possible to have '(value not set)' as\n    # the value data, and since an actual reboot won't be pending in that\n    # instance, just catch instances where we try unsuccessfully to cast as int.\n\n    reg_ret = __utils__['reg.read_value']('HKLM', key, vname)\n\n    if reg_ret['success']:\n        log.debug('Found key: %s', key)\n\n        try:\n            if int(reg_ret['vdata']) > 0:\n                return True\n        except ValueError:\n            pass\n    else:\n        log.debug('Unable to access key: %s', key)\n    return False", "output": "Determine whether there are pending Server Manager tasks that require a\n    reboot.\n\n    .. versionadded:: 2016.11.0\n\n    Returns:\n        bool: ``True`` if there are pending Server Manager tasks, otherwise\n        ``False``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' system.get_pending_servermanager", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unescape(url):\n    '''\n    \n    '''\n    scheme = urlparse(url).scheme\n    if not scheme:\n        return url.lstrip('|')\n    elif scheme == 'salt':\n        path, saltenv = parse(url)\n        if salt.utils.platform.is_windows() and '|' in url:\n            return create(path.lstrip('_'), saltenv)\n        else:\n            return create(path.lstrip('|'), saltenv)\n    else:\n        return url", "output": "remove escape character `|` from `url`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _info_to_string(info):\n  \"\"\"\n  \"\"\"\n  for key in _TENSORBOARD_INFO_FIELDS:\n    field_type = _TENSORBOARD_INFO_FIELDS[key]\n    if not isinstance(getattr(info, key), field_type.runtime_type):\n      raise ValueError(\n          \"expected %r of type %s, but found: %r\" %\n          (key, field_type.runtime_type, getattr(info, key))\n      )\n  if info.version != version.VERSION:\n    raise ValueError(\n        \"expected 'version' to be %r, but found: %r\" %\n        (version.VERSION, info.version)\n    )\n  json_value = {\n      k: _TENSORBOARD_INFO_FIELDS[k].serialize(getattr(info, k))\n      for k in _TENSORBOARD_INFO_FIELDS\n  }\n  return json.dumps(json_value, sort_keys=True, indent=4)", "output": "Convert a `TensorBoardInfo` to string form to be stored on disk.\n\n  The format returned by this function is opaque and should only be\n  interpreted by `_info_from_string`.\n\n  Args:\n    info: A valid `TensorBoardInfo` object.\n\n  Raises:\n    ValueError: If any field on `info` is not of the correct type.\n\n  Returns:\n    A string representation of the provided `TensorBoardInfo`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_valid_endpoint(endpoint):\n    \"\"\"\"\"\"\n    return any([is_number(endpoint),\n                isinstance(endpoint, Timestamp),\n                isinstance(endpoint, Timedelta),\n                endpoint is None])", "output": "helper for interval_range to check if start/end are valid types", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_keyboard_row(words):\n    \"\"\"\n    \n    \"\"\"\n    keyboard = [\n        set('qwertyuiop'),\n        set('asdfghjkl'),\n        set('zxcvbnm'),\n    ]\n    result = []\n    for word in words:\n        for key in keyboard:\n            if set(word.lower()).issubset(key):\n                result.append(word)\n    return result", "output": ":type words: List[str]\n    :rtype: List[str]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_opengl_implementation(option):\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    if option == 'software':\r\n        QCoreApplication.setAttribute(Qt.AA_UseSoftwareOpenGL)\r\n        if QQuickWindow is not None:\r\n            QQuickWindow.setSceneGraphBackend(QSGRendererInterface.Software)\r\n    elif option == 'desktop':\r\n        QCoreApplication.setAttribute(Qt.AA_UseDesktopOpenGL)\r\n        if QQuickWindow is not None:\r\n            QQuickWindow.setSceneGraphBackend(QSGRendererInterface.OpenGL)\r\n    elif option == 'gles':\r\n        QCoreApplication.setAttribute(Qt.AA_UseOpenGLES)\r\n        if QQuickWindow is not None:\r\n            QQuickWindow.setSceneGraphBackend(QSGRendererInterface.OpenGL)", "output": "Set the OpenGL implementation used by Spyder.\r\n\r\n    See issue 7447 for the details.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_log_filenames(self):\n        \"\"\"\"\"\"\n        log_filenames = os.listdir(self.logs_dir)\n\n        for log_filename in log_filenames:\n            full_path = os.path.join(self.logs_dir, log_filename)\n            if full_path not in self.log_filenames:\n                self.log_filenames.add(full_path)\n                self.closed_file_infos.append(\n                    LogFileInfo(\n                        filename=full_path,\n                        size_when_last_opened=0,\n                        file_position=0,\n                        file_handle=None))\n                logger.info(\"Beginning to track file {}\".format(log_filename))", "output": "Update the list of log files to monitor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_zones(region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    return [z.name for z in conn.get_all_zones()]", "output": "Get a list of AZs for the configured region.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_ec2.get_zones", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_minions_directories(self):\n        '''\n        \n        '''\n        minions_accepted = os.path.join(self.opts['pki_dir'], self.ACC)\n        minions_pre = os.path.join(self.opts['pki_dir'], self.PEND)\n        minions_rejected = os.path.join(self.opts['pki_dir'],\n                                        self.REJ)\n\n        minions_denied = os.path.join(self.opts['pki_dir'],\n                                        self.DEN)\n        return minions_accepted, minions_pre, minions_rejected, minions_denied", "output": "Return the minion keys directory paths", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sar_service_call(self, service_call_lambda, logical_id, *args):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            response = service_call_lambda(*args)\n            logging.info(response)\n            return response\n        except ClientError as e:\n            error_code = e.response['Error']['Code']\n            if error_code in ('AccessDeniedException', 'NotFoundException'):\n                raise InvalidResourceException(logical_id, e.response['Error']['Message'])\n\n            # 'ForbiddenException'- SAR rejects connection\n            logging.exception(e)\n            raise e", "output": "Handles service calls and exception management for service calls\n        to the Serverless Application Repository.\n\n        :param lambda service_call_lambda: lambda function that contains the service call\n        :param string logical_id: Logical ID of the resource being processed\n        :param list *args: arguments for the service call lambda", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy_model_to_fp32(m, optim):\n    \"\"\"  \n    \"\"\"\n    fp32_params = [m_param.clone().type(torch.cuda.FloatTensor).detach() for m_param in trainable_params_(m)]\n    optim_groups = [group['params'] for group in optim.param_groups]\n    iter_fp32_params = iter(fp32_params)\n    for group_params in optim_groups:\n        for i in range(len(group_params)):\n            if not group_params[i].requires_grad: continue # only update trainable_params_\n            fp32_param = next(iter_fp32_params)\n            assert(fp32_param.shape == group_params[i].shape)\n            fp32_param.requires_grad = group_params[i].requires_grad\n            group_params[i] = fp32_param\n    return fp32_params", "output": "Creates a fp32 copy of model parameters and sets optimizer parameters", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_valid_bucket_name(name):\n    \"\"\"\n    \n    \"\"\"\n    # Bucket names must be at least 3 and no more than 63 characters long.\n    if (len(name) < 3 or len(name) > 63):\n        return False\n    # Bucket names must not contain uppercase characters or underscores.\n    if (any(x.isupper() for x in name)):\n        return False\n    if \"_\" in name:\n        return False\n    # Bucket names must start with a lowercase letter or number.\n    if not (name[0].islower() or name[0].isdigit()):\n        return False\n    # Bucket names must be a series of one or more labels. Adjacent labels are separated by a single period (.).\n    for label in name.split(\".\"):\n        # Each label must start and end with a lowercase letter or a number.\n        if len(label) < 1:\n            return False\n        if not (label[0].islower() or label[0].isdigit()):\n            return False\n        if not (label[-1].islower() or label[-1].isdigit()):\n            return False\n    # Bucket names must not be formatted as an IP address (for example, 192.168.5.4).\n    looks_like_IP = True\n    for label in name.split(\".\"):\n        if not label.isdigit():\n            looks_like_IP = False\n            break\n    if looks_like_IP:\n        return False\n\n    return True", "output": "Checks if an S3 bucket name is valid according to https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html#bucketnamingrules", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_to_array(array_like, dtype):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(array_like, bytes):\n            return np.frombuffer(array_like, dtype=dtype)\n        return np.asarray(array_like, dtype=dtype)", "output": "Convert Matrix attributes which are array-like or buffer to array.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def usage(self, node_id=None, metric=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_nodes\", node_id, \"usage\", metric), params=params\n        )", "output": "The cluster nodes usage API allows to retrieve information on the usage\n        of features for each node.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-nodes-usage.html>`_\n\n        :arg node_id: A comma-separated list of node IDs or names to limit the\n            returned information; use `_local` to return information from the\n            node you're connecting to, leave empty to get information from all\n            nodes\n        :arg metric: Limit the information returned to the specified metrics\n        :arg human: Whether to return time and byte values in human-readable\n            format., default False\n        :arg timeout: Explicit operation timeout", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def removeRef(self, doc):\n        \"\"\" \"\"\"\n        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        ret = libxml2mod.xmlRemoveRef(doc__o, self._o)\n        return ret", "output": "Remove the given attribute from the Ref table maintained\n           internally.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vcs_command(self, fnames, action):\r\n        \"\"\"\"\"\"\r\n        try:\r\n            for path in sorted(fnames):\r\n                vcs.run_vcs_tool(path, action)\r\n        except vcs.ActionToolNotFound as error:\r\n            msg = _(\"For %s support, please install one of the<br/> \"\r\n                    \"following tools:<br/><br/>  %s\")\\\r\n                        % (error.vcsname, ', '.join(error.tools))\r\n            QMessageBox.critical(self, _(\"Error\"),\r\n                _(\"\"\"<b>Unable to find external program.</b><br><br>%s\"\"\")\r\n                    % to_text_string(msg))", "output": "VCS action (commit, browse)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _info_from_string(info_string):\n  \"\"\"\n  \"\"\"\n\n  try:\n    json_value = json.loads(info_string)\n  except ValueError:\n    raise ValueError(\"invalid JSON: %r\" % (info_string,))\n  if not isinstance(json_value, dict):\n    raise ValueError(\"not a JSON object: %r\" % (json_value,))\n  if json_value.get(\"version\") != version.VERSION:\n    raise ValueError(\"incompatible version: %r\" % (json_value,))\n  expected_keys = frozenset(_TENSORBOARD_INFO_FIELDS)\n  actual_keys = frozenset(json_value)\n  if expected_keys != actual_keys:\n    raise ValueError(\n        \"bad keys on TensorBoardInfo (missing: %s; extraneous: %s)\"\n        % (expected_keys - actual_keys, actual_keys - expected_keys)\n    )\n\n  # Validate and deserialize fields.\n  for key in _TENSORBOARD_INFO_FIELDS:\n    field_type = _TENSORBOARD_INFO_FIELDS[key]\n    if not isinstance(json_value[key], field_type.serialized_type):\n      raise ValueError(\n          \"expected %r of type %s, but found: %r\" %\n          (key, field_type.serialized_type, json_value[key])\n      )\n    json_value[key] = field_type.deserialize(json_value[key])\n\n  return TensorBoardInfo(**json_value)", "output": "Parse a `TensorBoardInfo` object from its string representation.\n\n  Args:\n    info_string: A string representation of a `TensorBoardInfo`, as\n      produced by a previous call to `_info_to_string`.\n\n  Returns:\n    A `TensorBoardInfo` value.\n\n  Raises:\n    ValueError: If the provided string is not valid JSON, or if it does\n      not represent a JSON object with a \"version\" field whose value is\n      `tensorboard.version.VERSION`, or if it has the wrong set of\n      fields, or if at least one field is of invalid type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ace_to_text(ace, objectType):\n    '''\n    \n    '''\n    dc = daclConstants()\n    objectType = dc.getObjectTypeBit(objectType)\n    try:\n        userSid = win32security.LookupAccountSid('', ace[2])\n        if userSid[1]:\n            userSid = '{1}\\\\{0}'.format(userSid[0], userSid[1])\n        else:\n            userSid = '{0}'.format(userSid[0])\n    except Exception:\n        userSid = win32security.ConvertSidToStringSid(ace[2])\n    tPerm = ace[1]\n    tAceType = ace[0][0]\n    tProps = ace[0][1]\n    tInherited = ''\n    for x in dc.validAceTypes:\n        if dc.validAceTypes[x]['BITS'] == tAceType:\n            tAceType = dc.validAceTypes[x]['TEXT']\n            break\n    for x in dc.rights[objectType]:\n        if dc.rights[objectType][x]['BITS'] == tPerm:\n            tPerm = dc.rights[objectType][x]['TEXT']\n            break\n    if (tProps & win32security.INHERITED_ACE) == win32security.INHERITED_ACE:\n        tInherited = '[Inherited]'\n        tProps = (tProps ^ win32security.INHERITED_ACE)\n    for x in dc.validPropagations[objectType]:\n        if dc.validPropagations[objectType][x]['BITS'] == tProps:\n            tProps = dc.validPropagations[objectType][x]['TEXT']\n            break\n    return ((\n        '{0} {1} {2} on {3} {4}'\n        ).format(userSid, tAceType, tPerm, tProps, tInherited))", "output": "helper function to convert an ace to a textual representation", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set(zpool, prop, value):\n    '''\n    \n\n    '''\n    ret = OrderedDict()\n\n    # set property\n    res = __salt__['cmd.run_all'](\n        __utils__['zfs.zpool_command'](\n            command='set',\n            property_name=prop,\n            property_value=value,\n            target=zpool,\n        ),\n        python_shell=False,\n    )\n\n    return __utils__['zfs.parse_command_result'](res, 'set')", "output": "Sets the given property on the specified pool\n\n    zpool : string\n        Name of storage pool\n\n    prop : string\n        Name of property to set\n\n    value : string\n        Value to set for the specified property\n\n    .. versionadded:: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zpool.set myzpool readonly yes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def l2_norm(x, filters=None, epsilon=1e-6, name=None, reuse=None):\n  \"\"\"\"\"\"\n  if filters is None:\n    filters = shape_list(x)[-1]\n  with tf.variable_scope(name, default_name=\"l2_norm\", values=[x], reuse=reuse):\n    scale = tf.get_variable(\n        \"l2_norm_scale\", [filters], initializer=tf.ones_initializer())\n    bias = tf.get_variable(\n        \"l2_norm_bias\", [filters], initializer=tf.zeros_initializer())\n    epsilon, scale, bias = [cast_like(t, x) for t in [epsilon, scale, bias]]\n    mean = tf.reduce_mean(x, axis=[-1], keepdims=True)\n    l2norm = tf.reduce_sum(\n        tf.squared_difference(x, mean), axis=[-1], keepdims=True)\n    norm_x = (x - mean) * tf.rsqrt(l2norm + epsilon)\n    return norm_x * scale + bias", "output": "Layer normalization with l2 norm.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_xml(self, value):\n        \"\"\"\n        \"\"\"\n        def normalize_iter(value):\n            if isinstance(value, (list, tuple)):\n                if isinstance(value[0], str):\n                    xmlval = value\n                else:\n                    xmlval = []\n            elif isinstance(value, dict):\n                xmlval = list(value.items())\n            else:\n                raise TemplateRuntimeError(\n                    'Value is not a dict or list. Cannot render as XML')\n            return xmlval\n\n        def recurse_tree(xmliter, element=None):\n            sub = None\n            for tag, attrs in xmliter:\n                if isinstance(attrs, list):\n                    for attr in attrs:\n                        recurse_tree(((tag, attr),), element)\n                elif element is not None:\n                    sub = SubElement(element, tag)\n                else:\n                    sub = Element(tag)\n                if isinstance(attrs, (str, int, bool, float)):\n                    sub.text = six.text_type(attrs)\n                    continue\n                if isinstance(attrs, dict):\n                    sub.attrib = {attr: six.text_type(val) for attr, val in attrs.items()\n                                  if not isinstance(val, (dict, list))}\n                for tag, val in [item for item in normalize_iter(attrs) if\n                                 isinstance(item[1], (dict, list))]:\n                    recurse_tree(((tag, val),), sub)\n            return sub\n\n        return Markup(minidom.parseString(\n            tostring(recurse_tree(normalize_iter(value)))\n        ).toprettyxml(indent=\" \"))", "output": "Render a formatted multi-line XML string from a complex Python\n        data structure. Supports tag attributes and nested dicts/lists.\n\n        :param value: Complex data structure representing XML contents\n        :returns: Formatted XML string rendered with newlines and indentation\n        :rtype: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_top(self):\n        '''\n        \n        '''\n        tops, errors = self.get_tops()\n        try:\n            merged_tops = self.merge_tops(tops)\n        except TypeError as err:\n            merged_tops = OrderedDict()\n            errors.append('Error encountered while rendering pillar top file.')\n        return merged_tops, errors", "output": "Returns the high data derived from the top file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reflected_binary_operator(op):\n    \"\"\"\n    \n    \"\"\"\n    assert not is_comparison(op)\n\n    @with_name(method_name_for_op(op, commute=True))\n    @coerce_numbers_to_my_dtype\n    def reflected_binary_operator(self, other):\n\n        if isinstance(self, NumericalExpression):\n            self_expr, other_expr, new_inputs = self.build_binary_op(\n                op, other\n            )\n            return NumExprFactor(\n                \"({left}) {op} ({right})\".format(\n                    left=other_expr,\n                    right=self_expr,\n                    op=op,\n                ),\n                new_inputs,\n                dtype=binop_return_dtype(op, other.dtype, self.dtype)\n            )\n\n        # Only have to handle the numeric case because in all other valid cases\n        # the corresponding left-binding method will be called.\n        elif isinstance(other, Number):\n            return NumExprFactor(\n                \"{constant} {op} x_0\".format(op=op, constant=other),\n                binds=(self,),\n                dtype=binop_return_dtype(op, other.dtype, self.dtype),\n            )\n        raise BadBinaryOperator(op, other, self)\n    return reflected_binary_operator", "output": "Factory function for making binary operator methods on a Factor.\n\n    Returns a function, \"reflected_binary_operator\" suitable for implementing\n    functions like __radd__.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(self, using=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        self._get_connection(using).indices.create(index=self._name, body=self.to_dict(), **kwargs)", "output": "Creates the index in elasticsearch.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.create`` unchanged.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _websocket_mask_python(mask: bytes, data: bytearray) -> None:\n    \"\"\"\n\n    \"\"\"\n    assert isinstance(data, bytearray), data\n    assert len(mask) == 4, mask\n\n    if data:\n        a, b, c, d = (_XOR_TABLE[n] for n in mask)\n        data[::4] = data[::4].translate(a)\n        data[1::4] = data[1::4].translate(b)\n        data[2::4] = data[2::4].translate(c)\n        data[3::4] = data[3::4].translate(d)", "output": "Websocket masking function.\n\n    `mask` is a `bytes` object of length 4; `data` is a `bytearray`\n    object of any length. The contents of `data` are masked with `mask`,\n    as specified in section 5.3 of RFC 6455.\n\n    Note that this function mutates the `data` argument.\n\n    This pure-python implementation may be replaced by an optimized\n    version when available.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_num_tokens_from_first_line(line: str) -> Optional[int]:\n        \"\"\"  \"\"\"\n        fields = line.split(' ')\n        if 1 <= len(fields) <= 2:\n            try:\n                int_fields = [int(x) for x in fields]\n            except ValueError:\n                return None\n            else:\n                num_tokens = max(int_fields)\n                logger.info('Recognized a header line in the embedding file with number of tokens: %d',\n                            num_tokens)\n                return num_tokens\n        return None", "output": "This function takes in input a string and if it contains 1 or 2 integers, it assumes the\n        largest one it the number of tokens. Returns None if the line doesn't match that pattern.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_file_to_archive(self, name: str) -> None:\n        \"\"\"\n        \n        \"\"\"\n        if not self.loading_from_archive:\n            self.files_to_archive[f\"{self.history}{name}\"] = cached_path(self.get(name))", "output": "Any class in its ``from_params`` method can request that some of its\n        input files be added to the archive by calling this method.\n\n        For example, if some class ``A`` had an ``input_file`` parameter, it could call\n\n        ```\n        params.add_file_to_archive(\"input_file\")\n        ```\n\n        which would store the supplied value for ``input_file`` at the key\n        ``previous.history.and.then.input_file``. The ``files_to_archive`` dict\n        is shared with child instances via the ``_check_is_dict`` method, so that\n        the final mapping can be retrieved from the top-level ``Params`` object.\n\n        NOTE: You must call ``add_file_to_archive`` before you ``pop()``\n        the parameter, because the ``Params`` instance looks up the value\n        of the filename inside itself.\n\n        If the ``loading_from_archive`` flag is True, this will be a no-op.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cluster_setup(nodes, pcsclustername='pcscluster', extra_args=None):\n    '''\n    \n    '''\n    cmd = ['pcs', 'cluster', 'setup']\n\n    cmd += ['--name', pcsclustername]\n\n    cmd += nodes\n    if isinstance(extra_args, (list, tuple)):\n        cmd += extra_args\n\n    return __salt__['cmd.run_all'](cmd, output_loglevel='trace', python_shell=False)", "output": "Setup pacemaker cluster via pcs command\n\n    nodes\n        a list of nodes which should be set up\n    pcsclustername\n        Name of the Pacemaker cluster (default: pcscluster)\n    extra_args\n        list of extra option for the \\'pcs cluster setup\\' command\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pcs.cluster_setup nodes='[ node1.example.org node2.example.org ]' pcsclustername=pcscluster", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_default_master_type(num_gpus=1):\n  \"\"\"\"\"\"\n  gpus_to_master_map = {\n      0: \"standard\",\n      1: \"standard_p100\",\n      4: \"complex_model_m_p100\",\n      8: \"complex_model_l_gpu\",\n  }\n  if num_gpus not in gpus_to_master_map:\n    raise ValueError(\"Num gpus must be in %s\" %\n                     str(sorted(list(gpus_to_master_map.keys()))))\n  return gpus_to_master_map[num_gpus]", "output": "Returns master_type for trainingInput.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_image(location, size, fmt):\n    '''\n    \n    '''\n    if not os.path.isabs(location):\n        return ''\n    if not os.path.isdir(os.path.dirname(location)):\n        return ''\n    if not __salt__['cmd.retcode'](\n            'qemu-img create -f {0} {1} {2}M'.format(\n                fmt,\n                location,\n                size),\n                python_shell=False):\n        return location\n    return ''", "output": "Create a blank virtual machine image file of the specified size in\n    megabytes. The image can be created in any format supported by qemu\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' qemu_img.make_image /tmp/image.qcow 2048 qcow2\n        salt '*' qemu_img.make_image /tmp/image.raw 10240 raw", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sendall_stderr(self, s):\n        \"\"\"\n        \n        \"\"\"\n        while s:\n            sent = self.send_stderr(s)\n            s = s[sent:]\n        return None", "output": "Send data to the channel's \"stderr\" stream, without allowing partial\n        results.  Unlike `send_stderr`, this method continues to send data\n        from the given string until all data has been sent or an error occurs.\n        Nothing is returned.\n\n        :param str s: data to send to the client as \"stderr\" output.\n\n        :raises socket.timeout:\n            if sending stalled for longer than the timeout set by `settimeout`.\n        :raises socket.error:\n            if an error occurred before the entire string was sent.\n\n        .. versionadded:: 1.1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_visual_input(camera_parameters, name):\n        \"\"\"\n        \n        \"\"\"\n        o_size_h = camera_parameters['height']\n        o_size_w = camera_parameters['width']\n        bw = camera_parameters['blackAndWhite']\n\n        if bw:\n            c_channels = 1\n        else:\n            c_channels = 3\n\n        visual_in = tf.placeholder(shape=[None, o_size_h, o_size_w, c_channels], dtype=tf.float32,\n                                   name=name)\n        return visual_in", "output": "Creates image input op.\n        :param camera_parameters: Parameters for visual observation from BrainInfo.\n        :param name: Desired name of input op.\n        :return: input op.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def evaluate_single_config(\n    hparams, sampling_temp, max_num_noops, agent_model_dir,\n    eval_fn=_eval_fn_with_learner\n):\n  \"\"\"\"\"\"\n  tf.logging.info(\"Evaluating metric %s\", get_metric_name(\n      sampling_temp, max_num_noops, clipped=False\n  ))\n  eval_hparams = trainer_lib.create_hparams(hparams.base_algo_params)\n  env = setup_env(\n      hparams, batch_size=hparams.eval_batch_size, max_num_noops=max_num_noops,\n      rl_env_max_episode_steps=hparams.eval_rl_env_max_episode_steps,\n      env_name=hparams.rl_env_name)\n  env.start_new_epoch(0)\n  eval_fn(env, hparams, eval_hparams, agent_model_dir, sampling_temp)\n  rollouts = env.current_epoch_rollouts()\n  env.close()\n\n  return tuple(\n      compute_mean_reward(rollouts, clipped) for clipped in (True, False)\n  )", "output": "Evaluate the PPO agent in the real environment.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _setattr_url_map(self):\n        '''\n        \n        '''\n        if self.apiopts.get('enable_sessions', True) is False:\n            url_blacklist = ['login', 'logout', 'minions', 'jobs']\n        else:\n            url_blacklist = []\n\n        urls = ((url, cls) for url, cls in six.iteritems(self.url_map)\n                if url not in url_blacklist)\n\n        for url, cls in urls:\n            setattr(self, url, cls())", "output": "Set an attribute on the local instance for each key/val in url_map\n\n        CherryPy uses class attributes to resolve URLs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _raise_error_if_column_exists(dataset, column_name = 'dataset',\n                            dataset_variable_name = 'dataset',\n                            column_name_error_message_name = 'column_name'):\n    \"\"\"\n    \n    \"\"\"\n    err_msg = 'The SFrame {0} must contain the column {1}.'.format(\n                                                dataset_variable_name,\n                                             column_name_error_message_name)\n    if column_name not in dataset.column_names():\n      raise ToolkitError(str(err_msg))", "output": "Check if a column exists in an SFrame with error message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _maybe_to_categorical(array):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(array, (ABCSeries, ABCCategoricalIndex)):\n        return array._values\n    elif isinstance(array, np.ndarray):\n        return Categorical(array)\n    return array", "output": "Coerce to a categorical if a series is given.\n\n    Internal use ONLY.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stderr():\n    \"\"\"\n    \n    \"\"\"\n\n    # We write all of the data to stderr with bytes, typically io.BytesIO. stderr in Python2\n    # accepts bytes but Python3 does not. This is due to a type change on the attribute. To keep\n    # this consistent, we leave Python2 the same and get the .buffer attribute on stderr in Python3\n    byte_stderr = sys.stderr\n\n    if sys.version_info.major > 2:\n        byte_stderr = sys.stderr.buffer  # pylint: disable=no-member\n\n    return byte_stderr", "output": "Returns the stderr as a byte stream in a Py2/PY3 compatible manner\n\n    Returns\n    -------\n    io.BytesIO\n        Byte stream of stderr", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_commands(package_names):\n        \"\"\"\n        \n        \"\"\"\n\n        commands = {}\n\n        for pkg_name in package_names:\n            cmd_name = pkg_name.split('.')[-1]\n            commands[cmd_name] = pkg_name\n\n        return commands", "output": "Extract the command name from package name. Last part of the module path is the command\n        ie. if path is foo.bar.baz, then \"baz\" is the command name.\n\n        :param package_names: List of package names\n        :return: Dictionary with command name as key and the package name as value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_batch_end(self, iteration:int, smooth_loss:TensorOrNumber, **kwargs:Any)->None:\n        \"\"\n        if iteration==0 or smooth_loss < self.best_loss: self.best_loss = smooth_loss\n        self.opt.lr = self.sched.step()\n        if self.sched.is_done or (self.stop_div and (smooth_loss > 4*self.best_loss or torch.isnan(smooth_loss))):\n            #We use the smoothed loss to decide on the stopping since it's less shaky.\n            return {'stop_epoch': True, 'stop_training': True}", "output": "Determine if loss has runaway and we should stop.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_connection(self, alias='default'):\n        \"\"\"\n        \n        \"\"\"\n        # do not check isinstance(Elasticsearch) so that people can wrap their\n        # clients\n        if not isinstance(alias, string_types):\n            return alias\n\n        # connection already established\n        try:\n            return self._conns[alias]\n        except KeyError:\n            pass\n\n        # if not, try to create it\n        try:\n            return self.create_connection(alias, **self._kwargs[alias])\n        except KeyError:\n            # no connection and no kwargs to set one up\n            raise KeyError('There is no connection with alias %r.' % alias)", "output": "Retrieve a connection, construct it if necessary (only configuration\n        was passed to us). If a non-string alias has been passed through we\n        assume it's already a client instance and will just return it as-is.\n\n        Raises ``KeyError`` if no client (or its definition) is registered\n        under the alias.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def draw_predictions(img, boxes, scores):\n    \"\"\"\n    \n    \"\"\"\n    if len(boxes) == 0:\n        return img\n    labels = scores.argmax(axis=1)\n    scores = scores.max(axis=1)\n    tags = [\"{},{:.2f}\".format(cfg.DATA.CLASS_NAMES[lb], score) for lb, score in zip(labels, scores)]\n    return viz.draw_boxes(img, boxes, tags)", "output": "Args:\n        boxes: kx4\n        scores: kxC", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_latent_decoder(x,\n                               encoder_output,\n                               ed_attention_bias,\n                               hparams,\n                               name=None):\n  \"\"\"\n  \"\"\"\n  with tf.variable_scope(name, default_name=\"transformer_latent_dec\"):\n    batch_size = common_layers.shape_list(x)[0]\n    compressed_img_len = (hparams.img_len //\n                          2**(hparams.num_compress_steps // 2))\n    x = tf.reshape(x, [batch_size,\n                       compressed_img_len,\n                       compressed_img_len * hparams.num_latents,\n                       hparams.hidden_size])\n    decoder_input, _, _ = cia.prepare_decoder(x, hparams)\n    decoder_output = cia.transformer_decoder_layers(\n        decoder_input,\n        encoder_output,\n        hparams.num_latent_layers or hparams.num_hidden_layers,\n        hparams,\n        attention_type=hparams.latent_attention_type,\n        encoder_decoder_attention_bias=ed_attention_bias,\n        name=\"decoder\")\n    decoder_output = tf.reshape(decoder_output,\n                                [batch_size,\n                                 compressed_img_len**2 * hparams.num_latents,\n                                 hparams.hidden_size])\n    return decoder_output", "output": "Transformer decoder over latents using latent_attention_type.\n\n  Args:\n    x: Tensor of shape [batch, length_q, hparams.hidden_size]. length_q is the\n      latent length, which is\n      height * width * hparams.num_latents / (2**hparams.num_compress_steps).\n    encoder_output: Tensor of shape [batch, length_kv, hparams.hidden_size].\n    ed_attention_bias: Tensor which broadcasts with shape [batch,\n      hparams.num_heads, length_q, length_kv]. Encoder-decoder attention bias.\n    hparams: HParams.\n    name: string, variable scope.\n\n  Returns:\n    Tensor of shape [batch, length_q, hparams.hidden_size].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_tfrecords_from_generator(generator, output_files, shuffle=True):\n  \"\"\"\"\"\"\n  if do_files_exist(output_files):\n    raise ValueError(\n        \"Pre-processed files already exists: {}.\".format(output_files))\n\n  with _incomplete_files(output_files) as tmp_files:\n    # Write all shards\n    writers = [tf.io.TFRecordWriter(fname) for fname in tmp_files]\n    with _close_on_exit(writers) as writers:\n      logging.info(\"Writing TFRecords\")\n      _round_robin_write(writers, generator)\n    # Shuffle each shard\n    if shuffle:\n      # WARNING: Using np instead of Python random because Python random\n      # produce different values between Python 2 and 3 and between\n      # architectures\n      random_gen = np.random.RandomState(42)\n      for path in utils.tqdm(\n          tmp_files, desc=\"Shuffling...\", unit=\" shard\", leave=False):\n        _shuffle_tfrecord(path, random_gen=random_gen)", "output": "Writes generated str records to output_files in round-robin order.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_periodic_callback(self, callback, period_milliseconds, callback_id=None):\n        \"\"\" \"\"\"\n\n        cb = _AsyncPeriodic(callback, period_milliseconds, io_loop=self._loop)\n        callback_id = self._assign_remover(callback, callback_id, self._periodic_callback_removers, cb.stop)\n        cb.start()\n        return callback_id", "output": "Adds a callback to be run every period_milliseconds until it is removed.\n        Returns an ID that can be used with remove_periodic_callback.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def path_ok(match_tuple: MatchTuple) -> bool:\n    \"\"\"\"\"\"\n    relative_path = match_tuple.link.split(\"#\")[0]\n    full_path = os.path.join(os.path.dirname(str(match_tuple.source)), relative_path)\n    return os.path.exists(full_path)", "output": "Check if a file in this repository exists.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getent(refresh=False):\n    '''\n    \n    '''\n    if 'user.getent' in __context__ and not refresh:\n        return __context__['user.getent']\n\n    ret = []\n    for user in __salt__['user.list_users']():\n        stuff = {}\n        user_info = __salt__['user.info'](user)\n\n        stuff['gid'] = ''\n        stuff['groups'] = user_info['groups']\n        stuff['home'] = user_info['home']\n        stuff['name'] = user_info['name']\n        stuff['passwd'] = user_info['passwd']\n        stuff['shell'] = ''\n        stuff['uid'] = user_info['uid']\n\n        ret.append(stuff)\n\n    __context__['user.getent'] = ret\n    return ret", "output": "Return the list of all info for all users\n\n    Args:\n        refresh (bool, optional): Refresh the cached user information. Useful\n            when used from within a state function. Default is False.\n\n    Returns:\n        dict: A dictionary containing information about all users on the system\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.getent", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_series(self, index=None, name=None):\n        \"\"\"\n        \n        \"\"\"\n\n        from pandas import Series\n\n        if index is None:\n            index = self._shallow_copy()\n        if name is None:\n            name = self.name\n\n        return Series(self.values.copy(), index=index, name=name)", "output": "Create a Series with both index and values equal to the index keys\n        useful with map for returning an indexer based on an index.\n\n        Parameters\n        ----------\n        index : Index, optional\n            index of resulting Series. If None, defaults to original index\n        name : string, optional\n            name of resulting Series. If None, defaults to name of original\n            index\n\n        Returns\n        -------\n        Series : dtype will be based on the type of the Index values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_pretrained_word2vec(infile):\n    \"\"\"\"\"\"\n    if isinstance(infile, str):\n        infile = open(infile)\n\n    word2vec_list = {}\n    for idx, line in enumerate(infile):\n        if idx == 0:\n            vocab_size, dim = line.strip().split()\n        else:\n            tks = line.strip().split()\n            word2vec_list[tks[0]] = map(float, tks[1:])\n\n    return word2vec_list", "output": "Load the pre-trained word2vec from file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stats():\n    '''\n    \n    '''\n    ret = {}\n    cmd = 'locate -S'\n    out = __salt__['cmd.run'](cmd).splitlines()\n    for line in out:\n        comps = line.strip().split()\n        if line.startswith('Database'):\n            ret['database'] = comps[1].replace(':', '')\n            continue\n        ret[' '.join(comps[1:])] = comps[0]\n    return ret", "output": "Returns statistics about the locate database\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' locate.stats", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send(self, request, stream=False, timeout=None, verify=True,\n             cert=None, proxies=None):\n        \"\"\"\n        \"\"\"\n        raise NotImplementedError", "output": "Sends PreparedRequest object. Returns Response object.\n\n        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\n        :param stream: (optional) Whether to stream the request content.\n        :param timeout: (optional) How long to wait for the server to send\n            data before giving up, as a float, or a :ref:`(connect timeout,\n            read timeout) <timeouts>` tuple.\n        :type timeout: float or tuple\n        :param verify: (optional) Either a boolean, in which case it controls whether we verify\n            the server's TLS certificate, or a string, in which case it must be a path\n            to a CA bundle to use\n        :param cert: (optional) Any user-provided SSL certificate to be trusted.\n        :param proxies: (optional) The proxies dictionary to apply to the request.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def import_graph(self, t_input=None, scope='import', forget_xy_shape=True):\n    \"\"\"\"\"\"\n    graph = tf.get_default_graph()\n    assert graph.unique_name(scope, False) == scope, (\n        'Scope \"%s\" already exists. Provide explicit scope names when '\n        'importing multiple instances of the model.') % scope\n    t_input, t_prep_input = self.create_input(t_input, forget_xy_shape)\n    tf.import_graph_def(\n        self.graph_def, {self.input_name: t_prep_input}, name=scope)\n    self.post_import(scope)", "output": "Import model GraphDef into the current graph.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_multiscale_dilated(image, resolutions, num_channels=3):\n  \"\"\"\n  \"\"\"\n  image_height = common_layers.shape_list(image)[0]\n  scaled_images = []\n  for height in resolutions:\n    dilation_rate = image_height // height  # assuming height = width\n    scaled_image = image[::dilation_rate, ::dilation_rate]\n    scaled_image = tf.to_int64(scaled_image)\n    scaled_image.set_shape([None, None, num_channels])\n    scaled_images.append(scaled_image)\n  return scaled_images", "output": "Returns list of scaled images, one for each resolution.\n\n  Resizes by skipping every nth pixel.\n\n  Args:\n    image: Tensor of shape [height, height, num_channels].\n    resolutions: List of heights that image's height is resized to. The function\n      assumes VALID padding, so the original image's height must be divisible\n      by each resolution's height to return the exact resolution size.\n    num_channels: Number of channels in image.\n\n  Returns:\n    List of Tensors, one for each resolution with shape given by\n    [resolutions[i], resolutions[i], num_channels] if resolutions properly\n    divide the original image's height; otherwise shape height and width is up\n    to valid skips.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(self, object_type, object_id):\n        \"\"\"\"\"\"\n        tag_names = request.get_json(force=True)\n        if not tag_names:\n            return Response(status=403)\n\n        db.session.query(TaggedObject).filter(and_(\n            TaggedObject.object_type == object_type,\n            TaggedObject.object_id == object_id),\n            TaggedObject.tag.has(Tag.name.in_(tag_names)),\n        ).delete(synchronize_session=False)\n        db.session.commit()\n\n        return Response(status=204)", "output": "Remove tags from an object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def destroy(name, conn=None, call=None):\n    '''\n    \n    '''\n    if call == 'function':\n        raise SaltCloudSystemExit(\n            'The destroy action must be called with -d, --destroy, '\n            '-a or --action.'\n        )\n\n    __utils__['cloud.fire_event'](\n        'event',\n        'destroying instance',\n        'salt/cloud/{0}/destroying'.format(name),\n        args={'name': name},\n        sock_dir=__opts__['sock_dir'],\n        transport=__opts__['transport']\n    )\n\n    if not conn:\n        conn = get_conn()\n    node = show_instance(name, conn=conn, call='action')\n    log.info('Destroying VM: %s', name)\n    ret = conn.delete_server(name)\n    if ret:\n        log.info('Destroyed VM: %s', name)\n        # Fire destroy action\n        __utils__['cloud.fire_event'](\n            'event',\n            'destroyed instance',\n            'salt/cloud/{0}/destroyed'.format(name),\n            args={'name': name},\n            sock_dir=__opts__['sock_dir'],\n            transport=__opts__['transport']\n        )\n        if __opts__.get('delete_sshkeys', False) is True:\n            __utils__['cloud.remove_sshkey'](getattr(node, __opts__.get('ssh_interface', 'public_ips'))[0])\n        if __opts__.get('update_cachedir', False) is True:\n            __utils__['cloud.delete_minion_cachedir'](name, __active_provider_name__.split(':')[0], __opts__)\n        __utils__['cloud.cachedir_index_del'](name)\n        return True\n\n    log.error('Failed to Destroy VM: %s', name)\n    return False", "output": "Delete a single VM", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cafferesnet101(num_classes=1000, pretrained='imagenet'):\n    \"\"\"\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['cafferesnet101'][pretrained]\n        assert num_classes == settings['num_classes'], \\\n            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n        model.load_state_dict(model_zoo.load_url(settings['url']))\n        model.input_space = settings['input_space']\n        model.input_size = settings['input_size']\n        model.input_range = settings['input_range']\n        model.mean = settings['mean']\n        model.std = settings['std']\n    return model", "output": "Constructs a ResNet-101 model.\n    Args:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(self, path):\n        \"\"\"\n        \n        \"\"\"\n        (bucket, key) = self._path_to_bucket_and_key(path)\n\n        # root always exists\n        if self._is_root(key):\n            return True\n\n        # file\n        if self._exists(bucket, key):\n            return True\n\n        if self.isdir(path):\n            return True\n\n        logger.debug('Path %s does not exist', path)\n        return False", "output": "Does provided path exist on S3?", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def migrate(move_data=True, update_alias=True):\n    \"\"\"\n    \n    \"\"\"\n    # construct a new index name by appending current timestamp\n    next_index = PATTERN.replace('*', datetime.now().strftime('%Y%m%d%H%M%S%f'))\n\n    # get the low level connection\n    es = connections.get_connection()\n\n    # create new index, it will use the settings from the template\n    es.indices.create(index=next_index)\n\n    if move_data:\n        # move data from current alias to the new index\n        es.reindex(\n            body={\"source\": {\"index\": ALIAS}, \"dest\": {\"index\": next_index}},\n            request_timeout=3600\n        )\n        # refresh the index to make the changes visible\n        es.indices.refresh(index=next_index)\n\n    if update_alias:\n        # repoint the alias to point to the newly created index\n        es.indices.update_aliases(body={\n            'actions': [\n                {\"remove\": {\"alias\": ALIAS, \"index\": PATTERN}},\n                {\"add\": {\"alias\": ALIAS, \"index\": next_index}},\n            ]\n        })", "output": "Upgrade function that creates a new index for the data. Optionally it also can\n    (and by default will) reindex previous copy of the data into the new index\n    (specify ``move_data=False`` to skip this step) and update the alias to\n    point to the latest index (set ``update_alias=False`` to skip).\n\n    Note that while this function is running the application can still perform\n    any and all searches without any loss of functionality. It should, however,\n    not perform any writes at this time as those might be lost.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __ensure_type (targets):\n    \"\"\" \n    \"\"\"\n    assert is_iterable_typed(targets, virtual_target.VirtualTarget)\n    for t in targets:\n        if not t.type ():\n            get_manager().errors()(\"target '%s' has no type\" % str (t))", "output": "Ensures all 'targets' have types. If this is not so, exists with\n        error.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_predicate_text(sent_tokens: List[Token], tags: List[str]) -> str:\n    \"\"\"\n    \n    \"\"\"\n    return \" \".join([sent_tokens[pred_id].text\n                     for pred_id in get_predicate_indices(tags)])", "output": "Get the predicate in this prediction.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convolved_1d(iterable, kernel_size=1, stride=1, padding=0, default_value=None):\n    \"\"\"\n    \"\"\"\n    return convolved(iterable, kernel_size, stride, padding, default_value)", "output": "1D Iterable to get every convolution window per loop iteration.\n\n    For more information, refer to:\n    - https://github.com/guillaume-chevalier/python-conv-lib/blob/master/conv/conv.py\n    - https://github.com/guillaume-chevalier/python-conv-lib\n    - MIT License, Copyright (c) 2018 Guillaume Chevalier", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(zpool, *vdevs, **kwargs):\n    '''\n    \n\n    '''\n    ## Configure pool\n    # NOTE: initialize the defaults\n    flags = []\n    target = []\n\n    # NOTE: set extra config based on kwargs\n    if kwargs.get('force', False):\n        flags.append('-f')\n\n    # NOTE: append the pool name and specifications\n    target.append(zpool)\n    target.extend(vdevs)\n\n    ## Update storage pool\n    res = __salt__['cmd.run_all'](\n        __utils__['zfs.zpool_command'](\n            command='add',\n            flags=flags,\n            target=target,\n        ),\n        python_shell=False,\n    )\n\n    ret = __utils__['zfs.parse_command_result'](res, 'added')\n    if ret['added']:\n        ## NOTE: lookup zpool status for vdev config\n        ret['vdevs'] = _clean_vdev_config(\n            __salt__['zpool.status'](zpool=zpool)[zpool]['config'][zpool],\n        )\n\n    return ret", "output": "Add the specified vdev\\'s to the given storage pool\n\n    zpool : string\n        Name of storage pool\n\n    vdevs : string\n        One or more devices\n\n    force : boolean\n        Forces use of device\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zpool.add myzpool /path/to/vdev1 /path/to/vdev2 [...]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_bond(iface):\n    '''\n    \n    '''\n    path = os.path.join(_DEB_NETWORK_CONF_FILES, '{0}.conf'.format(iface))\n    return _read_file(path)", "output": "Return the content of a bond script\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ip.get_bond bond0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def conv_block(name, x, mid_channels, dilations=None, activation=\"relu\",\n               dropout=0.0):\n  \"\"\"\n  \"\"\"\n  with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n\n    x_shape = common_layers.shape_list(x)\n    is_2d = len(x_shape) == 4\n    num_steps = x_shape[1]\n    if is_2d:\n      first_filter = [3, 3]\n      second_filter = [1, 1]\n    else:\n      # special case when number of steps equal 1 to avoid\n      # padding.\n      if num_steps == 1:\n        first_filter = [1, 3, 3]\n      else:\n        first_filter = [2, 3, 3]\n      second_filter = [1, 1, 1]\n\n    # Edge Padding + conv2d + actnorm + relu:\n    # [output: 512 channels]\n    x = conv(\"1_1\", x, output_channels=mid_channels, filter_size=first_filter,\n             dilations=dilations)\n    x = tf.nn.relu(x)\n    x = get_dropout(x, rate=dropout)\n\n    # Padding + conv2d + actnorm + activation.\n    # [input, output: 512 channels]\n    if activation == \"relu\":\n      x = conv(\"1_2\", x, output_channels=mid_channels,\n               filter_size=second_filter, dilations=dilations)\n      x = tf.nn.relu(x)\n    elif activation == \"gatu\":\n      # x = tanh(w1*x) * sigm(w2*x)\n      x_tanh = conv(\"1_tanh\", x, output_channels=mid_channels,\n                    filter_size=second_filter, dilations=dilations)\n      x_sigm = conv(\"1_sigm\", x, output_channels=mid_channels,\n                    filter_size=second_filter, dilations=dilations)\n      x = tf.nn.tanh(x_tanh) * tf.nn.sigmoid(x_sigm)\n\n    x = get_dropout(x, rate=dropout)\n    return x", "output": "2 layer conv block used in the affine coupling layer.\n\n  Args:\n    name: variable scope.\n    x: 4-D or 5-D Tensor.\n    mid_channels: Output channels of the second layer.\n    dilations: Optional, list of integers.\n    activation: relu or gatu.\n      If relu, the second layer is relu(W*x)\n      If gatu, the second layer is tanh(W1*x) * sigmoid(W2*x)\n    dropout: Dropout probability.\n  Returns:\n    x: 4-D Tensor: Output activations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def GetMessages(self, files):\n    # TODO(amauryfa): Fix the differences with MessageFactory.\n    \"\"\"\n    \"\"\"\n\n    def _GetAllMessageNames(desc):\n      \"\"\"Walk a message Descriptor and recursively yields all message names.\"\"\"\n      yield desc.full_name\n      for msg_desc in desc.nested_types:\n        for full_name in _GetAllMessageNames(msg_desc):\n          yield full_name\n\n    result = {}\n    for file_name in files:\n      file_desc = self.pool.FindFileByName(file_name)\n      for msg_desc in file_desc.message_types_by_name.values():\n        for full_name in _GetAllMessageNames(msg_desc):\n          try:\n            result[full_name] = self._classes[full_name]\n          except KeyError:\n            # This descriptor has no registered class, skip it.\n            pass\n    return result", "output": "Gets all registered messages from a specified file.\n\n    Only messages already created and registered will be returned; (this is the\n    case for imported _pb2 modules)\n    But unlike MessageFactory, this version also returns already defined nested\n    messages, but does not register any message extensions.\n\n    Args:\n      files: The file names to extract messages from.\n\n    Returns:\n      A dictionary mapping proto names to the message classes.\n\n    Raises:\n      KeyError: if a file could not be found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_nat_base():\n  \"\"\"\"\"\"\n  hparams = transformer_nat_small()\n  hparams.batch_size = 2048\n  hparams.hidden_size = 512\n  hparams.filter_size = 4096\n  hparams.num_hidden_layers = 6\n  return hparams", "output": "Set of hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def saveAsLibSVMFile(data, dir):\n        \"\"\"\n        \n        \"\"\"\n        lines = data.map(lambda p: MLUtils._convert_labeled_point_to_libsvm(p))\n        lines.saveAsTextFile(dir)", "output": "Save labeled data in LIBSVM format.\n\n        :param data: an RDD of LabeledPoint to be saved\n        :param dir: directory to save the data\n\n        >>> from tempfile import NamedTemporaryFile\n        >>> from fileinput import input\n        >>> from pyspark.mllib.regression import LabeledPoint\n        >>> from glob import glob\n        >>> from pyspark.mllib.util import MLUtils\n        >>> examples = [LabeledPoint(1.1, Vectors.sparse(3, [(0, 1.23), (2, 4.56)])),\n        ...             LabeledPoint(0.0, Vectors.dense([1.01, 2.02, 3.03]))]\n        >>> tempFile = NamedTemporaryFile(delete=True)\n        >>> tempFile.close()\n        >>> MLUtils.saveAsLibSVMFile(sc.parallelize(examples), tempFile.name)\n        >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n        '0.0 1:1.01 2:2.02 3:3.03\\\\n1.1 1:1.23 3:4.56\\\\n'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def intersect(lst1, lst2):\n    '''\n    \n    '''\n    if isinstance(lst1, collections.Hashable) and isinstance(lst2, collections.Hashable):\n        return set(lst1) & set(lst2)\n    return unique([ele for ele in lst1 if ele in lst2])", "output": "Returns the intersection of two lists.\n\n    .. code-block:: jinja\n\n        {% my_list = [1,2,3,4] -%}\n        {{ set my_list | intersect([2, 4, 6]) }}\n\n    will be rendered as:\n\n    .. code-block:: text\n\n        [2, 4]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def draw_tree(t, df, size=10, ratio=0.6, precision=0):\n    \"\"\" \n    \"\"\"\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n                      special_characters=True, rotate=True, precision=precision)\n    IPython.display.display(graphviz.Source(re.sub('Tree {',\n       f'Tree {{ size={size}; ratio={ratio}', s)))", "output": "Draws a representation of a random forest in IPython.\n    Parameters:\n    -----------\n    t: The tree you wish to draw\n    df: The data used to train the tree. This is used to get the names of the features.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _recurmatch(path, aug):\n    '''\n    \n    '''\n    if path:\n        clean_path = path.rstrip('/*')\n        yield (clean_path, aug.get(path))\n\n        for i in aug.match(clean_path + '/*'):\n            i = i.replace('!', '\\\\!')  # escape some dirs\n            for _match in _recurmatch(i, aug):\n                yield _match", "output": "Recursive generator providing the infrastructure for\n    augtools print behavior.\n\n    This function is based on test_augeas.py from\n    Harald Hoyer <harald@redhat.com>  in the python-augeas\n    repository", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validateNotationUse(self, doc, notationName):\n        \"\"\" \"\"\"\n        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        ret = libxml2mod.xmlValidateNotationUse(self._o, doc__o, notationName)\n        return ret", "output": "Validate that the given name match a notation declaration.\n           - [ VC: Notation Declared ]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_rule(self, dates):\n        \"\"\"\n        \n        \"\"\"\n        if self.observance is not None:\n            return dates.map(lambda d: self.observance(d))\n\n        if self.offset is not None:\n            if not isinstance(self.offset, list):\n                offsets = [self.offset]\n            else:\n                offsets = self.offset\n            for offset in offsets:\n\n                # if we are adding a non-vectorized value\n                # ignore the PerformanceWarnings:\n                with warnings.catch_warnings():\n                    warnings.simplefilter(\"ignore\", PerformanceWarning)\n                    dates += offset\n        return dates", "output": "Apply the given offset/observance to a DatetimeIndex of dates.\n\n        Parameters\n        ----------\n        dates : DatetimeIndex\n            Dates to apply the given offset/observance rule\n\n        Returns\n        -------\n        Dates with rules applied", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _options_browser(cfg, ret_config, defaults, virtualname, options):\n    \"\"\"\n    \n    \"\"\"\n\n    for option in options:\n\n        # default place for the option in the config\n        value = _fetch_option(cfg, ret_config, virtualname, options[option])\n\n        if value:\n            yield option, value\n            continue\n\n        # Attribute not found, check for a default value\n        if defaults:\n            if option in defaults:\n                log.info('Using default for %s %s', virtualname, option)\n                yield option, defaults[option]\n                continue\n\n        # fallback (implicit else for all ifs)\n        continue", "output": "Iterator generating all duples ```option name -> value```\n\n    @see :func:`get_returner_options`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_or_generate_tabbed_vocab(data_dir, tmp_dir, source_filename,\n                                 index, vocab_filename, vocab_size):\n  \n  \"\"\"\n  def generate():\n    filepath = os.path.join(tmp_dir, source_filename)\n    tf.logging.info(\"Generating vocab from %s\", filepath)\n    with tf.gfile.GFile(filepath, mode=\"r\") as source_file:\n      for line in source_file:\n        line = line.strip()\n        if line and \"\\t\" in line:\n          parts = line.split(\"\\t\", 1)\n          part = parts[index].strip()\n          yield part\n\n  return get_or_generate_vocab_inner(data_dir, vocab_filename, vocab_size,\n                                     generate())", "output": "r\"\"\"Generate a vocabulary from a tabbed source file.\n\n  The source is a file of source, target pairs, where each line contains\n  a source string and a target string, separated by a tab ('\\t') character.\n  The index parameter specifies 0 for the source or 1 for the target.\n\n  Args:\n    data_dir: path to the data directory.\n    tmp_dir: path to the temporary directory.\n    source_filename: the name of the tab-separated source file.\n    index: index.\n    vocab_filename: the name of the vocabulary file.\n    vocab_size: vocabulary size.\n\n  Returns:\n    The vocabulary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def changed_locations(a, include_first):\n    \"\"\"\n    \n    \"\"\"\n    if a.ndim > 1:\n        raise ValueError(\"indices_of_changed_values only supports 1D arrays.\")\n    indices = flatnonzero(diff(a)) + 1\n\n    if not include_first:\n        return indices\n\n    return hstack([[0], indices])", "output": "Compute indices of values in ``a`` that differ from the previous value.\n\n    Parameters\n    ----------\n    a : np.ndarray\n        The array on which to indices of change.\n    include_first : bool\n        Whether or not to consider the first index of the array as \"changed\".\n\n    Example\n    -------\n    >>> import numpy as np\n    >>> changed_locations(np.array([0, 0, 5, 5, 1, 1]), include_first=False)\n    array([2, 4])\n\n    >>> changed_locations(np.array([0, 0, 5, 5, 1, 1]), include_first=True)\n    array([0, 2, 4])", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pow4(x, alpha, a, b, c):\n    \"\"\"\n    \"\"\"\n    return c - (a*x+b)**-alpha", "output": "pow4\n\n    Parameters\n    ----------\n    x: int\n    alpha: float\n    a: float\n    b: float\n    c: float\n\n    Returns\n    -------\n    float\n        c - (a*x+b)**-alpha", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_gecos(name, key, value, root=None):\n    '''\n    \n    '''\n    if value is None:\n        value = ''\n    elif not isinstance(value, six.string_types):\n        value = six.text_type(value)\n    else:\n        value = salt.utils.stringutils.to_unicode(value)\n    pre_info = _get_gecos(name, root=root)\n    if not pre_info:\n        return False\n    if value == pre_info[key]:\n        return True\n    gecos_data = copy.deepcopy(pre_info)\n    gecos_data[key] = value\n\n    cmd = ['usermod']\n    if root is not None and __grains__['kernel'] != 'AIX':\n        cmd.extend(('-R', root))\n    cmd.extend(('-c', _build_gecos(gecos_data), name))\n\n    __salt__['cmd.run'](cmd, python_shell=False)\n    return _get_gecos(name, root=root).get(key) == value", "output": "Common code to change a user's GECOS information", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _simplify_shape(self, alist, rec=0):\r\n        \"\"\"\"\"\"\r\n        if rec != 0:\r\n            if len(alist) == 1:\r\n                return alist[-1]\r\n            return alist\r\n        if len(alist) == 1:\r\n            return self._simplify_shape(alist[-1], 1)\r\n        return [self._simplify_shape(al, 1) for al in alist]", "output": "Reduce the alist dimension if needed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def str_to_time(time_str: str) -> datetime.datetime:\n    \"\"\"\n    \n    \"\"\"\n    pieces: Any = [int(piece) for piece in time_str.split('-')]\n    return datetime.datetime(*pieces)", "output": "Convert human readable string to datetime.datetime.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_metric(self, eval_metric, labels, pre_sliced=False):\n        \"\"\"\n        \"\"\"\n        assert self.binded and self.params_initialized\n\n        for meta, module in zip(self._metas, self._modules):\n            if SequentialModule.META_TAKE_LABELS in meta and \\\n                    meta[SequentialModule.META_TAKE_LABELS]:\n                module.update_metric(eval_metric, labels, pre_sliced)", "output": "Evaluates and accumulates evaluation metric on outputs of the last forward computation.\n\n        Parameters\n        ----------\n        eval_metric : EvalMetric\n        labels : list of NDArray\n            Typically ``data_batch.label``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toggle_icontext(self, state):\r\n        \"\"\"\"\"\"\r\n        self.sig_option_changed.emit('show_icontext', state)\r\n        for widget in self.action_widgets:\r\n            if widget is not self.button_menu:\r\n                if state:\r\n                    widget.setToolButtonStyle(Qt.ToolButtonTextBesideIcon)\r\n                else:\r\n                    widget.setToolButtonStyle(Qt.ToolButtonIconOnly)", "output": "Toggle icon text", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_file_path(self, name, relative_path):\n        \"\"\"\n        \n        \"\"\"\n        dist = self.get_distribution(name)\n        if dist is None:\n            raise LookupError('no distribution named %r found' % name)\n        return dist.get_resource_path(relative_path)", "output": "Return the path to a resource file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def disable_auto_login():\n    '''\n    \n    '''\n    # Remove the kcpassword file\n    cmd = 'rm -f /etc/kcpassword'\n    __salt__['cmd.run'](cmd)\n\n    # Remove the entry from the defaults file\n    cmd = ['defaults',\n           'delete',\n           '/Library/Preferences/com.apple.loginwindow.plist',\n           'autoLoginUser']\n    __salt__['cmd.run'](cmd)\n    return True if not get_auto_login() else False", "output": ".. versionadded:: 2016.3.0\n\n    Disables auto login on the machine\n\n    Returns:\n        bool: ``True`` if successful, otherwise ``False``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.disable_auto_login", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _del_sub_prop(container, keys):\n    \"\"\"\n    \"\"\"\n    sub_val = container\n    for key in keys[:-1]:\n        if key not in sub_val:\n            sub_val[key] = {}\n        sub_val = sub_val[key]\n    if keys[-1] in sub_val:\n        del sub_val[keys[-1]]", "output": "Remove a nested key fro a dictionary.\n\n    Arguments:\n        container (dict):\n            A dictionary which may contain other dictionaries as values.\n        keys (iterable):\n            A sequence of keys to attempt to clear the value for. Each item in\n            the sequence represents a deeper nesting. The first key is for\n            the top level. If there is a dictionary there, the second key\n            attempts to get the value within that, and so on.\n\n    Examples:\n        Remove a top-level value (equivalent to ``del container['key']``).\n\n        >>> container = {'key': 'value'}\n        >>> _del_sub_prop(container, ['key'])\n        >>> container\n        {}\n\n        Remove a nested value.\n\n        >>> container = {'key': {'subkey': 'value'}}\n        >>> _del_sub_prop(container, ['key', 'subkey'])\n        >>> container\n        {'key': {}}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_from_pb(self, instance_pb):\n        \"\"\"\n        \"\"\"\n        if not instance_pb.display_name:  # Simple field (string)\n            raise ValueError(\"Instance protobuf does not contain display_name\")\n        self.display_name = instance_pb.display_name\n        self.type_ = instance_pb.type\n        self.labels = dict(instance_pb.labels)\n        self._state = instance_pb.state", "output": "Refresh self from the server-provided protobuf.\n        Helper for :meth:`from_pb` and :meth:`reload`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tab_navigate(self, delta=1):\r\n        \"\"\"\"\"\"\r\n        if delta > 0 and self.currentIndex() == self.count()-1:\r\n            index = delta-1\r\n        elif delta < 0 and self.currentIndex() == 0:\r\n            index = self.count()+delta\r\n        else:\r\n            index = self.currentIndex()+delta\r\n        self.setCurrentIndex(index)", "output": "Ctrl+Tab", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_transaction(self, transaction):\n        \"\"\"\n        \"\"\"\n        asset = transaction.asset\n        if isinstance(asset, Future):\n            try:\n                old_price = self._payout_last_sale_prices[asset]\n            except KeyError:\n                self._payout_last_sale_prices[asset] = transaction.price\n            else:\n                position = self.position_tracker.positions[asset]\n                amount = position.amount\n                price = transaction.price\n\n                self._cash_flow(\n                    self._calculate_payout(\n                        asset.price_multiplier,\n                        amount,\n                        old_price,\n                        price,\n                    ),\n                )\n\n                if amount + transaction.amount == 0:\n                    del self._payout_last_sale_prices[asset]\n                else:\n                    self._payout_last_sale_prices[asset] = price\n        else:\n            self._cash_flow(-(transaction.price * transaction.amount))\n\n        self.position_tracker.execute_transaction(transaction)\n\n        # we only ever want the dict form from now on\n        transaction_dict = transaction.to_dict()\n        try:\n            self._processed_transactions[transaction.dt].append(\n                transaction_dict,\n            )\n        except KeyError:\n            self._processed_transactions[transaction.dt] = [transaction_dict]", "output": "Add a transaction to ledger, updating the current state as needed.\n\n        Parameters\n        ----------\n        transaction : zp.Transaction\n            The transaction to execute.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_tables(self):\n        \"\"\"\n        \n        \"\"\"\n        tables = self._parse_tables(self._build_doc(), self.match, self.attrs)\n        return (self._parse_thead_tbody_tfoot(table) for table in tables)", "output": "Parse and return all tables from the DOM.\n\n        Returns\n        -------\n        list of parsed (header, body, footer) tuples from tables.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_loading_page(self):\r\n        \"\"\"\"\"\"\r\n        loading_template = Template(LOADING)\r\n        loading_img = get_image_path('loading_sprites.png')\r\n        if os.name == 'nt':\r\n            loading_img = loading_img.replace('\\\\', '/')\r\n        message = _(\"Connecting to kernel...\")\r\n        page = loading_template.substitute(css_path=self.css_path,\r\n                                           loading_img=loading_img,\r\n                                           message=message)\r\n        return page", "output": "Create html page to show while the kernel is starting", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def charset(self) -> Optional[str]:\n        \"\"\"\"\"\"\n        raw = self._headers.get(hdrs.CONTENT_TYPE)  # type: ignore\n        if self._stored_content_type != raw:\n            self._parse_content_type(raw)\n        return self._content_dict.get('charset')", "output": "The value of charset part for Content-Type HTTP header.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_socket(self):\n        '''\n        \n        '''\n        if hasattr(self, '_socket'):\n            if isinstance(self.poller.sockets, dict):\n                sockets = list(self.poller.sockets.keys())\n                for socket in sockets:\n                    log.trace('Unregistering socket: %s', socket)\n                    self.poller.unregister(socket)\n            else:\n                for socket in self.poller.sockets:\n                    log.trace('Unregistering socket: %s', socket)\n                    self.poller.unregister(socket[0])\n            del self._socket", "output": "delete socket if you have it", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_date(date):\n    '''\n    \n    '''\n    date_format = _get_date_time_format(date)\n    dt_obj = datetime.strptime(date, date_format)\n\n    cmd = 'systemsetup -setdate {0}'.format(dt_obj.strftime('%m:%d:%Y'))\n    return salt.utils.mac_utils.execute_return_success(cmd)", "output": "Set the current month, day, and year\n\n    :param str date: The date to set. Valid date formats are:\n\n        - %m:%d:%y\n        - %m:%d:%Y\n        - %m/%d/%y\n        - %m/%d/%Y\n\n    :return: True if successful, False if not\n    :rtype: bool\n\n    :raises: SaltInvocationError on Invalid Date format\n    :raises: CommandExecutionError on failure\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' timezone.set_date 1/13/2016", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_per_pixel_mean(self, names=('train', 'test')):\n        \"\"\"\n        \n        \"\"\"\n        for name in names:\n            assert name in ['train', 'test'], name\n        train_files, test_files, _ = get_filenames(self.dir, self.cifar_classnum)\n        all_files = []\n        if 'train' in names:\n            all_files.extend(train_files)\n        if 'test' in names:\n            all_files.extend(test_files)\n        all_imgs = [x[0] for x in read_cifar(all_files, self.cifar_classnum)]\n        arr = np.array(all_imgs, dtype='float32')\n        mean = np.mean(arr, axis=0)\n        return mean", "output": "Args:\n            names (tuple[str]): the names ('train' or 'test') of the datasets\n\n        Returns:\n            a mean image of all images in the given datasets, with size 32x32x3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_datafeed(self, datafeed_id, body, params=None):\n        \"\"\"\n        \n        \"\"\"\n        for param in (datafeed_id, body):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"POST\",\n            _make_path(\"_ml\", \"datafeeds\", datafeed_id, \"_update\"),\n            params=params,\n            body=body,\n        )", "output": "`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-update-datafeed.html>`_\n\n        :arg datafeed_id: The ID of the datafeed to update\n        :arg body: The datafeed update settings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_serializer_field(field, is_input=True):\n    \"\"\"\n    \n    \"\"\"\n\n    graphql_type = get_graphene_type_from_serializer_field(field)\n\n    args = []\n    kwargs = {\"description\": field.help_text, \"required\": is_input and field.required}\n\n    # if it is a tuple or a list it means that we are returning\n    # the graphql type and the child type\n    if isinstance(graphql_type, (list, tuple)):\n        kwargs[\"of_type\"] = graphql_type[1]\n        graphql_type = graphql_type[0]\n\n    if isinstance(field, serializers.ModelSerializer):\n        if is_input:\n            graphql_type = convert_serializer_to_input_type(field.__class__)\n        else:\n            global_registry = get_global_registry()\n            field_model = field.Meta.model\n            args = [global_registry.get_type_for_model(field_model)]\n    elif isinstance(field, serializers.ListSerializer):\n        field = field.child\n        if is_input:\n            kwargs[\"of_type\"] = convert_serializer_to_input_type(field.__class__)\n        else:\n            del kwargs[\"of_type\"]\n            global_registry = get_global_registry()\n            field_model = field.Meta.model\n            args = [global_registry.get_type_for_model(field_model)]\n\n    return graphql_type(*args, **kwargs)", "output": "Converts a django rest frameworks field to a graphql field\n    and marks the field as required if we are creating an input type\n    and the field itself is required", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_enabled():\n    '''\n    \n    '''\n    ret = set()\n    for name in _iter_service_names():\n        if _service_is_upstart(name):\n            if _upstart_is_enabled(name):\n                ret.add(name)\n        else:\n            if _service_is_sysv(name):\n                if _sysv_is_enabled(name):\n                    ret.add(name)\n    return sorted(ret)", "output": "Return the enabled services\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.get_enabled", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close(\n        self,\n        exc_info: Union[\n            None,\n            bool,\n            BaseException,\n            Tuple[\n                \"Optional[Type[BaseException]]\",\n                Optional[BaseException],\n                Optional[TracebackType],\n            ],\n        ] = False,\n    ) -> None:\n        \"\"\"\n        \"\"\"\n        if not self.closed():\n            if exc_info:\n                if isinstance(exc_info, tuple):\n                    self.error = exc_info[1]\n                elif isinstance(exc_info, BaseException):\n                    self.error = exc_info\n                else:\n                    exc_info = sys.exc_info()\n                    if any(exc_info):\n                        self.error = exc_info[1]\n            if self._read_until_close:\n                self._read_until_close = False\n                self._finish_read(self._read_buffer_size, False)\n            if self._state is not None:\n                self.io_loop.remove_handler(self.fileno())\n                self._state = None\n            self.close_fd()\n            self._closed = True\n        self._signal_closed()", "output": "Close this stream.\n\n        If ``exc_info`` is true, set the ``error`` attribute to the current\n        exception from `sys.exc_info` (or if ``exc_info`` is a tuple,\n        use that instead of `sys.exc_info`).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_usage(self, ctx, formatter):\n        \"\"\"\"\"\"\n        pieces = self.collect_usage_pieces(ctx)\n        formatter.write_usage(ctx.command_path, ' '.join(pieces))", "output": "Writes the usage line into the formatter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset(self):\n        \"\"\"\n        \n        \"\"\"\n        if getattr(self, 'num', None) is None:\n            self.num_inst = 0\n            self.sum_metric = 0.0\n        else:\n            self.num_inst = [0] * self.num\n            self.sum_metric = [0.0] * self.num", "output": "override reset behavior", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flatten(struct):\n    \"\"\"\n    \n    \"\"\"\n    if struct is None:\n        return []\n    flat = []\n    if isinstance(struct, dict):\n        for _, result in six.iteritems(struct):\n            flat += flatten(result)\n        return flat\n    if isinstance(struct, six.string_types):\n        return [struct]\n\n    try:\n        # if iterable\n        iterator = iter(struct)\n    except TypeError:\n        return [struct]\n\n    for result in iterator:\n        flat += flatten(result)\n    return flat", "output": "Creates a flat list of all all items in structured output (dicts, lists, items):\n\n    .. code-block:: python\n\n        >>> sorted(flatten({'a': 'foo', 'b': 'bar'}))\n        ['bar', 'foo']\n        >>> sorted(flatten(['foo', ['bar', 'troll']]))\n        ['bar', 'foo', 'troll']\n        >>> flatten('foo')\n        ['foo']\n        >>> flatten(42)\n        [42]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def connect_options_namespaced_service_proxy(self, name, namespace, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_options_namespaced_service_proxy_with_http_info(name, namespace, **kwargs)\n        else:\n            (data) = self.connect_options_namespaced_service_proxy_with_http_info(name, namespace, **kwargs)\n            return data", "output": "connect OPTIONS requests to proxy of Service\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_options_namespaced_service_proxy(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ServiceProxyOptions (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str path: Path is the part of URLs that include service endpoints, suffixes, and parameters to use for the current proxy request to service. For example, the whole request URL is http://localhost/api/v1/namespaces/kube-system/services/elasticsearch-logging/_search?q=user:kimchy. Path is _search?q=user:kimchy.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sort(self, session_groups):\n    \"\"\"\"\"\"\n\n    # Sort by session_group name so we have a deterministic order.\n    session_groups.sort(key=operator.attrgetter('name'))\n    # Sort by lexicographical order of the _request.col_params whose order\n    # is not ORDER_UNSPECIFIED. The first such column is the primary sorting\n    # key, the second is the secondary sorting key, etc. To achieve that we\n    # need to iterate on these columns in reverse order (thus the primary key\n    # is the key used in the last sort).\n    for col_param, extractor in reversed(list(zip(self._request.col_params,\n                                                  self._extractors))):\n      if col_param.order == api_pb2.ORDER_UNSPECIFIED:\n        continue\n      if col_param.order == api_pb2.ORDER_ASC:\n        session_groups.sort(\n            key=_create_key_func(\n                extractor,\n                none_is_largest=not col_param.missing_values_first))\n      elif col_param.order == api_pb2.ORDER_DESC:\n        session_groups.sort(\n            key=_create_key_func(\n                extractor,\n                none_is_largest=col_param.missing_values_first),\n            reverse=True)\n      else:\n        raise error.HParamsError('Unknown col_param.order given: %s' %\n                                 col_param)", "output": "Sorts 'session_groups' in place according to _request.col_params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def additions_removed(name, force=False):\n    '''\n    \n    '''\n    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''}\n    current_state = __salt__['vbox_guest.additions_version']()\n    if not current_state:\n        ret['result'] = True\n        ret['comment'] = 'System already in the correct state'\n        return ret\n    if __opts__['test']:\n        ret['comment'] = ('The state of VirtualBox Guest Additions will be '\n                          'changed.')\n        ret['changes'] = {\n            'old': current_state,\n            'new': True,\n        }\n        ret['result'] = None\n        return ret\n\n    new_state = __salt__['vbox_guest.additions_remove'](force=force)\n\n    ret['comment'] = 'The state of VirtualBox Guest Additions was changed!'\n    ret['changes'] = {\n        'old': current_state,\n        'new': new_state,\n    }\n    ret['result'] = bool(new_state)\n    return ret", "output": "Ensure that the VirtualBox Guest Additions are removed. Uses the CD,\n    connected by VirtualBox.\n\n    To connect VirtualBox Guest Additions via VirtualBox graphical interface\n    press 'Host+D' ('Host' is usually 'Right Ctrl').\n\n    name\n        The name has no functional value and is only used as a tracking\n        reference.\n    force\n        Force VirtualBox Guest Additions removing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shard(items, num_shards):\n  \"\"\"\"\"\"\n  sharded = []\n  num_per_shard = len(items) // num_shards\n  start = 0\n  for _ in range(num_shards):\n    sharded.append(items[start:start + num_per_shard])\n    start += num_per_shard\n\n  remainder = len(items) % num_shards\n  start = len(items) - remainder\n  for i in range(remainder):\n    sharded[i].append(items[start + i])\n\n  assert sum([len(fs) for fs in sharded]) == len(items)\n  return sharded", "output": "Split items into num_shards groups.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rank(self, **kwargs):\n        \"\"\"\n        \"\"\"\n        axis = kwargs.get(\"axis\", 0)\n        numeric_only = True if axis else kwargs.get(\"numeric_only\", False)\n        func = self._prepare_method(pandas.DataFrame.rank, **kwargs)\n        new_data = self._map_across_full_axis(axis, func)\n        # Since we assume no knowledge of internal state, we get the columns\n        # from the internal partitions.\n        if numeric_only:\n            new_columns = self.compute_index(1, new_data, True)\n        else:\n            new_columns = self.columns\n        new_dtypes = pandas.Series([np.float64 for _ in new_columns], index=new_columns)\n        return self.__constructor__(new_data, self.index, new_columns, new_dtypes)", "output": "Computes numerical rank along axis. Equal values are set to the average.\n\n        Returns:\n            DataManager containing the ranks of the values along an axis.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def salt_config_to_yaml(configuration, line_break='\\n'):\n    '''\n    \n    '''\n    return salt.utils.yaml.safe_dump(\n        configuration,\n        line_break=line_break,\n        default_flow_style=False)", "output": "Return a salt configuration dictionary, master or minion, as a yaml dump", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_intent(intent_request, session):\n    \"\"\"  \"\"\"\n\n    print(\"on_intent requestId=\" + intent_request['requestId'] +\n          \", sessionId=\" + session['sessionId'])\n\n    intent = intent_request['intent']\n    intent_name = intent_request['intent']['name']\n\n    # Dispatch to your skill's intent handlers\n    if intent_name == \"MyColorIsIntent\":\n        return set_color_in_session(intent, session)\n    elif intent_name == \"WhatsMyColorIntent\":\n        return get_color_from_session(intent, session)\n    elif intent_name == \"AMAZON.HelpIntent\":\n        return get_welcome_response()\n    elif intent_name == \"AMAZON.CancelIntent\" or intent_name == \"AMAZON.StopIntent\":\n        return handle_session_end_request()\n    else:\n        raise ValueError(\"Invalid intent\")", "output": "Called when the user specifies an intent for this skill", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __convert_order_params_for_blotter(asset,\n                                           limit_price,\n                                           stop_price,\n                                           style):\n        \"\"\"\n        \n        \"\"\"\n        if style:\n            assert (limit_price, stop_price) == (None, None)\n            return style\n        if limit_price and stop_price:\n            return StopLimitOrder(limit_price, stop_price, asset=asset)\n        if limit_price:\n            return LimitOrder(limit_price, asset=asset)\n        if stop_price:\n            return StopOrder(stop_price, asset=asset)\n        else:\n            return MarketOrder()", "output": "Helper method for converting deprecated limit_price and stop_price\n        arguments into ExecutionStyle instances.\n\n        This function assumes that either style == None or (limit_price,\n        stop_price) == (None, None).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_site(name):\n    '''\n    \n\n    '''\n    current_sites = list_sites()\n\n    if name not in current_sites:\n        log.debug('Site already absent: %s', name)\n        return True\n\n    ps_cmd = ['Remove-WebSite', '-Name', r\"'{0}'\".format(name)]\n\n    cmd_ret = _srvmgr(ps_cmd)\n\n    if cmd_ret['retcode'] != 0:\n        msg = 'Unable to remove site: {0}\\nError: {1}' \\\n              ''.format(name, cmd_ret['stderr'])\n        raise CommandExecutionError(msg)\n\n    log.debug('Site removed successfully: %s', name)\n    return True", "output": "Delete a website from IIS.\n\n    Args:\n        name (str): The IIS site name.\n\n    Returns:\n        bool: True if successful, otherwise False\n\n    .. note::\n\n        This will not remove the application pool used by the site.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' win_iis.remove_site name='My Test Site'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def first_fit(self, train_x, train_y):\n        \"\"\"  \"\"\"\n        train_x, train_y = np.array(train_x), np.array(train_y)\n\n        self._x = np.copy(train_x)\n        self._y = np.copy(train_y)\n\n        self._distance_matrix = edit_distance_matrix(self._x)\n        k_matrix = bourgain_embedding_matrix(self._distance_matrix)\n        k_matrix[np.diag_indices_from(k_matrix)] += self.alpha\n\n        self._l_matrix = cholesky(k_matrix, lower=True)  # Line 2\n\n        self._alpha_vector = cho_solve((self._l_matrix, True), self._y)  # Line 3\n\n        self._first_fitted = True\n        return self", "output": "Fit the regressor for the first time.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def summary(self, name=None):\n        \"\"\"\n        \n        \"\"\"\n        warnings.warn(\"'summary' is deprecated and will be removed in a \"\n                      \"future version.\", FutureWarning, stacklevel=2)\n        return self._summary(name)", "output": "Return a summarized representation.\n\n        .. deprecated:: 0.23.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_domains():\n    '''\n    \n    '''\n    with _get_xapi_session() as xapi:\n        hosts = xapi.VM.get_all()\n        ret = []\n\n        for _host in hosts:\n            if xapi.VM.get_record(_host)['is_control_domain'] is False:\n                ret.append(xapi.VM.get_name_label(_host))\n\n        return ret", "output": "Return a list of virtual machine names on the minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' virt.list_domains", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_secrets(namespace, name=\"\", apiserver_url=None, decode=False, brief=False):\n    '''\n    \n\n    '''\n    # Try to get kubernetes master\n    apiserver_url = _guess_apiserver(apiserver_url)\n    if apiserver_url is None:\n        return False\n\n    # Get data\n    if not decode:\n        ret = _get_secrets(namespace, name, apiserver_url)\n    else:\n        ret = _decode_secrets(_get_secrets(namespace, name, apiserver_url))\n    return ret", "output": "Get k8s namespaces\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' k8s.get_secrets namespace_name\n        salt '*' k8s.get_secrets namespace_name secret_name http://kube-master.cluster.local", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert(self, values, nan_rep, encoding, errors):\n        \"\"\"  \"\"\"\n\n        self.values = Int64Index(np.arange(self.table.nrows))\n        return self", "output": "set the values from this selection: take = take ownership", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def name(self):\n        \"\"\"\n        \"\"\"\n        ret = ctypes.c_char_p()\n        success = ctypes.c_int()\n        check_call(_LIB.MXSymbolGetName(\n            self.handle, ctypes.byref(ret), ctypes.byref(success)))\n        if success.value != 0:\n            return py_str(ret.value)\n        else:\n            return None", "output": "Gets name string from the symbol, this function only works for non-grouped symbol.\n\n        Returns\n        -------\n        value : str\n            The name of this symbol, returns ``None`` for grouped symbol.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _username():\n    '''\n    \n    '''\n    if pwd:\n        username = pwd.getpwuid(os.getuid()).pw_name\n    else:\n        username = getpass.getuser()\n\n    return username", "output": "Grain for the minion username", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def preprocess(from_idx, to_idx, _params):\n    \"\"\"\n    \n    \"\"\"\n    source_exts = '*.mpg'\n    src_path = _params['src_path']\n    tgt_path = _params['tgt_path']\n    face_predictor_path = './shape_predictor_68_face_landmarks.dat'\n\n    succ = set()\n    fail = set()\n    for idx in range(from_idx, to_idx):\n        s_id = 's' + str(idx) + '/'\n        source_path = src_path + '/' + s_id\n        target_path = tgt_path + '/' + s_id\n        fail_cnt = 0\n        for filepath in find_files(source_path, source_exts):\n            print(\"Processing: {}\".format(filepath))\n            filepath_wo_ext = os.path.splitext(filepath)[0].split('/')[-2:]\n            target_dir = os.path.join(tgt_path, '/'.join(filepath_wo_ext))\n\n            if os.path.exists(target_dir):\n                continue\n\n            try:\n                video = Video(vtype='face', \\\n                                face_predictor_path=face_predictor_path).from_video(filepath)\n                mkdir_p(target_dir)\n                i = 0\n                if video.mouth[0] is None:\n                    continue\n                for frame in video.mouth:\n                    io.imsave(os.path.join(target_dir, \"mouth_{0:03d}.png\".format(i)), frame)\n                    i += 1\n            except ValueError as error:\n                print(error)\n                fail_cnt += 1\n        if fail_cnt == 0:\n            succ.add(idx)\n        else:\n            fail.add(idx)\n    return (succ, fail)", "output": "Preprocess: Convert a video into the mouth images", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_port(self, port, name, admin_state_up=True):\n        '''\n        \n        '''\n        port_id = self._find_port_id(port)\n        body = {'name': name,\n                'admin_state_up': admin_state_up}\n        return self.network_conn.update_port(port=port_id,\n                                             body={'port': body})", "output": "Updates a port", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset(self, indices=None):\n    \"\"\"\n    \"\"\"\n    if self._store_rollouts and self.current_epoch is None:\n      raise ValueError(\n          \"No current epoch. start_new_epoch() should first be called.\"\n      )\n\n    if indices is None:\n      indices = np.arange(self.batch_size)\n    new_obs = self._reset(indices)\n    if self._should_preprocess_on_reset:\n      new_obs = self._preprocess_observations(new_obs)\n    if self._store_rollouts:\n      encoded_obs = self._encode_observations(new_obs)\n      for (index, ob) in zip(indices, encoded_obs):\n        frame = self._current_batch_frames[index]\n        if frame is not None:\n          rollout = self._current_batch_rollouts[index]\n          rollout.append(frame._replace(action=0))\n          self._current_epoch_rollouts.append(rollout)\n          self._current_batch_rollouts[index] = []\n        self._current_batch_frames[index] = Frame(\n            observation=ob, reward=0, unclipped_reward=0, done=False,\n            action=None\n        )\n    return new_obs", "output": "Resets environments at given indices.\n\n    Does any preprocessing and adds rollouts to history.\n\n    Args:\n      indices: Indices of environments to reset.\n\n    Returns:\n      Batch of initial observations of reset environments.\n\n    Raises:\n      ValueError: when there's no current epoch.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sniff_hosts(self, initial=False):\n        \"\"\"\n        \n        \"\"\"\n        node_info = self._get_sniff_data(initial)\n\n        hosts = list(filter(None, (self._get_host_info(n) for n in node_info)))\n\n        # we weren't able to get any nodes or host_info_callback blocked all -\n        # raise error.\n        if not hosts:\n            raise TransportError(\"N/A\", \"Unable to sniff hosts - no viable hosts found.\")\n\n        self.set_connections(hosts)", "output": "Obtain a list of nodes from the cluster and create a new connection\n        pool using the information retrieved.\n\n        To extract the node connection parameters use the ``nodes_to_host_callback``.\n\n        :arg initial: flag indicating if this is during startup\n            (``sniff_on_start``), ignore the ``sniff_timeout`` if ``True``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _connect(host=None, port=None, db=None, password=None):\n    '''\n    \n    '''\n    if not host:\n        host = __salt__['config.option']('redis.host')\n    if not port:\n        port = __salt__['config.option']('redis.port')\n    if not db:\n        db = __salt__['config.option']('redis.db')\n    if not password:\n        password = __salt__['config.option']('redis.password')\n\n    return redis.StrictRedis(host, port, db, password, decode_responses=True)", "output": "Returns an instance of the redis client", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def refresh_plugin(self):\r\n        \"\"\"\"\"\"\r\n        client = None\r\n        if self.tabwidget.count():\r\n            client = self.tabwidget.currentWidget()\r\n\r\n            # Decide what to show for each client\r\n            if client.info_page != client.blank_page:\r\n                # Show info_page if it has content\r\n                client.set_info_page()\r\n                client.shellwidget.hide()\r\n                client.layout.addWidget(self.infowidget)\r\n                self.infowidget.show()\r\n            else:\r\n                self.infowidget.hide()\r\n                client.shellwidget.show()\r\n\r\n            # Give focus to the control widget of the selected tab\r\n            control = client.get_control()\r\n            control.setFocus()\r\n\r\n            # Create corner widgets\r\n            buttons = [[b, -7] for b in client.get_toolbar_buttons()]\r\n            buttons = sum(buttons, [])[:-1]\r\n            widgets = [client.create_time_label()] + buttons\r\n        else:\r\n            control = None\r\n            widgets = []\r\n        self.find_widget.set_editor(control)\r\n        self.tabwidget.set_corner_widgets({Qt.TopRightCorner: widgets})\r\n        if client:\r\n            sw = client.shellwidget\r\n            self.main.variableexplorer.set_shellwidget_from_id(id(sw))\r\n            self.main.plots.set_shellwidget_from_id(id(sw))\r\n            self.main.help.set_shell(sw)\r\n        self.update_tabs_text()\r\n        self.sig_update_plugin_title.emit()", "output": "Refresh tabwidget", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sync_params_from_devices(self):\n        \"\"\"\n\n        \"\"\"\n        self._exec_group.get_params(self._arg_params, self._aux_params)\n        if self._kvstore and self._update_on_kvstore:\n            for param_name, param_val in sorted(self._arg_params.items()):\n                if param_val.stype == 'row_sparse':\n                    row_ids = nd.arange(0, param_val.shape[0], dtype='int64')\n                    self._kvstore.row_sparse_pull(param_name, param_val, row_ids=row_ids)\n        self._params_dirty = False", "output": "Synchronizes parameters from devices to CPU. This function should be called after\n        calling `update` that updates the parameters on the devices, before one can read the\n        latest parameters from ``self._arg_params`` and ``self._aux_params``.\n\n        For row_sparse parameters on devices, ther are pulled from KVStore with all row ids.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def role_assignment_path(cls, project, incident, role_assignment):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/incidents/{incident}/roleAssignments/{role_assignment}\",\n            project=project,\n            incident=incident,\n            role_assignment=role_assignment,\n        )", "output": "Return a fully-qualified role_assignment string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def TTA(self, n_aug=4, is_test=False):\n        \"\"\" \n        \"\"\"\n        dl1 = self.data.test_dl     if is_test else self.data.val_dl\n        dl2 = self.data.test_aug_dl if is_test else self.data.aug_dl\n        preds1,targs = predict_with_targs(self.model, dl1)\n        preds1 = [preds1]*math.ceil(n_aug/4)\n        preds2 = [predict_with_targs(self.model, dl2)[0] for i in tqdm(range(n_aug), leave=False)]\n        return np.stack(preds1+preds2), targs", "output": "Predict with Test Time Augmentation (TTA)\n\n        Additional to the original test/validation images, apply image augmentation to them\n        (just like for training images) and calculate the mean of predictions. The intent\n        is to increase the accuracy of predictions by examining the images using multiple\n        perspectives.\n\n\n            n_aug: a number of augmentation images to use per original image\n            is_test: indicate to use test images; otherwise use validation images\n\n        Returns:\n            (tuple): a tuple containing:\n\n                log predictions (numpy.ndarray): log predictions (i.e. `np.exp(log_preds)` will return probabilities)\n                targs (numpy.ndarray): target values when `is_test==False`; zeros otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_timeout(self, msecs: int) -> None:\n        \"\"\"\"\"\"\n        if self._timeout is not None:\n            self.io_loop.remove_timeout(self._timeout)\n        self._timeout = self.io_loop.add_timeout(\n            self.io_loop.time() + msecs / 1000.0, self._handle_timeout\n        )", "output": "Called by libcurl to schedule a timeout.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bot_has_role(item):\n    \"\"\"\n    \"\"\"\n\n    def predicate(ctx):\n        ch = ctx.channel\n        if not isinstance(ch, discord.abc.GuildChannel):\n            raise NoPrivateMessage()\n\n        me = ch.guild.me\n        if isinstance(item, int):\n            role = discord.utils.get(me.roles, id=item)\n        else:\n            role = discord.utils.get(me.roles, name=item)\n        if role is None:\n            raise BotMissingRole(item)\n        return True\n    return check(predicate)", "output": "Similar to :func:`.has_role` except checks if the bot itself has the\n    role.\n\n    This check raises one of two special exceptions, :exc:`.BotMissingRole` if the bot\n    is missing the role, or :exc:`.NoPrivateMessage` if it is used in a private message.\n    Both inherit from :exc:`.CheckFailure`.\n\n    .. versionchanged:: 1.1.0\n\n        Raise :exc:`.BotMissingRole` or :exc:`.NoPrivateMessage`\n        instead of generic :exc:`.CheckFailure`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_public_lan(lan_id):\n    '''\n    \n    '''\n    conn = get_conn()\n    datacenter_id = get_datacenter_id()\n\n    try:\n        lan = conn.get_lan(datacenter_id=datacenter_id, lan_id=lan_id)\n        if not lan['properties']['public']:\n            conn.update_lan(datacenter_id=datacenter_id,\n                            lan_id=lan_id,\n                            public=True)\n        return lan['id']\n    except Exception:\n        lan = conn.create_lan(datacenter_id,\n                              LAN(public=True,\n                                  name='Public LAN'))\n        return lan['id']", "output": "Enables public Internet access for the specified public_lan. If no public\n    LAN is available, then a new public LAN is created.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_ports(zone, permanent=True):\n    '''\n    \n    '''\n    cmd = '--zone={0} --list-ports'.format(zone)\n\n    if permanent:\n        cmd += ' --permanent'\n\n    return __firewall_cmd(cmd).split()", "output": "List all ports in a zone.\n\n    .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' firewalld.list_ports", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_content_disposition(self,\n                                disptype: str,\n                                quote_fields: bool=True,\n                                **params: Any) -> None:\n        \"\"\"\"\"\"\n        self._headers[hdrs.CONTENT_DISPOSITION] = content_disposition_header(\n            disptype, quote_fields=quote_fields, **params)", "output": "Sets ``Content-Disposition`` header.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ret_range_minions(self):\n        '''\n        \n        '''\n        if HAS_RANGE is False:\n            raise RuntimeError(\"Python lib 'seco.range' is not available\")\n\n        minions = {}\n        range_hosts = _convert_range_to_list(self.tgt, __opts__['range_server'])\n        return self._ret_minions(range_hosts.__contains__)", "output": "Return minions that are returned by a range query", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_beta(self, kl_loss=0.0):\n    \"\"\"\n\n    \"\"\"\n    if self.hparams.latent_loss_multiplier_dynamic:\n      beta = tf.Variable(self.hparams.latent_loss_multiplier,\n                         trainable=False, dtype=tf.float32)\n      alpha = self.hparams.latent_loss_multiplier_alpha\n      epsilon = self.hparams.latent_loss_multiplier_epsilon\n      shadow_beta = beta + alpha * (kl_loss - epsilon)\n      # Caping the beta between 0 and 1. May need to change this later on.\n      shadow_beta = tf.maximum(shadow_beta, 0.0)\n      shadow_beta = tf.minimum(shadow_beta, 1.0)\n      update_op = tf.assign(beta, shadow_beta)\n    else:\n      beta = common_video.beta_schedule(\n          schedule=self.hparams.latent_loss_multiplier_schedule,\n          global_step=self.get_iteration_num(),\n          final_beta=self.hparams.latent_loss_multiplier,\n          decay_start=(self.hparams.num_iterations_1st_stage +\n                       self.hparams.num_iterations_2nd_stage),\n          decay_end=self.hparams.anneal_end)\n      update_op = tf.identity(beta)  # fake update for regular beta.\n    with tf.control_dependencies([update_op]):\n      tf.summary.scalar(\"beta\", beta)\n      return beta", "output": "Get the KL multiplier, either dynamically or schedule based.\n\n    if hparams.latent_loss_multiplier_dynamic is set to true, then beta\n    is being adjusted to keep KL under hparams.latent_loss_multiplier_epsilon.\n    In order to do so, the beta is being updated at each iteration\n    by taking steps of size hparams.latent_loss_multiplier_alpha.\n    The same formulation can be retrieved by solving the Lagrangian\n    with KL < epsilon as a constraint.\n\n    Args:\n      kl_loss: KL loss. Only used for dynamic adjustment.\n\n    Returns:\n      beta: the final value of beta.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def properties(cls, with_bases=True):\n        ''' \n\n        '''\n        if with_bases:\n            return accumulate_from_superclasses(cls, \"__properties__\")\n        else:\n            return set(cls.__properties__)", "output": "Collect the names of properties on this class.\n\n        This method *optionally* traverses the class hierarchy and includes\n        properties defined on any parent classes.\n\n        Args:\n            with_bases (bool, optional) :\n                Whether to include properties defined on parent classes in\n                the results. (default: True)\n\n        Returns:\n           set[str] : property names", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_noise_to_dict_values(dictionary: Dict[A, float], noise_param: float) -> Dict[A, float]:\n    \"\"\"\n    \n    \"\"\"\n    new_dict = {}\n    for key, value in dictionary.items():\n        noise_value = value * noise_param\n        noise = random.uniform(-noise_value, noise_value)\n        new_dict[key] = value + noise\n    return new_dict", "output": "Returns a new dictionary with noise added to every key in ``dictionary``.  The noise is\n    uniformly distributed within ``noise_param`` percent of the value for every value in the\n    dictionary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _no_op(name, **kwargs):\n    '''\n    \n    '''\n    return dict(name=name, result=True, changes={}, comment='')", "output": "No-op state to support state config via the stateconf renderer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def raise_for_status(self, allow_redirects=True):\n        \"\"\"\"\"\"\n\n        if self.status_code == 304:\n            return\n        elif self.error:\n            if self.traceback:\n                six.reraise(Exception, Exception(self.error), Traceback.from_string(self.traceback).as_traceback())\n            http_error = HTTPError(self.error)\n        elif (self.status_code >= 300) and (self.status_code < 400) and not allow_redirects:\n            http_error = HTTPError('%s Redirection' % (self.status_code))\n        elif (self.status_code >= 400) and (self.status_code < 500):\n            http_error = HTTPError('%s Client Error' % (self.status_code))\n        elif (self.status_code >= 500) and (self.status_code < 600):\n            http_error = HTTPError('%s Server Error' % (self.status_code))\n        else:\n            return\n\n        http_error.response = self\n        raise http_error", "output": "Raises stored :class:`HTTPError` or :class:`URLError`, if one occurred.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def binaryRecordsStream(self, directory, recordLength):\n        \"\"\"\n        \n        \"\"\"\n        return DStream(self._jssc.binaryRecordsStream(directory, recordLength), self,\n                       NoOpSerializer())", "output": "Create an input stream that monitors a Hadoop-compatible file system\n        for new files and reads them as flat binary files with records of\n        fixed length. Files must be written to the monitored directory by \"moving\"\n        them from another location within the same file system.\n        File names starting with . are ignored.\n\n        @param directory:       Directory to load data from\n        @param recordLength:    Length of each record in bytes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_argv(config):\n    \"\"\"\"\"\"\n    argv = []\n    for k, v in config.items():\n        if \"-\" in k:\n            raise ValueError(\"Use '_' instead of '-' in `{}`\".format(k))\n        if v is None:\n            continue\n        if not isinstance(v, bool) or v:  # for argparse flags\n            argv.append(\"--{}\".format(k.replace(\"_\", \"-\")))\n        if isinstance(v, string_types):\n            argv.append(v)\n        elif isinstance(v, bool):\n            pass\n        else:\n            argv.append(json.dumps(v, cls=_SafeFallbackEncoder))\n    return argv", "output": "Converts configuration to a command line argument format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fully_connected(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    new_attrs = translation_utils._remove_attributes(attrs, ['axis'])\n\n    new_attrs = translation_utils._fix_bias('FullyConnected', new_attrs, len(inputs))\n\n    new_attrs = translation_utils._fix_channels('FullyConnected', new_attrs, inputs, proto_obj)\n\n    return 'FullyConnected', new_attrs, inputs", "output": "Applies a linear transformation: Y=XWT+b.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def padded_accuracy_topk(predictions,\n                         labels,\n                         k,\n                         weights_fn=common_layers.weights_nonzero):\n  \"\"\"\"\"\"\n  with tf.variable_scope(\"padded_accuracy_topk\", values=[predictions, labels]):\n    padded_predictions, padded_labels = common_layers.pad_with_zeros(\n        predictions, labels)\n    weights = weights_fn(padded_labels)\n    effective_k = tf.minimum(k,\n                             common_layers.shape_list(padded_predictions)[-1])\n    _, outputs = tf.nn.top_k(padded_predictions, k=effective_k)\n    outputs = tf.to_int32(outputs)\n    padded_labels = tf.to_int32(padded_labels)\n    padded_labels = tf.expand_dims(padded_labels, axis=-1)\n    padded_labels += tf.zeros_like(outputs)  # Pad to same shape.\n    same = tf.to_float(tf.equal(outputs, padded_labels))\n    same_topk = tf.reduce_sum(same, axis=-1)\n    return same_topk, weights", "output": "Percentage of times that top-k predictions matches labels on non-0s.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_bytes(self, num_bytes: int, partial: bool = False) -> Awaitable[bytes]:\n        \"\"\"\n\n        \"\"\"\n        future = self._start_read()\n        assert isinstance(num_bytes, numbers.Integral)\n        self._read_bytes = num_bytes\n        self._read_partial = partial\n        try:\n            self._try_inline_read()\n        except:\n            future.add_done_callback(lambda f: f.exception())\n            raise\n        return future", "output": "Asynchronously read a number of bytes.\n\n        If ``partial`` is true, data is returned as soon as we have\n        any bytes to return (but never more than ``num_bytes``)\n\n        .. versionchanged:: 4.0\n            Added the ``partial`` argument.  The callback argument is now\n            optional and a `.Future` will be returned if it is omitted.\n\n        .. versionchanged:: 6.0\n\n           The ``callback`` and ``streaming_callback`` arguments have\n           been removed. Use the returned `.Future` (and\n           ``partial=True`` for ``streaming_callback``) instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def query(conn_type, option, post_data=None):\n    '''\n    \n    '''\n    if ticket is None or csrf is None or url is None:\n        log.debug('Not authenticated yet, doing that now..')\n        _authenticate()\n\n    full_url = 'https://{0}:{1}/api2/json/{2}'.format(url, port, option)\n\n    log.debug('%s: %s (%s)', conn_type, full_url, post_data)\n\n    httpheaders = {'Accept': 'application/json',\n                   'Content-Type': 'application/x-www-form-urlencoded',\n                   'User-Agent': 'salt-cloud-proxmox'}\n\n    if conn_type == 'post':\n        httpheaders['CSRFPreventionToken'] = csrf\n        response = requests.post(full_url, verify=verify_ssl,\n                                 data=post_data,\n                                 cookies=ticket,\n                                 headers=httpheaders)\n    elif conn_type == 'put':\n        httpheaders['CSRFPreventionToken'] = csrf\n        response = requests.put(full_url, verify=verify_ssl,\n                                data=post_data,\n                                cookies=ticket,\n                                headers=httpheaders)\n    elif conn_type == 'delete':\n        httpheaders['CSRFPreventionToken'] = csrf\n        response = requests.delete(full_url, verify=verify_ssl,\n                                   data=post_data,\n                                   cookies=ticket,\n                                   headers=httpheaders)\n    elif conn_type == 'get':\n        response = requests.get(full_url, verify=verify_ssl,\n                                cookies=ticket)\n\n    response.raise_for_status()\n\n    try:\n        returned_data = response.json()\n        if 'data' not in returned_data:\n            raise SaltCloudExecutionFailure\n        return returned_data['data']\n    except Exception:\n        log.error('Error in trying to process JSON')\n        log.error(response)", "output": "Execute the HTTP request to the API", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def GetPossibleGroup(self):\n    \"\"\"\n    \"\"\"\n\n    # Remove this method from the tail of the queue so we can add it to a group.\n    this_method = self._call_queue.pop()\n    assert this_method == self\n\n    # Determine if the tail of the queue is a group, or just a regular ordered\n    # mock method.\n    group = None\n    try:\n      group = self._call_queue[-1]\n    except IndexError:\n      pass\n\n    return group", "output": "Returns a possible group from the end of the call queue or None if no\n    other methods are on the stack.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def search(self, using=None):\n        \"\"\"\n        \n        \"\"\"\n        return Search(\n            using=using or self._using,\n            index=self._name,\n            doc_type=self._doc_types\n        )", "output": "Return a :class:`~elasticsearch_dsl.Search` object searching over the\n        index (or all the indices belonging to this template) and its\n        ``Document``\\\\s.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def contrastive_loss(left, right, y, margin, extra=False, scope=\"constrastive_loss\"):\n    \n    \"\"\"\n    with tf.name_scope(scope):\n        y = tf.cast(y, tf.float32)\n\n        delta = tf.reduce_sum(tf.square(left - right), 1)\n        delta_sqrt = tf.sqrt(delta + 1e-10)\n\n        match_loss = delta\n        missmatch_loss = tf.square(tf.nn.relu(margin - delta_sqrt))\n\n        loss = tf.reduce_mean(0.5 * (y * match_loss + (1 - y) * missmatch_loss))\n\n        if extra:\n            num_pos = tf.count_nonzero(y)\n            num_neg = tf.count_nonzero(1 - y)\n            pos_dist = tf.where(tf.equal(num_pos, 0), 0.,\n                                tf.reduce_sum(y * delta_sqrt) / tf.cast(num_pos, tf.float32),\n                                name=\"pos-dist\")\n            neg_dist = tf.where(tf.equal(num_neg, 0), 0.,\n                                tf.reduce_sum((1 - y) * delta_sqrt) / tf.cast(num_neg, tf.float32),\n                                name=\"neg-dist\")\n            return loss, pos_dist, neg_dist\n        else:\n            return loss", "output": "r\"\"\"Loss for Siamese networks as described in the paper:\n    `Learning a Similarity Metric Discriminatively, with Application to Face\n    Verification <http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf>`_ by Chopra et al.\n\n    .. math::\n        \\frac{1}{2} [y \\cdot d^2 + (1-y) \\cdot \\max(0, m - d)^2], d = \\Vert l - r \\Vert_2\n\n    Args:\n        left (tf.Tensor): left feature vectors of shape [Batch, N].\n        right (tf.Tensor): right feature vectors of shape [Batch, N].\n        y (tf.Tensor): binary labels of shape [Batch]. 1: similar, 0: not similar.\n        margin (float): horizon for negative examples (y==0).\n        extra (bool): also return distances for pos and neg.\n\n    Returns:\n        tf.Tensor: constrastive_loss (averaged over the batch), (and optionally average_pos_dist, average_neg_dist)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def date(self, year: Number, month: Number, day: Number) -> Date:\n        \"\"\"\n        \n        \"\"\"\n        return Date(year, month, day)", "output": "Takes three numbers and returns a ``Date`` object whose year, month, and day are the three\n        numbers in that order.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_network_adapters(network_interfaces, parent=None):\n    '''\n    \n    '''\n    network_specs = []\n    nics_settings = []\n    keys = range(-4000, -4050, -1)\n    if network_interfaces:\n        devs = [inter['adapter'] for inter in network_interfaces]\n        log.trace('Creating network interfaces %s', devs)\n        for interface, key in zip(network_interfaces, keys):\n            network_spec = _apply_network_adapter_config(\n                key, interface['name'],\n                interface['adapter_type'],\n                interface['switch_type'],\n                network_adapter_label=interface['adapter'],\n                operation='add',\n                connectable=interface['connectable'] if 'connectable' in interface else None,\n                mac=interface['mac'], parent=parent)\n            network_specs.append(network_spec)\n            if 'mapping' in interface:\n                adapter_mapping = _set_network_adapter_mapping(\n                    interface['mapping']['domain'],\n                    interface['mapping']['gateway'],\n                    interface['mapping']['ip_addr'],\n                    interface['mapping']['subnet_mask'],\n                    interface['mac'])\n                nics_settings.append(adapter_mapping)\n    return (network_specs, nics_settings)", "output": "Returns a list of vim.vm.device.VirtualDeviceSpec objects representing\n    the interfaces to be created for a virtual machine\n\n    network_interfaces\n        List of network interfaces and properties\n\n    parent\n        Parent object reference\n\n    .. code-block: bash\n\n        interfaces:\n          adapter: 'Network adapter 1'\n          name: vlan100\n          switch_type: distributed or standard\n          adapter_type: vmxnet3 or vmxnet, vmxnet2, vmxnet3, e1000, e1000e\n          mac: '00:11:22:33:44:55'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_sequence_from_str(self, sequence):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        self._qsequences = [QKeySequence(s) for s in sequence.split(', ')]\r\n        self.update_warning()", "output": "This is a convenience method to set the new QKeySequence of the\r\n        shortcut editor from a string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def fetch_member(self, member_id):\n        \"\"\"\n        \"\"\"\n        data = await self._state.http.get_member(self.id, member_id)\n        return Member(data=data, state=self._state, guild=self)", "output": "|coro|\n\n        Retreives a :class:`Member` from a guild ID, and a member ID.\n\n        .. note::\n\n            This method is an API call. For general usage, consider :meth:`get_member` instead.\n\n        Parameters\n        -----------\n        member_id: :class:`int`\n            The member's ID to fetch from.\n\n        Raises\n        -------\n        Forbidden\n            You do not have access to the guild.\n        HTTPException\n            Getting the guild failed.\n\n        Returns\n        --------\n        :class:`Member`\n            The member from the member ID.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cmd(jail=None):\n    '''\n    \n    '''\n    service = salt.utils.path.which('service')\n    if not service:\n        raise CommandNotFoundError('\\'service\\' command not found')\n    if jail:\n        jexec = salt.utils.path.which('jexec')\n        if not jexec:\n            raise CommandNotFoundError('\\'jexec\\' command not found')\n        service = '{0} {1} {2}'.format(jexec, jail, service)\n    return service", "output": "Return full path to service command\n\n    .. versionchanged:: 2016.3.4\n\n    Support for jail (representing jid or jail name) keyword argument in kwargs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def approximate_interactions(index, shap_values, X, feature_names=None):\n    \"\"\" \n    \"\"\"\n\n    # convert from DataFrames if we got any\n    if str(type(X)).endswith(\"'pandas.core.frame.DataFrame'>\"):\n        if feature_names is None:\n            feature_names = X.columns\n        X = X.values\n\n    index = convert_name(index, shap_values, feature_names)\n\n    if X.shape[0] > 10000:\n        a = np.arange(X.shape[0])\n        np.random.shuffle(a)\n        inds = a[:10000]\n    else:\n        inds = np.arange(X.shape[0])\n\n    x = X[inds, index]\n    srt = np.argsort(x)\n    shap_ref = shap_values[inds, index]\n    shap_ref = shap_ref[srt]\n    inc = max(min(int(len(x) / 10.0), 50), 1)\n    interactions = []\n    for i in range(X.shape[1]):\n        val_other = X[inds, i][srt].astype(np.float)\n        v = 0.0\n        if not (i == index or np.sum(np.abs(val_other)) < 1e-8):\n            for j in range(0, len(x), inc):\n                if np.std(val_other[j:j + inc]) > 0 and np.std(shap_ref[j:j + inc]) > 0:\n                    v += abs(np.corrcoef(shap_ref[j:j + inc], val_other[j:j + inc])[0, 1])\n        val_v = v\n\n        val_other = np.isnan(X[inds, i][srt].astype(np.float))\n        v = 0.0\n        if not (i == index or np.sum(np.abs(val_other)) < 1e-8):\n            for j in range(0, len(x), inc):\n                if np.std(val_other[j:j + inc]) > 0 and np.std(shap_ref[j:j + inc]) > 0:\n                    v += abs(np.corrcoef(shap_ref[j:j + inc], val_other[j:j + inc])[0, 1])\n        nan_v = v\n\n        interactions.append(max(val_v, nan_v))\n\n    return np.argsort(-np.abs(interactions))", "output": "Order other features by how much interaction they seem to have with the feature at the given index.\n\n    This just bins the SHAP values for a feature along that feature's value. For true Shapley interaction\n    index values for SHAP see the interaction_contribs option implemented in XGBoost.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_environment(self, environment):\n        \"\"\"\n        \n        \"\"\"\n\n        non_strings = []\n        for (k,v) in environment.items():\n            if not isinstance(v, basestring):\n                non_strings.append(k)\n        if non_strings:\n            raise ValueError(\"The following environment variables are not strings: {}\".format(\", \".join(non_strings)))\n        else:\n            return True", "output": "Make sure the environment contains only strings\n\n        (since putenv needs a string)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rm_token(opts, tok):\n    '''\n    \n    '''\n    t_path = os.path.join(opts['token_dir'], tok)\n    try:\n        os.remove(t_path)\n        return {}\n    except (IOError, OSError):\n        log.warning('Could not remove token %s', tok)", "output": "Remove token from the store.\n\n    :param opts: Salt master config options\n    :param tok: Token to remove\n    :returns: Empty dict if successful. None if failed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def refit(self, data, label, decay_rate=0.9, **kwargs):\n        \"\"\"\n        \"\"\"\n        if self.__set_objective_to_none:\n            raise LightGBMError('Cannot refit due to null objective function.')\n        predictor = self._to_predictor(copy.deepcopy(kwargs))\n        leaf_preds = predictor.predict(data, -1, pred_leaf=True)\n        nrow, ncol = leaf_preds.shape\n        train_set = Dataset(data, label, silent=True)\n        new_booster = Booster(self.params, train_set, silent=True)\n        # Copy models\n        _safe_call(_LIB.LGBM_BoosterMerge(\n            new_booster.handle,\n            predictor.handle))\n        leaf_preds = leaf_preds.reshape(-1)\n        ptr_data, type_ptr_data, _ = c_int_array(leaf_preds)\n        _safe_call(_LIB.LGBM_BoosterRefit(\n            new_booster.handle,\n            ptr_data,\n            ctypes.c_int(nrow),\n            ctypes.c_int(ncol)))\n        new_booster.network = self.network\n        new_booster.__attr = self.__attr.copy()\n        return new_booster", "output": "Refit the existing Booster by new data.\n\n        Parameters\n        ----------\n        data : string, numpy array, pandas DataFrame, H2O DataTable's Frame or scipy.sparse\n            Data source for refit.\n            If string, it represents the path to txt file.\n        label : list, numpy 1-D array or pandas Series / one-column DataFrame\n            Label for refit.\n        decay_rate : float, optional (default=0.9)\n            Decay rate of refit,\n            will use ``leaf_output = decay_rate * old_leaf_output + (1.0 - decay_rate) * new_leaf_output`` to refit trees.\n        **kwargs\n            Other parameters for refit.\n            These parameters will be passed to ``predict`` method.\n\n        Returns\n        -------\n        result : Booster\n            Refitted Booster.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def answer_json_to_strings(answer: Dict[str, Any]) -> Tuple[Tuple[str, ...], str]:\n    \"\"\"\n    \n    \"\"\"\n    if \"number\" in answer and answer[\"number\"]:\n        return tuple([str(answer[\"number\"])]), \"number\"\n    elif \"spans\" in answer and answer[\"spans\"]:\n        return tuple(answer[\"spans\"]), \"span\" if len(answer[\"spans\"]) == 1 else \"spans\"\n    elif \"date\" in answer:\n        return tuple([\"{0} {1} {2}\".format(answer[\"date\"][\"day\"],\n                                           answer[\"date\"][\"month\"],\n                                           answer[\"date\"][\"year\"])]), \"date\"\n    else:\n        raise ValueError(f\"Answer type not found, should be one of number, spans or date at: {json.dumps(answer)}\")", "output": "Takes an answer JSON blob from the DROP data release and converts it into strings used for\n    evaluation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_server(self):\r\n        \"\"\"\"\"\"\r\n        if self.server is None:\r\n            self.port = select_port(default_port=self.DEFAULT_PORT)\r\n            self.set_home_url('http://localhost:%d/' % self.port)\r\n        elif self.server.isRunning():\r\n            self.server.server_started.disconnect(self.initialize_continued)\r\n            self.server.quit()\r\n        self.server = PydocServer(port=self.port)\r\n        self.server.server_started.connect(self.initialize_continued)\r\n        self.server.start()", "output": "Start pydoc server", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fromtarfile(cls, tarfile):\n        \"\"\"\n        \"\"\"\n        buf = tarfile.fileobj.read(BLOCKSIZE)\n        obj = cls.frombuf(buf, tarfile.encoding, tarfile.errors)\n        obj.offset = tarfile.fileobj.tell() - BLOCKSIZE\n        return obj._proc_member(tarfile)", "output": "Return the next TarInfo object from TarFile object\n           tarfile.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _valid_ip(ip_address):\n    '''\n    \n    '''\n\n    try:\n        address = ipaddress.IPv4Address(ip_address)\n    except ipaddress.AddressValueError:\n        return False\n\n    if address.is_unspecified or \\\n        address.is_loopback or \\\n        address.is_link_local or \\\n        address.is_multicast or \\\n        address.is_reserved:\n        return False\n\n    return True", "output": "Check if the IP address is valid and routable\n    Return either True or False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        \"\"\"\n        \n        \"\"\"\n        catNms = catNms if type(catNms) == list else [catNms]\n        supNms = supNms if type(supNms) == list else [supNms]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset['categories']\n        else:\n            cats = self.dataset['categories']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat['name']          in catNms]\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat['supercategory'] in supNms]\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat['id']            in catIds]\n        ids = [cat['id'] for cat in cats]\n        return ids", "output": "filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def subsequent_mask(size: int, device: str = 'cpu') -> torch.Tensor:\n    \"\"\"\"\"\"\n    mask = torch.tril(torch.ones(size, size, device=device, dtype=torch.int32)).unsqueeze(0)\n    return mask", "output": "Mask out subsequent positions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_sanitize_files(self):\n        \"\"\"\n        \n        \"\"\"\n        for fname in self.get_sanitize_files():\n            with open(fname, 'r') as f:\n                self.sanitize_patterns.update(get_sanitize_patterns(f.read()))", "output": "For each of the sanitize files that were specified as command line options\n        load the contents of the file into the sanitise patterns dictionary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_comment_markers(cellsource):\n    \"\"\"\n    \"\"\"\n    found = {}\n    for line in cellsource.splitlines():\n        line = line.strip()\n        if line.startswith('#'):\n            # print(\"Found comment in '{}'\".format(line))\n            comment = line.lstrip('#').strip()\n            if comment in comment_markers:\n                # print(\"Found marker {}\".format(comment))\n                marker = comment_markers[comment]\n                if not isinstance(marker, tuple):\n                    # If not an explicit tuple ('option', True/False),\n                    # imply ('option', True)\n                    marker = (marker, True)\n                marker_type = marker[0]\n                if marker_type in found:\n                    warnings.warn(\n                        \"Conflicting comment markers found, using the latest: \"\n                        \" %s VS %s\" %\n                        (found[marker_type], comment))\n                found[marker_type] = comment\n                yield marker", "output": "Look through the cell source for comments which affect nbval's behaviour\n\n    Yield an iterable of ``(MARKER_TYPE, True)``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop(name, vmid=None, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The stop action must be called with -a or --action.'\n        )\n\n    if not set_vm_status('stop', name, vmid=vmid):\n        log.error('Unable to bring VM %s (%s) down..', name, vmid)\n        raise SaltCloudExecutionFailure\n\n    # xxx: TBD: Check here whether the status was actually changed to 'stopped'\n\n    return {'Stopped': '{0} was stopped.'.format(name)}", "output": "Stop a node (\"pulling the plug\").\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a stop mymachine", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tfhub_cache_dir(default_cache_dir=None, use_temp=False):\n  \"\"\"\n  \"\"\"\n\n  # Note: We are using FLAGS[\"tfhub_cache_dir\"] (and not FLAGS.tfhub_cache_dir)\n  # to access the flag value in order to avoid parsing argv list. The flags\n  # should have been parsed by now in main() by tf.app.run(). If that was not\n  # the case (say in Colab env) we skip flag parsing because argv may contain\n  # unknown flags.\n  cache_dir = (\n      os.getenv(_TFHUB_CACHE_DIR, \"\") or FLAGS[\"tfhub_cache_dir\"].value or\n      default_cache_dir)\n  if not cache_dir and use_temp:\n    # Place all TF-Hub modules under <system's temp>/tfhub_modules.\n    cache_dir = os.path.join(tempfile.gettempdir(), \"tfhub_modules\")\n  if cache_dir:\n    logging.log_first_n(logging.INFO, \"Using %s to cache modules.\", 1,\n                        cache_dir)\n  return cache_dir", "output": "Returns cache directory.\n\n  Returns cache directory from either TFHUB_CACHE_DIR environment variable\n  or --tfhub_cache_dir or default, if set.\n\n  Args:\n    default_cache_dir: Default cache location to use if neither TFHUB_CACHE_DIR\n                       environment variable nor --tfhub_cache_dir are\n                       not specified.\n    use_temp: bool, Optional to enable using system's temp directory as a\n              module cache directory if neither default_cache_dir nor\n              --tfhub_cache_dir nor TFHUB_CACHE_DIR environment variable are\n              specified .", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_client():\n    '''\n    \n    '''\n    client = salt.cloud.CloudClient(\n            os.path.join(os.path.dirname(__opts__['conf_file']), 'cloud')\n            )\n    return client", "output": "Return cloud client", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_bgcolor(self, index):\r\n        \"\"\"\"\"\"\r\n        if index.column() == 0:\r\n            color = QColor(Qt.lightGray)\r\n            color.setAlphaF(.05)\r\n        elif index.column() < 3:\r\n            color = QColor(Qt.lightGray)\r\n            color.setAlphaF(.2)\r\n        else:\r\n            color = QColor(Qt.lightGray)\r\n            color.setAlphaF(.3)\r\n        return color", "output": "Background color depending on value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _handle_get_cfn_template_response(self, response, application_id, template_id):\n        \"\"\"\n        \n        \"\"\"\n        status = response['Status']\n        if status != \"ACTIVE\":\n            # Other options are PREPARING and EXPIRED.\n            if status == 'EXPIRED':\n                message = (\"Template for {} with id {} returned status: {}. Cannot access an expired \"\n                           \"template.\".format(application_id, template_id, status))\n                raise InvalidResourceException(application_id, message)\n            self._in_progress_templates.append((application_id, template_id))", "output": "Handles the response from the SAR service call\n\n        :param dict response: the response dictionary from the app repo\n        :param string application_id: the ApplicationId\n        :param string template_id: the unique TemplateId for this application", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ratio_scores(parameters_value, clusteringmodel_gmm_good, clusteringmodel_gmm_bad):\n    '''\n    \n    '''\n    ratio = clusteringmodel_gmm_good.score([parameters_value]) / clusteringmodel_gmm_bad.score([parameters_value])\n    sigma = 0\n    return ratio, sigma", "output": "The ratio is smaller the better", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cumulative_var(self):\n        \"\"\"\n        \n        \"\"\"\n        from .. import extensions\n        agg_op = \"__builtin__cum_var__\"\n        return SArray(_proxy = self.__proxy__.builtin_cumulative_aggregate(agg_op))", "output": "Return the cumulative variance of the elements in the SArray.\n\n        Returns an SArray where each element in the output corresponds to the\n        variance of all the elements preceding and including it. The SArray is\n        expected to be of numeric type, or a numeric vector type.\n\n        Returns\n        -------\n        out : SArray[int, float]\n\n        Notes\n        -----\n         - Missing values are ignored while performing the cumulative\n           aggregate operation.\n\n        Examples\n        --------\n        >>> sa = SArray([1, 2, 3, 4, 0])\n        >>> sa.cumulative_var()\n        dtype: float\n        rows: 3\n        [0.0, 0.25, 0.6666666666666666, 1.25, 2.0]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _pop_params(cls, kwargs):\n        \"\"\"\n        \n        \"\"\"\n        params = cls.params\n        if not isinstance(params, Mapping):\n            params = {k: NotSpecified for k in params}\n        param_values = []\n        for key, default_value in params.items():\n            try:\n                value = kwargs.pop(key, default_value)\n                if value is NotSpecified:\n                    raise KeyError(key)\n\n                # Check here that the value is hashable so that we fail here\n                # instead of trying to hash the param values tuple later.\n                hash(value)\n            except KeyError:\n                raise TypeError(\n                    \"{typename} expected a keyword parameter {name!r}.\".format(\n                        typename=cls.__name__,\n                        name=key\n                    )\n                )\n            except TypeError:\n                # Value wasn't hashable.\n                raise TypeError(\n                    \"{typename} expected a hashable value for parameter \"\n                    \"{name!r}, but got {value!r} instead.\".format(\n                        typename=cls.__name__,\n                        name=key,\n                        value=value,\n                    )\n                )\n\n            param_values.append((key, value))\n        return tuple(param_values)", "output": "Pop entries from the `kwargs` passed to cls.__new__ based on the values\n        in `cls.params`.\n\n        Parameters\n        ----------\n        kwargs : dict\n            The kwargs passed to cls.__new__.\n\n        Returns\n        -------\n        params : list[(str, object)]\n            A list of string, value pairs containing the entries in cls.params.\n\n        Raises\n        ------\n        TypeError\n            Raised if any parameter values are not passed or not hashable.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def new_repo(name, config_path=_DEFAULT_CONFIG_PATH, comment=None, component=None,\n             distribution=None, uploaders_file=None, from_snapshot=None,\n             saltenv='base'):\n    '''\n    \n    '''\n    _validate_config(config_path)\n\n    current_repo = __salt__['aptly.get_repo'](name=name, config_path=config_path)\n\n    if current_repo:\n        log.debug('Repository already exists: %s', name)\n        return True\n\n    cmd = ['repo', 'create', '-config={}'.format(config_path)]\n    repo_params = _format_repo_args(comment=comment, component=component,\n                                    distribution=distribution,\n                                    uploaders_file=uploaders_file, saltenv=saltenv)\n    cmd.extend(repo_params)\n    cmd.append(name)\n\n    if from_snapshot:\n        cmd.extend(['from', 'snapshot', from_snapshot])\n\n    _cmd_run(cmd)\n    repo = __salt__['aptly.get_repo'](name=name, config_path=config_path)\n\n    if repo:\n        log.debug('Created repo: %s', name)\n        return True\n    log.error('Unable to create repo: %s', name)\n    return False", "output": "Create a new local package repository.\n\n    :param str name: The name of the local repository.\n    :param str config_path: The path to the configuration file for the aptly instance.\n    :param str comment: The description of the repository.\n    :param str component: The default component to use when publishing.\n    :param str distribution: The default distribution to use when publishing.\n    :param str uploaders_file: The repository upload restrictions config.\n    :param str from_snapshot: The snapshot to initialize the repository contents from.\n    :param str saltenv: The environment the file resides in.\n\n    :return: A boolean representing whether all changes succeeded.\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' aptly.new_repo name=\"test-repo\" comment=\"Test main repo\" component=\"main\" distribution=\"trusty\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mtFeatureExtraction(signal, fs, mt_win, mt_step, st_win, st_step):\n    \"\"\"\n    \n    \"\"\"\n\n    mt_win_ratio = int(round(mt_win / st_step))\n    mt_step_ratio = int(round(mt_step / st_step))\n\n    mt_features = []\n\n    st_features, f_names = stFeatureExtraction(signal, fs, st_win, st_step)\n    n_feats = len(st_features)\n    n_stats = 2\n\n    mt_features, mid_feature_names = [], []\n    #for i in range(n_stats * n_feats + 1):\n    for i in range(n_stats * n_feats):\n        mt_features.append([])\n        mid_feature_names.append(\"\")\n\n    for i in range(n_feats):        # for each of the short-term features:\n        cur_p = 0\n        N = len(st_features[i])\n        mid_feature_names[i] = f_names[i] + \"_\" + \"mean\"\n        mid_feature_names[i + n_feats] = f_names[i] + \"_\" + \"std\"\n\n        while (cur_p < N):\n            N1 = cur_p\n            N2 = cur_p + mt_win_ratio\n            if N2 > N:\n                N2 = N\n            cur_st_feats = st_features[i][N1:N2]\n\n            mt_features[i].append(numpy.mean(cur_st_feats))\n            mt_features[i + n_feats].append(numpy.std(cur_st_feats))\n            #mt_features[i+2*n_feats].append(numpy.std(cur_st_feats) / (numpy.mean(cur_st_feats)+0.00000010))\n            cur_p += mt_step_ratio\n    return numpy.array(mt_features), st_features, mid_feature_names", "output": "Mid-term feature extraction", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def persist(name, value, config='/etc/sysctl.conf'):\n    '''\n    \n    '''\n    nlines = []\n    edited = False\n    value = six.text_type(value)\n\n    # create /etc/sysctl.conf if not present\n    if not os.path.isfile(config):\n        try:\n            with salt.utils.files.fopen(config, 'w+'):\n                pass\n        except (IOError, OSError):\n            msg = 'Could not create {0}'\n            raise CommandExecutionError(msg.format(config))\n\n    with salt.utils.files.fopen(config, 'r') as ifile:\n        for line in ifile:\n            line = salt.utils.stringutils.to_unicode(line)\n            m = re.match(r'{0}(\\??=)'.format(name), line)\n            if not m:\n                nlines.append(line)\n                continue\n            else:\n                key, rest = line.split('=', 1)\n                if rest.startswith('\"'):\n                    _, rest_v, rest = rest.split('\"', 2)\n                elif rest.startswith('\\''):\n                    _, rest_v, rest = rest.split('\\'', 2)\n                else:\n                    rest_v = rest.split()[0]\n                    rest = rest[len(rest_v):]\n                if rest_v == value:\n                    return 'Already set'\n                new_line = '{0}{1}{2}{3}'.format(name, m.group(1), value, rest)\n                nlines.append(new_line)\n                edited = True\n\n    if not edited:\n        newline = '{0}={1}'.format(name, value)\n        nlines.append(\"{0}\\n\".format(newline))\n\n    with salt.utils.files.fopen(config, 'wb') as ofile:\n        ofile.writelines(\n            salt.utils.data.encode(nlines)\n        )\n\n    assign(name, value)\n\n    return 'Updated'", "output": "Assign and persist a simple sysctl parameter for this minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sysctl.persist net.inet.icmp.icmplim 50", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build(self, grad_list, get_opt_fn):\n        \"\"\"\n        \n        \"\"\"\n        assert len(grad_list) == len(self.towers)\n        DataParallelBuilder._check_grad_list(grad_list)\n\n        # debug tower performance (without update):\n        # ops = [k[0] for k in grad_list[1]] + [k[0] for k in grad_list[0]]\n        # self.train_op = tf.group(*ops)\n        # return\n\n        self.grads = aggregate_grads(grad_list, colocation=True)\n        # grads = grad_list[0]\n\n        opt = get_opt_fn()\n        if self.ps_device == 'cpu':\n            with tf.device('/cpu:0'):\n                train_op = opt.apply_gradients(self.grads, name='train_op')\n        else:\n            train_op = opt.apply_gradients(self.grads, name='train_op')\n        return train_op", "output": "Reduce the gradients, apply them with the optimizer,\n        and set self.grads to a list of (g, v), containing the averaged gradients.\n\n        Args:\n            grad_list ([[(grad, var), ...], ...]): #GPU lists to be reduced. Each is the gradients computed on each GPU.\n            get_opt_fn (-> tf.train.Optimizer): callable which returns an optimizer\n\n        Returns:\n            tf.Operation: the training op", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_sshconfig():\n    \n    '''\n\n    with open(os.path.expanduser('~/.ssh/config')) as f:\n        cfg = paramiko.SSHConfig()\n        cfg.parse(f)\n        ret_dict = {}\n        for d in cfg._config:\n            _copy = dict(d)\n            # Avoid buggy behavior with strange host definitions, we need\n            # Hostname and not Host.\n            del _copy['host']\n            for host in d['host']:\n                ret_dict[host] = _copy['config']\n\n        return ret_dict", "output": "r'''\n    Read user's SSH configuration file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_model_config(model_name, dataset):\n    \"\"\"\"\"\"\n    model_map = _get_model_map(dataset.name)\n    if model_name not in model_map:\n        raise ValueError(\"Invalid model name \\\"%s\\\" for dataset \\\"%s\\\"\" %\n                         (model_name, dataset.name))\n    else:\n        return model_map[model_name]()", "output": "Map model name to model network configuration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_data_type(self, index, **kwargs):\r\n        \"\"\"\"\"\"\r\n        if not index.isValid():\r\n            return False\r\n        try:\r\n            if kwargs['atype'] == \"date\":\r\n                self._data[index.row()][index.column()] = \\\r\n                    datestr_to_datetime(self._data[index.row()][index.column()],\r\n                                    kwargs['dayfirst']).date()\r\n            elif kwargs['atype'] == \"perc\":\r\n                _tmp = self._data[index.row()][index.column()].replace(\"%\", \"\")\r\n                self._data[index.row()][index.column()] = eval(_tmp)/100.\r\n            elif kwargs['atype'] == \"account\":\r\n                _tmp = self._data[index.row()][index.column()].replace(\",\", \"\")\r\n                self._data[index.row()][index.column()] = eval(_tmp)\r\n            elif kwargs['atype'] == \"unicode\":\r\n                self._data[index.row()][index.column()] = to_text_string(\r\n                    self._data[index.row()][index.column()])\r\n            elif kwargs['atype'] == \"int\":\r\n                self._data[index.row()][index.column()] = int(\r\n                    self._data[index.row()][index.column()])\r\n            elif kwargs['atype'] == \"float\":\r\n                self._data[index.row()][index.column()] = float(\r\n                    self._data[index.row()][index.column()])\r\n            self.dataChanged.emit(index, index)\r\n        except Exception as instance:\r\n            print(instance)", "output": "Parse a type to an other type", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_link(url, processed, files):\n    \"\"\"\n    \n    \"\"\"\n    if url not in processed:\n        is_file = url.endswith(BAD_TYPES)\n        if is_file:\n            files.add(url)\n            return False\n        return True\n    return False", "output": "Determine whether or not a link should be crawled\n    A url should not be crawled if it\n        - Is a file\n        - Has already been crawled\n\n    Args:\n        url: str Url to be processed\n        processed: list[str] List of urls that have already been crawled\n\n    Returns:\n        bool If `url` should be crawled", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def argmax(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    axis = attrs.get('axis', 0)\n    keepdims = attrs.get('keepdims', 1)\n    argmax_op = symbol.argmax(inputs[0], axis=axis, keepdims=keepdims)\n    # onnx argmax operator always expects int64 as output type\n    cast_attrs = {'dtype': 'int64'}\n    return 'cast', cast_attrs, argmax_op", "output": "Returns indices of the maximum values along an axis", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _delete_advanced_config(config_spec, advanced_config, vm_extra_config):\n    '''\n    \n    '''\n    log.trace('Removing advanced configuration '\n              'parameters %s', advanced_config)\n    if isinstance(advanced_config, str):\n        raise salt.exceptions.ArgumentValueError(\n            'The specified \\'advanced_configs\\' configuration '\n            'option cannot be parsed, please check the parameters')\n    removed_configs = []\n    for key in advanced_config:\n        for option in vm_extra_config:\n            if option.key == key:\n                option = vim.option.OptionValue(key=key, value='')\n                config_spec.extraConfig.append(option)\n                removed_configs.append(key)\n    return removed_configs", "output": "Removes configuration parameters for the vm\n\n    config_spec\n        vm.ConfigSpec object\n\n    advanced_config\n        List of advanced config keys to be deleted\n\n    vm_extra_config\n        Virtual machine vm_ref.config.extraConfig object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _send_command(cmd,\n                  worker,\n                  lbn,\n                  target,\n                  profile='default',\n                  tgt_type='glob'):\n    '''\n    \n    '''\n\n    ret = {\n        'code': False,\n        'msg': 'OK',\n        'minions': [],\n    }\n\n    # Send the command to target\n    func = 'modjk.{0}'.format(cmd)\n    args = [worker, lbn, profile]\n    response = __salt__['publish.publish'](target, func, args, tgt_type)\n\n    # Get errors and list of affeced minions\n    errors = []\n    minions = []\n    for minion in response:\n        minions.append(minion)\n        if not response[minion]:\n            errors.append(minion)\n\n    # parse response\n    if not response:\n        ret['msg'] = 'no servers answered the published command {0}'.format(\n            cmd\n        )\n        return ret\n    elif errors:\n        ret['msg'] = 'the following minions return False'\n        ret['minions'] = errors\n        return ret\n    else:\n        ret['code'] = True\n        ret['msg'] = 'the commad was published successfully'\n        ret['minions'] = minions\n        return ret", "output": "Send a command to the modjk loadbalancer\n    The minion need to be able to publish the commands to the load balancer\n\n    cmd:\n        worker_stop - won't get any traffic from the lbn\n        worker_activate - activate the worker\n        worker_disable - will get traffic only for current sessions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gan_critic(n_channels:int=3, nf:int=128, n_blocks:int=3, p:int=0.15):\n    \"\"\n    layers = [\n        _conv(n_channels, nf, ks=4, stride=2),\n        nn.Dropout2d(p/2),\n        res_block(nf, dense=True,**_conv_args)]\n    nf *= 2 # after dense block\n    for i in range(n_blocks):\n        layers += [\n            nn.Dropout2d(p),\n            _conv(nf, nf*2, ks=4, stride=2, self_attention=(i==0))]\n        nf *= 2\n    layers += [\n        _conv(nf, 1, ks=4, bias=False, padding=0, use_activ=False),\n        Flatten()]\n    return nn.Sequential(*layers)", "output": "Critic to train a `GAN`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wrap(cls, value):\n        ''' \n\n        '''\n        if isinstance(value, list):\n            if isinstance(value, PropertyValueList):\n                return value\n            else:\n                return PropertyValueList(value)\n        else:\n            return value", "output": "Some property types need to wrap their values in special containers, etc.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_month(self, month):\n        \"\"\"\n        \n        \"\"\"\n\n        def _select_month(month):\n            return self.data.loc[month, slice(None)]\n\n        try:\n            return self.new(_select_month(month), self.type, self.if_fq)\n        except:\n            raise ValueError('QA CANNOT GET THIS Month {} '.format(month))", "output": "\u9009\u62e9\u6708\u4efd\n\n        @2018/06/03 pandas \u7684\u7d22\u5f15\u95ee\u9898\u5bfc\u81f4\n        https://github.com/pandas-dev/pandas/issues/21299\n\n        \u56e0\u6b64\u5148\u7528set_index\u53bb\u91cd\u505a\u4e00\u6b21index\n        \u5f71\u54cd\u7684\u6709selects,select_time,select_month,get_bar\n\n        @2018/06/04\n        \u5f53\u9009\u62e9\u7684\u65f6\u95f4\u8d8a\u754c/\u80a1\u7968\u4e0d\u5b58\u5728,raise ValueError\n\n        @2018/06/04 pandas\u7d22\u5f15\u95ee\u9898\u5df2\u7ecf\u89e3\u51b3\n        \u5168\u90e8\u6062\u590d", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def diff(compiled_requirements, installed_dists):\n    \"\"\"\n    \n    \"\"\"\n    requirements_lut = {r.link or key_from_req(r.req): r for r in compiled_requirements}\n\n    satisfied = set()  # holds keys\n    to_install = set()  # holds InstallRequirement objects\n    to_uninstall = set()  # holds keys\n\n    pkgs_to_ignore = get_dists_to_ignore(installed_dists)\n    for dist in installed_dists:\n        key = key_from_req(dist)\n        if key not in requirements_lut or not requirements_lut[key].match_markers():\n            to_uninstall.add(key)\n        elif requirements_lut[key].specifier.contains(dist.version):\n            satisfied.add(key)\n\n    for key, requirement in requirements_lut.items():\n        if key not in satisfied and requirement.match_markers():\n            to_install.add(requirement)\n\n    # Make sure to not uninstall any packages that should be ignored\n    to_uninstall -= set(pkgs_to_ignore)\n\n    return (to_install, to_uninstall)", "output": "Calculate which packages should be installed or uninstalled, given a set\n    of compiled requirements and a list of currently installed modules.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ext_pillar(minion_id,  # pylint: disable=W0613\n               pillar,  # pylint: disable=W0613\n               command):\n    '''\n    \n    '''\n    try:\n        command = command.replace('%s', minion_id)\n        return deserialize(__salt__['cmd.run']('{0}'.format(command)))\n    except Exception:\n        log.critical('YAML data from %s failed to parse', command)\n        return {}", "output": "Execute a command and read the output as YAMLEX", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_expiration_seconds_v2(expiration):\n    \"\"\"\n    \"\"\"\n    # If it's a timedelta, add it to `now` in UTC.\n    if isinstance(expiration, datetime.timedelta):\n        now = NOW().replace(tzinfo=_helpers.UTC)\n        expiration = now + expiration\n\n    # If it's a datetime, convert to a timestamp.\n    if isinstance(expiration, datetime.datetime):\n        micros = _helpers._microseconds_from_datetime(expiration)\n        expiration = micros // 10 ** 6\n\n    if not isinstance(expiration, six.integer_types):\n        raise TypeError(\n            \"Expected an integer timestamp, datetime, or \"\n            \"timedelta. Got %s\" % type(expiration)\n        )\n    return expiration", "output": "Convert 'expiration' to a number of seconds in the future.\n\n    :type expiration: Union[Integer, datetime.datetime, datetime.timedelta]\n    :param expiration: Point in time when the signed URL should expire.\n\n    :raises: :exc:`TypeError` when expiration is not a valid type.\n\n    :rtype: int\n    :returns: a timestamp as an absolute number of seconds since epoch.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def iou(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n    \"\"\"\n    \n    \"\"\"\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        iou = []    \n        for i in range(C):\n            if i != ignore: # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n                intersection = ((label == i) & (pred == i)).sum()\n                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n                if not union:\n                    iou.append(EMPTY)\n                else:\n                    iou.append(float(intersection) / union)\n        ious.append(iou)\n    ious = map(mean, zip(*ious)) # mean accross images if per_image\n    return 100 * np.array(ious)", "output": "Array of IoU for each (non ignored) class", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_as_error(self, color=Qt.red):\n        \"\"\"\n        \n        \"\"\"\n        self.format.setUnderlineStyle(\n            QTextCharFormat.WaveUnderline)\n        self.format.setUnderlineColor(color)", "output": "Highlights text as a syntax error.\n\n        :param color: Underline color\n        :type color: QtGui.QColor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dump_to_text(self, with_stats):\n        \"\"\"\n        \n        \"\"\"\n        return tc.extensions._xgboost_dump_model(self.__proxy__, with_stats=with_stats, format='text')", "output": "Dump the models into a list of strings. Each\n        string is a text representation of a tree.\n\n        Parameters\n        ----------\n        with_stats : bool\n            If true, include node statistics in the output.\n\n        Returns\n        -------\n        out : SFrame\n            A table with two columns: feature, count,\n            ordered by 'count' in descending order.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keyPressEvent(self, event):\r\n        \"\"\"\r\n        \"\"\"\r\n        if event.key() == Qt.Key_Return or event.key() == Qt.Key_Enter:\r\n            if self.add_current_text_if_valid():\r\n                self.selected()\r\n                self.hide_completer()\r\n        elif event.key() == Qt.Key_Escape:\r\n            self.set_current_text(self.selected_text)\r\n            self.hide_completer()\r\n        else:\r\n            QComboBox.keyPressEvent(self, event)", "output": "Qt Override.\r\n\r\n        Handle key press events.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list(self, bank):\n        '''\n        \n        '''\n        fun = '{0}.list'.format(self.driver)\n        return self.modules[fun](bank, **self._kwargs)", "output": "Lists entries stored in the specified bank.\n\n        :param bank:\n            The name of the location inside the cache which will hold the key\n            and its associated data.\n\n        :return:\n            An iterable object containing all bank entries. Returns an empty\n            iterator if the bank doesn't exists.\n\n        :raises SaltCacheError:\n            Raises an exception if cache driver detected an error accessing data\n            in the cache backend (auth, permissions, etc).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _edge_list_to_dataframe(ls, src_column_name, dst_column_name):\n    \"\"\"\n    \n    \"\"\"\n    assert HAS_PANDAS, 'Cannot use dataframe because Pandas is not available or version is too low.'\n    cols = reduce(set.union, (set(e.attr.keys()) for e in ls))\n    df = pd.DataFrame({\n        src_column_name: [e.src_vid for e in ls],\n        dst_column_name: [e.dst_vid for e in ls]})\n    for c in cols:\n        df[c] = [e.attr.get(c) for e in ls]\n    return df", "output": "Convert a list of edges into dataframe.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def step(self, metrics):\n        \"\"\" \n        \"\"\"\n\n        if self.best is None:\n            self.best = metrics\n            return False\n\n        if np.isnan(metrics):\n            return True\n\n        if self.is_better(metrics, self.best):\n            self.num_bad_epochs = 0\n            self.best = metrics\n        else:\n            self.num_bad_epochs += 1\n\n        if self.num_bad_epochs >= self.patience:\n            return True\n\n        return False", "output": "EarlyStopping step on each epoch\n        Arguments:\n            metrics {float} -- metric value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def metric_create(self, project, metric_name, filter_, description=None):\n        \"\"\"\n        \"\"\"\n        target = \"/projects/%s/metrics\" % (project,)\n        data = {\"name\": metric_name, \"filter\": filter_, \"description\": description}\n        self.api_request(method=\"POST\", path=target, data=data)", "output": "API call:  create a metric resource.\n\n        See\n        https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.metrics/create\n\n        :type project: str\n        :param project: ID of the project in which to create the metric.\n\n        :type metric_name: str\n        :param metric_name: the name of the metric\n\n        :type filter_: str\n        :param filter_: the advanced logs filter expression defining the\n                        entries exported by the metric.\n\n        :type description: str\n        :param description: description of the metric.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, project_name, updatetime=None, md5sum=None):\n        ''''''\n        if time.time() - self.last_check_projects > self.CHECK_PROJECTS_INTERVAL:\n            self._check_projects()\n        if self._need_update(project_name, updatetime, md5sum):\n            self._update_project(project_name)\n        return self.projects.get(project_name, None)", "output": "get project data object, return None if not exists", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe_db_instances(name=None, filters=None, jmespath='DBInstances',\n                          region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    pag = conn.get_paginator('describe_db_instances')\n    args = {}\n    args.update({'DBInstanceIdentifier': name}) if name else None\n    args.update({'Filters': filters}) if filters else None\n    pit = pag.paginate(**args)\n    pit = pit.search(jmespath) if jmespath else pit\n    try:\n        return [p for p in pit]\n    except ClientError as e:\n        code = getattr(e, 'response', {}).get('Error', {}).get('Code')\n        if code != 'DBInstanceNotFound':\n            log.error(__utils__['boto3.get_error'](e))\n    return []", "output": "Return a detailed listing of some, or all, DB Instances visible in the\n    current scope.  Arbitrary subelements or subsections of the returned dataset\n    can be selected by passing in a valid JMSEPath filter as well.\n\n    CLI example::\n\n        salt myminion boto_rds.describe_db_instances jmespath='DBInstances[*].DBInstanceIdentifier'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_system_properties(server=None):\n    '''\n    \n    '''\n    properties = {}\n    data = _api_get('system-properties', server)\n\n    # Get properties into a dict\n    if any(data['extraProperties']['systemProperties']):\n        for element in data['extraProperties']['systemProperties']:\n            properties[element['name']] = element['value']\n        return properties\n    return {}", "output": "Get system properties", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_encodings():\n    '''\n    \n    '''\n    encodings = [__salt_system_encoding__]\n\n    try:\n        sys_enc = sys.getdefaultencoding()\n    except ValueError:  # system encoding is nonstandard or malformed\n        sys_enc = None\n    if sys_enc and sys_enc not in encodings:\n        encodings.append(sys_enc)\n\n    for enc in ['utf-8', 'latin-1']:\n        if enc not in encodings:\n            encodings.append(enc)\n\n    return encodings", "output": "return a list of string encodings to try", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_distro_release_file(self, filepath):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            with open(filepath) as fp:\n                # Only parse the first line. For instance, on SLES there\n                # are multiple lines. We don't want them...\n                return self._parse_distro_release_content(fp.readline())\n        except (OSError, IOError):\n            # Ignore not being able to read a specific, seemingly version\n            # related file.\n            # See https://github.com/nir0s/distro/issues/162\n            return {}", "output": "Parse a distro release file.\n\n        Parameters:\n\n        * filepath: Path name of the distro release file.\n\n        Returns:\n            A dictionary containing all information items.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _id_resolv(self, iid, named=True, uid=True):\n        '''\n        \n        '''\n\n        if not self.local_identity:\n            self.local_identity['users'] = self._get_local_users()\n            self.local_identity['groups'] = self._get_local_groups()\n\n        if not named:\n            return iid\n\n        for name, meta in self.local_identity[uid and 'users' or 'groups'].items():\n            if (uid and int(meta.get('uid', -1)) == iid) or (not uid and int(meta.get('gid', -1)) == iid):\n                return name\n\n        return iid", "output": "Resolve local users and groups.\n\n        :param iid:\n        :param named:\n        :param uid:\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_user_password(name, passwd, **client_args):\n    '''\n    \n    '''\n    if not user_exists(name, **client_args):\n        log.info('User \\'%s\\' does not exist', name)\n        return False\n\n    client = _client(**client_args)\n    client.set_user_password(name, passwd)\n\n    return True", "output": "Change password of a user.\n\n    name\n        Name of the user for whom to set the password.\n\n    passwd\n        New password of the user.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb.set_user_password <name> <password>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def SetAlpha(self, alpha):\n        '''\n        \n        '''\n        self._AlphaChannel = alpha\n        self.TKroot.attributes('-alpha', alpha)", "output": "Change the window's transparency\n        :param alpha: From 0 to 1 with 0 being completely transparent\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_line(line):\n  \"\"\"\n  \"\"\"\n  columns = line.split()\n  token = columns.pop(0)\n  values = [float(column) for column in columns]\n  return token, values", "output": "Parses a line of a text embedding file.\n\n  Args:\n    line: (str) One line of the text embedding file.\n\n  Returns:\n    A token string and its embedding vector in floats.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup(app):\n    '''  '''\n\n    # These two are deprecated and no longer have any effect, to be removed 2.0\n    app.add_config_value('bokeh_plot_pyfile_include_dirs', [], 'html')\n    app.add_config_value('bokeh_plot_use_relative_paths', False, 'html')\n\n    app.add_directive('bokeh-plot', BokehPlotDirective)\n    app.add_config_value('bokeh_missing_google_api_key_ok', True, 'html')\n    app.connect('builder-inited', builder_inited)\n    app.connect('build-finished', build_finished)", "output": "Required Sphinx extension setup function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mouseDoubleClickEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        if self.rename_tabs is True and \\\r\n                event.buttons() == Qt.MouseButtons(Qt.LeftButton):\r\n            # Tab index\r\n            index = self.tabAt(event.pos())\r\n            if index >= 0:\r\n                # Tab is valid, call tab name editor\r\n                self.tab_name_editor.edit_tab(index)\r\n        else:\r\n            # Event is not interesting, raise to parent\r\n            QTabBar.mouseDoubleClickEvent(self, event)", "output": "Override Qt method to trigger the tab name editor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set(self, param, value):\n        \"\"\"\n        \n        \"\"\"\n        self._shouldOwn(param)\n        try:\n            value = param.typeConverter(value)\n        except ValueError as e:\n            raise ValueError('Invalid param value given for param \"%s\". %s' % (param.name, e))\n        self._paramMap[param] = value", "output": "Sets a parameter in the embedded param map.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_ae_a3():\n  \"\"\"\"\"\"\n  hparams = transformer_ae_base()\n  hparams.batch_size = 4096\n  hparams.layer_prepostprocess_dropout = 0.3\n  hparams.optimizer = \"Adafactor\"\n  hparams.learning_rate = 0.25\n  hparams.learning_rate_warmup_steps = 10000\n  return hparams", "output": "Set of hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_spacy_model(spacy_model_name: str, pos_tags: bool, parse: bool, ner: bool) -> SpacyModelType:\n    \"\"\"\n    \n    \"\"\"\n\n    options = (spacy_model_name, pos_tags, parse, ner)\n    if options not in LOADED_SPACY_MODELS:\n        disable = ['vectors', 'textcat']\n        if not pos_tags:\n            disable.append('tagger')\n        if not parse:\n            disable.append('parser')\n        if not ner:\n            disable.append('ner')\n        try:\n            spacy_model = spacy.load(spacy_model_name, disable=disable)\n        except OSError:\n            logger.warning(f\"Spacy models '{spacy_model_name}' not found.  Downloading and installing.\")\n            spacy_download(spacy_model_name)\n            # NOTE(mattg): The following four lines are a workaround suggested by Ines for spacy\n            # 2.1.0, which removed the linking that was done in spacy 2.0.  importlib doesn't find\n            # packages that were installed in the same python session, so the way `spacy_download`\n            # works in 2.1.0 is broken for this use case.  These four lines can probably be removed\n            # at some point in the future, once spacy has figured out a better way to handle this.\n            # See https://github.com/explosion/spaCy/issues/3435.\n            from spacy.cli import link\n            from spacy.util import get_package_path\n            package_path = get_package_path(spacy_model_name)\n            link(spacy_model_name, spacy_model_name, model_path=package_path)\n            spacy_model = spacy.load(spacy_model_name, disable=disable)\n\n        LOADED_SPACY_MODELS[options] = spacy_model\n    return LOADED_SPACY_MODELS[options]", "output": "In order to avoid loading spacy models a whole bunch of times, we'll save references to them,\n    keyed by the options we used to create the spacy model, so any particular configuration only\n    gets loaded once.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download(self, dirpath):\n        \"\"\"  \"\"\"\n        if os.path.isfile(self.lpath) and os.path.getsize(self.lpath) > 0:\n            return\n        print('Downloading %s -> %s' % (self.path, self.lpath))\n        if dry_run:\n            return\n        self.arts.s3_bucket.download_file(self.path, self.lpath)", "output": "Download artifact from S3 and store in dirpath directory.\n            If the artifact is already downloaded nothing is done.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_model_loss(ctx, model, pretrained, dataset_name, dtype, ckpt_dir=None, start_step=None):\n    \"\"\"\"\"\"\n    # model\n    model, vocabulary = nlp.model.get_model(model,\n                                            dataset_name=dataset_name,\n                                            pretrained=pretrained, ctx=ctx)\n\n    if not pretrained:\n        model.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n    model.cast(dtype)\n\n    if ckpt_dir and start_step:\n        param_path = os.path.join(ckpt_dir, '%07d.params'%start_step)\n        model.load_parameters(param_path, ctx=ctx)\n        logging.info('Loading step %d checkpoints from %s.', start_step, param_path)\n\n    model.hybridize(static_alloc=True)\n\n    # losses\n    nsp_loss = mx.gluon.loss.SoftmaxCELoss()\n    mlm_loss = mx.gluon.loss.SoftmaxCELoss()\n    nsp_loss.hybridize(static_alloc=True)\n    mlm_loss.hybridize(static_alloc=True)\n\n    return model, nsp_loss, mlm_loss, vocabulary", "output": "Get model for pre-training.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def notebook_content(model, notebook_comms_target=None, theme=FromCurdoc):\n    ''' \n\n    '''\n\n    if not isinstance(model, Model):\n        raise ValueError(\"notebook_content expects a single Model instance\")\n\n    # Comms handling relies on the fact that the new_doc returned here\n    # has models with the same IDs as they were started with\n    with OutputDocumentFor([model], apply_theme=theme, always_new=True) as new_doc:\n        (docs_json, [render_item]) = standalone_docs_json_and_render_items([model])\n\n    div = div_for_render_item(render_item)\n\n    render_item = render_item.to_json()\n    if notebook_comms_target:\n        render_item[\"notebook_comms_target\"] = notebook_comms_target\n\n    script = DOC_NB_JS.render(\n        docs_json=serialize_json(docs_json),\n        render_items=serialize_json([render_item]),\n    )\n\n    return encode_utf8(script), encode_utf8(div), new_doc", "output": "Return script and div that will display a Bokeh plot in a Jupyter\n    Notebook.\n\n    The data for the plot is stored directly in the returned HTML.\n\n    Args:\n        model (Model) : Bokeh object to render\n\n        notebook_comms_target (str, optional) :\n            A target name for a Jupyter Comms object that can update\n            the document that is rendered to this notebook div\n\n        theme (Theme, optional) :\n            Defaults to the ``Theme`` instance in the current document.\n            Setting this to ``None`` uses the default theme or the theme\n            already specified in the document. Any other value must be an\n            instance of the ``Theme`` class.\n\n    Returns:\n        script, div, Document\n\n    .. note::\n        Assumes :func:`~bokeh.io.notebook.load_notebook` or the equivalent\n        has already been executed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _clone(self):\n        \"\"\"\n        \n        \"\"\"\n        ubq = super(UpdateByQuery, self)._clone()\n\n        ubq._response_class = self._response_class\n        ubq._script = self._script.copy()\n        ubq.query._proxied = self.query._proxied\n        return ubq", "output": "Return a clone of the current search request. Performs a shallow copy\n        of all the underlying objects. Used internally by most state modifying\n        APIs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rm_auth_key_from_file(user,\n                          source,\n                          config='.ssh/authorized_keys',\n                          saltenv='base',\n                          fingerprint_hash_type=None):\n    '''\n    \n    '''\n    lfile = __salt__['cp.cache_file'](source, saltenv)\n    if not os.path.isfile(lfile):\n        raise CommandExecutionError(\n            'Failed to pull key file from salt file server'\n        )\n\n    s_keys = _validate_keys(lfile, fingerprint_hash_type)\n    if not s_keys:\n        err = (\n            'No keys detected in {0}. Is file properly formatted?'.format(\n                source\n            )\n        )\n        log.error(err)\n        __context__['ssh_auth.error'] = err\n        return 'fail'\n    else:\n        rval = ''\n        for key in s_keys:\n            rval += rm_auth_key(\n                user,\n                key,\n                config=config,\n                fingerprint_hash_type=fingerprint_hash_type\n            )\n        # Due to the ability for a single file to have multiple keys, it's\n        # possible for a single call to this function to have both \"replace\"\n        # and \"new\" as possible valid returns. I ordered the following as I\n        # thought best.\n        if 'Key not removed' in rval:\n            return 'Key not removed'\n        elif 'Key removed' in rval:\n            return 'Key removed'\n        else:\n            return 'Key not present'", "output": "Remove an authorized key from the specified user's authorized key file,\n    using a file as source\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ssh.rm_auth_key_from_file <user> salt://ssh_keys/<user>.id_rsa.pub", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nanall(values, axis=None, skipna=True, mask=None):\n    \"\"\"\n    \n    \"\"\"\n    values, mask, dtype, _, _ = _get_values(values, skipna, True, copy=skipna,\n                                            mask=mask)\n    return values.all(axis)", "output": "Check if all elements along an axis evaluate to True.\n\n    Parameters\n    ----------\n    values : ndarray\n    axis: int, optional\n    skipna : bool, default True\n    mask : ndarray[bool], optional\n        nan-mask if known\n\n    Returns\n    -------\n    result : bool\n\n    Examples\n    --------\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1, 2, np.nan])\n    >>> nanops.nanall(s)\n    True\n\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1, 0])\n    >>> nanops.nanall(s)\n    False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cmd_sync(self, low, timeout=None, full_return=False):\n        '''\n        \n        '''\n        event = salt.utils.event.get_master_event(self.opts, self.opts['sock_dir'], listen=True)\n        job = self.master_call(**low)\n        ret_tag = salt.utils.event.tagify('ret', base=job['tag'])\n\n        if timeout is None:\n            timeout = self.opts.get('rest_timeout', 300)\n        ret = event.get_event(tag=ret_tag, full=True, wait=timeout, auto_reconnect=True)\n        if ret is None:\n            raise salt.exceptions.SaltClientTimeout(\n                \"RunnerClient job '{0}' timed out\".format(job['jid']),\n                jid=job['jid'])\n\n        return ret if full_return else ret['data']['return']", "output": "Execute a runner function synchronously; eauth is respected\n\n        This function requires that :conf_master:`external_auth` is configured\n        and the user is authorized to execute runner functions: (``@runner``).\n\n        .. code-block:: python\n\n            runner.eauth_sync({\n                'fun': 'jobs.list_jobs',\n                'username': 'saltdev',\n                'password': 'saltdev',\n                'eauth': 'pam',\n            })", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fit_theta(self):\n        \"\"\"\n        \"\"\"\n        x = range(1, self.point_num + 1)\n        y = self.trial_history\n        for i in range(NUM_OF_FUNCTIONS):\n            model = curve_combination_models[i]\n            try:\n                # The maximum number of iterations to fit is 100*(N+1), where N is the number of elements in `x0`.\n                if model_para_num[model] == 2:\n                    a, b = optimize.curve_fit(all_models[model], x, y)[0]\n                    model_para[model][0] = a\n                    model_para[model][1] = b\n                elif model_para_num[model] == 3:\n                    a, b, c = optimize.curve_fit(all_models[model], x, y)[0]\n                    model_para[model][0] = a\n                    model_para[model][1] = b\n                    model_para[model][2] = c\n                elif model_para_num[model] == 4:\n                    a, b, c, d = optimize.curve_fit(all_models[model], x, y)[0]\n                    model_para[model][0] = a\n                    model_para[model][1] = b\n                    model_para[model][2] = c\n                    model_para[model][3] = d\n            except (RuntimeError, FloatingPointError, OverflowError, ZeroDivisionError):\n                # Ignore exceptions caused by numerical calculations\n                pass\n            except Exception as exception:\n                logger.critical(\"Exceptions in fit_theta:\", exception)", "output": "use least squares to fit all default curves parameter seperately\n        \n        Returns\n        -------\n        None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def revision(cwd,\n             rev='HEAD',\n             short=False,\n             user=None,\n             password=None,\n             ignore_retcode=False,\n             output_encoding=None):\n    '''\n    \n    '''\n    cwd = _expand_path(cwd, user)\n    command = ['git', 'rev-parse']\n    if short:\n        command.append('--short')\n    command.append(rev)\n    return _git_run(command,\n                    cwd=cwd,\n                    user=user,\n                    password=password,\n                    ignore_retcode=ignore_retcode,\n                    output_encoding=output_encoding)['stdout']", "output": "Returns the SHA1 hash of a given identifier (hash, branch, tag, HEAD, etc.)\n\n    cwd\n        The path to the git checkout\n\n    rev : HEAD\n        The revision\n\n    short : False\n        If ``True``, return an abbreviated SHA1 git hash\n\n    user\n        User under which to run the git command. By default, the command is run\n        by the user under which the minion is running.\n\n    password\n        Windows only. Required when specifying ``user``. This parameter will be\n        ignored on non-Windows platforms.\n\n      .. versionadded:: 2016.3.4\n\n    ignore_retcode : False\n        If ``True``, do not log an error to the minion log if the git command\n        returns a nonzero exit status.\n\n        .. versionadded:: 2015.8.0\n\n    output_encoding\n        Use this option to specify which encoding to use to decode the output\n        from any git commands which are run. This should not be needed in most\n        cases.\n\n        .. note::\n            This should only be needed if the files in the repository were\n            created with filenames using an encoding other than UTF-8 to handle\n            Unicode characters.\n\n        .. versionadded:: 2018.3.1\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion git.revision /path/to/repo mybranch", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def countDistinct(col, *cols):\n    \"\"\"\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.countDistinct(_to_java_column(col), _to_seq(sc, cols, _to_java_column))\n    return Column(jc)", "output": "Returns a new :class:`Column` for distinct count of ``col`` or ``cols``.\n\n    >>> df.agg(countDistinct(df.age, df.name).alias('c')).collect()\n    [Row(c=2)]\n\n    >>> df.agg(countDistinct(\"age\", \"name\").alias('c')).collect()\n    [Row(c=2)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def runCommand(cmd, timeout=None, window=None):\n    \"\"\" \n\t\"\"\"\n    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    output = ''\n    for line in p.stdout:\n        line = line.decode(errors='replace' if (sys.version_info) < (3, 5) else 'backslashreplace').rstrip()\n        output += line\n        print(line)\n        window.Refresh() if window else None        # yes, a 1-line if, so shoot me\n\n    retval = p.wait(timeout)\n    return (retval, output)", "output": "run shell command\n\t@param cmd: command to execute\n\t@param timeout: timeout for command execution\n\t@param window: the PySimpleGUI window that the output is going to (needed to do refresh on)\n\t@return: (return code from command, command output)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _try_coerce_args(self, values, other):\n        \"\"\"\n        \n        \"\"\"\n        values = values.view('i8')\n\n        if isinstance(other, bool):\n            raise TypeError\n        elif is_null_datetimelike(other):\n            other = tslibs.iNaT\n        elif isinstance(other, (timedelta, np.timedelta64)):\n            other = Timedelta(other).value\n        elif hasattr(other, 'dtype') and is_timedelta64_dtype(other):\n            other = other.astype('i8', copy=False).view('i8')\n        else:\n            # coercion issues\n            # let higher levels handle\n            raise TypeError(other)\n\n        return values, other", "output": "Coerce values and other to int64, with null values converted to\n        iNaT. values is always ndarray-like, other may not be\n\n        Parameters\n        ----------\n        values : ndarray-like\n        other : ndarray-like or scalar\n\n        Returns\n        -------\n        base-type values, base-type other", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def qt_message_handler(msg_type, msg_log_context, msg_string):\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    BLACKLIST = [\r\n        'QMainWidget::resizeDocks: all sizes need to be larger than 0',\r\n    ]\r\n    if DEV or msg_string not in BLACKLIST:\r\n        print(msg_string)", "output": "Qt warning messages are intercepted by this handler.\r\n\r\n    On some operating systems, warning messages might be displayed\r\n    even if the actual message does not apply. This filter adds a\r\n    blacklist for messages that are being printed for no apparent\r\n    reason. Anything else will get printed in the internal console.\r\n\r\n    In DEV mode, all messages are printed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_flag(env, var, fallback, expected=True, warn=True):\n    \"\"\"\"\"\"\n    val = env.config_var(var)\n    if val is None:\n        if warn:\n            warnings.warn(\n                \"Config variable '{0}' is unset, Python ABI tag may \"\n                \"be incorrect\".format(var),\n                RuntimeWarning,\n                2,\n            )\n        return fallback()\n    return val == expected", "output": "Use a fallback method for determining SOABI flags if the needed config\n    var is unset or unavailable.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_rmse(predictions, labels, weights_fn=common_layers.weights_all):\n  \"\"\"\"\"\"\n  if common_layers.shape_list(predictions)[-1] == 1:\n    predictions = tf.squeeze(predictions, axis=[-1])\n  else:\n    predictions = tf.argmax(predictions, axis=-1)\n  return padded_rmse(predictions, labels, weights_fn)", "output": "RMSE but will argmax if last dim is not 1.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reload(self):\n        \"\"\"\n        \"\"\"\n        api = self._client.instance_admin_api\n        metadata = _metadata_with_prefix(self.name)\n\n        instance_pb = api.get_instance(self.name, metadata=metadata)\n\n        self._update_from_pb(instance_pb)", "output": "Reload the metadata for this instance.\n\n        See\n        https://cloud.google.com/spanner/reference/rpc/google.spanner.admin.instance.v1#google.spanner.admin.instance.v1.InstanceAdmin.GetInstanceConfig\n\n        :raises NotFound: if the instance does not exist", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_element(name, element_type, server=None, with_properties=True):\n    '''\n    \n    '''\n    element = {}\n    name = quote(name, safe='')\n    data = _api_get('{0}/{1}'.format(element_type, name), server)\n\n    # Format data, get properties if asked, and return the whole thing\n    if any(data['extraProperties']['entity']):\n        for key, value in data['extraProperties']['entity'].items():\n            element[key] = value\n        if with_properties:\n            element['properties'] = _get_element_properties(name, element_type)\n        return element\n    return None", "output": "Get an element with or without properties", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _deserialize(s, proto):  # type: (bytes, _Proto) -> _Proto\n    '''\n    \n    '''\n    if not isinstance(s, bytes):\n        raise ValueError('Parameter s must be bytes, but got type: {}'.format(type(s)))\n\n    if not (hasattr(proto, 'ParseFromString') and callable(proto.ParseFromString)):\n        raise ValueError('No ParseFromString method is detected. '\n                         '\\ntype is {}'.format(type(proto)))\n\n    decoded = cast(Optional[int], proto.ParseFromString(s))\n    if decoded is not None and decoded != len(s):\n        raise google.protobuf.message.DecodeError(\n            \"Protobuf decoding consumed too few bytes: {} out of {}\".format(\n                decoded, len(s)))\n    return proto", "output": "Parse bytes into a in-memory proto\n\n    @params\n    s is bytes containing serialized proto\n    proto is a in-memory proto object\n\n    @return\n    The proto instance filled in by s", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vq_nearest_neighbor(x, means,\n                        soft_em=False, num_samples=10, temperature=None):\n  \"\"\"\"\"\"\n  bottleneck_size = common_layers.shape_list(means)[0]\n  x_norm_sq = tf.reduce_sum(tf.square(x), axis=-1, keepdims=True)\n  means_norm_sq = tf.reduce_sum(tf.square(means), axis=-1, keepdims=True)\n  scalar_prod = tf.matmul(x, means, transpose_b=True)\n  dist = x_norm_sq + tf.transpose(means_norm_sq) - 2 * scalar_prod\n  if soft_em:\n    x_means_idx = tf.multinomial(-dist, num_samples=num_samples)\n    x_means_hot = tf.one_hot(\n        x_means_idx, depth=common_layers.shape_list(means)[0])\n    x_means_hot = tf.reduce_mean(x_means_hot, axis=1)\n  else:\n    if temperature is None:\n      x_means_idx = tf.argmax(-dist, axis=-1)\n    else:\n      x_means_idx = tf.multinomial(- dist / temperature, 1)\n      x_means_idx = tf.squeeze(x_means_idx, axis=-1)\n    if (common_layers.should_generate_summaries() and\n        not common_layers.is_xla_compiled()):\n      tf.summary.histogram(\"means_idx\", tf.reshape(x_means_idx, [-1]))\n    x_means_hot = tf.one_hot(x_means_idx, bottleneck_size)\n  x_means_hot_flat = tf.reshape(x_means_hot, [-1, bottleneck_size])\n  x_means = tf.matmul(x_means_hot_flat, means)\n  e_loss = tf.reduce_mean(tf.squared_difference(x, tf.stop_gradient(x_means)))\n  return x_means_hot, e_loss, dist", "output": "Find the nearest element in means to elements in x.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def predict(self, x):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(x, RDD):\n            return x.map(lambda v: self.predict(v))\n        return np.interp(x, self.boundaries, self.predictions)", "output": "Predict labels for provided features.\n        Using a piecewise linear function.\n        1) If x exactly matches a boundary then associated prediction\n        is returned. In case there are multiple predictions with the\n        same boundary then one of them is returned. Which one is\n        undefined (same as java.util.Arrays.binarySearch).\n        2) If x is lower or higher than all boundaries then first or\n        last prediction is returned respectively. In case there are\n        multiple predictions with the same boundary then the lowest\n        or highest is returned respectively.\n        3) If x falls between two values in boundary array then\n        prediction is treated as piecewise linear function and\n        interpolated value is returned. In case there are multiple\n        values with the same boundary then the same rules as in 2)\n        are used.\n\n        :param x:\n          Feature or RDD of Features to be labeled.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_wrapper(wrapper,\n                   wrapped,\n                   assigned = functools.WRAPPER_ASSIGNMENTS,\n                   updated = functools.WRAPPER_UPDATES):\n    \"\"\"\n    \n    \"\"\"\n    # workaround for http://bugs.python.org/issue3445\n    assigned = tuple(attr for attr in assigned if hasattr(wrapped, attr))\n    wrapper = functools.update_wrapper(wrapper, wrapped, assigned, updated)\n    # workaround for https://bugs.python.org/issue17482\n    wrapper.__wrapped__ = wrapped\n    return wrapper", "output": "Patch two bugs in functools.update_wrapper.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_template(repo_dir):\n    \"\"\"\n    \"\"\"\n    logger.debug('Searching {} for the project template.'.format(repo_dir))\n\n    repo_dir_contents = os.listdir(repo_dir)\n\n    project_template = None\n    for item in repo_dir_contents:\n        if 'cookiecutter' in item and '{{' in item and '}}' in item:\n            project_template = item\n            break\n\n    if project_template:\n        project_template = os.path.join(repo_dir, project_template)\n        logger.debug(\n            'The project template appears to be {}'.format(project_template)\n        )\n        return project_template\n    else:\n        raise NonTemplatedInputDirException", "output": "Determine which child directory of `repo_dir` is the project template.\n\n    :param repo_dir: Local directory of newly cloned repo.\n    :returns project_template: Relative path to project template.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_deeper_model(self, target_id, new_layer):\n        \"\"\"\n        \"\"\"\n        self.operation_history.append((\"to_deeper_model\", target_id, new_layer))\n        input_id = self.layer_id_to_input_node_ids[target_id][0]\n        output_id = self.layer_id_to_output_node_ids[target_id][0]\n        if self.weighted:\n            if is_layer(new_layer, \"Dense\"):\n                init_dense_weight(new_layer)\n            elif is_layer(new_layer, \"Conv\"):\n                init_conv_weight(new_layer)\n            elif is_layer(new_layer, \"BatchNormalization\"):\n                init_bn_weight(new_layer)\n\n        self._insert_new_layers([new_layer], input_id, output_id)", "output": "Insert a relu-conv-bn block after the target block.\n        Args:\n            target_id: A convolutional layer ID. The new block should be inserted after the block.\n            new_layer: An instance of StubLayer subclasses.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_hc(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The show_hc function must be called with -f or --function.'\n        )\n    if not kwargs or 'name' not in kwargs:\n        log.error(\n            'Must specify name of health check.'\n        )\n        return False\n\n    conn = get_conn()\n    return _expand_item(conn.ex_get_healthcheck(kwargs['name']))", "output": "Show the details of an existing health check.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f show_hc gce name=hc", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def face_landmarks(face_image, face_locations=None, model=\"large\"):\n    \"\"\"\n    \n    \"\"\"\n    landmarks = _raw_face_landmarks(face_image, face_locations, model)\n    landmarks_as_tuples = [[(p.x, p.y) for p in landmark.parts()] for landmark in landmarks]\n\n    # For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png\n    if model == 'large':\n        return [{\n            \"chin\": points[0:17],\n            \"left_eyebrow\": points[17:22],\n            \"right_eyebrow\": points[22:27],\n            \"nose_bridge\": points[27:31],\n            \"nose_tip\": points[31:36],\n            \"left_eye\": points[36:42],\n            \"right_eye\": points[42:48],\n            \"top_lip\": points[48:55] + [points[64]] + [points[63]] + [points[62]] + [points[61]] + [points[60]],\n            \"bottom_lip\": points[54:60] + [points[48]] + [points[60]] + [points[67]] + [points[66]] + [points[65]] + [points[64]]\n        } for points in landmarks_as_tuples]\n    elif model == 'small':\n        return [{\n            \"nose_tip\": [points[4]],\n            \"left_eye\": points[2:4],\n            \"right_eye\": points[0:2],\n        } for points in landmarks_as_tuples]\n    else:\n        raise ValueError(\"Invalid landmarks model type. Supported models are ['small', 'large'].\")", "output": "Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\n\n    :param face_image: image to search\n    :param face_locations: Optionally provide a list of face locations to check.\n    :param model: Optional - which model to use. \"large\" (default) or \"small\" which only returns 5 points but is faster.\n    :return: A list of dicts of face feature locations (eyes, nose, etc)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _post_process_apply(self, result_data, axis, try_scale=True):\n        \"\"\"\n        \"\"\"\n        if try_scale:\n            try:\n                internal_index = self.compute_index(0, result_data, True)\n            except IndexError:\n                internal_index = self.compute_index(0, result_data, False)\n            try:\n                internal_columns = self.compute_index(1, result_data, True)\n            except IndexError:\n                internal_columns = self.compute_index(1, result_data, False)\n        else:\n            internal_index = self.compute_index(0, result_data, False)\n            internal_columns = self.compute_index(1, result_data, False)\n        if not axis:\n            index = internal_index\n            # We check if the two columns are the same length because if\n            # they are the same length, `self.columns` is the correct index.\n            # However, if the operation resulted in a different number of columns,\n            # we must use the derived columns from `self.compute_index()`.\n            if len(internal_columns) != len(self.columns):\n                columns = internal_columns\n            else:\n                columns = self.columns\n        else:\n            columns = internal_columns\n            # See above explanation for checking the lengths of columns\n            if len(internal_index) != len(self.index):\n                index = internal_index\n            else:\n                index = self.index\n        return self.__constructor__(result_data, index, columns)", "output": "Recompute the index after applying function.\n\n        Args:\n            result_data: a BaseFrameManager object.\n            axis: Target axis along which function was applied.\n\n        Returns:\n            A new PandasQueryCompiler.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def silence(warning, silence=True):\n    ''' \n\n    '''\n    if not isinstance(warning, int):\n        raise ValueError('Input to silence should be a warning object '\n                         '- not of type {}'.format(type(warning)))\n    if silence:\n        __silencers__.add(warning)\n    elif warning in __silencers__:\n        __silencers__.remove(warning)\n    return __silencers__", "output": "Silence a particular warning on all Bokeh models.\n\n    Args:\n        warning (Warning) : Bokeh warning to silence\n        silence (bool) : Whether or not to silence the warning\n\n    Returns:\n        A set containing the all silenced warnings\n\n    This function adds or removes warnings from a set of silencers which\n    is referred to when running ``check_integrity``. If a warning\n    is added to the silencers - then it will never be raised.\n\n    .. code-block:: python\n\n        >>> from bokeh.core.validation.warnings import EMPTY_LAYOUT\n        >>> bokeh.core.validation.silence(EMPTY_LAYOUT, True)\n        {1002}\n\n    To turn a warning back on use the same method but with the silence\n    argument set to false\n\n    .. code-block:: python\n\n        >>> bokeh.core.validation.silence(EMPTY_LAYOUT, False)\n        set()", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode_message(self, message, is_key=False):\n        \"\"\"\n        \n        \"\"\"\n\n        if message is None:\n            return None\n\n        if len(message) <= 5:\n            raise SerializerError(\"message is too small to decode\")\n\n        with ContextStringIO(message) as payload:\n            magic, schema_id = struct.unpack('>bI', payload.read(5))\n            if magic != MAGIC_BYTE:\n                raise SerializerError(\"message does not start with magic byte\")\n            decoder_func = self._get_decoder_func(schema_id, payload, is_key)\n            return decoder_func(payload)", "output": "Decode a message from kafka that has been encoded for use with\n        the schema registry.\n        :param str|bytes or None message: message key or value to be decoded\n        :returns: Decoded message contents.\n        :rtype dict:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_pending_domain_join():\n    '''\n    \n    '''\n    base_key = r'SYSTEM\\CurrentControlSet\\Services\\Netlogon'\n    avoid_key = r'{0}\\AvoidSpnSet'.format(base_key)\n    join_key = r'{0}\\JoinDomain'.format(base_key)\n\n    # If either the avoid_key or join_key is present,\n    # then there is a reboot pending.\n    if __utils__['reg.key_exists']('HKLM', avoid_key):\n        log.debug('Key exists: %s', avoid_key)\n        return True\n    else:\n        log.debug('Key does not exist: %s', avoid_key)\n\n    if __utils__['reg.key_exists']('HKLM', join_key):\n        log.debug('Key exists: %s', join_key)\n        return True\n    else:\n        log.debug('Key does not exist: %s', join_key)\n\n    return False", "output": "Determine whether there is a pending domain join action that requires a\n    reboot.\n\n    .. versionadded:: 2016.11.0\n\n    Returns:\n        bool: ``True`` if there is a pending domain join action, otherwise\n        ``False``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' system.get_pending_domain_join", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def minion_sign_in_payload(self):\n        '''\n        \n        '''\n        payload = {}\n        payload['cmd'] = '_auth'\n        payload['id'] = self.opts['id']\n        if 'autosign_grains' in self.opts:\n            autosign_grains = {}\n            for grain in self.opts['autosign_grains']:\n                autosign_grains[grain] = self.opts['grains'].get(grain, None)\n            payload['autosign_grains'] = autosign_grains\n        try:\n            pubkey_path = os.path.join(self.opts['pki_dir'], self.mpub)\n            pub = get_rsa_pub_key(pubkey_path)\n            if HAS_M2:\n                payload['token'] = pub.public_encrypt(self.token, RSA.pkcs1_oaep_padding)\n            else:\n                cipher = PKCS1_OAEP.new(pub)\n                payload['token'] = cipher.encrypt(self.token)\n        except Exception:\n            pass\n        with salt.utils.files.fopen(self.pub_path) as f:\n            payload['pub'] = f.read()\n        return payload", "output": "Generates the payload used to authenticate with the master\n        server. This payload consists of the passed in id_ and the ssh\n        public key to encrypt the AES key sent back from the master.\n\n        :return: Payload dictionary\n        :rtype: dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _format_opts(opts):\n    '''\n    \n    '''\n    if opts is None:\n        return []\n    elif isinstance(opts, list):\n        new_opts = []\n        for item in opts:\n            if isinstance(item, six.string_types):\n                new_opts.append(item)\n            else:\n                new_opts.append(six.text_type(item))\n        return new_opts\n    else:\n        if not isinstance(opts, six.string_types):\n            opts = [six.text_type(opts)]\n        else:\n            opts = salt.utils.args.shlex_split(opts)\n    opts = salt.utils.data.decode(opts)\n    try:\n        if opts[-1] == '--':\n            # Strip the '--' if it was passed at the end of the opts string,\n            # it'll be added back (if necessary) in the calling function.\n            # Putting this check here keeps it from having to be repeated every\n            # time _format_opts() is invoked.\n            return opts[:-1]\n    except IndexError:\n        pass\n    return opts", "output": "Common code to inspect opts and split them if necessary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_interface_connection(interface_a, interface_b):\n    '''\n    \n    '''\n    payload = {'interface_a': interface_a,\n               'interface_b': interface_b}\n    ret = _add('dcim', 'interface-connections', payload)\n    if ret:\n        return {'dcim': {'interface-connections': {ret['id']: payload}}}\n    else:\n        return ret", "output": ".. versionadded:: 2019.2.0\n\n    Create an interface connection between 2 interfaces\n\n    interface_a\n        Interface id for Side A\n    interface_b\n        Interface id for Side B\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion netbox.create_interface_connection 123 456", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _wait_for_reader(self):\n        \"\"\"\"\"\"\n        if self.max_size <= 0:  # Unlimited queue\n            return\n        if self.write_item_offset - self.cached_remote_offset <= self.max_size:\n            return  # Hasn't reached max size\n        remote_offset = internal_kv._internal_kv_get(self.read_ack_key)\n        if remote_offset is None:\n            # logger.debug(\"[writer] Waiting for reader to start...\")\n            while remote_offset is None:\n                time.sleep(0.01)\n                remote_offset = internal_kv._internal_kv_get(self.read_ack_key)\n        remote_offset = int(remote_offset)\n        if self.write_item_offset - remote_offset > self.max_size:\n            logger.debug(\n                \"[writer] Waiting for reader to catch up {} to {} - {}\".format(\n                    remote_offset, self.write_item_offset, self.max_size))\n            while self.write_item_offset - remote_offset > self.max_size:\n                time.sleep(0.01)\n                remote_offset = int(\n                    internal_kv._internal_kv_get(self.read_ack_key))\n        self.cached_remote_offset = remote_offset", "output": "Checks for backpressure by the downstream reader.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge(self, other, inplace=False):\n        \"\"\"\n        \n        \"\"\"\n        holidays = self.merge_class(self, other)\n        if inplace:\n            self.rules = holidays\n        else:\n            return holidays", "output": "Merge holiday calendars together.  The caller's class\n        rules take precedence.  The merge will be done\n        based on each holiday's name.\n\n        Parameters\n        ----------\n        other : holiday calendar\n        inplace : bool (default=False)\n            If True set rule_table to holidays, else return array of Holidays", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init(self, aggregators):\n        \"\"\"\"\"\"\n\n        assert len(aggregators) == self.num_aggregation_workers, aggregators\n        if len(self.remote_evaluators) < self.num_aggregation_workers:\n            raise ValueError(\n                \"The number of aggregation workers should not exceed the \"\n                \"number of total evaluation workers ({} vs {})\".format(\n                    self.num_aggregation_workers, len(self.remote_evaluators)))\n\n        assigned_evaluators = collections.defaultdict(list)\n        for i, ev in enumerate(self.remote_evaluators):\n            assigned_evaluators[i % self.num_aggregation_workers].append(ev)\n\n        self.workers = aggregators\n        for i, worker in enumerate(self.workers):\n            worker.init.remote(\n                self.broadcasted_weights, assigned_evaluators[i],\n                self.max_sample_requests_in_flight_per_worker,\n                self.replay_proportion, self.replay_buffer_num_slots,\n                self.train_batch_size, self.sample_batch_size)\n\n        self.agg_tasks = TaskPool()\n        for agg in self.workers:\n            agg.set_weights.remote(self.broadcasted_weights)\n            self.agg_tasks.add(agg, agg.get_train_batches.remote())\n\n        self.initialized = True", "output": "Deferred init so that we can pass in previously created workers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def started(name=None,\n            user=None,\n            group=None,\n            chroot=None,\n            caps=None,\n            no_caps=False,\n            pidfile=None,\n            enable_core=False,\n            fd_limit=None,\n            verbose=False,\n            debug=False,\n            trace=False,\n            yydebug=False,\n            persist_file=None,\n            control=None,\n            worker_threads=None,\n            *args,\n            **kwargs):\n    '''\n    \n    '''\n    return __salt__['syslog_ng.start'](name=name,\n                                       user=user,\n                                       group=group,\n                                       chroot=chroot,\n                                       caps=caps,\n                                       no_caps=no_caps,\n                                       pidfile=pidfile,\n                                       enable_core=enable_core,\n                                       fd_limit=fd_limit,\n                                       verbose=verbose,\n                                       debug=debug,\n                                       trace=trace,\n                                       yydebug=yydebug,\n                                       persist_file=persist_file,\n                                       control=control,\n                                       worker_threads=worker_threads)", "output": "Ensures, that syslog-ng is started via the given parameters.\n\n    Users shouldn't use this function, if the service module is available on\n    their system.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def initialize_plugin_in_mainwindow_layout(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.get_option('first_time', True):\n            try:\n                self.on_first_registration()\n            except NotImplementedError:\n                return\n            self.set_option('first_time', False)", "output": "If this is the first time the plugin is shown, perform actions to\n        initialize plugin position in Spyder's window layout.\n\n        Use on_first_registration to define the actions to be run\n        by your plugin", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def release(self, on_element=None):\n        \"\"\"\n        \n        \"\"\"\n        if on_element:\n            self.move_to_element(on_element)\n        if self._driver.w3c:\n            self.w3c_actions.pointer_action.release()\n            self.w3c_actions.key_action.pause()\n        else:\n            self._actions.append(lambda: self._driver.execute(Command.MOUSE_UP, {}))\n        return self", "output": "Releasing a held mouse button on an element.\n\n        :Args:\n         - on_element: The element to mouse up.\n           If None, releases on current mouse position.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_classes(folder:Path)->FilePathList:\n    \"\"\n    classes = [d for d in folder.iterdir()\n               if d.is_dir() and not d.name.startswith('.')]\n    assert(len(classes)>0)\n    return sorted(classes, key=lambda d: d.name)", "output": "List of label subdirectories in imagenet-style `folder`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def timestamp_pb(self):\n        \"\"\"\n        \"\"\"\n        inst = self if self.tzinfo is not None else self.replace(tzinfo=pytz.UTC)\n        delta = inst - _UTC_EPOCH\n        seconds = int(delta.total_seconds())\n        nanos = self._nanosecond or self.microsecond * 1000\n        return timestamp_pb2.Timestamp(seconds=seconds, nanos=nanos)", "output": "Return a timestamp message.\n\n        Returns:\n            (:class:`~google.protobuf.timestamp_pb2.Timestamp`): Timestamp message", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        \"\"\"\n        \"\"\"\n        while True:\n            self.update_log_filenames()\n            self.open_closed_files()\n            anything_published = self.check_log_files_and_publish_updates()\n            # If nothing was published, then wait a little bit before checking\n            # for logs to avoid using too much CPU.\n            if not anything_published:\n                time.sleep(0.05)", "output": "Run the log monitor.\n\n        This will query Redis once every second to check if there are new log\n        files to monitor. It will also store those log files in Redis.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def time_partitioning(self):\n        \"\"\"\n        \"\"\"\n        prop = self._properties.get(\"timePartitioning\")\n        if prop is not None:\n            return TimePartitioning.from_api_repr(prop)", "output": "google.cloud.bigquery.table.TimePartitioning: Configures time-based\n        partitioning for a table.\n\n        Raises:\n            ValueError:\n                If the value is not :class:`TimePartitioning` or :data:`None`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def trigger(self, attr, old, new, hint=None, setter=None):\n        ''' \n\n        '''\n        def invoke():\n            callbacks = self._callbacks.get(attr)\n            if callbacks:\n                for callback in callbacks:\n                    callback(attr, old, new)\n        if hasattr(self, '_document') and self._document is not None:\n            self._document._notify_change(self, attr, old, new, hint, setter, invoke)\n        else:\n            invoke()", "output": "Trigger callbacks for ``attr`` on this object.\n\n        Args:\n            attr (str) :\n            old (object) :\n            new (object) :\n\n        Returns:\n            None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_keys(hive, key=None, use_32bit_registry=False):\n    '''\n    \n    '''\n\n    local_hive = _to_unicode(hive)\n    local_key = _to_unicode(key)\n\n    registry = Registry()\n    try:\n        hkey = registry.hkeys[local_hive]\n    except KeyError:\n        raise CommandExecutionError('Invalid Hive: {0}'.format(local_hive))\n    access_mask = registry.registry_32[use_32bit_registry]\n\n    subkeys = []\n    handle = None\n    try:\n        handle = win32api.RegOpenKeyEx(hkey, local_key, 0, access_mask)\n\n        for i in range(win32api.RegQueryInfoKey(handle)[0]):\n            subkey = win32api.RegEnumKey(handle, i)\n            if PY2:\n                subkeys.append(_to_mbcs(subkey))\n            else:\n                subkeys.append(subkey)\n\n    except Exception:  # pylint: disable=E0602\n        log.debug(r'Cannot find key: %s\\%s', hive, key, exc_info=True)\n        return False, r'Cannot find key: {0}\\{1}'.format(hive, key)\n    finally:\n        if handle:\n            handle.Close()\n\n    return subkeys", "output": "Enumerates the subkeys in a registry key or hive.\n\n    Args:\n\n       hive (str):\n            The name of the hive. Can be one of the following:\n\n                - HKEY_LOCAL_MACHINE or HKLM\n                - HKEY_CURRENT_USER or HKCU\n                - HKEY_USERS or HKU\n                - HKEY_CLASSES_ROOT or HKCR\n                - HKEY_CURRENT_CONFIG or HKCC\n\n        key (str):\n            The key (looks like a path) to the value name. If a key is not\n            passed, the keys under the hive will be returned.\n\n        use_32bit_registry (bool):\n            Accesses the 32bit portion of the registry on 64 bit installations.\n            On 32bit machines this is ignored.\n\n    Returns:\n        list: A list of keys/subkeys under the hive or key.\n\n    Usage:\n\n        .. code-block:: python\n\n            import salt.utils.win_reg\n            winreg.list_keys(hive='HKLM', key='SOFTWARE\\\\Microsoft')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def value_to_position(self, y):\n        \"\"\"\"\"\"\n        vsb = self.editor.verticalScrollBar()\n        return (y-vsb.minimum())*self.get_scale_factor()+self.offset", "output": "Convert value to position in pixels", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_python_args(fname, python_args, interact, debug, end_args):\r\n    \"\"\"\"\"\"\r\n    p_args = []\r\n    if python_args is not None:\r\n        p_args += python_args.split()\r\n    if interact:\r\n        p_args.append('-i')\r\n    if debug:\r\n        p_args.extend(['-m', 'pdb'])\r\n    if fname is not None:\r\n        if os.name == 'nt' and debug:\r\n            # When calling pdb on Windows, one has to replace backslashes by\r\n            # slashes to avoid confusion with escape characters (otherwise, \r\n            # for example, '\\t' will be interpreted as a tabulation):\r\n            p_args.append(osp.normpath(fname).replace(os.sep, '/'))\r\n        else:\r\n            p_args.append(fname)\r\n    if end_args:\r\n        p_args.extend(shell_split(end_args))\r\n    return p_args", "output": "Construct Python interpreter arguments", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resource_present(name, resource_id, resource_type, resource_options=None, cibname=None):\n    '''\n    \n    '''\n    return _item_present(name=name,\n                         item='resource',\n                         item_id=resource_id,\n                         item_type=resource_type,\n                         extra_args=resource_options,\n                         cibname=cibname)", "output": "Ensure that a resource is created\n\n    Should be run on one cluster node only\n    (there may be races)\n    Can only be run on a node with a functional pacemaker/corosync\n\n    name\n        Irrelevant, not used (recommended: {{formulaname}}__resource_present_{{resource_id}})\n    resource_id\n        name for the resource\n    resource_type\n        resource type (f.e. ocf:heartbeat:IPaddr2 or VirtualIP)\n    resource_options\n        additional options for creating the resource\n    cibname\n        use a cached CIB-file named like cibname instead of the live CIB\n\n    Example:\n\n    .. code-block:: yaml\n\n        mysql_pcs__resource_present_galera:\n            pcs.resource_present:\n                - resource_id: galera\n                - resource_type: \"ocf:heartbeat:galera\"\n                - resource_options:\n                    - 'wsrep_cluster_address=gcomm://node1.example.org,node2.example.org,node3.example.org'\n                    - '--master'\n                - cibname: cib_for_galera", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def return_buffer_contents(self, frame, force_unescaped=False):\n        \"\"\"\"\"\"\n        if not force_unescaped:\n            if frame.eval_ctx.volatile:\n                self.writeline('if context.eval_ctx.autoescape:')\n                self.indent()\n                self.writeline('return Markup(concat(%s))' % frame.buffer)\n                self.outdent()\n                self.writeline('else:')\n                self.indent()\n                self.writeline('return concat(%s)' % frame.buffer)\n                self.outdent()\n                return\n            elif frame.eval_ctx.autoescape:\n                self.writeline('return Markup(concat(%s))' % frame.buffer)\n                return\n        self.writeline('return concat(%s)' % frame.buffer)", "output": "Return the buffer contents of the frame.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filter_international_words(buf):\n        \"\"\"\n        \n        \"\"\"\n        filtered = bytearray()\n\n        # This regex expression filters out only words that have at-least one\n        # international character. The word may include one marker character at\n        # the end.\n        words = re.findall(b'[a-zA-Z]*[\\x80-\\xFF]+[a-zA-Z]*[^a-zA-Z\\x80-\\xFF]?',\n                           buf)\n\n        for word in words:\n            filtered.extend(word[:-1])\n\n            # If the last character in the word is a marker, replace it with a\n            # space as markers shouldn't affect our analysis (they are used\n            # similarly across all languages and may thus have similar\n            # frequencies).\n            last_char = word[-1:]\n            if not last_char.isalpha() and last_char < b'\\x80':\n                last_char = b' '\n            filtered.extend(last_char)\n\n        return filtered", "output": "We define three types of bytes:\n        alphabet: english alphabets [a-zA-Z]\n        international: international characters [\\x80-\\xFF]\n        marker: everything else [^a-zA-Z\\x80-\\xFF]\n\n        The input buffer can be thought to contain a series of words delimited\n        by markers. This function works to filter all words that contain at\n        least one international character. All contiguous sequences of markers\n        are replaced by a single space ascii character.\n\n        This filter applies to all scripts which do not use English characters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dump_dataflow_to_process_queue(df, size, nr_consumer):\n    \"\"\"\n    \n    \"\"\"\n    q = mp.Queue(size)\n\n    class EnqueProc(mp.Process):\n\n        def __init__(self, df, q, nr_consumer):\n            super(EnqueProc, self).__init__()\n            self.df = df\n            self.q = q\n\n        def run(self):\n            self.df.reset_state()\n            try:\n                for idx, dp in enumerate(self.df):\n                    self.q.put((idx, dp))\n            finally:\n                for _ in range(nr_consumer):\n                    self.q.put((DIE, None))\n\n    proc = EnqueProc(df, q, nr_consumer)\n    return q, proc", "output": "Convert a DataFlow to a :class:`multiprocessing.Queue`.\n    The DataFlow will only be reset in the spawned process.\n\n    Args:\n        df (DataFlow): the DataFlow to dump.\n        size (int): size of the queue\n        nr_consumer (int): number of consumer of the queue.\n            The producer will add this many of ``DIE`` sentinel to the end of the queue.\n\n    Returns:\n        tuple(queue, process):\n            The process will take data from ``df`` and fill\n            the queue, once you start it. Each element in the queue is (idx,\n            dp). idx can be the ``DIE`` sentinel when ``df`` is exhausted.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stonith_show(stonith_id, extra_args=None, cibfile=None):\n    '''\n    \n    '''\n    return item_show(item='stonith', item_id=stonith_id, extra_args=extra_args, cibfile=cibfile)", "output": "Show the value of a cluster stonith\n\n    stonith_id\n        name for the stonith resource\n    extra_args\n        additional options for the pcs stonith command\n    cibfile\n        use cibfile instead of the live CIB\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pcs.stonith_show stonith_id='eps_fence' cibfile='/tmp/2_node_cluster.cib'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_pkg_license(pkg):\n    '''\n    \n    '''\n    licenses = set()\n    cpr = \"/usr/share/doc/{0}/copyright\".format(pkg)\n    if os.path.exists(cpr):\n        with salt.utils.files.fopen(cpr) as fp_:\n            for line in salt.utils.stringutils.to_unicode(fp_.read()).split(os.linesep):\n                if line.startswith(\"License:\"):\n                    licenses.add(line.split(\":\", 1)[1].strip())\n\n    return \", \".join(sorted(licenses))", "output": "Try to get a license from the package.\n    Based on https://www.debian.org/doc/packaging-manuals/copyright-format/1.0/\n\n    :param pkg:\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _collection_match_to_index(pspath, colfilter, name, match):\n    '''\n    \n    '''\n    collection = get_webconfiguration_settings(pspath, [{'name': name, 'filter': colfilter}])[0]['value']\n    for idx, collect_dict in enumerate(collection):\n        if all(item in collect_dict.items() for item in match.items()):\n            return idx\n    return -1", "output": "Returns index of collection item matching the match dictionary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _download_single_image(label_path:Path, img_tuple:tuple, i:int, timeout:int=4) -> None:\n    \"\"\"\n     \n    \"\"\"\n    suffix = re.findall(r'\\.\\w+?(?=(?:\\?|$))', img_tuple[1])\n    suffix = suffix[0].lower() if len(suffix)>0  else '.jpg'\n    fname = f\"{i:08d}{suffix}\"\n    download_url(img_tuple[1], label_path/fname, timeout=timeout)", "output": "Downloads a single image from Google Search results to `label_path`\n    given an `img_tuple` that contains `(fname, url)` of an image to download.\n    `i` is just an iteration number `int`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_autosave_file(self, fileinfo):\n        \"\"\"\n        \n        \"\"\"\n        filename = fileinfo.filename\n        if filename not in self.name_mapping:\n            return\n        autosave_filename = self.name_mapping[filename]\n        try:\n            os.remove(autosave_filename)\n        except EnvironmentError as error:\n            action = (_('Error while removing autosave file {}')\n                      .format(autosave_filename))\n            msgbox = AutosaveErrorDialog(action, error)\n            msgbox.exec_if_enabled()\n        del self.name_mapping[filename]\n        self.stack.sig_option_changed.emit(\n                'autosave_mapping', self.name_mapping)\n        logger.debug('Removing autosave file %s', autosave_filename)", "output": "Remove autosave file for specified file.\n\n        This function also updates `self.autosave_mapping` and clears the\n        `changed_since_autosave` flag.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_base_range(rhp):\n  \"\"\"\"\"\"\n  # After starting from base, set intervals for some parameters.\n  rhp.set_float(\"learning_rate\", 0.3, 3.0, scale=rhp.LOG_SCALE)\n  rhp.set_discrete(\"learning_rate_warmup_steps\",\n                   [1000, 2000, 4000, 8000, 16000])\n  rhp.set_float(\"initializer_gain\", 0.5, 2.0)\n  rhp.set_float(\"optimizer_adam_beta1\", 0.85, 0.95)\n  rhp.set_float(\"optimizer_adam_beta2\", 0.97, 0.99)\n  rhp.set_float(\"weight_decay\", 0.0, 1e-4)", "output": "Small range of hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_namespace(\n        name,\n        **kwargs):\n    '''\n    \n    '''\n\n    meta_obj = kubernetes.client.V1ObjectMeta(name=name)\n    body = kubernetes.client.V1Namespace(metadata=meta_obj)\n    body.metadata.name = name\n\n    cfg = _setup_conn(**kwargs)\n\n    try:\n        api_instance = kubernetes.client.CoreV1Api()\n        api_response = api_instance.create_namespace(body)\n\n        return api_response.to_dict()\n    except (ApiException, HTTPError) as exc:\n        if isinstance(exc, ApiException) and exc.status == 404:\n            return None\n        else:\n            log.exception(\n                'Exception when calling '\n                'CoreV1Api->create_namespace'\n            )\n            raise CommandExecutionError(exc)\n    finally:\n        _cleanup(**cfg)", "output": "Creates a namespace with the specified name.\n\n    CLI Example:\n        salt '*' kubernetes.create_namespace salt\n        salt '*' kubernetes.create_namespace name=salt", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_one(self, item:ItemBase, processor:PreProcessors=None):\n        \"\"\n        if processor is not None: self.processor = processor\n        self.processor = listify(self.processor)\n        for p in self.processor: item = p.process_one(item)\n        return item", "output": "Apply `processor` or `self.processor` to `item`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _random_helper(random, sampler, params, shape, dtype, kwargs):\n    \"\"\"\"\"\"\n    if isinstance(params[0], Symbol):\n        for i in params[1:]:\n            assert isinstance(i, Symbol), \\\n                \"Distribution parameters must all have the same type, but got \" \\\n                \"both %s and %s.\"%(type(params[0]), type(i))\n        return sampler(*params, shape=shape, dtype=dtype, **kwargs)\n    elif isinstance(params[0], numeric_types):\n        for i in params[1:]:\n            assert isinstance(i, numeric_types), \\\n                \"Distribution parameters must all have the same type, but got \" \\\n                \"both %s and %s.\"%(type(params[0]), type(i))\n        return random(*params, shape=shape, dtype=dtype, **kwargs)\n\n    raise ValueError(\"Distribution parameters must be either Symbol or numbers, \"\n                     \"but got %s.\"%type(params[0]))", "output": "Helper function for random generators.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _computeFractionForSampleSize(sampleSizeLowerBound, total, withReplacement):\n        \"\"\"\n        \n        \"\"\"\n        fraction = float(sampleSizeLowerBound) / total\n        if withReplacement:\n            numStDev = 5\n            if (sampleSizeLowerBound < 12):\n                numStDev = 9\n            return fraction + numStDev * sqrt(fraction / total)\n        else:\n            delta = 0.00005\n            gamma = - log(delta) / total\n            return min(1, fraction + gamma + sqrt(gamma * gamma + 2 * gamma * fraction))", "output": "Returns a sampling rate that guarantees a sample of\n        size >= sampleSizeLowerBound 99.99% of the time.\n\n        How the sampling rate is determined:\n        Let p = num / total, where num is the sample size and total is the\n        total number of data points in the RDD. We're trying to compute\n        q > p such that\n          - when sampling with replacement, we're drawing each data point\n            with prob_i ~ Pois(q), where we want to guarantee\n            Pr[s < num] < 0.0001 for s = sum(prob_i for i from 0 to\n            total), i.e. the failure rate of not having a sufficiently large\n            sample < 0.0001. Setting q = p + 5 * sqrt(p/total) is sufficient\n            to guarantee 0.9999 success rate for num > 12, but we need a\n            slightly larger q (9 empirically determined).\n          - when sampling without replacement, we're drawing each data point\n            with prob_i ~ Binomial(total, fraction) and our choice of q\n            guarantees 1-delta, or 0.9999 success rate, where success rate is\n            defined the same as in sampling with replacement.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def which(exe):\n    '''\n    \n    '''\n    def wrapper(function):\n        def wrapped(*args, **kwargs):\n            if salt.utils.path.which(exe) is None:\n                raise CommandNotFoundError(\n                    'The \\'{0}\\' binary was not found in $PATH.'.format(exe)\n                )\n            return function(*args, **kwargs)\n        return identical_signature_wrapper(function, wrapped)\n    return wrapper", "output": "Decorator wrapper for salt.utils.path.which", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _one_or_more_stages_remain(self, deploymentId):\n        '''\n        \n        '''\n        stages = __salt__['boto_apigateway.describe_api_stages'](restApiId=self.restApiId,\n                                                                 deploymentId=deploymentId,\n                                                                 **self._common_aws_args).get('stages')\n        return bool(stages)", "output": "Helper function to find whether there are other stages still associated with a deployment", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_params(self, fname):\n        \"\"\"\n        \"\"\"\n        arg_params, aux_params = self.get_params_from_kv(self._arg_params, self._aux_params)\n        save_dict = {('arg:%s' % k) : v.as_in_context(mx.cpu()) for k, v in arg_params.items()}\n        save_dict.update({('aux:%s' % k) : v.as_in_context(mx.cpu()) for k, v in aux_params.items()})\n        mx.nd.save(fname, save_dict)", "output": "Saves model parameters to file.\n        Parameters\n        ----------\n        fname : str\n            Path to output param file.\n        Examples\n        --------\n        >>> # An example of saving module parameters.\n        >>> mod.save_params('myfile')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def guess_pygments_highlighter(filename):\r\n    \"\"\"\r\n\r\n    \"\"\"\r\n    try:\r\n        from pygments.lexers import get_lexer_for_filename, get_lexer_by_name\r\n    except Exception:\r\n        return TextSH\r\n    root, ext = os.path.splitext(filename)\r\n    if ext in custom_extension_lexer_mapping:\r\n        try:\r\n            lexer = get_lexer_by_name(custom_extension_lexer_mapping[ext])\r\n        except Exception:\r\n            return TextSH\r\n    else:\r\n        try:\r\n            lexer = get_lexer_for_filename(filename)\r\n        except Exception:\r\n            return TextSH\r\n    class GuessedPygmentsSH(PygmentsSH):\r\n        _lexer = lexer\r\n    return GuessedPygmentsSH", "output": "Factory to generate syntax highlighter for the given filename.\r\n\r\n    If a syntax highlighter is not available for a particular file, this\r\n    function will attempt to generate one based on the lexers in Pygments.  If\r\n    Pygments is not available or does not have an appropriate lexer, TextSH\r\n    will be returned instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_non_posix(vars):\n    \"\"\"\"\"\"\n    # set basic install directories\n    vars['LIBDEST'] = get_path('stdlib')\n    vars['BINLIBDEST'] = get_path('platstdlib')\n    vars['INCLUDEPY'] = get_path('include')\n    vars['SO'] = '.pyd'\n    vars['EXE'] = '.exe'\n    vars['VERSION'] = _PY_VERSION_SHORT_NO_DOT\n    vars['BINDIR'] = os.path.dirname(_safe_realpath(sys.executable))", "output": "Initialize the module as appropriate for NT", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_template(self, sql, **kwargs):\n        \"\"\"\n        \"\"\"\n        template = self.env.from_string(sql)\n        kwargs.update(self.context)\n        return template.render(kwargs)", "output": "Processes a sql template\n\n        >>> sql = \"SELECT '{{ datetime(2017, 1, 1).isoformat() }}'\"\n        >>> process_template(sql)\n        \"SELECT '2017-01-01T00:00:00'\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rename_datastore(datastore_name, new_datastore_name,\n                     service_instance=None):\n    '''\n    \n    '''\n    # Argument validation\n    log.trace('Renaming datastore %s to %s', datastore_name, new_datastore_name)\n    target = _get_proxy_target(service_instance)\n    datastores = salt.utils.vmware.get_datastores(\n        service_instance,\n        target,\n        datastore_names=[datastore_name])\n    if not datastores:\n        raise VMwareObjectRetrievalError('Datastore \\'{0}\\' was not found'\n                                         ''.format(datastore_name))\n    ds = datastores[0]\n    salt.utils.vmware.rename_datastore(ds, new_datastore_name)\n    return True", "output": "Renames a datastore. The datastore needs to be visible to the proxy.\n\n    datastore_name\n        Current datastore name.\n\n    new_datastore_name\n        New datastore name.\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter/ESXi host.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.rename_datastore old_name new_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert(model, feature_names, target):\n    \"\"\"\n    \"\"\"\n    if not(_HAS_SKLEARN):\n        raise RuntimeError('scikit-learn not found. scikit-learn conversion API is disabled.')\n\n    _sklearn_util.check_expected_type(model, _ensemble.GradientBoostingClassifier)\n    def is_gbr_model(m):\n        if len(m.estimators_) == 0:\n            return False\n        if hasattr(m, 'estimators_') and m.estimators_ is not None:\n            for t in m.estimators_.flatten():\n                if not hasattr(t, 'tree_') or t.tree_ is None:\n                    return False\n            return True\n        else:\n            return False\n    _sklearn_util.check_fitted(model, is_gbr_model)\n    post_evaluation_transform = None\n    if model.n_classes_ == 2:\n        base_prediction = [model.init_.prior]\n        post_evaluation_transform = 'Regression_Logistic'\n    else:\n        base_prediction = list(model.init_.priors)\n        post_evaluation_transform = 'Classification_SoftMax'\n    return _MLModel(_convert_tree_ensemble(model, feature_names, target, mode = 'classifier',\n            base_prediction = base_prediction, class_labels = model.classes_,\n            post_evaluation_transform = post_evaluation_transform))", "output": "Convert a boosted tree model to protobuf format.\n\n    Parameters\n    ----------\n    decision_tree : GradientBoostingClassifier\n        A trained scikit-learn tree model.\n\n    feature_names: [str]\n        Name of the input columns.\n\n    target: str\n        Name of the output column.\n\n    Returns\n    -------\n    model_spec: An object of type Model_pb.\n        Protobuf representation of the model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def release(self):\n        \"\"\" \n\n            \"\"\"\n        if not self.is_locked():\n            raise NotLocked(\"%s is not locked\" % self.path)\n        if not self.i_am_locking():\n            raise NotMyLock(\"%s is locked, but not by me\" % self.path)\n        remove_existing_pidfile(self.path)", "output": "Release the lock.\n\n            Removes the PID file to release the lock, or raises an\n            error if the current process does not hold the lock.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vb_clone_vm(\n    name=None,\n    clone_from=None,\n    clone_mode=0,\n    timeout=10000,\n    **kwargs\n):\n    '''\n    \n    '''\n    vbox = vb_get_box()\n    log.info('Clone virtualbox machine %s from %s', name, clone_from)\n\n    source_machine = vbox.findMachine(clone_from)\n\n    groups = None\n    os_type_id = 'Other'\n    new_machine = vbox.createMachine(\n        None,  # Settings file\n        name,\n        groups,\n        os_type_id,\n        None  # flags\n    )\n\n    progress = source_machine.cloneTo(\n        new_machine,\n        clone_mode,  # CloneMode\n        None  # CloneOptions : None = Full?\n    )\n\n    progress.waitForCompletion(timeout)\n    log.info('Finished cloning %s from %s', name, clone_from)\n\n    vbox.registerMachine(new_machine)\n\n    return vb_xpcom_to_attribute_dict(new_machine, 'IMachine')", "output": "Tells virtualbox to create a VM by cloning from an existing one\n\n    @param name: Name for the new VM\n    @type name: str\n    @param clone_from:\n    @type clone_from: str\n    @param timeout: maximum time in milliseconds to wait or -1 to wait indefinitely\n    @type timeout: int\n    @return dict of resulting VM", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dt_to_float_ordinal(dt):\n    \"\"\"\n    \n    \"\"\"\n    if (isinstance(dt, (np.ndarray, Index, ABCSeries)\n                   ) and is_datetime64_ns_dtype(dt)):\n        base = dates.epoch2num(dt.asi8 / 1.0E9)\n    else:\n        base = dates.date2num(dt)\n    return base", "output": "Convert :mod:`datetime` to the Gregorian date as UTC float days,\n    preserving hours, minutes, seconds and microseconds.  Return value\n    is a :func:`float`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _indent(s_, numSpaces):\n    \"\"\"\n    \"\"\"\n    s = s_.split('\\n')\n    if len(s) == 1:\n        return s_\n    first = s.pop(0)\n    s = [first] + [(numSpaces * ' ') + line for line in s]\n    s = '\\n'.join(s)\n    return s", "output": "Indent string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def diff(file_, imports):\n    \"\"\"\"\"\"\n    modules_not_imported = compare_modules(file_, imports)\n\n    logging.info(\"The following modules are in {} but do not seem to be imported: \"\n                 \"{}\".format(file_, \", \".join(x for x in modules_not_imported)))", "output": "Display the difference between modules in a file and imported modules.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_private_ip(linode_id):\n    \n    '''\n    kwargs = {'LinodeID': linode_id}\n    result = _query('linode', 'ip.addprivate', args=kwargs)\n\n    return _clean_data(result)", "output": "r'''\n    Creates a private IP for the specified Linode.\n\n    linode_id\n        The ID of the Linode to create the IP address for.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_default_locale(code: str) -> None:\n    \"\"\"\n    \"\"\"\n    global _default_locale\n    global _supported_locales\n    _default_locale = code\n    _supported_locales = frozenset(list(_translations.keys()) + [_default_locale])", "output": "Sets the default locale.\n\n    The default locale is assumed to be the language used for all strings\n    in the system. The translations loaded from disk are mappings from\n    the default locale to the destination locale. Consequently, you don't\n    need to create a translation file for the default locale.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_datasource(jboss_config, name, profile=None):\n    '''\n    \n       '''\n    log.debug(\"======================== MODULE FUNCTION: jboss7.remove_datasource, name=%s, profile=%s\", name, profile)\n\n    operation = '/subsystem=datasources/data-source={name}:remove'.format(name=name)\n    if profile is not None:\n        operation = '/profile=\"{profile}\"'.format(profile=profile) + operation\n\n    return __salt__['jboss7_cli.run_operation'](jboss_config, operation, fail_on_error=False)", "output": "Remove an existing datasource from the running jboss instance.\n\n    jboss_config\n        Configuration dictionary with properties specified above.\n    name\n        Datasource name\n    profile\n        The profile (JBoss domain mode only)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' jboss7.remove_datasource '{\"cli_path\": \"integration.modules.sysmod.SysModuleTest.test_valid_docs\", \"controller\": \"10.11.12.13:9999\", \"cli_user\": \"jbossadm\", \"cli_password\": \"jbossadm\"}' my_datasource_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_set_from_file(filename: str) -> Set[str]:\n    \"\"\"\n    \n    \"\"\"\n    collection = set()\n    with open(filename, 'r') as file_:\n        for line in file_:\n            collection.add(line.rstrip())\n    return collection", "output": "Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fill_memory_slot(memory, value, index):\n  \"\"\"\n\n  \"\"\"\n  mask = tf.to_float(\n      tf.one_hot(index,\n                 tf.shape(memory)[0])[:, None, None, None])\n  fill_memory = (1 - mask) * memory + mask * value[None, ...]\n  return fill_memory", "output": "Fills the memory slot at a particular index with the given value.\n\n  Args:\n    memory: a 4-d tensor [memory_size, batch, length, channel] containing\n      the state of all steps\n    value: a 3-d tensor [batch, length, channel] as the sate\n    index: integer in [0, memory_size)\n\n  Returns:\n    filled memory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cancel_port_forward(self, address, port):\n        \"\"\"\n        \n        \"\"\"\n        if not self.active:\n            return\n        self._tcp_handler = None\n        self.global_request(\"cancel-tcpip-forward\", (address, port), wait=True)", "output": "Ask the server to cancel a previous port-forwarding request.  No more\n        connections to the given address & port will be forwarded across this\n        ssh connection.\n\n        :param str address: the address to stop forwarding\n        :param int port: the port to stop forwarding", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _len_lcs(x, y):\n  \"\"\"\n  \"\"\"\n  table = _lcs(x, y)\n  n, m = len(x), len(y)\n  return table[n, m]", "output": "Returns the length of the Longest Common Subsequence between two seqs.\n\n  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n  Args:\n    x: sequence of words\n    y: sequence of words\n\n  Returns\n    integer: Length of LCS between x and y", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _AddPropertiesForField(field, cls):\n  \"\"\"\n  \"\"\"\n  # Catch it if we add other types that we should\n  # handle specially here.\n  assert _FieldDescriptor.MAX_CPPTYPE == 10\n\n  constant_name = field.name.upper() + \"_FIELD_NUMBER\"\n  setattr(cls, constant_name, field.number)\n\n  if field.label == _FieldDescriptor.LABEL_REPEATED:\n    _AddPropertiesForRepeatedField(field, cls)\n  elif field.cpp_type == _FieldDescriptor.CPPTYPE_MESSAGE:\n    _AddPropertiesForNonRepeatedCompositeField(field, cls)\n  else:\n    _AddPropertiesForNonRepeatedScalarField(field, cls)", "output": "Adds a public property for a protocol message field.\n  Clients can use this property to get and (in the case\n  of non-repeated scalar fields) directly set the value\n  of a protocol message field.\n\n  Args:\n    field: A FieldDescriptor for this field.\n    cls: The class we're constructing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_feature (name):\n    \"\"\" \n    \"\"\"\n    assert isinstance(name, basestring)\n    if name not in __all_features:\n        raise InvalidFeature (\"'%s' is not a valid feature name\" % name)\n    else:\n        return __all_features[name]", "output": "Checks if all name is a valid feature. Otherwise, raises an exception.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def view(self, cls=None):\n        \"\"\"  \"\"\"\n        result = self.copy()\n        result._id = self._id\n        return result", "output": "this is defined as a copy with the same identity", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_expire(name, date):\n    '''\n    \n    '''\n    _set_account_policy(\n        name, 'usingHardExpirationDate=1 hardExpireDateGMT={0}'.format(date))\n\n    return get_expire(name) == date", "output": "Sets the date on which the account expires. The user will not be able to\n    login after this date. Date format is mm/dd/yyyy\n\n    :param str name: The name of the user account\n\n    :param datetime date: The date the account will expire. Format must be\n        mm/dd/yyyy.\n\n    :return: True if successful, False if not\n    :rtype: bool\n\n    :raises: CommandExecutionError on user not found or any other unknown error\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' shadow.set_expire username 07/23/2015", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_installation_local(name):\n    \"\"\"\n    \"\"\"\n    loc = os.path.normcase(pkg_resources.working_set.by_key[name].location)\n    pre = os.path.normcase(sys.prefix)\n    return os.path.commonprefix([loc, pre]) == pre", "output": "Check whether the distribution is in the current Python installation.\n\n    This is used to distinguish packages seen by a virtual environment. A venv\n    may be able to see global packages, but we don't want to mess with them.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_contained_extras(marker):\n    \"\"\"\n    \"\"\"\n    if not marker:\n        return set()\n    marker = Marker(str(marker))\n    extras = set()\n    _markers_collect_extras(marker._markers, extras)\n    return extras", "output": "Collect \"extra == ...\" operands from a marker.\n\n    Returns a list of str. Each str is a speficied extra in this marker.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def split_by_rand_pct(self, valid_pct:float=0.2, seed:int=None)->'ItemLists':\n        \"\"\n        if valid_pct==0.: return self.split_none()\n        if seed is not None: np.random.seed(seed)\n        rand_idx = np.random.permutation(range_of(self))\n        cut = int(valid_pct * len(self))\n        return self.split_by_idx(rand_idx[:cut])", "output": "Split the items randomly by putting `valid_pct` in the validation set, optional `seed` can be passed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_histogram(self, param_name:str, values)->None:\n        \"\"\n        tag = self.name + '/weights/' + param_name\n        self.tbwriter.add_histogram(tag=tag, values=values, global_step=self.iteration)", "output": "Writes single model histogram to Tensorboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def revnet_base():\n  \"\"\"\"\"\"\n  hparams = common_hparams.basic_params1()\n  hparams.add_hparam('num_channels', [64, 128, 256, 416])\n  hparams.add_hparam('num_layers_per_block', [1, 1, 10, 1])\n  hparams.add_hparam('bottleneck', True)\n  hparams.add_hparam('first_batch_norm', [False, True, True, True])\n  hparams.add_hparam('init_stride', 2)\n  hparams.add_hparam('init_kernel_size', 7)\n  hparams.add_hparam('init_maxpool', True)\n  hparams.add_hparam('strides', [1, 2, 2, 2])\n  hparams.add_hparam('num_channels_init_block', 64)\n  hparams.add_hparam('dim', '2d')\n\n  # Variable init\n  hparams.initializer = 'normal_unit_scaling'\n  hparams.initializer_gain = 2.\n\n  # Optimization\n  hparams.optimizer = 'Momentum'\n  hparams.optimizer_momentum_momentum = 0.9\n  hparams.optimizer_momentum_nesterov = True\n  hparams.weight_decay = 1e-4\n  hparams.clip_grad_norm = 0.0\n  # (base_lr=0.1) * (batch_size=128*8 (on TPU, or 8 GPUs)=1024) / (256.)\n  hparams.learning_rate = 0.4\n  hparams.learning_rate_decay_scheme = 'cosine'\n  # For image_imagenet224, 120k training steps, which effectively makes this a\n  # cosine decay (i.e. no cycles).\n  hparams.learning_rate_cosine_cycle_steps = 120000\n\n  # Can run with a batch size of 128 with Problem ImageImagenet224\n  hparams.batch_size = 128\n  return hparams", "output": "Default hparams for Revnet.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_value_from_value_pb(value_pb):\n    \"\"\"\n    \"\"\"\n    value_type = value_pb.WhichOneof(\"value_type\")\n\n    if value_type == \"timestamp_value\":\n        result = _pb_timestamp_to_datetime(value_pb.timestamp_value)\n\n    elif value_type == \"key_value\":\n        result = key_from_protobuf(value_pb.key_value)\n\n    elif value_type == \"boolean_value\":\n        result = value_pb.boolean_value\n\n    elif value_type == \"double_value\":\n        result = value_pb.double_value\n\n    elif value_type == \"integer_value\":\n        result = value_pb.integer_value\n\n    elif value_type == \"string_value\":\n        result = value_pb.string_value\n\n    elif value_type == \"blob_value\":\n        result = value_pb.blob_value\n\n    elif value_type == \"entity_value\":\n        result = entity_from_protobuf(value_pb.entity_value)\n\n    elif value_type == \"array_value\":\n        result = [\n            _get_value_from_value_pb(value) for value in value_pb.array_value.values\n        ]\n\n    elif value_type == \"geo_point_value\":\n        result = GeoPoint(\n            value_pb.geo_point_value.latitude, value_pb.geo_point_value.longitude\n        )\n\n    elif value_type == \"null_value\":\n        result = None\n\n    else:\n        raise ValueError(\"Value protobuf did not have any value set\")\n\n    return result", "output": "Given a protobuf for a Value, get the correct value.\n\n    The Cloud Datastore Protobuf API returns a Property Protobuf which\n    has one value set and the rest blank.  This function retrieves the\n    the one value provided.\n\n    Some work is done to coerce the return value into a more useful type\n    (particularly in the case of a timestamp value, or a key value).\n\n    :type value_pb: :class:`.entity_pb2.Value`\n    :param value_pb: The Value Protobuf.\n\n    :rtype: object\n    :returns: The value provided by the Protobuf.\n    :raises: :class:`ValueError <exceptions.ValueError>` if no value type\n             has been set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forward(self, x, mask):\n        \"\"\"\"\"\"\n        all_layers = []\n        for layer in self.layers:\n            x = layer(x, mask)\n            if self.return_all_layers:\n                all_layers.append(x)\n\n        if self.return_all_layers:\n            all_layers[-1] = self.norm(all_layers[-1])\n            return all_layers\n        return self.norm(x)", "output": "Pass the input (and mask) through each layer in turn.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xdg_config_dir():\n    '''\n    \n    '''\n    xdg_config = os.getenv('XDG_CONFIG_HOME', os.path.expanduser('~/.config'))\n    xdg_config_directory = os.path.join(xdg_config, 'salt')\n    return xdg_config_directory", "output": "Check xdg locations for config files", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ext_pillar(minion_id,\n               pillar,  # pylint: disable=W0613\n               conf):\n    '''\n    \n    '''\n    comps = conf.split()\n\n    profile = None\n    if comps[0]:\n        profile = comps[0]\n    client = salt.utils.etcd_util.get_conn(__opts__, profile)\n\n    path = '/'\n    if len(comps) > 1 and comps[1].startswith('root='):\n        path = comps[1].replace('root=', '')\n\n    # put the minion's ID in the path if necessary\n    path %= {\n        'minion_id': minion_id\n    }\n\n    try:\n        pillar = salt.utils.etcd_util.tree(client, path)\n    except KeyError:\n        log.error('No such key in etcd profile %s: %s', profile, path)\n        pillar = {}\n\n    return pillar", "output": "Check etcd for all data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_rand(num_digit_min, num_digit_max):\n        \"\"\"\n        \"\"\"\n        buf = \"\"\n        max_len = random.randint(num_digit_min, num_digit_max)\n        for i in range(max_len):\n            buf += str(random.randint(0, 9))\n        return buf", "output": "Generates a character string of digits. Number of digits are\n        between self.num_digit_min and self.num_digit_max\n        Returns\n        -------\n        str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_undeclared(nodes, names):\n    \"\"\"\n    \"\"\"\n    visitor = UndeclaredNameVisitor(names)\n    try:\n        for node in nodes:\n            visitor.visit(node)\n    except VisitorExit:\n        pass\n    return visitor.undeclared", "output": "Check if the names passed are accessed undeclared.  The return value\n    is a set of all the undeclared names from the sequence of names found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_stock_xdxr(code, format='pd', collections=DATABASE.stock_xdxr):\n    ''\n    code = QA_util_code_tolist(code)\n    data = pd.DataFrame([item for item in collections.find(\n        {'code':  {'$in': code}}, batch_size=10000)]).drop(['_id'], axis=1)\n    data['date'] = pd.to_datetime(data['date'])\n    return data.set_index('date', drop=False)", "output": "\u83b7\u53d6\u80a1\u7968\u9664\u6743\u4fe1\u606f/\u6570\u636e\u5e93", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def iter_entry_points(self, group, name=None):\n        \"\"\"\n        \"\"\"\n        return (\n            entry\n            for dist in self\n            for entry in dist.get_entry_map(group).values()\n            if name is None or name == entry.name\n        )", "output": "Yield entry point objects from `group` matching `name`\n\n        If `name` is None, yields all entry points in `group` from all\n        distributions in the working set, otherwise only ones matching\n        both `group` and `name` are yielded (in distribution order).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_params(self, deep=False):\n        \"\"\"\"\"\"\n        params = super(XGBModel, self).get_params(deep=deep)\n        if params['missing'] is np.nan:\n            params['missing'] = None  # sklearn doesn't handle nan. see #4725\n        if not params.get('eval_metric', True):\n            del params['eval_metric']  # don't give as None param to Booster\n        return params", "output": "Get parameter.s", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_predicate_indices(tags: List[str]) -> List[int]:\n    \"\"\"\n    \n    \"\"\"\n    return [ind for ind, tag in enumerate(tags) if 'V' in tag]", "output": "Return the word indices of a predicate in BIO tags.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_parameters(self, parameter_id):\n        \"\"\"\n        \n        \"\"\"\n        total_params = self.get_suggestion(random_search=False)\n        # avoid generating same parameter with concurrent trials because hyperopt doesn't support parallel mode\n        if total_params in self.total_data.values():\n            # but it can cause deplicate parameter rarely\n            total_params = self.get_suggestion(random_search=True)\n        self.total_data[parameter_id] = total_params\n        params = _split_index(total_params)\n        return params", "output": "Returns a set of trial (hyper-)parameters, as a serializable object.\n\n        Parameters\n        ----------\n        parameter_id : int\n\n        Returns\n        -------\n        params : dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def example_generator(all_files, urls_path, sum_token):\n  \"\"\"\"\"\"\n\n  def fix_run_on_sents(line):\n    if u\"@highlight\" in line:\n      return line\n    if not line:\n      return line\n    if line[-1] in END_TOKENS:\n      return line\n    return line + u\".\"\n\n  filelist = example_splits(urls_path, all_files)\n  story_summary_split_token = u\" <summary> \" if sum_token else \" \"\n\n  for story_file in filelist:\n    story = []\n    summary = []\n    reading_highlights = False\n    for line in tf.gfile.Open(story_file, \"rb\"):\n      line = text_encoder.to_unicode_utf8(line.strip())\n      line = fix_run_on_sents(line)\n      if not line:\n        continue\n      elif line.startswith(u\"@highlight\"):\n        if not story:\n          break  # No article text.\n        reading_highlights = True\n      elif reading_highlights:\n        summary.append(line)\n      else:\n        story.append(line)\n\n    if (not story) or not summary:\n      continue\n\n    yield \" \".join(story) + story_summary_split_token + \" \".join(summary)", "output": "Generate examples.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def system(session):\n    \"\"\"\"\"\"\n    system_test_path = os.path.join(\"tests\", \"system.py\")\n    system_test_folder_path = os.path.join(\"tests\", \"system\")\n    # Sanity check: Only run tests if the environment variable is set.\n    if not os.environ.get(\"GOOGLE_APPLICATION_CREDENTIALS\", \"\"):\n        session.skip(\"Credentials must be set via environment variable\")\n\n    system_test_exists = os.path.exists(system_test_path)\n    system_test_folder_exists = os.path.exists(system_test_folder_path)\n    # Sanity check: only run tests if found.\n    if not system_test_exists and not system_test_folder_exists:\n        session.skip(\"System tests were not found\")\n\n    # Use pre-release gRPC for system tests.\n    session.install(\"--pre\", \"grpcio\")\n\n    # Install all test dependencies, then install this package into the\n    # virtualenv's dist-packages.\n    session.install(\"mock\", \"pytest\")\n    for local_dep in LOCAL_DEPS:\n        session.install(\"-e\", local_dep)\n    session.install(\"-e\", \"../test_utils/\")\n    session.install(\"-e\", \".\")\n\n    # Run py.test against the system tests.\n    if system_test_exists:\n        session.run(\"py.test\", \"--quiet\", system_test_path, *session.posargs)\n    if system_test_folder_exists:\n        session.run(\"py.test\", \"--quiet\", system_test_folder_path, *session.posargs)", "output": "Run the system test suite.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_tools(self, *tools):\n        ''' \n\n        '''\n        for tool in tools:\n            if not isinstance(tool, Tool):\n                raise ValueError(\"All arguments to add_tool must be Tool subclasses.\")\n\n            self.toolbar.tools.append(tool)", "output": "Adds tools to the plot.\n\n        Args:\n            *tools (Tool) : the tools to add to the Plot\n\n        Returns:\n            None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _chunked_selector_output_shape(  # pylint: disable=invalid-name\n    input_shapes, selector=None, **unused_kwargs):\n  \"\"\"\"\"\"\n  # Read the main function below first, the shape logic just follows the ops.\n  selector = selector or (lambda x: [] if x < 1 else [x-1])\n  triples, _ = zip(*input_shapes)\n  (query_shapes, key_shapes, value_shapes) = zip(*triples)\n  result = []\n  for i in range(len(input_shapes)):\n    selected = selector(i)\n    cur_key_shape, cur_value_shape = key_shapes[i], value_shapes[i]\n    # Since keys and values are [batch, length, depth] we concatenate on axis=1.\n    new_key_len = sum([key_shapes[j][1] for j in selected]) + cur_key_shape[1]\n    new_key_shape = (cur_key_shape[0], new_key_len, cur_key_shape[2])\n    new_value_len = sum(\n        [value_shapes[j][1] for j in selected]) + cur_value_shape[1]\n    new_value_shape = (cur_value_shape[0], new_value_len, cur_value_shape[2])\n    # Masks are (1, query-len, key-len).\n    new_mask_shape = (1, query_shapes[i][1], new_key_len)\n    new_shape = ((query_shapes[i], new_key_shape, new_value_shape),\n                 new_mask_shape)\n    result.append(new_shape)\n  return tuple(result)", "output": "Helper: calculate output shape for chunked key selector (see below).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def query_boost_version(boost_root):\n        '''\n        \n        '''\n        boost_version = None\n        if os.path.exists(os.path.join(boost_root,'Jamroot')):\n            with codecs.open(os.path.join(boost_root,'Jamroot'), 'r', 'utf-8') as f:\n                for line in f.readlines():\n                    parts = line.split()\n                    if len(parts) >= 5 and parts[1] == 'BOOST_VERSION':\n                        boost_version = parts[3]\n                        break\n        if not boost_version:\n            boost_version = 'default'\n        return boost_version", "output": "Read in the Boost version from a given boost_root.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_table_from_object(self, obj):\n        '''\n        \n        '''\n        get_type = lambda item: str(type(item)).split(\"'\")[1]\n        if not os.path.exists(os.path.join(self.db_path, obj._TABLE)):\n            with gzip.open(os.path.join(self.db_path, obj._TABLE), 'wb') as table_file:\n                csv.writer(table_file).writerow(['{col}:{type}'.format(col=elm[0], type=get_type(elm[1]))\n                                                 for elm in tuple(obj.__dict__.items())])\n            self._tables[obj._TABLE] = self._load_table(obj._TABLE)", "output": "Create a table from the object.\n        NOTE: This method doesn't stores anything.\n\n        :param obj:\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_fun(fun):\n    '''\n    \n    '''\n    log.debug('sqlite3 returner <get_fun> called fun: %s', fun)\n    conn = _get_conn(ret=None)\n    cur = conn.cursor()\n    sql = '''SELECT s.id, s.full_ret, s.jid\n            FROM salt_returns s\n            JOIN ( SELECT MAX(jid) AS jid FROM salt_returns GROUP BY fun, id) max\n            ON s.jid = max.jid\n            WHERE s.fun = :fun\n            '''\n    cur.execute(sql,\n                {'fun': fun})\n    data = cur.fetchall()\n    ret = {}\n    if data:\n        # Pop the jid off the list since it is not\n        # needed and I am trying to get a perfect\n        # pylint score :-)\n        data.pop()\n        for minion, ret in data:\n            ret[minion] = salt.utils.json.loads(ret)\n    _close_conn(conn)\n    return ret", "output": "Return a dict of the last function called for all minions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_response_card(title, subtitle, options):\n    \"\"\"\n    \n    \"\"\"\n    buttons = None\n    if options is not None:\n        buttons = []\n        for i in range(min(5, len(options))):\n            buttons.append(options[i])\n\n    return {\n        'contentType': 'application/vnd.amazonaws.card.generic',\n        'version': 1,\n        'genericAttachments': [{\n            'title': title,\n            'subTitle': subtitle,\n            'buttons': buttons\n        }]\n    }", "output": "Build a responseCard with a title, subtitle, and an optional set of options which should be displayed as buttons.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def font_parse_string(font):\n    \"\"\"\n    \n    \"\"\"\n\n    if font is None:\n        return ''\n\n    if type(font) is str:\n        _font = font.split(' ')\n    else:\n        _font = font\n    family = _font[0]\n    point_size = int(_font[1])\n\n    style = _font[2:] if len(_font) > 1 else None\n\n    # underline =  'underline' in _font[2:]\n    # bold =  'bold' in _font\n\n    return family, point_size, style", "output": "Convert from font string/tyuple into a Qt style sheet string\n    :param font: \"Arial 10 Bold\" or ('Arial', 10, 'Bold)\n    :return: style string that can be combined with other style strings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def model_path(cls, project, location, model):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/locations/{location}/models/{model}\",\n            project=project,\n            location=location,\n            model=model,\n        )", "output": "Return a fully-qualified model string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ip_sort(ip):\n    ''''''\n    idx = '001'\n    if ip == '127.0.0.1':\n        idx = '200'\n    if ip == '::1':\n        idx = '201'\n    elif '::' in ip:\n        idx = '100'\n    return '{0}___{1}'.format(idx, ip)", "output": "Ip sorting", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def append_payload(self, payload: Payload) -> Payload:\n        \"\"\"\"\"\"\n        # compression\n        encoding = payload.headers.get(CONTENT_ENCODING, '').lower()  # type: Optional[str]  # noqa\n        if encoding and encoding not in ('deflate', 'gzip', 'identity'):\n            raise RuntimeError('unknown content encoding: {}'.format(encoding))\n        if encoding == 'identity':\n            encoding = None\n\n        # te encoding\n        te_encoding = payload.headers.get(\n            CONTENT_TRANSFER_ENCODING, '').lower()  # type: Optional[str]  # noqa\n        if te_encoding not in ('', 'base64', 'quoted-printable', 'binary'):\n            raise RuntimeError('unknown content transfer encoding: {}'\n                               ''.format(te_encoding))\n        if te_encoding == 'binary':\n            te_encoding = None\n\n        # size\n        size = payload.size\n        if size is not None and not (encoding or te_encoding):\n            payload.headers[CONTENT_LENGTH] = str(size)\n\n        self._parts.append((payload, encoding, te_encoding))  # type: ignore\n        return payload", "output": "Adds a new body part to multipart writer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_qsub_command(cmd, job_name, outfile, errfile, pe, n_cpu):\n    \"\"\"\"\"\"\n    qsub_template = \"\"\"echo {cmd} | qsub -o \":{outfile}\" -e \":{errfile}\" -V -r y -pe {pe} {n_cpu} -N {job_name}\"\"\"\n    return qsub_template.format(\n        cmd=cmd, job_name=job_name, outfile=outfile, errfile=errfile,\n        pe=pe, n_cpu=n_cpu)", "output": "Submit shell command to SGE queue via `qsub`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def subscribe(self, tag=None, match_type=None):\n        '''\n        \n        '''\n        if tag is None:\n            return\n        match_func = self._get_match_func(match_type)\n        self.pending_tags.append([tag, match_func])", "output": "Subscribe to events matching the passed tag.\n\n        If you do not subscribe to a tag, events will be discarded by calls to\n        get_event that request a different tag. In contexts where many different\n        jobs are outstanding it is important to subscribe to prevent one call\n        to get_event from discarding a response required by a subsequent call\n        to get_event.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def explain(self, extended=False):\n        \"\"\"\n        \"\"\"\n        if extended:\n            print(self._jdf.queryExecution().toString())\n        else:\n            print(self._jdf.queryExecution().simpleString())", "output": "Prints the (logical and physical) plans to the console for debugging purpose.\n\n        :param extended: boolean, default ``False``. If ``False``, prints only the physical plan.\n\n        >>> df.explain()\n        == Physical Plan ==\n        *(1) Scan ExistingRDD[age#0,name#1]\n\n        >>> df.explain(True)\n        == Parsed Logical Plan ==\n        ...\n        == Analyzed Logical Plan ==\n        ...\n        == Optimized Logical Plan ==\n        ...\n        == Physical Plan ==\n        ...", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_thread(self, checker, end_callback, source_code, parent):\r\n        \"\"\"\"\"\"\r\n        parent_id = id(parent)\r\n        thread = AnalysisThread(self, checker, source_code)\r\n        self.end_callbacks[id(thread)] = end_callback\r\n        self.pending_threads.append((thread, parent_id))\r\n        logger.debug(\"Added thread %r to queue\" % thread)\r\n        QTimer.singleShot(50, self.update_queue)", "output": "Add thread to queue", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def aligned_array(size, dtype, align=64):\n    \"\"\"\n    \"\"\"\n\n    n = size * dtype.itemsize\n    empty = np.empty(n + (align - 1), dtype=np.uint8)\n    data_align = empty.ctypes.data % align\n    offset = 0 if data_align == 0 else (align - data_align)\n    output = empty[offset:offset + n].view(dtype)\n\n    assert len(output) == size, len(output)\n    assert output.ctypes.data % align == 0, output.ctypes.data\n    return output", "output": "Returns an array of a given size that is 64-byte aligned.\n\n    The returned array can be efficiently copied into GPU memory by TensorFlow.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def max(self, key=None):\n        \"\"\"\n        \n        \"\"\"\n        if key is None:\n            return self.reduce(max)\n        return self.reduce(lambda a, b: max(a, b, key=key))", "output": "Find the maximum item in this RDD.\n\n        :param key: A function used to generate key for comparing\n\n        >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n        >>> rdd.max()\n        43.0\n        >>> rdd.max(key=str)\n        5.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(device_id, **params):\n    '''\n    \n    '''\n    params = _clean_salt_variables(params)\n\n    api_response = requests.put(\n        'https://api.serverdensity.io/inventory/devices/' + device_id,\n        params={'token': get_sd_auth('api_token')},\n        data=params\n    )\n    log.debug('Server Density API Response: %s', api_response)\n    log.debug('Server Density API Response content: %s', api_response.content)\n    if api_response.status_code == 200:\n        try:\n            return salt.utils.json.loads(api_response.content)\n        except ValueError:\n            log.error(\n                'Could not parse Server Density API Response content: %s',\n                api_response.content\n            )\n            raise CommandExecutionError(\n                'Failed to create, API Response: {0}'.format(api_response)\n            )\n    else:\n        return None", "output": "Updates device information in Server Density. For more information see the\n    `API docs`__.\n\n    .. __: https://apidocs.serverdensity.com/Inventory/Devices/Updating\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' serverdensity_device.update 51f7eafcdba4bb235e000ae4 name=lama group=lama_band\n        salt '*' serverdensity_device.update 51f7eafcdba4bb235e000ae4 name=better_lama group=rock_lamas swapSpace=512", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def recompute_grad(fn):\n  \"\"\"\n  \"\"\"\n\n  @functools.wraps(fn)\n  def wrapped(*args):\n    return _recompute_grad(fn, args)\n\n  return wrapped", "output": "Decorator that recomputes the function on the backwards pass.\n\n  Args:\n    fn: a function that takes Tensors (all as positional arguments) and returns\n      a tuple of Tensors.\n\n  Returns:\n    A wrapped fn that is identical to fn when called, but its activations will\n    be discarded and recomputed on the backwards pass (i.e. on a call to\n    tf.gradients).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def require_minimum_pyarrow_version():\n    \"\"\" \n    \"\"\"\n    # TODO(HyukjinKwon): Relocate and deduplicate the version specification.\n    minimum_pyarrow_version = \"0.12.1\"\n\n    from distutils.version import LooseVersion\n    try:\n        import pyarrow\n        have_arrow = True\n    except ImportError:\n        have_arrow = False\n    if not have_arrow:\n        raise ImportError(\"PyArrow >= %s must be installed; however, \"\n                          \"it was not found.\" % minimum_pyarrow_version)\n    if LooseVersion(pyarrow.__version__) < LooseVersion(minimum_pyarrow_version):\n        raise ImportError(\"PyArrow >= %s must be installed; however, \"\n                          \"your version was %s.\" % (minimum_pyarrow_version, pyarrow.__version__))", "output": "Raise ImportError if minimum version of pyarrow is not installed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parseURIReference(self, str):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlParseURIReference(self._o, str)\n        return ret", "output": "Parse an URI reference string based on RFC 3986 and fills\n          in the appropriate fields of the @uri structure\n           URI-reference = URI / relative-ref", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def route_filter_create_or_update(name, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    if 'location' not in kwargs:\n        rg_props = __salt__['azurearm_resource.resource_group_get'](\n            resource_group, **kwargs\n        )\n\n        if 'error' in rg_props:\n            log.error(\n                'Unable to determine location from resource group specified.'\n            )\n            return False\n        kwargs['location'] = rg_props['location']\n\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n\n    try:\n        rt_filter_model = __utils__['azurearm.create_object_model']('network', 'RouteFilter', **kwargs)\n    except TypeError as exc:\n        result = {'error': 'The object model could not be built. ({0})'.format(str(exc))}\n        return result\n\n    try:\n        rt_filter = netconn.route_filters.create_or_update(\n            resource_group_name=resource_group,\n            route_filter_name=name,\n            route_filter_parameters=rt_filter_model\n        )\n        rt_filter.wait()\n        rt_result = rt_filter.result()\n        result = rt_result.as_dict()\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n    except SerializationError as exc:\n        result = {'error': 'The object model could not be parsed. ({0})'.format(str(exc))}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Create or update a route filter within a specified resource group.\n\n    :param name: The name of the route filter to create.\n\n    :param resource_group: The resource group name assigned to the\n        route filter.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.route_filter_create_or_update test-filter testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tenant_list(profile=None, **connection_args):\n    '''\n    \n    '''\n    kstone = auth(profile, **connection_args)\n    ret = {}\n\n    for tenant in getattr(kstone, _TENANTS, None).list():\n        ret[tenant.name] = dict((value, getattr(tenant, value)) for value in dir(tenant)\n                                if not value.startswith('_') and\n                                isinstance(getattr(tenant, value), (six.string_types, dict, bool)))\n    return ret", "output": "Return a list of available tenants (keystone tenants-list)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystone.tenant_list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_signing_file(self, keyid, signing_file):\n        '''\n        \n        '''\n        if not signing_file or not os.path.exists(signing_file):\n            return False\n\n        if not self.check_permissions(signing_file):\n            log.warning('Wrong permissions for %s, ignoring content', signing_file)\n            return False\n\n        mtime = os.path.getmtime(signing_file)\n        if self.signing_files.get(signing_file, {}).get('mtime') != mtime:\n            self.signing_files.setdefault(signing_file, {})['mtime'] = mtime\n            with salt.utils.files.fopen(signing_file, 'r') as fp_:\n                self.signing_files[signing_file]['data'] = [\n                    entry for entry in [line.strip() for line in fp_] if not entry.strip().startswith('#')\n                ]\n        return any(salt.utils.stringutils.expr_match(keyid, line) for line\n                   in self.signing_files[signing_file].get('data', []))", "output": "Check a keyid for membership in a signing file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_name_version_sep(egg_info, canonical_name):\n    # type: (str, str) -> int\n    \"\"\"\n    \"\"\"\n    # Project name and version must be separated by one single dash. Find all\n    # occurrences of dashes; if the string in front of it matches the canonical\n    # name, this is the one separating the name and version parts.\n    for i, c in enumerate(egg_info):\n        if c != \"-\":\n            continue\n        if canonicalize_name(egg_info[:i]) == canonical_name:\n            return i\n    raise ValueError(\"{} does not match {}\".format(egg_info, canonical_name))", "output": "Find the separator's index based on the package's canonical name.\n\n    `egg_info` must be an egg info string for the given package, and\n    `canonical_name` must be the package's canonical name.\n\n    This function is needed since the canonicalized name does not necessarily\n    have the same length as the egg info's name part. An example::\n\n    >>> egg_info = 'foo__bar-1.0'\n    >>> canonical_name = 'foo-bar'\n    >>> _find_name_version_sep(egg_info, canonical_name)\n    8", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render_config(config: Config, indent: str = \"\") -> str:\n    \"\"\"\n    \n    \"\"\"\n    # Add four spaces to the indent.\n    new_indent = indent + \"    \"\n\n    return \"\".join([\n            # opening brace + newline\n            \"{\\n\",\n            # \"type\": \"...\", (if present)\n            f'{new_indent}\"type\": \"{config.typ3}\",\\n' if config.typ3 else '',\n            # render each item\n            \"\".join(_render(item, new_indent) for item in config.items),\n            # indent and close the brace\n            indent,\n            \"}\\n\"\n    ])", "output": "Pretty-print a config in sort-of-JSON+comments.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_fold_lvl(block, val):\n        \"\"\"\n        \n        \"\"\"\n        if block is None:\n            return\n        state = block.userState()\n        if state == -1:\n            state = 0\n        if val >= 0x3FF:\n            val = 0x3FF\n        state &= 0x7C00FFFF\n        state |= val << 16\n        block.setUserState(state)", "output": "Sets the block fold level.\n\n        :param block: block to modify\n        :param val: The new fold level [0-7]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_default_prefetch(input_source_or_dataflow, trainer):\n    \"\"\"\n    \n    \"\"\"\n    if not isinstance(input_source_or_dataflow, InputSource):\n        # to mimic same behavior of the old trainer interface\n        if type(trainer) == SimpleTrainer:\n            input = FeedInput(input_source_or_dataflow)\n        else:\n            logger.info(\"Automatically applying QueueInput on the DataFlow.\")\n            input = QueueInput(input_source_or_dataflow)\n    else:\n        input = input_source_or_dataflow\n    if hasattr(trainer, 'devices'):\n        towers = trainer.devices\n        if len(towers) > 1:\n            # seem to only improve on >1 GPUs\n            assert not isinstance(trainer, SimpleTrainer)\n\n            if isinstance(input, FeedfreeInput) and \\\n               not isinstance(input, (StagingInput, DummyConstantInput)):\n                logger.info(\"Automatically applying StagingInput on the DataFlow.\")\n                input = StagingInput(input)\n    return input", "output": "Apply a set of default rules to make a fast :class:`InputSource`.\n\n    Args:\n        input_source_or_dataflow(InputSource | DataFlow):\n        trainer (Trainer):\n\n    Returns:\n        InputSource", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _shuffle_single(fname, extra_fn=None):\n  \"\"\"\n  \"\"\"\n  records = read_records(fname)\n  random.shuffle(records)\n  if extra_fn is not None:\n    records = extra_fn(records)\n  out_fname = fname.replace(UNSHUFFLED_SUFFIX, \"\")\n  write_records(records, out_fname)\n  tf.gfile.Remove(fname)", "output": "Shuffle a single file of records.\n\n  Args:\n    fname: a string\n    extra_fn: an optional function from list of TFRecords to list of TFRecords\n      to be called after shuffling.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _verify_options(options):\n    '''\n    \n    '''\n\n    # sanity check all vals used for bitwise operations later\n    bitwise_args = [('level', options['level']),\n                    ('facility', options['facility'])\n                    ]\n    bitwise_args.extend([('option', x) for x in options['options']])\n\n    for opt_name, opt in bitwise_args:\n        if not hasattr(syslog, opt):\n            log.error('syslog has no attribute %s', opt)\n            return False\n        if not isinstance(getattr(syslog, opt), int):\n            log.error('%s is not a valid syslog %s', opt, opt_name)\n            return False\n\n    # Sanity check tag\n    if 'tag' in options:\n        if not isinstance(options['tag'], six.string_types):\n            log.error('tag must be a string')\n            return False\n        if len(options['tag']) > 32:\n            log.error('tag size is limited to 32 characters')\n            return False\n\n    return True", "output": "Verify options and log warnings\n\n    Returns True if all options can be verified,\n    otherwise False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_datafeed(self, datafeed_id, body=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if datafeed_id in SKIP_IN_PATH:\n            raise ValueError(\n                \"Empty value passed for a required argument 'datafeed_id'.\"\n            )\n        return self.transport.perform_request(\n            \"POST\",\n            _make_path(\"_ml\", \"datafeeds\", datafeed_id, \"_start\"),\n            params=params,\n            body=body,\n        )", "output": "`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-start-datafeed.html>`_\n\n        :arg datafeed_id: The ID of the datafeed to start\n        :arg body: The start datafeed parameters\n        :arg end: The end time when the datafeed should stop. When not set, the\n            datafeed continues in real time\n        :arg start: The start time from where the datafeed should begin\n        :arg timeout: Controls the time to wait until a datafeed has started.\n            Default to 20 seconds", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_encoder(self, name:str, device:torch.device=None):\n        \"\"\n        encoder = get_model(self.model)[0]\n        if device is None: device = self.data.device\n        if hasattr(encoder, 'module'): encoder = encoder.module\n        encoder.load_state_dict(torch.load(self.path/self.model_dir/f'{name}.pth'))\n        encoder.load_state_dict(torch.load(self.path/self.model_dir/f'{name}.pth', map_location=device))\n        self.freeze()", "output": "Load the encoder `name` from the model directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def accepts(self, tp, converter):\n        ''' \n\n        '''\n\n        tp = ParameterizedProperty._validate_type_param(tp)\n        self.alternatives.append((tp, converter))\n        return self", "output": "Declare that other types may be converted to this property type.\n\n        Args:\n            tp (Property) :\n                A type that may be converted automatically to this property\n                type.\n\n            converter (callable) :\n                A function accepting ``value`` to perform conversion of the\n                value to this property type.\n\n        Returns:\n            self", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def has_path(self, path, method=None):\n        \"\"\"\n        \n        \"\"\"\n        method = self._normalize_method_name(method)\n\n        path_dict = self.get_path(path)\n        path_dict_exists = path_dict is not None\n        if method:\n            return path_dict_exists and method in path_dict\n        return path_dict_exists", "output": "Returns True if this Swagger has the given path and optional method\n\n        :param string path: Path name\n        :param string method: HTTP method\n        :return: True, if this path/method is present in the document", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_cookie_by_name(cookiejar, name, domain=None, path=None):\n    \"\"\"\n    \"\"\"\n    clearables = []\n    for cookie in cookiejar:\n        if cookie.name != name:\n            continue\n        if domain is not None and domain != cookie.domain:\n            continue\n        if path is not None and path != cookie.path:\n            continue\n        clearables.append((cookie.domain, cookie.path, cookie.name))\n\n    for domain, path, name in clearables:\n        cookiejar.clear(domain, path, name)", "output": "Unsets a cookie by name, by default over all domains and paths.\n\n    Wraps CookieJar.clear(), is O(n).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_aws_include_relative_path(template_dict, original_root, new_root):\n    \"\"\"\n    \n    \"\"\"\n\n    for key, val in template_dict.items():\n        if key == \"Fn::Transform\":\n            if isinstance(val, dict) and val.get(\"Name\") == \"AWS::Include\":\n                path = val.get(\"Parameters\", {}).get(\"Location\", {})\n                updated_path = _resolve_relative_to(path, original_root, new_root)\n                if not updated_path:\n                    # This path does not need to get updated\n                    continue\n\n                val[\"Parameters\"][\"Location\"] = updated_path\n\n        # Recurse through all dictionary values\n        elif isinstance(val, dict):\n            _update_aws_include_relative_path(val, original_root, new_root)\n        elif isinstance(val, list):\n            for item in val:\n                if isinstance(item, dict):\n                    _update_aws_include_relative_path(item, original_root, new_root)\n\n    return template_dict", "output": "Update relative paths in \"AWS::Include\" directive. This directive can be present at any part of the template,\n    and not just within resources.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _retrieve_assets(self, sids, asset_tbl, asset_type):\n        \"\"\"\n        \n        \"\"\"\n        # Fastpath for empty request.\n        if not sids:\n            return {}\n\n        cache = self._asset_cache\n        hits = {}\n\n        querying_equities = issubclass(asset_type, Equity)\n        filter_kwargs = (\n            _filter_equity_kwargs\n            if querying_equities else\n            _filter_future_kwargs\n        )\n\n        rows = self._retrieve_asset_dicts(sids, asset_tbl, querying_equities)\n        for row in rows:\n            sid = row['sid']\n            asset = asset_type(**filter_kwargs(row))\n            hits[sid] = cache[sid] = asset\n\n        # If we get here, it means something in our code thought that a\n        # particular sid was an equity/future and called this function with a\n        # concrete type, but we couldn't actually resolve the asset.  This is\n        # an error in our code, not a user-input error.\n        misses = tuple(set(sids) - viewkeys(hits))\n        if misses:\n            if querying_equities:\n                raise EquitiesNotFound(sids=misses)\n            else:\n                raise FutureContractsNotFound(sids=misses)\n        return hits", "output": "Internal function for loading assets from a table.\n\n        This should be the only method of `AssetFinder` that writes Assets into\n        self._asset_cache.\n\n        Parameters\n        ---------\n        sids : iterable of int\n            Asset ids to look up.\n        asset_tbl : sqlalchemy.Table\n            Table from which to query assets.\n        asset_type : type\n            Type of asset to be constructed.\n\n        Returns\n        -------\n        assets : dict[int -> Asset]\n            Dict mapping requested sids to the retrieved assets.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_example_dict(encoder, inputs, mask, outputs):\n  \"\"\"\"\"\"\n  # Inputs\n  bases = []\n  input_ids = []\n  last_idx = -1\n  for row in np.argwhere(inputs):\n    idx, base_id = row\n    idx, base_id = int(idx), int(base_id)\n    assert idx > last_idx  # if not, means 2 True values in 1 row\n    # Some rows are all False. Those rows are mapped to UNK_ID.\n    while idx != last_idx + 1:\n      bases.append(encoder.UNK)\n      last_idx += 1\n    bases.append(encoder.BASES[base_id])\n    last_idx = idx\n  assert len(inputs) == len(bases)\n\n  input_ids = encoder.encode(bases)\n  input_ids.append(text_encoder.EOS_ID)\n\n  # Targets: mask and output\n  targets_mask = [float(v) for v in mask]\n  # The output is (n, m); store targets_shape so that it can be reshaped\n  # properly on the other end.\n  targets = [float(v) for v in outputs.flatten()]\n  targets_shape = [int(dim) for dim in outputs.shape]\n  assert mask.shape[0] == outputs.shape[0]\n\n  example_keys = [\"inputs\", \"targets_mask\", \"targets\", \"targets_shape\"]\n  ex_dict = dict(\n      zip(example_keys, [input_ids, targets_mask, targets, targets_shape]))\n  return ex_dict", "output": "Convert single h5 record to an example dict.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def event(self, event):\r\n        \"\"\"\r\n        \"\"\"\r\n        if (event.type() == QEvent.KeyPress) and (event.key() == Qt.Key_Tab):\r\n            self.sig_tab_pressed.emit(True)\r\n            self.numpress += 1\r\n            if self.numpress == 1:\r\n                self.presstimer = QTimer.singleShot(400, self.handle_keypress)\r\n            return True\r\n        return QComboBox.event(self, event)", "output": "Qt Override.\r\n\r\n        Filter tab keys and process double tab keys.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transform_python_types(self, obj):\n        ''' \n\n        '''\n\n        # date/time values that get serialized as milliseconds\n        if is_datetime_type(obj):\n            return convert_datetime_type(obj)\n\n        if is_timedelta_type(obj):\n            return convert_timedelta_type(obj)\n\n        # slice objects\n        elif isinstance(obj, slice):\n            return dict(start=obj.start, stop=obj.stop, step=obj.step)\n\n        # NumPy scalars\n        elif np.issubdtype(type(obj), np.floating):\n            return float(obj)\n        elif np.issubdtype(type(obj), np.integer):\n            return int(obj)\n        elif np.issubdtype(type(obj), np.bool_):\n            return bool(obj)\n\n        # Decimal values\n        elif isinstance(obj, decimal.Decimal):\n            return float(obj)\n\n        # RelativeDelta gets serialized as a dict\n        elif rd and isinstance(obj, rd.relativedelta):\n            return dict(years=obj.years,\n                    months=obj.months,\n                    days=obj.days,\n                    hours=obj.hours,\n                    minutes=obj.minutes,\n                    seconds=obj.seconds,\n                    microseconds=obj.microseconds)\n\n        else:\n            return super(BokehJSONEncoder, self).default(obj)", "output": "Handle special scalars such as (Python, NumPy, or Pandas)\n        datetimes, or Decimal values.\n\n        Args:\n            obj (obj) :\n\n                The object to encode. Anything not specifically handled in\n                this method is passed on to the default system JSON encoder.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_date(datestring, default_timezone=UTC):\n    \"\"\"\n\n    \"\"\"\n    if not isinstance(datestring, _basestring):\n        raise ParseError(\"Expecting a string %r\" % datestring)\n    m = ISO8601_REGEX.match(datestring)\n    if not m:\n        raise ParseError(\"Unable to parse date string %r\" % datestring)\n    groups = m.groupdict()\n\n    tz = parse_timezone(groups, default_timezone=default_timezone)\n\n    groups[\"second_fraction\"] = int(Decimal(\"0.%s\" % (groups[\"second_fraction\"] or 0)) * Decimal(\"1000000.0\"))\n\n    try:\n        return datetime.datetime(\n            year=to_int(groups, \"year\"),\n            month=to_int(groups, \"month\", default=to_int(groups, \"monthdash\", required=False, default=1)),\n            day=to_int(groups, \"day\", default=to_int(groups, \"daydash\", required=False, default=1)),\n            hour=to_int(groups, \"hour\", default_to_zero=True),\n            minute=to_int(groups, \"minute\", default_to_zero=True),\n            second=to_int(groups, \"second\", default_to_zero=True),\n            microsecond=groups[\"second_fraction\"],\n            tzinfo=tz,\n        )\n    except Exception as e:\n        raise ParseError(e)", "output": "Parses ISO 8601 dates into datetime objects\n\n    The timezone is parsed from the date string. However it is quite common to\n    have dates without a timezone (not strictly correct). In this case the\n    default timezone specified in default_timezone is used. This is UTC by\n    default.\n\n    :param datestring: The date to parse as a string\n    :param default_timezone: A datetime tzinfo instance to use when no timezone\n                             is specified in the datestring. If this is set to\n                             None then a naive datetime object is returned.\n    :returns: A datetime.datetime instance\n    :raises: ParseError when there is a problem parsing the date or\n             constructing the datetime instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _to_bytes_or_false(val):  # type: (Union[Text, bytes]) -> Union[bytes, bool]\n    \"\"\"\n    \"\"\"\n    if isinstance(val, bytes):\n        return val\n    else:\n        try:\n            return val.encode('utf-8')\n        except AttributeError:\n            return False", "output": "An internal graph to convert the input to a bytes or to False.\n\n    The criteria for conversion is as follows and should be python 2 and 3\n    compatible:\n    - If val is py2 str or py3 bytes: return bytes\n    - If val is py2 unicode or py3 str: return val.decode('utf-8')\n    - Otherwise, return False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zone_absent(domain, profile):\n    '''\n    \n    '''\n    zones = __salt__['libcloud_dns.list_zones'](profile)\n    matching_zone = [z for z in zones if z['domain'] == domain]\n    if not matching_zone:\n        return state_result(True, 'Zone already absent', domain)\n    else:\n        result = __salt__['libcloud_dns.delete_zone'](matching_zone[0]['id'], profile)\n        return state_result(result, 'Deleted zone', domain)", "output": "Ensures a record is absent.\n\n    :param domain: Zone name, i.e. the domain name\n    :type  domain: ``str``\n\n    :param profile: The profile key\n    :type  profile: ``str``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mtf_resnet_base():\n  \"\"\"\"\"\"\n  hparams = common_hparams.basic_params1()\n  hparams.no_data_parallelism = True\n  hparams.use_fixed_batch_size = True\n  hparams.batch_size = 32\n  hparams.max_length = 3072\n  hparams.hidden_size = 256\n  hparams.label_smoothing = 0.0\n  # 8-way model-parallelism\n  hparams.add_hparam(\"mesh_shape\", \"batch:8\")\n  hparams.add_hparam(\"layout\", \"batch:batch\")\n  hparams.add_hparam(\"filter_size\", 1024)\n\n  hparams.add_hparam(\"num_layers\", 6)\n  # Share weights between input and target embeddings\n  hparams.shared_embedding = True\n\n  hparams.shared_embedding_and_softmax_weights = True\n  hparams.optimizer = \"Adafactor\"\n  hparams.learning_rate_schedule = \"rsqrt_decay\"\n  hparams.learning_rate_warmup_steps = 10000\n  hparams.add_hparam(\"d_kv\", 32)\n\n  # Image related hparams\n  hparams.add_hparam(\"img_len\", 32)\n  hparams.add_hparam(\"num_channels\", 3)\n  hparams.add_hparam(\"row_blocks\", 1)\n  hparams.add_hparam(\"col_blocks\", 1)\n  hparams.add_hparam(\"rows_size\", 32)\n  hparams.add_hparam(\"cols_size\", 32)\n\n  # Model-specific parameters\n  hparams.add_hparam(\"layer_sizes\", [3, 4, 6, 3])\n  hparams.add_hparam(\"filter_sizes\", [64, 64, 128, 256, 512])\n  hparams.add_hparam(\"is_cifar\", False)\n\n  # Variable init\n  hparams.initializer = \"normal_unit_scaling\"\n  hparams.initializer_gain = 2.\n\n  # TODO(nikip): Change optimization scheme?\n  hparams.learning_rate = 0.1\n  return hparams", "output": "Set of hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def where(self, field_path, op_string, value):\n        \"\"\"\n        \"\"\"\n        query = query_mod.Query(self)\n        return query.where(field_path, op_string, value)", "output": "Create a \"where\" query with this collection as parent.\n\n        See\n        :meth:`~.firestore_v1beta1.query.Query.where` for\n        more information on this method.\n\n        Args:\n            field_path (str): A field path (``.``-delimited list of\n                field names) for the field to filter on.\n            op_string (str): A comparison operation in the form of a string.\n                Acceptable values are ``<``, ``<=``, ``==``, ``>=``\n                and ``>``.\n            value (Any): The value to compare the field against in the filter.\n                If ``value`` is :data:`None` or a NaN, then ``==`` is the only\n                allowed operation.\n\n        Returns:\n            ~.firestore_v1beta1.query.Query: A filtered query.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def availability_set_absent(name, resource_group, connection_auth=None):\n    '''\n    \n    '''\n    ret = {\n        'name': name,\n        'result': False,\n        'comment': '',\n        'changes': {}\n    }\n\n    if not isinstance(connection_auth, dict):\n        ret['comment'] = 'Connection information must be specified via connection_auth dictionary!'\n        return ret\n\n    aset = __salt__['azurearm_compute.availability_set_get'](\n        name,\n        resource_group,\n        azurearm_log_level='info',\n        **connection_auth\n    )\n\n    if 'error' in aset:\n        ret['result'] = True\n        ret['comment'] = 'Availability set {0} was not found.'.format(name)\n        return ret\n\n    elif __opts__['test']:\n        ret['comment'] = 'Availability set {0} would be deleted.'.format(name)\n        ret['result'] = None\n        ret['changes'] = {\n            'old': aset,\n            'new': {},\n        }\n        return ret\n\n    deleted = __salt__['azurearm_compute.availability_set_delete'](name, resource_group, **connection_auth)\n\n    if deleted:\n        ret['result'] = True\n        ret['comment'] = 'Availability set {0} has been deleted.'.format(name)\n        ret['changes'] = {\n            'old': aset,\n            'new': {}\n        }\n        return ret\n\n    ret['comment'] = 'Failed to delete availability set {0}!'.format(name)\n    return ret", "output": ".. versionadded:: 2019.2.0\n\n    Ensure an availability set does not exist in a resource group.\n\n    :param name:\n        Name of the availability set.\n\n    :param resource_group:\n        Name of the resource group containing the availability set.\n\n    :param connection_auth:\n        A dict with subscription and authentication parameters to be used in connecting to the\n        Azure Resource Manager API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _unbox_scalar(\n            self,\n            value: Union[Period, Timestamp, Timedelta, NaTType],\n    ) -> int:\n        \"\"\"\n        \n        \"\"\"\n        raise AbstractMethodError(self)", "output": "Unbox the integer value of a scalar `value`.\n\n        Parameters\n        ----------\n        value : Union[Period, Timestamp, Timedelta]\n\n        Returns\n        -------\n        int\n\n        Examples\n        --------\n        >>> self._unbox_scalar(Timedelta('10s'))  # DOCTEST: +SKIP\n        10000000000", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _SkipField(tokenizer):\n  \"\"\"\n  \"\"\"\n  if tokenizer.TryConsume('['):\n    # Consume extension name.\n    tokenizer.ConsumeIdentifier()\n    while tokenizer.TryConsume('.'):\n      tokenizer.ConsumeIdentifier()\n    tokenizer.Consume(']')\n  else:\n    tokenizer.ConsumeIdentifier()\n\n  _SkipFieldContents(tokenizer)\n\n  # For historical reasons, fields may optionally be separated by commas or\n  # semicolons.\n  if not tokenizer.TryConsume(','):\n    tokenizer.TryConsume(';')", "output": "Skips over a complete field (name and value/message).\n\n  Args:\n    tokenizer: A tokenizer to parse the field name and values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_file(self, fileobject, skip_unknown=False):\n        \"\"\"\"\"\"\n        self.set_metadata_version()\n\n        for field in _version2fieldlist(self['Metadata-Version']):\n            values = self.get(field)\n            if skip_unknown and values in ('UNKNOWN', [], ['UNKNOWN']):\n                continue\n            if field in _ELEMENTSFIELD:\n                self._write_field(fileobject, field, ','.join(values))\n                continue\n            if field not in _LISTFIELDS:\n                if field == 'Description':\n                    if self.metadata_version in ('1.0', '1.1'):\n                        values = values.replace('\\n', '\\n        ')\n                    else:\n                        values = values.replace('\\n', '\\n       |')\n                values = [values]\n\n            if field in _LISTTUPLEFIELDS:\n                values = [','.join(value) for value in values]\n\n            for value in values:\n                self._write_field(fileobject, field, value)", "output": "Write the PKG-INFO format data to a file object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cookies(self) -> Dict[str, http.cookies.Morsel]:\n        \"\"\"\"\"\"\n        return self.request.cookies", "output": "An alias for\n        `self.request.cookies <.httputil.HTTPServerRequest.cookies>`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def arrays_overlap(a1, a2):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.arrays_overlap(_to_java_column(a1), _to_java_column(a2)))", "output": "Collection function: returns true if the arrays contain any common non-null element; if not,\n    returns null if both the arrays are non-empty and any of them contains a null element; returns\n    false otherwise.\n\n    >>> df = spark.createDataFrame([([\"a\", \"b\"], [\"b\", \"c\"]), ([\"a\"], [\"b\", \"c\"])], ['x', 'y'])\n    >>> df.select(arrays_overlap(df.x, df.y).alias(\"overlap\")).collect()\n    [Row(overlap=True), Row(overlap=False)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next(self, fetch: bool = False, next_symbol: _NextSymbol = DEFAULT_NEXT_SYMBOL) -> _Next:\n        \"\"\"\n\n        \"\"\"\n\n        def get_next():\n            candidates = self.find('a', containing=next_symbol)\n\n            for candidate in candidates:\n                if candidate.attrs.get('href'):\n                    # Support 'next' rel (e.g. reddit).\n                    if 'next' in candidate.attrs.get('rel', []):\n                        return candidate.attrs['href']\n\n                    # Support 'next' in classnames.\n                    for _class in candidate.attrs.get('class', []):\n                        if 'next' in _class:\n                            return candidate.attrs['href']\n\n                    if 'page' in candidate.attrs['href']:\n                        return candidate.attrs['href']\n\n            try:\n                # Resort to the last candidate.\n                return candidates[-1].attrs['href']\n            except IndexError:\n                return None\n\n        __next = get_next()\n        if __next:\n            url = self._make_absolute(__next)\n        else:\n            return None\n\n        if fetch:\n            return self.session.get(url)\n        else:\n            return url", "output": "Attempts to find the next page, if there is one. If ``fetch``\n        is ``True`` (default), returns :class:`HTML <HTML>` object of\n        next page. If ``fetch`` is ``False``, simply returns the next URL.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getTreeWalker(treeType, implementation=None, **kwargs):\n    \"\"\"\n\n    \"\"\"\n\n    treeType = treeType.lower()\n    if treeType not in treeWalkerCache:\n        if treeType == \"dom\":\n            from . import dom\n            treeWalkerCache[treeType] = dom.TreeWalker\n        elif treeType == \"genshi\":\n            from . import genshi\n            treeWalkerCache[treeType] = genshi.TreeWalker\n        elif treeType == \"lxml\":\n            from . import etree_lxml\n            treeWalkerCache[treeType] = etree_lxml.TreeWalker\n        elif treeType == \"etree\":\n            from . import etree\n            if implementation is None:\n                implementation = default_etree\n            # XXX: NEVER cache here, caching is done in the etree submodule\n            return etree.getETreeModule(implementation, **kwargs).TreeWalker\n    return treeWalkerCache.get(treeType)", "output": "Get a TreeWalker class for various types of tree with built-in support\n\n    :arg str treeType: the name of the tree type required (case-insensitive).\n        Supported values are:\n\n        * \"dom\": The xml.dom.minidom DOM implementation\n        * \"etree\": A generic walker for tree implementations exposing an\n          elementtree-like interface (known to work with ElementTree,\n          cElementTree and lxml.etree).\n        * \"lxml\": Optimized walker for lxml.etree\n        * \"genshi\": a Genshi stream\n\n    :arg implementation: A module implementing the tree type e.g.\n        xml.etree.ElementTree or cElementTree (Currently applies to the \"etree\"\n        tree type only).\n\n    :arg kwargs: keyword arguments passed to the etree walker--for other\n        walkers, this has no effect\n\n    :returns: a TreeWalker class", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_list(self):\r\n        \"\"\"\"\"\"\r\n        self.listwidget.clear()\r\n        for name in self.pathlist+self.ro_pathlist:\r\n            item = QListWidgetItem(name)\r\n            item.setIcon(ima.icon('DirClosedIcon'))\r\n            if name in self.ro_pathlist:\r\n                item.setFlags(Qt.NoItemFlags | Qt.ItemIsUserCheckable)\r\n                item.setCheckState(Qt.Checked)\r\n            elif name in self.not_active_pathlist:\r\n                item.setFlags(item.flags() | Qt.ItemIsUserCheckable)\r\n                item.setCheckState(Qt.Unchecked)\r\n            else:\r\n                item.setFlags(item.flags() | Qt.ItemIsUserCheckable)\r\n                item.setCheckState(Qt.Checked)\r\n            self.listwidget.addItem(item)\r\n        self.refresh()", "output": "Update path list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nti(s):\n    \"\"\"\n    \"\"\"\n    # There are two possible encodings for a number field, see\n    # itn() below.\n    if s[0] != chr(0o200):\n        try:\n            n = int(nts(s, \"ascii\", \"strict\") or \"0\", 8)\n        except ValueError:\n            raise InvalidHeaderError(\"invalid header\")\n    else:\n        n = 0\n        for i in range(len(s) - 1):\n            n <<= 8\n            n += ord(s[i + 1])\n    return n", "output": "Convert a number field to a python number.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stn(s, length, encoding, errors):\n    \"\"\"\n    \"\"\"\n    s = s.encode(encoding, errors)\n    return s[:length] + (length - len(s)) * NUL", "output": "Convert a string to a null-terminated bytes object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cloud_init(names, host=None, quiet=False, **kwargs):\n    '''\n    \n    '''\n    if quiet:\n        log.warning(\"'quiet' argument is being deprecated. Please migrate to --quiet\")\n    return __salt__['lxc.init'](names=names, host=host,\n                                saltcloud_mode=True, quiet=quiet, **kwargs)", "output": "Wrapper for using lxc.init in saltcloud compatibility mode\n\n    names\n        Name of the containers, supports a single name or a comma delimited\n        list of names.\n\n    host\n        Minion to start the container on. Required.\n\n    path\n        path to the container parent\n        default: /var/lib/lxc (system default)\n\n        .. versionadded:: 2015.8.0\n\n    saltcloud_mode\n        init the container with the saltcloud opts format instead", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encipher_shift(plaintext, plain_vocab, shift):\n  \"\"\"\n  \"\"\"\n  ciphertext = []\n  cipher = ShiftEncryptionLayer(plain_vocab, shift)\n\n  for _, sentence in enumerate(plaintext):\n    cipher_sentence = []\n    for _, character in enumerate(sentence):\n      encrypted_char = cipher.encrypt_character(character)\n      cipher_sentence.append(encrypted_char)\n    ciphertext.append(cipher_sentence)\n\n  return ciphertext", "output": "Encrypt plain text with a single shift layer.\n\n  Args:\n    plaintext (list of list of Strings): a list of plain text to encrypt.\n    plain_vocab (list of Integer): unique vocabularies being used.\n    shift (Integer): number of shift, shift to the right if shift is positive.\n  Returns:\n    ciphertext (list of Strings): encrypted plain text.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filter(self, run_counts, criteria):\n    \"\"\"\n    \n    \"\"\"\n    wrong_confidence = criteria['wrong_confidence']\n    below_t = wrong_confidence <= self.t\n    filtered_counts = deep_copy(run_counts)\n    for key in filtered_counts:\n      filtered_counts[key] = filtered_counts[key][below_t]\n    return filtered_counts", "output": "Return the counts for only those examples that are below the threshold", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_make_string(out_f, max_step):\n    \"\"\"\"\"\"\n    steps = [2 ** n for n in xrange(int(math.log(max_step, 2)), -1, -1)]\n\n    with Namespace(\n        out_f,\n        ['boost', 'metaparse', 'v{0}'.format(VERSION), 'impl']\n    ) as nsp:\n        generate_take(out_f, steps, nsp.prefix())\n\n        out_f.write(\n            '{0}template <int LenNow, int LenRemaining, char... Cs>\\n'\n            '{0}struct make_string;\\n'\n            '\\n'\n            '{0}template <char... Cs>'\n            ' struct make_string<0, 0, Cs...> : string<> {{}};\\n'\n            .format(nsp.prefix())\n        )\n\n        disable_sun = False\n        for i in reversed(steps):\n            if i > 64 and not disable_sun:\n                out_f.write('#ifndef __SUNPRO_CC\\n')\n                disable_sun = True\n            out_f.write(\n                '{0}template <int LenRemaining,{1}char... Cs>'\n                ' struct make_string<{2},LenRemaining,{3}Cs...> :'\n                ' concat<string<{4}>,'\n                ' typename make_string<take(LenRemaining),'\n                'LenRemaining-take(LenRemaining),Cs...>::type> {{}};\\n'\n                .format(\n                    nsp.prefix(),\n                    ''.join('char {0},'.format(n) for n in unique_names(i)),\n                    i,\n                    ''.join('{0},'.format(n) for n in unique_names(i)),\n                    ','.join(unique_names(i))\n                )\n            )\n        if disable_sun:\n            out_f.write('#endif\\n')", "output": "Generate the make_string template", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def present(name, password, permission):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'result': True,\n           'changes': {},\n           'comment': ''}\n\n    users = __salt__['drac.list_users']()\n\n    if __opts__['test']:\n        if name in users:\n            ret['comment'] = '`{0}` already exists'.format(name)\n        else:\n            ret['comment'] = '`{0}` will be created'.format(name)\n            ret['changes'] = {name: 'will be created'}\n\n        return ret\n\n    if name in users:\n        ret['comment'] = '`{0}` already exists'.format(name)\n    else:\n        if __salt__['drac.create_user'](name, password, permission, users):\n            ret['comment'] = '`{0}` user created'.format(name)\n            ret['changes'] = {name: 'new user created'}\n        else:\n            ret['comment'] = 'Unable to create user'\n            ret['result'] = False\n\n    return ret", "output": "Ensure the user exists on the Dell DRAC\n\n    name:\n        The users username\n\n    password\n        The password used to authenticate\n\n    permission\n        The permissions that should be assigned to a user", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_prefix_and_suffix(specified_name, type, property_set):\n    \"\"\"\"\"\"\n\n    property_set = b2.util.jam_to_value_maybe(property_set)\n\n    suffix = \"\"\n    if type:\n        suffix = b2.build.type.generated_target_suffix(type, property_set)\n\n    # Handle suffixes for which no leading dot is desired.  Those are\n    # specified by enclosing them in <...>.  Needed by python so it\n    # can create \"_d.so\" extensions, for example.\n    if get_grist(suffix):\n        suffix = ungrist(suffix)\n    elif suffix:\n        suffix = \".\" + suffix\n\n    prefix = \"\"\n    if type:\n        prefix = b2.build.type.generated_target_prefix(type, property_set)\n\n    if specified_name.startswith(prefix):\n        prefix = \"\"\n\n    if not prefix:\n        prefix = \"\"\n    if not suffix:\n        suffix = \"\"\n    return prefix + specified_name + suffix", "output": "Appends the suffix appropriate to 'type/property-set' combination\n    to the specified name and returns the result.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def edit_tab(self, index):\r\n        \"\"\"\"\"\"\r\n\r\n        # Sets focus, shows cursor\r\n        self.setFocus(True)\r\n\r\n        # Updates tab index\r\n        self.tab_index = index\r\n\r\n        # Gets tab size and shrinks to avoid overlapping tab borders\r\n        rect = self.main.tabRect(index)\r\n        rect.adjust(1, 1, -2, -1)\r\n\r\n        # Sets size\r\n        self.setFixedSize(rect.size())\r\n\r\n        # Places on top of the tab\r\n        self.move(self.main.mapToGlobal(rect.topLeft()))\r\n\r\n        # Copies tab name and selects all\r\n        text = self.main.tabText(index)\r\n        text = text.replace(u'&', u'')\r\n        if self.split_char:\r\n            text = text.split(self.split_char)[self.split_index]\r\n\r\n        self.setText(text)\r\n        self.selectAll()\r\n\r\n        if not self.isVisible():\r\n            # Makes editor visible\r\n            self.show()", "output": "Activate the edit tab.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def collect(self, name, arr):\n        \"\"\"\"\"\"\n        name = py_str(name)\n        if self.include_layer is not None and not self.include_layer(name):\n            return\n        handle = ctypes.cast(arr, NDArrayHandle)\n        arr = NDArray(handle, writable=False).copyto(cpu())\n        if self.logger is not None:\n            self.logger.info(\"Collecting layer %s output of shape %s\" % (name, arr.shape))\n        if name in self.nd_dict:\n            self.nd_dict[name].append(arr)\n        else:\n            self.nd_dict[name] = [arr]", "output": "Callback function for collecting layer output NDArrays.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_options_button(self):\r\n        \"\"\"\"\"\"\r\n        if not self.options_button:\r\n            self.options_button = create_toolbutton(\r\n                self, text=_('Options'), icon=ima.icon('tooloptions'))\r\n\r\n            actions = self.actions + [MENU_SEPARATOR] + self.plugin_actions\r\n            self.options_menu = QMenu(self)\r\n            add_actions(self.options_menu, actions)\r\n            self.options_button.setMenu(self.options_menu)\r\n\r\n        if self.tools_layout.itemAt(self.tools_layout.count() - 1) is None:\r\n            self.tools_layout.insertWidget(\r\n                self.tools_layout.count() - 1, self.options_button)\r\n        else:\r\n            self.tools_layout.addWidget(self.options_button)", "output": "Add the cog menu button to the toolbar.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_context(name='', argspec='', note='', math=False, collapse=False,\n                     img_path='', css_path=CSS_PATH):\n    \"\"\"\n    \n    \"\"\"\n\n    if img_path and os.name == 'nt':\n        img_path = img_path.replace('\\\\', '/')\n\n    context = \\\n    {\n      # Arg dependent variables\n      'math_on': 'true' if math else '',\n      'name': name,\n      'argspec': argspec,\n      'note': note,\n      'collapse': collapse,\n      'img_path': img_path,\n      # Static variables\n      'css_path': css_path,\n      'js_path': JS_PATH,\n      'jquery_path': JQUERY_PATH,\n      'mathjax_path': MATHJAX_PATH,\n      'right_sphinx_version': '' if sphinx.__version__ < \"1.1\" else 'true',\n      'platform': sys.platform\n    }\n    \n    return context", "output": "Generate the html_context dictionary for our Sphinx conf file.\n    \n    This is a set of variables to be passed to the Jinja template engine and\n    that are used to control how the webpage is rendered in connection with\n    Sphinx\n\n    Parameters\n    ----------\n    name : str\n        Object's name.\n    note : str\n        A note describing what type has the function or method being\n        introspected\n    argspec : str\n        Argspec of the the function or method being introspected\n    math : bool\n        Turn on/off Latex rendering on the OI. If False, Latex will be shown in\n        plain text.\n    collapse : bool\n        Collapse sections\n    img_path : str\n        Path for images relative to the file containing the docstring\n\n    Returns\n    -------\n    A dict of strings to be used by Jinja to generate the webpage", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_plugin(self):\r\n        \"\"\"\"\"\"\r\n        self.main.restore_scrollbar_position.connect(\r\n            self.restore_scrollbar_position)\r\n        self.main.console.edit_goto.connect(self.load)\r\n        self.exec_in_extconsole.connect(self.main.execute_in_external_console)\r\n        self.redirect_stdio.connect(self.main.redirect_internalshell_stdio)\r\n        self.open_dir.connect(self.main.workingdirectory.chdir)\r\n        self.set_help(self.main.help)\r\n        if self.main.outlineexplorer is not None:\r\n            self.set_outlineexplorer(self.main.outlineexplorer)\r\n        editorstack = self.get_current_editorstack()\r\n        if not editorstack.data:\r\n            self.__load_temp_file()\r\n        self.main.add_dockwidget(self)\r\n        self.main.add_to_fileswitcher(self, editorstack.tabs, editorstack.data,\r\n                                      ima.icon('TextFileIcon'))", "output": "Register plugin in Spyder's main window", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve_color_default(color=None):\n    \"\"\"\"\n    \"\"\"\n    if color is not None:\n        return color\n    ctx = get_current_context(silent=True)\n    if ctx is not None:\n        return ctx.color", "output": "Internal helper to get the default value of the color flag.  If a\n    value is passed it's returned unchanged, otherwise it's looked up from\n    the current context.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _format_git_opts(opts):\n    '''\n    \n    '''\n    if opts:\n        version_ = version(versioninfo=False)\n        if _LooseVersion(version_) < _LooseVersion('1.7.2'):\n            raise SaltInvocationError(\n                'git_opts is only supported for git versions >= 1.7.2 '\n                '(detected: {0})'.format(version_)\n            )\n    return _format_opts(opts)", "output": "Do a version check and make sure that the installed version of git can\n    support git -c", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_valid_package(package,\n                        cyg_arch='x86_64',\n                        mirrors=None):\n    '''\n    \n    '''\n    if mirrors is None:\n        mirrors = [{DEFAULT_MIRROR: DEFAULT_MIRROR_KEY}]\n\n    LOG.debug('Checking Valid Mirrors: %s', mirrors)\n\n    for mirror in mirrors:\n        for mirror_url, key in mirror.items():\n            if package in _get_all_packages(mirror_url, cyg_arch):\n                return True\n    return False", "output": "Check if the package is valid on the given mirrors.\n\n    Args:\n        package: The name of the package\n        cyg_arch: The cygwin architecture\n        mirrors: any mirrors to check\n\n    Returns (bool): True if Valid, otherwise False\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cyg.check_valid_package <package name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_range_kwargs(self):\n        \"\"\" \n        \"\"\"\n        range_kwargs = {}\n        if self.start_key is not None:\n            start_key_key = \"start_key_open\"\n            if self.start_inclusive:\n                start_key_key = \"start_key_closed\"\n            range_kwargs[start_key_key] = _to_bytes(self.start_key)\n\n        if self.end_key is not None:\n            end_key_key = \"end_key_open\"\n            if self.end_inclusive:\n                end_key_key = \"end_key_closed\"\n            range_kwargs[end_key_key] = _to_bytes(self.end_key)\n        return range_kwargs", "output": "Convert row range object to dict which can be passed to\n        google.bigtable.v2.RowRange add method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dt_to_epoch_ns(dt_series):\n    \"\"\"\n    \"\"\"\n    index = pd.to_datetime(dt_series.values)\n    if index.tzinfo is None:\n        index = index.tz_localize('UTC')\n    else:\n        index = index.tz_convert('UTC')\n    return index.view(np.int64)", "output": "Convert a timeseries into an Int64Index of nanoseconds since the epoch.\n\n    Parameters\n    ----------\n    dt_series : pd.Series\n        The timeseries to convert.\n\n    Returns\n    -------\n    idx : pd.Int64Index\n        The index converted to nanoseconds since the epoch.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _crop_image(image, left=0, top=0, right=0, bottom=0, **kwargs):\n    ''' \n\n    '''\n    return image.crop((left, top, right, bottom))", "output": "Crop the border from the layout", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absent(name, purge=False, force=False):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    lusr = __salt__['user.info'](name)\n    if lusr:\n        # The user is present, make it not present\n        if __opts__['test']:\n            ret['result'] = None\n            ret['comment'] = 'User {0} set for removal'.format(name)\n            return ret\n        beforegroups = set(salt.utils.user.get_group_list(name))\n        ret['result'] = __salt__['user.delete'](name, purge, force)\n        aftergroups = set([g for g in beforegroups if __salt__['group.info'](g)])\n        if ret['result']:\n            ret['changes'] = {}\n            for g in beforegroups - aftergroups:\n                ret['changes']['{0} group'.format(g)] = 'removed'\n            ret['changes'][name] = 'removed'\n            ret['comment'] = 'Removed user {0}'.format(name)\n        else:\n            ret['result'] = False\n            ret['comment'] = 'Failed to remove user {0}'.format(name)\n        return ret\n\n    ret['comment'] = 'User {0} is not present'.format(name)\n\n    return ret", "output": "Ensure that the named user is absent\n\n    name\n        The name of the user to remove\n\n    purge\n        Set purge to True to delete all of the user's files as well as the user,\n        Default is ``False``.\n\n    force\n        If the user is logged in, the absent state will fail. Set the force\n        option to True to remove the user even if they are logged in. Not\n        supported in FreeBSD and Solaris, Default is ``False``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _load_version(cls, unpickler, version):\n        \"\"\"\n        \n        \"\"\"\n        obj = unpickler.load()\n        return TransformerChain(obj._state[\"steps\"])", "output": "An function to load an object with a specific version of the class.\n\n        Parameters\n        ----------\n        pickler : file\n            A GLUnpickler file handle.\n\n        version : int\n            A version number as maintained by the class writer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stream(self, transaction=None):\n        \"\"\"\n        \"\"\"\n        query = query_mod.Query(self)\n        return query.stream(transaction=transaction)", "output": "Read the documents in this collection.\n\n        This sends a ``RunQuery`` RPC and then returns an iterator which\n        consumes each document returned in the stream of ``RunQueryResponse``\n        messages.\n\n        .. note::\n\n           The underlying stream of responses will time out after\n           the ``max_rpc_timeout_millis`` value set in the GAPIC\n           client configuration for the ``RunQuery`` API.  Snapshots\n           not consumed from the iterator before that point will be lost.\n\n        If a ``transaction`` is used and it already has write operations\n        added, this method cannot be used (i.e. read-after-write is not\n        allowed).\n\n        Args:\n            transaction (Optional[~.firestore_v1beta1.transaction.\\\n                Transaction]): An existing transaction that the query will\n                run in.\n\n        Yields:\n            ~.firestore_v1beta1.document.DocumentSnapshot: The next\n            document that fulfills the query.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_predictor(self, predictor):\n        \"\"\"\n        \"\"\"\n        if predictor is self._predictor:\n            return self\n        if self.data is not None:\n            self._predictor = predictor\n            return self._free_handle()\n        else:\n            raise LightGBMError(\"Cannot set predictor after freed raw data, \"\n                                \"set free_raw_data=False when construct Dataset to avoid this.\")", "output": "Set predictor for continued training.\n\n        It is not recommended for user to call this function.\n        Please use init_model argument in engine.train() or engine.cv() instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def trace_graph(graph):\n    \"\"\"\n    \"\"\"\n    result = {None: []}\n    for vertex in graph:\n        result[vertex] = []\n        for root in graph.iter_children(None):\n            paths = []\n            _trace_visit_vertex(graph, root, vertex, {None}, [None], paths)\n            result[vertex].extend(paths)\n    return result", "output": "Build a collection of \"traces\" for each package.\n\n    A trace is a list of names that eventually leads to the package. For\n    example, if A and B are root dependencies, A depends on C and D, B\n    depends on C, and C depends on D, the return value would be like::\n\n        {\n            None: [],\n            \"A\": [None],\n            \"B\": [None],\n            \"C\": [[None, \"A\"], [None, \"B\"]],\n            \"D\": [[None, \"B\", \"C\"], [None, \"A\"]],\n        }", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _fresh_grad(self):\n        \"\"\"\n        \"\"\"\n        out = ctypes.c_int()\n        check_call(_LIB.MXNDArrayGetGradState(self.handle, ctypes.byref(out)))\n        return out.value", "output": "Whether this array's corresponding gradient array\n        (registered via `autograd.mark_variables`) has been\n        updated by `autograd.backward` since last reset.\n\n        `_fresh_grad` need to be manually set to False\n        after consuming gradient (usually after updating this\n        array).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def BoolEncoder(field_number, is_repeated, is_packed):\n  \"\"\"\"\"\"\n\n  false_byte = b'\\x00'\n  true_byte = b'\\x01'\n  if is_packed:\n    tag_bytes = TagBytes(field_number, wire_format.WIRETYPE_LENGTH_DELIMITED)\n    local_EncodeVarint = _EncodeVarint\n    def EncodePackedField(write, value):\n      write(tag_bytes)\n      local_EncodeVarint(write, len(value))\n      for element in value:\n        if element:\n          write(true_byte)\n        else:\n          write(false_byte)\n    return EncodePackedField\n  elif is_repeated:\n    tag_bytes = TagBytes(field_number, wire_format.WIRETYPE_VARINT)\n    def EncodeRepeatedField(write, value):\n      for element in value:\n        write(tag_bytes)\n        if element:\n          write(true_byte)\n        else:\n          write(false_byte)\n    return EncodeRepeatedField\n  else:\n    tag_bytes = TagBytes(field_number, wire_format.WIRETYPE_VARINT)\n    def EncodeField(write, value):\n      write(tag_bytes)\n      if value:\n        return write(true_byte)\n      return write(false_byte)\n    return EncodeField", "output": "Returns an encoder for a boolean field.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def assign_default_storage_policy_to_datastore(profile_manager, policy,\n                                               datastore):\n    '''\n    \n    '''\n    placement_hub = pbm.placement.PlacementHub(\n        hubId=datastore._moId, hubType='Datastore')\n    log.trace('placement_hub = %s', placement_hub)\n    try:\n        profile_manager.AssignDefaultRequirementProfile(policy.profileId,\n                                                        [placement_hub])\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise VMwareApiError('Not enough permissions. Required privilege: '\n                             '{0}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise VMwareRuntimeError(exc.msg)", "output": "Assigns a storage policy as the default policy to a datastore.\n\n    profile_manager\n        Reference to the profile manager.\n\n    policy\n        Reference to the policy to assigned.\n\n    datastore\n        Reference to the datastore.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fail(self, msg, lineno=None, exc=TemplateSyntaxError):\n        \"\"\"\n        \"\"\"\n        if lineno is None:\n            lineno = self.stream.current.lineno\n        raise exc(msg, lineno, self.name, self.filename)", "output": "Convenience method that raises `exc` with the message, passed\n        line number or last line number as well as the current name and\n        filename.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def assert_datasource_protocol(event):\n    \"\"\"\"\"\"\n\n    assert event.type in DATASOURCE_TYPE\n\n    # Done packets have no dt.\n    if not event.type == DATASOURCE_TYPE.DONE:\n        assert isinstance(event.dt, datetime)\n        assert event.dt.tzinfo == pytz.utc", "output": "Assert that an event meets the protocol for datasource outputs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_classification_results(storage_client, file_path):\n  \"\"\"\n  \"\"\"\n  if storage_client:\n    # file on Cloud\n    success = False\n    retry_count = 0\n    while retry_count < 4:\n      try:\n        blob = storage_client.get_blob(file_path)\n        if not blob:\n          return {}\n        if blob.size > MAX_ALLOWED_CLASSIFICATION_RESULT_SIZE:\n          logging.warning('Skipping classification result because it''s too '\n                          'big: %d bytes for %s', blob.size, file_path)\n          return None\n        buf = BytesIO()\n        blob.download_to_file(buf)\n        buf.seek(0)\n        success = True\n        break\n      except Exception:\n        retry_count += 1\n        time.sleep(5)\n    if not success:\n      return None\n  else:\n    # local file\n    try:\n      with open(file_path, 'rb') as f:\n        buf = BytesIO(f.read())\n    except IOError:\n      return None\n  result = {}\n  if PY3:\n    buf = StringIO(buf.read().decode('UTF-8'))\n  for row in csv.reader(buf):\n    try:\n      image_filename = row[0]\n      if image_filename.endswith('.png') or image_filename.endswith('.jpg'):\n        image_filename = image_filename[:image_filename.rfind('.')]\n      label = int(row[1])\n    except (IndexError, ValueError):\n      continue\n    result[image_filename] = label\n  return result", "output": "Reads classification results from the file in Cloud Storage.\n\n  This method reads file with classification results produced by running\n  defense on singe batch of adversarial images.\n\n  Args:\n    storage_client: instance of CompetitionStorageClient or None for local file\n    file_path: path of the file with results\n\n  Returns:\n    dictionary where keys are image names or IDs and values are classification\n      labels", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_namespaced_endpoints(self, namespace, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_endpoints_with_http_info(namespace, body, **kwargs)\n        else:\n            (data) = self.create_namespaced_endpoints_with_http_info(namespace, body, **kwargs)\n            return data", "output": "create Endpoints\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_endpoints(namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1Endpoints body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1Endpoints\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def train_return_grad(self, input, grad):\n        \"\"\"\n        \n        \"\"\"\n\n        assert self._mode == MpsGraphMode.TrainReturnGrad\n        assert input.shape == self._ishape\n        assert grad.shape == self._oshape\n\n        input_array = MpsFloatArray(input)\n        grad_array = MpsFloatArray(grad)\n        result_handle = _ctypes.c_void_p()\n        status_code = self._LIB.TCMPSTrainGraph(\n            self.handle, input_array.handle, grad_array.handle,\n                _ctypes.byref(result_handle))\n\n        assert status_code == 0, \"Error calling TCMPSTrainReturnGradGraph\"\n        assert result_handle, \"TCMPSTrainReturnGradGraph unexpectedly returned NULL pointer\"\n\n        result = MpsFloatArray(result_handle)\n        assert result.shape() == self._ishape\n\n        return result", "output": "Performs a forward pass from the input batch, followed by a backward\n        pass using the provided gradient (in place of a loss function). Returns\n        a MpsFloatArray representing the output (final gradient) of the backward\n        pass. Calling asnumpy() on this value will wait for the batch to finish\n        and yield the output as a numpy array.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def all_editable_exts():\r\n    \"\"\"\"\"\"\r\n    exts = []\r\n    for (language, extensions) in languages.ALL_LANGUAGES.items():\r\n        exts.extend(list(extensions))\r\n    return ['.' + ext for ext in exts]", "output": "Return a list of all editable extensions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_attend_to_encoder_cache(cache, attention_name, hparams, num_layers,\n                                 key_channels, value_channels,\n                                 vars_3d_num_heads, scope_prefix,\n                                 encoder_output):\n  \"\"\"\"\"\"\n  for layer in range(num_layers):\n    layer_name = \"layer_%d\" % layer\n    with tf.variable_scope(\"%sdecoder/%s/%s/multihead_attention\" %\n                           (scope_prefix, layer_name, attention_name)):\n      k_encdec = common_attention.compute_attention_component(\n          encoder_output,\n          key_channels,\n          name=\"k\",\n          vars_3d_num_heads=vars_3d_num_heads)\n      k_encdec = common_attention.split_heads(k_encdec, hparams.num_heads)\n      v_encdec = common_attention.compute_attention_component(\n          encoder_output,\n          value_channels,\n          name=\"v\",\n          vars_3d_num_heads=vars_3d_num_heads)\n      v_encdec = common_attention.split_heads(v_encdec, hparams.num_heads)\n    cache[layer_name][attention_name] = {\n        \"k_encdec\": k_encdec,\n        \"v_encdec\": v_encdec\n    }\n  return cache", "output": "Add attend-to-encoder layers to cache.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def follow(self, index, body, params=None):\n        \"\"\"\n        \n        \"\"\"\n        for param in (index, body):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"PUT\", _make_path(index, \"_ccr\", \"follow\"), params=params, body=body\n        )", "output": "`<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-put-follow.html>`_\n\n        :arg index: The name of the follower index\n        :arg body: The name of the leader index and other optional ccr related\n            parameters\n        :arg wait_for_active_shards: Sets the number of shard copies that must\n            be active before returning. Defaults to 0. Set to `all` for all\n            shard copies, otherwise set to any non-negative value less than or\n            equal to the total number of copies for the shard (number of\n            replicas + 1), default '0'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _draw_fold_region_background(self, block, painter):\n        \"\"\"\n        \n        \"\"\"\n        r = FoldScope(block)\n        th = TextHelper(self.editor)\n        start, end = r.get_range(ignore_blank_lines=True)\n        if start > 0:\n            top = th.line_pos_from_number(start)\n        else:\n            top = 0\n        bottom = th.line_pos_from_number(end + 1)\n        h = bottom - top\n        if h == 0:\n            h = self.sizeHint().height()\n        w = self.sizeHint().width()\n        self._draw_rect(QRectF(0, top, w, h), painter)", "output": "Draw the fold region when the mouse is over and non collapsed\n        indicator.\n\n        :param top: Top position\n        :param block: Current block.\n        :param painter: QPainter", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve_primary_keys_in_schema(sql_tokens: List[str],\n                                   schema: Dict[str, List[TableColumn]]) -> List[str]:\n    \"\"\"\n    \n    \"\"\"\n    primary_keys_for_tables = {name: max(columns, key=lambda x: x.is_primary_key).name\n                               for name, columns in schema.items()}\n    resolved_tokens = []\n    for i, token in enumerate(sql_tokens):\n        if i > 2:\n            table_name = sql_tokens[i - 2]\n            if token == \"ID\" and table_name in primary_keys_for_tables.keys():\n                token = primary_keys_for_tables[table_name]\n        resolved_tokens.append(token)\n    return resolved_tokens", "output": "Some examples in the text2sql datasets use ID as a column reference to the\n    column of a table which has a primary key. This causes problems if you are trying\n    to constrain a grammar to only produce the column names directly, because you don't\n    know what ID refers to. So instead of dealing with that, we just replace it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def high(data, test=None, queue=False, **kwargs):\n    '''\n    \n    '''\n    conflict = _check_queue(queue, kwargs)\n    if conflict is not None:\n        return conflict\n    opts = salt.utils.state.get_sls_opts(__opts__, **kwargs)\n\n    opts['test'] = _get_test_value(test, **kwargs)\n\n    pillar_override = kwargs.get('pillar')\n    pillar_enc = kwargs.get('pillar_enc')\n    if pillar_enc is None \\\n            and pillar_override is not None \\\n            and not isinstance(pillar_override, dict):\n        raise SaltInvocationError(\n            'Pillar data must be formatted as a dictionary, unless pillar_enc '\n            'is specified.'\n        )\n    try:\n        st_ = salt.state.State(opts,\n                               pillar_override,\n                               pillar_enc=pillar_enc,\n                               proxy=__proxy__,\n                               context=__context__,\n                               initial_pillar=_get_initial_pillar(opts))\n    except NameError:\n        st_ = salt.state.State(opts,\n                               pillar_override,\n                               pillar_enc=pillar_enc,\n                               initial_pillar=_get_initial_pillar(opts))\n\n    ret = st_.call_high(data)\n    _set_retcode(ret, highstate=data)\n    return ret", "output": "Execute the compound calls stored in a single set of high data\n\n    This function is mostly intended for testing the state system and is not\n    likely to be needed in everyday usage.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.high '{\"vim\": {\"pkg\": [\"installed\"]}}'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_text(fp_, blocksize=512):\n    '''\n    \n    '''\n    int2byte = (lambda x: bytes((x,))) if six.PY3 else chr\n    text_characters = (\n        b''.join(int2byte(i) for i in range(32, 127)) +\n        b'\\n\\r\\t\\f\\b')\n    try:\n        block = fp_.read(blocksize)\n    except AttributeError:\n        # This wasn't an open filehandle, so treat it as a file path and try to\n        # open the file\n        try:\n            with fopen(fp_, 'rb') as fp2_:\n                block = fp2_.read(blocksize)\n        except IOError:\n            # Unable to open file, bail out and return false\n            return False\n    if b'\\x00' in block:\n        # Files with null bytes are binary\n        return False\n    elif not block:\n        # An empty file is considered a valid text file\n        return True\n    try:\n        block.decode('utf-8')\n        return True\n    except UnicodeDecodeError:\n        pass\n\n    nontext = block.translate(None, text_characters)\n    return float(len(nontext)) / len(block) <= 0.30", "output": "Uses heuristics to guess whether the given file is text or binary,\n    by reading a single block of bytes from the file.\n    If more than 30% of the chars in the block are non-text, or there\n    are NUL ('\\x00') bytes in the block, assume this is a binary file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def layer_norm_vars(filters):\n  \"\"\"\"\"\"\n  scale = tf.get_variable(\n      \"layer_norm_scale\", [filters], initializer=tf.ones_initializer())\n  bias = tf.get_variable(\n      \"layer_norm_bias\", [filters], initializer=tf.zeros_initializer())\n  return scale, bias", "output": "Create Variables for layer norm.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def inspect(self, inspect_logic):\n        \"\"\"\n        \"\"\"\n        op = Operator(\n            _generate_uuid(),\n            OpType.Inspect,\n            \"Inspect\",\n            inspect_logic,\n            num_instances=self.env.config.parallelism)\n        return self.__register(op)", "output": "Inspects the content of the stream.\n\n        Attributes:\n             inspect_logic (function): The user-defined inspect function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mul(a, b):\n  \"\"\"\n  \n  \"\"\"\n  def multiply(a, b):\n    \"\"\"Multiplication\"\"\"\n    return a * b\n  return op_with_scalar_cast(a, b, multiply)", "output": "A wrapper around tf multiplication that does more automatic casting of\n  the input.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def SCM(root_dir, repo=None):  # pylint: disable=invalid-name\n    \"\"\"\n    \"\"\"\n    if Git.is_repo(root_dir) or Git.is_submodule(root_dir):\n        return Git(root_dir, repo=repo)\n\n    return NoSCM(root_dir, repo=repo)", "output": "Returns SCM instance that corresponds to a repo at the specified\n    path.\n\n    Args:\n        root_dir (str): path to a root directory of the repo.\n        repo (dvc.repo.Repo): dvc repo instance that root_dir belongs to.\n\n    Returns:\n        dvc.scm.base.Base: SCM instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def entity(self, entity_type, identifier=None):\n        \"\"\"\n        \"\"\"\n        entity = _ACLEntity(entity_type=entity_type, identifier=identifier)\n        if self.has_entity(entity):\n            entity = self.get_entity(entity)\n        else:\n            self.add_entity(entity)\n        return entity", "output": "Factory method for creating an Entity.\n\n        If an entity with the same type and identifier already exists,\n        this will return a reference to that entity.  If not, it will\n        create a new one and add it to the list of known entities for\n        this ACL.\n\n        :type entity_type: str\n        :param entity_type: The type of entity to create\n                            (ie, ``user``, ``group``, etc)\n\n        :type identifier: str\n        :param identifier: The ID of the entity (if applicable).\n                           This can be either an ID or an e-mail address.\n\n        :rtype: :class:`_ACLEntity`\n        :returns: A new Entity or a reference to an existing identical entity.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def started(self):\n        \"\"\"\n        \"\"\"\n        statistics = self._properties.get(\"statistics\")\n        if statistics is not None:\n            millis = statistics.get(\"startTime\")\n            if millis is not None:\n                return _helpers._datetime_from_microseconds(millis * 1000.0)", "output": "Datetime at which the job was started.\n\n        :rtype: ``datetime.datetime``, or ``NoneType``\n        :returns: the start time (None until set from the server).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def intersecting_ranges(ranges):\n    \"\"\"\n    \"\"\"\n    ranges = sorted(ranges, key=op.attrgetter('start'))\n    return sorted_diff(ranges, group_ranges(ranges))", "output": "Return any ranges that intersect.\n\n    Parameters\n    ----------\n    ranges : iterable[ranges]\n        A sequence of ranges to check for intersections.\n\n    Returns\n    -------\n    intersections : iterable[ranges]\n        A sequence of all of the ranges that intersected in ``ranges``.\n\n    Examples\n    --------\n    >>> ranges = [range(0, 1), range(2, 5), range(4, 7)]\n    >>> list(intersecting_ranges(ranges))\n    [range(2, 5), range(4, 7)]\n\n    >>> ranges = [range(0, 1), range(2, 3)]\n    >>> list(intersecting_ranges(ranges))\n    []\n\n    >>> ranges = [range(0, 1), range(1, 2)]\n    >>> list(intersecting_ranges(ranges))\n    [range(0, 1), range(1, 2)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def profile_list(self, provider, lookup='all'):\n        '''\n        \n        '''\n        data = {}\n        lookups = self.lookup_profiles(provider, lookup)\n\n        if not lookups:\n            return data\n\n        for alias, driver in lookups:\n            if alias not in data:\n                data[alias] = {}\n            if driver not in data[alias]:\n                data[alias][driver] = {}\n        return data", "output": "Return a mapping of all configured profiles", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(provider, instances, opts=None, **kwargs):\n    '''\n    \n    '''\n    client = _get_client()\n    if isinstance(opts, dict):\n        client.opts.update(opts)\n    info = client.create(provider, instances, **salt.utils.args.clean_kwargs(**kwargs))\n    return info", "output": "Create an instance using Salt Cloud\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run cloud.create my-ec2-config myinstance \\\n            image=ami-1624987f size='t1.micro' ssh_username=ec2-user \\\n            securitygroup=default delvol_on_destroy=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_docker_cache(registry, docker_tag) -> None:\n    \"\"\"\n    \n    \"\"\"\n    # We don't have to retag the image since it's already in the right format\n    if not registry:\n        return\n    assert docker_tag\n\n    logging.info('Loading Docker cache for %s from %s', docker_tag, registry)\n    pull_cmd = ['docker', 'pull', docker_tag]\n\n    # Don't throw an error if the image does not exist\n    subprocess.run(pull_cmd, timeout=DOCKER_CACHE_TIMEOUT_MINS*60)\n    logging.info('Successfully pulled docker cache')", "output": "Load the precompiled docker cache from the registry\n    :param registry: Docker registry name\n    :param docker_tag: Docker tag to load\n    :return: None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _conv_shape_tuple(self, lhs_shape, rhs_shape, strides, pads):\n    \"\"\"\"\"\"\n    if isinstance(pads, str):\n      pads = padtype_to_pads(lhs_shape[2:], rhs_shape[2:], strides, pads)\n    if len(pads) != len(lhs_shape) - 2:\n      msg = 'Wrong number of explicit pads for conv: expected {}, got {}.'\n      raise TypeError(msg.format(len(lhs_shape) - 2, len(pads)))\n    lhs_padded = onp.add(lhs_shape[2:], onp.add(*zip(*pads)))\n    out_space = onp.floor_divide(\n        onp.subtract(lhs_padded, rhs_shape[2:]), strides) + 1\n    out_space = onp.maximum(0, out_space)\n    out_shape = (lhs_shape[0], rhs_shape[0]) + tuple(out_space)\n    return tuple(out_shape)", "output": "Compute the shape of a conv given input shapes in canonical order.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def step(self, batch_size, ignore_stale_grad=False):\n        \"\"\"\n        \"\"\"\n        rescale_grad = self._scale / batch_size\n        self._check_and_rescale_grad(rescale_grad)\n\n        if not self._kv_initialized:\n            self._init_kvstore()\n        if self._params_to_init:\n            self._init_params()\n\n        self._allreduce_grads()\n        self._update(ignore_stale_grad)", "output": "Makes one step of parameter update. Should be called after\n        `autograd.backward()` and outside of `record()` scope.\n\n        For normal parameter updates, `step()` should be used, which internally calls\n        `allreduce_grads()` and then `update()`. However, if you need to get the reduced\n        gradients to perform certain transformation, such as in gradient clipping, then\n        you may want to manually call `allreduce_grads()` and `update()` separately.\n\n        Parameters\n        ----------\n        batch_size : int\n            Batch size of data processed. Gradient will be normalized by `1/batch_size`.\n            Set this to 1 if you normalized loss manually with `loss = mean(loss)`.\n        ignore_stale_grad : bool, optional, default=False\n            If true, ignores Parameters with stale gradient (gradient that has not\n            been updated by `backward` after last step) and skip update.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install(pkg,\n            dir,\n            pkgs=None,\n            runas=None,\n            env=None):\n    '''\n    \n\n    '''\n    _check_valid_version()\n\n    cmd = _construct_bower_command('install')\n\n    if pkg:\n        cmd.append(pkg)\n    elif pkgs:\n        cmd.extend(pkgs)\n\n    result = __salt__['cmd.run_all'](cmd,\n                                     cwd=dir,\n                                     runas=runas,\n                                     env=env,\n                                     python_shell=False)\n\n    if result['retcode'] != 0:\n        raise CommandExecutionError(result['stderr'])\n\n    # If package is already installed, Bower will emit empty dict to STDOUT\n    stdout = salt.utils.json.loads(result['stdout'])\n    return stdout != {}", "output": "Install a Bower package.\n\n    If no package is specified, the dependencies (from bower.json) of the\n    package in the given directory will be installed.\n\n    pkg\n        A package name in any format accepted by Bower, including a version\n        identifier\n\n    dir\n        The target directory in which to install the package\n\n    pkgs\n        A list of package names in the same format as the ``pkg`` parameter\n\n    runas\n        The user to run Bower with\n\n    env\n        Environment variables to set when invoking Bower. Uses the same ``env``\n        format as the :py:func:`cmd.run <salt.modules.cmdmod.run>` execution\n        function.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bower.install underscore /path/to/project\n\n        salt '*' bower.install jquery#2.0 /path/to/project", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_namespaced_service_account(self, namespace, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_service_account_with_http_info(namespace, body, **kwargs)\n        else:\n            (data) = self.create_namespaced_service_account_with_http_info(namespace, body, **kwargs)\n            return data", "output": "create a ServiceAccount\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_service_account(namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1ServiceAccount body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1ServiceAccount\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_data_desc(data_names, label_names, data_shapes, label_shapes):\n    \"\"\"\"\"\"\n    data_shapes = [x if isinstance(x, DataDesc) else DataDesc(*x) for x in data_shapes]\n    _check_names_match(data_names, data_shapes, 'data', True)\n    if label_shapes is not None:\n        label_shapes = [x if isinstance(x, DataDesc) else DataDesc(*x) for x in label_shapes]\n        _check_names_match(label_names, label_shapes, 'label', False)\n    else:\n        _check_names_match(label_names, [], 'label', False)\n    return data_shapes, label_shapes", "output": "parse data_attrs into DataDesc format and check that names match", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def disassociate_network_acl(subnet_id=None, vpc_id=None, subnet_name=None, vpc_name=None,\n                             region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    if not _exactly_one((subnet_name, subnet_id)):\n        raise SaltInvocationError('One (but not both) of subnet_id or subnet_name '\n                                  'must be provided.')\n\n    if all((vpc_name, vpc_id)):\n        raise SaltInvocationError('Only one of vpc_id or vpc_name '\n                                  'may be provided.')\n    try:\n        if subnet_name:\n            subnet_id = _get_resource_id('subnet', subnet_name,\n                                         region=region, key=key,\n                                         keyid=keyid, profile=profile)\n            if not subnet_id:\n                return {'disassociated': False,\n                        'error': {'message': 'Subnet {0} does not exist.'.format(subnet_name)}}\n\n        if vpc_name or vpc_id:\n            vpc_id = check_vpc(vpc_id, vpc_name, region, key, keyid, profile)\n\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        association_id = conn.disassociate_network_acl(subnet_id, vpc_id=vpc_id)\n        return {'disassociated': True, 'association_id': association_id}\n    except BotoServerError as e:\n        return {'disassociated': False, 'error': __utils__['boto.get_error'](e)}", "output": "Given a subnet ID, disassociates a network acl.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_vpc.disassociate_network_acl 'subnet-6a1fe403'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_standard (id, source_types, target_types, requirements = []):\n    \"\"\" \n    \"\"\"\n    g = Generator (id, False, source_types, target_types, requirements)\n    register (g)\n    return g", "output": "Creates new instance of the 'generator' class and registers it.\n        Returns the creates instance.\n        Rationale: the instance is returned so that it's possible to first register\n        a generator and then call 'run' method on that generator, bypassing all\n        generator selection.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_vcs_requirement_url(repo_url, rev, project_name, subdir=None):\n    \"\"\"\n    \n    \"\"\"\n    egg_project_name = pkg_resources.to_filename(project_name)\n    req = '{}@{}#egg={}'.format(repo_url, rev, egg_project_name)\n    if subdir:\n        req += '&subdirectory={}'.format(subdir)\n\n    return req", "output": "Return the URL for a VCS requirement.\n\n    Args:\n      repo_url: the remote VCS url, with any needed VCS prefix (e.g. \"git+\").\n      project_name: the (unescaped) project name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_indicator_PBX(DataFrame, N1=3, N2=5, N3=8, N4=13, N5=18, N6=24):\n    ''\n    C = DataFrame['close']\n    PBX1 = (EMA(C, N1) + EMA(C, 2 * N1) + EMA(C, 4 * N1)) / 3\n    PBX2 = (EMA(C, N2) + EMA(C, 2 * N2) + EMA(C, 4 * N2)) / 3\n    PBX3 = (EMA(C, N3) + EMA(C, 2 * N3) + EMA(C, 4 * N3)) / 3\n    PBX4 = (EMA(C, N4) + EMA(C, 2 * N4) + EMA(C, 4 * N4)) / 3\n    PBX5 = (EMA(C, N5) + EMA(C, 2 * N5) + EMA(C, 4 * N5)) / 3\n    PBX6 = (EMA(C, N6) + EMA(C, 2 * N6) + EMA(C, 4 * N6)) / 3\n    DICT = {'PBX1': PBX1, 'PBX2': PBX2, 'PBX3': PBX3,\n            'PBX4': PBX4, 'PBX5': PBX5, 'PBX6': PBX6}\n\n    return pd.DataFrame(DICT)", "output": "\u7011\u5e03\u7ebf", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flush(self, index=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"POST\", _make_path(index, \"_flush\"), params=params\n        )", "output": "Explicitly flush one or more indices.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-flush.html>`_\n\n        :arg index: A comma-separated list of index names; use `_all` or empty\n            string for all indices\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default 'open', valid\n            choices are: 'open', 'closed', 'none', 'all'\n        :arg force: Whether a flush should be forced even if it is not\n            necessarily needed ie. if no changes will be committed to the index.\n            This is useful if transaction log IDs should be incremented even if\n            no uncommitted changes are present. (This setting can be considered\n            as internal)\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)\n        :arg wait_if_ongoing: If set to true the flush operation will block\n            until the flush can be executed if another flush operation is\n            already executing. The default is true. If set to false the flush\n            will be skipped iff if another flush operation is already running.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_depth_embedding(x):\n  \"\"\"\n  \"\"\"\n  x_shape = common_layers.shape_list(x)\n  depth = x_shape[-1]\n  num_steps = x_shape[0]\n  shape = [num_steps, 1, 1, depth]\n  depth_embedding = (\n      tf.get_variable(\n          \"depth_embedding\",\n          shape,\n          initializer=tf.random_normal_initializer(0, depth**-0.5)) * (depth**\n                                                                       0.5))\n\n  x += depth_embedding\n  return x", "output": "Add n-dimensional embedding as the depth embedding (timing signal).\n\n  Adds embeddings to represent the position of the step in the recurrent\n  tower.\n\n  Args:\n    x: a tensor with shape [max_step, batch, length, depth]\n\n  Returns:\n    a Tensor the same shape as x.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_command(\n        cls,\n        cmd,  # type: List[str]\n        show_stdout=True,  # type: bool\n        cwd=None,  # type: Optional[str]\n        on_returncode='raise',  # type: str\n        extra_ok_returncodes=None,  # type: Optional[Iterable[int]]\n        command_desc=None,  # type: Optional[str]\n        extra_environ=None,  # type: Optional[Mapping[str, Any]]\n        spinner=None  # type: Optional[SpinnerInterface]\n    ):\n        # type: (...) -> Optional[Text]\n        \"\"\"\n        \n        \"\"\"\n        cmd = [cls.name] + cmd\n        try:\n            return call_subprocess(cmd, show_stdout, cwd,\n                                   on_returncode=on_returncode,\n                                   extra_ok_returncodes=extra_ok_returncodes,\n                                   command_desc=command_desc,\n                                   extra_environ=extra_environ,\n                                   unset_environ=cls.unset_environ,\n                                   spinner=spinner)\n        except OSError as e:\n            # errno.ENOENT = no such file or directory\n            # In other words, the VCS executable isn't available\n            if e.errno == errno.ENOENT:\n                raise BadCommand(\n                    'Cannot find command %r - do you have '\n                    '%r installed and in your '\n                    'PATH?' % (cls.name, cls.name))\n            else:\n                raise", "output": "Run a VCS subcommand\n        This is simply a wrapper around call_subprocess that adds the VCS\n        command name, and checks that the VCS is available", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def all_selected_options(self):\n        \"\"\"\"\"\"\n        ret = []\n        for opt in self.options:\n            if opt.is_selected():\n                ret.append(opt)\n        return ret", "output": "Returns a list of all selected options belonging to this select tag", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build(self):\n        \"\"\"\n        \n        \"\"\"\n\n        result = {}\n\n        for lambda_function in self._functions_to_build:\n\n            LOG.info(\"Building resource '%s'\", lambda_function.name)\n            result[lambda_function.name] = self._build_function(lambda_function.name,\n                                                                lambda_function.codeuri,\n                                                                lambda_function.runtime)\n\n        return result", "output": "Build the entire application\n\n        Returns\n        -------\n        dict\n            Returns the path to where each resource was built as a map of resource's LogicalId to the path string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ftp_proxy(self, value):\n        \"\"\"\n        \n        \"\"\"\n        self._verify_proxy_type_compatibility(ProxyType.MANUAL)\n        self.proxyType = ProxyType.MANUAL\n        self.ftpProxy = value", "output": "Sets ftp proxy setting.\n\n        :Args:\n         - value: The ftp proxy value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_shared_memory_bytes():\n    \"\"\"\n    \"\"\"\n    # Make sure this is only called on Linux.\n    assert sys.platform == \"linux\" or sys.platform == \"linux2\"\n\n    shm_fd = os.open(\"/dev/shm\", os.O_RDONLY)\n    try:\n        shm_fs_stats = os.fstatvfs(shm_fd)\n        # The value shm_fs_stats.f_bsize is the block size and the\n        # value shm_fs_stats.f_bavail is the number of available\n        # blocks.\n        shm_avail = shm_fs_stats.f_bsize * shm_fs_stats.f_bavail\n    finally:\n        os.close(shm_fd)\n\n    return shm_avail", "output": "Get the size of the shared memory file system.\n\n    Returns:\n        The size of the shared memory file system in bytes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def commit(name,\n           repository,\n           tag='latest',\n           message=None,\n           author=None):\n    '''\n    \n    '''\n    if not isinstance(repository, six.string_types):\n        repository = six.text_type(repository)\n    if not isinstance(tag, six.string_types):\n        tag = six.text_type(tag)\n\n    time_started = time.time()\n    response = _client_wrapper(\n        'commit',\n        name,\n        repository=repository,\n        tag=tag,\n        message=message,\n        author=author)\n    ret = {'Time_Elapsed': time.time() - time_started}\n    _clear_context()\n\n    image_id = None\n    for id_ in ('Id', 'id', 'ID'):\n        if id_ in response:\n            image_id = response[id_]\n            break\n\n    if image_id is None:\n        raise CommandExecutionError('No image ID was returned in API response')\n\n    ret['Id'] = image_id\n    return ret", "output": ".. versionchanged:: 2018.3.0\n        The repository and tag must now be passed separately using the\n        ``repository`` and ``tag`` arguments, rather than together in the (now\n        deprecated) ``image`` argument.\n\n    Commits a container, thereby promoting it to an image. Equivalent to\n    running the ``docker commit`` Docker CLI command.\n\n    name\n        Container name or ID to commit\n\n    repository\n        Repository name for the image being committed\n\n        .. versionadded:: 2018.3.0\n\n    tag : latest\n        Tag name for the image\n\n        .. versionadded:: 2018.3.0\n\n    image\n        .. deprecated:: 2018.3.0\n            Use both ``repository`` and ``tag`` instead\n\n    message\n        Commit message (Optional)\n\n    author\n        Author name (Optional)\n\n\n    **RETURN DATA**\n\n    A dictionary containing the following keys:\n\n    - ``Id`` - ID of the newly-created image\n    - ``Image`` - Name of the newly-created image\n    - ``Time_Elapsed`` - Time in seconds taken to perform the commit\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion docker.commit mycontainer myuser/myimage mytag", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def daily_returns(self, start, end=None):\n        \"\"\"\n        \"\"\"\n        if end is None:\n            return self._daily_returns[start]\n\n        return self._daily_returns[start:end]", "output": "Returns the daily returns for the given period.\n\n        Parameters\n        ----------\n        start : datetime\n            The inclusive starting session label.\n        end : datetime, optional\n            The inclusive ending session label. If not provided, treat\n            ``start`` as a scalar key.\n\n        Returns\n        -------\n        returns : pd.Series or float\n            The returns in the given period. The index will be the trading\n            calendar in the range [start, end]. If just ``start`` is provided,\n            return the scalar value on that day.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sleep(self, response=None):\n        \"\"\" \n        \"\"\"\n\n        if response:\n            slept = self.sleep_for_retry(response)\n            if slept:\n                return\n\n        self._sleep_backoff()", "output": "Sleep between retry attempts.\n\n        This method will respect a server's ``Retry-After`` response header\n        and sleep the duration of the time requested. If that is not present, it\n        will use an exponential backoff. By default, the backoff factor is 0 and\n        this method will return immediately.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_from_csr(self, csr):\n        \"\"\"\n        \n        \"\"\"\n        if len(csr.indices) != len(csr.data):\n            raise ValueError('length mismatch: {} vs {}'.format(len(csr.indices), len(csr.data)))\n        handle = ctypes.c_void_p()\n        _check_call(_LIB.XGDMatrixCreateFromCSREx(c_array(ctypes.c_size_t, csr.indptr),\n                                                  c_array(ctypes.c_uint, csr.indices),\n                                                  c_array(ctypes.c_float, csr.data),\n                                                  ctypes.c_size_t(len(csr.indptr)),\n                                                  ctypes.c_size_t(len(csr.data)),\n                                                  ctypes.c_size_t(csr.shape[1]),\n                                                  ctypes.byref(handle)))\n        self.handle = handle", "output": "Initialize data from a CSR matrix.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_docs(self, vocab):\n        \"\"\"\"\"\"\n        for string in self.strings:\n            vocab[string]\n        orth_col = self.attrs.index(ORTH)\n        for tokens, spaces in zip(self.tokens, self.spaces):\n            words = [vocab.strings[orth] for orth in tokens[:, orth_col]]\n            doc = Doc(vocab, words=words, spaces=spaces)\n            doc = doc.from_array(self.attrs, tokens)\n            yield doc", "output": "Recover Doc objects from the annotations, using the given vocab.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait(self, timeout: Union[float, datetime.timedelta] = None) -> Awaitable[None]:\n        \"\"\"\n        \"\"\"\n        fut = Future()  # type: Future[None]\n        if self._value:\n            fut.set_result(None)\n            return fut\n        self._waiters.add(fut)\n        fut.add_done_callback(lambda fut: self._waiters.remove(fut))\n        if timeout is None:\n            return fut\n        else:\n            timeout_fut = gen.with_timeout(\n                timeout, fut, quiet_exceptions=(CancelledError,)\n            )\n            # This is a slightly clumsy workaround for the fact that\n            # gen.with_timeout doesn't cancel its futures. Cancelling\n            # fut will remove it from the waiters list.\n            timeout_fut.add_done_callback(\n                lambda tf: fut.cancel() if not fut.done() else None\n            )\n            return timeout_fut", "output": "Block until the internal flag is true.\n\n        Returns an awaitable, which raises `tornado.util.TimeoutError` after a\n        timeout.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __read_and_render_yaml_file(source,\n                                template,\n                                saltenv):\n    '''\n    \n    '''\n    sfn = __salt__['cp.cache_file'](source, saltenv)\n    if not sfn:\n        raise CommandExecutionError(\n            'Source file \\'{0}\\' not found'.format(source))\n\n    with salt.utils.files.fopen(sfn, 'r') as src:\n        contents = src.read()\n\n        if template:\n            if template in salt.utils.templates.TEMPLATE_REGISTRY:\n                # TODO: should we allow user to set also `context` like  # pylint: disable=fixme\n                # `file.managed` does?\n                # Apply templating\n                data = salt.utils.templates.TEMPLATE_REGISTRY[template](\n                    contents,\n                    from_str=True,\n                    to_str=True,\n                    saltenv=saltenv,\n                    grains=__grains__,\n                    pillar=__pillar__,\n                    salt=__salt__,\n                    opts=__opts__)\n\n                if not data['result']:\n                    # Failed to render the template\n                    raise CommandExecutionError(\n                        'Failed to render file path with error: '\n                        '{0}'.format(data['data'])\n                    )\n\n                contents = data['data'].encode('utf-8')\n            else:\n                raise CommandExecutionError(\n                    'Unknown template specified: {0}'.format(\n                        template))\n\n        return salt.utils.yaml.safe_load(contents)", "output": "Read a yaml file and, if needed, renders that using the specifieds\n    templating. Returns the python objects defined inside of the file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe_usage_plans(name=None, plan_id=None, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        plans = _multi_call(conn.get_usage_plans, 'items')\n        if name:\n            plans = _filter_plans('name', name, plans)\n        if plan_id:\n            plans = _filter_plans('id', plan_id, plans)\n\n        return {'plans': [_convert_datetime_str(plan) for plan in plans]}\n\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "Returns a list of existing usage plans, optionally filtered to match a given plan name\n\n    .. versionadded:: 2017.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_apigateway.describe_usage_plans\n        salt myminion boto_apigateway.describe_usage_plans name='usage plan name'\n        salt myminion boto_apigateway.describe_usage_plans plan_id='usage plan id'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zpool_command(command, flags=None, opts=None, property_name=None, property_value=None,\n                  filesystem_properties=None, pool_properties=None, target=None):\n    '''\n    \n\n    '''\n    return _command(\n        'zpool',\n        command=command,\n        flags=flags,\n        opts=opts,\n        property_name=property_name,\n        property_value=property_value,\n        filesystem_properties=filesystem_properties,\n        pool_properties=pool_properties,\n        target=target,\n    )", "output": "Build and properly escape a zpool command\n\n    .. note::\n\n        Input is not considered safe and will be passed through\n        to_auto(from_auto('input_here')), you do not need to do so\n        your self first.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(name):\n    '''\n    \n    '''\n    contextkey = 'nspawn.exists.{0}'.format(name)\n    if contextkey in __context__:\n        return __context__[contextkey]\n    __context__[contextkey] = name in list_all()\n    return __context__[contextkey]", "output": "Returns true if the named container exists\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion nspawn.exists <name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def metric_delete(self, project, metric_name):\n        \"\"\"\n        \"\"\"\n        path = \"projects/%s/metrics/%s\" % (project, metric_name)\n        self._gapic_api.delete_log_metric(path)", "output": "API call:  delete a metric resource.\n\n        :type project: str\n        :param project: ID of the project containing the metric.\n\n        :type metric_name: str\n        :param metric_name: the name of the metric", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_guest(name, quiet=False, path=None):\n    '''\n    \n    '''\n    if quiet:\n        log.warning(\"'quiet' argument is being deprecated.\"\n                 ' Please migrate to --quiet')\n    for data in _list_iter(path=path):\n        host, l = next(six.iteritems(data))\n        for x in 'running', 'frozen', 'stopped':\n            if name in l[x]:\n                if not quiet:\n                    __jid_event__.fire_event(\n                        {'data': host,\n                         'outputter': 'lxc_find_host'},\n                        'progress')\n                return host\n    return None", "output": "Returns the host for a container.\n\n    path\n        path to the container parent\n        default: /var/lib/lxc (system default)\n\n        .. versionadded:: 2015.8.0\n\n\n    .. code-block:: bash\n\n        salt-run lxc.find_guest name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_item_children(item):\r\n    \"\"\"\"\"\"\r\n    children = [item.child(index) for index in range(item.childCount())]\r\n    for child in children[:]:\r\n        others = get_item_children(child)\r\n        if others is not None:\r\n            children += others\r\n    return sorted(children, key=lambda child: child.line)", "output": "Return a sorted list of all the children items of 'item'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def route_filters_list(resource_group, **kwargs):\n    '''\n    \n\n    '''\n    result = {}\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        filters = __utils__['azurearm.paged_object_to_list'](\n            netconn.route_filters.list_by_resource_group(\n                resource_group_name=resource_group\n            )\n        )\n\n        for route_filter in filters:\n            result[route_filter['name']] = route_filter\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    List all route filters within a resource group.\n\n    :param resource_group: The resource group name to list route\n        filters within.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.route_filters_list testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe(cwd,\n             rev='HEAD',\n             user=None,\n             password=None,\n             ignore_retcode=False,\n             output_encoding=None):\n    '''\n    \n    '''\n    cwd = _expand_path(cwd, user)\n    command = ['git', 'describe']\n    if _LooseVersion(version(versioninfo=False)) >= _LooseVersion('1.5.6'):\n        command.append('--always')\n    command.append(rev)\n    return _git_run(command,\n                    cwd=cwd,\n                    user=user,\n                    password=password,\n                    ignore_retcode=ignore_retcode,\n                    output_encoding=output_encoding)['stdout']", "output": "Returns the `git-describe(1)`_ string (or the SHA1 hash if there are no\n    tags) for the given revision.\n\n    cwd\n        The path to the git checkout\n\n    rev : HEAD\n        The revision to describe\n\n    user\n        User under which to run the git command. By default, the command is run\n        by the user under which the minion is running.\n\n    password\n        Windows only. Required when specifying ``user``. This parameter will be\n        ignored on non-Windows platforms.\n\n      .. versionadded:: 2016.3.4\n\n    ignore_retcode : False\n        If ``True``, do not log an error to the minion log if the git command\n        returns a nonzero exit status.\n\n        .. versionadded:: 2015.8.0\n\n    output_encoding\n        Use this option to specify which encoding to use to decode the output\n        from any git commands which are run. This should not be needed in most\n        cases.\n\n        .. note::\n            This should only be needed if the files in the repository were\n            created with filenames using an encoding other than UTF-8 to handle\n            Unicode characters.\n\n        .. versionadded:: 2018.3.1\n\n    .. _`git-describe(1)`: http://git-scm.com/docs/git-describe\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt myminion git.describe /path/to/repo\n        salt myminion git.describe /path/to/repo develop", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Setup(self, reader, URL, encoding, options):\n        \"\"\" \"\"\"\n        if reader is None: reader__o = None\n        else: reader__o = reader._o\n        ret = libxml2mod.xmlTextReaderSetup(reader__o, self._o, URL, encoding, options)\n        return ret", "output": "Setup an XML reader with new options", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_model(client, model_id):\n    \"\"\"\"\"\"\n\n    # [START bigquery_update_model_description]\n    from google.cloud import bigquery\n\n    # TODO(developer): Construct a BigQuery client object.\n    # client = bigquery.Client()\n\n    # TODO(developer): Set model_id to the ID of the model to fetch.\n    # model_id = 'your-project.your_dataset.your_model'\n\n    model = client.get_model(model_id)\n    model.description = \"This model was modified from a Python program.\"\n    model = client.update_model(model, [\"description\"])\n\n    full_model_id = \"{}.{}.{}\".format(model.project, model.dataset_id, model.model_id)\n    print(\n        \"Updated model '{}' with description '{}'.\".format(\n            full_model_id, model.description\n        )\n    )", "output": "Sample ID: go/samples-tracker/1533", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def asDict(self, sample=False):\n        \"\"\"\n        \"\"\"\n        return {\n            'count': self.count(),\n            'mean': self.mean(),\n            'sum': self.sum(),\n            'min': self.min(),\n            'max': self.max(),\n            'stdev': self.stdev() if sample else self.sampleStdev(),\n            'variance': self.variance() if sample else self.sampleVariance()\n        }", "output": "Returns the :class:`StatCounter` members as a ``dict``.\n\n        >>> sc.parallelize([1., 2., 3., 4.]).stats().asDict()\n        {'count': 4L,\n         'max': 4.0,\n         'mean': 2.5,\n         'min': 1.0,\n         'stdev': 1.2909944487358056,\n         'sum': 10.0,\n         'variance': 1.6666666666666667}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_satisfy(self, rand_box, gt_boxes):\n        \"\"\"\n        \n        \"\"\"\n        l, t, r, b = rand_box\n        num_gt = gt_boxes.shape[0]\n        ls = np.ones(num_gt) * l\n        ts = np.ones(num_gt) * t\n        rs = np.ones(num_gt) * r\n        bs = np.ones(num_gt) * b\n        mask = np.where(ls < gt_boxes[:, 1])[0]\n        ls[mask] = gt_boxes[mask, 1]\n        mask = np.where(ts < gt_boxes[:, 2])[0]\n        ts[mask] = gt_boxes[mask, 2]\n        mask = np.where(rs > gt_boxes[:, 3])[0]\n        rs[mask] = gt_boxes[mask, 3]\n        mask = np.where(bs > gt_boxes[:, 4])[0]\n        bs[mask] = gt_boxes[mask, 4]\n        w = rs - ls\n        w[w < 0] = 0\n        h = bs - ts\n        h[h < 0] = 0\n        inter_area = h * w\n        union_area = np.ones(num_gt) * max(0, r - l) * max(0, b - t)\n        union_area += (gt_boxes[:, 3] - gt_boxes[:, 1]) * (gt_boxes[:, 4] - gt_boxes[:, 2])\n        union_area -= inter_area\n        ious = inter_area / union_area\n        ious[union_area <= 0] = 0\n        max_iou = np.amax(ious)\n        if max_iou < self.min_overlap:\n            return None\n        # check ground-truth constraint\n        if self.config['gt_constraint'] == 'center':\n            for i in range(ious.shape[0]):\n                if ious[i] > 0:\n                    gt_x = (gt_boxes[i, 1] + gt_boxes[i, 3]) / 2.0\n                    gt_y = (gt_boxes[i, 2] + gt_boxes[i, 4]) / 2.0\n                    if gt_x < l or gt_x > r or gt_y < t or gt_y > b:\n                        return None\n        elif self.config['gt_constraint'] == 'corner':\n            for i in range(ious.shape[0]):\n                if ious[i] > 0:\n                    if gt_boxes[i, 1] < l or gt_boxes[i, 3] > r \\\n                        or gt_boxes[i, 2] < t or gt_boxes[i, 4] > b:\n                        return None\n        return ious", "output": "check if overlap with any gt box is larger than threshold", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def html(self):\n        \"\"\"\n        \n        \"\"\"\n        ret_code = self._sphinx_build('html')\n        zip_fname = os.path.join(BUILD_PATH, 'html', 'pandas.zip')\n        if os.path.exists(zip_fname):\n            os.remove(zip_fname)\n\n        if self.single_doc_html is not None:\n            self._open_browser(self.single_doc_html)\n        else:\n            self._add_redirects()\n        return ret_code", "output": "Build HTML documentation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def atomic_write(path, mode):\n  \"\"\"\"\"\"\n  tmp_path = \"%s%s_%s\" % (path, constants.INCOMPLETE_SUFFIX, uuid.uuid4().hex)\n  with tf.io.gfile.GFile(tmp_path, mode) as file_:\n    yield file_\n  tf.io.gfile.rename(tmp_path, path, overwrite=True)", "output": "Writes to path atomically, by writing to temp file and renaming it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remote(self, *args, **kwargs):\n        \"\"\"\n        \"\"\"\n        return self._remote(args=args, kwargs=kwargs)", "output": "Create an actor.\n\n        Args:\n            args: These arguments are forwarded directly to the actor\n                constructor.\n            kwargs: These arguments are forwarded directly to the actor\n                constructor.\n\n        Returns:\n            A handle to the newly created actor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_true(value=None):\n    '''\n    \n    '''\n    # First, try int/float conversion\n    try:\n        value = int(value)\n    except (ValueError, TypeError):\n        pass\n    try:\n        value = float(value)\n    except (ValueError, TypeError):\n        pass\n\n    # Now check for truthiness\n    if isinstance(value, (six.integer_types, float)):\n        return value > 0\n    elif isinstance(value, six.string_types):\n        return six.text_type(value).lower() == 'true'\n    else:\n        return bool(value)", "output": "Returns a boolean value representing the \"truth\" of the value passed. The\n    rules for what is a \"True\" value are:\n\n        1. Integer/float values greater than 0\n        2. The string values \"True\" and \"true\"\n        3. Any object for which bool(obj) returns True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def feed(self, count, total=1):\n        \"\"\"\n        \n        \"\"\"\n        self._tot += total\n        self._cnt += count", "output": "Args:\n            cnt(int): the count of some event of interest.\n            tot(int): the total number of events.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_deployment(name, namespace='default', **kwargs):\n    '''\n    \n    '''\n    cfg = _setup_conn(**kwargs)\n    try:\n        api_instance = kubernetes.client.ExtensionsV1beta1Api()\n        api_response = api_instance.read_namespaced_deployment(name, namespace)\n\n        return api_response.to_dict()\n    except (ApiException, HTTPError) as exc:\n        if isinstance(exc, ApiException) and exc.status == 404:\n            return None\n        else:\n            log.exception(\n                'Exception when calling '\n                'ExtensionsV1beta1Api->read_namespaced_deployment'\n            )\n            raise CommandExecutionError(exc)\n    finally:\n        _cleanup(**cfg)", "output": "Return the kubernetes deployment defined by name and namespace\n\n    CLI Examples::\n\n        salt '*' kubernetes.show_deployment my-nginx default\n        salt '*' kubernetes.show_deployment name=my-nginx namespace=default", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def readchunk(self) -> Tuple[bytes, bool]:\n        \"\"\"\n        \"\"\"\n        while True:\n            if self._exception is not None:\n                raise self._exception\n\n            while self._http_chunk_splits:\n                pos = self._http_chunk_splits.pop(0)\n                if pos == self._cursor:\n                    return (b\"\", True)\n                if pos > self._cursor:\n                    return (self._read_nowait(pos-self._cursor), True)\n                internal_logger.warning('Skipping HTTP chunk end due to data '\n                                        'consumption beyond chunk boundary')\n\n            if self._buffer:\n                return (self._read_nowait_chunk(-1), False)\n                # return (self._read_nowait(-1), False)\n\n            if self._eof:\n                # Special case for signifying EOF.\n                # (b'', True) is not a final return value actually.\n                return (b'', False)\n\n            await self._wait('readchunk')", "output": "Returns a tuple of (data, end_of_http_chunk). When chunked transfer\n        encoding is used, end_of_http_chunk is a boolean indicating if the end\n        of the data corresponds to the end of a HTTP chunk , otherwise it is\n        always False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_images(call=None, location='local'):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_images function must be called with '\n            '-f or --function, or with the --list-images option'\n        )\n\n    ret = {}\n    for host_name, host_details in six.iteritems(avail_locations()):\n        for item in query('get', 'nodes/{0}/storage/{1}/content'.format(host_name, location)):\n            ret[item['volid']] = item\n    return ret", "output": "Return a list of the images that are on the provider\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud --list-images my-proxmox-config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_regressor_interface_params(spec, features, output_features):\n    \"\"\" \n    \"\"\"\n    if output_features is None:\n        output_features = [(\"predicted_class\", datatypes.Double())]\n    else:\n        output_features = _fm.process_or_validate_features(output_features, 1)\n\n    if len(output_features) != 1:\n        raise ValueError(\"Provided output features for a regressor must be \"\n                    \"one Double feature.\")\n\n    if output_features[0][1] != datatypes.Double():\n        raise ValueError(\"Output type of a regressor must be a Double.\")\n\n    prediction_name = output_features[0][0]\n    spec.description.predictedFeatureName = prediction_name\n\n    # Normalize the features list.\n    features = _fm.process_or_validate_features(features)\n\n    # add input and output features\n    for cur_input_name, feature_type in features:\n        input_ = spec.description.input.add()\n        input_.name = cur_input_name\n        datatypes._set_datatype(input_.type, feature_type)\n\n    output_ = spec.description.output.add()\n    output_.name = prediction_name\n    datatypes._set_datatype(output_.type, 'Double')\n    return spec", "output": "Common utilities to set the regressor interface params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_platform(name, platform_set, server_url):\n    '''\n    \n    '''\n    config = _get_asam_configuration(server_url)\n    if not config:\n        return False\n\n    platforms = list_platforms(server_url)\n    if name in platforms[server_url]:\n        return {name: \"Specified platform already exists on {0}\".format(server_url)}\n\n    platform_sets = list_platform_sets(server_url)\n    if platform_set not in platform_sets[server_url]:\n        return {name: \"Specified platform set does not exist on {0}\".format(server_url)}\n\n    url = config['platform_edit_url']\n\n    data = {\n        'platformName': name,\n        'platformSetName': platform_set,\n        'manual': 'false',\n        'previousURL': '/config/platformAdd.html',\n        'postType': 'PlatformAdd',\n        'Submit': 'Apply'\n    }\n\n    auth = (\n        config['username'],\n        config['password']\n    )\n\n    try:\n        html_content = _make_post_request(url, data, auth, verify=False)\n    except Exception as exc:\n        err_msg = \"Failed to add platform on {0}\".format(server_url)\n        log.error('%s:\\n%s', err_msg, exc)\n        return {name: err_msg}\n\n    platforms = list_platforms(server_url)\n    if name in platforms[server_url]:\n        return {name: \"Successfully added platform on {0}\".format(server_url)}\n    else:\n        return {name: \"Failed to add platform on {0}\".format(server_url)}", "output": "To add an ASAM platform using the specified ASAM platform set on the Novell\n    Fan-Out Driver\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run asam.add_platform my-test-vm test-platform-set prov1.domain.com", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_kind_name(param_type, is_list):\n    \"\"\"\n    \"\"\"\n    if issubclass(param_type, bool):\n      # This check must happen before issubclass(param_type, six.integer_types),\n      # since Python considers bool to be a subclass of int.\n      typename = 'bool'\n    elif issubclass(param_type, six.integer_types):\n      # Setting 'int' and 'long' types to be 'int64' to ensure the type is\n      # compatible with both Python2 and Python3.\n      typename = 'int64'\n    elif issubclass(param_type, (six.string_types, six.binary_type)):\n      # Setting 'string' and 'bytes' types to be 'bytes' to ensure the type is\n      # compatible with both Python2 and Python3.\n      typename = 'bytes'\n    elif issubclass(param_type, float):\n      typename = 'float'\n    else:\n      raise ValueError('Unsupported parameter type: %s' % str(param_type))\n\n    suffix = 'list' if is_list else 'value'\n    return '_'.join([typename, suffix])", "output": "Returns the field name given parameter type and is_list.\n\n    Args:\n      param_type: Data type of the hparam.\n      is_list: Whether this is a list.\n\n    Returns:\n      A string representation of the field name.\n\n    Raises:\n      ValueError: If parameter type is not recognized.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def info(self, pretty=False, best=False):\n        \"\"\"\n        \n        \"\"\"\n        return dict(\n            id=self.id(),\n            version=self.version(pretty, best),\n            version_parts=dict(\n                major=self.major_version(best),\n                minor=self.minor_version(best),\n                build_number=self.build_number(best)\n            ),\n            like=self.like(),\n            codename=self.codename(),\n        )", "output": "Return certain machine-readable information about the OS\n        distribution.\n\n        For details, see :func:`distro.info`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def no_use_pep517_callback(option, opt, value, parser):\n    \"\"\"\n    \n    \"\"\"\n    # Since --no-use-pep517 doesn't accept arguments, the value argument\n    # will be None if --no-use-pep517 is passed via the command-line.\n    # However, the value can be non-None if the option is triggered e.g.\n    # by an environment variable, for example \"PIP_NO_USE_PEP517=true\".\n    if value is not None:\n        msg = \"\"\"A value was passed for --no-use-pep517,\n        probably using either the PIP_NO_USE_PEP517 environment variable\n        or the \"no-use-pep517\" config file option. Use an appropriate value\n        of the PIP_USE_PEP517 environment variable or the \"use-pep517\"\n        config file option instead.\n        \"\"\"\n        raise_option_error(parser, option=option, msg=msg)\n\n    # Otherwise, --no-use-pep517 was passed via the command-line.\n    parser.values.use_pep517 = False", "output": "Process a value provided for the --no-use-pep517 option.\n\n    This is an optparse.Option callback for the no_use_pep517 option.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flushdb(host=None, port=None, db=None, password=None):\n    '''\n    \n    '''\n    server = _connect(host, port, db, password)\n    return server.flushdb()", "output": "Remove all keys from the selected database\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.flushdb", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_tta_predict(args, model, ckp_path, tta_num=4):\n    '''\n    \n    '''\n    model.eval()\n    preds = []\n    meta = None\n\n    # i is tta index, 0: no change, 1: horizon flip, 2: vertical flip, 3: do both\n    for flip_index in range(tta_num):\n        print('flip_index:', flip_index)\n        test_loader = get_test_loader(args.batch_size, index=flip_index, dev_mode=False, pad_mode=args.pad_mode)\n        meta = test_loader.meta\n        outputs = None\n        with torch.no_grad():\n            for i, img in enumerate(test_loader):\n                add_depth_channel(img, args.pad_mode)\n                img = img.cuda()\n                output, _ = model(img)\n                output = torch.sigmoid(output)\n                if outputs is None:\n                    outputs = output.squeeze()\n                else:\n                    outputs = torch.cat([outputs, output.squeeze()], 0)\n\n                print('{} / {}'.format(args.batch_size*(i+1), test_loader.num), end='\\r')\n        outputs = outputs.cpu().numpy()\n        # flip back masks\n        if flip_index == 1:\n            outputs = np.flip(outputs, 2)\n        elif flip_index == 2:\n            outputs = np.flip(outputs, 1)\n        elif flip_index == 3:\n            outputs = np.flip(outputs, 2)\n            outputs = np.flip(outputs, 1)\n        #print(outputs.shape)\n        preds.append(outputs)\n    \n    parent_dir = ckp_path+'_out'\n    if not os.path.exists(parent_dir):\n        os.makedirs(parent_dir)\n    np_file = os.path.join(parent_dir, 'pred.npy')\n\n    model_pred_result = np.mean(preds, 0)\n    np.save(np_file, model_pred_result)\n\n    return model_pred_result, meta", "output": "return 18000x128x128 np array", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def regex_match(txt, rgx, ignorecase=False, multiline=False):\n    '''\n    \n    '''\n    flag = 0\n    if ignorecase:\n        flag |= re.I\n    if multiline:\n        flag |= re.M\n    obj = re.match(rgx, txt, flag)\n    if not obj:\n        return\n    return obj.groups()", "output": "Searches for a pattern in the text.\n\n    .. code-block:: jinja\n\n        {% set my_text = 'abcd' %}\n        {{ my_text | regex_match('^(.*)BC(.*)$', ignorecase=True) }}\n\n    will be rendered as:\n\n    .. code-block:: text\n\n        ('a', 'd')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def data(self, index, role=Qt.DisplayRole):\r\n        \"\"\"\"\"\"\r\n        row = index.row()\r\n        if not index.isValid() or not (0 <= row < len(self.shortcuts)):\r\n            return to_qvariant()\r\n\r\n        shortcut = self.shortcuts[row]\r\n        key = shortcut.key\r\n        column = index.column()\r\n\r\n        if role == Qt.DisplayRole:\r\n            if column == CONTEXT:\r\n                return to_qvariant(shortcut.context)\r\n            elif column == NAME:\r\n                color = self.text_color\r\n                if self._parent == QApplication.focusWidget():\r\n                    if self.current_index().row() == row:\r\n                        color = self.text_color_highlight\r\n                    else:\r\n                        color = self.text_color\r\n                text = self.rich_text[row]\r\n                text = '<p style=\"color:{0}\">{1}</p>'.format(color, text)\r\n                return to_qvariant(text)\r\n            elif column == SEQUENCE:\r\n                text = QKeySequence(key).toString(QKeySequence.NativeText)\r\n                return to_qvariant(text)\r\n            elif column == SEARCH_SCORE:\r\n                # Treating search scores as a table column simplifies the\r\n                # sorting once a score for a specific string in the finder\r\n                # has been defined. This column however should always remain\r\n                # hidden.\r\n                return to_qvariant(self.scores[row])\r\n        elif role == Qt.TextAlignmentRole:\r\n            return to_qvariant(int(Qt.AlignHCenter | Qt.AlignVCenter))\r\n        return to_qvariant()", "output": "Qt Override.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_states(self, fname):\n        \"\"\"\n        \"\"\"\n        assert self._optimizer is not None\n\n        if not self._kv_initialized:\n            self._init_kvstore()\n        if self._params_to_init:\n            self._init_params()\n\n        if self._update_on_kvstore:\n            assert not self._params_to_init, \"Cannot save trainer states when some \" \\\n                                             \"parameters are not yet initialized in kvstore.\"\n            self._kvstore.save_optimizer_states(fname, dump_optimizer=True)\n        else:\n            with open(fname, 'wb') as fout:\n                fout.write(self._updaters[0].get_states(dump_optimizer=True))", "output": "Saves trainer states (e.g. optimizer, momentum) to a file.\n\n\n        Parameters\n        ----------\n        fname : str\n            Path to output states file.\n\n        Note\n        ----\n        `optimizer.param_dict`, which contains Parameter information (such as\n        `lr_mult` and `wd_mult`) will not be saved.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_memory_config(config_spec, memory):\n    '''\n    \n    '''\n    log.trace('Configuring virtual machine memory '\n              'settings memory=%s', memory)\n    if 'size' in memory and 'unit' in memory:\n        try:\n            if memory['unit'].lower() == 'kb':\n                memory_mb = memory['size'] / 1024\n            elif memory['unit'].lower() == 'mb':\n                memory_mb = memory['size']\n            elif memory['unit'].lower() == 'gb':\n                memory_mb = int(float(memory['size']) * 1024)\n        except (TypeError, ValueError):\n            memory_mb = int(memory['size'])\n        config_spec.memoryMB = memory_mb\n    if 'reservation_max' in memory:\n        config_spec.memoryReservationLockedToMax = memory['reservation_max']\n    if 'hotadd' in memory:\n        config_spec.memoryHotAddEnabled = memory['hotadd']", "output": "Sets memory size to the given value\n\n    config_spec\n        vm.ConfigSpec object\n\n    memory\n        Memory size and unit", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_state_changed(self, state):\n        \"\"\"\n        \"\"\"\n        if state:\n            self.editor.sig_breakpoints_changed.connect(self.repaint)\n            self.editor.sig_debug_stop.connect(self.set_current_line_arrow)\n            self.editor.sig_debug_stop[()].connect(self.stop_clean)\n            self.editor.sig_debug_start.connect(self.start_clean)\n        else:\n            self.editor.sig_breakpoints_changed.disconnect(self.repaint)\n            self.editor.sig_debug_stop.disconnect(self.set_current_line_arrow)\n            self.editor.sig_debug_stop[()].disconnect(self.stop_clean)\n            self.editor.sig_debug_start.disconnect(self.start_clean)", "output": "Change visibility and connect/disconnect signal.\n\n        Args:\n            state (bool): Activate/deactivate.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_striptags(value):\n    \"\"\"\n    \"\"\"\n    if hasattr(value, '__html__'):\n        value = value.__html__()\n    return Markup(text_type(value)).striptags()", "output": "Strip SGML/XML tags and replace adjacent whitespace by one space.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cmp_version(item1, item2):\n    '''\n    \n    '''\n    vers1 = _LooseVersion(item1)\n    vers2 = _LooseVersion(item2)\n\n    if vers1 < vers2:\n        return -1\n    if vers1 > vers2:\n        return 1\n    return 0", "output": "Compare function for package version sorting", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filter_directories(self):\r\n        \"\"\"\"\"\"\r\n        index = self.get_index('.spyproject')\r\n        if index is not None:\r\n            self.setRowHidden(index.row(), index.parent(), True)", "output": "Filter the directories to show", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def begin_pending_transactions(self):\n        \"\"\"\"\"\"\n        while not self._pending_sessions.empty():\n            session = self._pending_sessions.get()\n            session._transaction.begin()\n            super(TransactionPingingPool, self).put(session)", "output": "Begin all transactions for sessions added to the pool.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def indices(self):\n        \"\"\"  \"\"\"\n        if len(self.groupings) == 1:\n            return self.groupings[0].indices\n        else:\n            label_list = [ping.labels for ping in self.groupings]\n            keys = [com.values_from_object(ping.group_index)\n                    for ping in self.groupings]\n            return get_indexer_dict(label_list, keys)", "output": "dict {group name -> group indices}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lint(session):\n    \"\"\"\n    \"\"\"\n\n    session.install(\"black\", \"flake8\", *LOCAL_DEPS)\n    session.install(\".\")\n    session.run(\"flake8\", os.path.join(\"google\", \"cloud\", \"bigquery\"))\n    session.run(\"flake8\", \"tests\")\n    session.run(\"flake8\", os.path.join(\"docs\", \"snippets.py\"))\n    session.run(\"black\", \"--check\", *BLACK_PATHS)", "output": "Run linters.\n\n    Returns a failure if the linters find linting errors or sufficiently\n    serious code quality issues.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def calc_tools_spacing(tools_layout):\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    metrics = {  # (tabbar_height, offset)\r\n        'nt.fusion': (32, 0),\r\n        'nt.windowsvista': (21, 3),\r\n        'nt.windowsxp': (24, 0),\r\n        'nt.windows': (21, 3),\r\n        'posix.breeze': (28, -1),\r\n        'posix.oxygen': (38, -2),\r\n        'posix.qtcurve': (27, 0),\r\n        'posix.windows': (26, 0),\r\n        'posix.fusion': (32, 0),\r\n    }\r\n\r\n    style_name = qapplication().style().property('name')\r\n    key = '%s.%s' % (os.name, style_name)\r\n\r\n    if key in metrics:\r\n        tabbar_height, offset = metrics[key]\r\n        tools_height = tools_layout.sizeHint().height()\r\n        spacing = tabbar_height - tools_height + offset\r\n        return max(spacing, 0)", "output": "Return a spacing (int) or None if we don't have the appropriate metrics\r\n    to calculate the spacing.\r\n\r\n    We're trying to adapt the spacing below the tools_layout spacing so that\r\n    the main_widget has the same vertical position as the editor widgets\r\n    (which have tabs above).\r\n\r\n    The required spacing is\r\n\r\n        spacing = tabbar_height - tools_height + offset\r\n\r\n    where the tabbar_heights were empirically determined for a combination of\r\n    operating systems and styles. Offsets were manually adjusted, so that the\r\n    heights of main_widgets and editor widgets match. This is probably\r\n    caused by a still not understood element of the layout and style metrics.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _AddMergeFromStringMethod(message_descriptor, cls):\n  \"\"\"\"\"\"\n  def MergeFromString(self, serialized):\n    length = len(serialized)\n    try:\n      if self._InternalParse(serialized, 0, length) != length:\n        # The only reason _InternalParse would return early is if it\n        # encountered an end-group tag.\n        raise message_mod.DecodeError('Unexpected end-group tag.')\n    except (IndexError, TypeError):\n      # Now ord(buf[p:p+1]) == ord('') gets TypeError.\n      raise message_mod.DecodeError('Truncated message.')\n    except struct.error as e:\n      raise message_mod.DecodeError(e)\n    return length   # Return this for legacy reasons.\n  cls.MergeFromString = MergeFromString\n\n  local_ReadTag = decoder.ReadTag\n  local_SkipField = decoder.SkipField\n  decoders_by_tag = cls._decoders_by_tag\n  is_proto3 = message_descriptor.syntax == \"proto3\"\n\n  def InternalParse(self, buffer, pos, end):\n    self._Modified()\n    field_dict = self._fields\n    unknown_field_list = self._unknown_fields\n    while pos != end:\n      (tag_bytes, new_pos) = local_ReadTag(buffer, pos)\n      field_decoder, field_desc = decoders_by_tag.get(tag_bytes, (None, None))\n      if field_decoder is None:\n        value_start_pos = new_pos\n        new_pos = local_SkipField(buffer, new_pos, end, tag_bytes)\n        if new_pos == -1:\n          return pos\n        if not is_proto3:\n          if not unknown_field_list:\n            unknown_field_list = self._unknown_fields = []\n          unknown_field_list.append(\n              (tag_bytes, buffer[value_start_pos:new_pos]))\n        pos = new_pos\n      else:\n        pos = field_decoder(buffer, new_pos, end, self, field_dict)\n        if field_desc:\n          self._UpdateOneofState(field_desc)\n    return pos\n  cls._InternalParse = InternalParse", "output": "Helper for _AddMessageMethods().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def array_join(col, delimiter, null_replacement=None):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    if null_replacement is None:\n        return Column(sc._jvm.functions.array_join(_to_java_column(col), delimiter))\n    else:\n        return Column(sc._jvm.functions.array_join(\n            _to_java_column(col), delimiter, null_replacement))", "output": "Concatenates the elements of `column` using the `delimiter`. Null values are replaced with\n    `null_replacement` if set, otherwise they are ignored.\n\n    >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\", None],)], ['data'])\n    >>> df.select(array_join(df.data, \",\").alias(\"joined\")).collect()\n    [Row(joined=u'a,b,c'), Row(joined=u'a')]\n    >>> df.select(array_join(df.data, \",\", \"NULL\").alias(\"joined\")).collect()\n    [Row(joined=u'a,b,c'), Row(joined=u'a,NULL')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log_handlers(opts):\n    '''\n    \n    '''\n    ret = LazyLoader(\n        _module_dirs(\n            opts,\n            'log_handlers',\n            int_type='handlers',\n            base_path=os.path.join(SALT_BASE_PATH, 'log'),\n        ),\n        opts,\n        tag='log_handlers',\n    )\n    return FilterDictWrapper(ret, '.setup_handlers')", "output": "Returns the custom logging handler modules\n\n    :param dict opts: The Salt options dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(self):\n        \"\"\"\"\"\"\n        result = nodes.Template(self.subparse(), lineno=1)\n        result.set_environment(self.environment)\n        return result", "output": "Parse the whole template into a `Template` node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def plot_item(self, funcname):\r\n        \"\"\"\"\"\"\r\n        index = self.currentIndex()\r\n        if self.__prepare_plot():\r\n            key = self.model.get_key(index)\r\n            try:\r\n                self.plot(key, funcname)\r\n            except (ValueError, TypeError) as error:\r\n                QMessageBox.critical(self, _( \"Plot\"),\r\n                                     _(\"<b>Unable to plot data.</b>\"\r\n                                       \"<br><br>Error message:<br>%s\"\r\n                                       ) % str(error))", "output": "Plot item", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def MLP(num_hidden_layers=2,\n        hidden_size=512,\n        activation_fn=layers.Relu,\n        num_output_classes=10,\n        mode=\"train\"):\n  \"\"\"\"\"\"\n  del mode\n  cur_layers = [layers.Flatten()]\n  for _ in range(num_hidden_layers):\n    cur_layers += [layers.Dense(hidden_size), activation_fn()]\n  cur_layers += [layers.Dense(num_output_classes), layers.LogSoftmax()]\n  return layers.Serial(*cur_layers)", "output": "Multi-layer feed-forward neural network with non-linear activations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_hash(path, form='sha256', chunk_size=65536):\n    '''\n    \n    '''\n    return salt.utils.hashutils.get_hash(os.path.expanduser(path), form, chunk_size)", "output": "Get the hash sum of a file\n\n    This is better than ``get_sum`` for the following reasons:\n        - It does not read the entire file into memory.\n        - It does not return a string on error. The returned value of\n            ``get_sum`` cannot really be trusted since it is vulnerable to\n            collisions: ``get_sum(..., 'xyz') == 'Hash xyz not supported'``\n\n    path\n        path to the file or directory\n\n    form\n        desired sum format\n\n    chunk_size\n        amount to sum at once\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.get_hash /etc/shadow", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_dict(cls, logical_id, deployment_preference_dict):\n        \"\"\"\n        \n        \"\"\"\n        enabled = deployment_preference_dict.get('Enabled', True)\n        if not enabled:\n            return DeploymentPreference(None, None, None, None, False, None)\n\n        if 'Type' not in deployment_preference_dict:\n            raise InvalidResourceException(logical_id, \"'DeploymentPreference' is missing required Property 'Type'\")\n\n        deployment_type = deployment_preference_dict['Type']\n        hooks = deployment_preference_dict.get('Hooks', dict())\n        if not isinstance(hooks, dict):\n            raise InvalidResourceException(logical_id,\n                                           \"'Hooks' property of 'DeploymentPreference' must be a dictionary\")\n\n        pre_traffic_hook = hooks.get('PreTraffic', None)\n        post_traffic_hook = hooks.get('PostTraffic', None)\n        alarms = deployment_preference_dict.get('Alarms', None)\n        role = deployment_preference_dict.get('Role', None)\n        return DeploymentPreference(deployment_type, pre_traffic_hook, post_traffic_hook, alarms, enabled, role)", "output": ":param logical_id: the logical_id of the resource that owns this deployment preference\n        :param deployment_preference_dict: the dict object taken from the SAM template\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(self):\n        \"\"\"\n        \"\"\"\n        api = self._instance._client.database_admin_api\n        metadata = _metadata_with_prefix(self.name)\n\n        try:\n            api.get_database_ddl(self.name, metadata=metadata)\n        except NotFound:\n            return False\n        return True", "output": "Test whether this database exists.\n\n        See\n        https://cloud.google.com/spanner/reference/rpc/google.spanner.admin.database.v1#google.spanner.admin.database.v1.DatabaseAdmin.GetDatabaseDDL\n\n        :rtype: bool\n        :returns: True if the database exists, else false.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_main_target (self, name):\n        \"\"\" \n        \"\"\"\n        assert isinstance(name, basestring)\n        if not self.built_main_targets_:\n            self.build_main_targets ()\n\n        return self.main_targets_.get (name, None)", "output": "Returns a 'MainTarget' class instance corresponding to the 'name'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_row(self, steps):\r\n        \"\"\"\r\n        \"\"\"\r\n        row = (self.currentRow() + steps) % self.count()\r\n        self.setCurrentRow(row)", "output": "Move selected row a number of steps.\r\n\r\n        Iterates in a cyclic behaviour.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_collections_are_supported(saved_model_handler, supported):\n  \"\"\"\"\"\"\n  for meta_graph in saved_model_handler.meta_graphs:\n    used_collection_keys = set(meta_graph.collection_def.keys())\n    unsupported = used_collection_keys - supported\n    if unsupported:\n      raise ValueError(\"Unsupported collections in graph: %s\\n\"\n                       \"Use hub.create_module_spec(..., drop_collections=[...])\"\n                       \" as appropriate.\" % list(unsupported))", "output": "Checks that SavedModelHandler only uses supported collections.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def contains_extra(marker):\n    \"\"\"\n    \"\"\"\n    if not marker:\n        return False\n    marker = Marker(str(marker))\n    return _markers_contains_extra(marker._markers)", "output": "Check whehter a marker contains an \"extra == ...\" operand.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_ema_callback(self):\n        \"\"\"\n        \n        \"\"\"\n        with self.cached_name_scope():\n            # in TF there is no API to get queue capacity, so we can only summary the size\n            size = tf.cast(self.queue.size(), tf.float32, name='queue_size')\n        size_ema_op = add_moving_summary(size, collection=None, decay=0.5)[0].op\n        ret = RunOp(\n            lambda: size_ema_op,\n            run_before=False,\n            run_as_trigger=False,\n            run_step=True)\n        ret.name_scope = \"InputSource/EMA\"\n        return ret", "output": "Create a hook-only callback which maintain EMA of the queue size.\n        Also tf.summary.scalar the EMA.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _CheckForRestartAndMaybePurge(self, event):\n    \"\"\"\n    \"\"\"\n    if event.HasField(\n        'session_log') and event.session_log.status == event_pb2.SessionLog.START:\n      self._Purge(event, by_tags=False)", "output": "Check and discard expired events using SessionLog.START.\n\n    Check for a SessionLog.START event and purge all previously seen events\n    with larger steps, because they are out of date. Because of supervisor\n    threading, it is possible that this logic will cause the first few event\n    messages to be discarded since supervisor threading does not guarantee\n    that the START message is deterministically written first.\n\n    This method is preferred over _CheckForOutOfOrderStepAndMaybePurge which\n    can inadvertently discard events due to supervisor threading.\n\n    Args:\n      event: The event to use as reference. If the event is a START event, all\n        previously seen events with a greater event.step will be purged.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_theme(self, property_values):\n        ''' \n\n        '''\n        old_dict = self.themed_values()\n\n        # if the same theme is set again, it should reuse the same dict\n        if old_dict is property_values:\n            return\n\n        removed = set()\n        # we're doing a little song-and-dance to avoid storing __themed_values__ or\n        # an empty dict, if there's no theme that applies to this HasProps instance.\n        if old_dict is not None:\n            removed.update(set(old_dict.keys()))\n        added = set(property_values.keys())\n        old_values = dict()\n        for k in added.union(removed):\n            old_values[k] = getattr(self, k)\n\n        if len(property_values) > 0:\n            setattr(self, '__themed_values__', property_values)\n        elif hasattr(self, '__themed_values__'):\n            delattr(self, '__themed_values__')\n\n        # Property container values might be cached even if unmodified. Invalidate\n        # any cached values that are not modified at this point.\n        for k, v in old_values.items():\n            if k in self._unstable_themed_values:\n                del self._unstable_themed_values[k]\n\n        # Emit any change notifications that result\n        for k, v in old_values.items():\n            descriptor = self.lookup(k)\n            descriptor.trigger_if_changed(self, v)", "output": "Apply a set of theme values which will be used rather than\n        defaults, but will not override application-set values.\n\n        The passed-in dictionary may be kept around as-is and shared with\n        other instances to save memory (so neither the caller nor the\n        |HasProps| instance should modify it).\n\n        Args:\n            property_values (dict) : theme values to use in place of defaults\n\n        Returns:\n            None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_list(sld, tld):\n    '''\n    \n    '''\n    opts = salt.utils.namecheap.get_opts('namecheap.domains.dns.getlist')\n    opts['TLD'] = tld\n    opts['SLD'] = sld\n\n    response_xml = salt.utils.namecheap.get_request(opts)\n    if response_xml is None:\n        return {}\n\n    domaindnsgetlistresult = response_xml.getElementsByTagName('DomainDNSGetListResult')[0]\n\n    return salt.utils.namecheap.xml_to_dict(domaindnsgetlistresult)", "output": "Gets a list of DNS servers associated with the requested domain.\n\n    returns a dictionary of information about requested domain\n\n    sld\n        SLD of the domain name\n\n    tld\n        TLD of the domain name\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' namecheap_domains_dns.get_list sld tld", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_elements_by_partial_link_text(self, link_text):\n        \"\"\"\n        \n        \"\"\"\n        return self.find_elements(by=By.PARTIAL_LINK_TEXT, value=link_text)", "output": "Finds elements by a partial match of their link text.\n\n        :Args:\n         - link_text: The text of the element to partial match on.\n\n        :Returns:\n         - list of webelement - a list with elements if any was found.  an\n           empty list if not\n\n        :Usage:\n            ::\n\n                elements = driver.find_elements_by_partial_link_text('Sign')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def release(self, release_id):\n        \"\"\"\n        \n        \"\"\"\n        if release_id not in self.release_ids:\n            return None\n        return [Release(release_id, r) for r in self._releases[release_id]]", "output": "A list of :class:`yarg.release.Release` objects for each file in a\n        release.\n\n        :param release_id: A pypi release id.\n\n            >>> package = yarg.get('yarg')\n            >>> last_release = yarg.releases[-1]\n            >>> package.release(last_release)\n            [<Release 0.1.0>, <Release 0.1.0>]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(self, client=None):\n        \"\"\"\n        \"\"\"\n        return self.bucket.delete_blob(\n            self.name, client=client, generation=self.generation\n        )", "output": "Deletes a blob from Cloud Storage.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: Optional. The client to use.  If not passed, falls back\n                       to the ``client`` stored on the blob's bucket.\n\n        :rtype: :class:`Blob`\n        :returns: The blob that was just deleted.\n        :raises: :class:`google.cloud.exceptions.NotFound`\n                 (propagated from\n                 :meth:`google.cloud.storage.bucket.Bucket.delete_blob`).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _raise_error_if_sframe_empty(dataset, variable_name=\"SFrame\"):\n    \"\"\"\n    \n    \"\"\"\n    err_msg = \"Input %s either has no rows or no columns. A non-empty SFrame \"\n    err_msg += \"is required.\"\n\n    if dataset.num_rows() == 0 or dataset.num_columns() == 0:\n        raise ToolkitError(err_msg % variable_name)", "output": "Check if the input is empty.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sorted_outrows(outrows):\n    # type: (Iterable[InstalledCSVRow]) -> List[InstalledCSVRow]\n    \"\"\"\n    \n    \"\"\"\n    # Normally, there should only be one row per path, in which case the\n    # second and third elements don't come into play when sorting.\n    # However, in cases in the wild where a path might happen to occur twice,\n    # we don't want the sort operation to trigger an error (but still want\n    # determinism).  Since the third element can be an int or string, we\n    # coerce each element to a string to avoid a TypeError in this case.\n    # For additional background, see--\n    # https://github.com/pypa/pip/issues/5868\n    return sorted(outrows, key=lambda row: tuple(str(x) for x in row))", "output": "Return the given rows of a RECORD file in sorted order.\n\n    Each row is a 3-tuple (path, hash, size) and corresponds to a record of\n    a RECORD file (see PEP 376 and PEP 427 for details).  For the rows\n    passed to this function, the size can be an integer as an int or string,\n    or the empty string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear(self):\n        \"\"\"\"\"\"\n        for key in self.conn.keys():\n            self.conn.delete(key)", "output": "Helper for clearing all the keys in a database. Use with\n        caution!", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _lookup_dnspython(name, rdtype, timeout=None, servers=None, secure=None):\n    '''\n    \n    '''\n    resolver = dns.resolver.Resolver()\n\n    if timeout is not None:\n        resolver.lifetime = float(timeout)\n    if servers:\n        resolver.nameservers = servers\n    if secure:\n        resolver.ednsflags += dns.flags.DO\n\n    try:\n        res = [_data_clean(rr.to_text())\n               for rr in resolver.query(name, rdtype, raise_on_no_answer=False)]\n        return res\n    except dns.rdatatype.UnknownRdatatype:\n        raise ValueError('Invalid DNS type {}'.format(rdtype))\n    except (dns.resolver.NXDOMAIN,\n            dns.resolver.YXDOMAIN,\n            dns.resolver.NoNameservers,\n            dns.exception.Timeout):\n        return False", "output": "Use dnspython to lookup addresses\n    :param name: Name of record to search\n    :param rdtype: DNS record type\n    :param timeout: query timeout\n    :param server: [] of server(s) to try in order\n    :return: [] of records or False if error", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_config(self):\n    \"\"\"\"\"\"\n    config = dict([(key, value) for key, value in iteritems(self.options)\n                   if key in self.cfg.settings and value is not None])\n    for key, value in iteritems(config):\n      self.cfg.set(key.lower(), value)", "output": "Loads the configuration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def duplicated(values, keep='first'):\n    \"\"\"\n    \n    \"\"\"\n\n    values, dtype, ndtype = _ensure_data(values)\n    f = getattr(htable, \"duplicated_{dtype}\".format(dtype=ndtype))\n    return f(values, keep=keep)", "output": "Return boolean ndarray denoting duplicate values.\n\n    .. versionadded:: 0.19.0\n\n    Parameters\n    ----------\n    values : ndarray-like\n        Array over which to check for duplicate values.\n    keep : {'first', 'last', False}, default 'first'\n        - ``first`` : Mark duplicates as ``True`` except for the first\n          occurrence.\n        - ``last`` : Mark duplicates as ``True`` except for the last\n          occurrence.\n        - False : Mark all duplicates as ``True``.\n\n    Returns\n    -------\n    duplicated : ndarray", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_agent(style=None) -> _UserAgent:\n    \"\"\"\n    \"\"\"\n    global useragent\n    if (not useragent) and style:\n        useragent = UserAgent()\n\n    return useragent[style] if style else DEFAULT_USER_AGENT", "output": "Returns an apparently legit user-agent, if not requested one of a specific\n    style. Defaults to a Chrome-style User-Agent.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_l2normalization(node, **kwargs):\n    \"\"\"\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    mode = attrs.get(\"mode\", \"instance\")\n\n    if mode != \"channel\":\n        raise AttributeError(\"L2Normalization: ONNX currently supports channel mode only\")\n\n    l2norm_node = onnx.helper.make_node(\n        \"LpNormalization\",\n        input_nodes,\n        [name],\n        axis=1,  # channel only\n        name=name\n    )\n    return [l2norm_node]", "output": "Map MXNet's L2Normalization operator attributes to onnx's LpNormalization operator\n    and return the created node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def not_alived(subset=None, show_ip=False, show_ipv4=None):\n    '''\n    \n    '''\n    show_ip = _show_ip_migration(show_ip, show_ipv4)\n    return list_not_state(subset=subset, show_ip=show_ip)", "output": ".. versionadded:: 2015.8.0\n    .. versionchanged:: 2019.2.0\n        The 'show_ipv4' argument has been renamed to 'show_ip' as it now\n        includes IPv6 addresses for IPv6-connected minions.\n\n    Print a list of all minions that are NOT up according to Salt's presence\n    detection (no commands will be sent)\n\n    subset : None\n        Pass in a CIDR range to filter minions by IP address.\n\n    show_ip : False\n        Also show the IP address each minion is connecting from.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run manage.not_alived", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def groups(self) -> Set[str]:\n        \"\"\"\n        \"\"\"\n        return set(opt.group_name for opt in self._options.values())", "output": "The set of option-groups created by ``define``.\n\n        .. versionadded:: 3.1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zeros(shape, ctx=None, dtype=None, stype=None, **kwargs):\n    \"\"\"\n    \"\"\"\n\n    if stype is None or stype == 'default':\n        return _zeros_ndarray(shape, ctx, dtype, **kwargs)\n    else:\n        return _zeros_sparse_ndarray(stype, shape, ctx, dtype, **kwargs)", "output": "Return a new array of given shape and type, filled with zeros.\n\n    Parameters\n    ----------\n    shape : int or tuple of int\n        The shape of the empty array\n    ctx : Context, optional\n        An optional device context (default is the current default context)\n    dtype : str or numpy.dtype, optional\n        An optional value type (default is `float32`)\n    stype: string, optional\n        The storage type of the empty array, such as 'row_sparse', 'csr', etc.\n\n    Returns\n    -------\n    NDArray, CSRNDArray or RowSparseNDArray\n        A created array\n    Examples\n    --------\n    >>> mx.nd.zeros((1,2), mx.cpu(), stype='csr')\n    <CSRNDArray 1x2 @cpu(0)>\n    >>> mx.nd.zeros((1,2), mx.cpu(), 'float16', stype='row_sparse').asnumpy()\n    array([[ 0.,  0.]], dtype=float16)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def runner_async(self, fun, **kwargs):\n        '''\n        \n        '''\n        kwargs['fun'] = fun\n        runner = salt.runner.RunnerClient(self.opts)\n        return runner.cmd_async(kwargs)", "output": "Run `runner modules <all-salt.runners>` asynchronously\n\n        Wraps :py:meth:`salt.runner.RunnerClient.cmd_async`.\n\n        Note that runner functions must be called using keyword arguments.\n        Positional arguments are not supported.\n\n        :return: event data and a job ID for the executed function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build(self, format='qcow2', path='/tmp'):\n        '''\n        \n        '''\n        if kiwi is None:\n            msg = 'Unable to build the image due to the missing dependencies: Kiwi module is not available.'\n            log.error(msg)\n            raise CommandExecutionError(msg)\n\n        raise CommandExecutionError(\"Build is not yet implemented\")", "output": "Build an image using Kiwi.\n\n        :param format:\n        :param path:\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_mode(self, mode):\n    \"\"\"\"\"\"\n    log_info(\"Setting T2TModel mode to '%s'\", mode)\n    hparams = hparams_lib.copy_hparams(self._original_hparams)\n    hparams.add_hparam(\"mode\", mode)\n    # When not in training mode, set all forms of dropout to zero.\n    if mode != tf.estimator.ModeKeys.TRAIN:\n      for key in hparams.values():\n        if key.endswith(\"dropout\") or key == \"label_smoothing\":\n          log_info(\"Setting hparams.%s to 0.0\", key)\n          setattr(hparams, key, 0.0)\n    self._hparams = hparams", "output": "Set hparams with the given mode.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_gradient_scalar(self, name:str, scalar_value)->None:\n        \"\"\n        tag = self.name + '/gradients/' + name\n        self.tbwriter.add_scalar(tag=tag, scalar_value=scalar_value, global_step=self.iteration)", "output": "Writes a single scalar value for a gradient statistic to Tensorboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_languages(self):\n        \"\"\"\n        \n        \"\"\"\n        languages = ['python']\n        all_options = CONF.options(self.CONF_SECTION)\n        for option in all_options:\n            if option in [l.lower() for l in LSP_LANGUAGES]:\n                languages.append(option)\n        return languages", "output": "Get the list of languages we need to start servers and create\n        clients for.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_variants(unresolved_spec):\n    \"\"\"\n    \"\"\"\n    for resolved_vars, spec in _generate_variants(unresolved_spec):\n        assert not _unresolved_values(spec)\n        yield format_vars(resolved_vars), spec", "output": "Generates variants from a spec (dict) with unresolved values.\n\n    There are two types of unresolved values:\n\n        Grid search: These define a grid search over values. For example, the\n        following grid search values in a spec will produce six distinct\n        variants in combination:\n\n            \"activation\": grid_search([\"relu\", \"tanh\"])\n            \"learning_rate\": grid_search([1e-3, 1e-4, 1e-5])\n\n        Lambda functions: These are evaluated to produce a concrete value, and\n        can express dependencies or conditional distributions between values.\n        They can also be used to express random search (e.g., by calling\n        into the `random` or `np` module).\n\n            \"cpu\": lambda spec: spec.config.num_workers\n            \"batch_size\": lambda spec: random.uniform(1, 1000)\n\n    Finally, to support defining specs in plain JSON / YAML, grid search\n    and lambda functions can also be defined alternatively as follows:\n\n        \"activation\": {\"grid_search\": [\"relu\", \"tanh\"]}\n        \"cpu\": {\"eval\": \"spec.config.num_workers\"}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_temporary_source(self):\n        # type: () -> None\n        \"\"\"\"\"\"\n        if self.source_dir and os.path.exists(\n                os.path.join(self.source_dir, PIP_DELETE_MARKER_FILENAME)):\n            logger.debug('Removing source in %s', self.source_dir)\n            rmtree(self.source_dir)\n        self.source_dir = None\n        self._temp_build_dir.cleanup()\n        self.build_env.cleanup()", "output": "Remove the source files from this requirement, if they are marked\n        for deletion", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def example_describe_configs(a, args):\n    \"\"\"  \"\"\"\n\n    resources = [ConfigResource(restype, resname) for\n                 restype, resname in zip(args[0::2], args[1::2])]\n\n    fs = a.describe_configs(resources)\n\n    # Wait for operation to finish.\n    for res, f in fs.items():\n        try:\n            configs = f.result()\n            for config in iter(configs.values()):\n                print_config(config, 1)\n\n        except KafkaException as e:\n            print(\"Failed to describe {}: {}\".format(res, e))\n        except Exception:\n            raise", "output": "describe configs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_scalar_asset_spot_value(self, asset, field, dt, data_frequency):\n        \"\"\"\n        \n        \"\"\"\n        return self._get_single_asset_value(\n            self.trading_calendar.minute_to_session_label(dt),\n            asset,\n            field,\n            dt,\n            data_frequency,\n        )", "output": "Public API method that returns a scalar value representing the value\n        of the desired asset's field at either the given dt.\n\n        Parameters\n        ----------\n        assets : Asset\n            The asset or assets whose data is desired. This cannot be\n            an arbitrary AssetConvertible.\n        field : {'open', 'high', 'low', 'close', 'volume',\n                 'price', 'last_traded'}\n            The desired field of the asset.\n        dt : pd.Timestamp\n            The timestamp for the desired value.\n        data_frequency : str\n            The frequency of the data to query; i.e. whether the data is\n            'daily' or 'minute' bars\n\n        Returns\n        -------\n        value : float, int, or pd.Timestamp\n            The spot value of ``field`` for ``asset`` The return type is based\n            on the ``field`` requested. If the field is one of 'open', 'high',\n            'low', 'close', or 'price', the value will be a float. If the\n            ``field`` is 'volume' the value will be a int. If the ``field`` is\n            'last_traded' the value will be a Timestamp.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_file(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            return S_ISREG(self.stat().st_mode)\n        except OSError as e:\n            if e.errno not in (ENOENT, ENOTDIR):\n                raise\n            # Path doesn't exist or is a broken symlink\n            # (see https://bitbucket.org/pitrou/pathlib/issue/12/)\n            return False", "output": "Whether this path is a regular file (also True for symlinks pointing\n        to regular files).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def patch(self, client=None):\n        \"\"\"\n        \"\"\"\n        client = self._require_client(client)\n        query_params = self._query_params\n        # Pass '?projection=full' here because 'PATCH' documented not\n        # to work properly w/ 'noAcl'.\n        query_params[\"projection\"] = \"full\"\n        update_properties = {key: self._properties[key] for key in self._changes}\n\n        # Make the API call.\n        api_response = client._connection.api_request(\n            method=\"PATCH\",\n            path=self.path,\n            data=update_properties,\n            query_params=query_params,\n            _target_object=self,\n        )\n        self._set_properties(api_response)", "output": "Sends all changed properties in a PATCH request.\n\n        Updates the ``_properties`` with the response from the backend.\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: the client to use.  If not passed, falls back to the\n                       ``client`` stored on the current object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_kexgss_complete(self, m):\n        \"\"\"\n        \n        \"\"\"\n        # client mode\n        if self.transport.host_key is None:\n            self.transport.host_key = NullHostKey()\n        self.f = m.get_mpint()\n        if (self.f < 1) or (self.f > self.P - 1):\n            raise SSHException('Server kex \"f\" is out of range')\n        mic_token = m.get_string()\n        # This must be TRUE, if there is a GSS-API token in this message.\n        bool = m.get_boolean()\n        srv_token = None\n        if bool:\n            srv_token = m.get_string()\n        K = pow(self.f, self.x, self.P)\n        # okay, build up the hash H of\n        # (V_C || V_S || I_C || I_S || K_S || e || f || K)\n        hm = Message()\n        hm.add(\n            self.transport.local_version,\n            self.transport.remote_version,\n            self.transport.local_kex_init,\n            self.transport.remote_kex_init,\n        )\n        hm.add_string(self.transport.host_key.__str__())\n        hm.add_mpint(self.e)\n        hm.add_mpint(self.f)\n        hm.add_mpint(K)\n        H = sha1(str(hm)).digest()\n        self.transport._set_K_H(K, H)\n        if srv_token is not None:\n            self.kexgss.ssh_init_sec_context(\n                target=self.gss_host, recv_token=srv_token\n            )\n            self.kexgss.ssh_check_mic(mic_token, H)\n        else:\n            self.kexgss.ssh_check_mic(mic_token, H)\n        self.transport.gss_kex_used = True\n        self.transport._activate_outbound()", "output": "Parse the SSH2_MSG_KEXGSS_COMPLETE message (client mode).\n\n        :param `.Message` m: The content of the\n            SSH2_MSG_KEXGSS_COMPLETE message", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def all_files(models=[]):\n    \n    '''\n\n    def nsort(a, b):\n        fa = os.path.basename(a).split('.')\n        fb = os.path.basename(b).split('.')\n        elements_to_remove = []\n\n        assert len(fa) == len(fb)\n\n        for i in range(0, len(fa)):\n            if fa[i] == fb[i]:\n                elements_to_remove.append(fa[i])\n\n        for e in elements_to_remove:\n            fa.remove(e)\n            fb.remove(e)\n\n        assert len(fa) == len(fb)\n        assert len(fa) == 1\n\n        fa = keep_only_digits(fa[0])\n        fb = keep_only_digits(fb[0])\n\n        if fa < fb:\n            return -1\n        if fa == fb:\n            return 0\n        if fa > fb:\n            return 1\n\n    base = list(map(lambda x: os.path.abspath(x), maybe_inspect_zip(models)))\n    base.sort(cmp=nsort)\n\n    return base", "output": "r'''\n    Return a list of full path of files matching 'models', sorted in human\n    numerical order (i.e., 0 1 2 ..., 10 11 12, ..., 100, ..., 1000).\n\n    Files are supposed to be named identically except one variable component\n    e.g. the list,\n      test.weights.e5.lstm1200.ldc93s1.pb\n      test.weights.e5.lstm1000.ldc93s1.pb\n      test.weights.e5.lstm800.ldc93s1.pb\n    gets sorted:\n      test.weights.e5.lstm800.ldc93s1.pb\n      test.weights.e5.lstm1000.ldc93s1.pb\n      test.weights.e5.lstm1200.ldc93s1.pb", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lastElementChild(self):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlLastElementChild(self._o)\n        if ret is None:return None\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Finds the last child node of that element which is a\n          Element node Note the handling of entities references is\n          different than in the W3C DOM element traversal spec since\n          we don't have back reference from entities content to\n           entities references.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def display(name):\n    '''\n    \n    '''\n    cmd = [_get_cmd(), '--display', name]\n    out = __salt__['cmd.run_all'](cmd, python_shell=False)\n    if out['retcode'] > 0 and out['stderr'] != '':\n        return out['stderr']\n    return out['stdout']", "output": "Display alternatives settings for defined command name\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' alternatives.display editor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def returner(load):\n    '''\n    \n    '''\n    serial = salt.payload.Serial(__opts__)\n\n    # if a minion is returning a standalone job, get a jobid\n    if load['jid'] == 'req':\n        load['jid'] = prep_jid(nocache=load.get('nocache', False))\n\n    jid_dir = salt.utils.jid.jid_dir(load['jid'], _job_dir(), __opts__['hash_type'])\n    if os.path.exists(os.path.join(jid_dir, 'nocache')):\n        return\n\n    hn_dir = os.path.join(jid_dir, load['id'])\n\n    try:\n        os.makedirs(hn_dir)\n    except OSError as err:\n        if err.errno == errno.EEXIST:\n            # Minion has already returned this jid and it should be dropped\n            log.error(\n                'An extra return was detected from minion %s, please verify '\n                'the minion, this could be a replay attack', load['id']\n            )\n            return False\n        elif err.errno == errno.ENOENT:\n            log.error(\n                'An inconsistency occurred, a job was received with a job id '\n                '(%s) that is not present in the local cache', load['jid']\n            )\n            return False\n        raise\n\n    serial.dump(\n        dict((key, load[key]) for key in ['return', 'retcode', 'success'] if key in load),\n        # Use atomic open here to avoid the file being read before it's\n        # completely written to. Refs #1935\n        salt.utils.atomicfile.atomic_open(\n            os.path.join(hn_dir, RETURN_P), 'w+b'\n        )\n    )\n\n    if 'out' in load:\n        serial.dump(\n            load['out'],\n            # Use atomic open here to avoid the file being read before\n            # it's completely written to. Refs #1935\n            salt.utils.atomicfile.atomic_open(\n                os.path.join(hn_dir, OUT_P), 'w+b'\n            )\n        )", "output": "Return data to the local job cache", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_channel_subsystem_request(self, channel, name):\n        \"\"\"\n        \n        \"\"\"\n        transport = channel.get_transport()\n        handler_class, larg, kwarg = transport._get_subsystem_handler(name)\n        if handler_class is None:\n            return False\n        handler = handler_class(channel, name, self, *larg, **kwarg)\n        handler.start()\n        return True", "output": "Determine if a requested subsystem will be provided to the client on\n        the given channel.  If this method returns ``True``, all future I/O\n        through this channel will be assumed to be connected to the requested\n        subsystem.  An example of a subsystem is ``sftp``.\n\n        The default implementation checks for a subsystem handler assigned via\n        `.Transport.set_subsystem_handler`.\n        If one has been set, the handler is invoked and this method returns\n        ``True``.  Otherwise it returns ``False``.\n\n        .. note:: Because the default implementation uses the `.Transport` to\n            identify valid subsystems, you probably won't need to override this\n            method.\n\n        :param .Channel channel: the `.Channel` the pty request arrived on.\n        :param str name: name of the requested subsystem.\n        :return:\n            ``True`` if this channel is now hooked up to the requested\n            subsystem; ``False`` if that subsystem can't or won't be provided.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _change_source_state(name, state):\n    '''\n    \n\n    '''\n    choc_path = _find_chocolatey(__context__, __salt__)\n    cmd = [choc_path, 'source', state, '--name', name]\n    result = __salt__['cmd.run_all'](cmd, python_shell=False)\n\n    if result['retcode'] != 0:\n        raise CommandExecutionError(\n            'Running chocolatey failed: {0}'.format(result['stdout'])\n        )\n\n    return result['stdout']", "output": "Instructs Chocolatey to change the state of a source.\n\n    name\n        Name of the repository to affect.\n\n    state\n        State in which you want the chocolatey repository.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download(url, dir, filename=None, expect_size=None):\n    \"\"\"\n    \n    \"\"\"\n    mkdir_p(dir)\n    if filename is None:\n        filename = url.split('/')[-1]\n    fpath = os.path.join(dir, filename)\n\n    if os.path.isfile(fpath):\n        if expect_size is not None and os.stat(fpath).st_size == expect_size:\n            logger.info(\"File {} exists! Skip download.\".format(filename))\n            return fpath\n        else:\n            logger.warn(\"File {} exists. Will overwrite with a new download!\".format(filename))\n\n    def hook(t):\n        last_b = [0]\n\n        def inner(b, bsize, tsize=None):\n            if tsize is not None:\n                t.total = tsize\n            t.update((b - last_b[0]) * bsize)\n            last_b[0] = b\n        return inner\n    try:\n        with tqdm.tqdm(unit='B', unit_scale=True, miniters=1, desc=filename) as t:\n            fpath, _ = urllib.request.urlretrieve(url, fpath, reporthook=hook(t))\n        statinfo = os.stat(fpath)\n        size = statinfo.st_size\n    except IOError:\n        logger.error(\"Failed to download {}\".format(url))\n        raise\n    assert size > 0, \"Downloaded an empty file from {}!\".format(url)\n\n    if expect_size is not None and size != expect_size:\n        logger.error(\"File downloaded from {} does not match the expected size!\".format(url))\n        logger.error(\"You may have downloaded a broken file, or the upstream may have modified the file.\")\n\n    # TODO human-readable size\n    logger.info('Succesfully downloaded ' + filename + \". \" + str(size) + ' bytes.')\n    return fpath", "output": "Download URL to a directory.\n    Will figure out the filename automatically from URL, if not given.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_nodes_full(mask='mask[id]', call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The list_nodes_full function must be called with -f or --function.'\n        )\n\n    ret = {}\n    conn = get_conn(service='SoftLayer_Account')\n    response = conn.getVirtualGuests()\n    for node_id in response:\n        hostname = node_id['hostname']\n        ret[hostname] = node_id\n    __utils__['cloud.cache_node_list'](ret, __active_provider_name__.split(':')[0], __opts__)\n    return ret", "output": "Return a list of the VMs that are on the provider", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dot_vals(value):\n    '''\n    \n    '''\n    ret = {}\n    for key, val in six.iteritems(__pillar__.get('master', {})):\n        if key.startswith('{0}.'.format(value)):\n            ret[key] = val\n    for key, val in six.iteritems(__opts__):\n        if key.startswith('{0}.'.format(value)):\n            ret[key] = val\n    return ret", "output": "Pass in a configuration value that should be preceded by the module name\n    and a dot, this will return a list of all read key/value pairs\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' config.dot_vals host", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_nodelay(self, value: bool) -> None:\n        \"\"\"\n        \"\"\"\n        assert self.ws_connection is not None\n        self.ws_connection.set_nodelay(value)", "output": "Set the no-delay flag for this stream.\n\n        By default, small messages may be delayed and/or combined to minimize\n        the number of packets sent.  This can sometimes cause 200-500ms delays\n        due to the interaction between Nagle's algorithm and TCP delayed\n        ACKs.  To reduce this delay (at the expense of possibly increasing\n        bandwidth usage), call ``self.set_nodelay(True)`` once the websocket\n        connection is established.\n\n        See `.BaseIOStream.set_nodelay` for additional details.\n\n        .. versionadded:: 3.1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def config_set(key, value):\n    '''\n    \n\n    '''\n    cmd = 'lxc config set \"{0}\" \"{1}\"'.format(\n        key,\n        value,\n    )\n\n    output = __salt__['cmd.run'](cmd)\n    if 'error:' in output:\n        raise CommandExecutionError(\n            output[output.index('error:') + 7:],\n        )\n\n    return 'Config value \"{0}\" successfully set.'.format(key),", "output": "Set an LXD daemon config option\n\n    CLI Examples:\n\n    To listen on IPv4 and IPv6 port 8443,\n    you can omit the :8443 its the default:\n\n    .. code-block:: bash\n\n        salt '*' lxd.config_set core.https_address [::]:8443\n\n    To set the server trust password:\n\n    .. code-block:: bash\n\n        salt '*' lxd.config_set core.trust_password blah", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _insert_error(self, path, node):\n        \"\"\" \n        \"\"\"\n        field = path[0]\n        if len(path) == 1:\n            if field in self.tree:\n                subtree = self.tree[field].pop()\n                self.tree[field] += [node, subtree]\n            else:\n                self.tree[field] = [node, {}]\n        elif len(path) >= 1:\n            if field not in self.tree:\n                self.tree[field] = [{}]\n            subtree = self.tree[field][-1]\n\n            if subtree:\n                new = self.__class__(tree=copy(subtree))\n            else:\n                new = self.__class__()\n            new._insert_error(path[1:], node)\n            subtree.update(new.tree)", "output": "Adds an error or sub-tree to :attr:tree.\n\n        :param path: Path to the error.\n        :type path: Tuple of strings and integers.\n        :param node: An error message or a sub-tree.\n        :type node: String or dictionary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_cache_nodes_full(opts=None, provider=None, base=None):\n    '''\n    \n    '''\n    if opts is None:\n        opts = __opts__\n    if opts.get('update_cachedir', False) is False:\n        return\n\n    if base is None:\n        base = os.path.join(opts['cachedir'], 'active')\n\n    minions = {}\n    # First, get a list of all drivers in use\n    for driver in os.listdir(base):\n        minions[driver] = {}\n        prov_dir = os.path.join(base, driver)\n        # Then, get a list of all providers per driver\n        for prov in os.listdir(prov_dir):\n            # If a specific provider is requested, filter out everyone else\n            if provider and provider != prov:\n                continue\n            minions[driver][prov] = {}\n            min_dir = os.path.join(prov_dir, prov)\n            # Get a list of all nodes per provider\n            for fname in os.listdir(min_dir):\n                # Finally, get a list of full minion data\n                fpath = os.path.join(min_dir, fname)\n                minion_id = fname[:-2]  # strip '.p' from end of msgpack filename\n                mode = 'rb' if six.PY3 else 'r'\n                with salt.utils.files.fopen(fpath, mode) as fh_:\n                    minions[driver][prov][minion_id] = salt.utils.data.decode(\n                        salt.utils.msgpack.load(fh_, encoding=MSGPACK_ENCODING))\n\n    return minions", "output": "Return a list of minion data from the cloud cache, rather from the cloud\n    providers themselves. This is the cloud cache version of list_nodes_full().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def new_dataset(expr, missing_values, domain):\n    \"\"\"\n    \n    \"\"\"\n    missing_values = dict(missing_values)\n    class_dict = {'ndim': 2 if SID_FIELD_NAME in expr.fields else 1}\n    for name, type_ in expr.dshape.measure.fields:\n        # Don't generate a column for sid or timestamp, since they're\n        # implicitly the labels if the arrays that will be passed to pipeline\n        # Terms.\n        if name in (SID_FIELD_NAME, TS_FIELD_NAME):\n            continue\n        type_ = datashape_type_to_numpy(type_)\n        if can_represent_dtype(type_):\n            col = Column(\n                type_,\n                missing_values.get(name, NotSpecified),\n            )\n        else:\n            col = NonPipelineField(name, type_)\n        class_dict[name] = col\n\n    if 'domain' in class_dict:\n        raise ValueError(\"Got a column named 'domain' in new_dataset(). \"\n                         \"'domain' is reserved.\")\n    class_dict['domain'] = domain\n\n    name = expr._name\n    if name is None:\n        name = next(_new_names)\n\n    # unicode is a name error in py3 but the branch is only hit\n    # when we are in python 2.\n    if PY2 and isinstance(name, unicode):  # pragma: no cover # noqa\n        name = name.encode('utf-8')\n\n    return type(name, (DataSet,), class_dict)", "output": "Creates or returns a dataset from a blaze expression.\n\n    Parameters\n    ----------\n    expr : Expr\n        The blaze expression representing the values.\n    missing_values : frozenset((name, value) pairs\n        Association pairs column name and missing_value for that column.\n\n        This needs to be a frozenset rather than a dict or tuple of tuples\n        because we want a collection that's unordered but still hashable.\n    domain : zipline.pipeline.domain.Domain\n        Domain of the dataset to be created.\n\n    Returns\n    -------\n    ds : type\n        A new dataset type.\n\n    Notes\n    -----\n    This function is memoized. repeated calls with the same inputs will return\n    the same type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_key_pair(name, profile, **libcloud_kwargs):\n    '''\n    \n    '''\n    conn = _get_driver(profile=profile)\n    libcloud_kwargs = salt.utils.args.clean_kwargs(**libcloud_kwargs)\n    key = conn.get_key_pair(name)\n    return conn.delete_key_pair(key, **libcloud_kwargs)", "output": "Delete a key pair\n\n    :param name: Key pair name.\n    :type  name: ``str``\n\n    :param profile: The profile key\n    :type  profile: ``str``\n\n    :param libcloud_kwargs: Extra arguments for the driver's import_key_pair_from_xxx method\n    :type  libcloud_kwargs: ``dict``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion libcloud_compute.delete_key_pair pair1 profile1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_nb(fname, dest_path='.'):\n    \"\"\n    from .gen_notebooks import remove_undoc_cells, remove_code_cell_jupyter_widget_state_elem\n    nb = read_nb(fname)\n    nb['cells'] = remove_undoc_cells(nb['cells'])\n    nb['cells'] = remove_code_cell_jupyter_widget_state_elem(nb['cells'])\n    fname = Path(fname).absolute()\n    dest_name = fname.with_suffix('.html').name\n    meta = nb['metadata']\n    meta_jekyll = meta['jekyll'] if 'jekyll' in meta else {'title': fname.with_suffix('').name}\n    meta_jekyll['nb_path'] = f'{fname.parent.name}/{fname.name}'\n    with open(f'{dest_path}/{dest_name}','w') as f:\n        f.write(exporter.from_notebook_node(nb, resources=meta_jekyll)[0])", "output": "Convert a notebook `fname` to html file in `dest_path`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_virtual_machine_scale_set_network_interfaces(scale_set, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    result = {}\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        nics = __utils__['azurearm.paged_object_to_list'](\n            netconn.network_interfaces.list_virtual_machine_scale_set_network_interfaces(\n                virtual_machine_scale_set_name=scale_set,\n                resource_group_name=resource_group\n            )\n        )\n\n        for nic in nics:\n            result[nic['name']] = nic\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Get information about all network interfaces within a scale set.\n\n    :param scale_set: The name of the scale set to query.\n\n    :param resource_group: The resource group name assigned to the\n        scale set.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.list_virtual_machine_scale_set_vm_network_interfaces testset testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_delete(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.delete_image(**kwargs)", "output": "Delete an image\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' glanceng.image_delete name=image1\n        salt '*' glanceng.image_delete name=0e4febc2a5ab4f2c8f374b054162506d", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_symbol(prototxt_fname):\n    \"\"\"\n    \"\"\"\n    sym, output_name, input_dim = _parse_proto(prototxt_fname)\n    exec(sym)                   # pylint: disable=exec-used\n    _locals = locals()\n    ret = []\n    for i in  output_name:\n        exec(\"ret = \" + i, globals(), _locals)  # pylint: disable=exec-used\n        ret.append(_locals['ret'])\n    ret = mx.sym.Group(ret)\n    return ret, input_dim", "output": "Convert caffe model definition into Symbol\n\n    Parameters\n    ----------\n    prototxt_fname : str\n        Filename of the prototxt file\n\n    Returns\n    -------\n    Symbol\n        Converted Symbol\n    tuple\n        Input shape", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def concatenate_join_units(join_units, concat_axis, copy):\n    \"\"\"\n    \n    \"\"\"\n    if concat_axis == 0 and len(join_units) > 1:\n        # Concatenating join units along ax0 is handled in _merge_blocks.\n        raise AssertionError(\"Concatenating join units along axis0\")\n\n    empty_dtype, upcasted_na = get_empty_dtype_and_na(join_units)\n\n    to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n                                         upcasted_na=upcasted_na)\n                 for ju in join_units]\n\n    if len(to_concat) == 1:\n        # Only one block, nothing to concatenate.\n        concat_values = to_concat[0]\n        if copy:\n            if isinstance(concat_values, np.ndarray):\n                # non-reindexed (=not yet copied) arrays are made into a view\n                # in JoinUnit.get_reindexed_values\n                if concat_values.base is not None:\n                    concat_values = concat_values.copy()\n            else:\n                concat_values = concat_values.copy()\n    else:\n        concat_values = _concat._concat_compat(to_concat, axis=concat_axis)\n\n    return concat_values", "output": "Concatenate values from several join units along selected axis.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def solve(self):  # type: () -> SolverResult\n        \"\"\"\n        \n        \"\"\"\n        start = time.time()\n        root_dependency = Dependency(self._root.name, self._root.version)\n        root_dependency.is_root = True\n\n        self._add_incompatibility(\n            Incompatibility([Term(root_dependency, False)], RootCause())\n        )\n\n        try:\n            next = self._root.name\n            while next is not None:\n                self._propagate(next)\n                next = self._choose_package_version()\n\n            return self._result()\n        except Exception:\n            raise\n        finally:\n            self._log(\n                \"Version solving took {:.3f} seconds.\\n\"\n                \"Tried {} solutions.\".format(\n                    time.time() - start, self._solution.attempted_solutions\n                )\n            )", "output": "Finds a set of dependencies that match the root package's constraints,\n        or raises an error if no such set is available.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def MergeMessage(\n      self, source, destination,\n      replace_message_field=False, replace_repeated_field=False):\n    \"\"\"\n    \"\"\"\n    tree = _FieldMaskTree(self)\n    tree.MergeMessage(\n        source, destination, replace_message_field, replace_repeated_field)", "output": "Merges fields specified in FieldMask from source to destination.\n\n    Args:\n      source: Source message.\n      destination: The destination message to be merged into.\n      replace_message_field: Replace message field if True. Merge message\n          field if False.\n      replace_repeated_field: Replace repeated field if True. Append\n          elements of repeated field if False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def drop_retention_policy(database, name, **client_args):\n    '''\n    \n    '''\n    client = _client(**client_args)\n    client.drop_retention_policy(name, database)\n\n    return True", "output": "Drop a retention policy.\n\n    database\n        Name of the database for which the retention policy will be dropped.\n\n    name\n        Name of the retention policy to drop.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb.drop_retention_policy mydb mypr", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def location_path(cls, project, location):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/locations/{location}\",\n            project=project,\n            location=location,\n        )", "output": "Return a fully-qualified location string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dropout_mask(dropout_probability: float, tensor_for_masking: torch.Tensor):\n    \"\"\"\n    \n    \"\"\"\n    binary_mask = (torch.rand(tensor_for_masking.size()) > dropout_probability).to(tensor_for_masking.device)\n    # Scale mask by 1/keep_prob to preserve output statistics.\n    dropout_mask = binary_mask.float().div(1.0 - dropout_probability)\n    return dropout_mask", "output": "Computes and returns an element-wise dropout mask for a given tensor, where\n    each element in the mask is dropped out with probability dropout_probability.\n    Note that the mask is NOT applied to the tensor - the tensor is passed to retain\n    the correct CUDA tensor type for the mask.\n\n    Parameters\n    ----------\n    dropout_probability : float, required.\n        Probability of dropping a dimension of the input.\n    tensor_for_masking : torch.Tensor, required.\n\n\n    Returns\n    -------\n    A torch.FloatTensor consisting of the binary mask scaled by 1/ (1 - dropout_probability).\n    This scaling ensures expected values and variances of the output of applying this mask\n     and the original tensor are the same.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_packages(self, node):\n        '''\n        \n        '''\n        pkgs = etree.SubElement(node, 'packages')\n        for pkg_name, pkg_version in sorted(self._data.software.get('packages', {}).items()):\n            pkg = etree.SubElement(pkgs, 'package')\n            pkg.set('name', pkg_name)\n\n        # Add collections (SUSE)\n        if self.__grains__.get('os_family', '') == 'Suse':\n            for ptn_id, ptn_data in self._data.software.get('patterns', {}).items():\n                if ptn_data.get('installed'):\n                    ptn = etree.SubElement(pkgs, 'namedCollection')\n                    ptn.set('name', ptn_id)\n\n        return pkgs", "output": "Set packages and collections.\n\n        :param node:\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def MakeClass(descriptor):\n  \"\"\"\n  \"\"\"\n  if descriptor in MESSAGE_CLASS_CACHE:\n    return MESSAGE_CLASS_CACHE[descriptor]\n\n  attributes = {}\n  for name, nested_type in descriptor.nested_types_by_name.items():\n    attributes[name] = MakeClass(nested_type)\n\n  attributes[GeneratedProtocolMessageType._DESCRIPTOR_KEY] = descriptor\n\n  result = GeneratedProtocolMessageType(\n      str(descriptor.name), (message.Message,), attributes)\n  MESSAGE_CLASS_CACHE[descriptor] = result\n  return result", "output": "Construct a class object for a protobuf described by descriptor.\n\n  Composite descriptors are handled by defining the new class as a member of the\n  parent class, recursing as deep as necessary.\n  This is the dynamic equivalent to:\n\n  class Parent(message.Message):\n    __metaclass__ = GeneratedProtocolMessageType\n    DESCRIPTOR = descriptor\n    class Child(message.Message):\n      __metaclass__ = GeneratedProtocolMessageType\n      DESCRIPTOR = descriptor.nested_types[0]\n\n  Sample usage:\n    file_descriptor = descriptor_pb2.FileDescriptorProto()\n    file_descriptor.ParseFromString(proto2_string)\n    msg_descriptor = descriptor.MakeDescriptor(file_descriptor.message_type[0])\n    msg_class = reflection.MakeClass(msg_descriptor)\n    msg = msg_class()\n\n  Args:\n    descriptor: A descriptor.Descriptor object describing the protobuf.\n  Returns:\n    The Message class object described by the descriptor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _default_key_normalizer(key_class, request_context):\n    \"\"\"\n    \n    \"\"\"\n    # Since we mutate the dictionary, make a copy first\n    context = request_context.copy()\n    context['scheme'] = context['scheme'].lower()\n    context['host'] = context['host'].lower()\n\n    # These are both dictionaries and need to be transformed into frozensets\n    for key in ('headers', '_proxy_headers', '_socks_options'):\n        if key in context and context[key] is not None:\n            context[key] = frozenset(context[key].items())\n\n    # The socket_options key may be a list and needs to be transformed into a\n    # tuple.\n    socket_opts = context.get('socket_options')\n    if socket_opts is not None:\n        context['socket_options'] = tuple(socket_opts)\n\n    # Map the kwargs to the names in the namedtuple - this is necessary since\n    # namedtuples can't have fields starting with '_'.\n    for key in list(context.keys()):\n        context['key_' + key] = context.pop(key)\n\n    # Default to ``None`` for keys missing from the context\n    for field in key_class._fields:\n        if field not in context:\n            context[field] = None\n\n    return key_class(**context)", "output": "Create a pool key out of a request context dictionary.\n\n    According to RFC 3986, both the scheme and host are case-insensitive.\n    Therefore, this function normalizes both before constructing the pool\n    key for an HTTPS request. If you wish to change this behaviour, provide\n    alternate callables to ``key_fn_by_scheme``.\n\n    :param key_class:\n        The class to use when constructing the key. This should be a namedtuple\n        with the ``scheme`` and ``host`` keys at a minimum.\n    :type  key_class: namedtuple\n    :param request_context:\n        A dictionary-like object that contain the context for a request.\n    :type  request_context: dict\n\n    :return: A namedtuple that can be used as a connection pool key.\n    :rtype:  PoolKey", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def factorial(n, mod=None):\n    \"\"\"\"\"\"\n    if not (isinstance(n, int) and n >= 0):\n        raise ValueError(\"'n' must be a non-negative integer.\")\n    if mod is not None and not (isinstance(mod, int) and mod > 0):\n        raise ValueError(\"'mod' must be a positive integer\")\n    result = 1\n    if n == 0:\n        return 1\n    for i in range(2, n+1):\n        result *= i\n        if mod:\n            result %= mod\n    return result", "output": "Calculates factorial iteratively.\n    If mod is not None, then return (n! % mod)\n    Time Complexity - O(n)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_csv_labels(fn, skip_header=True, cat_separator = ' '):\n    \"\"\"\n    \"\"\"\n    df = pd.read_csv(fn, index_col=0, header=0 if skip_header else None, dtype=str)\n    fnames = df.index.values\n    df.iloc[:,0] = df.iloc[:,0].str.split(cat_separator)\n    return fnames, list(df.to_dict().values())[0]", "output": "Parse filenames and label sets from a CSV file.\n\n    This method expects that the csv file at path :fn: has two columns. If it\n    has a header, :skip_header: should be set to True. The labels in the\n    label set are expected to be space separated.\n\n    Arguments:\n        fn: Path to a CSV file.\n        skip_header: A boolean flag indicating whether to skip the header.\n\n    Returns:\n        a two-tuple of (\n            image filenames,\n            a dictionary of filenames and corresponding labels\n        )\n    .\n    :param cat_separator: the separator for the categories column", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _to_integral(self, output_array):\n        \"\"\"\n        \n        \"\"\"\n        if self.dtype == int64_dtype:\n            group_labels = output_array\n            null_label = self.missing_value\n        elif self.dtype == categorical_dtype:\n            # Coerce LabelArray into an isomorphic array of ints.  This is\n            # necessary because np.where doesn't know about LabelArrays or the\n            # void dtype.\n            group_labels = output_array.as_int_array()\n            null_label = output_array.missing_value_code\n        else:\n            raise AssertionError(\n                \"Unexpected Classifier dtype: %s.\" % self.dtype\n            )\n        return group_labels, null_label", "output": "Convert an array produced by this classifier into an array of integer\n        labels and a missing value label.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_tip(self, point, tip):\n        \"\"\"\n        \n        \"\"\"\n        # Don't attempt to show it if it's already visible and the text\n        # to be displayed is the same as the one displayed before.\n        if self.isVisible():\n            if self.tip == tip:\n                return True\n            else:\n                self.hide()\n\n        # Set the text and resize the widget accordingly.\n        self.tip = tip\n        self.setText(tip)\n        self.resize(self.sizeHint())\n        self.move(point)\n        self.show()\n        return True", "output": "Attempts to show the specified tip at the current cursor location.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def monitor(name):\n    '''\n    \n    '''\n    ret = {'result': None,\n           'name': name,\n           'comment': '',\n           'changes': {}\n           }\n    result = __salt__['monit.summary'](name)\n\n    try:\n        for key, value in result.items():\n            if 'Running' in value[name]:\n                ret['comment'] = ('{0} is being being monitored.').format(name)\n                ret['result'] = True\n            else:\n                if __opts__['test']:\n                    ret['comment'] = 'Service {0} is set to be monitored.'.format(name)\n                    ret['result'] = None\n                    return ret\n                __salt__['monit.monitor'](name)\n                ret['comment'] = ('{0} started to be monitored.').format(name)\n                ret['changes'][name] = 'Running'\n                ret['result'] = True\n                break\n    except KeyError:\n        ret['comment'] = ('{0} not found in configuration.').format(name)\n        ret['result'] = False\n\n    return ret", "output": "Get the summary from module monit and try to see if service is\n    being monitored. If not then monitor the service.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_config(config, schema=CLUSTER_CONFIG_SCHEMA):\n    \"\"\"\"\"\"\n    if not isinstance(config, dict):\n        raise ValueError(\"Config {} is not a dictionary\".format(config))\n\n    check_required(config, schema)\n    check_extraneous(config, schema)", "output": "Required Dicts indicate that no extra fields can be introduced.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode(data_url):\n    \"\"\"\n    \n    \"\"\"\n    metadata, data = data_url.rsplit(',', 1)\n    _, metadata = metadata.split('data:', 1)\n    parts = metadata.split(';')\n    if parts[-1] == 'base64':\n        data = b64decode(data)\n    else:\n        data = unquote(data)\n\n    for part in parts:\n        if part.startswith(\"charset=\"):\n            data = data.decode(part[8:])\n    return data", "output": "Decode DataURL data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_rich_rule(zone, rule, permanent=True):\n    '''\n    \n    '''\n    cmd = \"--zone={0} --remove-rich-rule='{1}'\".format(zone, rule)\n\n    if permanent:\n        cmd += ' --permanent'\n\n    return __firewall_cmd(cmd)", "output": "Add a rich rule to a zone\n\n    .. versionadded:: 2016.11.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' firewalld.remove_rich_rule zone 'rule'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __get_host(node, vm_):\n    '''\n    \n    '''\n    if __get_ssh_interface(vm_) == 'private_ips' or vm_['external_ip'] is None:\n        ip_address = node.private_ips[0]\n        log.info('Salt node data. Private_ip: %s', ip_address)\n    else:\n        ip_address = node.public_ips[0]\n        log.info('Salt node data. Public_ip: %s', ip_address)\n\n    if ip_address:\n        return ip_address\n\n    return node.name", "output": "Return public IP, private IP, or hostname for the libcloud 'node' object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_count(self, index):\n        \"\"\"\n        \"\"\"\n        if not isinstance(index, (list, tuple)):\n            index = [index]\n        for idx in index:\n            if idx not in self._index_update_count:\n                self._index_update_count[idx] = self.begin_num_update\n            self._index_update_count[idx] += 1\n            self.num_update = max(self._index_update_count[idx], self.num_update)", "output": "Updates num_update.\n\n        Parameters\n        ----------\n        index : int or list of int\n            The index to be updated.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vis_detection(im_orig, detections, class_names, thresh=0.7):\n    \"\"\"\"\"\"\n    import matplotlib.pyplot as plt\n    import random\n    plt.imshow(im_orig)\n    colors = [(random.random(), random.random(), random.random()) for _ in class_names]\n    for [cls, conf, x1, y1, x2, y2] in detections:\n        cls = int(cls)\n        if cls > 0 and conf > thresh:\n            rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n                                 fill=False, edgecolor=colors[cls], linewidth=3.5)\n            plt.gca().add_patch(rect)\n            plt.gca().text(x1, y1 - 2, '{:s} {:.3f}'.format(class_names[cls], conf),\n                           bbox=dict(facecolor=colors[cls], alpha=0.5), fontsize=12, color='white')\n    plt.show()", "output": "visualize [cls, conf, x1, y1, x2, y2]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def datashape_type_to_numpy(type_):\n    \"\"\"\n    \n\n    \"\"\"\n    if isinstance(type_, Option):\n        type_ = type_.ty\n    if isinstance(type_, DateTime):\n        return np.dtype('datetime64[ns]')\n    if isinstance(type_, String):\n        return np.dtype(object)\n    if type_ in integral:\n        return np.dtype('int64')\n    else:\n        return type_.to_numpy_dtype()", "output": "Given a datashape type, return the associated numpy type. Maps\n    datashape's DateTime type to numpy's `datetime64[ns]` dtype, since the\n    numpy datetime returned by datashape isn't supported by pipeline.\n\n    Parameters\n    ----------\n    type_: datashape.coretypes.Type\n        The datashape type.\n\n    Returns\n    -------\n    type_ np.dtype\n        The numpy dtype.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_list(shapes, types):\n        \"\"\"\n        \"\"\"\n        if types is not None:\n            type_dict = dict(types)\n            return [DataDesc(x[0], x[1], type_dict[x[0]]) for x in shapes]\n        else:\n            return [DataDesc(x[0], x[1]) for x in shapes]", "output": "Get DataDesc list from attribute lists.\n\n        Parameters\n        ----------\n        shapes : a tuple of (name_, shape_)\n        types : a tuple of  (name_, np.dtype)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lower_endian_to_number(l, base):\n  \"\"\"\"\"\"\n  return sum([d * (base**i) for i, d in enumerate(l)])", "output": "Helper function: convert a list of digits in the given base to a number.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_resource(zone, resource_type, resource_key, resource_value):\n    '''\n    \n    '''\n    ret = {'status': True}\n\n    # generate update script\n    cfg_file = salt.utils.files.mkstemp()\n    with salt.utils.files.fpopen(cfg_file, 'w+', mode=0o600) as fp_:\n        if resource_key:\n            fp_.write(\"remove {0} {1}={2}\\n\".format(resource_type, resource_key, _sanitize_value(resource_value)))\n        else:\n            fp_.write(\"remove {0}\\n\".format(resource_type))\n\n    # update property\n    if cfg_file:\n        _dump_cfg(cfg_file)\n        res = __salt__['cmd.run_all']('zonecfg -z {zone} -f {path}'.format(\n            zone=zone,\n            path=cfg_file,\n        ))\n        ret['status'] = res['retcode'] == 0\n        ret['message'] = res['stdout'] if ret['status'] else res['stderr']\n        if ret['message'] == '':\n            del ret['message']\n        else:\n            ret['message'] = _clean_message(ret['message'])\n\n        # cleanup config file\n        if __salt__['file.file_exists'](cfg_file):\n            __salt__['file.remove'](cfg_file)\n\n    return ret", "output": "Remove a resource\n\n    zone : string\n        name of zone\n    resource_type : string\n        type of resource\n    resource_key : string\n        key for resource selection\n    resource_value : string\n        value for resource selection\n\n    .. note::\n        Set resource_selector to None for resource that do not require one.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zonecfg.remove_resource tallgeese rctl name zone.max-locked-memory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ensure_extra_rows(self, term, N):\n        \"\"\"\n        \n        \"\"\"\n        attrs = self.graph.node[term]\n        attrs['extra_rows'] = max(N, attrs.get('extra_rows', 0))", "output": "Ensure that we're going to compute at least N extra rows of `term`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_network_profile(name, network_profile, nic_opts=None, path=None):\n    '''\n    \n    '''\n    cpath = get_root_path(path)\n    cfgpath = os.path.join(cpath, name, 'config')\n\n    before = []\n    with salt.utils.files.fopen(cfgpath, 'r') as fp_:\n        for line in fp_:\n            before.append(line)\n\n    lxcconfig = _LXCConfig(name=name, path=path)\n    old_net = lxcconfig._filter_data('lxc.network')\n\n    network_params = {}\n    for param in _network_conf(\n        conf_tuples=old_net,\n        network_profile=network_profile, nic_opts=nic_opts\n    ):\n        network_params.update(param)\n    if network_params:\n        edit_conf(cfgpath, out_format='commented', **network_params)\n\n    after = []\n    with salt.utils.files.fopen(cfgpath, 'r') as fp_:\n        for line in fp_:\n            after.append(line)\n\n    diff = ''\n    for line in difflib.unified_diff(before,\n                                     after,\n                                     fromfile='before',\n                                     tofile='after'):\n        diff += line\n    return diff", "output": ".. versionadded:: 2015.5.0\n\n    Apply a network profile to a container\n\n    network_profile\n        profile name or default values (dict)\n\n    nic_opts\n        values to override in defaults (dict)\n        indexed by nic card names\n\n    path\n        path to the container parent\n\n        .. versionadded:: 2015.8.0\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt 'minion' lxc.apply_network_profile web1 centos\n        salt 'minion' lxc.apply_network_profile web1 centos \\\\\n                nic_opts=\"{'eth0': {'mac': 'xx:xx:xx:xx:xx:xx'}}\"\n        salt 'minion' lxc.apply_network_profile web1 \\\\\n                \"{'eth0': {'mac': 'xx:xx:xx:xx:xx:yy'}}\"\n                nic_opts=\"{'eth0': {'mac': 'xx:xx:xx:xx:xx:xx'}}\"\n\n    The special case to disable use of ethernet nics:\n\n    .. code-block:: bash\n\n        salt 'minion' lxc.apply_network_profile web1 centos \\\\\n                \"{eth0: {disable: true}}\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_forecast(self, job_id, forecast_id=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if job_id in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n        return self.transport.perform_request(\n            \"DELETE\",\n            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_forecast\", forecast_id),\n            params=params,\n        )", "output": "`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-forecast.html>`_\n\n        :arg job_id: The ID of the job from which to delete forecasts\n        :arg forecast_id: The ID of the forecast to delete, can be comma\n            delimited list. Leaving blank implies `_all`\n        :arg allow_no_forecasts: Whether to ignore if `_all` matches no\n            forecasts\n        :arg timeout: Controls the time to wait until the forecast(s) are\n            deleted. Default to 30 seconds", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show(self, ax:plt.Axes=None, figsize:tuple=(3,3), title:Optional[str]=None, hide_axis:bool=True,\n        cmap:str='tab20', alpha:float=0.5, **kwargs):\n        \"\"\n        ax = show_image(self, ax=ax, hide_axis=hide_axis, cmap=cmap, figsize=figsize,\n                        interpolation='nearest', alpha=alpha, vmin=0)\n        if title: ax.set_title(title)", "output": "Show the `ImageSegment` on `ax`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_reshape(net, node, module, builder):\n    \"\"\"\n    \"\"\"\n    input_name, output_name = _get_input_output_name(net, node)\n    name = node['name']\n    param = _get_attr(node)\n    target_shape = literal_eval(param['shape'])\n    if target_shape == (0, -1):\n        convert_flatten(net, node, module, builder)\n        return\n\n    if any(item <= 0 for item in target_shape):\n        raise NotImplementedError('Special dimensional values less than or equal to 0 are not supported yet.'\n                                  'Feel free to file an issue here: https://github.com/dmlc/mxnet/issues.')\n\n    if 'reverse' in node and node['reverse'] == 'True':\n        raise NotImplementedError('\"reverse\" parameter is not supported by yet.'\n                                  'Feel free to file an issue here: https://github.com/dmlc/mxnet/issues.')\n\n    mode = 0 # CHANNEL_FIRST\n    builder.add_reshape(name, input_name, output_name, target_shape, mode)", "output": "Converts a reshape layer from mxnet to coreml.\n\n    This doesn't currently handle the deprecated parameters for the reshape layer.\n\n    Parameters\n    ----------\n    net: network\n        An mxnet network object.\n\n    node: layer\n        Node to convert.\n\n    module: module\n        A module for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_objects_with_same_attribute(self,\n                                         objects: Set[Object],\n                                         attribute_function: Callable[[Object], str]) -> Set[Object]:\n        \"\"\"\n        \n        \"\"\"\n        objects_of_attribute: Dict[str, Set[Object]] = defaultdict(set)\n        for entity in objects:\n            objects_of_attribute[attribute_function(entity)].add(entity)\n        if not objects_of_attribute:\n            return set()\n        most_frequent_attribute = max(objects_of_attribute, key=lambda x: len(objects_of_attribute[x]))\n        if len(objects_of_attribute[most_frequent_attribute]) <= 1:\n            return set()\n        return objects_of_attribute[most_frequent_attribute]", "output": "Returns the set of objects for which the attribute function returns an attribute value that\n        is most frequent in the initial set, if the frequency is greater than 1. If not, all\n        objects have different attribute values, and this method returns an empty set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def FixedUnPooling(x, shape, unpool_mat=None, data_format='channels_last'):\n    \"\"\"\n    \n    \"\"\"\n    data_format = get_data_format(data_format, keras_mode=False)\n    shape = shape2d(shape)\n\n    output_shape = StaticDynamicShape(x)\n    output_shape.apply(1 if data_format == 'NHWC' else 2, lambda x: x * shape[0])\n    output_shape.apply(2 if data_format == 'NHWC' else 3, lambda x: x * shape[1])\n\n    # a faster implementation for this special case\n    if shape[0] == 2 and shape[1] == 2 and unpool_mat is None and data_format == 'NHWC':\n        ret = UnPooling2x2ZeroFilled(x)\n    else:\n        # check unpool_mat\n        if unpool_mat is None:\n            mat = np.zeros(shape, dtype='float32')\n            mat[0][0] = 1\n            unpool_mat = tf.constant(mat, name='unpool_mat')\n        elif isinstance(unpool_mat, np.ndarray):\n            unpool_mat = tf.constant(unpool_mat, name='unpool_mat')\n        assert unpool_mat.shape.as_list() == list(shape)\n\n        if data_format == 'NHWC':\n            x = tf.transpose(x, [0, 3, 1, 2])\n        # perform a tensor-matrix kronecker product\n        x = tf.expand_dims(x, -1)       # bchwx1\n        mat = tf.expand_dims(unpool_mat, 0)  # 1xshxsw\n        ret = tf.tensordot(x, mat, axes=1)  # bxcxhxwxshxsw\n\n        if data_format == 'NHWC':\n            ret = tf.transpose(ret, [0, 2, 4, 3, 5, 1])\n        else:\n            ret = tf.transpose(ret, [0, 1, 2, 4, 3, 5])\n\n        shape3_dyn = [output_shape.get_dynamic(k) for k in range(1, 4)]\n        ret = tf.reshape(ret, tf.stack([-1] + shape3_dyn))\n\n    ret.set_shape(tf.TensorShape(output_shape.get_static()))\n    return ret", "output": "Unpool the input with a fixed matrix to perform kronecker product with.\n\n    Args:\n        x (tf.Tensor): a 4D image tensor\n        shape: int or (h, w) tuple\n        unpool_mat: a tf.Tensor or np.ndarray 2D matrix with size=shape.\n            If is None, will use a matrix with 1 at top-left corner.\n\n    Returns:\n        tf.Tensor: a 4D image tensor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def profile(self, profile, names, vm_overrides=None, **kwargs):\n        '''\n        \n\n\n        '''\n        if not vm_overrides:\n            vm_overrides = {}\n        kwargs['profile'] = profile\n        mapper = salt.cloud.Map(self._opts_defaults(**kwargs))\n        if isinstance(names, six.string_types):\n            names = names.split(',')\n        return salt.utils.data.simple_types_filter(\n            mapper.run_profile(profile, names, vm_overrides=vm_overrides)\n        )", "output": "Pass in a profile to create, names is a list of vm names to allocate\n\n            vm_overrides is a special dict that will be per node options\n            overrides\n\n        Example:\n\n        .. code-block:: python\n\n            >>> client= salt.cloud.CloudClient(path='/etc/salt/cloud')\n            >>> client.profile('do_512_git', names=['minion01',])\n            {'minion01': {'backups_active': 'False',\n                    'created_at': '2014-09-04T18:10:15Z',\n                    'droplet': {'event_id': 31000502,\n                                 'id': 2530006,\n                                 'image_id': 5140006,\n                                 'name': 'minion01',\n                                 'size_id': 66},\n                    'id': '2530006',\n                    'image_id': '5140006',\n                    'ip_address': '107.XXX.XXX.XXX',\n                    'locked': 'True',\n                    'name': 'minion01',\n                    'private_ip_address': None,\n                    'region_id': '4',\n                    'size_id': '66',\n                    'status': 'new'}}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_availabilities_for_duration(duration, availabilities):\n    \"\"\"\n    \n    \"\"\"\n    duration_availabilities = []\n    start_time = '10:00'\n    while start_time != '17:00':\n        if start_time in availabilities:\n            if duration == 30:\n                duration_availabilities.append(start_time)\n            elif increment_time_by_thirty_mins(start_time) in availabilities:\n                duration_availabilities.append(start_time)\n\n        start_time = increment_time_by_thirty_mins(start_time)\n\n    return duration_availabilities", "output": "Helper function to return the windows of availability of the given duration, when provided a set of 30 minute windows.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def positions(self):\n        \"\"\"\n        \n        \"\"\"\n        if self._mixed_positions is None:\n            self._mixed_positions = MixedPositions(self._accounts)\n        return self._mixed_positions", "output": "[dict] \u6301\u4ed3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_dependency (self, targets, sources):\n        \"\"\"\n        \"\"\"\n        if isinstance (targets, str):\n            targets = [targets]\n        if isinstance (sources, str):\n            sources = [sources]\n        assert is_iterable(targets)\n        assert is_iterable(sources)\n\n        for target in targets:\n            for source in sources:\n                self.do_add_dependency (target, source)", "output": "Adds a dependency from 'targets' to 'sources'\n\n        Both 'targets' and 'sources' can be either list\n        of target names, or a single target name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def calculate_bbox_area(bbox, rows, cols):\n    \"\"\"\"\"\"\n    bbox = denormalize_bbox(bbox, rows, cols)\n    x_min, y_min, x_max, y_max = bbox[:4]\n    area = (x_max - x_min) * (y_max - y_min)\n    return area", "output": "Calculate the area of a bounding box in pixels.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dumps(o, encoder=None):\n    \"\"\"\n    \"\"\"\n\n    retval = \"\"\n    if encoder is None:\n        encoder = TomlEncoder(o.__class__)\n    addtoretval, sections = encoder.dump_sections(o, \"\")\n    retval += addtoretval\n    while sections:\n        newsections = encoder.get_empty_table()\n        for section in sections:\n            addtoretval, addtosections = encoder.dump_sections(\n                sections[section], section)\n\n            if addtoretval or (not addtoretval and not addtosections):\n                if retval and retval[-2:] != \"\\n\\n\":\n                    retval += \"\\n\"\n                retval += \"[\" + section + \"]\\n\"\n                if addtoretval:\n                    retval += addtoretval\n            for s in addtosections:\n                newsections[section + \".\" + s] = addtosections[s]\n        sections = newsections\n    return retval", "output": "Stringifies input dict as toml\n\n    Args:\n        o: Object to dump into toml\n\n        preserve: Boolean parameter. If true, preserve inline tables.\n\n    Returns:\n        String containing the toml corresponding to dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def CheckValue(self, proposed_value):\n    \"\"\"\n    \"\"\"\n    if not isinstance(proposed_value, self._acceptable_types):\n      message = ('%.1024r has type %s, but expected one of: %s' %\n                 (proposed_value, type(proposed_value), self._acceptable_types))\n      raise TypeError(message)\n    return proposed_value", "output": "Type check the provided value and return it.\n\n    The returned value might have been normalized to another type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enable_chunked_encoding(self, chunk_size: Optional[int]=None) -> None:\n        \"\"\"\"\"\"\n        self._chunked = True\n\n        if hdrs.CONTENT_LENGTH in self._headers:\n            raise RuntimeError(\"You can't enable chunked encoding when \"\n                               \"a content length is set\")\n        if chunk_size is not None:\n            warnings.warn('Chunk size is deprecated #1615', DeprecationWarning)", "output": "Enables automatic chunked transfer encoding.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _item_to_tf_feature(item, key_name):\n  \"\"\"\"\"\"\n  v = item\n  if isinstance(v, (list, tuple)) and not v:\n    raise ValueError(\n        \"Feature {} received an empty list value, so is unable to infer the \"\n        \"feature type to record. To support empty value, the corresponding \"\n        \"FeatureConnector should return a numpy array with the correct dtype \"\n        \"instead of a Python list.\".format(key_name)\n    )\n\n  # Handle strings/bytes first\n  if isinstance(v, (six.binary_type, six.string_types)):\n    v = [tf.compat.as_bytes(v)]\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=v))\n  elif (isinstance(v, (tuple, list)) and\n        all(isinstance(x, (six.binary_type, six.string_types)) for x in v)):\n    v = [tf.compat.as_bytes(x) for x in v]\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=v))\n  elif (isinstance(v, np.ndarray) and\n        (v.dtype.kind in (\"U\", \"S\") or v.dtype == object)):  # binary or unicode\n    v = [tf.compat.as_bytes(x) for x in v.flatten()]\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=v))\n\n  # Use NumPy for numeric types\n  v = np.array(v).flatten()  # Convert v into a 1-d array\n\n  if np.issubdtype(v.dtype, np.integer):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=v))\n  elif np.issubdtype(v.dtype, np.floating):\n    return tf.train.Feature(float_list=tf.train.FloatList(value=v))\n  else:\n    raise ValueError(\n        \"Value received: {}.\\n\"\n        \"tf.train.Feature does not support type {} for feature key {}. \"\n        \"This may indicate that one of the FeatureConnectors received an \"\n        \"unsupported value as input.\".format(repr(v), repr(type(v)), key_name)\n    )", "output": "Single item to a tf.train.Feature.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, resource, client):\n        \"\"\"\n        \"\"\"\n        sink_name = resource[\"name\"]\n        instance = cls(sink_name, client=client)\n        instance._update_from_api_repr(resource)\n        return instance", "output": "Factory:  construct a sink given its API representation\n\n        :type resource: dict\n        :param resource: sink resource representation returned from the API\n\n        :type client: :class:`google.cloud.logging.client.Client`\n        :param client: Client which holds credentials and project\n                       configuration for the sink.\n\n        :rtype: :class:`google.cloud.logging.sink.Sink`\n        :returns: Sink parsed from ``resource``.\n        :raises: :class:`ValueError` if ``client`` is not ``None`` and the\n                 project from the resource does not agree with the project\n                 from the client.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reroute(self, body=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request('POST', '/_cluster/reroute',\n            params=params, body=body)", "output": "Explicitly execute a cluster reroute allocation command including specific commands.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-reroute.html>`_\n\n        :arg body: The definition of `commands` to perform (`move`, `cancel`,\n            `allocate`)\n        :arg dry_run: Simulate the operation only and return the resulting state\n        :arg explain: Return an explanation of why the commands can or cannot be\n            executed\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg metric: Limit the information returned to the specified metrics.\n            Defaults to all but metadata, valid choices are: '_all', 'blocks',\n            'metadata', 'nodes', 'routing_table', 'master_node', 'version'\n        :arg retry_failed: Retries allocation of shards that are blocked due to\n            too many subsequent allocation failures\n        :arg timeout: Explicit operation timeout", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sink(self):\n        \"\"\"\"\"\"\n        op = Operator(\n            _generate_uuid(),\n            OpType.Sink,\n            \"Sink\",\n            num_instances=self.env.config.parallelism)\n        return self.__register(op)", "output": "Closes the stream with a sink operator.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download_and_extract(self, url_or_urls):\n    \"\"\"\n    \"\"\"\n    # Add progress bar to follow the download state\n    with self._downloader.tqdm():\n      with self._extractor.tqdm():\n        return _map_promise(self._download_extract, url_or_urls)", "output": "Download and extract given url_or_urls.\n\n    Is roughly equivalent to:\n\n    ```\n    extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))\n    ```\n\n    Args:\n      url_or_urls: url or `list`/`dict` of urls to download and extract. Each\n        url can be a `str` or `tfds.download.Resource`.\n\n    If not explicitly specified in `Resource`, the extraction method will\n    automatically be deduced from downloaded file name.\n\n    Returns:\n      extracted_path(s): `str`, extracted paths of given URL(s).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def plot(self, skip_start:int=10, skip_end:int=5, suggestion:bool=False, return_fig:bool=None,\n             **kwargs)->Optional[plt.Figure]:\n        \"\"\n        lrs = self._split_list(self.lrs, skip_start, skip_end)\n        losses = self._split_list(self.losses, skip_start, skip_end)\n        losses = [x.item() for x in losses]\n        if 'k' in kwargs: losses = self.smoothen_by_spline(lrs, losses, **kwargs)\n        fig, ax = plt.subplots(1,1)\n        ax.plot(lrs, losses)\n        ax.set_ylabel(\"Loss\")\n        ax.set_xlabel(\"Learning Rate\")\n        ax.set_xscale('log')\n        ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%.0e'))\n        if suggestion:\n            try: mg = (np.gradient(np.array(losses))).argmin()\n            except:\n                print(\"Failed to compute the gradients, there might not be enough points.\")\n                return\n            print(f\"Min numerical gradient: {lrs[mg]:.2E}\")\n            ax.plot(lrs[mg],losses[mg],markersize=10,marker='o',color='red')\n            self.min_grad_lr = lrs[mg]\n        if ifnone(return_fig, defaults.return_fig): return fig\n        if not IN_NOTEBOOK: plot_sixel(fig)", "output": "Plot learning rate and losses, trimmed between `skip_start` and `skip_end`. Optionally plot and return min gradient", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _port_add_or_delete_policy(action, name, sel_type=None, protocol=None, port=None, sel_range=None):\n    '''\n    \n    '''\n    if action not in ['add', 'delete']:\n        raise SaltInvocationError('Actions supported are \"add\" and \"delete\", not \"{0}\".'.format(action))\n    if action == 'add' and not sel_type:\n        raise SaltInvocationError('SELinux Type is required to add a policy')\n    (protocol, port) = _parse_protocol_port(name, protocol, port)\n    cmd = 'semanage port --{0} --proto {1}'.format(action, protocol)\n    if sel_type:\n        cmd += ' --type {0}'.format(sel_type)\n    if sel_range:\n        cmd += ' --range {0}'.format(sel_range)\n    cmd += ' {0}'.format(port)\n    return __salt__['cmd.run_all'](cmd)", "output": ".. versionadded:: 2019.2.0\n\n    Performs the action as called from ``port_add_policy`` or ``port_delete_policy``.\n\n    Returns the result of the call to semanage.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _object_table(self, object_id):\n        \"\"\"\n        \"\"\"\n        # Allow the argument to be either an ObjectID or a hex string.\n        if not isinstance(object_id, ray.ObjectID):\n            object_id = ray.ObjectID(hex_to_binary(object_id))\n\n        # Return information about a single object ID.\n        message = self._execute_command(object_id, \"RAY.TABLE_LOOKUP\",\n                                        ray.gcs_utils.TablePrefix.OBJECT, \"\",\n                                        object_id.binary())\n        if message is None:\n            return {}\n        gcs_entry = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(\n            message, 0)\n\n        assert gcs_entry.EntriesLength() > 0\n\n        entry = ray.gcs_utils.ObjectTableData.GetRootAsObjectTableData(\n            gcs_entry.Entries(0), 0)\n\n        object_info = {\n            \"DataSize\": entry.ObjectSize(),\n            \"Manager\": entry.Manager(),\n        }\n\n        return object_info", "output": "Fetch and parse the object table information for a single object ID.\n\n        Args:\n            object_id: An object ID to get information about.\n\n        Returns:\n            A dictionary with information about the object ID in question.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def template_shebang(template, renderers, default, blacklist, whitelist, input_data):\n    '''\n    \n\n    '''\n    line = ''\n    # Open up the first line of the sls template\n    if template == ':string:':\n        line = input_data.split()[0]\n    else:\n        with salt.utils.files.fopen(template, 'r') as ifile:\n            line = salt.utils.stringutils.to_unicode(ifile.readline())\n\n    # Check if it starts with a shebang and not a path\n    if line.startswith('#!') and not line.startswith('#!/'):\n        # pull out the shebang data\n        # If the shebang does not contain recognized/not-blacklisted/whitelisted\n        # renderers, do not fall back to the default renderer\n        return check_render_pipe_str(line.strip()[2:], renderers, blacklist, whitelist)\n    else:\n        return check_render_pipe_str(default, renderers, blacklist, whitelist)", "output": "Check the template shebang line and return the list of renderers specified\n    in the pipe.\n\n    Example shebang lines::\n\n      #!yaml_jinja\n      #!yaml_mako\n      #!mako|yaml\n      #!jinja|yaml\n      #!jinja|mako|yaml\n      #!mako|yaml|stateconf\n      #!jinja|yaml|stateconf\n      #!mako|yaml_odict\n      #!mako|yaml_odict|stateconf", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dataset_list_cli(self,\n                         sort_by=None,\n                         size=None,\n                         file_type=None,\n                         license_name=None,\n                         tag_ids=None,\n                         search=None,\n                         user=None,\n                         mine=False,\n                         page=1,\n                         csv_display=False):\n        \"\"\" \n        \"\"\"\n        datasets = self.dataset_list(sort_by, size, file_type, license_name,\n                                     tag_ids, search, user, mine, page)\n        fields = ['ref', 'title', 'size', 'lastUpdated', 'downloadCount']\n        if datasets:\n            if csv_display:\n                self.print_csv(datasets, fields)\n            else:\n                self.print_table(datasets, fields)\n        else:\n            print('No datasets found')", "output": "a wrapper to datasets_list for the client. Additional parameters\n            are described here, see dataset_list for others.\n\n            Parameters\n            ==========\n            sort_by: how to sort the result, see valid_sort_bys for options\n            size: the size of the dataset, see valid_sizes for string options\n            file_type: the format, see valid_file_types for string options\n            license_name: string descriptor for license, see valid_license_names\n            tag_ids: tag identifiers to filter the search\n            search: a search term to use (default is empty string)\n            user: username to filter the search to\n            mine: boolean if True, group is changed to \"my\" to return personal\n            page: the page to return (default is 1)\n            csv_display: if True, print comma separated values instead of table", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _fetch(self):\n        \"\"\"\n        \n        \"\"\"\n        while True:\n            url = self._to_fetch.get()\n            try:\n                if url:\n                    page = self.get_page(url)\n                    if page is None:    # e.g. after an error\n                        continue\n                    for link, rel in page.links:\n                        if link not in self._seen:\n                            try:\n                                self._seen.add(link)\n                                if (not self._process_download(link) and\n                                    self._should_queue(link, url, rel)):\n                                    logger.debug('Queueing %s from %s', link, url)\n                                    self._to_fetch.put(link)\n                            except MetadataInvalidError:  # e.g. invalid versions\n                                pass\n            except Exception as e:  # pragma: no cover\n                self.errors.put(text_type(e))\n            finally:\n                # always do this, to avoid hangs :-)\n                self._to_fetch.task_done()\n            if not url:\n                #logger.debug('Sentinel seen, quitting.')\n                break", "output": "Get a URL to fetch from the work queue, get the HTML page, examine its\n        links for download candidates and candidates for further scraping.\n\n        This is a handy method to run in a thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_group_host(name, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    if not conn:\n        return None\n    try:\n        cc = conn.describe_replication_groups(name)\n    except boto.exception.BotoServerError as e:\n        msg = 'Failed to get config for cache cluster {0}.'.format(name)\n        log.error(msg)\n        log.debug(e)\n        return {}\n\n    cc = cc['DescribeReplicationGroupsResponse']['DescribeReplicationGroupsResult']\n    cc = cc['ReplicationGroups'][0]['NodeGroups'][0]['PrimaryEndpoint']\n    host = cc['Address']\n    return host", "output": "Get hostname from replication cache group\n\n    CLI example::\n\n        salt myminion boto_elasticache.get_group_host myelasticachegroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def version(syslog_ng_sbin_dir=None):\n    '''\n    \n    '''\n    try:\n        ret = _run_command_in_extended_path(syslog_ng_sbin_dir,\n                                            'syslog-ng',\n                                            ('-V',))\n    except CommandExecutionError as err:\n        return _format_return_data(retcode=-1, stderr=six.text_type(err))\n\n    if ret['retcode'] != 0:\n        return _format_return_data(ret['retcode'],\n                                   stderr=ret['stderr'],\n                                   stdout=ret['stdout'])\n\n    lines = ret['stdout'].split('\\n')\n    # The format of the first line in the output is:\n    # syslog-ng 3.6.0alpha0\n    version_line_index = 0\n    version_column_index = 1\n    line = lines[version_line_index].split()[version_column_index]\n    return _format_return_data(0, stdout=line)", "output": "Returns the version of the installed syslog-ng. If syslog_ng_sbin_dir is\n    specified, it is added to the PATH during the execution of the command\n    syslog-ng.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' syslog_ng.version\n        salt '*' syslog_ng.version /home/user/install/syslog-ng/sbin", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def runCommand(cmd, timeout=None):\n    \"\"\" \n\t\"\"\"\n    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    output = ''\n\n    out, err = p.communicate()\n    p.wait(timeout)\n\n    return (out, err)", "output": "run shell command\n\n\t@param cmd: command to execute\n\t@param timeout: timeout for command execution\n\n\t@return: (return code from command, command output)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def (acquisition_function,\n              samples_y_aggregation,\n              x_bounds, x_types,\n              regressor_gp,\n              minimize_starting_points,\n              minimize_constraints_fun=None):\n    '''\n    \n    '''\n    outputs = None\n\n    sys.stderr.write(\"[%s] Exercise \\\"%s\\\" acquisition function\\n\" \\\n                        % (os.path.basename(__file__), acquisition_function))\n\n    if acquisition_function == \"ei\":\n        outputs = lib_acquisition_function.next_hyperparameter_expected_improvement(\\\n                        gp_prediction.predict, [regressor_gp], x_bounds, x_types, \\\n                        samples_y_aggregation, minimize_starting_points, \\\n                        minimize_constraints_fun=minimize_constraints_fun)\n    elif acquisition_function == \"lc\":\n        outputs = lib_acquisition_function.next_hyperparameter_lowest_confidence(\\\n                        gp_prediction.predict, [regressor_gp], x_bounds, x_types,\\\n                        minimize_starting_points, minimize_constraints_fun=minimize_constraints_fun)\n    elif acquisition_function == \"lm\":\n        outputs = lib_acquisition_function.next_hyperparameter_lowest_mu(\\\n                        gp_prediction.predict, [regressor_gp], x_bounds, x_types,\\\n                        minimize_starting_points, minimize_constraints_fun=minimize_constraints_fun)\n    return outputs", "output": "selection", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _next_page(self):\n        \"\"\"\n        \"\"\"\n        if self._has_next_page():\n            response = self._get_next_page_response()\n            items = response.get(self._items_key, ())\n            page = Page(self, items, self.item_to_value)\n            self._page_start(self, page, response)\n            self.next_page_token = response.get(self._next_token)\n            return page\n        else:\n            return None", "output": "Get the next page in the iterator.\n\n        Returns:\n            Optional[Page]: The next page in the iterator or :data:`None` if\n                there are no pages left.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_asset_node_def(node_def):\n  \"\"\"\"\"\"\n  if node_def.op != \"Const\":\n    raise TypeError(\"Asset node must be of type constant.\")\n  if tf.as_dtype(node_def.attr[\"dtype\"].type) != tf.string:\n    raise TypeError(\"Asset node must be of dtype string.\")\n  if len(node_def.attr[\"value\"].tensor.string_val) != 1:\n    raise TypeError(\"Asset node must be a scalar.\")", "output": "Raises TypeError if `node_def` does not match the expectations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def accumulator(self, value, accum_param=None):\n        \"\"\"\n        \n        \"\"\"\n        if accum_param is None:\n            if isinstance(value, int):\n                accum_param = accumulators.INT_ACCUMULATOR_PARAM\n            elif isinstance(value, float):\n                accum_param = accumulators.FLOAT_ACCUMULATOR_PARAM\n            elif isinstance(value, complex):\n                accum_param = accumulators.COMPLEX_ACCUMULATOR_PARAM\n            else:\n                raise TypeError(\"No default accumulator param for type %s\" % type(value))\n        SparkContext._next_accum_id += 1\n        return Accumulator(SparkContext._next_accum_id - 1, value, accum_param)", "output": "Create an L{Accumulator} with the given initial value, using a given\n        L{AccumulatorParam} helper object to define how to add values of the\n        data type if provided. Default AccumulatorParams are used for integers\n        and floating-point numbers if you do not provide one. For other types,\n        a custom AccumulatorParam can be used.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_model(self, steps):\n        \"\"\"\n        \n        \"\"\"\n        with self.graph.as_default():\n            last_checkpoint = self.model_path + '/model-' + str(steps) + '.cptk'\n            self.saver.save(self.sess, last_checkpoint)\n            tf.train.write_graph(self.graph, self.model_path,\n                                 'raw_graph_def.pb', as_text=False)", "output": "Saves the model\n        :param steps: The number of steps the model was trained for\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.purge_existing_index:\n            self.delete_index()\n        self.create_index()\n        es = self._init_connection()\n        if self.mapping:\n            es.indices.put_mapping(index=self.index, doc_type=self.doc_type,\n                                   body=self.mapping)\n        es.indices.put_settings({\"index\": {\"refresh_interval\": \"-1\"}},\n                                index=self.index)\n\n        bulk(es, self._docs(), chunk_size=self.chunk_size,\n             raise_on_error=self.raise_on_error)\n\n        es.indices.put_settings({\"index\": {\"refresh_interval\": \"1s\"}},\n                                index=self.index)\n        es.indices.refresh()\n        self.output().touch()", "output": "Run task, namely:\n\n        * purge existing index, if requested (`purge_existing_index`),\n        * create the index, if missing,\n        * apply mappings, if given,\n        * set refresh interval to -1 (disable) for performance reasons,\n        * bulk index in batches of size `chunk_size` (2000),\n        * set refresh interval to 1s,\n        * refresh Elasticsearch,\n        * create entry in marker index.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tsplot(series, plotf, ax=None, **kwargs):\n    import warnings\n    \"\"\"\n    \n    \"\"\"\n    warnings.warn(\"'tsplot' is deprecated and will be removed in a \"\n                  \"future version. Please use Series.plot() instead.\",\n                  FutureWarning, stacklevel=2)\n\n    # Used inferred freq is possible, need a test case for inferred\n    if ax is None:\n        import matplotlib.pyplot as plt\n        ax = plt.gca()\n\n    freq, series = _maybe_resample(series, ax, kwargs)\n\n    # Set ax with freq info\n    _decorate_axes(ax, freq, kwargs)\n    ax._plot_data.append((series, plotf, kwargs))\n    lines = plotf(ax, series.index._mpl_repr(), series.values, **kwargs)\n\n    # set date formatter, locators and rescale limits\n    format_dateaxis(ax, ax.freq, series.index)\n    return lines", "output": "Plots a Series on the given Matplotlib axes or the current axes\n\n    Parameters\n    ----------\n    axes : Axes\n    series : Series\n\n    Notes\n    _____\n    Supports same kwargs as Axes.plot\n\n\n    .. deprecated:: 0.23.0\n       Use Series.plot() instead", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge_with(self, other):\n        \"\"\"\n        \"\"\"\n        other = as_shape(other)\n        if self._dims is None:\n            return other\n        else:\n            try:\n                self.assert_same_rank(other)\n                new_dims = []\n                for i, dim in enumerate(self._dims):\n                    new_dims.append(dim.merge_with(other[i]))\n                return TensorShape(new_dims)\n            except ValueError:\n                raise ValueError(\"Shapes %s and %s are not convertible\" % (self, other))", "output": "Returns a `TensorShape` combining the information in `self` and `other`.\n\n        The dimensions in `self` and `other` are merged elementwise,\n        according to the rules defined for `Dimension.merge_with()`.\n\n        Args:\n          other: Another `TensorShape`.\n\n        Returns:\n          A `TensorShape` containing the combined information of `self` and\n          `other`.\n\n        Raises:\n          ValueError: If `self` and `other` are not convertible.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_page(self, index=None):\r\n        \"\"\"\"\"\"\r\n        if index is None:\r\n            widget = self.pages_widget.currentWidget()\r\n        else:\r\n            widget = self.pages_widget.widget(index)\r\n        return widget.widget()", "output": "Return page widget", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rm(device, minor):  # pylint: disable=C0103\n    '''\n    \n    '''\n    _validate_device(device)\n\n    try:\n        int(minor)\n    except Exception:\n        raise CommandExecutionError(\n            'Invalid minor number passed to partition.rm'\n        )\n\n    cmd = 'parted -m -s {0} rm {1}'.format(device, minor)\n    out = __salt__['cmd.run'](cmd).splitlines()\n    return out", "output": "Removes the partition with number <minor>.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' partition.rm /dev/sda 5", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_id_to_path_mapping(self):\n    \"\"\"\n    \"\"\"\n    if not self.id_to_path_mapping:\n      return\n    with open(self.local_id_to_path_mapping_file, 'w') as f:\n      writer = csv.writer(f)\n      writer.writerow(['id', 'path'])\n      for k, v in sorted(iteritems(self.id_to_path_mapping)):\n        writer.writerow([k, v])\n    cmd = ['gsutil', 'cp', self.local_id_to_path_mapping_file,\n           os.path.join(self.target_dir, 'id_to_path_mapping.csv')]\n    if subprocess.call(cmd) != 0:\n      logging.error('Can\\'t copy id_to_path_mapping.csv to target directory')", "output": "Saves mapping from submission IDs to original filenames.\n\n    This mapping is saved as CSV file into target directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_previous(self):\r\n        \"\"\"\"\"\"\r\n        state = self.find(changed=False, forward=False, rehighlight=False,\r\n                          multiline_replace_check=False)\r\n        self.editor.setFocus()\r\n        return state", "output": "Find previous occurrence", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_repo_file(filename):\n    '''\n    \n    '''\n    parsed = configparser.ConfigParser()\n    config = {}\n\n    try:\n        parsed.read(filename)\n    except configparser.MissingSectionHeaderError as err:\n        log.error(\n            'Failed to parse file %s, error: %s',\n            filename, err.message\n        )\n        return ('', {})\n\n    for section in parsed._sections:\n        section_dict = dict(parsed._sections[section])\n        section_dict.pop('__name__', None)\n        config[section] = section_dict\n\n    # Try to extract header comments, as well as comments for each repo. Read\n    # from the beginning of the file and assume any leading comments are\n    # header comments. Continue to read each section header and then find the\n    # comments for each repo.\n    headers = ''\n    section = None\n    with salt.utils.files.fopen(filename, 'r') as repofile:\n        for line in repofile:\n            line = salt.utils.stringutils.to_unicode(line)\n            line = line.strip()\n            if line.startswith('#'):\n                if section is None:\n                    headers += line + '\\n'\n                else:\n                    try:\n                        comments = config[section].setdefault('comments', [])\n                        comments.append(line[1:].lstrip())\n                    except KeyError:\n                        log.debug(\n                            'Found comment in %s which does not appear to '\n                            'belong to any repo section: %s', filename, line\n                        )\n            elif line.startswith('[') and line.endswith(']'):\n                section = line[1:-1]\n\n    return (headers, salt.utils.data.decode(config))", "output": "Turn a single repo file into a dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_conflicting_arguments(self):\n        \"\"\"\"\"\"\n        group = self._parser.add_mutually_exclusive_group()\n        group.add_argument(\n            '-y', '--yes', '--yeah',\n            action='store_true',\n            help='execute fixed command without confirmation')\n        group.add_argument(\n            '-r', '--repeat',\n            action='store_true',\n            help='repeat on failure')", "output": "It's too dangerous to use `-y` and `-r` together.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sell_holding_pnl(self):\n        \"\"\"\n        \n        \"\"\"\n        return (self.sell_avg_holding_price - self.last_price) * self.sell_quantity * self.contract_multiplier", "output": "[float] \u5356\u65b9\u5411\u5f53\u65e5\u6301\u4ed3\u76c8\u4e8f", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def binop_return_dtype(op, left, right):\n    \"\"\"\n    \n    \"\"\"\n    if is_comparison(op):\n        if left != right:\n            raise TypeError(\n                \"Don't know how to compute {left} {op} {right}.\\n\"\n                \"Comparisons are only supported between Factors of equal \"\n                \"dtypes.\".format(left=left, op=op, right=right)\n            )\n        return bool_dtype\n\n    elif left != float64_dtype or right != float64_dtype:\n        raise TypeError(\n            \"Don't know how to compute {left} {op} {right}.\\n\"\n            \"Arithmetic operators are only supported between Factors of \"\n            \"dtype 'float64'.\".format(\n                left=left.name,\n                op=op,\n                right=right.name,\n            )\n        )\n    return float64_dtype", "output": "Compute the expected return dtype for the given binary operator.\n\n    Parameters\n    ----------\n    op : str\n        Operator symbol, (e.g. '+', '-', ...).\n    left : numpy.dtype\n        Dtype of left hand side.\n    right : numpy.dtype\n        Dtype of right hand side.\n\n    Returns\n    -------\n    outdtype : numpy.dtype\n        The dtype of the result of `left <op> right`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_load_paths(load_path):\n    '''\n    \n    '''\n    if load_path is None or not isinstance(load_path, six.string_types):\n        return None\n\n    _paths = []\n\n    for _path in load_path.split(':'):\n        if os.path.isabs(_path) and os.path.isdir(_path):\n            _paths.append(_path)\n        else:\n            log.info('Invalid augeas_cfg load_path entry: %s removed', _path)\n\n    if not _paths:\n        return None\n\n    return ':'.join(_paths)", "output": "Checks the validity of the load_path, returns a sanitized version\n    with invalid paths removed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_fifo(name):\n    '''\n    \n    '''\n    name = os.path.expanduser(name)\n\n    stat_structure = None\n    try:\n        stat_structure = os.stat(name)\n    except OSError as exc:\n        if exc.errno == errno.ENOENT:\n            # If the fifo does not exist in the first place\n            return False\n        else:\n            raise\n    return stat.S_ISFIFO(stat_structure.st_mode)", "output": "Check if a file exists and is a FIFO.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n       salt '*' file.is_fifo /dev/fifo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_shell_command(cmdstr, **subprocess_kwargs):\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    if 'shell' in subprocess_kwargs and not subprocess_kwargs['shell']:\r\n        raise ProgramError(\r\n                'The \"shell\" kwarg may be omitted, but if '\r\n                'provided it must be True.')\r\n    else:\r\n        subprocess_kwargs['shell'] = True\r\n\r\n    if 'executable' not in subprocess_kwargs:\r\n        subprocess_kwargs['executable'] = os.getenv('SHELL')\r\n\r\n    for stream in ['stdin', 'stdout', 'stderr']:\r\n        subprocess_kwargs.setdefault(stream, subprocess.PIPE)\r\n    subprocess_kwargs = alter_subprocess_kwargs_by_platform(\r\n            **subprocess_kwargs)\r\n    return subprocess.Popen(cmdstr, **subprocess_kwargs)", "output": "Execute the given shell command.\r\n    \r\n    Note that *args and **kwargs will be passed to the subprocess call.\r\n\r\n    If 'shell' is given in subprocess_kwargs it must be True,\r\n    otherwise ProgramError will be raised.\r\n    .\r\n    If 'executable' is not given in subprocess_kwargs, it will\r\n    be set to the value of the SHELL environment variable.\r\n\r\n    Note that stdin, stdout and stderr will be set by default\r\n    to PIPE unless specified in subprocess_kwargs.\r\n\r\n    :str cmdstr: The string run as a shell command.\r\n    :subprocess_kwargs: These will be passed to subprocess.Popen.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def asDict( self ):\n        \"\"\"\n        \n        \"\"\"\n        if PY_3:\n            item_fn = self.items\n        else:\n            item_fn = self.iteritems\n\n        def toItem(obj):\n            if isinstance(obj, ParseResults):\n                if obj.haskeys():\n                    return obj.asDict()\n                else:\n                    return [toItem(v) for v in obj]\n            else:\n                return obj\n\n        return dict((k,toItem(v)) for k,v in item_fn())", "output": "Returns the named parse results as a nested dictionary.\n\n        Example::\n\n            integer = Word(nums)\n            date_str = integer(\"year\") + '/' + integer(\"month\") + '/' + integer(\"day\")\n\n            result = date_str.parseString('12/31/1999')\n            print(type(result), repr(result)) # -> <class 'pyparsing.ParseResults'> (['12', '/', '31', '/', '1999'], {'day': [('1999', 4)], 'year': [('12', 0)], 'month': [('31', 2)]})\n\n            result_dict = result.asDict()\n            print(type(result_dict), repr(result_dict)) # -> <class 'dict'> {'day': '1999', 'year': '12', 'month': '31'}\n\n            # even though a ParseResults supports dict-like access, sometime you just need to have a dict\n            import json\n            print(json.dumps(result)) # -> Exception: TypeError: ... is not JSON serializable\n            print(json.dumps(result.asDict())) # -> {\"month\": \"31\", \"day\": \"1999\", \"year\": \"12\"}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encodeSpecialChars(self, input):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlEncodeSpecialChars(self._o, input)\n        return ret", "output": "Do a global encoding of a string, replacing the predefined\n          entities this routine is reentrant, and result must be\n           deallocated.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sync_auth(saltenv='base', extmod_whitelist=None, extmod_blacklist=None):\n    '''\n    \n    '''\n    return salt.utils.extmods.sync(__opts__, 'auth', saltenv=saltenv, extmod_whitelist=extmod_whitelist,\n                                   extmod_blacklist=extmod_blacklist)[0]", "output": "Sync execution modules from ``salt://_auth`` to the master\n\n    saltenv : base\n        The fileserver environment from which to sync. To sync from more than\n        one environment, pass a comma-separated list.\n\n    extmod_whitelist : None\n        comma-separated list of modules to sync\n\n    extmod_blacklist : None\n        comma-separated list of modules to blacklist based on type\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run saltutil.sync_auth", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_subnet(self, network, cidr, name=None, ip_version=4):\n        '''\n        \n        '''\n        net_id = self._find_network_id(network)\n        body = {'cidr': cidr,\n                'ip_version': ip_version,\n                'network_id': net_id,\n                'name': name}\n        return self.network_conn.create_subnet(body={'subnet': body})", "output": "Creates a new subnet", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_reg():\n    '''\n    \n    '''\n    reg_dir = _reg_dir()\n    regfile = os.path.join(reg_dir, 'register')\n    try:\n        with salt.utils.files.fopen(regfile, 'r') as fh_:\n            return salt.utils.msgpack.load(fh_)\n    except Exception:\n        log.error('Could not write to msgpack file %s', __opts__['outdir'])\n        raise", "output": "Load the register from msgpack files", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_principals():\n    '''\n    \n    '''\n    ret = {}\n\n    cmd = __execute_kadmin('list_principals')\n\n    if cmd['retcode'] != 0 or cmd['stderr']:\n        ret['comment'] = cmd['stderr'].splitlines()[-1]\n        ret['result'] = False\n\n        return ret\n\n    ret = {'principals': []}\n\n    for i in cmd['stdout'].splitlines()[1:]:\n        ret['principals'].append(i)\n\n    return ret", "output": "Get all principals\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'kde.example.com' kerberos.list_principals", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_network(self, machines, local_listen_port=12400,\n                    listen_time_out=120, num_machines=1):\n        \"\"\"\n        \"\"\"\n        _safe_call(_LIB.LGBM_NetworkInit(c_str(machines),\n                                         ctypes.c_int(local_listen_port),\n                                         ctypes.c_int(listen_time_out),\n                                         ctypes.c_int(num_machines)))\n        self.network = True\n        return self", "output": "Set the network configuration.\n\n        Parameters\n        ----------\n        machines : list, set or string\n            Names of machines.\n        local_listen_port : int, optional (default=12400)\n            TCP listen port for local machines.\n        listen_time_out : int, optional (default=120)\n            Socket time-out in minutes.\n        num_machines : int, optional (default=1)\n            The number of machines for parallel learning application.\n\n        Returns\n        -------\n        self : Booster\n            Booster with set network.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def use_np_compat(func):\n    \"\"\"\n    \"\"\"\n    @wraps(func)\n    def _with_np_compat(*args, **kwargs):\n        with np_compat(active=True):\n            return func(*args, **kwargs)\n\n    return _with_np_compat", "output": "Wraps a function with an activated NumPy-compatibility scope. This ensures\n    that the execution of the function is guaranteed with NumPy compatible semantics,\n    such as zero-dim and zero size tensors.\n\n    Example::\n        import mxnet as mx\n        @mx.use_np_compat\n        def scalar_one():\n            return mx.nd.ones(())\n        print(scalar_one())\n\n    Parameters\n    ----------\n    func : a user-provided callable function to be scoped by the NumPy compatibility state.\n\n    Returns\n    -------\n    Function\n        A function for wrapping the user functions in the NumPy compatibility scope.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_in_virtualenv():\n    \"\"\"\n    \n    \"\"\"\n\n    pipenv_active = os.environ.get(\"PIPENV_ACTIVE\", False)\n    virtual_env = None\n    use_system = False\n    ignore_virtualenvs = bool(os.environ.get(\"PIPENV_IGNORE_VIRTUALENVS\", False))\n\n    if not pipenv_active and not ignore_virtualenvs:\n        virtual_env = os.environ.get(\"VIRTUAL_ENV\")\n        use_system = bool(virtual_env)\n    return (use_system or virtual_env) and not (pipenv_active or ignore_virtualenvs)", "output": "Check virtualenv membership dynamically\n\n    :return: True or false depending on whether we are in a regular virtualenv or not\n    :rtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        ''''''\n        logger.info(\"scheduler starting...\")\n\n        while not self._quit:\n            try:\n                time.sleep(self.LOOP_INTERVAL)\n                self.run_once()\n                self._exceptions = 0\n            except KeyboardInterrupt:\n                break\n            except Exception as e:\n                logger.exception(e)\n                self._exceptions += 1\n                if self._exceptions > self.EXCEPTION_LIMIT:\n                    break\n                continue\n\n        logger.info(\"scheduler exiting...\")\n        self._dump_cnt()", "output": "Start scheduler loop", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_all_dependencies(self):\n        '''\n        \n        '''\n        ret = {}\n        for model, schema in six.iteritems(self._models()):\n            dep_list = self._build_dependent_model_list(schema)\n            ret[model] = dep_list\n        return ret", "output": "Helper function to build a map of model to their list of model reference dependencies", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_join(eval_ctx, value, d=u'', attribute=None):\n    \"\"\"\n    \"\"\"\n    if attribute is not None:\n        value = imap(make_attrgetter(eval_ctx.environment, attribute), value)\n\n    # no automatic escaping?  joining is a lot eaiser then\n    if not eval_ctx.autoescape:\n        return text_type(d).join(imap(text_type, value))\n\n    # if the delimiter doesn't have an html representation we check\n    # if any of the items has.  If yes we do a coercion to Markup\n    if not hasattr(d, '__html__'):\n        value = list(value)\n        do_escape = False\n        for idx, item in enumerate(value):\n            if hasattr(item, '__html__'):\n                do_escape = True\n            else:\n                value[idx] = text_type(item)\n        if do_escape:\n            d = escape(d)\n        else:\n            d = text_type(d)\n        return d.join(value)\n\n    # no html involved, to normal joining\n    return soft_unicode(d).join(imap(soft_unicode, value))", "output": "Return a string which is the concatenation of the strings in the\n    sequence. The separator between elements is an empty string per\n    default, you can define it with the optional parameter:\n\n    .. sourcecode:: jinja\n\n        {{ [1, 2, 3]|join('|') }}\n            -> 1|2|3\n\n        {{ [1, 2, 3]|join }}\n            -> 123\n\n    It is also possible to join certain attributes of an object:\n\n    .. sourcecode:: jinja\n\n        {{ users|join(', ', attribute='username') }}\n\n    .. versionadded:: 2.6\n       The `attribute` parameter was added.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def listen(self, port: int, address: str = \"\", **kwargs: Any) -> HTTPServer:\n        \"\"\"\n        \"\"\"\n        server = HTTPServer(self, **kwargs)\n        server.listen(port, address)\n        return server", "output": "Starts an HTTP server for this application on the given port.\n\n        This is a convenience alias for creating an `.HTTPServer`\n        object and calling its listen method.  Keyword arguments not\n        supported by `HTTPServer.listen <.TCPServer.listen>` are passed to the\n        `.HTTPServer` constructor.  For advanced uses\n        (e.g. multi-process mode), do not use this method; create an\n        `.HTTPServer` and call its\n        `.TCPServer.bind`/`.TCPServer.start` methods directly.\n\n        Note that after calling this method you still need to call\n        ``IOLoop.current().start()`` to start the server.\n\n        Returns the `.HTTPServer` object.\n\n        .. versionchanged:: 4.3\n           Now returns the `.HTTPServer` object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def as_flat_dict(self):\n        \"\"\"\n        \n        \"\"\"\n        flat_params = {}\n        def recurse(parameters, path):\n            for key, value in parameters.items():\n                newpath = path + [key]\n                if isinstance(value, dict):\n                    recurse(value, newpath)\n                else:\n                    flat_params['.'.join(newpath)] = value\n\n        recurse(self.params, [])\n        return flat_params", "output": "Returns the parameters of a flat dictionary from keys to values.\n        Nested structure is collapsed with periods.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def changed(self, selection='all'):\n        '''\n        \n        '''\n        changed = []\n        if selection == 'all':\n            for recursive_item in self._get_recursive_difference(type='all'):\n                # We want the unset values as well\n                recursive_item.ignore_unset_values = False\n                key_val = six.text_type(recursive_item.past_dict[self._key]) \\\n                        if self._key in recursive_item.past_dict \\\n                        else six.text_type(recursive_item.current_dict[self._key])\n\n                for change in recursive_item.changed():\n                    if change != self._key:\n                        changed.append('.'.join([self._key, key_val, change]))\n            return changed\n        elif selection == 'intersect':\n                # We want the unset values as well\n            for recursive_item in self._get_recursive_difference(type='intersect'):\n                recursive_item.ignore_unset_values = False\n                key_val = six.text_type(recursive_item.past_dict[self._key]) \\\n                        if self._key in recursive_item.past_dict \\\n                        else six.text_type(recursive_item.current_dict[self._key])\n\n                for change in recursive_item.changed():\n                    if change != self._key:\n                        changed.append('.'.join([self._key, key_val, change]))\n            return changed", "output": "Returns the list of changed values.\n        The key is added to each item.\n\n        selection\n            Specifies the desired changes.\n            Supported values are\n                ``all`` - all changed items are included in the output\n                ``intersect`` - changed items present in both lists are included", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pad_decr(ids):\n  \"\"\"\"\"\"\n  if len(ids) < 1:\n    return list(ids)\n  if not any(ids):\n    return []  # all padding.\n  idx = -1\n  while not ids[idx]:\n    idx -= 1\n  if idx == -1:\n    ids = ids\n  else:\n    ids = ids[:idx + 1]\n  return [i - 1 for i in ids]", "output": "Strip ID 0 and decrement ids by 1.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mk_token(self, load):\n        '''\n        \n        '''\n        if not self.authenticate_eauth(load):\n            return {}\n\n        if self._allow_custom_expire(load):\n            token_expire = load.pop('token_expire', self.opts['token_expire'])\n        else:\n            _ = load.pop('token_expire', None)\n            token_expire = self.opts['token_expire']\n\n        tdata = {'start': time.time(),\n                 'expire': time.time() + token_expire,\n                 'name': self.load_name(load),\n                 'eauth': load['eauth']}\n\n        if self.opts['keep_acl_in_token']:\n            acl_ret = self.__get_acl(load)\n            tdata['auth_list'] = acl_ret\n\n        groups = self.get_groups(load)\n        if groups:\n            tdata['groups'] = groups\n\n        return self.tokens[\"{0}.mk_token\".format(self.opts['eauth_tokens'])](self.opts, tdata)", "output": "Run time_auth and create a token. Return False or the token", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_example(example_name, environ):\n    \"\"\"\n    \n    \"\"\"\n    mod = EXAMPLE_MODULES[example_name]\n\n    register_calendar(\"YAHOO\", get_calendar(\"NYSE\"), force=True)\n\n    return run_algorithm(\n        initialize=getattr(mod, 'initialize', None),\n        handle_data=getattr(mod, 'handle_data', None),\n        before_trading_start=getattr(mod, 'before_trading_start', None),\n        analyze=getattr(mod, 'analyze', None),\n        bundle='test',\n        environ=environ,\n        # Provide a default capital base, but allow the test to override.\n        **merge({'capital_base': 1e7}, mod._test_args())\n    )", "output": "Run an example module from zipline.examples.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toggle_show_cd_only(self, checked):\r\n        \"\"\"\"\"\"\r\n        self.parent_widget.sig_option_changed.emit('show_cd_only', checked)\r\n        self.show_cd_only = checked\r\n        if checked:\r\n            if self.__last_folder is not None:\r\n                self.set_current_folder(self.__last_folder)\r\n        elif self.__original_root_index is not None:\r\n            self.setRootIndex(self.__original_root_index)", "output": "Toggle show current directory only mode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _decay():\n  \"\"\"\"\"\"\n  costs = []\n  for var in tf.trainable_variables():\n    if var.op.name.find('DW') > 0:\n      costs.append(tf.nn.l2_loss(var))\n  return tf.add_n(costs)", "output": "L2 weight decay loss.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _copy_stream_position(position):\n    \"\"\"\n    \"\"\"\n    if isinstance(position, types.StreamPosition):\n        output = types.StreamPosition()\n        output.CopyFrom(position)\n        return output\n\n    return types.StreamPosition(**position)", "output": "Copy a StreamPosition.\n\n    Args:\n        position (Union[ \\\n            dict, \\\n            ~google.cloud.bigquery_storage_v1beta1.types.StreamPosition \\\n        ]):\n            StreamPostion (or dictionary in StreamPosition format) to copy.\n\n    Returns:\n        ~google.cloud.bigquery_storage_v1beta1.types.StreamPosition:\n            A copy of the input StreamPostion.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clean_old_jobs():\n    '''\n    \n    '''\n    if __opts__.get('keep_jobs', False) and int(__opts__.get('keep_jobs', 0)) > 0:\n        try:\n            with _get_serv() as cur:\n                sql = 'select date_sub(now(), interval {0} hour) as stamp;'.format(__opts__['keep_jobs'])\n                cur.execute(sql)\n                rows = cur.fetchall()\n                stamp = rows[0][0]\n\n            if __opts__.get('archive_jobs', False):\n                _archive_jobs(stamp)\n            else:\n                _purge_jobs(stamp)\n        except MySQLdb.Error as e:\n            log.error('Mysql returner was unable to get timestamp for purge/archive of jobs')\n            log.error(six.text_type(e))\n            raise salt.exceptions.SaltRunnerError(six.text_type(e))", "output": "Called in the master's event loop every loop_interval.  Archives and/or\n    deletes the events and job details from the database.\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def POST(self, **kwargs):\n        '''\n        \n        '''\n        return {\n            'return': list(self.exec_lowstate(\n                token=cherrypy.session.get('token')))\n        }", "output": "Send one or more Salt commands in the request body\n\n        .. http:post:: /\n\n            :reqheader X-Auth-Token: |req_token|\n            :reqheader Accept: |req_accept|\n            :reqheader Content-Type: |req_ct|\n\n            :resheader Content-Type: |res_ct|\n\n            :status 200: |200|\n            :status 400: |400|\n            :status 401: |401|\n            :status 406: |406|\n\n            :term:`lowstate` data describing Salt commands must be sent in the\n            request body.\n\n        **Example request:**\n\n        .. code-block:: bash\n\n            curl -sSik https://localhost:8000 \\\\\n                -b ~/cookies.txt \\\\\n                -H \"Accept: application/x-yaml\" \\\\\n                -H \"Content-type: application/json\" \\\\\n                -d '[{\"client\": \"local\", \"tgt\": \"*\", \"fun\": \"test.ping\"}]'\n\n        .. code-block:: text\n\n            POST / HTTP/1.1\n            Host: localhost:8000\n            Accept: application/x-yaml\n            X-Auth-Token: d40d1e1e\n            Content-Type: application/json\n\n            [{\"client\": \"local\", \"tgt\": \"*\", \"fun\": \"test.ping\"}]\n\n        **Example response:**\n\n        .. code-block:: text\n\n            HTTP/1.1 200 OK\n            Content-Length: 200\n            Allow: GET, HEAD, POST\n            Content-Type: application/x-yaml\n\n            return:\n            - ms-0: true\n              ms-1: true\n              ms-2: true\n              ms-3: true\n              ms-4: true", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_sub_prop(container, keys, default=None):\n    \"\"\"\n    \"\"\"\n    sub_val = container\n    for key in keys:\n        if key not in sub_val:\n            return default\n        sub_val = sub_val[key]\n    return sub_val", "output": "Get a nested value from a dictionary.\n\n    This method works like ``dict.get(key)``, but for nested values.\n\n    Arguments:\n        container (dict):\n            A dictionary which may contain other dictionaries as values.\n        keys (iterable):\n            A sequence of keys to attempt to get the value for. Each item in\n            the sequence represents a deeper nesting. The first key is for\n            the top level. If there is a dictionary there, the second key\n            attempts to get the value within that, and so on.\n        default (object):\n            (Optional) Value to returned if any of the keys are not found.\n            Defaults to ``None``.\n\n    Examples:\n        Get a top-level value (equivalent to ``container.get('key')``).\n\n        >>> _get_sub_prop({'key': 'value'}, ['key'])\n        'value'\n\n        Get a top-level value, providing a default (equivalent to\n        ``container.get('key', default='default')``).\n\n        >>> _get_sub_prop({'nothere': 123}, ['key'], default='not found')\n        'not found'\n\n        Get a nested value.\n\n        >>> _get_sub_prop({'key': {'subkey': 'value'}}, ['key', 'subkey'])\n        'value'\n\n    Returns:\n        object: The value if present or the default.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait(self, timeout: Union[float, datetime.timedelta] = None) -> Awaitable[bool]:\n        \"\"\"\n        \"\"\"\n        waiter = Future()  # type: Future[bool]\n        self._waiters.append(waiter)\n        if timeout:\n\n            def on_timeout() -> None:\n                if not waiter.done():\n                    future_set_result_unless_cancelled(waiter, False)\n                self._garbage_collect()\n\n            io_loop = ioloop.IOLoop.current()\n            timeout_handle = io_loop.add_timeout(timeout, on_timeout)\n            waiter.add_done_callback(lambda _: io_loop.remove_timeout(timeout_handle))\n        return waiter", "output": "Wait for `.notify`.\n\n        Returns a `.Future` that resolves ``True`` if the condition is notified,\n        or ``False`` after a timeout.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _residual(x, in_filter, out_filter, stride,\n              activate_before_residual=False):\n  \"\"\"\"\"\"\n  if activate_before_residual:\n    with tf.variable_scope('shared_activation'):\n      x = _batch_norm('init_bn', x)\n      x = _relu(x, 0.1)\n      orig_x = x\n  else:\n    with tf.variable_scope('residual_only_activation'):\n      orig_x = x\n      x = _batch_norm('init_bn', x)\n      x = _relu(x, 0.1)\n\n  with tf.variable_scope('sub1'):\n    x = _conv('conv1', x, 3, in_filter, out_filter, stride)\n\n  with tf.variable_scope('sub2'):\n    x = _batch_norm('bn2', x)\n    x = _relu(x, 0.1)\n    x = _conv('conv2', x, 3, out_filter, out_filter, [1, 1, 1, 1])\n\n  with tf.variable_scope('sub_add'):\n    if in_filter != out_filter:\n      orig_x = tf.nn.avg_pool(orig_x, stride, stride, 'VALID')\n      orig_x = tf.pad(\n          orig_x, [[0, 0], [0, 0],\n                   [0, 0], [(out_filter - in_filter) // 2,\n                            (out_filter - in_filter) // 2]])\n    x += orig_x\n\n  tf.logging.debug('image after unit %s', x.get_shape())\n  return x", "output": "Residual unit with 2 sub layers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def padded_ds(ll_input, size=(250, 300), resize_method=ResizeMethod.CROP, padding_mode='zeros', **kwargs):\n        \"\"\n        return ll_input.transform(tfms=crop_pad(), size=size, resize_method=resize_method, padding_mode=padding_mode)", "output": "For a LabelList `ll_input`, resize each image to `size` using `resize_method` and `padding_mode`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_or_reuse_placeholder(tensor_spec):\n    \"\"\"\n    \n    \"\"\"\n    g = tfv1.get_default_graph()\n    name = tensor_spec.name\n    try:\n        tensor = g.get_tensor_by_name(name + ':0')\n        assert \"Placeholder\" in tensor.op.type, \"Tensor {} exists but is not a placeholder!\".format(name)\n        assert tensor_spec.is_compatible_with(tensor), \\\n            \"Tensor {} exists but is not compatible with the signature!\".format(tensor)\n        return tensor\n    except KeyError:\n        with tfv1.name_scope(None):   # clear any name scope it might get called in\n            ret = tfv1.placeholder(\n                tensor_spec.dtype, shape=tensor_spec.shape, name=tensor_spec.name)\n        return ret", "output": "Build a tf.placeholder from the metadata in the given tensor spec, or return an existing one.\n\n    Args:\n        tensor_spec (tf.TensorSpec):\n\n    Returns:\n        tf.Tensor:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(self, s):\n        \"\"\"\n        \n        \"\"\"\n        return datetime.datetime.strptime(s, self.date_format).date()", "output": "Parses a date string formatted like ``YYYY-MM-DD``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def revoke(self, role):\n        \"\"\"\n        \"\"\"\n        if role in self.roles:\n            self.roles.remove(role)", "output": "Remove a role from the entity.\n\n        :type role: str\n        :param role: The role to remove from the entity.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset(name):\n    '''\n    \n    '''\n    ret = {}\n    client = salt.client.get_local_client(__opts__['conf_file'])\n    data = vm_info(name, quiet=True)\n    if not data:\n        __jid_event__.fire_event({'message': 'Failed to find VM {0} to reset'.format(name)}, 'progress')\n        return 'fail'\n    host = next(six.iterkeys(data))\n    try:\n        cmd_ret = client.cmd_iter(\n                host,\n                'virt.reset',\n                [name],\n                timeout=600)\n        for comp in cmd_ret:\n            ret.update(comp)\n        __jid_event__.fire_event({'message': 'Reset VM {0}'.format(name)}, 'progress')\n    except SaltClientError as client_error:\n        print(client_error)\n    return ret", "output": "Force power down and restart an existing VM", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_entries(\n        self, projects, filter_=\"\", order_by=\"\", page_size=0, page_token=None\n    ):\n        \"\"\"\n        \"\"\"\n        page_iter = self._gapic_api.list_log_entries(\n            [],\n            project_ids=projects,\n            filter_=filter_,\n            order_by=order_by,\n            page_size=page_size,\n        )\n        page_iter.client = self._client\n        page_iter.next_page_token = page_token\n\n        # We attach a mutable loggers dictionary so that as Logger\n        # objects are created by entry_from_resource, they can be\n        # re-used by other log entries from the same logger.\n        loggers = {}\n        page_iter.item_to_value = functools.partial(_item_to_entry, loggers=loggers)\n        return page_iter", "output": "Return a page of log entry resources.\n\n        :type projects: list of strings\n        :param projects: project IDs to include. If not passed,\n                         defaults to the project bound to the API's client.\n\n        :type filter_: str\n        :param filter_:\n            a filter expression. See\n            https://cloud.google.com/logging/docs/view/advanced_filters\n\n        :type order_by: str\n        :param order_by: One of :data:`~google.cloud.logging.ASCENDING`\n                         or :data:`~google.cloud.logging.DESCENDING`.\n\n        :type page_size: int\n        :param page_size: maximum number of entries to return, If not passed,\n                          defaults to a value set by the API.\n\n        :type page_token: str\n        :param page_token: opaque marker for the next \"page\" of entries. If not\n                           passed, the API will return the first page of\n                           entries.\n\n        :rtype: :class:`~google.api_core.page_iterator.Iterator`\n        :returns: Iterator of :class:`~google.cloud.logging.entries._BaseEntry`\n                  accessible to the current API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_policy_update_timer(self, number_experiences: int, mean_return: float):\n        \"\"\"\n        \n        \"\"\"\n        self.last_buffer_length = number_experiences\n        self.last_mean_return = mean_return\n        self.time_policy_update_start = time()", "output": "Inform Metrics class that policy update has started.\n        :int number_experiences: Number of experiences in Buffer at this point.\n        :float mean_return: Return averaged across all cumulative returns since last policy update", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _name_from_project_path(path, project, template):\n    \"\"\"\n    \"\"\"\n    if isinstance(template, str):\n        template = re.compile(template)\n\n    match = template.match(path)\n\n    if not match:\n        raise ValueError(\n            'path \"%s\" did not match expected pattern \"%s\"' % (path, template.pattern)\n        )\n\n    if project is not None:\n        found_project = match.group(\"project\")\n        if found_project != project:\n            raise ValueError(\n                \"Project from client (%s) should agree with \"\n                \"project from resource(%s).\" % (project, found_project)\n            )\n\n    return match.group(\"name\")", "output": "Validate a URI path and get the leaf object's name.\n\n    :type path: str\n    :param path: URI path containing the name.\n\n    :type project: str\n    :param project: (Optional) The project associated with the request. It is\n                    included for validation purposes.  If passed as None,\n                    disables validation.\n\n    :type template: str\n    :param template: Template regex describing the expected form of the path.\n                     The regex must have two named groups, 'project' and\n                     'name'.\n\n    :rtype: str\n    :returns: Name parsed from ``path``.\n    :raises ValueError: if the ``path`` is ill-formed or if the project from\n                        the ``path`` does not agree with the ``project``\n                        passed in.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def output(self):\n        \"\"\"\n        \n        \"\"\"\n        return ElasticsearchTarget(\n            host=self.host,\n            port=self.port,\n            http_auth=self.http_auth,\n            index=self.index,\n            doc_type=self.doc_type,\n            update_id=self.update_id(),\n            marker_index_hist_size=self.marker_index_hist_size,\n            timeout=self.timeout,\n            extra_elasticsearch_args=self.extra_elasticsearch_args\n        )", "output": "Returns a ElasticsearchTarget representing the inserted dataset.\n\n        Normally you don't override this.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read_filepattern(filepattern, max_lines=None, split_on_newlines=True):\n  \"\"\"\n  \"\"\"\n  filenames = sorted(tf.gfile.Glob(filepattern))\n  lines_read = 0\n  for filename in filenames:\n    with tf.gfile.Open(filename) as f:\n      if split_on_newlines:\n        for line in f:\n          yield line.strip()\n          lines_read += 1\n          if max_lines and lines_read >= max_lines:\n            return\n\n      else:\n        if max_lines:\n          doc = []\n          for line in f:\n            doc.append(line)\n            lines_read += 1\n            if max_lines and lines_read >= max_lines:\n              yield \"\".join(doc)\n              return\n          yield \"\".join(doc)\n\n        else:\n          yield f.read()", "output": "Reads files matching a wildcard pattern, yielding the contents.\n\n  Args:\n    filepattern: A wildcard pattern matching one or more files.\n    max_lines: If set, stop reading after reading this many lines.\n    split_on_newlines: A boolean. If true, then split files by lines and strip\n        leading and trailing whitespace from each line. Otherwise, treat each\n        file as a single string.\n\n  Yields:\n    The contents of the files as lines, if split_on_newlines is True, or\n    the entire contents of each file if False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _available_connections(self, key: 'ConnectionKey') -> int:\n        \"\"\"\n        \n        \"\"\"\n\n        if self._limit:\n            # total calc available connections\n            available = self._limit - len(self._acquired)\n\n            # check limit per host\n            if (self._limit_per_host and available > 0 and\n                    key in self._acquired_per_host):\n                acquired = self._acquired_per_host.get(key)\n                assert acquired is not None\n                available = self._limit_per_host - len(acquired)\n\n        elif self._limit_per_host and key in self._acquired_per_host:\n            # check limit per host\n            acquired = self._acquired_per_host.get(key)\n            assert acquired is not None\n            available = self._limit_per_host - len(acquired)\n        else:\n            available = 1\n\n        return available", "output": "Return number of available connections taking into account\n        the limit, limit_per_host and the connection key.\n\n        If it returns less than 1 means that there is no connections\n        availables.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_image(self, key):\r\n        \"\"\"\"\"\"\r\n        data = self.model.get_data()\r\n        return isinstance(data[key], Image)", "output": "Return True if variable is a PIL.Image image", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __dynamic_expected_value(self, y):\n        \"\"\" \n        \"\"\"\n\n        return self.model.predict(self.data, np.ones(self.data.shape[0]) * y, output=self.model_output).mean(0)", "output": "This computes the expected value conditioned on the given label value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _invoke_callbacks(self, *args, **kwargs):\n        \"\"\"\"\"\"\n        for callback in self._done_callbacks:\n            _helpers.safe_invoke_callback(callback, *args, **kwargs)", "output": "Invoke all done callbacks.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def word_error_rate(raw_predictions,\n                    labels,\n                    lookup=None,\n                    weights_fn=common_layers.weights_nonzero):\n  \"\"\"\n  \"\"\"\n\n  def from_tokens(raw, lookup_):\n    gathered = tf.gather(lookup_, tf.cast(raw, tf.int32))\n    joined = tf.regex_replace(tf.reduce_join(gathered, axis=1), b\"<EOS>.*\", b\"\")\n    cleaned = tf.regex_replace(joined, b\"_\", b\" \")\n    tokens = tf.string_split(cleaned, \" \")\n    return tokens\n\n  def from_characters(raw, lookup_):\n    \"\"\"Convert ascii+2 encoded codes to string-tokens.\"\"\"\n    corrected = tf.bitcast(\n        tf.clip_by_value(tf.subtract(raw, 2), 0, 255), tf.uint8)\n\n    gathered = tf.gather(lookup_, tf.cast(corrected, tf.int32))[:, :, 0]\n    joined = tf.reduce_join(gathered, axis=1)\n    cleaned = tf.regex_replace(joined, b\"\\0\", b\"\")\n    tokens = tf.string_split(cleaned, \" \")\n    return tokens\n\n  if lookup is None:\n    lookup = tf.constant([chr(i) for i in range(256)])\n    convert_fn = from_characters\n  else:\n    convert_fn = from_tokens\n\n  if weights_fn is not common_layers.weights_nonzero:\n    raise ValueError(\"Only weights_nonzero can be used for this metric.\")\n\n  with tf.variable_scope(\"word_error_rate\", values=[raw_predictions, labels]):\n\n    raw_predictions = tf.squeeze(\n        tf.argmax(raw_predictions, axis=-1), axis=(2, 3))\n    labels = tf.squeeze(labels, axis=(2, 3))\n\n    reference = convert_fn(labels, lookup)\n    predictions = convert_fn(raw_predictions, lookup)\n\n    distance = tf.reduce_sum(\n        tf.edit_distance(predictions, reference, normalize=False))\n    reference_length = tf.cast(\n        tf.size(reference.values, out_type=tf.int32), dtype=tf.float32)\n\n    return distance / reference_length, reference_length", "output": "Calculate word error rate.\n\n  Args:\n    raw_predictions: The raw predictions.\n    labels: The actual labels.\n    lookup: A tf.constant mapping indices to output tokens.\n    weights_fn: Weighting function.\n\n  Returns:\n    The word error rate.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def execute(self, stmt, *args):\n        \"\"\"\n        \"\"\"\n        with (await self.application.db.cursor()) as cur:\n            await cur.execute(stmt, args)", "output": "Execute a SQL statement.\n\n        Must be called with ``await self.execute(...)``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_font(self):\r\n        \"\"\"\"\"\"\r\n        color_scheme = self.get_color_scheme()\r\n        font = self.get_plugin_font()\r\n        rich_font = self.get_plugin_font(rich_text=True)\r\n\r\n        self.set_plain_text_font(font, color_scheme=color_scheme)\r\n        self.set_rich_text_font(rich_font)", "output": "Update font from Preferences", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dnsdumpster(domain, output_dir):\n    \"\"\"\"\"\"\n    response = requests.Session().get('https://dnsdumpster.com/').text\n    csrf_token = re.search(\n        r\"name='csrfmiddlewaretoken' value='(.*?)'\", response).group(1)\n\n    cookies = {'csrftoken': csrf_token}\n    headers = {'Referer': 'https://dnsdumpster.com/'}\n    data = {'csrfmiddlewaretoken': csrf_token, 'targetip': domain}\n    response = requests.Session().post(\n        'https://dnsdumpster.com/', cookies=cookies, data=data, headers=headers)\n\n    image = requests.get('https://dnsdumpster.com/static/map/%s.png' % domain)\n    if image.status_code == 200:\n        with open('%s/%s.png' % (output_dir, domain), 'wb') as f:\n            f.write(image.content)", "output": "Query dnsdumpster.com.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_vmconfig(vmid, node=None, node_type='openvz'):\n    '''\n    \n    '''\n    if node is None:\n        # We need to figure out which node this VM is on.\n        for host_name, host_details in six.iteritems(avail_locations()):\n            for item in query('get', 'nodes/{0}/{1}'.format(host_name, node_type)):\n                if item['vmid'] == vmid:\n                    node = host_name\n\n    # If we reached this point, we have all the information we need\n    data = query('get', 'nodes/{0}/{1}/{2}/config'.format(node, node_type, vmid))\n\n    return data", "output": "Get VM configuration", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find(self, dtype):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(dtype, str):\n            dtype_type = dtype\n            if not isinstance(dtype, type):\n                dtype_type = type(dtype)\n            if issubclass(dtype_type, ExtensionDtype):\n                return dtype\n\n            return None\n\n        for dtype_type in self.dtypes:\n            try:\n                return dtype_type.construct_from_string(dtype)\n            except TypeError:\n                pass\n\n        return None", "output": "Parameters\n        ----------\n        dtype : PandasExtensionDtype or string\n\n        Returns\n        -------\n        return the first matching dtype, otherwise return None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_avg_gradient(self)->None:\n        \"\"\n        avg_gradient = sum(x.data.mean() for x in self.gradients)/len(self.gradients)\n        self._add_gradient_scalar('avg_gradient', scalar_value=avg_gradient)", "output": "Writes the average of the gradients to Tensorboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _masked_softmax(F, att_score, mask, dtype):\n    \"\"\"\n    \"\"\"\n    if mask is not None:\n        # Fill in the masked scores with a very small value\n        neg = -1e4 if np.dtype(dtype) == np.float16 else -1e18\n        att_score = F.where(mask, att_score, neg * F.ones_like(att_score))\n        att_weights = F.softmax(att_score, axis=-1) * mask\n    else:\n        att_weights = F.softmax(att_score, axis=-1)\n    return att_weights", "output": "Ignore the masked elements when calculating the softmax\n\n    Parameters\n    ----------\n    F : symbol or ndarray\n    att_score : Symborl or NDArray\n        Shape (batch_size, query_length, memory_length)\n    mask : Symbol or NDArray or None\n        Shape (batch_size, query_length, memory_length)\n    Returns\n    -------\n    att_weights : Symborl or NDArray\n        Shape (batch_size, query_length, memory_length)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sparse_clip_norm(parameters, max_norm, norm_type=2) -> float:\n    \"\"\"\n    \"\"\"\n    # pylint: disable=invalid-name,protected-access\n    parameters = list(filter(lambda p: p.grad is not None, parameters))\n    max_norm = float(max_norm)\n    norm_type = float(norm_type)\n    if norm_type == float('inf'):\n        total_norm = max(p.grad.data.abs().max() for p in parameters)\n    else:\n        total_norm = 0\n        for p in parameters:\n            if p.grad.is_sparse:\n                # need to coalesce the repeated indices before finding norm\n                grad = p.grad.data.coalesce()\n                param_norm = grad._values().norm(norm_type)\n            else:\n                param_norm = p.grad.data.norm(norm_type)\n            total_norm += param_norm ** norm_type\n        total_norm = total_norm ** (1. / norm_type)\n    clip_coef = max_norm / (total_norm + 1e-6)\n    if clip_coef < 1:\n        for p in parameters:\n            if p.grad.is_sparse:\n                p.grad.data._values().mul_(clip_coef)\n            else:\n                p.grad.data.mul_(clip_coef)\n    return total_norm", "output": "Clips gradient norm of an iterable of parameters.\n\n    The norm is computed over all gradients together, as if they were\n    concatenated into a single vector. Gradients are modified in-place.\n    Supports sparse gradients.\n\n    Parameters\n    ----------\n    parameters : ``(Iterable[torch.Tensor])``\n        An iterable of Tensors that will have gradients normalized.\n    max_norm : ``float``\n        The max norm of the gradients.\n    norm_type : ``float``\n        The type of the used p-norm. Can be ``'inf'`` for infinity norm.\n\n    Returns\n    -------\n    Total norm of the parameters (viewed as a single vector).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def package_info(self):\n        \"\"\"\n        \"\"\"\n        self.cpp_info.libs = tools.collect_libs(self)\n        self.user_info.flatc = os.path.join(self.package_folder, \"bin\", \"flatc\")", "output": "Collect built libraries names and solve flatc path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_authorizers(self, authorizers):\n        \"\"\"\n        \n        \"\"\"\n        self.security_definitions = self.security_definitions or {}\n\n        for authorizer_name, authorizer in authorizers.items():\n            self.security_definitions[authorizer_name] = authorizer.generate_swagger()", "output": "Add Authorizer definitions to the securityDefinitions part of Swagger.\n\n        :param list authorizers: List of Authorizer configurations which get translated to securityDefinitions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tf2():\n  \"\"\"\n  \"\"\"\n  # Import the `tf` compat API from this file and check if it's already TF 2.0.\n  if tf.__version__.startswith('2.'):\n    return tf\n  elif hasattr(tf, 'compat') and hasattr(tf.compat, 'v2'):\n    # As a fallback, try `tensorflow.compat.v2` if it's defined.\n    return tf.compat.v2\n  raise ImportError('cannot import tensorflow 2.0 API')", "output": "Provide the root module of a TF-2.0 API for use within TensorBoard.\n\n  Returns:\n    The root module of a TF-2.0 API, if available.\n\n  Raises:\n    ImportError: if a TF-2.0 API is not available.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def groupBy(self, *cols):\n        \"\"\"\n        \"\"\"\n        jgd = self._jdf.groupBy(self._jcols(*cols))\n        from pyspark.sql.group import GroupedData\n        return GroupedData(jgd, self)", "output": "Groups the :class:`DataFrame` using the specified columns,\n        so we can run aggregation on them. See :class:`GroupedData`\n        for all the available aggregate functions.\n\n        :func:`groupby` is an alias for :func:`groupBy`.\n\n        :param cols: list of columns to group by.\n            Each element should be a column name (string) or an expression (:class:`Column`).\n\n        >>> df.groupBy().avg().collect()\n        [Row(avg(age)=3.5)]\n        >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n        [Row(name=u'Alice', avg(age)=2.0), Row(name=u'Bob', avg(age)=5.0)]\n        >>> sorted(df.groupBy(df.name).avg().collect())\n        [Row(name=u'Alice', avg(age)=2.0), Row(name=u'Bob', avg(age)=5.0)]\n        >>> sorted(df.groupBy(['name', df.age]).count().collect())\n        [Row(name=u'Alice', age=2, count=1), Row(name=u'Bob', age=5, count=1)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def daily_cash(self):\n        ''\n        res = self.cash_table.drop_duplicates(subset='date', keep='last')\n        le=pd.DataFrame(pd.Series(data=None, index=pd.to_datetime(self.trade_range_max).set_names('date'), name='predrop'))\n        ri=res.set_index('date')\n        res_=pd.merge(le,ri,how='left',left_index=True,right_index=True)\n        res_=res_.ffill().fillna(self.init_cash).drop(['predrop','datetime','account_cookie'], axis=1).reset_index().set_index(['date'],drop=False).sort_index()        \n        res_=res_[res_.index.isin(self.trade_range)]\n        return res_", "output": "\u6bcf\u65e5\u4ea4\u6613\u7ed3\u7b97\u65f6\u7684\u73b0\u91d1\u8868", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _align_method_SERIES(left, right, align_asobject=False):\n    \"\"\"  \"\"\"\n\n    # ToDo: Different from _align_method_FRAME, list, tuple and ndarray\n    # are not coerced here\n    # because Series has inconsistencies described in #13637\n\n    if isinstance(right, ABCSeries):\n        # avoid repeated alignment\n        if not left.index.equals(right.index):\n\n            if align_asobject:\n                # to keep original value's dtype for bool ops\n                left = left.astype(object)\n                right = right.astype(object)\n\n            left, right = left.align(right, copy=False)\n\n    return left, right", "output": "align lhs and rhs Series", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cocktail_shaker_sort(arr):\n    \"\"\"\n    \n    \"\"\"\n\n    def swap(i, j):\n        arr[i], arr[j] = arr[j], arr[i]\n\n    n = len(arr)\n    swapped = True\n    while swapped:\n        swapped = False\n        for i in range(1, n):\n            if arr[i - 1] > arr[i]:\n                swap(i - 1, i)\n                swapped = True\n        if swapped == False:\n            return arr\n        swapped = False\n        for i in range(n-1,0,-1):\n            if arr[i - 1] > arr[i]:\n                swap(i - 1, i)\n                swapped = True\n    return arr", "output": "Cocktail_shaker_sort\n    Sorting a given array\n    mutation of bubble sort\n\n    reference: https://en.wikipedia.org/wiki/Cocktail_shaker_sort\n    \n    Worst-case performance: O(N^2)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_from_datastore(self):\n    \"\"\"\"\"\"\n    self._data = {}\n    for entity in self._datastore_client.query_fetch(\n        kind=self._entity_kind_batches):\n      batch_id = entity.key.flat_path[-1]\n      self._data[batch_id] = dict(entity)\n      self._data[batch_id]['images'] = {}\n    for entity in self._datastore_client.query_fetch(\n        kind=self._entity_kind_images):\n      batch_id = entity.key.flat_path[-3]\n      image_id = entity.key.flat_path[-1]\n      self._data[batch_id]['images'][image_id] = dict(entity)", "output": "Initializes batches by reading from the datastore.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clean_en_xx_pairs(en_xx_pairs):\n  \"\"\"\n  \"\"\"\n  for s1, s2 in en_xx_pairs:\n    if _regex_filter(s1):\n      continue\n    s1_list, s2_list = _split_sentences(s1, s2)\n    if len(s1_list) != len(s2_list):\n      continue  # discard this pair\n    elif len(s1_list) == 1:\n      yield s1, s2\n    else:\n      for s1_subsentence, s2_subsentence in itertools.izip(s1_list, s2_list):\n        if _regex_filter(s1_subsentence):\n          continue\n        yield s1_subsentence, s2_subsentence", "output": "Generates a cleaned-up stream of (English, other) translation pairs.\n\n  Cleaning includes both filtering and simplistic sentence splitting, with\n  minimal assumptions on the non-English pair member: (1) All filtering is\n  done based on the English member of the pair, and (2) sentence splitting\n  assumes only that sentences can end with one of '.!?' and begin with an\n  ASCII uppercase letter. Input pairs that would get split into different\n  numbers of sentences (e.g., three English sentences vs. two German ones) are\n  discarded.\n\n  Args:\n    en_xx_pairs: A stream (iterable) of Unicode string pairs. Each item in the\n        stream should be a (sentence_en, sentence_xx) pair.\n  Yields:\n    Cleaned-up (sentence_en, sentence_xx) pairs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, resource):\n        \"\"\"\n        \"\"\"\n        if (\n            \"datasetReference\" not in resource\n            or \"datasetId\" not in resource[\"datasetReference\"]\n        ):\n            raise KeyError(\n                \"Resource lacks required identity information:\"\n                '[\"datasetReference\"][\"datasetId\"]'\n            )\n        project_id = resource[\"datasetReference\"][\"projectId\"]\n        dataset_id = resource[\"datasetReference\"][\"datasetId\"]\n        dataset = cls(DatasetReference(project_id, dataset_id))\n        dataset._properties = copy.deepcopy(resource)\n        return dataset", "output": "Factory: construct a dataset given its API representation\n\n        Args:\n            resource (Dict[str: object]):\n                Dataset resource representation returned from the API\n\n        Returns:\n            google.cloud.bigquery.dataset.Dataset:\n                Dataset parsed from ``resource``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def coords2px(y, x):\n    \"\"\" \n    \"\"\"\n    rows = np.rint([y[0], y[0], y[2], y[2]]).astype(int)\n    cols = np.rint([y[1], y[3], y[1], y[3]]).astype(int)\n    r,c,*_ = x.shape\n    Y = np.zeros((r, c))\n    Y[rows, cols] = 1\n    return Y", "output": "Transforming coordinates to pixels.\n\n    Arguments:\n        y : np array\n            vector in which (y[0], y[1]) and (y[2], y[3]) are the\n            the corners of a bounding box.\n        x : image\n            an image\n    Returns:\n        Y : image\n            of shape x.shape", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_lambda_function(self, function_name):\n        \"\"\"\n        \n        \"\"\"\n        response = self.lambda_client.get_function(\n                FunctionName=function_name)\n        return response['Configuration']['FunctionArn']", "output": "Returns the lambda function ARN, given a name\n\n        This requires the \"lambda:GetFunction\" role.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def restore_expanded_state(self):\r\n        \"\"\"\"\"\"\r\n        if self.__expanded_state is not None:\r\n            # In the old project explorer, the expanded state was a dictionnary:\r\n            if isinstance(self.__expanded_state, list):\r\n                self.fsmodel.directoryLoaded.connect(\r\n                                                  self.restore_directory_state)\r\n                self.fsmodel.directoryLoaded.connect(\r\n                                                self.follow_directories_loaded)", "output": "Restore all items expanded state", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_index(in_x, parameter):\n    \"\"\"\n    \n    \"\"\"\n    if TYPE not in in_x: # if at the top level\n        out_y = dict()\n        for key, value in parameter.items():\n            out_y[key] = _add_index(in_x[key], value)\n        return out_y\n    elif isinstance(in_x, dict):\n        value_type = in_x[TYPE]\n        value_format = in_x[VALUE]\n        if value_type == \"choice\":\n            choice_name = parameter[0] if isinstance(parameter, list) else parameter\n            for pos, item in enumerate(value_format): # here value_format is a list\n                if isinstance(item, list): # this format is [\"choice_key\", format_dict]\n                    choice_key = item[0]\n                    choice_value_format = item[1]\n                    if choice_key == choice_name:\n                        return {INDEX: pos, VALUE: [choice_name, _add_index(choice_value_format, parameter[1])]}\n                elif choice_name == item:\n                    return {INDEX: pos, VALUE: item}\n        else:\n            return parameter", "output": "change parameters in NNI format to parameters in hyperopt format(This function also support nested dict.).\n    For example, receive parameters like:\n        {'dropout_rate': 0.8, 'conv_size': 3, 'hidden_size': 512}\n    Will change to format in hyperopt, like:\n        {'dropout_rate': 0.8, 'conv_size': {'_index': 1, '_value': 3}, 'hidden_size': {'_index': 1, '_value': 512}}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imagenet50(display=False, resolution=224):\n    \"\"\" \n    \"\"\"\n\n    prefix = github_data_url + \"imagenet50_\"\n    X = np.load(cache(prefix + \"%sx%s.npy\" % (resolution, resolution))).astype(np.float32)\n    y = np.loadtxt(cache(prefix + \"labels.csv\"))\n    return X, y", "output": "This is a set of 50 images representative of ImageNet images.\n\n    This dataset was collected by randomly finding a working ImageNet link and then pasting the\n    original ImageNet image into Google image search restricted to images licensed for reuse. A\n    similar image (now with rights to reuse) was downloaded as a rough replacment for the original\n    ImageNet image. The point is to have a random sample of ImageNet for use as a background\n    distribution for explaining models trained on ImageNet data.\n\n    Note that because the images are only rough replacements the labels might no longer be correct.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_datastore_api(client):\n    \"\"\"\n    \"\"\"\n    parse_result = six.moves.urllib_parse.urlparse(client._base_url)\n    host = parse_result.netloc\n    if parse_result.scheme == \"https\":\n        channel = make_secure_channel(client._credentials, DEFAULT_USER_AGENT, host)\n    else:\n        channel = insecure_channel(host)\n\n    return datastore_client.DatastoreClient(\n        channel=channel,\n        client_info=client_info.ClientInfo(\n            client_library_version=__version__, gapic_version=__version__\n        ),\n    )", "output": "Create an instance of the GAPIC Datastore API.\n\n    :type client: :class:`~google.cloud.datastore.client.Client`\n    :param client: The client that holds configuration details.\n\n    :rtype: :class:`.datastore.v1.datastore_client.DatastoreClient`\n    :returns: A datastore API instance with the proper credentials.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close(self):\n    \"\"\"\"\"\"\n    if not self._closed:\n      self._event_writer.close()\n      self._closed = True\n      del self._event_writer", "output": "Close SummaryWriter. Final!", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait_for_crm_operation(operation):\n    \"\"\"\"\"\"\n    logger.info(\"wait_for_crm_operation: \"\n                \"Waiting for operation {} to finish...\".format(operation))\n\n    for _ in range(MAX_POLLS):\n        result = crm.operations().get(name=operation[\"name\"]).execute()\n        if \"error\" in result:\n            raise Exception(result[\"error\"])\n\n        if \"done\" in result and result[\"done\"]:\n            logger.info(\"wait_for_crm_operation: Operation done.\")\n            break\n\n        time.sleep(POLL_INTERVAL)\n\n    return result", "output": "Poll for cloud resource manager operation until finished.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flatten(l):\n    \"\"\"\n    \n    \"\"\"\n    for el in l:\n        if _iterable_not_string(el):\n            for s in flatten(el):\n                yield s\n        else:\n            yield el", "output": "Flatten an arbitrarily nested sequence.\n\n    Parameters\n    ----------\n    l : sequence\n        The non string sequence to flatten\n\n    Notes\n    -----\n    This doesn't consider strings sequences.\n\n    Returns\n    -------\n    flattened : generator", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def FullyConnected(\n        inputs,\n        units,\n        activation=None,\n        use_bias=True,\n        kernel_initializer=None,\n        bias_initializer=tf.zeros_initializer(),\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None):\n    \"\"\"\n    \n    \"\"\"\n    if kernel_initializer is None:\n        if get_tf_version_tuple() <= (1, 12):\n            kernel_initializer = tf.contrib.layers.variance_scaling_initializer(2.0)\n        else:\n            kernel_initializer = tf.keras.initializers.VarianceScaling(2.0, distribution='untruncated_normal')\n\n    inputs = batch_flatten(inputs)\n    with rename_get_variable({'kernel': 'W', 'bias': 'b'}):\n        layer = tf.layers.Dense(\n            units=units,\n            activation=activation,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            _reuse=tf.get_variable_scope().reuse)\n        ret = layer.apply(inputs, scope=tf.get_variable_scope())\n        ret = tf.identity(ret, name='output')\n\n    ret.variables = VariableHolder(W=layer.kernel)\n    if use_bias:\n        ret.variables.b = layer.bias\n    return ret", "output": "A wrapper around `tf.layers.Dense`.\n    One difference to maintain backward-compatibility:\n    Default weight initializer is variance_scaling_initializer(2.0).\n\n    Variable Names:\n\n    * ``W``: weights of shape [in_dim, out_dim]\n    * ``b``: bias", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_stylesheet(self, widget):\r\n        \"\"\"\"\"\"\r\n        if is_dark_interface():\r\n            css = qdarkstyle.load_stylesheet_from_environment()\r\n            widget.setStyleSheet(css)\r\n            palette = widget.palette()\r\n            background = palette.color(palette.Window).lighter(150).name()\r\n            border = palette.color(palette.Window).lighter(200).name()\r\n            name = widget.__class__.__name__\r\n            widget.setObjectName(name)\r\n            extra_css = '''\r\n                {0}#{0} {{\r\n                    background-color:{1};\r\n                    border: 1px solid {2};\r\n                }}'''.format(name, background, border)\r\n            widget.setStyleSheet(css + extra_css)", "output": "Update the background stylesheet to make it lighter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_bool_data(self, copy=False):\n        \"\"\"\n        \n        \"\"\"\n        self._consolidate_inplace()\n        return self.combine([b for b in self.blocks if b.is_bool], copy)", "output": "Parameters\n        ----------\n        copy : boolean, default False\n            Whether to copy the blocks", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def indent(block, spaces):\n    \"\"\"  \"\"\"\n    new_block = ''\n    for line in block.split('\\n'):\n        new_block += spaces + line + '\\n'\n    return new_block", "output": "indents paragraphs of text for rst formatting", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_properties(self, value):\n        \"\"\"\n        \"\"\"\n        self._label_removals.clear()\n        return super(Bucket, self)._set_properties(value)", "output": "Set the properties for the current object.\n\n        :type value: dict or :class:`google.cloud.storage.batch._FutureDict`\n        :param value: The properties to be set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_listlike_indexer(self, keyarr, kind=None):\n        \"\"\"\n        \n        \"\"\"\n        indexer, keyarr = super()._convert_listlike_indexer(keyarr, kind=kind)\n\n        # are we indexing a specific level\n        if indexer is None and len(keyarr) and not isinstance(keyarr[0],\n                                                              tuple):\n            level = 0\n            _, indexer = self.reindex(keyarr, level=level)\n\n            # take all\n            if indexer is None:\n                indexer = np.arange(len(self))\n\n            check = self.levels[0].get_indexer(keyarr)\n            mask = check == -1\n            if mask.any():\n                raise KeyError('%s not in index' % keyarr[mask])\n\n        return indexer, keyarr", "output": "Parameters\n        ----------\n        keyarr : list-like\n            Indexer to convert.\n\n        Returns\n        -------\n        tuple (indexer, keyarr)\n            indexer is an ndarray or None if cannot convert\n            keyarr are tuple-safe keys", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_single_batch_images_to_datastore(self, batch_id):\n    \"\"\"\"\"\"\n    client = self._datastore_client\n    with client.no_transact_batch() as client_batch:\n      self._write_single_batch_images_internal(batch_id, client_batch)", "output": "Writes only images from one batch to the datastore.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def current_snapshot(id_or_symbol):\n    \"\"\"\n    \n    \"\"\"\n    env = Environment.get_instance()\n    frequency = env.config.base.frequency\n    order_book_id = assure_order_book_id(id_or_symbol)\n\n    dt = env.calendar_dt\n\n    if env.config.base.run_type == RUN_TYPE.BACKTEST:\n        if ExecutionContext.phase() == EXECUTION_PHASE.BEFORE_TRADING:\n            dt = env.data_proxy.get_previous_trading_date(env.trading_dt.date())\n            return env.data_proxy.current_snapshot(order_book_id, \"1d\", dt)\n        elif ExecutionContext.phase() == EXECUTION_PHASE.AFTER_TRADING:\n            return env.data_proxy.current_snapshot(order_book_id, \"1d\", dt)\n\n    # PT\u3001\u5b9e\u76d8\u76f4\u63a5\u53d6\u6700\u65b0\u5feb\u7167\uff0c\u5ffd\u7565 frequency, dt \u53c2\u6570\n    return env.data_proxy.current_snapshot(order_book_id, frequency, dt)", "output": "\u83b7\u5f97\u5f53\u524d\u5e02\u573a\u5feb\u7167\u6570\u636e\u3002\u53ea\u80fd\u5728\u65e5\u5185\u4ea4\u6613\u9636\u6bb5\u8c03\u7528\uff0c\u83b7\u53d6\u5f53\u65e5\u8c03\u7528\u65f6\u70b9\u7684\u5e02\u573a\u5feb\u7167\u6570\u636e\u3002\n    \u5e02\u573a\u5feb\u7167\u6570\u636e\u8bb0\u5f55\u4e86\u6bcf\u65e5\u4ece\u5f00\u76d8\u5230\u5f53\u524d\u7684\u6570\u636e\u4fe1\u606f\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u52a8\u6001\u7684day bar\u6570\u636e\u3002\n    \u5728\u76ee\u524d\u5206\u949f\u56de\u6d4b\u4e2d\uff0c\u5feb\u7167\u6570\u636e\u4e3a\u5f53\u65e5\u6240\u6709\u5206\u949f\u7ebf\u7d2f\u79ef\u800c\u6210\uff0c\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u6700\u540e\u4e00\u4e2a\u5206\u949f\u7ebf\u83b7\u53d6\u5230\u7684\u5feb\u7167\u6570\u636e\u5e94\u5f53\u4e0e\u5f53\u65e5\u7684\u65e5\u7ebf\u884c\u60c5\u4fdd\u6301\u4e00\u81f4\u3002\n    \u9700\u8981\u6ce8\u610f\uff0c\u5728\u5b9e\u76d8\u6a21\u62df\u4e2d\uff0c\u8be5\u51fd\u6570\u8fd4\u56de\u7684\u662f\u8c03\u7528\u5f53\u65f6\u7684\u5e02\u573a\u5feb\u7167\u60c5\u51b5\uff0c\u6240\u4ee5\u5728\u540c\u4e00\u4e2ahandle_bar\u4e2d\u4e0d\u540c\u65f6\u70b9\u8c03\u7528\u53ef\u80fd\u8fd4\u56de\u7684\u6570\u636e\u4e0d\u540c\u3002\n    \u5982\u679c\u5f53\u65e5\u622a\u6b62\u5230\u8c03\u7528\u65f6\u5019\u5bf9\u5e94\u80a1\u7968\u6ca1\u6709\u4efb\u4f55\u6210\u4ea4\uff0c\u90a3\u4e48snapshot\u4e2d\u7684close, high, low, last\u51e0\u4e2a\u4ef7\u683c\u6c34\u5e73\u90fd\u5c06\u4ee50\u8868\u793a\u3002\n\n    :param str id_or_symbol: \u5408\u7ea6\u4ee3\u7801\u6216\u7b80\u79f0\n\n    :return: :class:`~Snapshot`\n\n    :example:\n\n    \u5728handle_bar\u4e2d\u8c03\u7528\u8be5\u51fd\u6570\uff0c\u5047\u8bbe\u7b56\u7565\u5f53\u524d\u65f6\u95f4\u662f20160104 09:33:\n\n    ..  code-block:: python3\n        :linenos:\n\n        [In]\n        logger.info(current_snapshot('000001.XSHE'))\n        [Out]\n        2016-01-04 09:33:00.00  INFO\n        Snapshot(order_book_id: '000001.XSHE', datetime: datetime.datetime(2016, 1, 4, 9, 33), open: 10.0, high: 10.025, low: 9.9667, last: 9.9917, volume: 2050320, total_turnover: 20485195, prev_close: 9.99)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _objective_decorator(func):\n    \"\"\"\n    \"\"\"\n    def inner(preds, dmatrix):\n        \"\"\"internal function\"\"\"\n        labels = dmatrix.get_label()\n        return func(labels, preds)\n    return inner", "output": "Decorate an objective function\n\n    Converts an objective function using the typical sklearn metrics\n    signature so that it is usable with ``xgboost.training.train``\n\n    Parameters\n    ----------\n    func: callable\n        Expects a callable with signature ``func(y_true, y_pred)``:\n\n        y_true: array_like of shape [n_samples]\n            The target values\n        y_pred: array_like of shape [n_samples]\n            The predicted values\n\n    Returns\n    -------\n    new_func: callable\n        The new objective function as expected by ``xgboost.training.train``.\n        The signature is ``new_func(preds, dmatrix)``:\n\n        preds: array_like, shape [n_samples]\n            The predicted values\n        dmatrix: ``DMatrix``\n            The training set from which the labels will be extracted using\n            ``dmatrix.get_label()``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lpad(col, len, pad):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.lpad(_to_java_column(col), len, pad))", "output": "Left-pad the string column to width `len` with `pad`.\n\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n    >>> df.select(lpad(df.s, 6, '#').alias('s')).collect()\n    [Row(s=u'##abcd')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ensure_term(where, scope_level):\n    \"\"\"\n    \n    \"\"\"\n\n    # only consider list/tuple here as an ndarray is automatically a coordinate\n    # list\n    level = scope_level + 1\n    if isinstance(where, (list, tuple)):\n        wlist = []\n        for w in filter(lambda x: x is not None, where):\n            if not maybe_expression(w):\n                wlist.append(w)\n            else:\n                wlist.append(Term(w, scope_level=level))\n        where = wlist\n    elif maybe_expression(where):\n        where = Term(where, scope_level=level)\n    return where", "output": "ensure that the where is a Term or a list of Term\n    this makes sure that we are capturing the scope of variables\n    that are passed\n    create the terms here with a frame_level=2 (we are 2 levels down)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_xyzs(self, xs, ys, zs, imgsize:int=4, figsize:Optional[Tuple[int,int]]=None, **kwargs):\n        \"\"\n        title = 'Input / Prediction / Target'\n        axs = subplots(len(xs), 3, imgsize=imgsize, figsize=figsize, title=title, weight='bold', size=14)\n        for i,(x,y,z) in enumerate(zip(xs,ys,zs)):\n            x.show(ax=axs[i,0], **kwargs)\n            y.show(ax=axs[i,2], **kwargs)\n            z.show(ax=axs[i,1], **kwargs)", "output": "Show `xs` (inputs), `ys` (targets) and `zs` (predictions) on a figure of `figsize`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def force_delete(func, path, exc_info):\n    \"\"\"\n    \"\"\"\n    os.chmod(path, stat.S_IWRITE)\n    func(path)", "output": "Error handler for `shutil.rmtree()` equivalent to `rm -rf`.\n\n    Usage: `shutil.rmtree(path, onerror=force_delete)`\n    From stackoverflow.com/questions/1889597", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def alignment(layer, decay_ratio=2):\n  \"\"\"\n  \"\"\"\n  def inner(T):\n    batch_n = T(layer).get_shape().as_list()[0]\n    arr = T(layer)\n    accum = 0\n    for d in [1, 2, 3, 4]:\n      for i in range(batch_n - d):\n        a, b = i, i+d\n        arr1, arr2 = arr[a], arr[b]\n        accum += tf.reduce_mean((arr1-arr2)**2) / decay_ratio**float(d)\n    return -accum\n  return inner", "output": "Encourage neighboring images to be similar.\n\n  When visualizing the interpolation between two objectives, it's often\n  desireable to encourage analagous boejcts to be drawn in the same position,\n  to make them more comparable.\n\n  This term penalizes L2 distance between neighboring images, as evaluated at\n  layer.\n\n  In general, we find this most effective if used with a paramaterization that\n  shares across the batch. (In fact, that works quite well by iteself, so this\n  function may just be obselete.)\n\n  Args:\n    layer: layer to penalize at.\n    decay_ratio: how much to decay penalty as images move apart in batch.\n\n  Returns:\n    Objective.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def swap_columns(self, column_name_1, column_name_2, inplace=False):\n        \"\"\"\n        \n        \"\"\"\n        if inplace:\n            self.__is_dirty__ = True\n            with cython_context():\n                if self._is_vertex_frame():\n                    graph_proxy = self.__graph__.__proxy__.swap_vertex_fields(column_name_1, column_name_2)\n                    self.__graph__.__proxy__ = graph_proxy\n                elif self._is_edge_frame():\n                    graph_proxy = self.__graph__.__proxy__.swap_edge_fields(column_name_1, column_name_2)\n                    self.__graph__.__proxy__ = graph_proxy\n            return self\n        else:\n            return super(GFrame, self).swap_columns(column_name_1, column_name_2, inplace=inplace)", "output": "Swaps the columns with the given names.\n\n        If inplace == False (default) this operation does not modify the\n        current SFrame, returning a new SFrame.\n\n        If inplace == True, this operation modifies the current\n        SFrame, returning self.\n\n        Parameters\n        ----------\n        column_name_1 : string\n            Name of column to swap\n\n        column_name_2 : string\n            Name of other column to swap\n\n        inplace : bool, optional. Defaults to False.\n            Whether the SFrame is modified in place.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def treat_machine_dict(machine):\n    '''\n    \n    '''\n    machine.update({\n        'id': machine.get('id', ''),\n        'image': machine.get('image', ''),\n        'size': '{0} MB'.format(machine.get('memorySize', 0)),\n        'state': machine_get_machinestate_str(machine),\n        'private_ips': [],\n        'public_ips': [],\n    })\n\n    # Replaced keys\n    if 'memorySize' in machine:\n        del machine['memorySize']\n    return machine", "output": "Make machine presentable for outside world.\n\n    !!!Modifies the input machine!!!\n\n    @param machine:\n    @type machine: dict\n    @return: the modified input machine\n    @rtype: dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_graph_title(self):\n        \"\"\"\"\"\"\n        start_time = datetime.fromtimestamp(int(self.timestamp_list[0]))\n        end_time = datetime.fromtimestamp(int(self.timestamp_list[-1]))\n        end_time = end_time.strftime('%H:%M:%S')\n        title = \"Timespan: %s \u2014\u2014 %s\" % (start_time, end_time)\n\n        return title", "output": "\u83b7\u53d6\u56fe\u50cf\u7684title.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def voice_channels(self):\n        \"\"\"\n        \"\"\"\n        r = [ch for ch in self._channels.values() if isinstance(ch, VoiceChannel)]\n        r.sort(key=lambda c: (c.position, c.id))\n        return r", "output": "List[:class:`VoiceChannel`]: A list of voice channels that belongs to this guild.\n\n        This is sorted by the position and are in UI order from top to bottom.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_active(self):\n    \"\"\"\"\"\"\n    if self._db_connection_provider:\n      # The plugin is active if one relevant tag can be found in the database.\n      db = self._db_connection_provider()\n      cursor = db.execute(\n          '''\n          SELECT 1\n          FROM Tags\n          WHERE Tags.plugin_name = ?\n          LIMIT 1\n          ''',\n          (metadata.PLUGIN_NAME,))\n      return bool(list(cursor))\n    if not self._multiplexer:\n      return False\n    return bool(self._multiplexer.PluginRunToTagToContent(metadata.PLUGIN_NAME))", "output": "The images plugin is active iff any run has at least one relevant tag.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def minimize(self, loss_fn, x, optim_state):\n    \"\"\"\n    \n    \"\"\"\n    grads = self._compute_gradients(loss_fn, x, optim_state)\n    return self._apply_gradients(grads, x, optim_state)", "output": "Analogous to tf.Optimizer.minimize\n\n    :param loss_fn: tf Tensor, representing the loss to minimize\n    :param x: list of Tensor, analogous to tf.Optimizer's var_list\n    :param optim_state: A possibly nested dict, containing any optimizer state.\n\n    Returns:\n      new_x: list of Tensor, updated version of `x`\n      new_optim_state: dict, updated version of `optim_state`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imagetransformer_sep_channels_8l_8h_local_and_global_att():\n  \"\"\"\"\"\"\n  hparams = imagetransformer_sep_channels_8l_8h()\n  hparams.num_heads = 8\n  hparams.batch_size = 1\n  hparams.attention_key_channels = hparams.attention_value_channels = 0\n  hparams.hidden_size = 256\n  hparams.filter_size = 256\n  hparams.num_hidden_layers = 4\n  hparams.sampling_method = \"random\"\n  hparams.local_and_global_att = True\n  return hparams", "output": "separate rgb embeddings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _has_plotted_object(self, ax):\n        \"\"\"\"\"\"\n        return (len(ax.lines) != 0 or\n                len(ax.artists) != 0 or\n                len(ax.containers) != 0)", "output": "check whether ax has data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_queue(self):\r\n        \"\"\"\"\"\"\r\n        started = 0\r\n        for parent_id, threadlist in list(self.started_threads.items()):\r\n            still_running = []\r\n            for thread in threadlist:\r\n                if thread.isFinished():\r\n                    end_callback = self.end_callbacks.pop(id(thread))\r\n                    if thread.results is not None:\r\n                        #  The thread was executed successfully\r\n                        end_callback(thread.results)\r\n                    thread.setParent(None)\r\n                    thread = None\r\n                else:\r\n                    still_running.append(thread)\r\n                    started += 1\r\n            threadlist = None\r\n            if still_running:\r\n                self.started_threads[parent_id] = still_running\r\n            else:\r\n                self.started_threads.pop(parent_id)\r\n        logger.debug(\"Updating queue:\")\r\n        logger.debug(\"    started: %d\" % started)\r\n        logger.debug(\"    pending: %d\" % len(self.pending_threads))\r\n        if self.pending_threads and started < self.max_simultaneous_threads:\r\n            thread, parent_id = self.pending_threads.pop(0)\r\n            thread.finished.connect(self.update_queue)\r\n            threadlist = self.started_threads.get(parent_id, [])\r\n            self.started_threads[parent_id] = threadlist+[thread]\r\n            logger.debug(\"===>starting: %r\" % thread)\r\n            thread.start()", "output": "Update queue", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_json(event, colored):\n        \"\"\"\n        \n        \"\"\"\n\n        try:\n            if event.message.startswith(\"{\"):\n                msg_dict = json.loads(event.message)\n                event.message = json.dumps(msg_dict, indent=2)\n        except Exception:\n            # Skip if the event message was not JSON\n            pass\n\n        return event", "output": "If the event message is a JSON string, then pretty print the JSON with 2 indents and sort the keys. This makes\n        it very easy to visually parse and search JSON data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_volume(name):\n    '''\n    \n    '''\n    docker_volumes = __salt__['docker.volumes']()['Volumes']\n    if docker_volumes:\n        volumes = [v for v in docker_volumes if v['Name'] == name]\n        if volumes:\n            return volumes[0]\n\n    return None", "output": "Find volume by name on minion", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dig(host):\n    '''\n    \n    '''\n    cmd = ['dig', salt.utils.network.sanitize_host(host)]\n    return __salt__['cmd.run'](cmd, python_shell=False)", "output": "Performs a DNS lookup with dig\n\n    Note: dig must be installed on the Windows minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' network.dig archlinux.org", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_outputter(self, func, mod):\n        '''\n        \n        '''\n        if hasattr(mod, '__outputter__'):\n            outp = mod.__outputter__\n            if func.__name__ in outp:\n                func.__outputter__ = outp[func.__name__]", "output": "Apply the __outputter__ variable to the functions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_dropout(node, **kwargs):\n    \"\"\"\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    probability = float(attrs.get(\"p\", 0.5))\n\n    dropout_node = onnx.helper.make_node(\n        \"Dropout\",\n        input_nodes,\n        [name],\n        ratio=probability,\n        name=name\n    )\n    return [dropout_node]", "output": "Map MXNet's Dropout operator attributes to onnx's Dropout operator\n    and return the created node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def selectShapePoint(self, point):\n        \"\"\"\"\"\"\n        self.deSelectShape()\n        if self.selectedVertex():  # A vertex is marked for selection.\n            index, shape = self.hVertex, self.hShape\n            shape.highlightVertex(index, shape.MOVE_VERTEX)\n            self.selectShape(shape)\n            return\n        for shape in reversed(self.shapes):\n            if self.isVisible(shape) and shape.containsPoint(point):\n                self.selectShape(shape)\n                self.calculateOffsets(shape, point)\n                return", "output": "Select the first shape created which contains this point.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_periodic_pipeline(self, gen):\n        \"\"\"\n        \"\"\"\n        self._update_top_pipeline()\n        if self.periodic_checkpoint_folder is not None:\n            total_since_last_pipeline_save = (datetime.now() - self._last_pipeline_write).total_seconds()\n            if total_since_last_pipeline_save > self._output_best_pipeline_period_seconds:\n                self._last_pipeline_write = datetime.now()\n                self._save_periodic_pipeline(gen)\n\n        if self.early_stop is not None:\n            if self._last_optimized_pareto_front_n_gens >= self.early_stop:\n                raise StopIteration(\"The optimized pipeline was not improved after evaluating {} more generations. \"\n                                    \"Will end the optimization process.\\n\".format(self.early_stop))", "output": "If enough time has passed, save a new optimized pipeline. Currently used in the per generation hook in the optimization loop.\n        Parameters\n        ----------\n        gen: int\n            Generation number\n\n        Returns\n        -------\n        None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _extract_and_flatten_nested_structure(data, flattened=None):\n    \"\"\"\n    \"\"\"\n    if flattened is None:\n        flattened = []\n        structure = _extract_and_flatten_nested_structure(data, flattened)\n        return structure, flattened\n    if isinstance(data, list):\n        return list(_extract_and_flatten_nested_structure(x, flattened) for x in data)\n    elif isinstance(data, tuple):\n        return tuple(_extract_and_flatten_nested_structure(x, flattened) for x in data)\n    elif isinstance(data, dict):\n        return {k: _extract_and_flatten_nested_structure(v) for k, v in data.items()}\n    elif isinstance(data, (mx.sym.Symbol, mx.nd.NDArray)):\n        flattened.append(data)\n        return len(flattened) - 1\n    else:\n        raise NotImplementedError", "output": "Flatten the structure of a nested container to a list.\n\n    Parameters\n    ----------\n    data : A single NDArray/Symbol or nested container with NDArrays/Symbol.\n        The nested container to be flattened.\n    flattened : list or None\n        The container thats holds flattened result.\n    Returns\n    -------\n    structure : An integer or a nested container with integers.\n        The extracted structure of the container of `data`.\n    flattened : (optional) list\n        The container thats holds flattened result.\n        It is returned only when the input argument `flattened` is not given.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def golds_to_gold_tuples(docs, golds):\n    \"\"\"\"\"\"\n    tuples = []\n    for doc, gold in zip(docs, golds):\n        text = doc.text\n        ids, words, tags, heads, labels, iob = zip(*gold.orig_annot)\n        sents = [((ids, words, tags, heads, labels, iob), [])]\n        tuples.append((text, sents))\n    return tuples", "output": "Get out the annoying 'tuples' format used by begin_training, given the\n    GoldParse objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def listFunctions(self, dbName=None):\n        \"\"\"\n        \"\"\"\n        if dbName is None:\n            dbName = self.currentDatabase()\n        iter = self._jcatalog.listFunctions(dbName).toLocalIterator()\n        functions = []\n        while iter.hasNext():\n            jfunction = iter.next()\n            functions.append(Function(\n                name=jfunction.name(),\n                description=jfunction.description(),\n                className=jfunction.className(),\n                isTemporary=jfunction.isTemporary()))\n        return functions", "output": "Returns a list of functions registered in the specified database.\n\n        If no database is specified, the current database is used.\n        This includes all temporary functions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hasProp(self, name):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlHasProp(self._o, name)\n        if ret is None:return None\n        __tmp = xmlAttr(_obj=ret)\n        return __tmp", "output": "Search an attribute associated to a node This function also\n          looks in DTD attribute declaration for #FIXED or default\n           declaration values unless DTD use has been turned off.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rlmb_dqn_base():\n  \"\"\"\"\"\"\n  hparams = _rlmb_base()\n  simulated_rollout_length = 10\n  dqn_params = dict(\n      base_algo=\"dqn\",\n      base_algo_params=\"dqn_original_params\",\n      real_batch_size=1,\n      simulated_batch_size=16,\n      dqn_agent_generates_trainable_dones=False,\n      eval_batch_size=1,\n      # Must be equal to dqn_time_limit for now\n      simulated_rollout_length=simulated_rollout_length,\n      dqn_time_limit=simulated_rollout_length,\n      simulation_flip_first_random_for_beginning=False,\n      dqn_eval_episodes_num=3,\n\n      # TODO(kc): only for model-free compatibility, remove this\n      epochs_num=-1,\n  )\n  update_hparams(hparams, dqn_params)\n  return hparams", "output": "rlmb_dqn_base params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def error_info():\n    \"\"\"\"\"\"\n    worker = global_worker\n    worker.check_connected()\n    return (global_state.error_messages(driver_id=worker.task_driver_id) +\n            global_state.error_messages(driver_id=DriverID.nil()))", "output": "Return information about failed tasks.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def glob(self, filename):\n        \"\"\"\"\"\"\n        if isinstance(filename, six.string_types):\n            return [\n                # Convert the filenames to string from bytes.\n                compat.as_str_any(matching_filename)\n                for matching_filename in py_glob.glob(\n                    compat.as_bytes(filename))\n            ]\n        else:\n            return [\n                # Convert the filenames to string from bytes.\n                compat.as_str_any(matching_filename)\n                for single_filename in filename\n                for matching_filename in py_glob.glob(\n                    compat.as_bytes(single_filename))\n            ]", "output": "Returns a list of files that match the given pattern(s).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_result(self, type, task, result):\n        ''''''\n        status_code = result.get('status_code', 599)\n        if status_code != 599:\n            status_code = (int(status_code) / 100 * 100)\n        self._cnt['5m'].event((task.get('project'), status_code), +1)\n        self._cnt['1h'].event((task.get('project'), status_code), +1)\n\n        if type in ('http', 'phantomjs') and result.get('time'):\n            content_len = len(result.get('content', ''))\n            self._cnt['5m'].event((task.get('project'), 'speed'),\n                                  float(content_len) / result.get('time'))\n            self._cnt['1h'].event((task.get('project'), 'speed'),\n                                  float(content_len) / result.get('time'))\n            self._cnt['5m'].event((task.get('project'), 'time'), result.get('time'))\n            self._cnt['1h'].event((task.get('project'), 'time'), result.get('time'))", "output": "Called after task fetched", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def aligned_8k_grouped():\n  \"\"\"\n  \"\"\"\n  hparams = aligned_grouped()\n  hparams.batch_size = 8192\n  # hparams.attention_image_summary = False\n  hparams.num_groups = 16\n  hparams.multiplicative_overhead = 1.1\n  return hparams", "output": "version for languagemodel_wiki_scramble8k50.\n\n  languagemodel_wiki_scramble1k50, 1gpu, 7k steps: log(ppl)_eval = 2.92\n  3.3 steps/sec on P100\n  8gpu (8x batch), 7k steps: log(ppl)_eval = 2.15\n\n  Returns:\n    a hparams object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def grid_visual(*args, **kwargs):\n  \"\"\"\"\"\"\n  warnings.warn(\"`grid_visual` has moved to `cleverhans.plot.pyplot_image`. \"\n                \"cleverhans.utils.grid_visual may be removed on or after \"\n                \"2019-04-24.\")\n  from cleverhans.plot.pyplot_image import grid_visual as new_grid_visual\n  return new_grid_visual(*args, **kwargs)", "output": "Deprecation wrapper", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def iterdir(self):\n        \"\"\"\n        \"\"\"\n        if self._closed:\n            self._raise_closed()\n        for name in self._accessor.listdir(self):\n            if name in ('.', '..'):\n                # Yielding a path object for these makes little sense\n                continue\n            yield self._make_child_relpath(name)\n            if self._closed:\n                self._raise_closed()", "output": "Iterate over the files in this directory.  Does not yield any\n        result for the special paths '.' and '..'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get(self, field):\n        \"\"\"\n        \n        \"\"\"\n        if field in self._list_fields():\n            return self.__proxy__.get(field)\n        else:\n            raise KeyError('Key \\\"%s\\\" not in model. Available fields are %s.' % (field, ', '.join(self._list_fields())))", "output": "Return the value for the queried field.\n\n        Get the value of a given field. The list of all queryable fields is\n        documented in the beginning of the model class.\n\n        >>> out = m._get('graph')\n\n        Parameters\n        ----------\n        field : string\n            Name of the field to be retrieved.\n\n        Returns\n        -------\n        out : value\n            The current value of the requested field.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def this_week_day(base_date, weekday):\n    \"\"\"\n    \n    \"\"\"\n    day_of_week = base_date.weekday()\n    # If today is Tuesday and the query is `this monday`\n    # We should output the next_week monday\n    if day_of_week > weekday:\n        return next_week_day(base_date, weekday)\n    start_of_this_week = base_date - timedelta(days=day_of_week + 1)\n    day = start_of_this_week + timedelta(days=1)\n    while day.weekday() != weekday:\n        day = day + timedelta(days=1)\n    return day", "output": "Finds coming weekday", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show(commands, raw_text=True, **kwargs):\n    '''\n    \n    '''\n    if not isinstance(raw_text, bool):\n        msg = \"\"\"\n        INPUT ERROR: Second argument 'raw_text' must be either True or False\n        Value passed: {0}\n        Hint: White space separated show commands should be wrapped by double quotes\n        \"\"\".format(raw_text)\n        return msg\n\n    if raw_text:\n        method = 'cli_show_ascii'\n    else:\n        method = 'cli_show'\n\n    response_list = sendline(commands, method, **kwargs)\n    if isinstance(response_list, list):\n        ret = [response for response in response_list if response]\n        if not ret:\n            ret = ['']\n        return ret\n    else:\n        return response_list", "output": "Execute one or more show (non-configuration) commands.\n\n    commands\n        The commands to be executed.\n\n    raw_text: ``True``\n        Whether to return raw text or structured data.\n        NOTE: raw_text option is ignored for SSH proxy minion.  Data is\n        returned unstructured.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call --local nxos.show 'show version'\n        salt '*' nxos.show 'show bgp sessions ; show processes' raw_text=False\n        salt 'regular-minion' nxos.show 'show interfaces' host=sw01.example.com username=test password=test", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def iterallitems(self, key=_absent):\n        '''\n        \n        '''\n        if key is not _absent:\n            # Raises KeyError if <key> is not in self._map.\n            return self.iteritems(key)\n        return self._items.iteritems()", "output": "Example:\n          omd = omdict([(1,1), (1,11), (1,111), (2,2), (3,3)])\n          omd.iterallitems() == (1,1) -> (1,11) -> (1,111) -> (2,2) -> (3,3)\n          omd.iterallitems(1) == (1,1) -> (1,11) -> (1,111)\n\n        Raises: KeyError if <key> is provided and not in the dictionary.\n        Returns: An iterator over every item in the diciontary. If <key> is\n          provided, only items with the key <key> are iterated over.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy_file(self, infile, outfile, check=True):\n        \"\"\"\n        \"\"\"\n        self.ensure_dir(os.path.dirname(outfile))\n        logger.info('Copying %s to %s', infile, outfile)\n        if not self.dry_run:\n            msg = None\n            if check:\n                if os.path.islink(outfile):\n                    msg = '%s is a symlink' % outfile\n                elif os.path.exists(outfile) and not os.path.isfile(outfile):\n                    msg = '%s is a non-regular file' % outfile\n            if msg:\n                raise ValueError(msg + ' which would be overwritten')\n            shutil.copyfile(infile, outfile)\n        self.record_as_written(outfile)", "output": "Copy a file respecting dry-run and force flags.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def del_hparam(self, name):\n    \"\"\"\n    \"\"\"\n    if hasattr(self, name):\n      delattr(self, name)\n      del self._hparam_types[name]", "output": "Removes the hyperparameter with key 'name'.\n\n    Does nothing if it isn't present.\n\n    Args:\n      name: Name of the hyperparameter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(name, runas=None):\n    '''\n    \n    '''\n    return prlctl('delete', salt.utils.data.decode(name), runas=runas)", "output": "Delete a VM\n\n    .. versionadded:: 2016.11.0\n\n    :param str name:\n        Name/ID of VM to clone\n\n    :param str runas:\n        The user that the prlctl command will be run as\n\n    Example:\n\n    .. code-block:: bash\n\n        salt '*' parallels.exec macvm 'find /etc/paths.d' runas=macdev", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def normalize_path(path):\n    # type: (AnyStr) -> AnyStr\n    \"\"\"\n    \n    \"\"\"\n\n    return os.path.normpath(\n        os.path.normcase(\n            os.path.abspath(os.path.expandvars(os.path.expanduser(str(path))))\n        )\n    )", "output": "Return a case-normalized absolute variable-expanded path.\n\n    :param str path: The non-normalized path\n    :return: A normalized, expanded, case-normalized path\n    :rtype: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def translate(self, src_seq, src_valid_length):\n        \"\"\"\n        \"\"\"\n        batch_size = src_seq.shape[0]\n        encoder_outputs, _ = self._model.encode(src_seq, valid_length=src_valid_length)\n        decoder_states = self._model.decoder.init_state_from_encoder(encoder_outputs,\n                                                                     src_valid_length)\n        inputs = mx.nd.full(shape=(batch_size,), ctx=src_seq.context, dtype=np.float32,\n                            val=self._model.tgt_vocab.token_to_idx[self._model.tgt_vocab.bos_token])\n        samples, scores, sample_valid_length = self._sampler(inputs, decoder_states)\n        return samples, scores, sample_valid_length", "output": "Get the translation result given the input sentence.\n\n        Parameters\n        ----------\n        src_seq : mx.nd.NDArray\n            Shape (batch_size, length)\n        src_valid_length : mx.nd.NDArray\n            Shape (batch_size,)\n\n        Returns\n        -------\n        samples : NDArray\n            Samples draw by beam search. Shape (batch_size, beam_size, length). dtype is int32.\n        scores : NDArray\n            Scores of the samples. Shape (batch_size, beam_size). We make sure that scores[i, :] are\n            in descending order.\n        valid_length : NDArray\n            The valid length of the samples. Shape (batch_size, beam_size). dtype will be int32.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_vm_by_id(vmid, allDetails=False):\n    '''\n    \n    '''\n    for vm_name, vm_details in six.iteritems(get_resources_vms(includeConfig=allDetails)):\n        if six.text_type(vm_details['vmid']) == six.text_type(vmid):\n            return vm_details\n\n    log.info('VM with ID \"%s\" could not be found.', vmid)\n    return False", "output": "Retrieve a VM based on the ID.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_interface(iface, iface_type, enabled, **settings):\n    '''\n    \n    '''\n    if __grains__['os'] == 'Fedora':\n        if __grains__['osmajorrelease'] >= 18:\n            rh_major = '7'\n        else:\n            rh_major = '6'\n    else:\n        rh_major = __grains__['osrelease'][:1]\n\n    iface_type = iface_type.lower()\n\n    if iface_type not in _IFACE_TYPES:\n        _raise_error_iface(iface, iface_type, _IFACE_TYPES)\n\n    if iface_type == 'slave':\n        settings['slave'] = 'yes'\n        if 'master' not in settings:\n            msg = 'master is a required setting for slave interfaces'\n            log.error(msg)\n            raise AttributeError(msg)\n\n    if iface_type == 'vlan':\n        settings['vlan'] = 'yes'\n\n    if iface_type == 'bridge':\n        __salt__['pkg.install']('bridge-utils')\n\n    if iface_type in ['eth', 'bond', 'bridge', 'slave', 'vlan', 'ipip', 'ib', 'alias']:\n        opts = _parse_settings_eth(settings, iface_type, enabled, iface)\n        try:\n            template = JINJA.get_template('rh{0}_eth.jinja'.format(rh_major))\n        except jinja2.exceptions.TemplateNotFound:\n            log.error(\n                'Could not load template rh%s_eth.jinja',\n                rh_major\n            )\n            return ''\n        ifcfg = template.render(opts)\n\n    if 'test' in settings and settings['test']:\n        return _read_temp(ifcfg)\n\n    _write_file_iface(iface, ifcfg, _RH_NETWORK_SCRIPT_DIR, 'ifcfg-{0}')\n    path = os.path.join(_RH_NETWORK_SCRIPT_DIR, 'ifcfg-{0}'.format(iface))\n\n    return _read_file(path)", "output": "Build an interface script for a network interface.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ip.build_interface eth0 eth <settings>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _try_to_compute_deterministic_class_id(cls, depth=5):\n    \"\"\"\n    \"\"\"\n    # Pickling, loading, and pickling again seems to produce more consistent\n    # results than simply pickling. This is a bit\n    class_id = pickle.dumps(cls)\n    for _ in range(depth):\n        new_class_id = pickle.dumps(pickle.loads(class_id))\n        if new_class_id == class_id:\n            # We appear to have reached a fix point, so use this as the ID.\n            return hashlib.sha1(new_class_id).digest()\n        class_id = new_class_id\n\n    # We have not reached a fixed point, so we may end up with a different\n    # class ID for this custom class on each worker, which could lead to the\n    # same class definition being exported many many times.\n    logger.warning(\n        \"WARNING: Could not produce a deterministic class ID for class \"\n        \"{}\".format(cls))\n    return hashlib.sha1(new_class_id).digest()", "output": "Attempt to produce a deterministic class ID for a given class.\n\n    The goal here is for the class ID to be the same when this is run on\n    different worker processes. Pickling, loading, and pickling again seems to\n    produce more consistent results than simply pickling. This is a bit crazy\n    and could cause problems, in which case we should revert it and figure out\n    something better.\n\n    Args:\n        cls: The class to produce an ID for.\n        depth: The number of times to repeatedly try to load and dump the\n            string while trying to reach a fixed point.\n\n    Returns:\n        A class ID for this class. We attempt to make the class ID the same\n            when this function is run on different workers, but that is not\n            guaranteed.\n\n    Raises:\n        Exception: This could raise an exception if cloudpickle raises an\n            exception.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_ae_small_noatt():\n  \"\"\"\"\"\"\n  hparams = transformer_ae_small()\n  hparams.reshape_method = \"slice\"\n  hparams.bottleneck_kind = \"dvq\"\n  hparams.hidden_size = 512\n  hparams.num_blocks = 1\n  hparams.num_decode_blocks = 1\n  hparams.z_size = 12\n  hparams.do_attend_decompress = False\n  return hparams", "output": "Set of hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def corpus_token_counts(\n    text_filepattern, corpus_max_lines, split_on_newlines=True):\n  \"\"\"\n  \"\"\"\n  counts = collections.Counter()\n  for doc in _read_filepattern(\n      text_filepattern,\n      max_lines=corpus_max_lines,\n      split_on_newlines=split_on_newlines):\n    counts.update(encode(_native_to_unicode(doc)))\n\n  mlperf_log.transformer_print(\n      key=mlperf_log.PREPROC_VOCAB_SIZE, value=len(counts))\n  return counts", "output": "Read the corpus and compute a dictionary of token counts.\n\n  Args:\n    text_filepattern: A pattern matching one or more files.\n    corpus_max_lines: An integer; maximum total lines to read.\n    split_on_newlines: A boolean. If true, then split files by lines and strip\n        leading and trailing whitespace from each line. Otherwise, treat each\n        file as a single string.\n\n  Returns:\n    a dictionary mapping token to count.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ParseFromString(self, text, message):\n    \"\"\"\"\"\"\n    if not isinstance(text, str):\n      text = text.decode('utf-8')\n    return self.ParseLines(text.split('\\n'), message)", "output": "Parses a text representation of a protocol message into a message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _new_pool(self, scheme, host, port, request_context=None):\n        \"\"\"\n        \n        \"\"\"\n        pool_cls = self.pool_classes_by_scheme[scheme]\n        if request_context is None:\n            request_context = self.connection_pool_kw.copy()\n\n        # Although the context has everything necessary to create the pool,\n        # this function has historically only used the scheme, host, and port\n        # in the positional args. When an API change is acceptable these can\n        # be removed.\n        for key in ('scheme', 'host', 'port'):\n            request_context.pop(key, None)\n\n        if scheme == 'http':\n            for kw in SSL_KEYWORDS:\n                request_context.pop(kw, None)\n\n        return pool_cls(host, port, **request_context)", "output": "Create a new :class:`ConnectionPool` based on host, port, scheme, and\n        any additional pool keyword arguments.\n\n        If ``request_context`` is provided, it is provided as keyword arguments\n        to the pool class used. This method is used to actually create the\n        connection pools handed out by :meth:`connection_from_url` and\n        companion methods. It is intended to be overridden for customization.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def findSynonyms(self, word, num):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(word, basestring):\n            word = _convert_to_vector(word)\n        words, similarity = self.call(\"findSynonyms\", word, num)\n        return zip(words, similarity)", "output": "Find synonyms of a word\n\n        :param word: a word or a vector representation of word\n        :param num: number of synonyms to find\n        :return: array of (word, cosineSimilarity)\n\n        .. note:: Local use only", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_security_group(self, sec_grp, name=None, desc=None):\n        '''\n        \n        '''\n        sec_grp_id = self._find_security_group_id(sec_grp)\n        body = {'security_group': {}}\n        if name:\n            body['security_group']['name'] = name\n        if desc:\n            body['security_group']['description'] = desc\n        return self.network_conn.update_security_group(sec_grp_id,\n                                                       body=body)", "output": "Updates a security group", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_experiences(self, current_info: AllBrainInfo, next_info: AllBrainInfo):\n        \"\"\"\n        \n        \"\"\"\n        info_student = next_info[self.brain_name]\n        for l in range(len(info_student.agents)):\n            if info_student.local_done[l]:\n                agent_id = info_student.agents[l]\n                self.stats['Environment/Cumulative Reward'].append(\n                    self.cumulative_rewards.get(agent_id, 0))\n                self.stats['Environment/Episode Length'].append(\n                    self.episode_steps.get(agent_id, 0))\n                self.cumulative_rewards[agent_id] = 0\n                self.episode_steps[agent_id] = 0", "output": "Checks agent histories for processing condition, and processes them as necessary.\n        Processing involves calculating value and advantage targets for model updating step.\n        :param current_info: Current AllBrainInfo\n        :param next_info: Next AllBrainInfo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_kde_desktop():\n    \"\"\"\"\"\"\n    if sys.platform.startswith('linux'):\n        xdg_desktop = os.environ.get('XDG_CURRENT_DESKTOP', '')\n        if xdg_desktop:\n            if 'KDE' in xdg_desktop:\n                return True\n            else:\n                return False\n        else:\n            return False\n    else:\n        return False", "output": "Detect if we are running in a KDE desktop", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mtf_resnet_base_single():\n  \"\"\"\"\"\"\n  hparams = mtf_resnet_base()\n  hparams.num_layers = 6\n  hparams.filter_size = 256\n  hparams.block_length = 128\n  hparams.mesh_shape = \"\"\n  hparams.layout = \"\"\n  return hparams", "output": "Small single parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def output(self):\n        \"\"\"\n        \n        \"\"\"\n        return PostgresTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id,\n            port=self.port\n        )", "output": "Returns a PostgresTarget representing the inserted dataset.\n\n        Normally you don't override this.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mean(self, **kwargs):\n        \"\"\"\n        \"\"\"\n        if self._is_transposed:\n            kwargs[\"axis\"] = kwargs.get(\"axis\", 0) ^ 1\n            return self.transpose().mean(**kwargs)\n        # Pandas default is 0 (though not mentioned in docs)\n        axis = kwargs.get(\"axis\", 0)\n        sums = self.sum(**kwargs)\n        counts = self.count(axis=axis, numeric_only=kwargs.get(\"numeric_only\", None))\n        if sums._is_transposed and counts._is_transposed:\n            sums = sums.transpose()\n            counts = counts.transpose()\n        result = sums.binary_op(\"truediv\", counts, axis=axis)\n        return result.transpose() if axis == 0 else result", "output": "Returns the mean for each numerical column or row.\n\n        Return:\n            A new QueryCompiler object containing the mean from each numerical column or\n            row.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def with_fallback(preferred: Dict[str, Any], fallback: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    \n    \"\"\"\n    def merge(preferred_value: Any, fallback_value: Any) -> Any:\n        if isinstance(preferred_value, dict) and isinstance(fallback_value, dict):\n            return with_fallback(preferred_value, fallback_value)\n        elif isinstance(preferred_value, dict) and isinstance(fallback_value, list):\n            # treat preferred_value as a sparse list, where each key is an index to be overridden\n            merged_list = fallback_value\n            for elem_key, preferred_element in preferred_value.items():\n                try:\n                    index = int(elem_key)\n                    merged_list[index] = merge(preferred_element, fallback_value[index])\n                except ValueError:\n                    raise ConfigurationError(\"could not merge dicts - the preferred dict contains \"\n                                             f\"invalid keys (key {elem_key} is not a valid list index)\")\n                except IndexError:\n                    raise ConfigurationError(\"could not merge dicts - the preferred dict contains \"\n                                             f\"invalid keys (key {index} is out of bounds)\")\n            return merged_list\n        else:\n            return copy.deepcopy(preferred_value)\n\n    preferred_keys = set(preferred.keys())\n    fallback_keys = set(fallback.keys())\n    common_keys = preferred_keys & fallback_keys\n\n    merged: Dict[str, Any] = {}\n\n    for key in preferred_keys - fallback_keys:\n        merged[key] = copy.deepcopy(preferred[key])\n    for key in fallback_keys - preferred_keys:\n        merged[key] = copy.deepcopy(fallback[key])\n\n    for key in common_keys:\n        preferred_value = preferred[key]\n        fallback_value = fallback[key]\n\n        merged[key] = merge(preferred_value, fallback_value)\n    return merged", "output": "Deep merge two dicts, preferring values from `preferred`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _shape_text(self, text, colsep=u\"\\t\", rowsep=u\"\\n\",\r\n                    transpose=False, skiprows=0, comments='#'):\r\n        \"\"\"\"\"\"\r\n        assert colsep != rowsep\r\n        out = []\r\n        text_rows = text.split(rowsep)[skiprows:]\r\n        for row in text_rows:\r\n            stripped = to_text_string(row).strip()\r\n            if len(stripped) == 0 or stripped.startswith(comments):\r\n                continue\r\n            line = to_text_string(row).split(colsep)\r\n            line = [try_to_parse(to_text_string(x)) for x in line]\r\n            out.append(line)\r\n        # Replace missing elements with np.nan's or None's\r\n        if programs.is_module_installed('numpy'):\r\n            from numpy import nan\r\n            out = list(zip_longest(*out, fillvalue=nan))\r\n        else:\r\n            out = list(zip_longest(*out, fillvalue=None))\r\n        # Tranpose the last result to get the expected one\r\n        out = [[r[col] for r in out] for col in range(len(out[0]))]\r\n        if transpose:\r\n            return [[r[col] for r in out] for col in range(len(out[0]))]\r\n        return out", "output": "Decode the shape of the given text", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def split_by_idx(self, valid_idx:Collection[int])->'ItemLists':\n        \"\"\n        #train_idx = [i for i in range_of(self.items) if i not in valid_idx]\n        train_idx = np.setdiff1d(arange_of(self.items), valid_idx)\n        return self.split_by_idxs(train_idx, valid_idx)", "output": "Split the data according to the indexes in `valid_idx`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_stdlib_modules():\n    \"\"\"\n    \n    \"\"\"\n    modules = list(sys.builtin_module_names)\n    for path in sys.path[1:]:\n        if 'site-packages' not in path:\n            modules += module_list(path)\n    \n    modules = set(modules)\n    if '__init__' in modules:\n        modules.remove('__init__')\n    modules = list(modules)\n    return modules", "output": "Returns a list containing the names of all the modules available in the\n    standard library.\n    \n    Based on the function get_root_modules from the IPython project.\n    Present in IPython.core.completerlib in v0.13.1\n    \n    Copyright (C) 2010-2011 The IPython Development Team.\n    Distributed under the terms of the BSD License.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_frames(self, path):\n        \"\"\"\n        \n        \"\"\"\n        frames_path = sorted([os.path.join(path, x) for x in os.listdir(path)])\n        frames = [ndimage.imread(frame_path) for frame_path in frames_path]\n        self.handle_type(frames)\n        return self", "output": "Read from frames", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _prime_queue(self, init_points):\n        \"\"\"\"\"\"\n        if self._queue.empty and self._space.empty:\n            init_points = max(init_points, 1)\n\n        for _ in range(init_points):\n            self._queue.add(self._space.random_sample())", "output": "Make sure there's something in the queue at the very beginning.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def float16_activations_var_getter(getter, *args, **kwargs):\n  \"\"\"\n  \"\"\"\n  requested_dtype = kwargs[\"dtype\"]\n\n  if requested_dtype == tf.float16:\n    kwargs[\"dtype\"] = tf.float32\n\n  if requested_dtype == tf.float32:\n    requested_dtype = tf.float16\n  var = getter(*args, **kwargs)\n  # This if statement is needed to guard the cast, because batch norm\n  # assigns directly to the return value of this custom getter. The cast\n  # makes the return value not a variable so it cannot be assigned. Batch\n  # norm variables are always in fp32 so this if statement is never\n  # triggered for them.\n  if var.dtype.base_dtype != requested_dtype:\n    var = tf.cast(var, requested_dtype)\n  return var", "output": "A custom getter function for float32 parameters and float16 activations.\n\n  This function ensures the following:\n    1. All variables requested with type fp16 are stored as type fp32.\n    2. All variables requested with type fp32 are returned as type fp16.\n  See https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/\n  #training_tensorflow for more information on this strategy.\n\n  Args:\n    getter: custom getter\n    *args: arguments\n    **kwargs: keyword arguments\n\n  Returns:\n    variables with the correct dtype.\n\n  Raises:\n    KeyError: if \"dtype\" is not provided as a kwarg.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(zpool):\n    '''\n    \n\n    '''\n    # list for zpool\n    # NOTE: retcode > 0 if zpool does not exists\n    res = __salt__['cmd.run_all'](\n        __utils__['zfs.zpool_command'](\n            command='list',\n            target=zpool,\n        ),\n        python_shell=False,\n        ignore_retcode=True,\n    )\n\n    return res['retcode'] == 0", "output": "Check if a ZFS storage pool is active\n\n    zpool : string\n        Name of storage pool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zpool.exists myzpool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_status_descr_by_id(status_id):\n    '''\n    \n    '''\n    for status_name, status_data in six.iteritems(LINODE_STATUS):\n        if status_data['code'] == int(status_id):\n            return status_data['descr']\n    return LINODE_STATUS.get(status_id, None)", "output": "Return linode status by ID\n\n    status_id\n        linode VM status ID", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def if_modified_since(self) -> Optional[datetime.datetime]:\n        \"\"\"\n        \"\"\"\n        return self._http_date(self.headers.get(hdrs.IF_MODIFIED_SINCE))", "output": "The value of If-Modified-Since HTTP header, or None.\n\n        This header is represented as a `datetime` object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _runlevel():\n    '''\n    \n    '''\n    contextkey = 'systemd._runlevel'\n    if contextkey in __context__:\n        return __context__[contextkey]\n    out = __salt__['cmd.run']('runlevel', python_shell=False, ignore_retcode=True)\n    try:\n        ret = out.split()[1]\n    except IndexError:\n        # The runlevel is unknown, return the default\n        ret = _default_runlevel()\n    __context__[contextkey] = ret\n    return ret", "output": "Return the current runlevel", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def begin(self):\n    \"\"\"\"\"\"\n    if self._cur_batch:\n      raise ValueError('Previous batch is not committed.')\n    self._cur_batch = self._client.batch()\n    self._cur_batch.begin()\n    self._num_mutations = 0", "output": "Begins a batch.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fit_transform(self, data):\n        \"\"\"\n        \n        \"\"\"\n\n        self._setup_from_data(data)\n        ret = self.transform_chain.fit_transform(data)\n        self.__proxy__.update({\"fitted\" : True})\n        return ret", "output": "Fits and transforms the SFrame `data` using a fitted model.\n\n        Parameters\n        ----------\n        data : SFrame\n            The data  to be transformed.\n\n        Returns\n        -------\n        A transformed SFrame.\n\n        Returns\n        -------\n        out: SFrame\n            A transformed SFrame.\n\n        See Also\n        --------\n        fit, transform", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def preview_data_frame_transform(self, body, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"POST\", \"/_data_frame/transforms/_preview\", params=params, body=body\n        )", "output": "`<https://www.elastic.co/guide/en/elasticsearch/reference/current/preview-data-frame-transform.html>`_\n\n        :arg body: The definition for the data_frame transform to preview", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_link(self, path_info):\n        \"\"\"\n        \"\"\"\n        assert path_info[\"scheme\"] == \"local\"\n        path = path_info[\"path\"]\n\n        if not os.path.exists(path):\n            return\n\n        mtime, _ = get_mtime_and_size(path)\n        inode = get_inode(path)\n        relpath = os.path.relpath(path, self.root_dir)\n\n        cmd = (\n            \"REPLACE INTO {}(path, inode, mtime) \"\n            'VALUES (\"{}\", {}, \"{}\")'.format(\n                self.LINK_STATE_TABLE, relpath, self._to_sqlite(inode), mtime\n            )\n        )\n        self._execute(cmd)", "output": "Adds the specified path to the list of links created by dvc. This\n        list is later used on `dvc checkout` to cleanup old links.\n\n        Args:\n            path_info (dict): path info to add to the list of links.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_floating_ip(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        log.error(\n            'The show_floating_ip function must be called with -f or --function.'\n        )\n        return False\n\n    if not kwargs:\n        kwargs = {}\n\n    if 'floating_ip' not in kwargs:\n        log.error('A floating IP is required.')\n        return False\n\n    floating_ip = kwargs['floating_ip']\n    log.debug('Floating ip is %s', floating_ip)\n\n    details = query(method='floating_ips', command=floating_ip)\n\n    return details", "output": "Show the details of a floating IP\n\n    .. versionadded:: 2016.3.0\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-cloud -f show_floating_ip my-digitalocean-config floating_ip='45.55.96.47'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_validating_webhook_configuration(self, name, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_validating_webhook_configuration_with_http_info(name, body, **kwargs)\n        else:\n            (data) = self.replace_validating_webhook_configuration_with_http_info(name, body, **kwargs)\n            return data", "output": "replace the specified ValidatingWebhookConfiguration\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_validating_webhook_configuration(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ValidatingWebhookConfiguration (required)\n        :param V1beta1ValidatingWebhookConfiguration body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1beta1ValidatingWebhookConfiguration\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_params_for_auth(self, headers, querys, auth_settings):\n        \"\"\"\n        \n        \"\"\"\n        if not auth_settings:\n            return\n\n        for auth in auth_settings:\n            auth_setting = self.configuration.auth_settings().get(auth)\n            if auth_setting:\n                if not auth_setting['value']:\n                    continue\n                elif auth_setting['in'] == 'header':\n                    headers[auth_setting['key']] = auth_setting['value']\n                elif auth_setting['in'] == 'query':\n                    querys.append((auth_setting['key'], auth_setting['value']))\n                else:\n                    raise ValueError(\n                        'Authentication token must be in `query` or `header`'\n                    )", "output": "Updates header and query params based on authentication setting.\n\n        :param headers: Header parameters dict to be updated.\n        :param querys: Query parameters tuple list to be updated.\n        :param auth_settings: Authentication setting identifiers list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _gen_unzip(it, elem_len):\n    \"\"\"\n    \"\"\"\n    elem = next(it)\n    first_elem_len = len(elem)\n\n    if elem_len is not None and elem_len != first_elem_len:\n        raise ValueError(\n            'element at index 0 was length %d, expected %d' % (\n                first_elem_len,\n                elem_len,\n            )\n        )\n    else:\n        elem_len = first_elem_len\n\n    yield elem\n    for n, elem in enumerate(it, 1):\n        if len(elem) != elem_len:\n            raise ValueError(\n                'element at index %d was length %d, expected %d' % (\n                    n,\n                    len(elem),\n                    elem_len,\n                ),\n            )\n        yield elem", "output": "Helper for unzip which checks the lengths of each element in it.\n    Parameters\n    ----------\n    it : iterable[tuple]\n        An iterable of tuples. ``unzip`` should map ensure that these are\n        already tuples.\n    elem_len : int or None\n        The expected element length. If this is None it is infered from the\n        length of the first element.\n    Yields\n    ------\n    elem : tuple\n        Each element of ``it``.\n    Raises\n    ------\n    ValueError\n        Raised when the lengths do not match the ``elem_len``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_datasource(jboss_config, name, datasource_properties, profile=None):\n    '''\n    \n    '''\n    log.debug(\"======================== MODULE FUNCTION: jboss7.create_datasource, name=%s, profile=%s\", name, profile)\n    ds_resource_description = __get_datasource_resource_description(jboss_config, name, profile)\n\n    operation = '/subsystem=datasources/data-source=\"{name}\":add({properties})'.format(\n        name=name,\n        properties=__get_properties_assignment_string(datasource_properties, ds_resource_description)\n    )\n    if profile is not None:\n        operation = '/profile=\"{profile}\"'.format(profile=profile) + operation\n\n    return __salt__['jboss7_cli.run_operation'](jboss_config, operation, fail_on_error=False)", "output": "Create datasource in running jboss instance\n\n    jboss_config\n        Configuration dictionary with properties specified above.\n    name\n        Datasource name\n    datasource_properties\n        A dictionary of datasource properties to be created:\n          - driver-name: mysql\n          - connection-url: 'jdbc:mysql://localhost:3306/sampleDatabase'\n          - jndi-name: 'java:jboss/datasources/sampleDS'\n          - user-name: sampleuser\n          - password: secret\n          - min-pool-size: 3\n          - use-java-context: True\n    profile\n        The profile name (JBoss domain mode only)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' jboss7.create_datasource '{\"cli_path\": \"integration.modules.sysmod.SysModuleTest.test_valid_docs\", \"controller\": \"10.11.12.13:9999\", \"cli_user\": \"jbossadm\", \"cli_password\": \"jbossadm\"}' 'my_datasource' '{\"driver-name\": \"mysql\", \"connection-url\": \"jdbc:mysql://localhost:3306/sampleDatabase\", \"jndi-name\": \"java:jboss/datasources/sampleDS\", \"user-name\": \"sampleuser\", \"password\": \"secret\", \"min-pool-size\": 3, \"use-java-context\": True}'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def created(self):\n        \"\"\"\n        \"\"\"\n        value = self._proto.creation_time\n        if value is not None and value != 0:\n            # value will be in milliseconds.\n            return google.cloud._helpers._datetime_from_microseconds(\n                1000.0 * float(value)\n            )", "output": "Union[datetime.datetime, None]: Datetime at which the model was\n        created (:data:`None` until set from the server).\n\n        Read-only.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vad_collector(self, padding_ms=300, ratio=0.75, frames=None):\n        \"\"\"\n        \"\"\"\n        if frames is None: frames = self.frame_generator()\n        num_padding_frames = padding_ms // self.frame_duration_ms\n        ring_buffer = collections.deque(maxlen=num_padding_frames)\n        triggered = False\n\n        for frame in frames:\n            is_speech = self.vad.is_speech(frame, self.sample_rate)\n\n            if not triggered:\n                ring_buffer.append((frame, is_speech))\n                num_voiced = len([f for f, speech in ring_buffer if speech])\n                if num_voiced > ratio * ring_buffer.maxlen:\n                    triggered = True\n                    for f, s in ring_buffer:\n                        yield f\n                    ring_buffer.clear()\n\n            else:\n                yield frame\n                ring_buffer.append((frame, is_speech))\n                num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n                if num_unvoiced > ratio * ring_buffer.maxlen:\n                    triggered = False\n                    yield None\n                    ring_buffer.clear()", "output": "Generator that yields series of consecutive audio frames comprising each utterence, separated by yielding a single None.\n            Determines voice activity by ratio of frames in padding_ms. Uses a buffer to include padding_ms prior to being triggered.\n            Example: (frame, ..., frame, None, frame, ..., frame, None, ...)\n                      |---utterence---|        |---utterence---|", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_validation_labels(val_path):\n    \"\"\"\n    \"\"\"\n    labels_path = tfds.core.get_tfds_path(_VALIDATION_LABELS_FNAME)\n    with tf.io.gfile.GFile(labels_path) as labels_f:\n      labels = labels_f.read().strip().split('\\n')\n    with tf.io.gfile.GFile(val_path, 'rb') as tar_f_obj:\n      tar = tarfile.open(mode='r:', fileobj=tar_f_obj)\n      images = sorted(tar.getnames())\n    return dict(zip(images, labels))", "output": "Returns labels for validation.\n\n    Args:\n      val_path: path to TAR file containing validation images. It is used to\n      retrieve the name of pictures and associate them to labels.\n\n    Returns:\n      dict, mapping from image name (str) to label (str).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_shellwidget(self, shellwidget):\n        \"\"\"\"\"\"\n        self.shellwidget = shellwidget\n        shellwidget.set_figurebrowser(self)\n        shellwidget.sig_new_inline_figure.connect(self._handle_new_figure)", "output": "Bind the shellwidget instance to the figure browser", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_source(self, environment, template):\n        \"\"\"\n        \"\"\"\n        if not self.has_source_access:\n            raise RuntimeError('%s cannot provide access to the source' %\n                               self.__class__.__name__)\n        raise TemplateNotFound(template)", "output": "Get the template source, filename and reload helper for a template.\n        It's passed the environment and template name and has to return a\n        tuple in the form ``(source, filename, uptodate)`` or raise a\n        `TemplateNotFound` error if it can't locate the template.\n\n        The source part of the returned tuple must be the source of the\n        template as unicode string or a ASCII bytestring.  The filename should\n        be the name of the file on the filesystem if it was loaded from there,\n        otherwise `None`.  The filename is used by python for the tracebacks\n        if no loader extension is used.\n\n        The last item in the tuple is the `uptodate` function.  If auto\n        reloading is enabled it's always called to check if the template\n        changed.  No arguments are passed so the function must store the\n        old state somewhere (for example in a closure).  If it returns `False`\n        the template will be reloaded.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_string(self, s, args, kwargs):\n        \"\"\"\n        \"\"\"\n        if isinstance(s, Markup):\n            formatter = SandboxedEscapeFormatter(self, s.escape)\n        else:\n            formatter = SandboxedFormatter(self)\n        kwargs = _MagicFormatMapping(args, kwargs)\n        rv = formatter.vformat(s, args, kwargs)\n        return type(s)(rv)", "output": "If a format call is detected, then this is routed through this\n        method so that our safety sandbox can be used for it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_path(experiment_config, config_path):\n    ''''''\n    expand_path(experiment_config, 'searchSpacePath')\n    if experiment_config.get('trial'):\n        expand_path(experiment_config['trial'], 'codeDir')\n    if experiment_config.get('tuner'):\n        expand_path(experiment_config['tuner'], 'codeDir')\n    if experiment_config.get('assessor'):\n        expand_path(experiment_config['assessor'], 'codeDir')\n    if experiment_config.get('advisor'):\n        expand_path(experiment_config['advisor'], 'codeDir')\n    \n    #if users use relative path, convert it to absolute path\n    root_path = os.path.dirname(config_path)\n    if experiment_config.get('searchSpacePath'):\n        parse_relative_path(root_path, experiment_config, 'searchSpacePath')\n    if experiment_config.get('trial'):\n        parse_relative_path(root_path, experiment_config['trial'], 'codeDir')\n    if experiment_config.get('tuner'):\n        parse_relative_path(root_path, experiment_config['tuner'], 'codeDir')\n    if experiment_config.get('assessor'):\n        parse_relative_path(root_path, experiment_config['assessor'], 'codeDir')\n    if experiment_config.get('advisor'):\n        parse_relative_path(root_path, experiment_config['advisor'], 'codeDir')\n    if experiment_config.get('machineList'):\n        for index in range(len(experiment_config['machineList'])):\n            parse_relative_path(root_path, experiment_config['machineList'][index], 'sshKeyPath')", "output": "Parse path in config file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_unicode(value: Union[None, str, bytes]) -> Optional[str]:  # noqa: F811\n    \"\"\"\n    \"\"\"\n    if isinstance(value, _TO_UNICODE_TYPES):\n        return value\n    if not isinstance(value, bytes):\n        raise TypeError(\"Expected bytes, unicode, or None; got %r\" % type(value))\n    return value.decode(\"utf-8\")", "output": "Converts a string argument to a unicode string.\n\n    If the argument is already a unicode string or None, it is returned\n    unchanged.  Otherwise it must be a byte string and is decoded as utf8.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getent(refresh=False, root=None):\n    '''\n    \n    '''\n    if 'user.getent' in __context__ and not refresh:\n        return __context__['user.getent']\n\n    ret = []\n    if root is not None and __grains__['kernel'] != 'AIX':\n        getpwall = functools.partial(_getpwall, root=root)\n    else:\n        getpwall = functools.partial(pwd.getpwall)\n\n    for data in getpwall():\n        ret.append(_format_info(data))\n    __context__['user.getent'] = ret\n    return ret", "output": "Return the list of all info for all users\n\n    refresh\n        Force a refresh of user information\n\n    root\n        Directory to chroot into\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.getent", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def readline(self):\n        \"\"\"\n        \"\"\"\n        b = super(PtyProcessUnicode, self).readline()\n        return self.decoder.decode(b, final=False)", "output": "Read one line from the pseudoterminal, and return it as unicode.\n\n        Can block if there is nothing to read. Raises :exc:`EOFError` if the\n        terminal was closed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def raw_cron(user):\n    '''\n    \n    '''\n    if _check_instance_uid_match(user) or __grains__.get('os_family') in ('Solaris', 'AIX'):\n        cmd = 'crontab -l'\n        # Preserve line endings\n        lines = salt.utils.data.decode(\n            __salt__['cmd.run_stdout'](cmd,\n                                       runas=user,\n                                       ignore_retcode=True,\n                                       rstrip=False,\n                                       python_shell=False)\n        ).splitlines(True)\n    else:\n        cmd = 'crontab -u {0} -l'.format(user)\n        # Preserve line endings\n        lines = salt.utils.data.decode(\n            __salt__['cmd.run_stdout'](cmd,\n                                       ignore_retcode=True,\n                                       rstrip=False,\n                                       python_shell=False)\n        ).splitlines(True)\n\n    if lines and lines[0].startswith('# DO NOT EDIT THIS FILE - edit the master and reinstall.'):\n        del lines[0:3]\n    return ''.join(lines)", "output": "Return the contents of the user's crontab\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cron.raw_cron root", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def peer_status():\n    '''\n    \n\n\n    '''\n    root = _gluster_xml('peer status')\n    if not _gluster_ok(root):\n        return None\n\n    result = {}\n    for peer in _iter(root, 'peer'):\n        uuid = peer.find('uuid').text\n        result[uuid] = {'hostnames': []}\n        for item in peer:\n            if item.tag == 'hostname':\n                result[uuid]['hostnames'].append(item.text)\n            elif item.tag == 'hostnames':\n                for hostname in item:\n                    if hostname.text not in result[uuid]['hostnames']:\n                        result[uuid]['hostnames'].append(hostname.text)\n            elif item.tag != 'uuid':\n                result[uuid][item.tag] = item.text\n    return result", "output": "Return peer status information\n\n    The return value is a dictionary with peer UUIDs as keys and dicts of peer\n    information as values. Hostnames are listed in one list. GlusterFS separates\n    one of the hostnames but the only reason for this seems to be which hostname\n    happens to be used first in peering.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' glusterfs.peer_status\n\n    GLUSTER direct CLI example (to show what salt is sending to gluster):\n\n        $ gluster peer status\n\n    GLUSTER CLI 3.4.4 return example (so we know what we are parsing):\n\n        Number of Peers: 2\n\n        Hostname: ftp2\n        Port: 24007\n        Uuid: cbcb256b-e66e-4ec7-a718-21082d396c24\n        State: Peer in Cluster (Connected)\n\n        Hostname: ftp3\n        Uuid: 5ea10457-6cb2-427b-a770-7897509625e9\n        State: Peer in Cluster (Connected)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def win_rate(self):\n        \"\"\"\n        \"\"\"\n        data = self.pnl\n        try:\n            return round(len(data.query('pnl_money>0')) / len(data), 2)\n        except ZeroDivisionError:\n            return 0", "output": "\u80dc\u7387\n\n        \u80dc\u7387\n        \u76c8\u5229\u6b21\u6570/\u603b\u6b21\u6570", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hybrid_forward(self, F, x):\n        \"\"\"\"\"\"\n        f = self._factor\n                                             # (N, C*f, W)\n        x = F.reshape(x, (0, -4, -1, f, 0))  # (N, C, f, W)\n        x = F.transpose(x, (0, 1, 3, 2))     # (N, C, W, f)\n        x = F.reshape(x, (0, 0, -3))         # (N, C, W*f)\n        return x", "output": "Perform pixel-shuffling on the input.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _send_signed_request(url, payload):\n    \"\"\"\n    \n    \"\"\"\n    payload64 = _b64(json.dumps(payload).encode('utf8'))\n\n    out = parse_account_key()\n    header = get_boulder_header(out)\n\n    protected = copy.deepcopy(header)\n    protected[\"nonce\"] = urlopen(DEFAULT_CA + \"/directory\").headers['Replay-Nonce']\n    protected64 = _b64(json.dumps(protected).encode('utf8'))\n    cmd = [\n        'openssl', 'dgst',\n        '-sha256',\n        '-sign', os.path.join(gettempdir(), 'account.key')\n    ]\n    proc = subprocess.Popen(\n        cmd,\n        stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n    )\n    out, err = proc.communicate(\"{0}.{1}\".format(protected64, payload64).encode('utf8'))\n    if proc.returncode != 0: # pragma: no cover\n        raise IOError(\"OpenSSL Error: {0}\".format(err))\n    data = json.dumps({\n        \"header\": header, \"protected\": protected64,\n        \"payload\": payload64, \"signature\": _b64(out),\n    })\n    try:\n        resp = urlopen(url, data.encode('utf8'))\n        return resp.getcode(), resp.read()\n    except IOError as e:\n        return getattr(e, \"code\", None), getattr(e, \"read\", e.__str__)()", "output": "Helper function to make signed requests to Boulder", "category": "Python"}]